[
    {
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
        "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
        "authors": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "E. Xing",
            "Haotong Zhang",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "citations": 1000,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities. Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.",
        "authors": [
            "Tianle Cai",
            "Yuhong Li",
            "Zhengyang Geng",
            "Hongwu Peng",
            "Jason D. Lee",
            "De-huai Chen",
            "Tri Dao"
        ],
        "citations": 160,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "abstract": "Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.",
        "authors": [
            "Haoran Xu",
            "Amr Sharaf",
            "Yunmo Chen",
            "Weiting Tan",
            "Lingfeng Shen",
            "Benjamin Van Durme",
            "Kenton Murray",
            "Young Jin Kim"
        ],
        "citations": 125,
        "references": 47,
        "year": 2024
    },
    {
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "abstract": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",
        "authors": [
            "DeepSeek-AI Xiao Bi",
            "Deli Chen",
            "Guanting Chen",
            "Shanhuang Chen",
            "Damai Dai",
            "C. Deng",
            "Honghui Ding",
            "Kai Dong",
            "Qiushi Du",
            "Zhe Fu",
            "Huazuo Gao",
            "Kaige Gao",
            "W. Gao",
            "Ruiqi Ge",
            "Kang Guan",
            "Daya Guo",
            "Jianzhong Guo",
            "Guangbo Hao",
            "Zhewen Hao",
            "Ying He",
            "Wen-Hui Hu",
            "Panpan Huang",
            "Erhang Li",
            "Guowei Li",
            "Jiashi Li",
            "Yao Li",
            "Y. K. Li",
            "W. Liang",
            "Fangyun Lin",
            "A. Liu",
            "Bo Liu (Benjamin Liu)",
            "Wen Liu",
            "Xiaodong Liu",
            "Xin Liu",
            "Yiyuan Liu",
            "Haoyu Lu",
            "Shanghao Lu",
            "Fuli Luo",
            "Shirong Ma",
            "X. Nie",
            "Tian Pei",
            "Yishi Piao",
            "Junjie Qiu",
            "Hui Qu",
            "Tongzheng Ren",
            "Z. Ren",
            "C. Ruan",
            "Zhangli Sha",
            "Zhihong Shao",
            "Jun-Mei Song",
            "Xuecheng Su",
            "Jingxiang Sun",
            "Yaofeng Sun",
            "Min Tang",
            "Bing-Li Wang",
            "Peiyi Wang",
            "Shiyu Wang",
            "Yaohui Wang",
            "Yongji Wang",
            "Tong Wu",
            "Yu Wu",
            "Xin Xie",
            "Zhenda Xie",
            "Ziwei Xie",
            "Yi Xiong",
            "Hanwei Xu",
            "R. X. Xu",
            "Yanhong Xu",
            "Dejian Yang",
            "Yu-mei You",
            "Shuiping Yu",
            "Xin-yuan Yu",
            "Bo Zhang",
            "Haowei Zhang",
            "Lecong Zhang",
            "Liyue Zhang",
            "Mingchuan Zhang",
            "Minghu Zhang",
            "Wentao Zhang",
            "Yichao Zhang",
            "Chenggang Zhao",
            "Yao Zhao",
            "Shangyan Zhou",
            "Shunfeng Zhou",
            "Qihao Zhu",
            "Yuheng Zou"
        ],
        "citations": 187,
        "references": 71,
        "year": 2024
    },
    {
        "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
        "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a\"compute-optimal\"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
        "authors": [
            "Charlie Snell",
            "Jaehoon Lee",
            "Kelvin Xu",
            "Aviral Kumar"
        ],
        "citations": 119,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic",
        "abstract": "In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.",
        "authors": [
            "Ke Chen",
            "Zhao Zhang",
            "Weili Zeng",
            "Richong Zhang",
            "Feng Zhu",
            "Rui Zhao"
        ],
        "citations": 493,
        "references": 57,
        "year": 2023
    },
    {
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration",
        "abstract": "Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce cloud computing costs and protect users' privacy. However, the astronomical model size and the limited hardware resources pose significant deployment challenges. To solve these issues, we propose Activation-aware Weight Quantization (AWQ) and TinyChat, an algorithm-system full-stack solution for efficient on-device LLM deployment. AWQ is a novel quantization method that identifies and protects salient weights based on activation distribution, significantly reducing model size while preserving performance. TinyChat, an optimized inference framework, translates AWQ's theoretical memory savings into practical speedups through techniques such as on-the-fly dequantization, SIMD-aware weight packing, and kernel fusion. Together, they enable 4x model size reduction and 3-4x acceleration across various edge platforms, from high-end desktop GPUs to resource-constrained IoT devices. This solution democratizes on-device LLM deployment, offering privacy-preserving, low-latency AI capabilities across a wide range of applications.",
        "authors": [
            "Ji Lin",
            "Jiaming Tang",
            "Haotian Tang",
            "Shang Yang",
            "Xingyu Dang",
            "Song Han"
        ],
        "citations": 302,
        "references": 68,
        "year": 2023
    },
    {
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
        "abstract": "Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights . AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs’ generalization ability on different domains and modalities, without overfitting to the calibration set; it also does not rely on any data layout reordering, maintaining the hardware efficiency. AWQ outperforms existing work on various language modeling, common sense QA, and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. We also implement efficient tensor core kernels with reorder-free online dequantization to accelerate AWQ, achieving a 1.45 × speedup over GPTQ and is 1.85 × faster than the cuBLAS FP16 implementation. Our method provides a turn-key solution to compress LLMs to 3/4 bits for efficient deployment.",
        "authors": [
            "Ji Lin",
            "Jiaming Tang",
            "Haotian Tang",
            "Shang Yang",
            "Xingyu Dang",
            "Song Han"
        ],
        "citations": 243,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
        "abstract": "We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",
        "authors": [
            "Hakan Inan",
            "K. Upasani",
            "Jianfeng Chi",
            "Rashi Rungta",
            "Krithika Iyer",
            "Yuning Mao",
            "Michael Tontchev",
            "Qing Hu",
            "Brian Fuller",
            "Davide Testuggine",
            "Madian Khabsa"
        ],
        "citations": 248,
        "references": 26,
        "year": 2023
    },
    {
        "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
        "abstract": "Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.",
        "authors": [
            "Guilherme Penedo",
            "Quentin Malartic",
            "Daniel Hesslow",
            "Ruxandra-Aimée Cojocaru",
            "Alessandro Cappelli",
            "Hamza Alobeidli",
            "B. Pannier",
            "Ebtesam Almazrouei",
            "Julien Launay"
        ],
        "citations": 651,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Jailbroken: How Does LLM Safety Training Fail?",
        "abstract": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of\"jailbreak\"attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.",
        "authors": [
            "Alexander Wei",
            "Nika Haghtalab",
            "J. Steinhardt"
        ],
        "citations": 641,
        "references": 67,
        "year": 2023
    },
    {
        "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",
        "abstract": "Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.",
        "authors": [
            "Yiran Ding",
            "L. Zhang",
            "Chengruidong Zhang",
            "Yuanyuan Xu",
            "Ning Shang",
            "Jiahang Xu",
            "Fan Yang",
            "Mao Yang"
        ],
        "citations": 93,
        "references": 26,
        "year": 2024
    },
    {
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
        "abstract": "In this paper, we introduce the \\textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails. Warning: this paper contains example data that may be offensive or harmful.",
        "authors": [
            "Jiaming Ji",
            "Mickel Liu",
            "Juntao Dai",
            "Xuehai Pan",
            "Chi Zhang",
            "Ce Bian",
            "Ruiyang Sun",
            "Yizhou Wang",
            "Yaodong Yang"
        ],
        "citations": 300,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
        "abstract": "Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding",
        "authors": [
            "Yichao Fu",
            "Peter D. Bailis",
            "Ion Stoica",
            "Hao Zhang"
        ],
        "citations": 88,
        "references": 67,
        "year": 2024
    },
    {
        "title": "LLM Evaluators Recognize and Favor Their Own Generations",
        "abstract": "Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others' while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By fine-tuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally.",
        "authors": [
            "Arjun Panickssery",
            "Samuel R. Bowman",
            "Shi Feng"
        ],
        "citations": 85,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Executable Code Actions Elicit Better LLM Agents",
        "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.",
        "authors": [
            "Xingyao Wang",
            "Yangyi Chen",
            "Lifan Yuan",
            "Yizhe Zhang",
            "Yunzhu Li",
            "Hao Peng",
            "Heng Ji"
        ],
        "citations": 74,
        "references": 134,
        "year": 2024
    },
    {
        "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve",
        "abstract": "Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt and produces the first output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency. We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address this throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes. Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency. Furthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between iterations resulting in minimal pipeline bubbles. Our techniques yield significant improvements in inference performance across models and hardware under tail latency constraints. For Mistral-7B on single A100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher serving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM. When used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up to 5.6x gain in the end-to-end serving capacity. The source code for Sarathi-Serve is available at https://github.com/microsoft/sarathi-serve.",
        "authors": [
            "Amey Agrawal",
            "Nitin Kedia",
            "Ashish Panwar",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Bhargav S. Gulavani",
            "Alexey Tumanov",
            "R. Ramjee"
        ],
        "citations": 70,
        "references": 62,
        "year": 2024
    },
    {
        "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
        "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/",
        "authors": [
            "Jun Zhan",
            "Junqi Dai",
            "Jiasheng Ye",
            "Yunhua Zhou",
            "Dong Zhang",
            "Zhigeng Liu",
            "Xin Zhang",
            "Ruibin Yuan",
            "Ge Zhang",
            "Linyang Li",
            "Hang Yan",
            "Jie Fu",
            "Tao Gui",
            "Tianxiang Sun",
            "Yugang Jiang",
            "Xipeng Qiu"
        ],
        "citations": 71,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Understanding the planning of LLM agents: A survey",
        "abstract": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.",
        "authors": [
            "Xu Huang",
            "Weiwen Liu",
            "Xiaolong Chen",
            "Xingmei Wang",
            "Hao Wang",
            "Defu Lian",
            "Yasheng Wang",
            "Ruiming Tang",
            "Enhong Chen"
        ],
        "citations": 62,
        "references": 61,
        "year": 2024
    },
    {
        "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework",
        "abstract": "This technical report presents AutoGen , 1 a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable , and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen ’s design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.",
        "authors": [
            "Qingyun Wu",
            "Gagan Bansal",
            "Jieyu Zhang",
            "Yiran Wu",
            "Shaokun Zhang",
            "Erkang Zhu",
            "Beibin Li",
            "Li Jiang",
            "Xiaoyun Zhang",
            "Chi Wang"
        ],
        "citations": 448,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
        "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",
        "authors": [
            "Ming Jin",
            "Shiyu Wang",
            "Lintao Ma",
            "Zhixuan Chu",
            "James Y. Zhang",
            "X. Shi",
            "Pin-Yu Chen",
            "Yuxuan Liang",
            "Yuan-Fang Li",
            "Shirui Pan",
            "Qingsong Wen"
        ],
        "citations": 224,
        "references": 63,
        "year": 2023
    },
    {
        "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
        "abstract": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
        "authors": [
            "Xinyin Ma",
            "Gongfan Fang",
            "Xinchao Wang"
        ],
        "citations": 275,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
        "abstract": "Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.",
        "authors": [
            "Kai Greshake",
            "Sahar Abdelnabi",
            "Shailesh Mishra",
            "C. Endres",
            "Thorsten Holz",
            "Mario Fritz"
        ],
        "citations": 309,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Exploring and Evaluating Hallucinations in LLM-Powered Code Generation",
        "abstract": "The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.",
        "authors": [
            "Fang Liu",
            "Yang Liu",
            "Lin Shi",
            "Houkun Huang",
            "Ruifeng Wang",
            "Zhen Yang",
            "Li Zhang"
        ],
        "citations": 58,
        "references": 38,
        "year": 2024
    },
    {
        "title": "CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs",
        "abstract": "Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student’s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI’s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.",
        "authors": [
            "Majeed Kazemitabaar",
            "Runlong Ye",
            "Xiaoning Wang",
            "Austin Z. Henley",
            "Paul Denny",
            "Michelle Craig",
            "Tovi Grossman"
        ],
        "citations": 58,
        "references": 64,
        "year": 2024
    },
    {
        "title": "Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts",
        "abstract": "Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.",
        "authors": [
            "J.D. Zamfirescu-Pereira",
            "Richmond Y. Wong",
            "Bjoern Hartmann",
            "Qian Yang"
        ],
        "citations": 471,
        "references": 57,
        "year": 2023
    },
    {
        "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
        "abstract": "Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\\footnote{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.",
        "authors": [
            "B. Liu",
            "Yuqian Jiang",
            "Xiaohan Zhang",
            "Qian Liu",
            "Shiqi Zhang",
            "Joydeep Biswas",
            "Peter Stone"
        ],
        "citations": 311,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
        "abstract": "As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.",
        "authors": [
            "Pat Verga",
            "Sebastian Hofstätter",
            "Sophia Althammer",
            "Yixuan Su",
            "Aleksandra Piktus",
            "Arkady Arkhangorodsky",
            "Minjie Xu",
            "Naomi White",
            "Patrick Lewis"
        ],
        "citations": 49,
        "references": 33,
        "year": 2024
    },
    {
        "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",
        "abstract": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.",
        "authors": [
            "Yuanchun Li",
            "Hao Wen",
            "Weijun Wang",
            "Xiangyu Li",
            "Yizhen Yuan",
            "Guohong Liu",
            "Jiacheng Liu",
            "Wenxing Xu",
            "Xiang Wang",
            "Yi Sun",
            "Rui Kong",
            "Yile Wang",
            "Hanfei Geng",
            "Jian Luan",
            "Xuefeng Jin",
            "Zi-Liang Ye",
            "Guanjing Xiong",
            "Fan Zhang",
            "Xiang Li",
            "Mengwei Xu",
            "Zhijun Li",
            "Peng Li",
            "Yang Liu",
            "Yaqiong Zhang",
            "Yunxin Liu"
        ],
        "citations": 93,
        "references": 0,
        "year": 2024
    },
    {
        "title": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
        "abstract": "There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of {\\bf LLM-Modulo Frameworks} that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.",
        "authors": [
            "Subbarao Kambhampati",
            "Karthik Valmeekam",
            "L. Guan",
            "Kaya Stechly",
            "Mudit Verma",
            "Siddhant Bhambri",
            "Lucas Saldyt",
            "Anil Murthy"
        ],
        "citations": 79,
        "references": 52,
        "year": 2024
    },
    {
        "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
        "abstract": "It is well known that LLMs cannot generalize well to long contexts whose lengths are larger than the training sequence length. This poses challenges when employing LLMs for processing long input sequences during inference. In this work, we argue that LLMs themselves have inherent capabilities to handle long contexts without fine-tuning. To achieve this goal, we propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range. The two-level attentions are computed based on the original model's self-attention mechanism during inference. With minor code modification, our SelfExtend can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments on multiple benchmarks and the results show that our SelfExtend can effectively extend existing LLMs' context window length. The code can be found at \\url{https://github.com/datamllab/LongLM}.",
        "authors": [
            "Hongye Jin",
            "Xiaotian Han",
            "Jingfeng Yang",
            "Zhimeng Jiang",
            "Zirui Liu",
            "Chia-yuan Chang",
            "Huiyuan Chen",
            "Xia Hu"
        ],
        "citations": 73,
        "references": 55,
        "year": 2024
    },
    {
        "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",
        "abstract": "While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.",
        "authors": [
            "Biao Zhang",
            "Zhongtao Liu",
            "Colin Cherry",
            "Orhan Firat"
        ],
        "citations": 80,
        "references": 56,
        "year": 2024
    },
    {
        "title": "ST-LLM: Large Language Models Are Effective Temporal Learners",
        "abstract": "Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.",
        "authors": [
            "Ruyang Liu",
            "Chen Li",
            "Haoran Tang",
            "Yixiao Ge",
            "Ying Shan",
            "Ge Li"
        ],
        "citations": 44,
        "references": 54,
        "year": 2024
    },
    {
        "title": "NExT-GPT: Any-to-Any Multimodal LLM",
        "abstract": "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/",
        "authors": [
            "Shengqiong Wu",
            "Hao Fei",
            "Leigang Qu",
            "Wei Ji",
            "Tat-Seng Chua"
        ],
        "citations": 332,
        "references": 136,
        "year": 2023
    },
    {
        "title": "The Internal State of an LLM Knows When its Lying",
        "abstract": "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\% to 83\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",
        "authors": [
            "A. Azaria",
            "Tom M. Mitchell"
        ],
        "citations": 242,
        "references": 20,
        "year": 2023
    },
    {
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
        "abstract": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
        "authors": [
            "Dongfu Jiang",
            "Xiang Ren",
            "Bill Yuchen Lin"
        ],
        "citations": 177,
        "references": 53,
        "year": 2023
    },
    {
        "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
        "abstract": "Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.",
        "authors": [
            "Tim Dettmers",
            "Ruslan Svirschevski",
            "Vage Egiazarian",
            "Denis Kuznedelev",
            "Elias Frantar",
            "Saleh Ashkboos",
            "Alexander Borzunov",
            "T. Hoefler",
            "Dan Alistarh"
        ],
        "citations": 177,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Prompt Injection attack against LLM-integrated Applications",
        "abstract": "Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.",
        "authors": [
            "Yi Liu",
            "Gelei Deng",
            "Yuekang Li",
            "Kailong Wang",
            "Tianwei Zhang",
            "Yepang Liu",
            "Haoyu Wang",
            "Yanhong Zheng",
            "Yang Liu"
        ],
        "citations": 241,
        "references": 56,
        "year": 2023
    },
    {
        "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving",
        "abstract": "Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.",
        "authors": [
            "Yujun Lin",
            "Haotian Tang",
            "Shang Yang",
            "Zhekai Zhang",
            "Guangxuan Xiao",
            "Chuang Gan",
            "Song Han"
        ],
        "citations": 37,
        "references": 44,
        "year": 2024
    },
    {
        "title": "OpenAGI: When LLM Meets Domain Experts",
        "abstract": "Human intelligence excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive intelligent models, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.",
        "authors": [
            "Yingqiang Ge",
            "Wenyue Hua",
            "Jianchao Ji",
            "Juntao Tan",
            "Shuyuan Xu",
            "Yongfeng Zhang"
        ],
        "citations": 169,
        "references": 64,
        "year": 2023
    },
    {
        "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
        "abstract": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.",
        "authors": [
            "Zhiqiang Hu",
            "Yihuai Lan",
            "Lei Wang",
            "Wanyu Xu",
            "Ee-Peng Lim",
            "R. Lee",
            "Lidong Bing",
            "Soujanya Poria"
        ],
        "citations": 171,
        "references": 57,
        "year": 2023
    },
    {
        "title": "An Embarrassingly Simple Approach for LLM with Strong ASR Capacity",
        "abstract": "In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data. Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment. We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community.",
        "authors": [
            "Ziyang Ma",
            "Yifan Yang",
            "Yifan Yang",
            "Zhifu Gao",
            "Jiaming Wang",
            "Zhihao Du",
            "Fan Yu",
            "Qian Chen",
            "Siqi Zheng",
            "Shiliang Zhang",
            "Xie Chen"
        ],
        "citations": 35,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads",
        "abstract": "Transformer-based large language model (LLM) inference serving is now the backbone of many cloud services. LLM inference consists of a prefill phase and a decode phase. However, existing LLM deployment practices often overlook the distinct characteristics of these phases, leading to significant interference. To mitigate interference, our insight is to carefully schedule and group inference requests based on their characteristics. We realize this idea in TetriInfer through three pillars. First, it partitions prompts into fixed-size chunks so that the accelerator always runs close to its computationsaturated limit. Second, it disaggregates prefill and decode instances so each can run independently. Finally, it uses a smart two-level scheduling algorithm augmented with predicted resource usage to avoid decode scheduling hotspots. Results show that TetriInfer improves time-to-first-token (TTFT), job completion time (JCT), and inference efficiency in turns of performance per dollar by a large margin, e.g., it uses 38% less resources all the while lowering average TTFT and average JCT by 97% and 47%, respectively.",
        "authors": [
            "Cunchen Hu",
            "Heyang Huang",
            "Liangliang Xu",
            "Xusheng Chen",
            "Jiang Xu",
            "Shuang Chen",
            "Hao Feng",
            "Chenxi Wang",
            "Sa Wang",
            "Yungang Bao",
            "Ninghui Sun",
            "Yizhou Shan"
        ],
        "citations": 34,
        "references": 39,
        "year": 2024
    },
    {
        "title": "3D-LLM: Injecting the 3D World into Large Language Models",
        "abstract": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 1M 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on held-out evaluation dataset, ScanQA, SQA3D and 3DMV-VQA, outperform state-of-the-art baselines. In particular, experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ( e.g. , the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/ .",
        "authors": [
            "Yining Hong",
            "Haoyu Zhen",
            "Peihao Chen",
            "Shuhong Zheng",
            "Yilun Du",
            "Zhenfang Chen",
            "Chuang Gan"
        ],
        "citations": 181,
        "references": 52,
        "year": 2023
    },
    {
        "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
        "abstract": "Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.",
        "authors": [
            "Yidong Wang",
            "Zhuohao Yu",
            "Zhengran Zeng",
            "Linyi Yang",
            "Cunxiang Wang",
            "Hao Chen",
            "Chaoya Jiang",
            "Rui Xie",
            "Jindong Wang",
            "Xingxu Xie",
            "Wei Ye",
            "Shi-Bo Zhang",
            "Yue Zhang"
        ],
        "citations": 170,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
        "abstract": "Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data. To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method based on two scores, i.e., influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of sample removal on the overall performance. To achieve low costs of the data pruning process, we use a small-sized surrogate model to replace LLMs to obtain the influence score. Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. Empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, the proposed method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%.",
        "authors": [
            "Xinyu Lin",
            "Wenjie Wang",
            "Yongqi Li",
            "Shuo Yang",
            "Fuli Feng",
            "Yinwei Wei",
            "Tat-Seng Chua"
        ],
        "citations": 53,
        "references": 64,
        "year": 2024
    },
    {
        "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models",
        "abstract": "Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.",
        "authors": [
            "Mengwei Xu",
            "Wangsong Yin",
            "Dongqi Cai",
            "Rongjie Yi",
            "Daliang Xu",
            "Qipeng Wang",
            "Bingyang Wu",
            "Yihao Zhao",
            "Chen Yang",
            "Shihe Wang",
            "Qiyang Zhang",
            "Zhenyan Lu",
            "Li Zhang",
            "Shangguang Wang",
            "Yuanchun Li",
            "Yunxin Liu",
            "Xin Jin",
            "Xuanzhe Liu"
        ],
        "citations": 52,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache",
        "abstract": "Large Language Models (LLMs) demonstrate substantial potential across a diverse array of domains via request serving. However, as trends continue to push for expanding context sizes, the autoregressive nature of LLMs results in highly dynamic behavior of the attention layers, showcasing significant differences in computational characteristics and memory requirements from the non-attention layers. This presents substantial challenges for resource management and performance optimization in service systems. Existing static model parallelism and resource allocation strategies fall short when dealing with this dynamicity. To address the issue, we propose Infinite-LLM, a novel LLM serving system designed to effectively handle dynamic context lengths. Infinite-LLM disaggregates attention layers from an LLM's inference process, facilitating flexible and independent resource scheduling that optimizes computational performance and enhances memory utilization jointly. By leveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only significantly boosts system throughput but also supports extensive context lengths. Evaluated on a dataset with context lengths ranging from a few to 2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates throughput improvement of 1.35-3.4x compared to state-of-the-art methods, enabling efficient and elastic LLM deployment.",
        "authors": [
            "Bin Lin",
            "Tao Peng",
            "Chen Zhang",
            "Minmin Sun",
            "Lanbo Li",
            "Hanyu Zhao",
            "Wencong Xiao",
            "Qi Xu",
            "Xiafei Qiu",
            "Shen Li",
            "Zhigang Ji",
            "Yong Li",
            "Wei Lin"
        ],
        "citations": 33,
        "references": 67,
        "year": 2024
    },
    {
        "title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents",
        "abstract": "Evaluating Large Language Models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.",
        "authors": [
            "Chang Ma",
            "Junlei Zhang",
            "Zhihao Zhu",
            "Cheng Yang",
            "Yujiu Yang",
            "Yaohui Jin",
            "Zhenzhong Lan",
            "Lingpeng Kong",
            "Junxian He"
        ],
        "citations": 33,
        "references": 58,
        "year": 2024
    },
    {
        "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
        "abstract": "The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework identifies the bottlenecks when deploying LLMs on hardware devices and provides a clear understanding of practical problems, such as why LLMs are memory-bound, how much memory and computation they need, and how to choose the right hardware. We systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as model compression (e.g., Knowledge Distillation and Quantization), algorithm improvements (e.g., Early Exit and Mixture-of-Expert), and both hardware and system-level enhancements. Our survey stands out by analyzing these methods with roofline model, helping us understand their impact on memory access and computation. This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient LLM deployment. The analyze tool, LLM-Viewer, is open-sourced.",
        "authors": [
            "Zhihang Yuan",
            "Yuzhang Shang",
            "Yang Zhou",
            "Zhen Dong",
            "Chenhao Xue",
            "Bingzhe Wu",
            "Zhikai Li",
            "Qingyi Gu",
            "Yong Jae Lee",
            "Yan Yan",
            "Beidi Chen",
            "Guangyu Sun",
            "Kurt Keutzer"
        ],
        "citations": 47,
        "references": 234,
        "year": 2024
    },
    {
        "title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods",
        "abstract": "Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods.",
        "authors": [
            "Hanlei Jin",
            "Yang Zhang",
            "Dan Meng",
            "Jun Wang",
            "Jinghua Tan"
        ],
        "citations": 48,
        "references": 222,
        "year": 2024
    },
    {
        "title": "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction",
        "abstract": "To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our code will be available at \\url{https://github.com/microsoft/JARVIS/} in the future.",
        "authors": [
            "Siyu Yuan",
            "Kaitao Song",
            "Jiangjie Chen",
            "Xu Tan",
            "Yongliang Shen",
            "Ren Kan",
            "Dongsheng Li",
            "Deqing Yang"
        ],
        "citations": 32,
        "references": 26,
        "year": 2024
    },
    {
        "title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",
        "abstract": "Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.",
        "authors": [
            "Shangbin Feng",
            "Weijia Shi",
            "Yike Wang",
            "Wenxuan Ding",
            "Vidhisha Balachandran",
            "Yulia Tsvetkov"
        ],
        "citations": 44,
        "references": 89,
        "year": 2024
    },
    {
        "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
        "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 65% accuracy. We release all questions, code, and model answers. Questions will be added and updated on a monthly basis, and we will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.",
        "authors": [
            "Colin White",
            "Samuel Dooley",
            "∗. ManleyRoberts",
            "Arka Pal",
            "Ben Feuer",
            "Siddhartha Jain",
            "Ravid Shwartz-Ziv",
            "Neel Jain",
            "Khalid Saifullah",
            "Siddartha Naidu",
            "Chinmay Hegde",
            "Yann LeCun",
            "Tom Goldstein",
            "W. Neiswanger",
            "Micah Goldblum",
            "Abacus.AI",
            "Nyu",
            "Nvidia"
        ],
        "citations": 25,
        "references": 62,
        "year": 2024
    },
    {
        "title": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair",
        "abstract": "Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.",
        "authors": [
            "Islem Bouzenia",
            "Prem Devanbu",
            "Michael Pradel"
        ],
        "citations": 27,
        "references": 75,
        "year": 2024
    },
    {
        "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems",
        "abstract": "Large Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on. Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems. However, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects. Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints. To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4. Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components. We found that although the OpenAI GPT4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user's chat history, all without the need to manipulate the user's input or gain direct access to OpenAI GPT4. Our demo is in the link: https://fzwark.github.io/LLM-System-Attack-Demo/",
        "authors": [
            "Fangzhou Wu",
            "Ning Zhang",
            "Somesh Jha",
            "P. McDaniel",
            "Chaowei Xiao"
        ],
        "citations": 41,
        "references": 50,
        "year": 2024
    },
    {
        "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
        "abstract": "Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on the harmlessness of LLM-generated content in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction records. R-Judge comprises 569 records of multi-turn agent interaction, encompassing 27 key risk scenarios among 5 application categories and 10 risk types. It is of high-quality curation with annotated safety labels and risk descriptions. Evaluation of 11 LLMs on R-Judge shows considerable room for enhancing the risk awareness of LLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models significantly exceed the random. Moreover, we reveal that risk awareness in open agent scenarios is a multi-dimensional capability involving knowledge and reasoning, thus challenging for LLMs. With further experiments, we find that fine-tuning on safety judgment significantly improve model performance while straightforward prompting mechanisms fail. R-Judge is publicly available at https://github.com/Lordog/R-Judge.",
        "authors": [
            "Tongxin Yuan",
            "Zhiwei He",
            "Lingzhong Dong",
            "Yiming Wang",
            "Ruijie Zhao",
            "Tian Xia",
            "Lizhen Xu",
            "Binglin Zhou",
            "Fangqi Li",
            "Zhuosheng Zhang",
            "Rui Wang",
            "Gongshen Liu"
        ],
        "citations": 40,
        "references": 81,
        "year": 2024
    },
    {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "abstract": "Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.",
        "authors": [
            "Chi-Min Chan",
            "Weize Chen",
            "Yusheng Su",
            "Jianxuan Yu",
            "Wei Xue",
            "Shan Zhang",
            "Jie Fu",
            "Zhiyuan Liu"
        ],
        "citations": 310,
        "references": 49,
        "year": 2023
    },
    {
        "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly",
        "abstract": null,
        "authors": [
            "Yifan Yao",
            "Jinhao Duan",
            "Kaidi Xu",
            "Yuanfang Cai",
            "Eric Sun",
            "Yue Zhang"
        ],
        "citations": 288,
        "references": 340,
        "year": 2023
    },
    {
        "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
        "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.",
        "authors": [
            "Weizhou Shen",
            "Chenliang Li",
            "Hongzhan Chen",
            "Ming Yan",
            "Xiaojun Quan",
            "Hehong Chen",
            "Ji Zhang",
            "Fei Huang"
        ],
        "citations": 37,
        "references": 48,
        "year": 2024
    },
    {
        "title": "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts",
        "abstract": "As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment",
        "authors": [
            "Shaona Ghosh",
            "Prasoon Varshney",
            "Erick Galinkin",
            "Christopher Parisien"
        ],
        "citations": 19,
        "references": 32,
        "year": 2024
    },
    {
        "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
        "abstract": "Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning.",
        "authors": [
            "Jinghan Jia",
            "Yihua Zhang",
            "Yimeng Zhang",
            "Jiancheng Liu",
            "Bharat Runwal",
            "James Diffenderfer",
            "B. Kailkhura",
            "Sijia Liu"
        ],
        "citations": 19,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
        "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.",
        "authors": [
            "Xudong Guo",
            "Kaixuan Huang",
            "Jiale Liu",
            "Wenhui Fan",
            "Natalia V'elez",
            "Qingyun Wu",
            "Huazheng Wang",
            "Thomas L. Griffiths",
            "Mengdi Wang"
        ],
        "citations": 20,
        "references": 75,
        "year": 2024
    },
    {
        "title": "SpinQuant: LLM quantization with learned rotations",
        "abstract": "Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures while enhancing quantization accuracy. In addition, we find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant, a novel approach that incorporates learned rotation matrices for optimal quantized network accuracy. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
        "authors": [
            "Zechun Liu",
            "Changsheng Zhao",
            "Igor Fedorov",
            "Bilge Soran",
            "Dhruv Choudhary",
            "Raghuraman Krishnamoorthi",
            "Vikas Chandra",
            "Yuandong Tian",
            "Tijmen Blankevoort"
        ],
        "citations": 34,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
        "abstract": "Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.",
        "authors": [
            "Zhichen Dong",
            "Zhanhui Zhou",
            "Chao Yang",
            "Jing Shao",
            "Yu Qiao"
        ],
        "citations": 34,
        "references": 99,
        "year": 2024
    },
    {
        "title": "(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",
        "abstract": "Large language models (LLMs) are increasingly capable of providing users with advice in a wide range of professional domains, including legal advice. However, relying on LLMs for legal queries raises concerns due to the significant expertise required and the potential real-world consequences of the advice. To explore when and why LLMs should or should not provide advice to users, we conducted workshops with 20 legal experts using methods inspired by case-based reasoning. The provided realistic queries (“cases”) allowed experts to examine granular, situation-specific concerns and overarching technical and legal constraints, producing a concrete set of contextual considerations for LLM developers. By synthesizing the factors that impacted LLM response appropriateness, we present a 4-dimension framework: (1) User attributes and behaviors, (2) Nature of queries, (3) AI capabilities, and (4) Social impacts. We share experts’ recommendations for LLM response strategies, which center around helping users identify ‘right questions to ask’ and relevant information rather than providing definitive legal judgments. Our findings reveal novel legal considerations, such as unauthorized practice of law, confidentiality, and liability for inaccurate advice, that have been overlooked in the literature. The case-based deliberation method enabled us to elicit fine-grained, practice-informed insights that surpass those from de-contextualized surveys or speculative principles. These findings underscore the applicability of our method for translating domain-specific professional knowledge and practices into policies that can guide LLM behavior in a more responsible direction.",
        "authors": [
            "Inyoung Cheong",
            "King Xia",
            "K. Feng",
            "Quan Ze Chen",
            "Amy X. Zhang"
        ],
        "citations": 34,
        "references": 102,
        "year": 2024
    },
    {
        "title": "LLM Critics Help Catch LLM Bugs",
        "abstract": "Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains\"critic\"models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as\"flawless\", even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone.",
        "authors": [
            "Nat McAleese",
            "Rai Michael Pokorny",
            "Juan Felipe Cer'on Uribe",
            "Evgenia Nitishinskaya",
            "Maja Trebacz",
            "Jan Leike"
        ],
        "citations": 36,
        "references": 28,
        "year": 2024
    },
    {
        "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers",
        "abstract": "Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT4, as the evaluator. Alterna-tively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the fine-tuned judge models achieve high accuracy on in-domain test sets, even surpassing GPT4, they are inherently task-specific classifiers, and their generalizability and fairness severely underperform GPT4.",
        "authors": [
            "Hui Huang",
            "Yingqi Qu",
            "Jing Liu",
            "Muyun Yang",
            "Tiejun Zhao"
        ],
        "citations": 36,
        "references": 19,
        "year": 2024
    },
    {
        "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents",
        "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.",
        "authors": [
            "Ke Yang",
            "Jiateng Liu",
            "John Wu",
            "Chaoqi Yang",
            "Y. Fung",
            "Sha Li",
            "Zixuan Huang",
            "Xu Cao",
            "Xingyao Wang",
            "Yiquan Wang",
            "Heng Ji",
            "Chengxiang Zhai"
        ],
        "citations": 55,
        "references": 168,
        "year": 2024
    },
    {
        "title": "Challenges and barriers of using large language models (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology – a recent scoping review",
        "abstract": null,
        "authors": [
            "E. Ullah",
            "Anil Parwani",
            "M. Baig",
            "Rajendra Singh"
        ],
        "citations": 51,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
        "abstract": "Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears \\emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \\emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.",
        "authors": [
            "Shreya Shankar",
            "J.D. Zamfirescu-Pereira",
            "Bjorn Hartmann",
            "Aditya G. Parameswaran",
            "Ian Arawjo"
        ],
        "citations": 33,
        "references": 59,
        "year": 2024
    },
    {
        "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
        "abstract": "This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.",
        "authors": [
            "Zihao Yi",
            "Jiarui Ouyang",
            "Yuwen Liu",
            "Tianhao Liao",
            "Zhe Xu",
            "Ying Shen"
        ],
        "citations": 31,
        "references": 149,
        "year": 2024
    },
    {
        "title": "A multimodal approach to cross-lingual sentiment analysis with ensemble of transformer and LLM",
        "abstract": null,
        "authors": [
            "Md. Saef Ullah Miah",
            "M. Kabir",
            "Talha Bin Sarwar",
            "Mejdl S. Safran",
            "Sultan Alfarhood",
            "M. Mridha"
        ],
        "citations": 32,
        "references": 46,
        "year": 2024
    },
    {
        "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
        "abstract": "Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.",
        "authors": [
            "Zechun Liu",
            "Barlas Oğuz",
            "Changsheng Zhao",
            "Ernie Chang",
            "Pierre Stock",
            "Yashar Mehdad",
            "Yangyang Shi",
            "Raghuraman Krishnamoorthi",
            "Vikas Chandra"
        ],
        "citations": 144,
        "references": 47,
        "year": 2023
    },
    {
        "title": "LLM Agents can Autonomously Exploit One-day Vulnerabilities",
        "abstract": "LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities. In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents.",
        "authors": [
            "Richard Fang",
            "R. Bindu",
            "Akul Gupta",
            "Daniel Kang"
        ],
        "citations": 32,
        "references": 53,
        "year": 2024
    },
    {
        "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning",
        "abstract": "Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs.",
        "authors": [
            "Ming Li",
            "Lichang Chen",
            "Jiuhai Chen",
            "Shwai He",
            "Jiuxiang Gu",
            "Tianyi Zhou"
        ],
        "citations": 29,
        "references": 58,
        "year": 2024
    },
    {
        "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
        "abstract": "Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.",
        "authors": [
            "Wenkai Yang",
            "Xiaohan Bi",
            "Yankai Lin",
            "Sishuo Chen",
            "Jie Zhou",
            "Xu Sun"
        ],
        "citations": 30,
        "references": 70,
        "year": 2024
    },
    {
        "title": "Systematic Biases in LLM Simulations of Debates",
        "abstract": "The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. Current research suggests that LLM-based agents become increasingly human-like in their performance, sparking interest in using these AI agents as substitutes for human participants in behavioral studies. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs’ ability to simulate political debates on topics that are important aspects of people’s day-to-day lives and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model’s inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.",
        "authors": [
            "Amir Taubenfeld",
            "Yaniv Dover",
            "Roi Reichart",
            "Ariel Goldstein"
        ],
        "citations": 29,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Character-LLM: A Trainable Agent for Role-Playing",
        "abstract": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \\textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",
        "authors": [
            "Yunfan Shao",
            "Linyang Li",
            "Junqi Dai",
            "Xipeng Qiu"
        ],
        "citations": 140,
        "references": 106,
        "year": 2023
    },
    {
        "title": "Understanding Large-Language Model (LLM)-powered Human-Robot Interaction",
        "abstract": "Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.CCS CONCEPTS• Human-centered computing → HCI design and evaluation methods; • Computing methodologies → Natural language processing; • Computer systems organization → Robotics.",
        "authors": [
            "Callie Y. Kim",
            "Christine P. Lee",
            "Bilge Mutlu"
        ],
        "citations": 30,
        "references": 80,
        "year": 2024
    },
    {
        "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
        "abstract": "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.",
        "authors": [
            "Qingyun Wu",
            "Gagan Bansal",
            "Jieyu Zhang",
            "Yiran Wu",
            "Beibin Li",
            "Erkang Zhu",
            "Li Jiang",
            "Xiaoyun Zhang",
            "Shaokun Zhang",
            "Jiale Liu",
            "A. Awadallah",
            "Ryen W. White",
            "Doug Burger",
            "Chi Wang"
        ],
        "citations": 140,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents",
        "abstract": "In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the\"Gorilla\"model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.",
        "authors": [
            "Yashar Talebirad",
            "Amirhossein Nadiri"
        ],
        "citations": 136,
        "references": 16,
        "year": 2023
    },
    {
        "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",
        "abstract": "Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. Macaw-LLM consists of three main components: a modality module for encoding multi-modal data, a cognitive module for harnessing pretrained LLMs, and an alignment module for harmonizing diverse representations. Our novel alignment module seamlessly bridges multi-modal features to textual features, simplifying the adaptation process from the modality modules to the cognitive module. In addition, we construct a large-scale multi-modal instruction dataset in terms of multi-turn dialogue, including 69K image instances and 50K video instances. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs to handle diverse data modalities and address complex real-world scenarios.",
        "authors": [
            "Chenyang Lyu",
            "Minghao Wu",
            "Longyue Wang",
            "Xinting Huang",
            "Bingshuai Liu",
            "Zefeng Du",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "citations": 132,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast",
        "abstract": "This study focuses on the utilization of Large Language Models (LLMs) for the rapid development of applications, with a spotlight on LangChain, an open-source software library. LLMs have been rapidly adopted due to their capabilities in a range of tasks, including essay composition, code writing, explanation, and debugging, with OpenAI’s ChatGPT popularizing their usage among millions ofusers. The crux of the study centers around LangChain, designed to expedite the development of bespoke AI applications using LLMs. LangChain has been widely recognized in the AI community for its ability to seamlessly interact with various data sources and applications. The paper provides an examination of LangChain's core features, including its components and chains, acting as modular abstractions and customizable, use-case-specific pipelines, respectively. Through a series of practical examples, the study elucidates the potential of this framework in fostering the swift development of LLM-based applications.",
        "authors": [
            "Oguzhan Topsakal",
            "T. Akinci"
        ],
        "citations": 134,
        "references": 15,
        "year": 2023
    },
    {
        "title": "A Survey on Data Selection for LLM Instruction Tuning",
        "abstract": "Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.",
        "authors": [
            "Jiahao Wang",
            "Bolin Zhang",
            "Qianlong Du",
            "Jiajun Zhang",
            "Dianhui Chu"
        ],
        "citations": 28,
        "references": 34,
        "year": 2024
    },
    {
        "title": "From LLM to NMT: Advancing Low-Resource Machine Translation with Claude",
        "abstract": "We show that Claude 3 Opus, a large language model (LLM) released by Anthropic in March 2024, exhibits stronger machine translation competence than other LLMs. Though we find evidence of data contamination with Claude on FLORES-200, we curate new benchmarks that corroborate the effectiveness of Claude for low-resource machine translation into English. We find that Claude has remarkable \\textit{resource efficiency} -- the degree to which the quality of the translation model depends on a language pair's resource level. Finally, we show that advancements in LLM translation can be compressed into traditional neural machine translation (NMT) models. Using Claude to generate synthetic data, we demonstrate that knowledge distillation advances the state-of-the-art in Yoruba-English translation, meeting or surpassing strong baselines like NLLB-54B and Google Translate.",
        "authors": [
            "Maxim Enis",
            "Mark Hopkins"
        ],
        "citations": 27,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Generative Echo Chamber? Effect of LLM-Powered Search Systems on Diverse Information Seeking",
        "abstract": "Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers—limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user’s view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implications for the development of LLMs and conversational search systems, and the policy governing these technologies.",
        "authors": [
            "Nikhil Sharma",
            "Q. V. Liao",
            "Ziang Xiao"
        ],
        "citations": 28,
        "references": 74,
        "year": 2024
    },
    {
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
        "abstract": "Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focus from massive data acquisition and new model training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage existing generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky’s theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed and applied to FSA. The framework instantiates specialized agents using prior guiding knowledge from both linguistics and finance. Then, a summative agent reasons on the aggregated agent discussions. Comprehensive evaluations using six FSA datasets show that the framework yields better accuracies compared to many alternative multi-LLM agent settings, especially when the discussion contents are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA and potentially other tasks. Lastly, implications for business and management have also been discussed.",
        "authors": [
            "Frank Xing"
        ],
        "citations": 27,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems",
        "abstract": "Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls – e.g., when asking the LLM to answer each question multiple times and taking a consensus – affects such a compound system’s performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems – one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems’ performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LLM calls lead to higher performance on “easy” queries, but lower performance on “hard” queries, and non-monotone behavior emerges when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of LLM calls that maximizes system performance, and define a scaling law of Voting Inference Systems. Experiments show that our scaling law can predict the performance of Voting Inference Systems and find the optimal number of LLM calls to make.",
        "authors": [
            "Lingjiao Chen",
            "J. Q. Davis",
            "Boris Hanin",
            "Peter D. Bailis",
            "Ion Stoica",
            "Matei Zaharia",
            "James Zou"
        ],
        "citations": 27,
        "references": 15,
        "year": 2024
    },
    {
        "title": "Determinants of LLM-assisted Decision-Making",
        "abstract": "Decision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decision-specific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.",
        "authors": [
            "Eva Eigner",
            "Thorsten Händler"
        ],
        "citations": 28,
        "references": 205,
        "year": 2024
    },
    {
        "title": "LLM Agents can Autonomously Hack Websites",
        "abstract": "In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.",
        "authors": [
            "Richard Fang",
            "R. Bindu",
            "Akul Gupta",
            "Qiusi Zhan",
            "Daniel Kang"
        ],
        "citations": 25,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities",
        "abstract": "Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.",
        "authors": [
            "Hao Zhou",
            "Chengming Hu",
            "Ye Yuan",
            "Yufei Cui",
            "Yili Jin",
            "Can Chen",
            "Haolun Wu",
            "Dun Yuan",
            "Li Jiang",
            "Di Wu",
            "Xue Liu",
            "Charlie Zhang",
            "Xianbin Wang",
            "Jiangchuan Liu"
        ],
        "citations": 25,
        "references": 204,
        "year": 2024
    },
    {
        "title": "Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels",
        "abstract": "Large language models (LLMs) have shown remarkable performance across various natural language processing (NLP) tasks, indicating their significant potential as data annotators. Although LLM-generated annotations are more cost-effective and efficient to obtain, they are often erroneous for complex or domain-specific tasks and may introduce bias when compared to human annotations. Therefore, instead of completely replacing human annotators with LLMs, we need to leverage the strengths of both LLMs and humans to ensure the accuracy and reliability of annotations. This paper presents a multi-step human-LLM collaborative approach where (1) LLMs generate labels and provide explanations, (2) a verifier assesses the quality of LLM-generated labels, and (3) human annotators re-annotate a subset of labels with lower verification scores. To facilitate human-LLM collaboration, we make use of LLM’s ability to rationalize its decisions. LLM-generated explanations can provide additional information to the verifier model as well as help humans better understand LLM labels. We demonstrate that our verifier is able to identify potentially incorrect LLM labels for human re-annotation. Furthermore, we investigate the impact of presenting LLM labels and explanations on human re-annotation through crowdsourced studies.",
        "authors": [
            "Xinru Wang",
            "H. Kim",
            "Sajjadur Rahman",
            "Kushan Mitra",
            "Zhengjie Miao"
        ],
        "citations": 24,
        "references": 106,
        "year": 2024
    },
    {
        "title": "A Computational Framework for Behavioral Assessment of LLM Therapists",
        "abstract": "The emergence of large language models (LLMs) like ChatGPT has increased interest in their use as therapists to address mental health challenges and the widespread lack of access to care. However, experts have emphasized the critical need for systematic evaluation of LLM-based mental health interventions to accurately assess their capabilities and limitations. Here, we propose BOLT, a proof-of-concept computational framework to systematically assess the conversational behavior of LLM therapists. We quantitatively measure LLM behavior across 13 psychotherapeutic approaches with in-context learning methods. Then, we compare the behavior of LLMs against high- and low-quality human therapy. Our analysis based on Motivational Interviewing therapy reveals that LLMs often resemble behaviors more commonly exhibited in low-quality therapy rather than high-quality therapy, such as offering a higher degree of problem-solving advice when clients share emotions. However, unlike low-quality therapy, LLMs reflect significantly more upon clients' needs and strengths. Our findings caution that LLM therapists still require further research for consistent, high-quality care.",
        "authors": [
            "Yu Ying Chiu",
            "Ashish Sharma",
            "Inna Wanyin Lin",
            "Tim Althoff"
        ],
        "citations": 24,
        "references": 72,
        "year": 2024
    },
    {
        "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
        "abstract": "Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",
        "authors": [
            "Long Lian",
            "Boyi Li",
            "Adam Yala",
            "Trevor Darrell"
        ],
        "citations": 114,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model",
        "abstract": "The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmentation, whereas the prior methods take a random mix.",
        "authors": [
            "Deepanway Ghosal",
            "Navonil Majumder",
            "Ambuj Mehrish",
            "Soujanya Poria"
        ],
        "citations": 118,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Splitwise: Efficient Generative LLM Inference Using Phase Splitting",
        "abstract": "Generative large language model (LLM) applications are growing rapidly, leading to large-scale deployments of expensive and power-hungry GPUs. Our characterization of LLM inference shows that each inference request undergoes two phases: a compute-intensive prompt computation phase and a memory intensive token generation phase, each with distinct latency, throughput, memory, and power characteristics. Despite state-of-the-art batching and scheduling, the token generation phase underutilizes compute resources. Unlike prompt computation, token generation does not need the compute capability of the latest GPUs and can be run with lower power and cost. Based on these insights, we propose Splitwise, a model deployment and scheduling technique that splits the two phases of LLM inference requests on to separate machines. Splitwise enables phase-specific resource management using hardware that is well suited for each phase. Request state is transferred efficiently between machines using optimized network libraries on the fast back-plane interconnects available in today’s GPU clusters. Using Splitwise, we design homogeneous and heterogeneous LLM inference clusters optimized for throughput, cost, and power Compared to current designs, Splitwise clusters achieve up to $1.4 \\times$ higher throughput at $\\mathbf{2 0 \\%}$ lower cost. Alternatively, they can deliver $2.35 \\times$ more throughput under the same power and cost budgets.",
        "authors": [
            "Pratyush Patel",
            "Esha Choukse",
            "Chaojie Zhang",
            "Íñigo Goiri",
            "Aashaka Shah",
            "Saeed Maleki",
            "Ricardo Bianchini"
        ],
        "citations": 101,
        "references": 85,
        "year": 2023
    },
    {
        "title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available to the broader scientific community on GitHub.",
        "authors": [
            "Yuchen Zhuang",
            "Yue Yu",
            "Kuan Wang",
            "Haotian Sun",
            "Chao Zhang"
        ],
        "citations": 152,
        "references": 103,
        "year": 2023
    },
    {
        "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
        "abstract": "In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",
        "authors": [
            "Oscar Sainz",
            "Jon Ander Campos",
            "Iker García-Ferrero",
            "Julen Etxaniz",
            "Oier López de Lacalle",
            "Eneko Agirre"
        ],
        "citations": 124,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Can LLM-Generated Misinformation Be Detected?",
        "abstract": "The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",
        "authors": [
            "Canyu Chen",
            "Kai Shu"
        ],
        "citations": 123,
        "references": 361,
        "year": 2023
    },
    {
        "title": "Certifying LLM Safety against Adversarial Prompting",
        "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.",
        "authors": [
            "Aounon Kumar",
            "Chirag Agarwal",
            "Suraj Srinivas",
            "S. Feizi",
            "Himabindu Lakkaraju"
        ],
        "citations": 124,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Can LLM-Generated Misinformation Be Detected?",
        "abstract": "The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",
        "authors": [
            "Canyu Chen",
            "Kai Shu"
        ],
        "citations": 123,
        "references": 361,
        "year": 2023
    },
    {
        "title": "Using an LLM to Help with Code Understanding",
        "abstract": "Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domainspecific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.",
        "authors": [
            "Daye Nam",
            "A. Macvean",
            "Vincent J. Hellendoorn",
            "Bogdan Vasilescu",
            "B. Myers"
        ],
        "citations": 111,
        "references": 79,
        "year": 2023
    },
    {
        "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
        "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model’s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available.",
        "authors": [
            "Ming Li",
            "Yong Zhang",
            "Zhitao Li",
            "Jiuhai Chen",
            "Lichang Chen",
            "Ning Cheng",
            "Jianzong Wang",
            "Tianyi Zhou",
            "Jing Xiao"
        ],
        "citations": 114,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Large language models (LLM) and ChatGPT: what will the impact on nuclear medicine be?",
        "abstract": null,
        "authors": [
            "I. Alberts",
            "L. Mercolli",
            "T. Pyka",
            "G. Prenosil",
            "Kuangyu Shi",
            "A. Rominger",
            "A. Afshar-Oromieh"
        ],
        "citations": 109,
        "references": 31,
        "year": 2023
    },
    {
        "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset",
        "abstract": "Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",
        "authors": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Tianle Li",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zhuohan Li",
            "Zi Lin",
            "Eric P. Xing",
            "Joseph E. Gonzalez",
            "Ion Stoica",
            "Haotong Zhang"
        ],
        "citations": 114,
        "references": 64,
        "year": 2023
    },
    {
        "title": "\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters",
        "abstract": "Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.",
        "authors": [
            "Yixin Wan",
            "George Pu",
            "Jiao Sun",
            "Aparna Garimella",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "citations": 110,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM",
        "abstract": "Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100% to around 10% or less.",
        "authors": [
            "Bochuan Cao",
            "Yu Cao",
            "Lu Lin",
            "Jinghui Chen"
        ],
        "citations": 102,
        "references": 53,
        "year": 2023
    },
    {
        "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification",
        "abstract": "The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree veriﬁcation. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM’s outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is veriﬁed by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree veriﬁer instead of an incremental decoder, which signiﬁcantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.",
        "authors": [
            "Xupeng Miao",
            "G. Oliaro",
            "Zhihao Zhang",
            "Xinhao Cheng",
            "Zeyu Wang",
            "Rae Ying Yee Wong",
            "Zhuoming Chen",
            "Daiyaan Arfeen",
            "Reyna Abhyankar",
            "Zhihao Jia"
        ],
        "citations": 96,
        "references": 44,
        "year": 2023
    },
    {
        "title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
        "abstract": "NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.",
        "authors": [
            "Traian Rebedea",
            "R. Dinu",
            "Makesh Narsimhan Sreedhar",
            "Christopher Parisien",
            "Jonathan Cohen"
        ],
        "citations": 97,
        "references": 34,
        "year": 2023
    },
    {
        "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",
        "abstract": "The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weight after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation under high compression ratios. We evaluate SVD-LLM on a total of 10 datasets and eight models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios.",
        "authors": [
            "Xin Wang",
            "Yu Zheng",
            "Zhongwei Wan",
            "Mi Zhang"
        ],
        "citations": 23,
        "references": 39,
        "year": 2024
    },
    {
        "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
        "abstract": "Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.",
        "authors": [
            "Feilong Chen",
            "Minglun Han",
            "Haozhi Zhao",
            "Qingyang Zhang",
            "Jing Shi",
            "Shuang Xu",
            "Bo Xu"
        ],
        "citations": 95,
        "references": 61,
        "year": 2023
    },
    {
        "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
        "abstract": "Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 11 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.",
        "authors": [
            "Wenbo Hu",
            "Y. Xu",
            "Y. Li",
            "W. Li",
            "Z. Chen",
            "Z. Tu"
        ],
        "citations": 93,
        "references": 62,
        "year": 2023
    },
    {
        "title": "ImageBind-LLM: Multi-modality Instruction Tuning",
        "abstract": "We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.",
        "authors": [
            "Jiaming Han",
            "Renrui Zhang",
            "Wenqi Shao",
            "Peng Gao",
            "Peng Xu",
            "Han Xiao",
            "Kaipeng Zhang",
            "Chris Liu",
            "Song Wen",
            "Ziyu Guo",
            "Xudong Lu",
            "Shuai Ren",
            "Yafei Wen",
            "Xiaoxin Chen",
            "Xiangyu Yue",
            "Hongsheng Li",
            "Y. Qiao"
        ],
        "citations": 95,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",
        "abstract": "We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.",
        "authors": [
            "Ziyu Guo",
            "Renrui Zhang",
            "Xiangyang Zhu",
            "Yiwen Tang",
            "Xianzheng Ma",
            "Jiaming Han",
            "Ke Chen",
            "Peng Gao",
            "Xianzhi Li",
            "Hongsheng Li",
            "P. Heng"
        ],
        "citations": 95,
        "references": 100,
        "year": 2023
    },
    {
        "title": "Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning",
        "abstract": "This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates. The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information. Unique to our approach is the integration of both scene-level and ego-centric 3D information. This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization. Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model's ability to align features of small objects within the scene. Our experiments with Scene-LLM demonstrate its strong capabilities in dense captioning, question answering, and interactive planning. We believe Scene-LLM advances the field of 3D visual understanding and reasoning, offering new possibilities for sophisticated agent interactions in indoor settings.",
        "authors": [
            "Rao Fu",
            "Jingyu Liu",
            "Xilun Chen",
            "Yixin Nie",
            "Wenhan Xiong"
        ],
        "citations": 23,
        "references": 87,
        "year": 2024
    },
    {
        "title": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
        "abstract": "The rise of large language models (LLMs) has enabled LLM-based applications (a.k.a. AI agents or co-pilots), a new software paradigm that combines the strength of LLM and conventional software. Diverse LLM applications from different tenants could design complex workflows using multiple LLM requests to accomplish one task. However, they have to use the over-simplified request-level API provided by today's public LLM services, losing essential application-level information. Public LLM services have to blindly optimize individual LLM requests, leading to sub-optimal end-to-end performance of LLM applications. This paper introduces Parrot, an LLM service system that focuses on the end-to-end experience of LLM-based applications. Parrot proposes Semantic Variable, a unified abstraction to expose application-level knowledge to public LLM services. A Semantic Variable annotates an input/output variable in the prompt of a request, and creates the data pipeline when connecting multiple LLM requests, providing a natural way to program LLM applications. Exposing Semantic Variables to the public LLM service allows it to perform conventional data flow analysis to uncover the correlation across multiple LLM requests. This correlation opens a brand-new optimization space for the end-to-end performance of LLM-based applications. Extensive evaluations demonstrate that Parrot can achieve up to an order-of-magnitude improvement for popular and practical use cases of LLM applications.",
        "authors": [
            "Chaofan Lin",
            "Zhenhua Han",
            "Chengruidong Zhang",
            "Yuqing Yang",
            "Fan Yang",
            "Chen Chen",
            "Lili Qiu"
        ],
        "citations": 18,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
        "abstract": "LLM-as-a-Judge uses a large language model (LLM) to select the best response from a set of candidates for a given question. LLM-as-a-Judge has many applications such as LLM-powered search, reinforcement learning with AI feedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver, an optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver injects a carefully crafted sequence into an attacker-controlled candidate response such that LLM-as-a-Judge selects the candidate response for an attacker-chosen question no matter what other candidate responses are. Specifically, we formulate finding such sequence as an optimization problem and propose a gradient based method to approximately solve it. Our extensive evaluation shows that JudgeDeceive is highly effective, and is much more effective than existing prompt injection attacks that manually craft the injected sequences and jailbreak attacks when extended to our problem. We also show the effectiveness of JudgeDeceiver in three case studies, i.e., LLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses including known-answer detection, perplexity detection, and perplexity windowed detection. Our results show these defenses are insufficient, highlighting the urgent need for developing new defense strategies. Our implementation is available at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver.",
        "authors": [
            "Jiawen Shi",
            "Zenghui Yuan",
            "Yinuo Liu",
            "Yue Huang",
            "Pan Zhou",
            "Lichao Sun",
            "Neil Zhenqiang Gong"
        ],
        "citations": 17,
        "references": 74,
        "year": 2024
    },
    {
        "title": "LLM as a System Service on Mobile Devices",
        "abstract": "Being more powerful and intrusive into user-device interactions, LLMs are eager for on-device execution to better preserve user privacy. In this work, we propose a new paradigm of mobile AI: LLM as a system service on mobile devices (LLMaaS). Unlike traditional DNNs that execute in a stateless manner, such a system service is stateful: LLMs execution often needs to maintain persistent states (mainly KV cache) across multiple invocations. To minimize the LLM context switching overhead under tight device memory budget, this work presents LLMS, which decouples the memory management of app and LLM contexts with a key idea of fine-grained, chunk-wise, globally-optimized KV cache compression and swapping. By fully leveraging KV cache's unique characteristics, it proposes three novel techniques: (1) Tolerance-Aware Compression: it compresses chunks based on their measured accuracy tolerance to compression. (2) IO-Recompute Pipelined Loading: it introduces recompute to swapping-in for acceleration. (3) Chunk Lifecycle Management: it optimizes the memory activities of chunks with an ahead-of-time swapping-out and an LCTRU (Least Compression-Tolerable and Recently-Used) queue based eviction. In evaluations conducted on well-established traces and various edge devices, \\sys reduces context switching latency by up to 2 orders of magnitude when compared to competitive baseline solutions.",
        "authors": [
            "Wangsong Yin",
            "Mengwei Xu",
            "Yuanchun Li",
            "Xuanzhe Liu"
        ],
        "citations": 19,
        "references": 71,
        "year": 2024
    },
    {
        "title": "RouterBench: A Benchmark for Multi-LLM Routing System",
        "abstract": "As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present RouterBench, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and deliver a comparative analysis of various routing approaches through RouterBench, highlighting their potentials and limitations within our evaluation framework. This work not only formalizes and advances the development of LLM routing systems but also sets a standard for their assessment, paving the way for more accessible and economically viable LLM deployments. The code and data are available at https://github.com/withmartian/routerbench.",
        "authors": [
            "Qitian Jason Hu",
            "Jacob Bieker",
            "Xiuyu Li",
            "Nan Jiang",
            "Benjamin Keigwin",
            "Gaurav Ranganath",
            "Kurt Keutzer",
            "Shriyash Kaustubh Upadhyay"
        ],
        "citations": 17,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference",
        "abstract": "With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective LLM deployment in data center environments.",
        "authors": [
            "Jovan Stojkovic",
            "Esha Choukse",
            "Chaojie Zhang",
            "Íñigo Goiri",
            "Josep Torrellas"
        ],
        "citations": 17,
        "references": 40,
        "year": 2024
    },
    {
        "title": "A Complete Survey on LLM-based AI Chatbots",
        "abstract": "The past few decades have witnessed an upsurge in data, forming the foundation for data-hungry, learning-based AI technology. Conversational agents, often referred to as AI chatbots, rely heavily on such data to train large language models (LLMs) and generate new content (knowledge) in response to user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have set new standards in the AI community. This paper presents a complete survey of the evolution and deployment of LLM-based chatbots in various sectors. We first summarize the development of foundational chatbots, followed by the evolution of LLMs, and then provide an overview of LLM-based chatbots currently in use and those in the development phase. Recognizing AI chatbots as tools for generating new knowledge, we explore their diverse applications across various industries. We then discuss the open challenges, considering how the data used to train the LLMs and the misuse of the generated knowledge can cause several issues. Finally, we explore the future outlook to augment their efficiency and reliability in numerous applications. By addressing key milestones and the present-day context of LLM-based chatbots, our survey invites readers to delve deeper into this realm, reflecting on how their next generation will reshape conversational AI.",
        "authors": [
            "Sumit Kumar Dam",
            "Choong-Seon Hong",
            "Yu Qiao",
            "Chaoning Zhang"
        ],
        "citations": 18,
        "references": 207,
        "year": 2024
    },
    {
        "title": "LLM-based NLG Evaluation: Current Status and Challenges",
        "abstract": "Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.",
        "authors": [
            "Mingqi Gao",
            "Xinyu Hu",
            "Jie Ruan",
            "Xiao Pu",
            "Xiaojun Wan"
        ],
        "citations": 21,
        "references": 101,
        "year": 2024
    },
    {
        "title": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration",
        "abstract": "Recent studies show that collaborating multiple large language model (LLM) powered agents is a promising way for task solving. However, current approaches are constrained by using a fixed number of agents and static communication structures. In this work, we propose automatically selecting a team of agents from candidates to collaborate in a dynamic communication structure toward different tasks and domains. Specifically, we build a framework named Dynamic LLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent collaboration, operating a two-stage paradigm: (1) Team Optimization and (2) Task Solving. During the first stage, we utilize an $\\textit{agent selection}$ algorithm, based on an unsupervised metric called $\\textit{Agent Importance Score}$, enabling the selection of best agents according to their contributions in a preliminary trial, oriented to the given task. Then, in the second stage, the selected agents collaborate dynamically according to the query. Empirically, we demonstrate that DyLAN outperforms strong baselines in code generation, decision-making, general reasoning, and arithmetic reasoning tasks with moderate computational cost. On specific subjects in MMLU, selecting a team of agents in the team optimization stage improves accuracy by up to 25.0% in DyLAN.",
        "authors": [
            "Zijun Liu",
            "Yanzhe Zhang",
            "Peng Li",
            "Yang Liu",
            "Diyi Yang"
        ],
        "citations": 90,
        "references": 0,
        "year": 2023
    },
    {
        "title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
        "abstract": "Abstract Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.",
        "authors": [
            "Moran Mizrahi",
            "Guy Kaplan",
            "Daniel Malkin",
            "Rotem Dror",
            "Dafna Shahaf",
            "Gabriel Stanovsky"
        ],
        "citations": 84,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Accelerating LLM Inference with Staged Speculative Decoding",
        "abstract": "Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of speculative decoding. Taken together, we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L model while perfectly preserving output quality.",
        "authors": [
            "Benjamin Spector",
            "Christal Re"
        ],
        "citations": 82,
        "references": 22,
        "year": 2023
    },
    {
        "title": "Atom: Low-bit Quantization for Efficient and Accurate LLM Serving",
        "abstract": "The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use GPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity. However, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance. To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization. It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process. We evaluate Atom on 4-bit weight-activation quantization setups in the serving context. Atom improves end-to-end throughput by up to $7.73\\times$ compared to the FP16 and by $2.53\\times$ compared to INT8 quantization, while maintaining the same latency target.",
        "authors": [
            "Yilong Zhao",
            "Chien-Yu Lin",
            "Kan Zhu",
            "Zihao Ye",
            "Lequn Chen",
            "Size Zheng",
            "Luis Ceze",
            "Arvind Krishnamurthy",
            "Tianqi Chen",
            "Baris Kasikci"
        ],
        "citations": 81,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
        "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\\footnote{\\url{https://github.com/zjunlp/MachineSoM}.}, hoping to catalyze further research in this promising avenue.",
        "authors": [
            "Jintian Zhang",
            "Xin Xu",
            "Ruibo Liu",
            "Shumin Deng"
        ],
        "citations": 80,
        "references": 69,
        "year": 2023
    },
    {
        "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
        "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",
        "authors": [
            "Yen-Ting Lin",
            "Yun-Nung (Vivian) Chen"
        ],
        "citations": 73,
        "references": 41,
        "year": 2023
    },
    {
        "title": "An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation",
        "abstract": "Despite making significant progress in multi-modal tasks, current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences. Therefore, evaluating MLLMs' hallucinations is becoming increasingly important in model improvement and practical application deployment. Previous works are limited in high evaluation costs (e.g., relying on humans or advanced LLMs) and insufficient evaluation dimensions (e.g., types of tasks and hallucinations). In this paper, we propose an LLM-free multi-dimensional benchmark AMBER, which can be used to evaluate both generative task and discriminative task including existence, attribute and relation hallucination. Based on AMBER, we design a low-cost and efficient evaluation pipeline. Additionally, we conduct a comprehensive evaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision), and also give guideline suggestions for mitigating hallucinations. The data and code of AMBER are available at https://github.com/junyangwang0410/AMBER.",
        "authors": [
            "Junyang Wang",
            "Yuhang Wang",
            "Guohai Xu",
            "Jing Zhang",
            "Yukai Gu",
            "Haitao Jia",
            "Haiyang Xu",
            "Ming Yan",
            "Ji Zhang",
            "Jitao Sang"
        ],
        "citations": 74,
        "references": 22,
        "year": 2023
    },
    {
        "title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
        "abstract": "Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.",
        "authors": [
            "Yuan Li",
            "Yixuan Zhang",
            "Lichao Sun"
        ],
        "citations": 74,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Can ChatGPT Replace Traditional KBQA Models? An In-Depth Analysis of the Question Answering Performance of the GPT LLM Family",
        "abstract": "ChatGPT is a powerful large language model (LLM) that covers knowledge resources such as Wikipedia and supports natural language question answering using its own knowledge. Therefore, there is growing interest in exploring whether ChatGPT can replace traditional knowledge-based question answering (KBQA) models. Although there have been some works analyzing the question answering performance of ChatGPT, there is still a lack of large-scale, comprehensive testing of various types of complex questions to analyze the limitations of the model. In this paper, we present a framework that follows the black-box testing specifications of CheckList proposed by Ribeiro et. al. We evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex question answering datasets, which include six English datasets and two multilingual datasets. The total number of test cases is approximately 190,000. In addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5 to identify commonalities between the GPT family and other LLMs. The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git",
        "authors": [
            "Yiming Tan",
            "Dehai Min",
            "Y. Li",
            "Wenbo Li",
            "Nan Hu",
            "Yongrui Chen",
            "G. Qi"
        ],
        "citations": 78,
        "references": 55,
        "year": 2023
    },
    {
        "title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents",
        "abstract": "The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \\url{https://github.com/salesforce/BOLAA}.",
        "authors": [
            "Zhiwei Liu",
            "Weiran Yao",
            "Jianguo Zhang",
            "Le Xue",
            "Shelby Heinecke",
            "Rithesh Murthy",
            "Yihao Feng",
            "Zeyuan Chen",
            "Juan Carlos Niebles",
            "Devansh Arpit",
            "Ran Xu",
            "P. Mùi",
            "Haiquan Wang",
            "Caiming Xiong",
            "S. Savarese"
        ],
        "citations": 73,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Prompt Distillation for Efficient LLM-based Recommendation",
        "abstract": "Large language models (LLM) have manifested unparalleled modeling capability on various tasks, e.g., multi-step reasoning, but the input to these models is mostly limited to plain text, which could be very long and contain noisy information. Long text could take long time to process, and thus may not be efficient enough for recommender systems that require immediate response. In LLM-based recommendation models, user and item IDs are usually filled in a template (i.e., discrete prompt) to allow the models to understand a given task, but the models usually need extensive fine-tuning to bridge the user/item IDs and the template words and to unleash the power of LLM for recommendation. To address the problems, we propose to distill the discrete prompt for a specific task to a set of continuous prompt vectors so as to bridge IDs and words and to reduce the inference time. We also design a training strategy with an attempt to improve the efficiency of training these models. Experimental results on three real-world datasets demonstrate the effectiveness of our PrOmpt Distillation (POD) approach on both sequential recommendation and top-N recommendation tasks. Although the training efficiency can be significantly improved, the improvement of inference efficiency is limited. This finding may inspire researchers in the community to further improve the inference efficiency of LLM-based recommendation models.",
        "authors": [
            "Lei Li",
            "Yongfeng Zhang",
            "Li Chen"
        ],
        "citations": 75,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
        "abstract": "Large Language Models (LLMs) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-LLMs to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment LLMs, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores. Since adversaries may not know or have access to the judge-LLMs, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-LLMs. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-LLMs are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment. Our findings raise concerns on the reliability of LLM-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.",
        "authors": [
            "Vyas Raina",
            "Adian Liusie",
            "Mark J. F. Gales"
        ],
        "citations": 26,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation",
        "abstract": "We present language-guided exploration (LGX), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of large language models (LLMs) for this task by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.",
        "authors": [
            "Vishnu Sashank Dorbala",
            "James F. Mullen",
            "Dinesh Manocha"
        ],
        "citations": 69,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
        "abstract": "Measuring the quality of responses generated by LLMs is a challenging task, particularly when it comes to evaluating whether the response is aligned with human preference. A novel approach involves using the LLM itself to make evaluation and stabilizing the results through multiple independent evaluations, similar to a single-layer narrow LLM network. This network consists of a fixed number of neurons, with each neuron being the same LLM. In this paper, we draw upon the extensive research on deep neural networks to explore whether deeper and wider networks can lead to fairer evaluations. Specifically, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts, we first adaptively generate as many neuron roles as possible for each evaluation sample. Each perspective corresponds to the role of a specific LLM neuron in the first layer. In subsequent layers, we follow the idea that higher layers in deep networks are responsible for more comprehensive features, each layer receives representations from all neurons in the previous layer, integrating the locally learned evaluation information to obtain a more comprehensive evaluation result. Interestingly, this network design resembles the process of academic paper reviewing. To validate the effectiveness of our method, we construct the largest and most diverse English evaluation benchmark LLMEval$^2$ for LLM evaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers (one round of discussion) performs the best, improving kappa correlation coefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the assessment of Chinese LLMs, which has accelerated the evaluation time by 4.6 times, resulting in a 60% cost saving. WideDeep achieves a remarkable 93% agreement level among humans.",
        "authors": [
            "Xinghua Zhang",
            "Yu Bowen",
            "Haiyang Yu",
            "Yangyu Lv",
            "Tingwen Liu",
            "Fei Huang",
            "Hongbo Xu",
            "Yongbin Li"
        ],
        "citations": 69,
        "references": 44,
        "year": 2023
    },
    {
        "title": "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
        "abstract": "This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today's SOTA TS models and offer benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM's ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.",
        "authors": [
            "Chenxi Sun",
            "Yaliang Li",
            "Hongyan Li",
            "linda Qiao"
        ],
        "citations": 69,
        "references": 74,
        "year": 2023
    },
    {
        "title": "The Science of Detecting LLM-Generated Text",
        "abstract": "While many detection methods have been proposed, understanding the challenges is far more daunting.",
        "authors": [
            "Ruixiang Tang",
            "Yu-Neng Chuang",
            "Xia Hu"
        ],
        "citations": 142,
        "references": 87,
        "year": 2023
    },
    {
        "title": "ExpeL: LLM Agents Are Experiential Learners",
        "abstract": "The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.",
        "authors": [
            "Andrew Zhao",
            "Daniel Huang",
            "Quentin Xu",
            "M. Lin",
            "Y. Liu",
            "Gao Huang"
        ],
        "citations": 130,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Towards Mitigating LLM Hallucination via Self Reflection",
        "abstract": null,
        "authors": [
            "Ziwei Ji",
            "Tiezheng Yu",
            "Yan Xu",
            "Nayeon Lee",
            "Etsuko Ishii",
            "Pascale Fung"
        ],
        "citations": 128,
        "references": 0,
        "year": 2023
    },
    {
        "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
        "abstract": "This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. 1",
        "authors": [
            "Chan Hee Song",
            "Jiaman Wu",
            "Clay Washington",
            "Brian M. Sadler",
            "Wei-Lun Chao",
            "Yu Su"
        ],
        "citations": 296,
        "references": 52,
        "year": 2022
    },
    {
        "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",
        "abstract": "Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still cannot completely trust their answers, since LLMs suffer from \\textbf{hallucination}\\textemdash fabricating non-existent facts, deceiving users with or without their awareness. However, the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that nonsensical prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. Moreover, we provide both theoretical and experimental evidence that transformers can be manipulated to produce specific pre-define tokens by perturbing its input sequence. This phenomenon forces us to revisit that \\emph{hallucination may be another view of adversarial examples}, and it shares similar characteristics with conventional adversarial examples as a basic property of LLMs. Therefore, we formalize an automatic hallucination triggering method as the \\textit{hallucination attack} in an adversarial way. Finally, we explore the basic properties of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub\\footnote{https://github.com/PKU-YuanGroup/Hallucination-Attack}.",
        "authors": [
            "Jia-Yu Yao",
            "Kun-Peng Ning",
            "Zhen-Hui Liu",
            "Munan Ning",
            "Li Yuan"
        ],
        "citations": 110,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
        "abstract": "Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \\ie \\emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance. To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers. We hope this work can draw attention to appropriate training and evaluation of LLMs.",
        "authors": [
            "Kun Zhou",
            "Yutao Zhu",
            "Zhipeng Chen",
            "Wentong Chen",
            "Wayne Xin Zhao",
            "Xu Chen",
            "Yankai Lin",
            "Ji-Rong Wen",
            "Jiawei Han"
        ],
        "citations": 110,
        "references": 44,
        "year": 2023
    },
    {
        "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent",
        "abstract": "3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics.",
        "authors": [
            "Jianing Yang",
            "Xuweiyi Chen",
            "Shengyi Qian",
            "Nikhil Madaan",
            "Madhavan Iyengar",
            "D. Fouhey",
            "Joyce Chai"
        ],
        "citations": 63,
        "references": 62,
        "year": 2023
    },
    {
        "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming",
        "abstract": "Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them.In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM.Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning.On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.",
        "authors": [
            "Suyu Ge",
            "Chunting Zhou",
            "Rui Hou",
            "Madian Khabsa",
            "Yi-Chia Wang",
            "Qifan Wang",
            "Jiawei Han",
            "Yuning Mao"
        ],
        "citations": 62,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
        "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",
        "authors": [
            "Robert Kirk",
            "Ishita Mediratta",
            "Christoforos Nalmpantis",
            "Jelena Luketina",
            "Eric Hambro",
            "Edward Grefenstette",
            "Roberta Raileanu"
        ],
        "citations": 84,
        "references": 65,
        "year": 2023
    },
    {
        "title": "The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches",
        "abstract": "Chatbots have been an interesting application of natural language generation since its inception. With novel transformer based Generative AI methods, building chatbots have become trivial. Chatbots which are targeted at specific domains for example medicine and psychology are implemented rapidly. This however, should not distract from the need to evaluate the chatbot responses. Especially because the natural language generation community does not entirely agree upon how to effectively evaluate such applications. With this work we discuss the issue further with the increasingly popular LLM based evaluations and how they correlate with human evaluations. Additionally, we introduce a comprehensive factored evaluation mechanism that can be utilized in conjunction with both human and LLM-based evaluations. We present the results of an experimental evaluation conducted using this scheme in one of our chatbot implementations which consumed educational reports, and subsequently compare automated, traditional human evaluation, factored human evaluation, and factored LLM evaluation. Results show that factor based evaluation produces better insights on which aspects need to be improved in LLM applications and further strengthens the argument to use human evaluation in critical spaces where main functionality is not direct retrieval.",
        "authors": [
            "Bhashithe Abeysinghe",
            "Ruhan Circi"
        ],
        "citations": 14,
        "references": 39,
        "year": 2024
    },
    {
        "title": "VTimeLLM: Empower LLM to Grasp Video Moments",
        "abstract": "Large language models (LLMs) have shown remarkable text understanding capabilities, which have been ex-tended as Video LLMs to handle video data for compre-hending visual details. However, existing Video LLMs can only provide a coarse description of the entire video, failing to capture the precise start and end time bound-ary of specific events. In this paper, we solve this issue via proposing VTimeLLM, a novel Video LLM designed for fine-grained video moment understanding and reasoning with respect to time boundary. Specifically, our VTimeLLM adopts a boundary-aware three-stage training strategy, which respectively utilizes image-text pairs for feature alignment, multiple-event videos to increase temporal-boundary awareness, and high-quality video-instruction tuning to further improve temporal understanding ability as well as align with human intents. Extensive experiments demonstrate that in fine-grained time-related comprehension tasks for videos such as Temporal Video Grounding and Dense Video Captioning, VTimeLLM significantly outperforms existing Video LLMs. Besides, benefits from the fine-grained temporal understanding of the videos further enable VTimeLLM to beat existing Video LLMs in video di-alogue benchmark, showing its superior cross-modal understanding and reasoning abilities. 11Our project page is at https://github.com/huangb23/VTimeLLM",
        "authors": [
            "Bin Huang",
            "Xin Wang",
            "Hong Chen",
            "Zihan Song",
            "Wenwu Zhu"
        ],
        "citations": 60,
        "references": 37,
        "year": 2023
    },
    {
        "title": "OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples",
        "abstract": "Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. In this paper, we propose OUTFOX, a framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. In this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points F1-score. Furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points F1-score, beating existing detectors on non-attacked texts. Finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points F1-score, massively outperforming the baseline paraphrasing method for evading detection.",
        "authors": [
            "Ryuto Koike",
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "citations": 56,
        "references": 35,
        "year": 2023
    },
    {
        "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification",
        "abstract": "Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.",
        "authors": [
            "Kushal Tirumala",
            "Daniel Simig",
            "Armen Aghajanyan",
            "Ari S. Morcos"
        ],
        "citations": 68,
        "references": 66,
        "year": 2023
    },
    {
        "title": "PentestGPT: An LLM-empowered Automatic Penetration Testing Tool",
        "abstract": "Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this research, we evaluate the performance of LLMs on real-world penetration testing tasks using a robust benchmark created from test machines with platforms. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining an integrated understanding of the overall testing scenario. In response to these insights, we introduce PentestGPT, an LLM-empowered automatic penetration testing tool that leverages the abundant domain knowledge inherent in LLMs. PentestGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that PentestGPT not only outperforms LLMs with a task-completion increase of 228.6\\% compared to the \\gptthree model among the benchmark targets but also proves effective in tackling real-world penetration testing challenges. Having been open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.",
        "authors": [
            "Gelei Deng",
            "Yi Liu",
            "V'ictor Mayoral-Vilches",
            "Peng Liu",
            "Yuekang Li",
            "Yuan Xu",
            "Tianwei Zhang",
            "Yang Liu",
            "M. Pinzger",
            "S. Rass"
        ],
        "citations": 52,
        "references": 59,
        "year": 2023
    },
    {
        "title": "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning",
        "abstract": "LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution, and performance evaluation on federated LLM fine-tuning; (2) we provide comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extension in FL scenarios with low communication and computation costs, even without accessing the full model; (3) we adopt several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study. We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, which also yields valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.",
        "authors": [
            "Weirui Kuang",
            "Bingchen Qian",
            "Zitao Li",
            "Daoyuan Chen",
            "Dawei Gao",
            "Xuchen Pan",
            "Yuexiang Xie",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "citations": 64,
        "references": 93,
        "year": 2023
    },
    {
        "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory",
        "abstract": "Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First,\"windowing\"strategically reduces data transfer by reusing previously activated neurons, and second,\"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.",
        "authors": [
            "Keivan Alizadeh-Vahid",
            "Iman Mirzadeh",
            "Dmitry Belenko",
            "Karen Khatamifard",
            "Minsik Cho",
            "C. C. D. Mundo",
            "Mohammad Rastegari",
            "Mehrdad Farajtabar"
        ],
        "citations": 74,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework",
        "abstract": "Conversational Health Agents (CHAs) are interactive systems that provide healthcare services, such as assistance and diagnosis. Current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation aspects. However, they offer limited agent capabilities, specifically lacking multi-step problem-solving, personalized conversations, and multimodal data analysis. Our aim is to overcome these limitations. We propose openCHA, an open-source LLM-powered framework, to empower conversational agents to generate a personalized response for users' healthcare queries. This framework enables developers to integrate external sources including data sources, knowledge bases, and analysis models, into their LLM-based solutions. openCHA includes an orchestrator to plan and execute actions for gathering information from external sources, essential for formulating responses to user inquiries. It facilitates knowledge acquisition, problem-solving capabilities, multilingual and multimodal conversations, and fosters interaction with various AI platforms. We illustrate the framework's proficiency in handling complex healthcare tasks via two demonstrations and four use cases. Moreover, we release openCHA as open source available to the community via GitHub.",
        "authors": [
            "Mahyar Abbasian",
            "Iman Azimi",
            "Amir M. Rahmani",
            "Ramesh C. Jain"
        ],
        "citations": 50,
        "references": 97,
        "year": 2023
    },
    {
        "title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models",
        "abstract": "In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.",
        "authors": [
            "S. S. Kannan",
            "Vishnunandan L. N. Venkatesh",
            "Byung-Cheol Min"
        ],
        "citations": 50,
        "references": 44,
        "year": 2023
    },
    {
        "title": "LLM Based Generation of Item-Description for Recommendation System",
        "abstract": "The description of an item plays a pivotal role in providing concise and informative summaries to captivate potential viewers and is essential for recommendation systems. Traditionally, such descriptions were obtained through manual web scraping techniques, which are time-consuming and susceptible to data inconsistencies. In recent years, Large Language Models (LLMs), such as GPT-3.5, and open source LLMs like Alpaca have emerged as powerful tools for natural language processing tasks. In this paper, we have explored how we can use LLMs to generate detailed descriptions of the items. To conduct the study, we have used the MovieLens 1M dataset comprising movie titles and the Goodreads Dataset consisting of names of books and subsequently, an open-sourced LLM, Alpaca, was prompted with few-shot prompting on this dataset to generate detailed movie descriptions considering multiple features like the names of the cast and directors for the ML dataset and the names of the author and publisher for the Goodreads dataset. The generated description was then compared with the scraped descriptions using a combination of Top Hits, MRR, and NDCG as evaluation metrics. The results demonstrated that LLM-based movie description generation exhibits significant promise, with results comparable to the ones obtained by web-scraped descriptions.",
        "authors": [
            "Arkadeep Acharya",
            "Brijraj Singh",
            "N. Onoe"
        ],
        "citations": 59,
        "references": 18,
        "year": 2023
    },
    {
        "title": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models",
        "abstract": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research of FedLLM and enable a broad range of industrial applications.",
        "authors": [
            "Tao Fan",
            "Yan Kang",
            "Guoqiang Ma",
            "Weijing Chen",
            "Wenbin Wei",
            "Lixin Fan",
            "Qiang Yang"
        ],
        "citations": 49,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents",
        "abstract": "Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel\"Formal-LLM\"framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows agent developers to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The source code of this work is available at https://github.com/agiresearch/Formal-LLM.",
        "authors": [
            "Zelong Li",
            "Wenyue Hua",
            "Hao Wang",
            "He Zhu",
            "Yongfeng Zhang"
        ],
        "citations": 12,
        "references": 53,
        "year": 2024
    },
    {
        "title": "VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View",
        "abstract": "Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation (VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve around 25% relative improvement in task completion over the previous state-of-the-art for two datasets.",
        "authors": [
            "Raphael Schumann",
            "Wanrong Zhu",
            "Weixi Feng",
            "Tsu-Jui Fu",
            "S. Riezler",
            "William Yang Wang"
        ],
        "citations": 46,
        "references": 42,
        "year": 2023
    },
    {
        "title": "A Simple LLM Framework for Long-Range Video Question-Answering",
        "abstract": "We present LLoVi, a simple yet effective **L**anguage-based **Lo**ng-range **Vi**deo question-answering (LVQA) framework. Our method decomposes the short- and long-range modeling aspects of LVQA into two stages. First, we use a short-term visual captioner to generate textual descriptions of short video clips (0.5-8 seconds in length) densely sampled from a long input video. Afterward, an LLM aggregates the densely extracted short-term captions to answer a given question. Furthermore, we propose a novel multi-round summarization prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question. To analyze what makes our simple framework so effective, we thoroughly evaluate various components of our framework. Our empirical analysis reveals that the choice of the visual captioner and LLM is critical for good LVQA performance. The proposed multi-round summarization prompt also leads to a significant LVQA performance boost. Our method achieves the best-reported results on the EgoSchema dataset, best known for very long-form video question-answering. LLoVi also outperforms the previous state-of-the-art by **10.2%** and **6.2%** on NExT-QA and IntentQA for LVQA. Finally, we extend LLoVi to grounded VideoQA, which requires both QA and temporal localization, and show that it outperforms all prior methods on NExT-GQA. Code is available at https://github.com/CeeZh/LLoVi.",
        "authors": [
            "Ce Zhang",
            "Taixi Lu",
            "Md Mohaiminul Islam",
            "Ziyang Wang",
            "Shoubin Yu",
            "Mohit Bansal",
            "Gedas Bertasius"
        ],
        "citations": 47,
        "references": 90,
        "year": 2023
    },
    {
        "title": "From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?",
        "abstract": "Large Language Models (LLMs) have found widespread applications in various domains, including web applications, where they facilitate human interaction via chatbots with natural language interfaces. Internally, aided by an LLM-integration middleware such as Langchain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users. However, unsanitized user prompts can lead to SQL injection attacks, potentially compromising the security of the database. Despite the growing interest in prompt injection vulnerabilities targeting LLMs, the specific risks of generating SQL injection attacks through prompt injections have not been extensively studied. In this paper, we present a comprehensive examination of prompt-to-SQL (P$_2$SQL) injections targeting web applications based on the Langchain framework. Using Langchain as our case study, we characterize P$_2$SQL injections, exploring their variants and impact on application security through multiple concrete examples. Furthermore, we evaluate 7 state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks across language models. Our findings indicate that LLM-integrated applications based on Langchain are highly susceptible to P$_2$SQL injection attacks, warranting the adoption of robust defenses. To counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the Langchain framework. We validate the defenses through an experimental evaluation with a real-world use case application.",
        "authors": [
            "Rodrigo Pedro",
            "Daniel Castro",
            "Paulo Carreira",
            "Nuno Santos"
        ],
        "citations": 45,
        "references": 46,
        "year": 2023
    },
    {
        "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
        "abstract": "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",
        "authors": [
            "Myra Cheng",
            "Tiziano Piccardi",
            "Diyi Yang"
        ],
        "citations": 47,
        "references": 88,
        "year": 2023
    },
    {
        "title": "LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding",
        "abstract": "Recently, Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have shown promise in instruction following and 2D image understanding. While these models are powerful, they have not yet been developed to comprehend the more challenging 3D physical scenes, especially when it comes to the sparse outdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes raw LiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs to gain a comprehensive understanding of outdoor 3D scenes. The central insight of our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a language modeling problem, encompassing tasks such as 3D captioning, 3D grounding, 3D question answering, etc. Specifically, due to the scarcity of 3D LiDAR-text pairing data, we introduce a three-stage training strategy and generate relevant datasets, progressively aligning the 3D modality with the language embedding space of LLM. Furthermore, we design a View-Aware Transformer (VAT) to connect the 3D encoder with the LLM, which effectively bridges the modality gap and enhances the LLM's spatial orientation comprehension of visual features. Our experiments show that LiDAR-LLM possesses favorable capabilities to comprehend various instructions regarding 3D scenes and engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the 3D captioning task and achieves a 63.1\\% classification accuracy and a 14.3\\% BEV mIoU on the 3D grounding task. Web page: https://sites.google.com/view/lidar-llm",
        "authors": [
            "Senqiao Yang",
            "Jiaming Liu",
            "Ray Zhang",
            "Mingjie Pan",
            "Zoey Guo",
            "Xiaoqi Li",
            "Zehui Chen",
            "Peng Gao",
            "Yandong Guo",
            "Shanghang Zhang"
        ],
        "citations": 44,
        "references": 53,
        "year": 2023
    },
    {
        "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
        "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness. This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.",
        "authors": [
            "Xilie Xu",
            "Keyi Kong",
            "Ninghao Liu",
            "Li-zhen Cui",
            "Di Wang",
            "Jingfeng Zhang",
            "Mohan S. Kankanhalli"
        ],
        "citations": 45,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Empowering LLM to use Smartphone for Intelligent Task Automation",
        "abstract": "Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smart-phones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to re-think the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce Auto-Droid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that Auto-Droid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo, benchmark suites, and source code of AutoDroid will be released at https://autodroid-sys.github.io/.",
        "authors": [
            "Hao Wen",
            "Yuanchun Li",
            "Guohong Liu",
            "Shanhui Zhao",
            "Tao Yu",
            "Toby Jia-Jun Li",
            "Shiqi Jiang",
            "Yunhao Liu",
            "Yaqin Zhang",
            "Yunxin Liu"
        ],
        "citations": 46,
        "references": 69,
        "year": 2023
    },
    {
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
        "abstract": "We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",
        "authors": [
            "Shih-Yang Liu",
            "Zechun Liu",
            "Xijie Huang",
            "Pingcheng Dong",
            "Kwang-Ting Cheng"
        ],
        "citations": 42,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Detecting LLM-Generated Text in Computing Education: Comparative Study for ChatGPT Cases",
        "abstract": "Due to the recent improvements and wide availability of Large Language Models (LLMs), they have posed a serious threat to academic integrity in education. Modern LLM-generated text detectors attempt to combat the problem by offering educators with services to assess whether some text is LLM-generated. In this work, we have collected 124 submissions from computer science students before the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this data to evaluate eight publicly-available LLM-generated text detectors through the measures of accuracy, false positives, and resilience. Our results find that Copy Leaks is the most accurate LLM-generated text detector, G PTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resilient LLM-generated text detector. We note that all LLM-generated text detectors are less accurate with code, other languages (aside from English), and after the use of paraphrasing tools.",
        "authors": [
            "Michael Sheinman Orenstrakh",
            "Oscar Karnalim",
            "C. Suárez",
            "Michael Liut"
        ],
        "citations": 42,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels",
        "abstract": "Zero-shot text rankers powered by recent LLMs achieve remarkable ranking performance by simply prompting. Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like “Yes” and “No”. However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking. We study two variants of the prompt template, coupled with different numbers of relevance levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance labels significantly improves the performance of LLM rankers.",
        "authors": [
            "Honglei Zhuang",
            "Zhen Qin",
            "Kai Hui",
            "Junru Wu",
            "Le Yan",
            "Xuanhui Wang",
            "Michael Bendersky"
        ],
        "citations": 53,
        "references": 43,
        "year": 2023
    },
    {
        "title": "ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing",
        "abstract": "Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.",
        "authors": [
            "Ian Arawjo",
            "Chelse Swoopes",
            "Priyan Vaithilingam",
            "Martin Wattenberg",
            "Elena L. Glassman"
        ],
        "citations": 54,
        "references": 53,
        "year": 2023
    },
    {
        "title": "AutoDroid: LLM-powered Task Automation in Android",
        "abstract": "Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or endusers. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system capable of handling arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%.",
        "authors": [
            "Hao Wen",
            "Yuanchun Li",
            "Guohong Liu",
            "Shanhui Zhao",
            "Tao Yu",
            "Toby Jia-Jun Li",
            "Shiqi Jiang",
            "Yunhao Liu",
            "Yaqin Zhang",
            "Yunxin Liu"
        ],
        "citations": 39,
        "references": 45,
        "year": 2023
    },
    {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution",
        "abstract": "The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources. Building generative information-seeking models demands openly accessible datasets, which currently remain lacking. In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations. Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset. HAGRID is constructed based on human and LLM collaboration. We first automatically collect attributed explanations that follow an in-context citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability. HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.",
        "authors": [
            "Ehsan Kamalloo",
            "A. Jafari",
            "Xinyu Crystina Zhang",
            "Nandan Thakur",
            "Jimmy J. Lin"
        ],
        "citations": 38,
        "references": 57,
        "year": 2023
    },
    {
        "title": "ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning",
        "abstract": "Motivated by the substantial achievements of Large Language Models (LLMs) in the field of natural language processing, recent research has commenced investigations into the application of LLMs for complex, long-horizon sequential task planning challenges in robotics. LLMs are advantageous in offering the potential to enhance the generalizability as task-agnostic planners and facilitate flexible interaction between human instructors and planning systems. However, task plans generated by LLMs often lack feasibility and correctness. To address this challenge, we introduce ISR-LLM, a novel framework that improves LLM-based planning through an iterative self-refinement process. The framework operates through three sequential steps: preprocessing, planning, and iterative self-refinement. During preprocessing, an LLM translator is employed to convert natural language input into a Planning Domain Definition Language (PDDL) formulation. In the planning phase, an LLM planner formulates an initial plan, which is then assessed and refined in the iterative self-refinement step by a validator. We examine the performance of ISR-LLM across three distinct planning domains. Our experimental results show that ISR-LLM is able to achieve markedly higher success rates in sequential task planning compared to state-of-the-art LLM-based planners. Moreover, it also preserves the broad applicability and generalizability of working with natural language instructions.",
        "authors": [
            "Zhehua Zhou",
            "Jiayang Song",
            "Kunpeng Yao",
            "Zhan Shu",
            "Lei Ma"
        ],
        "citations": 39,
        "references": 55,
        "year": 2023
    },
    {
        "title": "“It's a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",
        "abstract": "The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users’ perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users’ erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users’ ability to navigate the trade-offs. We discuss practical design guidelines and the needs for paradigm shifts to protect the privacy of LLM-based CA users.",
        "authors": [
            "Zhiping Zhang",
            "Michelle Jia",
            "Hao-Ping Lee",
            "Bingsheng Yao",
            "Sauvik Das",
            "Ada Lerner",
            "Dakuo Wang",
            "Tianshi Li"
        ],
        "citations": 36,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Simulating Opinion Dynamics with Networks of LLM-based Agents",
        "abstract": "Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.",
        "authors": [
            "Yun-Shiuan Chuang",
            "Agam Goyal",
            "Nikunj Harlalka",
            "Siddharth Suresh",
            "Robert Hawkins",
            "Sijia Yang",
            "Dhavan Shah",
            "Junjie Hu",
            "Timothy T. Rogers"
        ],
        "citations": 36,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Can LLM be a Personalized Judge?",
        "abstract": "Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks. Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization.",
        "authors": [
            "Yijiang River Dong",
            "Tiancheng Hu",
            "Nigel Collier"
        ],
        "citations": 11,
        "references": 48,
        "year": 2024
    },
    {
        "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis",
        "abstract": "Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield significant improvements, except for dropout, which demonstrates remarkable effectiveness but requires careful tuning when scaling up the model size. Additionally, we discover that leveraging mixture-of-experts (MoE) enables cost-effective and efficient hyper-parameter tuning for computationally intensive dense LLMs with comparable trainable parameters, potentially impacting efficient LLM development on a broader scale.",
        "authors": [
            "Fuzhao Xue",
            "Yao Fu",
            "Wangchunshu Zhou",
            "Zangwei Zheng",
            "Yang You"
        ],
        "citations": 61,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Learning From Mistakes Makes LLM Better Reasoner",
        "abstract": "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a ''corrector'' to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA.",
        "authors": [
            "Shengnan An",
            "Zexiong Ma",
            "Zeqi Lin",
            "Nanning Zheng",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "citations": 58,
        "references": 59,
        "year": 2023
    },
    {
        "title": "VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning",
        "abstract": "Recent text-to-video (T2V) generation methods have seen significant advancements. However, the majority of these works focus on producing short video clips of a single event (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules. This prompts an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which includes the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency groupings of the entities. Next, guided by this video plan, our video generator, named Layout2Vid, has explicit control over spatial layouts and can maintain temporal consistency of entities across multiple scenes, while being trained only with image-level annotations. Our experiments demonstrate that our proposed VideoDirectorGPT framework substantially improves layout and movement control in both single- and multi-scene video generation and can generate multi-scene videos with consistency, while achieving competitive performance with SOTAs in open-domain single-scene T2V generation. Detailed ablation studies, including dynamic adjustment of layout control strength with an LLM and video generation with user-provided images, confirm the effectiveness of each component of our framework and its future potential.",
        "authors": [
            "Han Lin",
            "Abhaysinh Zala",
            "Jaemin Cho",
            "Mohit Bansal"
        ],
        "citations": 50,
        "references": 66,
        "year": 2023
    },
    {
        "title": "FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design",
        "abstract": "Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains. Their prowess in integrating extensive web knowledge has fueled interest in developing LLM-based autonomous agents. While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. Addressing this, we introduce FinMem, a novel LLM-based agent framework devised for financial decision-making. It encompasses three core modules: Profiling, to customize the agent's characteristics; Memory, with layered message processing, to aid the agent in assimilating hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions. Notably, FinMem's memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. Its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. This framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. We first compare FinMem with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks. We then fine-tuned the agent's perceptual span and character setting to achieve a significantly enhanced trading performance. Collectively, FinMem presents a cutting-edge LLM agent framework for automated trading, boosting cumulative investment returns.",
        "authors": [
            "Yangyang Yu",
            "Haohang Li",
            "Zhi Chen",
            "Yuechen Jiang",
            "Yang Li",
            "Denghui Zhang",
            "Rong Liu",
            "Jordan W. Suchow",
            "K. Khashanah"
        ],
        "citations": 33,
        "references": 56,
        "year": 2023
    },
    {
        "title": "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization",
        "abstract": "Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.",
        "authors": [
            "Sungmin Kang",
            "Gabin An",
            "S. Yoo"
        ],
        "citations": 31,
        "references": 50,
        "year": 2023
    },
    {
        "title": "LLM-grounded Video Diffusion Models",
        "abstract": "Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion. To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free and can be integrated into any video diffusion model that admits classifier guidance. Our results demonstrate that LVD significantly outperforms its base video diffusion model and several strong baseline methods in faithfully generating videos with the desired attributes and motion patterns.",
        "authors": [
            "Long Lian",
            "Baifeng Shi",
            "Adam Yala",
            "Trevor Darrell",
            "Boyi Li"
        ],
        "citations": 33,
        "references": 57,
        "year": 2023
    },
    {
        "title": "PB-LLM: Partially Binarized Large Language Models",
        "abstract": "This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs.The code is available at https://github.com/hahnyuan/BinaryLLM.",
        "authors": [
            "Yuzhang Shang",
            "Zhihang Yuan",
            "Qiang Wu",
            "Zhen Dong"
        ],
        "citations": 32,
        "references": 59,
        "year": 2023
    },
    {
        "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
        "abstract": "Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",
        "authors": [
            "Jiongnan Liu",
            "Jiajie Jin",
            "Zihan Wang",
            "Jiehan Cheng",
            "Zhicheng Dou",
            "Ji-rong Wen"
        ],
        "citations": 49,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate",
        "abstract": "Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. Our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that LLMs can improve their responses based on feedback.",
        "authors": [
            "Boshi Wang",
            "Xiang Yue",
            "Huan Sun"
        ],
        "citations": 47,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Temporal Data Meets LLM - Explainable Financial Time Series Forecasting",
        "abstract": "This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4.",
        "authors": [
            "Xinli Yu",
            "Zheng Chen",
            "Yuan Ling",
            "Shujing Dong",
            "Zongying Liu",
            "Yanbin Lu"
        ],
        "citations": 44,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
        "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
        "authors": [
            "Xuan Zhang",
            "Wei Gao"
        ],
        "citations": 55,
        "references": 52,
        "year": 2023
    },
    {
        "title": "LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?",
        "abstract": "Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.",
        "authors": [
            "David Glukhov",
            "Ilia Shumailov",
            "Y. Gal",
            "Nicolas Papernot",
            "V. Papyan"
        ],
        "citations": 42,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators",
        "abstract": "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, taking into account both length and semantics, and merges them back into a single prompt for evaluation by LLMs. Extensive experiments with six LLMs on 11,520 answer pairs demonstrate that PORTIA markedly enhances the consistency rates for all models and forms of comparison tested, achieving an average relative improvement of 47.46%. It also enables PORTIA-enhanced GPT-3.5 to achieve agreement rates with humans comparable to GPT-4 and elevates GPT-4’s consistency rate up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass standalone GPT-4 in terms of alignment with human evaluators, highlighting PORTIA’s ability to correct position bias, improve LLM consistency, and boost performance while keeping cost efficiency.",
        "authors": [
            "Zongjie Li",
            "Chaozheng Wang",
            "Pingchuan Ma",
            "Daoyuan Wu",
            "Tianxiang Li",
            "Shuai Wang",
            "Cuiyun Gao",
            "Yang Liu"
        ],
        "citations": 30,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents",
        "abstract": "Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, exhibiting impressive human-like behaviors and cooperative capabilities in various scenarios. However, these agents also bring some exclusive risks, stemming from the complexity of interaction environments and the usability of tools. This paper delves into the safety of LLM-based agents from three perspectives: agent quantity, role definition, and attack level. Specifically, we initially propose to employ a template-based attack strategy on LLM-based agents to find the influence of agent quantity. In addition, to address interaction environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts related to the original role to examine the impact across various role definitions and attack levels. EG leverages Red-Blue exercises, significantly improving the generated prompt aggressiveness and similarity to original roles. Our evaluations on CAMEL, Metagpt and ChatDev based on GPT-3.5 and GPT-4, demonstrate high success rates. Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research. Our code is available at https://github.com/T1aNS1R/Evil-Geniuses.",
        "authors": [
            "Yu Tian",
            "Xiao Yang",
            "Jingyuan Zhang",
            "Yinpeng Dong",
            "Hang Su"
        ],
        "citations": 39,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena",
        "abstract": "Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM performance and occasional outperformance by simpler methods indicate opportunities for further advancements in LLM design and the value of our simulation environment for ongoing testing and refinement.",
        "authors": [
            "Jiangjie Chen",
            "Siyu Yuan",
            "Rong Ye",
            "Bodhisattwa Prasad Majumder",
            "Kyle Richardson"
        ],
        "citations": 41,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Low-code LLM: Visual Programming over LLMs",
        "abstract": "Effectively utilizing LLMs for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper intro-duces a novel human-LLM interaction framework, Low-code LLM . It incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the workﬂow without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workﬂow for complex tasks, which can be correspondingly edited and conﬁrmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-conﬁrmed workﬂow. We highlight three advantages of the low-code LLM: controllable generation results, user-friendly human-LLM interaction, and broadly applicable scenarios. We demonstrate its beneﬁts using four typical applications. By introducing this approach, we aim to bridge the gap between humans and LLMs, enabling more effective and efﬁcient utilization of LLMs for complex tasks. Our system will be soon publicly available at LowCodeLLM .",
        "authors": [
            "Yuzhe Cai",
            "Shaoguang Mao",
            "Wenshan Wu",
            "Zehua Wang",
            "Yaobo Liang",
            "Tao Ge",
            "Chenfei Wu",
            "Wang You",
            "Ting Song",
            "Yan Xia",
            "Jonathan Tien",
            "Nan Duan"
        ],
        "citations": 40,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Make LLM a Testing Expert: Bringing Human-Like Interaction to Mobile GUI Testing via Functionality-Aware Decisions",
        "abstract": "Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32% in activity coverage, and detects 31% more bugs at a faster rate. Moreover, GPTDroid identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed.",
        "authors": [
            "Zhe Liu",
            "Chunyang Chen",
            "Junjie Wang",
            "Mengzhuo Chen",
            "Boyu Wu",
            "Xing Che",
            "Dandan Wang",
            "Qing Wang"
        ],
        "citations": 39,
        "references": 78,
        "year": 2023
    },
    {
        "title": "MathChat: Converse to Tackle Challenging Math Problems with LLM Agents",
        "abstract": "Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. LLMs, with their generalized ability, are used as a foundation model to build AI agents for different tasks. In this paper, we study the effectiveness of utilizing LLM agents to solve math problems through conversations. We propose MathChat, a conversational problem-solving framework designed for math problems. MathChat consists of an LLM agent and a user proxy agent which is responsible for tool execution and additional guidance. This synergy facilitates a collaborative problem-solving process, where the agents engage in a dialogue to solve the problems. We perform evaluation on difficult high school competition problems from the MATH dataset. Utilizing Python, we show that MathChat can further improve previous tool-using prompting methods by 6%.",
        "authors": [
            "Yiran Wu",
            "Feiran Jia",
            "Shaokun Zhang",
            "Han-Tai Li",
            "Erkang Zhu",
            "Yue Wang",
            "Y. Lee",
            "Richard Peng",
            "Qingyun Wu",
            "Chi Wang"
        ],
        "citations": 41,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline",
        "abstract": "Large language models (LLMs) have revolutionized the field of AI, demonstrating unprecedented capacity across various tasks. However, the inference process for LLMs comes with significant computational costs. In this paper, we propose an efficient LLM inference pipeline that harnesses the power of LLMs. Our approach begins by tapping into the potential of LLMs to accurately perceive and predict the response length with minimal overhead. By leveraging this information, we introduce an efficient sequence scheduling technique that groups queries with similar response lengths into micro-batches. We evaluate our approach on real-world instruction datasets using the LLaMA-based model, and our results demonstrate an impressive 86% improvement in inference throughput without compromising effectiveness. Notably, our method is orthogonal to other inference acceleration techniques, making it a valuable addition to many existing toolkits (e.g., FlashAttention, Quantization) for LLM inference.",
        "authors": [
            "Zangwei Zheng",
            "Xiaozhe Ren",
            "Fuzhao Xue",
            "Yang Luo",
            "Xin Jiang",
            "Yang You"
        ],
        "citations": 37,
        "references": 41,
        "year": 2023
    },
    {
        "title": "LLM-Rec: Personalized Recommendation via Prompting Large Language Models",
        "abstract": "Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model's comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.",
        "authors": [
            "Hanjia Lyu",
            "Song Jiang",
            "Hanqing Zeng",
            "Yinglong Xia",
            "Jiebo Luo"
        ],
        "citations": 36,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions",
        "abstract": "CQA systems aim to create interactive search systems that effectively retrieve information by interacting with users. To replicate human-to-human conversations, existing work uses human annotators to play the roles of the questioner (student) and the answerer (teacher). Despite its effectiveness, challenges exist as human annotation is time-consuming, inconsistent, and not scalable. To address this issue and investigate the applicability of LLM in CQA simulation, we propose a simulation framework that employs zero-shot learner LLM for simulating teacher--student interactions. Our framework involves two LLMs interacting on a specific topic, with the first LLM acting as a student, generating questions to explore a given search topic. The second LLM plays the role of a teacher by answering questions and is equipped with additional information, including a text on the given topic. We implement both the student and teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness of LLMs in simulating CQA interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the simulated data from various perspectives. We begin by evaluating the teacher's performance through both automatic and human assessment. Next, we evaluate the performance of the student, analyzing and comparing the disparities between questions generated by the LLM and those generated by humans. Furthermore, we conduct extensive analyses to thoroughly examine the LLM performance by benchmarking state-of-the-art reading comprehension models on both datasets. Our results reveal that the teacher LLM generates lengthier answers that tend to be more accurate and complete. The student LLM generates more diverse questions, covering more aspects of a given topic.",
        "authors": [
            "Zahra Abbasiantaeb",
            "Yifei Yuan",
            "E. Kanoulas",
            "Mohammad Aliannejadi"
        ],
        "citations": 37,
        "references": 55,
        "year": 2023
    },
    {
        "title": "LLMParser: A LLM-based Log Parsing Framework",
        "abstract": "—The process of log parsing, which converts log messages into structured formats, is a crucial step for various log analysis tasks. Although numerous log parsers have been proposed, their effectiveness on complex log data is often hindered due to reliance on human-made rules or learning-based models with limited training data. The recent rise of powerful large language models (LLMs) shows potential for log parsing due to their extensive pre-trained knowledge related to code and logging. However, their accuracy is currently limited due to the lack of specialized log parsing capabilities. Additionally, the inconsistency of their answers and significant overhead obstruct the practical implementation of LLM-based log parsing. To tackle these challenges, we introduce LLMParser, the first practical LLM-based log parsing framework. LLMParser enables accurate and robust log parsing by leveraging the in-context learning (ICL) capability of the LLM, employing a hierarchical candidate sampling algorithm, and selecting high-quality demonstrations. LLMParser also includes a novel adaptive parsing cache component to store and refine the templates generated by the LLM. This design aids in addressing the inefficiency of LLMs by rapid matching to previously parsed log templates. LLMParser also adaptively updates the templates in the parsing cache to ensure consistent parsed results. Extensive evaluation on large-scale public datasets demonstrates that LLMParser surpasses the state-of-the-art",
        "authors": [
            "Zhihan Jiang",
            "Jinyang Liu",
            "Zhuangbin Chen",
            "Yichen Li",
            "Junjie Huang",
            "Yintong Huo",
            "Pinjia He",
            "Jiazhen Gu",
            "Michael R. Lyu"
        ],
        "citations": 24,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Audio-Visual LLM for Video Understanding",
        "abstract": "This paper presents Audio-Visual LLM, a Multimodal Large Language Model that takes both visual and auditory inputs for holistic video understanding. A key design is the modality-augmented training, which involves the integration of modality-specific tokens engineered to activate the appropriate visual and/or auditory encoder selectively. This mechanism is pivotal in enabling end-to-end joint training with video data at different modalities, including visual-only, audio-only, and audio-visual formats. Moreover, we introduce a high-quality video instruction dataset, derived from GPT-4. This dataset allows Audio-Visual LLM to adeptly process a variety of task-oriented video instructions, ranging from multi-turn conversations and audio-visual narratives to complex reasoning tasks. Extensive experiments demonstrate that Audio-Visual LLM impressively achieves strong zero-shot results across a range of video understanding tasks. For example, Audio-Visual LLM achieves an accuracy of 53.7% on MSRVTT-QA, outperforming non-LLM-based InterVideo by 6.6% and LLM-based Valley by 4.4%, respectively. Additionally, our Audio-Visual LLM also achieves competitive performance on audio tasks (e.g., AudioCaps).",
        "authors": [
            "Fangxun Shu",
            "Lei Zhang",
            "Hao Jiang",
            "Cihang Xie"
        ],
        "citations": 24,
        "references": 86,
        "year": 2023
    },
    {
        "title": "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay",
        "abstract": "This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents’ social behaviors. Results affirm the framework’s effectiveness in creating adaptive agents and suggest LLM-based agents’ potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field’s research and applications.",
        "authors": [
            "Yihuai Lan",
            "Zhiqiang Hu",
            "Lei Wang",
            "Yang Wang",
            "De-Yong Ye",
            "Peilin Zhao",
            "Ee-Peng Lim",
            "Hui Xiong",
            "Hao Wang"
        ],
        "citations": 24,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Cascade Speculative Drafting for Even Faster LLM Inference",
        "abstract": "Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves up to an 81 percent additional speedup over speculative decoding in our experiments, while maintaining the same output distribution as the target model. Our code is publicly available at https://github.com/lfsszd/CS-Drafting.",
        "authors": [
            "Ziyi Chen",
            "Xiaocong Yang",
            "Jiacheng Lin",
            "Chenkai Sun",
            "Jie Huang",
            "Kevin Chen-Chuan Chang"
        ],
        "citations": 35,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Empowering LLM-based Machine Translation with Cultural Awareness",
        "abstract": "Traditional neural machine translation (NMT) systems often fail to translate sentences that contain culturally specific information. Most previous NMT methods have incorporated external cultural knowledge during training, which requires fine-tuning on low-frequency items specific to the culture. Recent in-context learning utilizes lightweight prompts to guide large language models (LLMs) to perform machine translation, however, whether such an approach works in terms of injecting culture awareness into machine translation remains unclear. To this end, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with annotations of cultural-specific entities. Additionally, we design simple but effective prompting strategies to assist this LLM-based translation. Extensive experiments show that our approaches can largely help incorporate cultural knowledge into LLM-based machine translation, outper-forming traditional NMT systems in translating cultural-specific sentences.",
        "authors": [
            "Binwei Yao",
            "Ming Jiang",
            "Diyi Yang",
            "Junjie Hu"
        ],
        "citations": 32,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment",
        "abstract": "Recent advances in the development of large language models are rapidly changing how online applications function. LLM-based search tools, for instance, offer a natural language interface that can accommodate complex queries and provide detailed, direct responses. At the same time, there have been concerns about the veracity of the information provided by LLM-based tools due to potential mistakes or fabrications that can arise in algorithmically generated text. In a set of online experiments we investigate how LLM-based search changes people's behavior relative to traditional search, and what can be done to mitigate overreliance on LLM-based output. Participants in our experiments were asked to solve a series of decision tasks that involved researching and comparing different products, and were randomly assigned to do so with either an LLM-based search tool or a traditional search engine. In our first experiment, we find that participants using the LLM-based tool were able to complete their tasks more quickly, using fewer but more complex queries than those who used traditional search. Moreover, these participants reported a more satisfying experience with the LLM-based search tool. When the information presented by the LLM was reliable, participants using the tool made decisions with a comparable level of accuracy to those using traditional search, however we observed overreliance on incorrect information when the LLM erred. Our second experiment further investigated this issue by randomly assigning some users to see a simple color-coded highlighting scheme to alert them to potentially incorrect or misleading information in the LLM responses. Overall we find that this confidence-based highlighting substantially increases the rate at which users spot incorrect information, improving the accuracy of their overall decisions while leaving most other measures unaffected.",
        "authors": [
            "S. Spatharioti",
            "David M. Rothschild",
            "D. Goldstein",
            "Jake M. Hofman"
        ],
        "citations": 34,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models",
        "abstract": "The demand for psychological counselling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counselling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based assistive tool leveraging Large Language Models (LLMs) for question-answering in psychological consultation settings to ease the demand for mental health professions. Our framework combines pre-trained LLMs with real-world professional Q\\&A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and extrinsic evaluation metrics, with human participant assessments of response helpfulness, fluency, relevance, and logic. The results demonstrate the effectiveness of the Psy-LLM framework in generating coherent and relevant answers to psychological questions. This article discusses the potential and limitations of using large language models to enhance mental health support through AI technologies.",
        "authors": [
            "Tin Lai",
            "Yukun Shi",
            "Zicong Du",
            "Jiajie Wu",
            "Ken Fu",
            "Yichao Dou",
            "Ziqi Wang"
        ],
        "citations": 33,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Learning to Generate Better Than Your LLM",
        "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users after finetuning with RL. Capitalizing on key properties of text generation, we seek to investigate RL algorithms beyond general purpose algorithms like Proximal Policy Optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We provide two ways for the guide LLM to interact with the LLM to be optimized for maximizing rewards. The guide LLM can generate text which serves as additional starting states for the RL optimization procedure. The guide LLM can also be used to complete the partial sentences generated by the LLM that is being optimized, treating the guide LLM as an expert to imitate and surpass eventually. We experiment on the IMDB positive sentiment, CommonGen, and TL;DR summarization tasks. We show that our RL algorithms achieve higher performance than supervised learning (SL) and the RL baseline PPO, demonstrating the benefit of interaction with the guide LLM. On both CommonGen and TL;DR, we not only outperform our SL baselines but also improve upon PPO across a variety of metrics beyond the one we optimized for. Our code can be found at https://github.com/Cornell-RL/tril.",
        "authors": [
            "Jonathan D. Chang",
            "Kianté Brantley",
            "Rajkumar Ramamurthy",
            "Dipendra Kumar Misra",
            "Wen Sun"
        ],
        "citations": 32,
        "references": 108,
        "year": 2023
    },
    {
        "title": "LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis",
        "abstract": "Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA's labor and time demands.",
        "authors": [
            "Shih-Chieh Dai",
            "Aiping Xiong",
            "Lun-Wei Ku"
        ],
        "citations": 32,
        "references": 39,
        "year": 2023
    },
    {
        "title": "LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs",
        "abstract": "This paper introduces LLM-jp, a cross-organizational project for the research and development of Japanese large language models (LLMs). LLM-jp aims to develop open-source and strong Japanese LLMs, and as of this writing, more than 1,500 participants from academia and industry are working together for this purpose. This paper presents the background of the establishment of LLM-jp, summaries of its activities, and technical reports on the LLMs developed by LLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.",
        "authors": [
            "LLM-jp Akiko Aizawa",
            "Eiji Aramaki",
            "Bowen Chen",
            "Fei Cheng",
            "Hiroyuki Deguchi",
            "Rintaro Enomoto",
            "Kazuki Fujii",
            "Kensuke Fukumoto",
            "Takuya Fukushima",
            "Namgi Han",
            "Yuto Harada",
            "Chikara Hashimoto",
            "Tatsuya Hiraoka",
            "Shohei Hisada",
            "Sosuke Hosokawa",
            "Lu Jie",
            "Keisuke Kamata",
            "T. Kanazawa",
            "H. Kanezashi",
            "Hiroshi Kataoka",
            "Satoru Katsumata",
            "Daisuke Kawahara",
            "Seiya Kawano",
            "Atsushi Keyaki",
            "Keisuke Kiryu",
            "Hirokazu Kiyomaru",
            "Takashi Kodama",
            "Takahiro Kubo",
            "Yohei Kuga",
            "Ryoma Kumon",
            "Shuhei Kurita",
            "S. Kurohashi",
            "Conglong Li",
            "Taiki Maekawa",
            "Hiroshi Matsuda",
            "Yusuke Miyao",
            "Kentaro Mizuki",
            "Sakae Mizuki",
            "Yugo Murawaki",
            "Ryo Nakamura",
            "Taishi Nakamura",
            "Kouta Nakayama",
            "Tomoka Nakazato",
            "Takuro Niitsuma",
            "Jiro Nishitoba",
            "Yusuke Oda",
            "Hayato Ogawa",
            "Takumi Okamoto",
            "Naoaki Okazaki",
            "Yohei Oseki",
            "Shintaro Ozaki",
            "Koki Ryu",
            "Rafał Rzepka",
            "Keisuke Sakaguchi",
            "S. Sasaki",
            "Satoshi Sekine",
            "Kohei Suda",
            "Saku Sugawara",
            "Issa Sugiura",
            "Hiroaki Sugiyama",
            "Hisami Suzuki",
            "Jun Suzuki",
            "T. Suzumura",
            "Kensuke Tachibana",
            "Yu Takagi",
            "Kyosuke Takami",
            "Koichi Takeda",
            "Masashi Takeshita",
            "Masahiro Tanaka",
            "K. Taura",
            "A. Tolmachev",
            "Nobuhiro Ueda",
            "Zhen Wan",
            "Shuntaro Yada",
            "Sakiko Yahata",
            "Yuya Yamamoto",
            "Yusuke Yamauchi",
            "Hitomi Yanaka",
            "Rio Yokota",
            "Koichiro Yoshino"
        ],
        "citations": 6,
        "references": 59,
        "year": 2024
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "abstract": "While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw more accurate conclusions about LLMs' performance on a specific complex task.",
        "authors": [
            "Shubhra (Santu) Karmaker",
            "Dongji Feng"
        ],
        "citations": 37,
        "references": 43,
        "year": 2023
    },
    {
        "title": "LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins",
        "abstract": "Large language model (LLM) platforms, such as ChatGPT, have recently begun offering an app ecosystem to interface with third-party services on the internet. While these apps extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Apps also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future third-party integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin (apps) ecosystem. We uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms. The full version of this paper is available online at https://arxiv.org/abs/2309.10254",
        "authors": [
            "Umar Iqbal",
            "Tadayoshi Kohno",
            "Franziska Roesner"
        ],
        "citations": 30,
        "references": 77,
        "year": 2023
    },
    {
        "title": "LLM as A Robotic Brain: Unifying Egocentric Memory and Control",
        "abstract": "Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The active exploration tasks require the robot to extensively explore an unknown environment within a limited number of actions. Meanwhile, the embodied question answering tasks necessitate that the robot answers questions based on observations acquired during prior explorations.",
        "authors": [
            "Jinjie Mai",
            "Jun Chen",
            "Bing-chuan Li",
            "Guocheng Qian",
            "Mohamed Elhoseiny",
            "Bernard Ghanem"
        ],
        "citations": 29,
        "references": 25,
        "year": 2023
    },
    {
        "title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately",
        "abstract": "Today, users ask Large language models (LLMs) as assistants to answer queries that require external knowledge; they ask about the weather in a specific city, about stock prices, and even about where specific locations are within their neighborhood. These queries require the LLM to produce code that invokes external APIs to answer the user's question, yet LLMs rarely produce correct code on the first try, requiring iterative code refinement upon execution results. In addition, using LLM assistants to support high query volumes can be expensive. In this work, we contribute a framework, EcoAssistant, that enables LLMs to answer code-driven queries more affordably and accurately. EcoAssistant contains three components. First, it allows the LLM assistants to converse with an automatic code executor to iteratively refine code or to produce answers based on the execution results. Second, we use a hierarchy of LLM assistants, which attempts to answer the query with weaker, cheaper LLMs before backing off to stronger, expensive ones. Third, we retrieve solutions from past successful queries as in-context demonstrations to help subsequent queries. Empirically, we show that EcoAssistant offers distinct advantages for affordability and accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of GPT-4's cost.",
        "authors": [
            "Jieyu Zhang",
            "Ranjay Krishna",
            "A. Awadallah",
            "Chi Wang"
        ],
        "citations": 27,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Self-Correcting LLM-Controlled Diffusion Models",
        "abstract": "Text-to-image generation has witnessed significant progress with the advent of diffusion models. Despite the ability to generate photorealistic images, current text-to-image diffusion models still often struggle to accurately interpret and follow complex input text prompts. In contrast to existing models that aim to generate images only with their best effort, we introduce Self-correcting LLM-controlled Diffusion (SLD). SLD is a framework that generates an image from the input prompt, assesses its alignment with the prompt, and performs self-corrections on the inaccuracies in the generated image. Steered by an LLM controller, SLD turns text-to-image generation into an iterative closed-loop process, ensuring correctness in the resulting image. SLD is not only training-free but can also be seamlessly integrated with diffusion models behind API access, such as DALL-E 3, to further boost the performance of state-of-the-art diffusion models. Experimental results show that our approach can rectify a majority of incorrect generations, particularly in generative numeracy, attribute binding, and spatial relationships. Furthermore, by simply adjusting the instructions to the LLM, SLD can perform image editing tasks, bridging the gap between text-to-image generation and image editing pipelines. Our code is available at: https://self-correcting-llm-diffusion.github.io.",
        "authors": [
            "Tsung-Han Wu",
            "Long Lian",
            "Joseph Gonzalez",
            "Boyi Li",
            "Trevor Darrell"
        ],
        "citations": 25,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception",
        "abstract": "Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role that teachers can play in shaping LLM-supported learning environments.",
        "authors": [
            "Harsh Kumar",
            "Ilya Musabirov",
            "Mohi Reza",
            "Jiakai Shi",
            "Anastasia Kuzminykh",
            "J. Williams",
            "Michael Liut"
        ],
        "citations": 24,
        "references": 128,
        "year": 2023
    },
    {
        "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization",
        "abstract": "Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.",
        "authors": [
            "Yang Jin",
            "Kun Xu",
            "Kun Xu",
            "Liwei Chen",
            "Chao Liao",
            "Jianchao Tan",
            "Quzhe Huang",
            "Bin Chen",
            "Chenyi Lei",
            "An Liu",
            "Chengru Song",
            "Xiaoqiang Lei",
            "Di Zhang",
            "Wenwu Ou",
            "Kun Gai",
            "Yadong Mu"
        ],
        "citations": 26,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Dynamic Planning with a LLM",
        "abstract": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.",
        "authors": [
            "Gautier Dagan",
            "Frank Keller",
            "A. Lascarides"
        ],
        "citations": 26,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study",
        "abstract": "Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. Although tables can be used as input to LLMs with serialization, there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \\eg, cell lookup, row retrieval, and size detection. We perform a series of evaluations on GPT-3.5 and GPT-4. We find that performance varied depending on several input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we proposeself-augmentation for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, \\eg, TabFact(\\uparrow2.31%), HybridQA(\\uparrow2.13%), SQA(\\uparrow2.72%), Feverous(\\uparrow0.84%), and ToTTo(\\uparrow5.68%). We believe that our open-source (please find code and data at https://github.com/microsoft/TableProvider) benchmark and proposed prompting methods can serve as a simple yet generic selection for future research.",
        "authors": [
            "Yuan Sui",
            "Mengyu Zhou",
            "Mingjie Zhou",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "citations": 49,
        "references": 40,
        "year": 2023
    },
    {
        "title": "AutoChip: Automating HDL Generation Using LLM Feedback",
        "abstract": "Traditionally, designs are written in Verilog hardware description language (HDL) and debugged by hardware engineers. While this approach is effective, it is time-consuming and error-prone for complex designs. Large language models (LLMs) are promising in automating HDL code generation. LLMs are trained on massive datasets of text and code, and they can learn to generate code that compiles and is functionally accurate. We aim to evaluate the ability of LLMs to generate functionally correct HDL models. We build AutoChip by combining the interactive capabilities of LLMs and the output from Verilog simulations to generate Verilog modules. We start with a design prompt for a module and the context from compilation errors and debugging messages, which highlight differences between the expected and actual outputs. This ensures that accurate Verilog code can be generated without human intervention. We evaluate AutoChip using problem sets from HDLBits. We conduct a comprehensive analysis of the AutoChip using several LLMs and problem categories. The results show that incorporating context from compiler tools, such as Icarus Verilog, improves the effectiveness, yielding 24.20% more accurate Verilog. We release our evaluation scripts and datasets as open-source contributions at the following link https://github.com/shailja-thakur/AutoChip.",
        "authors": [
            "Shailja Thakur",
            "Jason Blocklove",
            "H. Pearce",
            "Benjamin Tan",
            "Siddharth Garg",
            "Ramesh Karri"
        ],
        "citations": 44,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Mental-LLM",
        "abstract": "Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.",
        "authors": [
            "Xuhai Xu",
            "Bingsheng Yao",
            "Yu Dong",
            "Hongfeng Yu",
            "James A. Hendler",
            "A. Dey",
            "Dakuo Wang"
        ],
        "citations": 41,
        "references": 134,
        "year": 2023
    },
    {
        "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
        "abstract": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
        "authors": [
            "Zhiqiang Shen",
            "Tianhua Tao",
            "Liqun Ma",
            "W. Neiswanger",
            "Zhengzhong Liu",
            "Hongyi Wang",
            "Bowen Tan",
            "Joel Hestness",
            "Natalia Vassilieva",
            "Daria Soboleva",
            "Eric P. Xing"
        ],
        "citations": 40,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Calibrating LLM-Based Evaluator",
        "abstract": "Recent advancements in large language models (LLMs) and their emergent capabilities make LLM a promising reference-free evaluator on the quality of natural language generation, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. Our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria.",
        "authors": [
            "Yuxuan Liu",
            "Tianchi Yang",
            "Shaohan Huang",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang"
        ],
        "citations": 22,
        "references": 40,
        "year": 2023
    },
    {
        "title": "More human than human: LLM-generated narratives outperform human-LLM interleaved narratives",
        "abstract": "Narrative story generation has gained emerging interest in the field of large language models. The present paper aims to compare stories generated by an LLM only (non-interleaved) with those generated by interleaving human-generated and LLM-generated text (interleaved). The study’s hypothesis is that interleaved stories would perform better than non-interleaved stories. To verify this hypothesis, we conducted two tests with roughly 500 participants each. Participants were asked to rate stories of each type, including an overall score or preference and four facets—logical soundness, plausibility, understandability, and novelty. Our findings indicate that interleaved stories were in fact less preferred than non-interleaved stories. The result has implications for the design and implementation of our story generators. This study contributes new insights into the potential uses and restrictions of interleaved and non-interleaved systems regarding generating narrative stories, which may help to improve the performance of such story generators.",
        "authors": [
            "Zoie Zhao",
            "Sophie Song",
            "Bridget Duah",
            "J. Macbeth",
            "Scott Carter",
            "Monica P Van",
            "N. Bravo",
            "M. Klenk",
            "Kate Sick",
            "Alexandre L. S. Filipowicz"
        ],
        "citations": 22,
        "references": 4,
        "year": 2023
    },
    {
        "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
        "abstract": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual IO. This direction of research is particularly relevant to medical imaging because medical image analysis and generation consist of reasoning based on a combination of visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our model, LLM-CXR, trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks. The code is at https://github.com/hyn2028/llm-cxr.",
        "authors": [
            "Suhyeon Lee",
            "Won Jun Kim",
            "Jinho Chang",
            "Jong-Chul Ye"
        ],
        "citations": 26,
        "references": 46,
        "year": 2023
    },
    {
        "title": "SELF-GUARD: Empower the LLM to Safeguard Itself",
        "abstract": "With the increasing risk posed by jailbreak attacks, recent studies have investigated various methods to improve the safety of large language models (LLMs), mainly falling into two strategies: safety training and safeguards. Safety training involves fine-tuning the LLM with adversarial samples, which activate the LLM’s capabilities against jailbreak. However, it is not always effective in countering new attacks and often leads to potential performance degradation. Safeguards, on the other hand, are methods using additional models to filter harmful content from the LLM’s response. Nevertheless, they can only reduce a limited amount of harmful output and introduce extra computational costs. Given the distinct strengths and weaknesses of both, we combine them to balance out their flaws and propose a more effective method called Self-Guard.Specifically, we train the LLM to review its responses for any harmful content and append a [harmful] or [harmless] tag to the end of the response. In this way, Self-Guard possesses the advantages of safety training, leveraging the powerful capabilities of the LLMs themselves to detect harmfulness. Besides that, it gains flexibility like safeguards, making the safety check target the output side, which makes the system less vulnerable to attack updates. Experimental results indicate that our Self-Guard can effectively defend against jailbreak attacks and will not cause LLMs’ performance degradation.",
        "authors": [
            "Zezhong Wang",
            "Fangkai Yang",
            "Lu Wang",
            "Pu Zhao",
            "Hongru Wang",
            "Liang Chen",
            "Qingwei Lin",
            "Kam-Fai Wong"
        ],
        "citations": 19,
        "references": 69,
        "year": 2023
    },
    {
        "title": "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models",
        "abstract": "We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. Extensive evaluations on multiple unseen datasets highlight REMARK-LLM proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against a spectrum of watermark detection and removal attacks.",
        "authors": [
            "Ruisi Zhang",
            "Shehzeen Samarah Hussain",
            "Paarth Neekhara",
            "F. Koushanfar"
        ],
        "citations": 19,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction",
        "abstract": "Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose Libro, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of Libro shows that, on the widely studied Defects4J benchmark, Libro can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate Libro against 31 bug reports submitted after the collection of the LLM training data terminated: Libro produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show Libro has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.",
        "authors": [
            "Sungmin Kang",
            "Juyeon Yoon",
            "Shin Yoo"
        ],
        "citations": 128,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Chainpoll: A high efficacy method for LLM hallucination detection",
        "abstract": "Large language models (LLMs) have experienced notable advancements in generating coherent and contextually relevant responses. However, hallucinations - incorrect or unfounded claims - are still prevalent, prompting the creation of automated metrics to detect these in LLM outputs. Our contributions include: introducing ChainPoll, an innovative hallucination detection method that excels compared to its counterparts, and unveiling RealHall, a refined collection of benchmark datasets to assess hallucination detection metrics from recent studies. While creating RealHall, we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use. Overcoming this, we opted for four datasets challenging for modern LLMs and pertinent to real-world scenarios. Using RealHall, we conducted a comprehensive comparison of ChainPoll with numerous hallucination metrics from recent studies. Our findings indicate that ChainPoll outperforms in all RealHall benchmarks, achieving an overall AUROC of 0.781. This surpasses the next best theoretical method by 11% and exceeds industry standards by over 23%. Additionally, ChainPoll is cost-effective and offers greater transparency than other metrics. We introduce two novel metrics to assess LLM hallucinations: Adherence and Correctness. Adherence is relevant to Retrieval Augmented Generation workflows, evaluating an LLM's analytical capabilities within given documents and contexts. In contrast, Correctness identifies logical and reasoning errors.",
        "authors": [
            "Robert Friel",
            "Atindriyo Sanyal"
        ],
        "citations": 18,
        "references": 24,
        "year": 2023
    },
    {
        "title": "Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces",
        "abstract": "Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman’s gulfs of execution and evaluation. To address this gap, we theorize how end-users ‘envision’ translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM’s output in meeting the goal. Finally, we make recommendations to narrow the envisioning gulf in human-LLM interactions",
        "authors": [
            "Hariharan Subramonyam",
            "Christopher Pondoc",
            "Colleen Seifert",
            "Maneesh Agrawala",
            "Roy Pea"
        ],
        "citations": 18,
        "references": 162,
        "year": 2023
    },
    {
        "title": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation",
        "abstract": "Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM’s output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM’s output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM’s remarkable abilities. To address these limitations, we propose **LLatrieval** (**L**arge **La**nguage Model Verified Re**trieval**),where the LLM updates the retrieval result until it verifies that the retrieved documents can sufficiently support answering the question. Thus, the LLM can iteratively provide feedback to retrieval and facilitate the retrieval result to fully support verifiable generation. Experiments on ALCE show that LLatrieval significantly outperforms extensive baselines and achieves state-of-the-art results.",
        "authors": [
            "Xiaonan Li",
            "Changtai Zhu",
            "Linyang Li",
            "Zhangyue Yin",
            "Tianxiang Sun",
            "Xipeng Qiu"
        ],
        "citations": 17,
        "references": 65,
        "year": 2023
    },
    {
        "title": "LLM Performance Predictors are good initializers for Architecture Search",
        "abstract": "In this work, we utilize Large Language Models (LLMs) for a novel use case: constructing Performance Predictors (PP) that estimate the performance of specific deep neural network architectures on downstream tasks. We create PP prompts for LLMs, comprising (i) role descriptions, (ii) instructions for the LLM, (iii) hyperparameter definitions, and (iv) demonstrations presenting sample architectures with efficiency metrics and `training from scratch' performance. In machine translation (MT) tasks, GPT-4 with our PP prompts (LLM-PP) achieves a SoTA mean absolute error and a slight degradation in rank correlation coefficient compared to baseline predictors. Additionally, we demonstrate that predictions from LLM-PP can be distilled to a compact regression model (LLM-Distill-PP), which surprisingly retains much of the performance of LLM-PP. This presents a cost-effective alternative for resource-intensive performance estimation. Specifically, for Neural Architecture Search (NAS), we introduce a Hybrid-Search algorithm (HS-NAS) employing LLM-Distill-PP for the initial search stages and reverting to the baseline predictor later. HS-NAS performs similarly to SoTA NAS, reducing search hours by approximately 50%, and in some cases, improving latency, GFLOPs, and model size. The code can be found at: https://github.com/UBC-NLP/llmas.",
        "authors": [
            "Ganesh Jawahar",
            "Muhammad Abdul-Mageed",
            "L. Lakshmanan",
            "Dujian Ding"
        ],
        "citations": 15,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Will we run out of data? Limits of LLM scaling based on human-generated data",
        "abstract": "We investigate the potential constraints on LLM scaling posed by the availability of public human-generated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from data-rich domains, and data efficiency improvements might support further progress.",
        "authors": [
            "Pablo Villalobos",
            "J. Sevilla",
            "Lennart Heim",
            "T. Besiroglu",
            "Marius Hobbhahn",
            "A. Ho"
        ],
        "citations": 74,
        "references": 105,
        "year": 2022
    },
    {
        "title": "Visual Instruction Tuning",
        "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
        "authors": [
            "Haotian Liu",
            "Chunyuan Li",
            "Qingyang Wu",
            "Yong Jae Lee"
        ],
        "citations": 1000,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Large Language Models are Zero-Shot Reasoners",
        "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
        "authors": [
            "Takeshi Kojima",
            "S. Gu",
            "Machel Reid",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "citations": 1000,
        "references": 61,
        "year": 2022
    },
    {
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
        "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
        "authors": [
            "Andy Zou",
            "Zifan Wang",
            "J. Z. Kolter",
            "Matt Fredrikson"
        ],
        "citations": 937,
        "references": 49,
        "year": 2023
    },
    {
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
        "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.",
        "authors": [
            "Deyao Zhu",
            "Jun Chen",
            "Xiaoqian Shen",
            "Xiang Li",
            "Mohamed Elhoseiny"
        ],
        "citations": 1000,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
        "abstract": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.",
        "authors": [
            "Woosuk Kwon",
            "Zhuohan Li",
            "Siyuan Zhuang",
            "Ying Sheng",
            "Lianmin Zheng",
            "Cody Hao Yu",
            "Joseph E. Gonzalez",
            "Haotong Zhang",
            "Ion Stoica"
        ],
        "citations": 1000,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Prakhar Gupta",
            "Skyler Hallinan",
            "Luyu Gao",
            "Sarah Wiegreffe",
            "Uri Alon",
            "Nouha Dziri",
            "Shrimai Prabhumoye",
            "Yiming Yang",
            "S. Welleck",
            "Bodhisattwa Prasad Majumder",
            "Shashank Gupta",
            "A. Yazdanbakhsh",
            "Peter Clark"
        ],
        "citations": 1000,
        "references": 52,
        "year": 2023
    },
    {
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
        "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
        "authors": [
            "Yang Liu",
            "Dan Iter",
            "Yichong Xu",
            "Shuo Wang",
            "Ruochen Xu",
            "Chenguang Zhu"
        ],
        "citations": 844,
        "references": 40,
        "year": 2023
    },
    {
        "title": "ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns",
        "abstract": "ChatGPT is an artificial intelligence (AI)-based conversational large language model (LLM). The potential applications of LLMs in health care education, research, and practice could be promising if the associated valid concerns are proactively examined and addressed. The current systematic review aimed to investigate the utility of ChatGPT in health care education, research, and practice and to highlight its potential limitations. Using the PRIMSA guidelines, a systematic search was conducted to retrieve English records in PubMed/MEDLINE and Google Scholar (published research or preprints) that examined ChatGPT in the context of health care education, research, or practice. A total of 60 records were eligible for inclusion. Benefits of ChatGPT were cited in 51/60 (85.0%) records and included: (1) improved scientific writing and enhancing research equity and versatility; (2) utility in health care research (efficient analysis of datasets, code generation, literature reviews, saving time to focus on experimental design, and drug discovery and development); (3) benefits in health care practice (streamlining the workflow, cost saving, documentation, personalized medicine, and improved health literacy); and (4) benefits in health care education including improved personalized learning and the focus on critical thinking and problem-based learning. Concerns regarding ChatGPT use were stated in 58/60 (96.7%) records including ethical, copyright, transparency, and legal issues, the risk of bias, plagiarism, lack of originality, inaccurate content with risk of hallucination, limited knowledge, incorrect citations, cybersecurity issues, and risk of infodemics. The promising applications of ChatGPT can induce paradigm shifts in health care education, research, and practice. However, the embrace of this AI chatbot should be conducted with extreme caution considering its potential limitations. As it currently stands, ChatGPT does not qualify to be listed as an author in scientific articles unless the ICMJE/COPE guidelines are revised or amended. An initiative involving all stakeholders in health care education, research, and practice is urgently needed. This will help to set a code of ethics to guide the responsible use of ChatGPT among other LLMs in health care and academia.",
        "authors": [
            "Malik Sallam"
        ],
        "citations": 1000,
        "references": 101,
        "year": 2023
    },
    {
        "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
        "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
        "authors": [
            "Can Xu",
            "Qingfeng Sun",
            "Kai Zheng",
            "Xiubo Geng",
            "Pu Zhao",
            "Jiazhan Feng",
            "Chongyang Tao",
            "Daxin Jiang"
        ],
        "citations": 781,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
        "abstract": "We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
        "authors": [
            "Hang Zhang",
            "Xin Li",
            "Lidong Bing"
        ],
        "citations": 679,
        "references": 42,
        "year": 2023
    },
    {
        "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
        "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.",
        "authors": [
            "Qinghao Ye",
            "Haiyang Xu",
            "Guohai Xu",
            "Jiabo Ye",
            "Ming Yan",
            "Yi Zhou",
            "Junyan Wang",
            "Anwen Hu",
            "Pengcheng Shi",
            "Yaya Shi",
            "Chenliang Li",
            "Yuanhong Xu",
            "Hehong Chen",
            "Junfeng Tian",
            "Qiang Qi",
            "Ji Zhang",
            "Feiyan Huang"
        ],
        "citations": 755,
        "references": 36,
        "year": 2023
    },
    {
        "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
        "abstract": "Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.",
        "authors": [
            "Jules White",
            "Quchen Fu",
            "Sam Hays",
            "Michael Sandborn",
            "Carlos Olea",
            "Henry Gilbert",
            "Ashraf Elnashar",
            "Jesse Spencer-Smith",
            "Douglas C. Schmidt"
        ],
        "citations": 818,
        "references": 54,
        "year": 2023
    },
    {
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
        "authors": [
            "Yongliang Shen",
            "Kaitao Song",
            "Xu Tan",
            "Dongsheng Li",
            "Weiming Lu",
            "Y. Zhuang"
        ],
        "citations": 711,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Capabilities of GPT-4 on Medical Challenge Problems",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.",
        "authors": [
            "Harsha Nori",
            "Nicholas King",
            "S. McKinney",
            "Dean Carignan",
            "E. Horvitz"
        ],
        "citations": 657,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Gemma: Open Models Based on Gemini Research and Technology",
        "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.",
        "authors": [
            "Gemma Team Thomas Mesnard",
            "Cassidy Hardin",
            "Robert Dadashi",
            "Surya Bhupatiraju",
            "Shreya Pathak",
            "L. Sifre",
            "Morgane Rivière",
            "Mihir Kale",
            "J Christopher Love",
            "P. Tafti",
            "L'eonard Hussenot",
            "Aakanksha Chowdhery",
            "Adam Roberts",
            "Aditya Barua",
            "Alex Botev",
            "Alex Castro-Ros",
            "Ambrose Slone",
            "Am'elie H'eliou",
            "Andrea Tacchetti",
            "Anna Bulanova",
            "Antonia Paterson",
            "Beth Tsai",
            "Bobak Shahriari",
            "Charline Le Lan",
            "Christopher A. Choquette-Choo",
            "Clé-ment Crepy",
            "Daniel Cer",
            "Daphne Ippolito",
            "David Reid",
            "Elena Buchatskaya",
            "Eric Ni",
            "Eric Noland",
            "Geng Yan",
            "George Tucker",
            "George-Christian Muraru",
            "Grig-ory Rozhdestvenskiy",
            "H. Michalewski",
            "Ian Tenney",
            "Ivan Grishchenko",
            "Jacob Austin",
            "James Keeling",
            "Jane Labanowski",
            "Jean-Baptiste Lespiau",
            "J. Stanway",
            "Jenny Brennan",
            "Jeremy Chen",
            "Johan Ferret",
            "Justin Chiu",
            "J. Mao-Jones",
            "Kather-ine Lee",
            "Kathy Yu",
            "Katie Millican",
            "Lars Lowe Sjoesund",
            "Lisa Lee",
            "Lucas Dixon",
            "Machel Reid",
            "Maciej Mikuła",
            "Mateo Wirth",
            "Michael Sharman",
            "Nikolai Chinaev",
            "Nithum Thain",
            "Olivier Bachem",
            "Oscar Chang",
            "O. Wahltinez",
            "Paige Bailey",
            "Paul Michel",
            "Petko Yotov",
            "Pier Giuseppe Sessa",
            "Rahma Chaabouni",
            "Ramona Comanescu",
            "Reena Jana",
            "Rohan Anil",
            "Ross McIlroy",
            "Ruibo Liu",
            "Ryan Mullins",
            "Samuel L Smith",
            "Sebastian Borgeaud",
            "Sertan Girgin",
            "Sholto Douglas",
            "Shree Pandya",
            "Siamak Shakeri",
            "Soham De",
            "Ted Klimenko",
            "Tom Hennigan",
            "Vladimir Feinberg",
            "Wojciech Stokowiec",
            "Yu-hui Chen",
            "Zafarali Ahmed",
            "Zhitao Gong",
            "Tris Warkentin",
            "Ludovic Peran",
            "Minh Giang",
            "Clément Farabet",
            "O. Vinyals",
            "Jeffrey Dean",
            "K. Kavukcuoglu",
            "D. Hassabis",
            "Z. Ghahramani",
            "Douglas Eck",
            "Joelle Barral",
            "Fernando Pereira",
            "Eli Collins",
            "Armand Joulin",
            "Noah Fiedel",
            "Evan Senter",
            "Alek Andreev",
            "Kathleen Kenealy"
        ],
        "citations": 297,
        "references": 47,
        "year": 2024
    },
    {
        "title": "A Survey on Large Language Model based Autonomous Agents",
        "abstract": null,
        "authors": [
            "Lei Wang",
            "Chengbang Ma",
            "Xueyang Feng",
            "Zeyu Zhang",
            "Hao-ran Yang",
            "Jingsen Zhang",
            "Zhi-Yang Chen",
            "Jiakai Tang",
            "Xu Chen",
            "Yankai Lin",
            "Wayne Xin Zhao",
            "Zhewei Wei",
            "Ji-rong Wen"
        ],
        "citations": 785,
        "references": 193,
        "year": 2023
    },
    {
        "title": "StarCoder: may the source be with you!",
        "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
        "authors": [
            "Raymond Li",
            "Loubna Ben Allal",
            "Yangtian Zi",
            "Niklas Muennighoff",
            "Denis Kocetkov",
            "Chenghao Mou",
            "Marc Marone",
            "Christopher Akiki",
            "Jia Li",
            "Jenny Chim",
            "Qian Liu",
            "Evgenii Zheltonozhskii",
            "Terry Yue Zhuo",
            "Thomas Wang",
            "Olivier Dehaene",
            "Mishig Davaadorj",
            "J. Lamy-Poirier",
            "João Monteiro",
            "Oleh Shliazhko",
            "Nicolas Gontier",
            "Nicholas Meade",
            "A. Zebaze",
            "Ming-Ho Yee",
            "Logesh Kumar Umapathi",
            "Jian Zhu",
            "Benjamin Lipkin",
            "Muhtasham Oblokulov",
            "Zhiruo Wang",
            "Rudra Murthy",
            "J. Stillerman",
            "S. Patel",
            "Dmitry Abulkhanov",
            "Marco Zocca",
            "Manan Dey",
            "Zhihan Zhang",
            "N. Fahmy",
            "Urvashi Bhattacharyya",
            "W. Yu",
            "Swayam Singh",
            "Sasha Luccioni",
            "Paulo Villegas",
            "M. Kunakov",
            "Fedor Zhdanov",
            "Manuel Romero",
            "Tony Lee",
            "Nadav Timor",
            "Jennifer Ding",
            "Claire Schlesinger",
            "Hailey Schoelkopf",
            "Jana Ebert",
            "Tri Dao",
            "Mayank Mishra",
            "A. Gu",
            "Jennifer Robinson",
            "Carolyn Jane Anderson",
            "Brendan Dolan-Gavitt",
            "Danish Contractor",
            "Siva Reddy",
            "Daniel Fried",
            "Dzmitry Bahdanau",
            "Yacine Jernite",
            "Carlos Muñoz Ferrandis",
            "Sean M. Hughes",
            "Thomas Wolf",
            "Arjun Guha",
            "L. V. Werra",
            "H. D. Vries"
        ],
        "citations": 565,
        "references": 113,
        "year": 2023
    },
    {
        "title": "BloombergGPT: A Large Language Model for Finance",
        "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",
        "authors": [
            "Shijie Wu",
            "Ozan Irsoy",
            "Steven Lu",
            "Vadim Dabravolski",
            "Mark Dredze",
            "Sebastian Gehrmann",
            "P. Kambadur",
            "D. Rosenberg",
            "Gideon Mann"
        ],
        "citations": 623,
        "references": 138,
        "year": 2023
    },
    {
        "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
        "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",
        "authors": [
            "Jiawei Liu",
            "Chun Xia",
            "Yuyao Wang",
            "Lingming Zhang"
        ],
        "citations": 534,
        "references": 92,
        "year": 2023
    },
    {
        "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
        "abstract": "LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce complex biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for chat LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question:\"What would the preference be if the model's and baseline's output had the same length?\". To achieve this, we first fit a generalized linear model to predict the biased output of interest (auto-annotator preferences) based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, we also find that it increases the Spearman correlation with LMSYS' Chatbot Arena from 0.94 to 0.98. We release the code and leaderboard at https://tatsu-lab.github.io/alpaca_eval/ .",
        "authors": [
            "Yann Dubois",
            "Bal'azs Galambosi",
            "Percy Liang",
            "Tatsunori Hashimoto"
        ],
        "citations": 198,
        "references": 27,
        "year": 2024
    },
    {
        "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
        "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",
        "authors": [
            "Zhiheng Xi",
            "Wenxiang Chen",
            "Xin Guo",
            "Wei He",
            "Yiwen Ding",
            "Boyang Hong",
            "Ming Zhang",
            "Junzhe Wang",
            "Senjie Jin",
            "Enyu Zhou",
            "Rui Zheng",
            "Xiaoran Fan",
            "Xiao Wang",
            "Limao Xiong",
            "Qin Liu",
            "Yuhao Zhou",
            "Weiran Wang",
            "Changhao Jiang",
            "Yicheng Zou",
            "Xiangyang Liu",
            "Zhangyue Yin",
            "Shihan Dou",
            "Rongxiang Weng",
            "Wensen Cheng",
            "Qi Zhang",
            "Wenjuan Qin",
            "Yongyan Zheng",
            "Xipeng Qiu",
            "Xuanjing Huan",
            "Tao Gui"
        ],
        "citations": 615,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Large language models in medicine",
        "abstract": null,
        "authors": [
            "A. J. Thirunavukarasu",
            "D. Ting",
            "Kabilan Elangovan",
            "Laura Gutierrez",
            "Ting Fang Tan",
            "D. Ting"
        ],
        "citations": 1000,
        "references": 136,
        "year": 2023
    },
    {
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
        "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.",
        "authors": [
            "Mantas Mazeika",
            "Long Phan",
            "Xuwang Yin",
            "Andy Zou",
            "Zifan Wang",
            "Norman Mu",
            "Elham Sakhaee",
            "Nathaniel Li",
            "Steven Basart",
            "Bo Li",
            "David Forsyth",
            "Dan Hendrycks"
        ],
        "citations": 181,
        "references": 107,
        "year": 2024
    },
    {
        "title": "Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios",
        "abstract": null,
        "authors": [
            "M. Cascella",
            "J. Montomoli",
            "Valentina Bellini",
            "E. Bignami"
        ],
        "citations": 622,
        "references": 18,
        "year": 2023
    },
    {
        "title": "Large language models encode clinical knowledge",
        "abstract": null,
        "authors": [
            "K. Singhal",
            "Shekoofeh Azizi",
            "T. Tu",
            "S. Mahdavi",
            "Jason Wei",
            "Hyung Won Chung",
            "Nathan Scales",
            "A. Tanwani",
            "H. Cole-Lewis",
            "S. Pfohl",
            "P. Payne",
            "Martin G. Seneviratne",
            "P. Gamble",
            "C. Kelly",
            "Nathaneal Scharli",
            "Aakanksha Chowdhery",
            "P. A. Mansfield",
            "B. A. Y. Arcas",
            "D. Webster",
            "Greg S. Corrado",
            "Yossi Matias",
            "K. Chou",
            "Juraj Gottweis",
            "Nenad Tomašev",
            "Yun Liu",
            "A. Rajkomar",
            "J. Barral",
            "Christopher Semturs",
            "A. Karthikesalingam",
            "Vivek Natarajan"
        ],
        "citations": 1000,
        "references": 111,
        "year": 2022
    },
    {
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
        "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents. Codes are available at https://github.com/uclaml/SPIN.",
        "authors": [
            "Zixiang Chen",
            "Yihe Deng",
            "Huizhuo Yuan",
            "Kaixuan Ji",
            "Quanquan Gu"
        ],
        "citations": 187,
        "references": 99,
        "year": 2024
    },
    {
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
        "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g.\"make breakfast\"), to a chosen set of actionable steps (e.g.\"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner",
        "authors": [
            "Wenlong Huang",
            "P. Abbeel",
            "Deepak Pathak",
            "Igor Mordatch"
        ],
        "citations": 901,
        "references": 55,
        "year": 2022
    },
    {
        "title": "StarCoder 2 and The Stack v2: The Next Generation",
        "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.",
        "authors": [
            "Anton Lozhkov",
            "Raymond Li",
            "Loubna Ben Allal",
            "Federico Cassano",
            "J. Lamy-Poirier",
            "Nouamane Tazi",
            "Ao Tang",
            "Dmytro Pykhtar",
            "Jiawei Liu",
            "Yuxiang Wei",
            "Tianyang Liu",
            "Max Tian",
            "Denis Kocetkov",
            "Arthur Zucker",
            "Younes Belkada",
            "Zijian Wang",
            "Qian Liu",
            "Dmitry Abulkhanov",
            "Indraneil Paul",
            "Zhuang Li",
            "Wen-Ding Li",
            "Megan L. Risdal",
            "Jia Li",
            "Jian Zhu",
            "Terry Yue Zhuo",
            "Evgenii Zheltonozhskii",
            "Nii Osae Osae Dade",
            "W. Yu",
            "Lucas Krauss",
            "Naman Jain",
            "Yixuan Su",
            "Xuanli He",
            "Manan Dey",
            "Edoardo Abati",
            "Yekun Chai",
            "Niklas Muennighoff",
            "Xiangru Tang",
            "Muhtasham Oblokulov",
            "Christopher Akiki",
            "Marc Marone",
            "Chenghao Mou",
            "Mayank Mishra",
            "A. Gu",
            "Binyuan Hui",
            "Tri Dao",
            "A. Zebaze",
            "Olivier Dehaene",
            "N. Patry",
            "Canwen Xu",
            "Julian J. McAuley",
            "Han Hu",
            "Torsten Scholak",
            "Sébastien Paquet",
            "Jennifer Robinson",
            "C. Anderson",
            "Nicolas Chapados",
            "M. Patwary",
            "Nima Tajbakhsh",
            "Yacine Jernite",
            "Carlos Muñoz Ferrandis",
            "Lingming Zhang",
            "Sean Hughes",
            "Thomas Wolf",
            "Arjun Guha",
            "L. V. Werra",
            "H. D. Vries"
        ],
        "citations": 176,
        "references": 0,
        "year": 2024
    },
    {
        "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
        "abstract": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
        "authors": [
            "E. Mitchell",
            "Yoonho Lee",
            "Alexander Khazatsky",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "citations": 469,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.",
        "authors": [
            "Yifan Li",
            "Yifan Du",
            "Kun Zhou",
            "Jinpeng Wang",
            "Wayne Xin Zhao",
            "Ji-rong Wen"
        ],
        "citations": 488,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Instruction Tuning with GPT-4",
        "abstract": "Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.",
        "authors": [
            "Baolin Peng",
            "Chunyuan Li",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "citations": 504,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Self-Rewarding Language Models",
        "abstract": "We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.",
        "authors": [
            "Weizhe Yuan",
            "Richard Yuanzhe Pang",
            "Kyunghyun Cho",
            "Sainbayar Sukhbaatar",
            "Jing Xu",
            "Jason Weston"
        ],
        "citations": 217,
        "references": 51,
        "year": 2024
    },
    {
        "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
        "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .",
        "authors": [
            "Shengding Hu",
            "Yuge Tu",
            "Xu Han",
            "Chaoqun He",
            "Ganqu Cui",
            "Xiang Long",
            "Zhi Zheng",
            "Yewei Fang",
            "Yuxiang Huang",
            "Weilin Zhao",
            "Xinrong Zhang",
            "Zhen Leng Thai",
            "Kaihuo Zhang",
            "Chongyi Wang",
            "Yuan Yao",
            "Chenyang Zhao",
            "Jie Zhou",
            "Jie Cai",
            "Zhongwu Zhai",
            "Ning Ding",
            "Chaochao Jia",
            "Guoyang Zeng",
            "Dahai Li",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "citations": 175,
        "references": 73,
        "year": 2024
    },
    {
        "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
        "abstract": "How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset. During inference, we incorporate additional expert models (e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image understanding capability without incurring training costs. Compared to the original LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA. The newly designed framework also exhibits stronger language-only instruction-following capabilities and even excels in chat interactions. Our code and models are available at https://github.com/ZrrSkywalker/LLaMA-Adapter.",
        "authors": [
            "Peng Gao",
            "Jiaming Han",
            "Renrui Zhang",
            "Ziyi Lin",
            "Shijie Geng",
            "Aojun Zhou",
            "W. Zhang",
            "Pan Lu",
            "Conghui He",
            "Xiangyu Yue",
            "Hongsheng Li",
            "Y. Qiao"
        ],
        "citations": 481,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Large Language Models: A Survey",
        "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.",
        "authors": [
            "Shervin Minaee",
            "Tomáš Mikolov",
            "Narjes Nikzad",
            "M. Chenaghlu",
            "R. Socher",
            "Xavier Amatriain",
            "Jianfeng Gao"
        ],
        "citations": 208,
        "references": 238,
        "year": 2024
    },
    {
        "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",
        "abstract": "Recently, remarkable progress has been made in automated task-solving through the use of multi-agent driven by large language models (LLMs). However, existing LLM-based multi-agent works primarily focus on solving simple dialogue tasks, and complex tasks are rarely studied, mainly due to the LLM hallucination problem. This type of hallucination becomes cascading when naively chaining multiple intelligent agents, resulting in a failure to effectively address complex problems. Therefore, we introduce MetaGPT, an innovative framework that incorporates efficient human workflows as a meta programming approach into LLM-based multi-agent collaboration. Specifically, MetaGPT encodes Standardized Operating Procedures (SOPs) into prompts to enhance structured coordination. Subsequently, it mandates modular outputs, empowering agents with domain expertise comparable to human professionals, to validate outputs and minimize compounded errors. In this way, MetaGPT leverages the assembly line paradigm to assign diverse roles to various agents, thereby establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems. Our experiments on collaborative software engineering benchmarks demonstrate that MetaGPT generates more coherent and correct solutions compared to existing chat-based multi-agent systems. This highlights the potential of integrating human domain knowledge into multi-agent systems, thereby creating new opportunities to tackle complex real-world challenges. The GitHub repository of this project is publicly available on:https://github.com/geekan/MetaGPT.",
        "authors": [
            "Sirui Hong",
            "Xiawu Zheng",
            "Jonathan P. Chen",
            "Yuheng Cheng",
            "Ceyao Zhang",
            "Zili Wang",
            "Steven Ka Shing Yau",
            "Z. Lin",
            "Liyang Zhou",
            "Chenyu Ran",
            "Lingfeng Xiao",
            "Chenglin Wu"
        ],
        "citations": 474,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
        "abstract": "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models.",
        "authors": [
            "Zhe Chen",
            "Jiannan Wu",
            "Wenhai Wang",
            "Weijie Su",
            "Guo Chen",
            "Sen Xing",
            "Zhong Muyan",
            "Qinglong Zhang",
            "Xizhou Zhu",
            "Lewei Lu",
            "Bin Li",
            "Ping Luo",
            "Tong Lu",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "citations": 455,
        "references": 190,
        "year": 2023
    },
    {
        "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.",
        "authors": [
            "Yanis Labrak",
            "Adrien Bazoge",
            "Emmanuel Morin",
            "P. Gourraud",
            "Mickael Rouvier",
            "Richard Dufour"
        ],
        "citations": 131,
        "references": 69,
        "year": 2024
    },
    {
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
        "abstract": "Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.",
        "authors": [
            "Shuming Ma",
            "Hongyu Wang",
            "Lingxiao Ma",
            "Lei Wang",
            "Wenhui Wang",
            "Shaohan Huang",
            "Lifeng Dong",
            "Ruiping Wang",
            "Jilong Xue",
            "Furu Wei"
        ],
        "citations": 136,
        "references": 28,
        "year": 2024
    },
    {
        "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
        "abstract": "Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.9% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.",
        "authors": [
            "Lin Chen",
            "Jinsong Li",
            "Xiao-wen Dong",
            "Pan Zhang",
            "Yuhang Zang",
            "Zehui Chen",
            "Haodong Duan",
            "Jiaqi Wang",
            "Yu Qiao",
            "Dahua Lin",
            "Feng Zhao"
        ],
        "citations": 121,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to their notable capabilities in planning and reasoning, LLMs have been utilized as autonomous agents for the automatic execution of various tasks. Recently, LLM-based agent systems have rapidly evolved from single-agent planning or decision-making to operating as multi-agent systems, enhancing their ability in complex problem-solving and world simulation. To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi-agent (LLM-MA) systems. Our objective is to provide readers with an in-depth understanding of these key points: the domains and settings where LLM-MA systems operate or simulate; the profiling and communication methods of these agents; and the means by which these agents develop their skills. For those interested in delving into this field, we also summarize the commonly used datasets or benchmarks. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository (github.com/taichengguo/LLM_MultiAgents_Survey_Papers), dedicated to outlining the research of LLM-MA research.",
        "authors": [
            "Taicheng Guo",
            "Xiuying Chen",
            "Yaqi Wang",
            "Ruidi Chang",
            "Shichao Pei",
            "N. Chawla",
            "Olaf Wiest",
            "Xiangliang Zhang"
        ],
        "citations": 136,
        "references": 87,
        "year": 2024
    },
    {
        "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
        "abstract": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
        "authors": [
            "Huiwen Chang",
            "Han Zhang",
            "Jarred Barber",
            "AJ Maschinot",
            "José Lezama",
            "Lu Jiang",
            "Ming Yang",
            "K. Murphy",
            "W. Freeman",
            "Michael Rubinstein",
            "Yuanzhen Li",
            "Dilip Krishnan"
        ],
        "citations": 456,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models",
        "abstract": "Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.",
        "authors": [
            "Ziwei Xu",
            "Sanjay Jain",
            "Mohan S. Kankanhalli"
        ],
        "citations": 138,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Large Language Models (LLMs) as Agents for Augmented Democracy",
        "abstract": "We explore an augmented democracy system built on off-the-shelf large language models (LLMs) fine-tuned to augment data on citizens' preferences elicited over policies extracted from the government programmes of the two main candidates of Brazil's 2022 presidential election. We use a train-test cross-validation set-up to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants. At the individual level, we find that LLMs predict out of sample preferences more accurately than a 'bundle rule', which would assume that citizens always vote for the proposals of the candidate aligned with their self-reported political orientation. At the population level, we show that a probabilistic sample augmented by an LLM provides a more accurate estimate of the aggregate preferences of a population than the non-augmented probabilistic sample alone. Together, these results indicate that policy preference data augmented using LLMs can capture nuances that transcend party lines and represents a promising avenue of research for data augmentation. This article is part of the theme issue 'Co-creating the future: participatory cities and digital governance'.",
        "authors": [
            "Jairo Gudiño-Rosero",
            "Umberto Grandi",
            "César A. Hidalgo"
        ],
        "citations": 133,
        "references": 92,
        "year": 2024
    },
    {
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
        "abstract": "Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their strong instruction-following abilities. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following requires tackling three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 50x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, DPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003. We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.",
        "authors": [
            "Yann Dubois",
            "Xuechen Li",
            "Rohan Taori",
            "Tianyi Zhang",
            "Ishaan Gulrajani",
            "Jimmy Ba",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori Hashimoto"
        ],
        "citations": 438,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
        "abstract": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.",
        "authors": [
            "Patrick Chao",
            "Alexander Robey",
            "Edgar Dobriban",
            "Hamed Hassani",
            "George J. Pappas",
            "Eric Wong"
        ],
        "citations": 400,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
        "abstract": "Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of \\emph{video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.",
        "authors": [
            "Muhammad Maaz",
            "H. Rasheed",
            "Salman H. Khan",
            "F. Khan"
        ],
        "citations": 388,
        "references": 40,
        "year": 2023
    },
    {
        "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
        "abstract": "Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.",
        "authors": [
            "Guangxuan Xiao",
            "Ji Lin",
            "Mickael Seznec",
            "Julien Demouth",
            "Song Han"
        ],
        "citations": 566,
        "references": 45,
        "year": 2022
    },
    {
        "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
        "abstract": "Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.",
        "authors": [
            "Parishad BehnamGhader",
            "Vaibhav Adlakha",
            "Marius Mosbach",
            "Dzmitry Bahdanau",
            "Nicolas Chapados",
            "Siva Reddy"
        ],
        "citations": 96,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
        "abstract": "Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.",
        "authors": [
            "K. Singhal",
            "Tao Tu",
            "Juraj Gottweis",
            "R. Sayres",
            "Ellery Wulczyn",
            "Le Hou",
            "Kevin Clark",
            "S. Pfohl",
            "H. Cole-Lewis",
            "Darlene Neal",
            "M. Schaekermann",
            "Amy Wang",
            "Mohamed Amin",
            "S. Lachgar",
            "P. A. Mansfield",
            "Sushant Prakash",
            "Bradley Green",
            "Ewa Dominowska",
            "B. A. Y. Arcas",
            "Nenad Tomašev",
            "Yun Liu",
            "Renee C Wong",
            "Christopher Semturs",
            "S. S. Mahdavi",
            "J. Barral",
            "D. Webster",
            "G. Corrado",
            "Yossi Matias",
            "Shekoofeh Azizi",
            "A. Karthikesalingam",
            "Vivek Natarajan"
        ],
        "citations": 447,
        "references": 51,
        "year": 2023
    },
    {
        "title": "PMC-LLaMA: toward building open-source language models for medicine",
        "abstract": "OBJECTIVE\nRecently, large language models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering (QA) situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this article, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA.\n\n\nMATERIALS AND METHODS\nWe adapt a general-purpose LLM toward the medical domain, involving data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive domain-specific instruction fine-tuning, encompassing medical QA, rationale for reasoning, and conversational dialogues with 202M tokens.\n\n\nRESULTS\nWhile evaluating various public medical QA benchmarks and manual rating, our lightweight PMC-LLaMA, which consists of only 13B parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, and datasets for instruction tuning will be released to the research community.\n\n\nDISCUSSION\nOur contributions are 3-fold: (1) we build up an open-source LLM toward the medical domain. We believe the proposed PMC-LLaMA model can promote further development of foundation models in medicine, serving as a medical trainable basic generative language backbone; (2) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component, demonstrating how different training data and model scales affect medical LLMs; (3) we contribute a large-scale, comprehensive dataset for instruction tuning.\n\n\nCONCLUSION\nIn this article, we systematically investigate the process of building up an open-source medical-specific LLM, PMC-LLaMA.",
        "authors": [
            "Chaoyi Wu",
            "Weixiong Lin",
            "Xiaoman Zhang",
            "Ya Zhang",
            "Weidi Xie",
            "Yanfeng Wang"
        ],
        "citations": 96,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Detecting hallucinations in large language models using semantic entropy",
        "abstract": null,
        "authors": [
            "Sebastian Farquhar",
            "Jannik Kossen",
            "Lorenz Kuhn",
            "Yarin Gal"
        ],
        "citations": 98,
        "references": 27,
        "year": 2024
    },
    {
        "title": "Large Language Models Are Human-Level Prompt Engineers",
        "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.",
        "authors": [
            "Yongchao Zhou",
            "Andrei Ioan Muresanu",
            "Ziwen Han",
            "Keiran Paster",
            "Silviu Pitis",
            "Harris Chan",
            "Jimmy Ba"
        ],
        "citations": 688,
        "references": 62,
        "year": 2022
    },
    {
        "title": "Scaling Vision Transformers to 22 Billion Parameters",
        "abstract": "The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for\"LLM-like\"scaling in vision, and provides key steps towards getting there.",
        "authors": [
            "Mostafa Dehghani",
            "Josip Djolonga",
            "Basil Mustafa",
            "Piotr Padlewski",
            "J. Heek",
            "J. Gilmer",
            "A. Steiner",
            "Mathilde Caron",
            "Robert Geirhos",
            "Ibrahim M. Alabdulmohsin",
            "Rodolphe Jenatton",
            "Lucas Beyer",
            "M. Tschannen",
            "Anurag Arnab",
            "Xiao Wang",
            "C. Riquelme",
            "Matthias Minderer",
            "J. Puigcerver",
            "Utku Evci",
            "Manoj Kumar",
            "Sjoerd van Steenkiste",
            "Gamaleldin F. Elsayed",
            "Aravindh Mahendran",
            "F. Yu",
            "Avital Oliver",
            "Fantine Huot",
            "Jasmijn Bastings",
            "Mark Collier",
            "A. Gritsenko",
            "Vighnesh Birodkar",
            "C. Vasconcelos",
            "Yi Tay",
            "Thomas Mensink",
            "Alexander Kolesnikov",
            "Filip Paveti'c",
            "Dustin Tran",
            "Thomas Kipf",
            "Mario Luvci'c",
            "Xiaohua Zhai",
            "Daniel Keysers",
            "Jeremiah Harmsen",
            "N. Houlsby"
        ],
        "citations": 470,
        "references": 134,
        "year": 2023
    },
    {
        "title": "The Power of Noise: Redefining Retrieval for RAG Systems",
        "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.",
        "authors": [
            "Florin Cuconasu",
            "Giovanni Trappolini",
            "F. Siciliano",
            "Simone Filice",
            "Cesare Campagnano",
            "Y. Maarek",
            "Nicola Tonellotto",
            "Fabrizio Silvestri"
        ],
        "citations": 85,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "abstract": "Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.",
        "authors": [
            "Bin Lin",
            "Bin Zhu",
            "Yang Ye",
            "Munan Ning",
            "Peng Jin",
            "Li Yuan"
        ],
        "citations": 347,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Reasoning with Language Model is Planning with World Model",
        "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
        "authors": [
            "Shibo Hao",
            "Yi Gu",
            "Haodi Ma",
            "Joshua Jiahua Hong",
            "Zhen Wang",
            "D. Wang",
            "Zhiting Hu"
        ],
        "citations": 354,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Direct Language Model Alignment from Online AI Feedback",
        "abstract": "Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.",
        "authors": [
            "Shangmin Guo",
            "Biao Zhang",
            "Tianlin Liu",
            "Tianqi Liu",
            "Misha Khalman",
            "Felipe Llinares-López",
            "Alexandre Ramé",
            "Thomas Mesnard",
            "Yao Zhao",
            "Bilal Piot",
            "Johan Ferret",
            "Mathieu Blondel"
        ],
        "citations": 81,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study",
        "abstract": "Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.",
        "authors": [
            "Yi Liu",
            "Gelei Deng",
            "Zhengzi Xu",
            "Yuekang Li",
            "Yaowen Zheng",
            "Ying Zhang",
            "Lida Zhao",
            "Tianwei Zhang",
            "Yang Liu"
        ],
        "citations": 362,
        "references": 22,
        "year": 2023
    },
    {
        "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding",
        "abstract": "To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.",
        "authors": [
            "Heming Xia",
            "Zhe Yang",
            "Qingxiu Dong",
            "Peiyi Wang",
            "Yongqi Li",
            "Tao Ge",
            "Tianyu Liu",
            "Wenjie Li",
            "Zhifang Sui"
        ],
        "citations": 67,
        "references": 60,
        "year": 2024
    },
    {
        "title": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving",
        "abstract": "DistServe improves the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation. Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests. We find that this strategy not only leads to strong prefill-decoding interferences but also couples the resource allocation and parallelism plans for both phases. LLM applications often emphasize individual latency for each phase: time to first token (TTFT) for the prefill phase and time per output token (TPOT) of each request for the decoding phase. In the presence of stringent latency requirements, existing systems have to prioritize one latency over the other, or over-provision compute resources to meet both. DistServe assigns prefill and decoding computation to different GPUs, hence eliminating prefill-decoding interferences. Given the application's TTFT and TPOT requirements, DistServe co-optimizes the resource allocation and parallelism strategy tailored for each phase. DistServe also places the two phases according to the serving cluster's bandwidth to minimize the communication caused by disaggregation. As a result, DistServe significantly improves LLM serving performance in terms of the maximum rate that can be served within both TTFT and TPOT constraints on each GPU. Our evaluations show that on various popular LLMs, applications, and latency requirements, DistServe can serve 7.4x more requests or 12.6x tighter SLO, compared to state-of-the-art systems, while staying within latency constraints for>90% of requests.",
        "authors": [
            "Yinmin Zhong",
            "Shengyu Liu",
            "Junda Chen",
            "Jianbo Hu",
            "Yibo Zhu",
            "Xuanzhe Liu",
            "Xin Jin",
            "Hao Zhang"
        ],
        "citations": 78,
        "references": 48,
        "year": 2024
    },
    {
        "title": "∞Bench: Extending Long Context Evaluation Beyond 100K Tokens",
        "abstract": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.",
        "authors": [
            "Xinrong Zhang",
            "Yingfa Chen",
            "Shengding Hu",
            "Zihang Xu",
            "Junhao Chen",
            "Moo Khai Hao",
            "Xu Han",
            "Zhen Leng Thai",
            "Shuo Wang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "citations": 80,
        "references": 53,
        "year": 2024
    },
    {
        "title": "The potential of generative AI for personalized persuasion at scale",
        "abstract": null,
        "authors": [
            "S. C. Matz",
            "J. D. Teeny",
            "S. Vaid",
            "H. Peters",
            "G. M. Harari",
            "M. Cerf"
        ],
        "citations": 78,
        "references": 79,
        "year": 2024
    },
    {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "M. Gales"
        ],
        "citations": 308,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
        "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
        "authors": [
            "Neel Jain",
            "Avi Schwarzschild",
            "Yuxin Wen",
            "Gowthami Somepalli",
            "John Kirchenbauer",
            "Ping-yeh Chiang",
            "Micah Goldblum",
            "Aniruddha Saha",
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "citations": 252,
        "references": 75,
        "year": 2023
    },
    {
        "title": "A Simple and Effective Pruning Approach for Large Language Models",
        "abstract": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
        "authors": [
            "Mingjie Sun",
            "Zhuang Liu",
            "Anna Bair",
            "J. Z. Kolter"
        ],
        "citations": 251,
        "references": 107,
        "year": 2023
    },
    {
        "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
        "abstract": "Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .",
        "authors": [
            "Cheng-Yu Hsieh",
            "Chun-Liang Li",
            "Chih-Kuan Yeh",
            "Hootan Nakhost",
            "Yasuhisa Fujii",
            "Alexander J. Ratner",
            "Ranjay Krishna",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "citations": 411,
        "references": 57,
        "year": 2023
    },
    {
        "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale",
        "abstract": "The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.",
        "authors": [
            "Guilherme Penedo",
            "Hynek Kydlícek",
            "Loubna Ben Allal",
            "Anton Lozhkov",
            "Margaret Mitchell",
            "Colin Raffel",
            "L. V. Werra",
            "Thomas Wolf"
        ],
        "citations": 65,
        "references": 61,
        "year": 2024
    },
    {
        "title": "A Survey on Large Language Models for Code Generation",
        "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.",
        "authors": [
            "Juyong Jiang",
            "Fan Wang",
            "Jiasi Shen",
            "Sungju Kim",
            "Sunghun Kim"
        ],
        "citations": 75,
        "references": 245,
        "year": 2024
    },
    {
        "title": "Large Language Models for Data Annotation: A Survey",
        "abstract": "Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Anno-tations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. We provide a comprehensive papers list at https://github.com/ Zhen-Tan-dmml",
        "authors": [
            "Zhen Tan",
            "Dawei Li",
            "Alimohammad Beigi",
            "Song Wang",
            "Ruocheng Guo",
            "Amrita Bhattacharjee",
            "Bohan Jiang",
            "Mansooreh Karami",
            "Jundong Li",
            "Lu Cheng",
            "Huan Liu"
        ],
        "citations": 80,
        "references": 260,
        "year": 2024
    },
    {
        "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
        "abstract": "Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.",
        "authors": [
            "Miao Xiong",
            "Zhiyuan Hu",
            "Xinyang Lu",
            "Yifei Li",
            "Jie Fu",
            "Junxian He",
            "Bryan Hooi"
        ],
        "citations": 254,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications.",
        "abstract": "Although chatbots have existed for decades, the emergence of transformer-based large language models (LLMs) has captivated the world through the most recent wave of artificial intelligence chatbots, including ChatGPT. Transformers are a type of neural network architecture that enables better contextual understanding of language and efficient training on massive amounts of unlabeled data, such as unstructured text from the internet. As LLMs have increased in size, their improved performance and emergent abilities have revolutionized natural language processing. Since language is integral to human thought, applications based on LLMs have transformative potential in many industries. In fact, LLM-based chatbots have demonstrated human-level performance on many professional benchmarks, including in radiology. LLMs offer numerous clinical and research applications in radiology, several of which have been explored in the literature with encouraging results. Multimodal LLMs can simultaneously interpret text and images to generate reports, closely mimicking current diagnostic pathways in radiology. Thus, from requisition to report, LLMs have the opportunity to positively impact nearly every step of the radiology journey. Yet, these impressive models are not without limitations. This article reviews the limitations of LLMs and mitigation strategies, as well as potential uses of LLMs, including multimodal models. Also reviewed are existing LLM-based applications that can enhance efficiency in supervised settings.",
        "authors": [
            "R. Bhayana"
        ],
        "citations": 76,
        "references": 33,
        "year": 2024
    },
    {
        "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
        "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms.Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided.In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation.We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks.We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer.We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "citations": 444,
        "references": 55,
        "year": 2023
    },
    {
        "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
        "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.",
        "authors": [
            "Tyna Eloundou",
            "Sam Manning",
            "Pamela Mishkin",
            "Daniel Rock"
        ],
        "citations": 340,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Large Language Models as Optimizers",
        "abstract": "Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.",
        "authors": [
            "Chengrun Yang",
            "Xuezhi Wang",
            "Yifeng Lu",
            "Hanxiao Liu",
            "Quoc V. Le",
            "Denny Zhou",
            "Xinyun Chen"
        ],
        "citations": 288,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
        "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",
        "authors": [
            "Jie Huang",
            "Xinyun Chen",
            "Swaroop Mishra",
            "Huaixiu Steven Zheng",
            "Adams Wei Yu",
            "Xinying Song",
            "Denny Zhou"
        ],
        "citations": 285,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
        "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of “tit for tat” state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Wenxiang Jiao",
            "Xing Wang",
            "Yan Wang",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi"
        ],
        "citations": 266,
        "references": 41,
        "year": 2023
    },
    {
        "title": "High-throughput Generative Inference of Large Language Models with a Single GPU",
        "abstract": "The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen",
        "authors": [
            "Ying Sheng",
            "Lianmin Zheng",
            "Binhang Yuan",
            "Zhuohan Li",
            "Max Ryabinin",
            "Daniel Y. Fu",
            "Zhiqiang Xie",
            "Beidi Chen",
            "Clark W. Barrett",
            "Joseph Gonzalez",
            "Percy Liang",
            "Christopher Ré",
            "Ion Stoica",
            "Ce Zhang"
        ],
        "citations": 277,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
        "abstract": "We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks",
        "authors": [
            "Maciej Besta",
            "Nils Blach",
            "Aleš Kubíček",
            "Robert Gerstenberger",
            "Lukas Gianinazzi",
            "Joanna Gajda",
            "Tomasz Lehmann",
            "Michal Podstawski",
            "H. Niewiadomski",
            "P. Nyczyk",
            "Torsten Hoefler"
        ],
        "citations": 415,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
        "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
        "authors": [
            "Yue Zhang",
            "Yafu Li",
            "Leyang Cui",
            "Deng Cai",
            "Lemao Liu",
            "Tingchen Fu",
            "Xinting Huang",
            "Enbo Zhao",
            "Yu Zhang",
            "Yulong Chen",
            "Longyue Wang",
            "A. Luu",
            "Wei Bi",
            "Freda Shi",
            "Shuming Shi"
        ],
        "citations": 402,
        "references": 215,
        "year": 2023
    },
    {
        "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
        "abstract": "Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM.",
        "authors": [
            "Wen Wang",
            "Zhe Chen",
            "Xiaokang Chen",
            "Jiannan Wu",
            "Xizhou Zhu",
            "Gang Zeng",
            "Ping Luo",
            "Tong Lu",
            "Jie Zhou",
            "Y. Qiao",
            "Jifeng Dai"
        ],
        "citations": 376,
        "references": 81,
        "year": 2023
    },
    {
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
        "authors": [
            "Lei Huang",
            "Weijiang Yu",
            "Weitao Ma",
            "Weihong Zhong",
            "Zhangyin Feng",
            "Haotian Wang",
            "Qianglong Chen",
            "Weihua Peng",
            "Xiaocheng Feng",
            "Bing Qin",
            "Ting Liu"
        ],
        "citations": 391,
        "references": 288,
        "year": 2023
    },
    {
        "title": "Benchmarking Large Language Models for News Summarization",
        "abstract": "Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.",
        "authors": [
            "Tianyi Zhang",
            "Faisal Ladhak",
            "Esin Durmus",
            "Percy Liang",
            "K. McKeown",
            "Tatsunori Hashimoto"
        ],
        "citations": 389,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
        "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
        "authors": [
            "Zhiqing Sun",
            "Yikang Shen",
            "Qinhong Zhou",
            "Hongxin Zhang",
            "Zhenfang Chen",
            "David D. Cox",
            "Yiming Yang",
            "Chuang Gan"
        ],
        "citations": 267,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Language Models can Solve Computer Tasks",
        "abstract": "Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.",
        "authors": [
            "Geunwoo Kim",
            "P. Baldi",
            "S. McAleer"
        ],
        "citations": 271,
        "references": 82,
        "year": 2023
    },
    {
        "title": "How is ChatGPT's behavior changing over time?",
        "abstract": "GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. We provide evidence that GPT-4's ability to follow user instructions has decreased over time, which is one common factor behind the many behavior drifts. Overall, our findings show that the behavior of the\"same\"LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.",
        "authors": [
            "Lingjiao Chen",
            "Matei Zaharia",
            "James Y. Zou"
        ],
        "citations": 327,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "abstract": "Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.",
        "authors": [
            "Asma Ghandeharioun",
            "Avi Caciularu",
            "Adam Pearce",
            "Lucas Dixon",
            "Mor Geva"
        ],
        "citations": 57,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
        "abstract": "Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.",
        "authors": [
            "Pan Lu",
            "Baolin Peng",
            "Hao Cheng",
            "Michel Galley",
            "Kai-Wei Chang",
            "Y. Wu",
            "Song-Chun Zhu",
            "Jianfeng Gao"
        ],
        "citations": 244,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
        "abstract": "Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language\"gradients\"that criticize the current prompt. The gradients are then\"propagated\"into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.",
        "authors": [
            "Reid Pryzant",
            "Dan Iter",
            "Jerry Li",
            "Y. Lee",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "citations": 220,
        "references": 44,
        "year": 2023
    },
    {
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "abstract": "People are spending an enormous amount of time on dig-ital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogA-gent supports input at a resolution of1120 × 1120, enabling it to recognize tiny page elements and text. As a general-ist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK- VQA, Text- Vqa, St- Vqa, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks-Mind2Web and AITW, ad-vancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM.",
        "authors": [
            "Wenyi Hong",
            "Weihan Wang",
            "Qingsong Lv",
            "Jiazheng Xu",
            "Wenmeng Yu",
            "Junhui Ji",
            "Yan Wang",
            "Zihan Wang",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "citations": 215,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
        "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.",
        "authors": [
            "Josef Dai",
            "Xuehai Pan",
            "Ruiyang Sun",
            "Jiaming Ji",
            "Xinbo Xu",
            "Mickel Liu",
            "Yizhou Wang",
            "Yaodong Yang"
        ],
        "citations": 207,
        "references": 100,
        "year": 2023
    },
    {
        "title": "The imperative for regulatory oversight of large language models (or generative AI) in healthcare",
        "abstract": null,
        "authors": [
            "B. Meskó",
            "E. Topol"
        ],
        "citations": 407,
        "references": 7,
        "year": 2023
    },
    {
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
        "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards\"self-improvement\"by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.",
        "authors": [
            "Harrison Lee",
            "Samrat Phatale",
            "Hassan Mansoor",
            "Kellie Lu",
            "Thomas Mesnard",
            "Colton Bishop",
            "Victor Carbune",
            "Abhinav Rastogi"
        ],
        "citations": 275,
        "references": 54,
        "year": 2023
    },
    {
        "title": "A Comprehensive Overview of Large Language Models",
        "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.",
        "authors": [
            "Humza Naveed",
            "Asad Ullah Khan",
            "Shi Qiu",
            "Muhammad Saqib",
            "Saeed Anwar",
            "Muhammad Usman",
            "Nick Barnes",
            "A. Mian"
        ],
        "citations": 338,
        "references": 527,
        "year": 2023
    },
    {
        "title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
        "abstract": "Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.",
        "authors": [
            "Janice Ahn",
            "Rishu Verma",
            "Renze Lou",
            "Di Liu",
            "Rui Zhang",
            "Wenpeng Yin"
        ],
        "citations": 64,
        "references": 94,
        "year": 2024
    },
    {
        "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
        "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.",
        "authors": [
            "Baolin Peng",
            "Michel Galley",
            "Pengcheng He",
            "Hao Cheng",
            "Yujia Xie",
            "Yu Hu",
            "Qiuyuan Huang",
            "Lars Lidén",
            "Zhou Yu",
            "Weizhu Chen",
            "Jianfeng Gao"
        ],
        "citations": 322,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
        "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
        "authors": [
            "Miles Turpin",
            "Julian Michael",
            "Ethan Perez",
            "Sam Bowman"
        ],
        "citations": 299,
        "references": 75,
        "year": 2023
    },
    {
        "title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
        "abstract": "Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.",
        "authors": [
            "Zeyu Zhang",
            "Xiaohe Bo",
            "Chen Ma",
            "Rui Li",
            "Xu Chen",
            "Quanyu Dai",
            "Jieming Zhu",
            "Zhenhua Dong",
            "Ji-Rong Wen"
        ],
        "citations": 59,
        "references": 170,
        "year": 2024
    },
    {
        "title": "Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation",
        "abstract": "Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar.\n To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.",
        "authors": [
            "Dawei Gao",
            "Haibin Wang",
            "Yaliang Li",
            "Xiuyu Sun",
            "Yichen Qian",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "citations": 150,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Evaluating the Performance of ChatGPT in Ophthalmology",
        "abstract": "We tested the accuracy of ChatGPT, a large language model (LLM), in the ophthalmology question-answering space using two popular multiple choice question banks used for the high-stakes Ophthalmic Knowledge Assessment Program (OKAP) exam. The testing sets were of easy-to-moderate difficulty and were diversified, including recall, interpretation, practical and clinical decision-making problems. ChatGPT achieved 55.8% and 42.7% accuracy in the two 260-question simulated exams. Its performance varied across subspecialties, with the best results in general medicine and the worst in neuro-ophthalmology and ophthalmic pathology and intraocular tumors. These results are encouraging but suggest that specialising LLMs through domain-specific pre-training may be necessary to improve their performance in ophthalmic subspecialties.",
        "authors": [
            "F. Antaki",
            "Samir Touma",
            "Daniel Milad",
            "J. El-Khoury",
            "R. Duval"
        ],
        "citations": 283,
        "references": 24,
        "year": 2023
    },
    {
        "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
        "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.",
        "authors": [
            "A. Deshpande",
            "Vishvak Murahari",
            "Tanmay Rajpurohit",
            "A. Kalyan",
            "Karthik Narasimhan"
        ],
        "citations": 284,
        "references": 52,
        "year": 2023
    },
    {
        "title": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
        "abstract": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under https://github.com/OpenSafetyLab/SALAD-BENCH.",
        "authors": [
            "Lijun Li",
            "Bowen Dong",
            "Ruohui Wang",
            "Xuhao Hu",
            "Wangmeng Zuo",
            "Dahua Lin",
            "Yu Qiao",
            "Jing Shao"
        ],
        "citations": 54,
        "references": 58,
        "year": 2024
    },
    {
        "title": "Generating Images with Multimodal Language Models",
        "abstract": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
        "authors": [
            "Jing Yu Koh",
            "Daniel Fried",
            "R. Salakhutdinov"
        ],
        "citations": 190,
        "references": 73,
        "year": 2023
    },
    {
        "title": "VILA: On Pre-training for Visual Language Models",
        "abstract": "Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge. VILA is also deployable on Jetson Orin for on-device VLM.",
        "authors": [
            "Ji Lin",
            "Hongxu Yin",
            "Wei Ping",
            "Yao Lu",
            "Pavlo Molchanov",
            "Andrew Tao",
            "Huizi Mao",
            "Jan Kautz",
            "Mohammad Shoeybi",
            "Song Han"
        ],
        "citations": 204,
        "references": 74,
        "year": 2023
    },
    {
        "title": "MegaScale: Scaling Large Language Model Training to More Than 10, 000 GPUs",
        "abstract": "We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",
        "authors": [
            "Ziheng Jiang",
            "Haibin Lin",
            "Yinmin Zhong",
            "Qi Huang",
            "Yangrui Chen",
            "Zhi Zhang",
            "Yanghua Peng",
            "Xiang Li",
            "Cong Xie",
            "Shibiao Nong",
            "Yulu Jia",
            "Sun He",
            "Hongmin Chen",
            "Zhihao Bai",
            "Qi Hou",
            "Shipeng Yan",
            "Ding Zhou",
            "Yiyao Sheng",
            "Zhuo Jiang",
            "Haohan Xu",
            "Haoran Wei",
            "Zhang Zhang",
            "Pengfei Nie",
            "Leqi Zou",
            "Sida Zhao",
            "Liang Xiang",
            "Zherui Liu",
            "Zhe Li",
            "X. Jia",
            "Jia-jun Ye",
            "Xin Jin",
            "Xin Liu"
        ],
        "citations": 54,
        "references": 69,
        "year": 2024
    },
    {
        "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
        "abstract": "As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their token probabilities, while simultaneously attenuating the probabilities of token sequences that are aligned with the objectives of jailbreak attacks. We perform extensive experiments on five LLMs using six state-of-the-art jailbreak attacks and four benchmark datasets. Our results show that SafeDecoding significantly reduces the attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries. SafeDecoding outperforms six defense methods.",
        "authors": [
            "Zhangchen Xu",
            "Fengqing Jiang",
            "Luyao Niu",
            "Jinyuan Jia",
            "Bill Yuchen Lin",
            "R. Poovendran"
        ],
        "citations": 54,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Humans or LLMs as the Judge? A Study on Judgement Bias",
        "abstract": "Adopting human and large language models (LLM) as judges (*a.k.a* human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating **Misinformation Oversight Bias**, **Gender Bias**, **Authority Bias** and **Beauty Bias** on LLM and human judges. We curate a dataset referring to the revised Bloom’s Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems.",
        "authors": [
            "Guiming Hardy Chen",
            "Shunian Chen",
            "Ziche Liu",
            "Feng Jiang",
            "Benyou Wang"
        ],
        "citations": 54,
        "references": 105,
        "year": 2024
    },
    {
        "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "abstract": "To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.",
        "authors": [
            "Fuzhao Xue",
            "Zian Andy Zheng",
            "Yao Fu",
            "Jinjie Ni",
            "Zangwei Zheng",
            "Wangchunshu Zhou",
            "Yang You"
        ],
        "citations": 53,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation",
        "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
        "authors": [
            "Lijun Yu",
            "José Lezama",
            "N. B. Gundavarapu",
            "Luca Versari",
            "Kihyuk Sohn",
            "David C. Minnen",
            "Yong Cheng",
            "Agrim Gupta",
            "Xiuye Gu",
            "Alexander G. Hauptmann",
            "Boqing Gong",
            "Ming-Hsuan Yang",
            "Irfan Essa",
            "David A. Ross",
            "Lu Jiang"
        ],
        "citations": 158,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
        "abstract": "Large Language Models (LLMs) have shown remarkable success in various tasks, yet their safety and the risk of generating harmful content remain pressing concerns. In this paper, we delve into the potential of In-Context Learning (ICL) to modulate the alignment of LLMs. Specifically, we propose the In-Context Attack (ICA) which employs harmful demonstrations to subvert LLMs, and the In-Context Defense (ICD) which bolsters model resilience through examples that demonstrate refusal to produce harmful responses. We offer theoretical insights to elucidate how a limited set of in-context demonstrations can pivotally influence the safety alignment of LLMs. Through extensive experiments, we demonstrate the efficacy of ICA and ICD in respectively elevating and mitigating the success rates of jailbreaking prompts. Our findings illuminate the profound influence of ICL on LLM behavior, opening new avenues for improving the safety of LLMs.",
        "authors": [
            "Zeming Wei",
            "Yifei Wang",
            "Yisen Wang"
        ],
        "citations": 176,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "abstract": "Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS• Software and its engineering → Empirical software validation.",
        "authors": [
            "Scott Barnett",
            "Stefanus Kurniawan",
            "Srikanth Thudumu",
            "Zach Brannelly",
            "Mohamed Abdelrazek"
        ],
        "citations": 53,
        "references": 21,
        "year": 2024
    },
    {
        "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
        "abstract": "Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.",
        "authors": [
            "Abhimanyu Hans",
            "Avi Schwarzschild",
            "Valeriia Cherepanova",
            "Hamid Kazemi",
            "Aniruddha Saha",
            "Micah Goldblum",
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "citations": 52,
        "references": 54,
        "year": 2024
    },
    {
        "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
        "abstract": "Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.",
        "authors": [
            "Liyan Tang",
            "Philippe Laban",
            "Greg Durrett"
        ],
        "citations": 48,
        "references": 76,
        "year": 2024
    },
    {
        "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
        "abstract": "Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs. Our code is available at https://github.com/uw-nsl/ArtPrompt.",
        "authors": [
            "Fengqing Jiang",
            "Zhangchen Xu",
            "Luyao Niu",
            "Zhen Xiang",
            "Bhaskar Ramasubramanian",
            "Bo Li",
            "R. Poovendran"
        ],
        "citations": 51,
        "references": 47,
        "year": 2024
    },
    {
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
        "abstract": "Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io",
        "authors": [
            "Ishika Singh",
            "Valts Blukis",
            "A. Mousavian",
            "Ankit Goyal",
            "Danfei Xu",
            "Jonathan Tremblay",
            "D. Fox",
            "Jesse Thomason",
            "Animesh Garg"
        ],
        "citations": 510,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Spatial-Temporal Large Language Model for Traffic Prediction",
        "abstract": "Traffic prediction, an essential component for intelligent transportation systems, endeavours to use historical data to foresee future traffic features at specific locations. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not improved. Recently, large language models have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pretraining while maintaining their fundamental structures. Motivated by these developments, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. In the ST-LLM, we define timesteps at each location as tokens and design a spatial-temporal embedding to learn the spatial location and global temporal patterns of these tokens. Additionally, we integrate these embeddings by a fusion convolution to each token for a unified spatial-temporal representation. Furthermore, we innovate a partially frozen attention strategy to adapt the LLM to capture global spatial-temporal dependencies for traffic prediction. Comprehensive experiments on real traffic datasets offer evidence that ST-LLM is a powerful spatial-temporal learner that outperforms state-of-the-art models. Notably, the ST-LLM also exhibits robust performance in both few-shot and zero-shot prediction scenarios. The code is publicly available at https://github.com/ChenxiLiu-HNU/ST-LLM.",
        "authors": [
            "Chenxi Liu",
            "Sun Yang",
            "Qianxiong Xu",
            "Zhishuai Li",
            "Cheng Long",
            "Ziyue Li",
            "Rui Zhao"
        ],
        "citations": 48,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Security and Privacy Challenges of Large Language Models: A Survey",
        "abstract": "Large language models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLMs have become very popular tools in natural language processing (NLP) tasks, with the capability to analyze complicated linguistic patterns and provide relevant responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and personally identifiable information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks against LLMs, and review potential defense mechanisms. Additionally, the survey outlines existing research gaps and highlights future research directions.",
        "authors": [
            "B. Das",
            "M. H. Amini",
            "Yanzhao Wu"
        ],
        "citations": 47,
        "references": 223,
        "year": 2024
    },
    {
        "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
        "abstract": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.",
        "authors": [
            "Weixin Liang",
            "Zachary Izzo",
            "Yaohui Zhang",
            "Haley Lepp",
            "Hancheng Cao",
            "Xuandong Zhao",
            "Lingjiao Chen",
            "Haotian Ye",
            "Sheng Liu",
            "Zhi Huang",
            "Daniel A. McFarland",
            "James Y. Zou"
        ],
        "citations": 50,
        "references": 86,
        "year": 2024
    },
    {
        "title": "Rethinking Machine Unlearning for Large Language Models",
        "abstract": "We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.",
        "authors": [
            "Sijia Liu",
            "Yuanshun Yao",
            "Jinghan Jia",
            "Stephen Casper",
            "Nathalie Baracaldo",
            "Peter Hase",
            "Xiaojun Xu",
            "Yuguang Yao",
            "Chris Liu",
            "Hang Li",
            "Kush R. Varshney",
            "Mohit Bansal",
            "Sanmi Koyejo",
            "Yang Liu"
        ],
        "citations": 49,
        "references": 151,
        "year": 2024
    },
    {
        "title": "Augmenting large language models with chemistry tools",
        "abstract": null,
        "authors": [
            "Andrés M Bran",
            "Sam Cox",
            "Oliver Schilter",
            "Carlo Baldassari",
            "Andrew D. White",
            "P. Schwaller"
        ],
        "citations": 245,
        "references": 114,
        "year": 2023
    },
    {
        "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
        "abstract": "While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art guardrails, e.g., LlamaGuard.",
        "authors": [
            "Anay Mehrotra",
            "Manolis Zampetakis",
            "Paul Kassianik",
            "Blaine Nelson",
            "Hyrum Anderson",
            "Yaron Singer",
            "Amin Karbasi"
        ],
        "citations": 140,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Instruction : Translate the phrase ” Bonne chance ” into English Response : Good Luck",
        "abstract": "Large language models (LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., ALPACA’s 52k data) surprisingly contain many lowquality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce ALPAGASUS1, which is finetuned on only 9k high-quality data filtered from the 52k ALPACA data. ALPAGASUS significantly outperforms the original ALPACA as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches > 90% performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for ALPACA) to 14 minutes 2. Moreover, the experiments prove the efficacy of our method across diverse datasets, base models, and LLM filters. Overall, ALPAGASUS demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models. Our project page is available at: https://lichang-chen.github.io/AlpaGasus/.",
        "authors": [
            "Lichang Chen",
            "SHIYANG LI",
            "Jun Yan",
            "Hai Wang",
            "Kalpa Gunaratna",
            "Vikas Yadav",
            "Zheng Tang",
            "Vijay Srinivasan",
            "Tianyi Zhou",
            "Heng Huang",
            "Hongxia Jin"
        ],
        "citations": 141,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Aligning Large Language Models with Human: A Survey",
        "abstract": "Large Language Models (LLMs) trained on extensive textual corpora have emerged as leading solutions for a broad array of Natural Language Processing (NLP) tasks. Despite their notable performance, these models are prone to certain limitations such as mis-understanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information. Hence, aligning LLMs with human expectations has become an active area of interest within the research community. This survey presents a comprehensive overview of these alignment technologies, including the following aspects. (1) Data collection : the methods for effectively collecting high-quality instructions for LLM alignment, including the use of NLP benchmarks, human annotations, and leveraging strong LLMs. (2) Training methodologies : a detailed review of the prevailing training methods employed for LLM alignment. Our exploration encompasses Supervised Fine-tuning, both Online and Ofﬂine human preference training, along with parameter-efﬁcient training mechanisms. (3) Model Evaluation : the methods for evaluating the effectiveness of these human-aligned LLMs, presenting a multifaceted approach towards their assessment. In conclusion, we collate and distill our ﬁndings, shedding light on several promising future research avenues in the ﬁeld. This survey, therefore, serves as a valuable resource for anyone invested in understanding and advancing the alignment of LLMs to better suit human-oriented tasks and expectations. An associated GitHub link collecting the latest papers is available at",
        "authors": [
            "Yufei Wang",
            "Wanjun Zhong",
            "Liangyou Li",
            "Fei Mi",
            "Xingshan Zeng",
            "Wenyong Huang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "citations": 245,
        "references": 113,
        "year": 2023
    },
    {
        "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
        "abstract": "We study whether Large Language Models (LLMs) latently perform multi-hop reasoning with complex prompts such as\"The mother of the singer of 'Superstition' is\". We look for evidence of a latent reasoning pathway where an LLM (1) latently identifies\"the singer of 'Superstition'\"as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to complete the prompt. We analyze these two hops individually and consider their co-occurrence as indicative of latent multi-hop reasoning. For the first hop, we test if changing the prompt to indirectly mention the bridge entity instead of any other entity increases the LLM's internal recall of the bridge entity. For the second hop, we test if increasing this recall causes the LLM to better utilize what it knows about the bridge entity. We find strong evidence of latent multi-hop reasoning for the prompts of certain relation types, with the reasoning pathway used in more than 80% of the prompts. However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop. Our experimental findings suggest potential challenges and opportunities for future development and applications of LLMs.",
        "authors": [
            "Sohee Yang",
            "E. Gribovskaya",
            "Nora Kassner",
            "Mor Geva",
            "Sebastian Riedel"
        ],
        "citations": 50,
        "references": 55,
        "year": 2024
    },
    {
        "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
        "abstract": "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM.",
        "authors": [
            "Liangming Pan",
            "Alon Albalak",
            "Xinyi Wang",
            "William Yang Wang"
        ],
        "citations": 177,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Large Language Models for Software Engineering: A Systematic Literature Review",
        "abstract": "\n Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at\n https://github.com/xinyi-hou/LLM4SE_SLR\n .\n",
        "authors": [
            "Xinying Hou",
            "Yanjie Zhao",
            "Yue Liu",
            "Zhou Yang",
            "Kailong Wang",
            "Li Li",
            "Xiapu Luo",
            "David Lo",
            "John C. Grundy",
            "Haoyu Wang"
        ],
        "citations": 221,
        "references": 405,
        "year": 2023
    },
    {
        "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
        "abstract": "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for ChatGPT to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.",
        "authors": [
            "Viet Dac Lai",
            "Nghia Trung Ngo",
            "Amir Pouran Ben Veyseh",
            "Hieu Man",
            "Franck Dernoncourt",
            "Trung Bui",
            "Thien Huu Nguyen"
        ],
        "citations": 226,
        "references": 116,
        "year": 2023
    },
    {
        "title": "Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment",
        "abstract": "Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.",
        "authors": [
            "Yang Liu",
            "Yuanshun Yao",
            "Jean-François Ton",
            "Xiaoying Zhang",
            "Ruocheng Guo",
            "Hao Cheng",
            "Yegor Klochkov",
            "Muhammad Faaiz Taufiq",
            "Hanguang Li"
        ],
        "citations": 232,
        "references": 0,
        "year": 2023
    },
    {
        "title": "A Survey on Knowledge Distillation of Large Language Models",
        "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of KD's role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement. Our survey is meticulously structured around three foundational pillars: \\textit{algorithm}, \\textit{skill}, and \\textit{verticalization} -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in KD and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.",
        "authors": [
            "Xiaohan Xu",
            "Ming Li",
            "Chongyang Tao",
            "Tao Shen",
            "Reynold Cheng",
            "Jinyang Li",
            "Can Xu",
            "Dacheng Tao",
            "Tianyi Zhou"
        ],
        "citations": 50,
        "references": 388,
        "year": 2024
    },
    {
        "title": "Large Language Models Can Learn Temporal Reasoning",
        "abstract": "While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation.",
        "authors": [
            "Siheng Xiong",
            "Ali Payani",
            "R. Kompella",
            "F. Fekri"
        ],
        "citations": 50,
        "references": 92,
        "year": 2024
    },
    {
        "title": "A Survey on Efficient Inference for Large Language Models",
        "abstract": "Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.",
        "authors": [
            "Zixuan Zhou",
            "Xuefei Ning",
            "Ke Hong",
            "Tianyu Fu",
            "Jiaming Xu",
            "Shiyao Li",
            "Yuming Lou",
            "Luning Wang",
            "Zhihang Yuan",
            "Xiuhong Li",
            "Shengen Yan",
            "Guohao Dai",
            "Xiao-Ping Zhang",
            "Yuhan Dong",
            "Yu Wang"
        ],
        "citations": 48,
        "references": 304,
        "year": 2024
    },
    {
        "title": "Reinforced Self-Training (ReST) for Language Modeling",
        "abstract": "Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.",
        "authors": [
            "Caglar Gulcehre",
            "T. Paine",
            "S. Srinivasan",
            "Ksenia Konyushkova",
            "L. Weerts",
            "Abhishek Sharma",
            "Aditya Siddhant",
            "Alexa Ahern",
            "Miaosen Wang",
            "Chenjie Gu",
            "Wolfgang Macherey",
            "A. Doucet",
            "Orhan Firat",
            "Nando de Freitas"
        ],
        "citations": 217,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Language to Rewards for Robotic Skill Synthesis",
        "abstract": "Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate interface generated by LLMs, we can effectively bridge the gap between high-level language instructions or corrections to low-level robot actions. Meanwhile, combining this with a real-time optimizer, MuJoCo MPC, empowers an interactive behavior creation experience where users can immediately observe the results and provide feedback to the system. To systematically evaluate the performance of our proposed method, we designed a total of 17 tasks for a simulated quadruped robot and a dexterous manipulator robot. We demonstrate that our proposed method reliably tackles 90% of the designed tasks, while a baseline using primitive skills as the interface with Code-as-policies achieves 50% of the tasks. We further validated our method on a real robot arm where complex manipulation skills such as non-prehensile pushing emerge through our interactive system.",
        "authors": [
            "Wenhao Yu",
            "Nimrod Gileadi",
            "Chuyuan Fu",
            "Sean Kirmani",
            "Kuang-Huei Lee",
            "Montse Gonzalez Arenas",
            "H. Chiang",
            "Tom Erez",
            "Leonard Hasenclever",
            "Jan Humplik",
            "Brian Ichter",
            "Ted Xiao",
            "Peng Xu",
            "Andy Zeng",
            "Tingnan Zhang",
            "N. Heess",
            "Dorsa Sadigh",
            "Jie Tan",
            "Yuval Tassa",
            "F. Xia"
        ],
        "citations": 215,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
        "abstract": "Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment&MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at https://kaistai.github.io/prometheus/.",
        "authors": [
            "Seungone Kim",
            "Jamin Shin",
            "Yejin Cho",
            "Joel Jang",
            "S. Longpre",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Seongjin Shin",
            "Sungdong Kim",
            "James Thorne",
            "Minjoon Seo"
        ],
        "citations": 138,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "authors": [
            "Melanie Sclar",
            "Yejin Choi",
            "Yulia Tsvetkov",
            "Alane Suhr"
        ],
        "citations": 224,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models",
        "abstract": "This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innova-tions such as large-scale pre-training that captures knowledge across the entire world wide web, instruction ﬁne-tuning and Reinforcement Learning from Human Feedback (RLHF) have played signiﬁcant roles in enhancing LLMs’ adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation",
        "authors": [
            "Yi-Hsueh Liu",
            "Tianle Han",
            "Siyuan Ma",
            "Jiayue Zhang",
            "Yuanyuan Yang",
            "Jiaming Tian",
            "Haoyang He",
            "Antong Li",
            "Mengshen He",
            "Zheng Liu",
            "Zihao Wu",
            "Dajiang Zhu",
            "Xiang Li",
            "Ning Qiang",
            "Dinggang Shen",
            "Tianming Liu",
            "Bao Ge"
        ],
        "citations": 216,
        "references": 107,
        "year": 2023
    },
    {
        "title": "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks",
        "abstract": "Recent advances in instruction-following large language models (LLMs) have led to dramatic improvements in a range of NLP tasks. Unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models. Dual-use is difficult to prevent as instruction-following capabilities now enable standard attacks from computer security. The capabilities of these instruction-following LLMs provide strong economic incentives for dual-use by malicious actors. In particular, we show that instruction-following LLMs can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors. Our analysis shows that this content can be generated economically and at cost of $125-500 \\times$ cheaper than human effort alone. Together, our findings suggest that LLMs will increasingly attract more sophisticated adversaries and attacks, and addressing these attacks may require new approaches to mitigations.",
        "authors": [
            "Daniel Kang",
            "Xuechen Li",
            "Ion Stoica",
            "Carlos Guestrin",
            "M. Zaharia",
            "Tatsunori Hashimoto"
        ],
        "citations": 198,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Self-collaboration Code Generation via ChatGPT",
        "abstract": "Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLM agents act as distinct ‘experts’, each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9%-47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Zhi Jin",
            "Ge Li"
        ],
        "citations": 191,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Preference Ranking Optimization for Human Alignment",
        "abstract": "Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise contrast, thus lacking contrasts from a macro perspective. In this paper, we propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast to accommodate preference rankings of any length. By iteratively contrasting candidates, PRO instructs the LLM to prioritize the best response while progressively ranking the rest responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through automatic-based, reward-based, GPT-4, and human evaluations.",
        "authors": [
            "Feifan Song",
            "Yu Bowen",
            "Minghao Li",
            "Haiyang Yu",
            "Fei Huang",
            "Yongbin Li",
            "Houfeng Wang"
        ],
        "citations": 197,
        "references": 52,
        "year": 2023
    },
    {
        "title": "A Survey on Large Language Models for Recommendation",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, https://github.com/WLiK/LLM4Rec.",
        "authors": [
            "Likang Wu",
            "Zhilan Zheng",
            "Zhaopeng Qiu",
            "Hao Wang",
            "Hongchao Gu",
            "Tingjia Shen",
            "Chuan Qin",
            "Chen Zhu",
            "Hengshu Zhu",
            "Qi Liu",
            "Hui Xiong",
            "Enhong Chen"
        ],
        "citations": 232,
        "references": 139,
        "year": 2023
    },
    {
        "title": "Large language models in education: A focus on the complementary relationship between human teachers and ChatGPT",
        "abstract": null,
        "authors": [
            "Jae-Bong Jeon",
            "Seongyong Lee"
        ],
        "citations": 228,
        "references": 44,
        "year": 2023
    },
    {
        "title": "AgentBench: Evaluating LLMs as Agents",
        "abstract": "Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released at \\url{https://github.com/THUDM/AgentBench}.",
        "authors": [
            "Xiao Liu",
            "Hao Yu",
            "Hanchen Zhang",
            "Yifan Xu",
            "Xuanyu Lei",
            "Hanyu Lai",
            "Yu Gu",
            "Yuxian Gu",
            "Hangliang Ding",
            "Kai Men",
            "Kejuan Yang",
            "Shudan Zhang",
            "Xiang Deng",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chenhui Zhang",
            "Shengqi Shen",
            "Tianjun Zhang",
            "Sheng Shen",
            "Yu Su",
            "Huan Sun",
            "Minlie Huang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "citations": 195,
        "references": 172,
        "year": 2023
    },
    {
        "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting",
        "abstract": "Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.",
        "authors": [
            "Zhen Qin",
            "R. Jagerman",
            "Kai Hui",
            "Honglei Zhuang",
            "Junru Wu",
            "Jiaming Shen",
            "Tianqi Liu",
            "Jialu Liu",
            "Donald Metzler",
            "Xuanhui Wang",
            "Michael Bendersky"
        ],
        "citations": 164,
        "references": 56,
        "year": 2023
    },
    {
        "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution",
        "abstract": "The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.",
        "authors": [
            "Ansong Ni",
            "Srini Iyer",
            "Dragomir R. Radev",
            "Ves Stoyanov",
            "Wen-tau Yih",
            "Sida I. Wang",
            "Xi Victoria Lin"
        ],
        "citations": 172,
        "references": 76,
        "year": 2023
    },
    {
        "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
        "abstract": "The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.",
        "authors": [
            "Xinyue Shen",
            "Z. Chen",
            "M. Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "citations": 166,
        "references": 96,
        "year": 2023
    },
    {
        "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
        "abstract": "Despite efforts to align large language models (LLMs) with human intentions, widely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. Across a range of popular LLMs, SmoothLLM sets the state-of-the-art for robustness against the GCG, PAIR, RandomSearch, and AmpleGCG jailbreaks. SmoothLLM is also resistant against adaptive GCG attacks, exhibits a small, though non-negligible trade-off between robustness and nominal performance, and is compatible with any LLM. Our code is publicly available at \\url{https://github.com/arobey1/smooth-llm}.",
        "authors": [
            "Alexander Robey",
            "Eric Wong",
            "Hamed Hassani",
            "George J. Pappas"
        ],
        "citations": 171,
        "references": 114,
        "year": 2023
    },
    {
        "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
        "abstract": "Visual instruction tuning large language model(LLM) on image-text pairs has achieved general-purpose vision-language abilities. However, the lack of region-text pairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin (the second place is 75.6%) and almost reaching human-level performance of 85.0%. The code, dataset, and demo can be found at https://github.com/jshilong/GPT4RoI.",
        "authors": [
            "Shilong Zhang",
            "Pei Sun",
            "Shoufa Chen",
            "Min Xiao",
            "Wenqi Shao",
            "Wenwei Zhang",
            "Kai Chen",
            "Ping Luo"
        ],
        "citations": 193,
        "references": 118,
        "year": 2023
    },
    {
        "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
        "abstract": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving a satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs' knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.",
        "authors": [
            "Yun Luo",
            "Zhen Yang",
            "Fandong Meng",
            "Yafu Li",
            "Jie Zhou",
            "Yue Zhang"
        ],
        "citations": 208,
        "references": 40,
        "year": 2023
    },
    {
        "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models",
        "abstract": "Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.",
        "authors": [
            "Kaiyu Yang",
            "Aidan M. Swope",
            "Alex Gu",
            "Rahul Chalamala",
            "Peiyang Song",
            "Shixing Yu",
            "Saad Godil",
            "R. Prenger",
            "Anima Anandkumar"
        ],
        "citations": 144,
        "references": 106,
        "year": 2023
    },
    {
        "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners",
        "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io",
        "authors": [
            "Allen Z. Ren",
            "Anushri Dixit",
            "Alexandra Bodrova",
            "Sumeet Singh",
            "Stephen Tu",
            "Noah Brown",
            "Peng Xu",
            "L. Takayama",
            "F. Xia",
            "Jacob Varley",
            "Zhenjia Xu",
            "Dorsa Sadigh",
            "Andy Zeng",
            "Anirudha Majumdar"
        ],
        "citations": 175,
        "references": 67,
        "year": 2023
    },
    {
        "title": "SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning",
        "abstract": "Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic search' for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an 'iterative replanning' pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.",
        "authors": [
            "Krishan Rana",
            "Jesse Haviland",
            "Sourav Garg",
            "Jad Abou-Chakra",
            "I. Reid",
            "Niko Sünderhauf"
        ],
        "citations": 165,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Mixture-of-Agents Enhances Large Language Model Capabilities",
        "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.",
        "authors": [
            "Junlin Wang",
            "Jue Wang",
            "Ben Athiwaratkun",
            "Ce Zhang",
            "James Zou"
        ],
        "citations": 43,
        "references": 42,
        "year": 2024
    },
    {
        "title": "ChatGPT and large language models in academia: opportunities and challenges",
        "abstract": null,
        "authors": [
            "Jesse G. Meyer",
            "R. Urbanowicz",
            "Patrick Martin",
            "K. O’Connor",
            "Ruowang Li",
            "Pei-Chen Peng",
            "T. Bright",
            "N. P. Tatonetti",
            "Kyoung-Jae Won",
            "G. Gonzalez-Hernandez",
            "Jason H. Moore"
        ],
        "citations": 222,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Mathematical discoveries from program search with large language models",
        "abstract": null,
        "authors": [
            "Bernardino Romera-Paredes",
            "M. Barekatain",
            "Alexander Novikov",
            "Matej Balog",
            "M. P. Kumar",
            "Emilien Dupont",
            "Francisco J. R. Ruiz",
            "J. Ellenberg",
            "Pengming Wang",
            "Omar Fawzi",
            "Pushmeet Kohli",
            "Alhussein Fawzi",
            "Josh Grochow",
            "Andrea Lodi",
            "Jean-Baptiste Mouret",
            "Talia Ringer",
            "Tao Yu"
        ],
        "citations": 201,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Exploring AI Ethics of ChatGPT: A Diagnostic Analysis",
        "abstract": "—Recent breakthroughs in natural language pro- cessing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language-model (LLM) has signiﬁcantly impacted businesses such as report summarization softwares and copywriters. Ob-servations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale bench- marks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical diﬃculties in advanced LLMs, there is no systematic examination and user study of the ethics of current LLMs use. To further educate future eﬀorts on constructing ethical LLMs responsibly, we perform a qualitative research method on OpenAI’s ChatGPT 1 to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) Bias 2) Reliability 3) Robustness 4) Toxicity. In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We ﬁnd that a signiﬁcant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our ﬁndings on the AI ethics of ChatGPT, as well as future problems and practical design considerations for LLMs. We believe that our ﬁndings may give light on future eﬀorts to determine and mitigate the ethical hazards posed by machines in LLM applications.",
        "authors": [
            "Terry Yue Zhuo",
            "Yujin Huang",
            "Chunyang Chen",
            "Zhenchang Xing"
        ],
        "citations": 210,
        "references": 98,
        "year": 2023
    },
    {
        "title": "Performance of ChatGPT, GPT-4, and Google Bard on a Neurosurgery Oral Boards Preparation Question Bank",
        "abstract": "BACKGROUND AND OBJECTIVES: General large language models (LLMs), such as ChatGPT (GPT-3.5), have demonstrated the capability to pass multiple-choice medical board examinations. However, comparative accuracy of different LLMs and LLM performance on assessments of predominantly higher-order management questions is poorly understood. We aimed to assess the performance of 3 LLMs (GPT-3.5, GPT-4, and Google Bard) on a question bank designed specifically for neurosurgery oral boards examination preparation. METHODS: The 149-question Self-Assessment Neurosurgery Examination Indications Examination was used to query LLM accuracy. Questions were inputted in a single best answer, multiple-choice format. χ2, Fisher exact, and univariable logistic regression tests assessed differences in performance by question characteristics. RESULTS: On a question bank with predominantly higher-order questions (85.2%), ChatGPT (GPT-3.5) and GPT-4 answered 62.4% (95% CI: 54.1%-70.1%) and 82.6% (95% CI: 75.2%-88.1%) of questions correctly, respectively. By contrast, Bard scored 44.2% (66/149, 95% CI: 36.2%-52.6%). GPT-3.5 and GPT-4 demonstrated significantly higher scores than Bard (both P < .01), and GPT-4 outperformed GPT-3.5 (P = .023). Among 6 subspecialties, GPT-4 had significantly higher accuracy in the Spine category relative to GPT-3.5 and in 4 categories relative to Bard (all P < .01). Incorporation of higher-order problem solving was associated with lower question accuracy for GPT-3.5 (odds ratio [OR] = 0.80, P = .042) and Bard (OR = 0.76, P = .014), but not GPT-4 (OR = 0.86, P = .085). GPT-4's performance on imaging-related questions surpassed GPT-3.5's (68.6% vs 47.1%, P = .044) and was comparable with Bard's (68.6% vs 66.7%, P = 1.000). However, GPT-4 demonstrated significantly lower rates of “hallucination” on imaging-related questions than both GPT-3.5 (2.3% vs 57.1%, P < .001) and Bard (2.3% vs 27.3%, P = .002). Lack of question text description for questions predicted significantly higher odds of hallucination for GPT-3.5 (OR = 1.45, P = .012) and Bard (OR = 2.09, P < .001). CONCLUSION: On a question bank of predominantly higher-order management case scenarios for neurosurgery oral boards preparation, GPT-4 achieved a score of 82.6%, outperforming ChatGPT and Google Bard.",
        "authors": [
            "R. Ali",
            "O. Tang",
            "I. Connolly",
            "J. Fridley",
            "John H. Shin",
            "P. Z. Zadnik Sullivan",
            "D. Cielo",
            "A. Oyelese",
            "C. Doberstein",
            "A. Telfeian",
            "Z. Gokaslan",
            "Wael Asaad"
        ],
        "citations": 182,
        "references": 10,
        "year": 2023
    },
    {
        "title": "Large Language Models as Tool Makers",
        "abstract": "Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set of tasks. 2) tool using: another LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. On the problem-solving server side, tool-making enables continual tool generation and caching as new requests emerge. This framework enables subsequent requests to access cached tools via their corresponding APIs, enhancing the efficiency of task resolution. Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. Furthermore, our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates performance equivalent to using GPT-4 for both roles, but with a significantly reduced inference cost.",
        "authors": [
            "Tianle Cai",
            "Xuezhi Wang",
            "Tengyu Ma",
            "Xinyun Chen",
            "Denny Zhou"
        ],
        "citations": 158,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Large Language Models Can Self-Improve",
        "abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate\"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",
        "authors": [
            "Jiaxin Huang",
            "S. Gu",
            "Le Hou",
            "Yuexin Wu",
            "Xuezhi Wang",
            "Hongkun Yu",
            "Jiawei Han"
        ],
        "citations": 457,
        "references": 65,
        "year": 2022
    },
    {
        "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
        "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.",
        "authors": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "S. Sreedharan",
            "Subbarao Kambhampati"
        ],
        "citations": 174,
        "references": 40,
        "year": 2023
    },
    {
        "title": "A study of generative large language model for medical research and healthcare",
        "abstract": null,
        "authors": [
            "C.A.I. Peng",
            "Xi Yang",
            "Aokun Chen",
            "Kaleb E. Smith",
            "Nima M. Pournejatian",
            "Anthony B Costa",
            "Cheryl Martin",
            "Mona G. Flores",
            "Ying Zhang",
            "Tanja Magoc",
            "Gloria P. Lipori",
            "Duane A. Mitchell",
            "N. Ospina",
            "M. M. Ahmed",
            "W. Hogan",
            "E. Shenkman",
            "Yi Guo",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "citations": 172,
        "references": 61,
        "year": 2023
    },
    {
        "title": "The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development",
        "abstract": "Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.",
        "authors": [
            "Steven I. Ross",
            "Fernando Martinez",
            "Stephanie Houde",
            "Michael J. Muller",
            "Justin D. Weisz"
        ],
        "citations": 171,
        "references": 117,
        "year": 2023
    },
    {
        "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",
        "authors": [
            "Liangming Pan",
            "Michael Stephen Saxon",
            "Wenda Xu",
            "Deepak Nathani",
            "Xinyi Wang",
            "William Yang Wang"
        ],
        "citations": 172,
        "references": 158,
        "year": 2023
    },
    {
        "title": "A Survey on Multimodal Large Language Models for Autonomous Driving",
        "abstract": "With the emergence of Large Language Models (LLMs) and Vision Foundation Models (VFMs), multimodal AI systems benefiting from large models have the potential to equally perceive the real world, make decisions, and control tools as humans. In recent months, LLMs have shown widespread attention in autonomous driving and map systems. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors to apply in LLM driving systems. In this paper, we present a systematic investigation in this field. We first introduce the background of Multimodal Large Language Models (MLLMs), the multimodal models development using LLMs, and the history of autonomous driving. Then, we overview existing MLLM tools for driving, transportation, and map systems together with existing datasets and benchmarks. Moreover, we summarized the works in The 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD), which is the first workshop of its kind regarding LLMs in autonomous driving. To further promote the development of this field, we also discuss several important problems regarding using MLLMs in autonomous driving systems that need to be solved by both academia and industry.",
        "authors": [
            "Can Cui",
            "Yunsheng Ma",
            "Xu Cao",
            "Wenqian Ye",
            "Yang Zhou",
            "Kaizhao Liang",
            "Jintai Chen",
            "Juanwu Lu",
            "Zichong Yang",
            "Kuei-Da Liao",
            "Tianren Gao",
            "Erlong Li",
            "Kun Tang",
            "Zhipeng Cao",
            "Tongxi Zhou",
            "Ao Liu",
            "Xinrui Yan",
            "Shuqi Mei",
            "Jianguo Cao",
            "Ziran Wang",
            "Chao Zheng"
        ],
        "citations": 163,
        "references": 209,
        "year": 2023
    },
    {
        "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees",
        "abstract": "This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/Cornell-RelaxML/QuIP.",
        "authors": [
            "Jerry Chee",
            "Yaohui Cai",
            "Volodymyr Kuleshov",
            "Chris De Sa"
        ],
        "citations": 131,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",
        "abstract": "Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them. Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities. This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.",
        "authors": [
            "Yuheng Cheng",
            "Ceyao Zhang",
            "Zhengwen Zhang",
            "Xiangrui Meng",
            "Sirui Hong",
            "Wenhao Li",
            "Zihao Wang",
            "Zekai Wang",
            "Feng Yin",
            "Junhua Zhao",
            "Xiuqiang He"
        ],
        "citations": 46,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT",
        "abstract": "Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then queries the LLM to generate patches. While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches. To address these limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same mistakes. For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM -- ChatGPT. By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs for \\$0.42 each!",
        "authors": [
            "Chun Xia",
            "Lingming Zhang"
        ],
        "citations": 143,
        "references": 76,
        "year": 2023
    },
    {
        "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
        "abstract": "There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost. The ideas and findings presented here lay a foundation for using LLMs sustainably and efficiently.",
        "authors": [
            "Lingjiao Chen",
            "M. Zaharia",
            "James Y. Zou"
        ],
        "citations": 152,
        "references": 42,
        "year": 2023
    },
    {
        "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
        "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.",
        "authors": [
            "Tu Vu",
            "Mohit Iyyer",
            "Xuezhi Wang",
            "Noah Constant",
            "Jerry Wei",
            "Jason Wei",
            "Chris Tar",
            "Yun-Hsuan Sung",
            "Denny Zhou",
            "Quoc Le",
            "Thang Luong"
        ],
        "citations": 136,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
        "abstract": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu.",
        "authors": [
            "Zichang Liu",
            "Jue Wang",
            "Tri Dao",
            "Tianyi Zhou",
            "Binhang Yuan",
            "Zhao Song",
            "Anshumali Shrivastava",
            "Ce Zhang",
            "Yuandong Tian",
            "Christopher Ré",
            "Beidi Chen"
        ],
        "citations": 149,
        "references": 143,
        "year": 2023
    },
    {
        "title": "Is ChatGPT the Ultimate Programming Assistant - How far is it?",
        "abstract": "Recently, the ChatGPT LLM has received great attention: it can be used as a bot for discussing source code, prompting it to suggest changes, provide descriptions or even generate code. Typical demonstrations generally focus on existing benchmarks, which may have been used in model training (i.e., data leakage). To assess the feasibility of using an LLM as a useful assistant bot for programmers, we must assess its realistic capabilities on unseen problems as well as its capabilities on various tasks. In this paper, we present an empirical study of ChatGPT's potential as a fully automated programming assistant, focusing on the tasks of code generation, program repair, and code summariziation. The study investigates ChatGPT's performance on common programming problems and compares it with state-of-the-art approaches on two benchmarks. Among several findings, our study shows that ChatGPT is effective in dealing with common programming problems. However, our experiments also reveal limitations in terms of its attention span: detailed descriptions will constrain the focus of ChatGPT and prevent it from leveraging its vast knowledge to solve the actual problem. Surprisingly, we have identified the ability of ChatGPT to reason the original intention of the code. We expect future work to build on this insight for dealing with the open question of the oracle problem. Our findings contribute interesting insights to the development of LLMs for programming assistance, notably by demonstrating the importance of prompt engineering, and providing a better understanding of ChatGPT's practical applications for software engineering.",
        "authors": [
            "Haoye Tian",
            "Weiqi Lu",
            "T. Li",
            "Xunzhu Tang",
            "S. Cheung",
            "Jacques Klein",
            "Tegawend'e F. Bissyand'e"
        ],
        "citations": 155,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models",
        "abstract": "Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.",
        "authors": [
            "Esin Durmus",
            "Karina Nyugen",
            "Thomas Liao",
            "Nicholas Schiefer",
            "Amanda Askell",
            "A. Bakhtin",
            "Carol Chen",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Nicholas Joseph",
            "Liane Lovitt",
            "Sam McCandlish",
            "Orowa Sikder",
            "Alex Tamkin",
            "Janel Thamkul",
            "Jared Kaplan",
            "Jack Clark",
            "Deep Ganguli"
        ],
        "citations": 156,
        "references": 90,
        "year": 2023
    },
    {
        "title": "GPT-Driver: Learning to Drive with GPT",
        "abstract": "We present a simple yet effective approach that can transform the OpenAI GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion planning is a core challenge in autonomous driving, aiming to plan a driving trajectory that is safe and comfortable. Existing motion planners predominantly leverage heuristic methods to forecast driving trajectories, yet these approaches demonstrate insufficient generalization capabilities in the face of novel and unseen driving scenarios. In this paper, we propose a novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs). The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem, a perspective not previously explored. Specifically, we represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coordinate positions. Furthermore, we propose a novel prompting-reasoning-finetuning strategy to stimulate the numerical reasoning potential of the LLM. With this strategy, the LLM can describe highly precise trajectory coordinates and also its internal decision-making process in natural language. We evaluate our approach on the large-scale nuScenes dataset, and extensive experiments substantiate the effectiveness, generalization ability, and interpretability of our GPT-based motion planner. Code is now available at https://github.com/PointsCoder/GPT-Driver.",
        "authors": [
            "Jiageng Mao",
            "Yuxi Qian",
            "Hang Zhao",
            "Yue Wang"
        ],
        "citations": 152,
        "references": 43,
        "year": 2023
    },
    {
        "title": "GPT-NER: Named Entity Recognition via Large Language Models",
        "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
        "authors": [
            "Shuhe Wang",
            "Xiaofei Sun",
            "Xiaoya Li",
            "Rongbin Ouyang",
            "Fei Wu",
            "Tianwei Zhang",
            "Jiwei Li",
            "Guoyin Wang"
        ],
        "citations": 140,
        "references": 57,
        "year": 2023
    },
    {
        "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
        "abstract": "We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/",
        "authors": [
            "D. Kondratyuk",
            "Lijun Yu",
            "Xiuye Gu",
            "José Lezama",
            "Jonathan Huang",
            "Rachel Hornung",
            "Hartwig Adam",
            "Hassan Akbari",
            "Y. Alon",
            "Vighnesh Birodkar",
            "Yong Cheng",
            "Ming-Chang Chiu",
            "Josh Dillon",
            "Irfan Essa",
            "Agrim Gupta",
            "Meera Hahn",
            "Anja Hauth",
            "David Hendon",
            "Alonso Martinez",
            "David C. Minnen",
            "David A. Ross",
            "Grant Schindler",
            "Mikhail Sirotenko",
            "Kihyuk Sohn",
            "Krishna Somandepalli",
            "Huisheng Wang",
            "Jimmy Yan",
            "Ming Yang",
            "Xuan Yang",
            "Bryan Seybold",
            "Lu Jiang"
        ],
        "citations": 153,
        "references": 97,
        "year": 2023
    },
    {
        "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
        "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\\underline{tool}$ as a to$\\underline{ken}$ ($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.",
        "authors": [
            "Shibo Hao",
            "Tianyang Liu",
            "Zhen Wang",
            "Zhiting Hu"
        ],
        "citations": 143,
        "references": 87,
        "year": 2023
    },
    {
        "title": "DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task",
        "abstract": "The recent progress of large language models (LLMs), including ChatGPT and GPT-4, in comprehending and responding to human instructions has been remarkable. Nevertheless, these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs. To tackle these challenges, we have collected databases of medical dialogues in Chinese with ChatGPT's help and adopted several techniques to train an easy-deploy LLM. Remarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13 hours, which means having a healthcare-purpose LLM can be very affordable. DoctorGLM is currently an early-stage engineering attempt and contain various mistakes. We are sharing it with the broader community to invite feedback and suggestions to improve its healthcare-focused capabilities: https://github.com/xionghonglin/DoctorGLM.",
        "authors": [
            "Honglin Xiong",
            "Sheng Wang",
            "Yitao Zhu",
            "Zihao Zhao",
            "Yuxiao Liu",
            "Linlin Huang",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "citations": 139,
        "references": 16,
        "year": 2023
    },
    {
        "title": "Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study",
        "abstract": "Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI’s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.",
        "authors": [
            "Perttu Hämäläinen",
            "Mikke Tavast",
            "Anton Kunnari"
        ],
        "citations": 144,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Evaluating Large Language Models: A Comprehensive Survey",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs. This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability. We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.",
        "authors": [
            "Zishan Guo",
            "Renren Jin",
            "Chuang Liu",
            "Yufei Huang",
            "Dan Shi",
            "Supryadi",
            "Linhao Yu",
            "Yan Liu",
            "Jiaxuan Li",
            "Bojian Xiong",
            "Deyi Xiong"
        ],
        "citations": 136,
        "references": 0,
        "year": 2023
    },
    {
        "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation",
        "abstract": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in <sc>TestPilot</sc>, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate <sc>TestPilot</sc> using OpenAI's <italic>gpt3.5-turbo</italic> LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3% statement coverage and 25.6% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8% of <sc>TestPilot</sc>'s generated tests have <inline-formula><tex-math notation=\"LaTeX\">$\\leq$</tex-math><alternatives><mml:math display=\"inline\"><mml:mo>≤</mml:mo></mml:math><inline-graphic xlink:href=\"schaefer-ieq1-3334955.gif\"/></alternatives></inline-formula> 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run <sc>TestPilot</sc> with two additional LLMs, OpenAI's older <italic>code-cushman-002</italic> LLM and <italic>StarCoder</italic>, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2% median statement coverage), and somewhat worse results with the latter (54.0% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.",
        "authors": [
            "Max Schäfer",
            "Sarah Nadi",
            "A. Eghbali",
            "F. Tip"
        ],
        "citations": 136,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models",
        "abstract": "We document the capability of large language models (LLMs) like ChatGPT to predict stock price movements using news headlines, even without direct financial training. ChatGPT scores significantly predict out-of-sample daily stock returns, subsuming traditional methods, and predictability is stronger among smaller stocks and following negative news. To explain these findings, we develop a theoretical model incorporating information capacity constraints, underreaction, limits-to-arbitrage, and LLMs. The model generates several key predictions, which we empirically test: (i) it establishes a critical threshold in AI capabilities necessary for profitable predictions, (ii) it demonstrates that only advanced LLMs can effectively interpret complex information, and (iii) it predicts that widespread LLM adoption can enhance market efficiency. Our results suggest that sophisticated return forecasting is an emerging capability of AI systems and that these technologies can alter information diffusion and decision-making processes in financial markets. Finally, we introduce an interpretability framework to evaluate LLMs' reasoning, contributing to AI transparency and economic decision-making.",
        "authors": [
            "Alejandro Lopez-Lira",
            "Yuehua Tang"
        ],
        "citations": 139,
        "references": 53,
        "year": 2023
    },
    {
        "title": "More human than human: measuring ChatGPT political bias",
        "abstract": "We investigate the political bias of a large language model (LLM), ChatGPT, which has become popular for retrieving factual information and generating content. Although ChatGPT assures that it is impartial, the literature suggests that LLMs exhibit bias involving race, gender, religion, and political orientation. Political bias in LLMs can have adverse political and electoral consequences similar to bias from traditional and social media. Moreover, political bias can be harder to detect and eradicate than gender or racial bias. We propose a novel empirical design to infer whether ChatGPT has political biases by requesting it to impersonate someone from a given side of the political spectrum and comparing these answers with its default. We also propose dose-response, placebo, and profession-politics alignment robustness tests. To reduce concerns about the randomness of the generated text, we collect answers to the same questions 100 times, with question order randomized on each round. We find robust evidence that ChatGPT presents a significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK. These results translate into real concerns that ChatGPT, and LLMs in general, can extend or even amplify the existing challenges involving political processes posed by the Internet and social media. Our findings have important implications for policymakers, media, politics, and academia stakeholders.",
        "authors": [
            "Fabio Motoki",
            "Valdemar Pinho Neto",
            "Victor Rodrigues"
        ],
        "citations": 154,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
        "abstract": "Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.",
        "authors": [
            "Tianyu Cui",
            "Yanling Wang",
            "Chuanpu Fu",
            "Yong Xiao",
            "Sijia Li",
            "Xinhao Deng",
            "Yunpeng Liu",
            "Qinglin Zhang",
            "Ziyi Qiu",
            "Peiyang Li",
            "Zhixing Tan",
            "Junwu Xiong",
            "Xinyu Kong",
            "Zujie Wen",
            "Ke Xu",
            "Qi Li"
        ],
        "citations": 41,
        "references": 417,
        "year": 2024
    },
    {
        "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
        "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.",
        "authors": [
            "Linhao Luo",
            "Yuan-Fang Li",
            "Gholamreza Haffari",
            "Shirui Pan"
        ],
        "citations": 132,
        "references": 76,
        "year": 2023
    },
    {
        "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
        "abstract": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
        "authors": [
            "Bill Yuchen Lin",
            "Abhilasha Ravichander",
            "Ximing Lu",
            "Nouha Dziri",
            "Melanie Sclar",
            "Khyathi Raghavi Chandu",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "citations": 133,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Knowledge Fusion of Large Language Models",
        "abstract": "While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseLLM}.",
        "authors": [
            "Fanqi Wan",
            "Xinting Huang",
            "Deng Cai",
            "Xiaojun Quan",
            "Wei Bi",
            "Shuming Shi"
        ],
        "citations": 39,
        "references": 62,
        "year": 2024
    },
    {
        "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs",
        "abstract": "Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLMs families and evaluation metrics, outperforms SOTA quantization methods of LLM by significant margins. Moreover, BiLLM enables the binarization process of the LLM with 7 billion weights within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency. Our code is available at https://github.com/Aaronhuang-778/BiLLM.",
        "authors": [
            "Wei Huang",
            "Yangdong Liu",
            "Haotong Qin",
            "Ying Li",
            "Shiming Zhang",
            "Xianglong Liu",
            "Michele Magno",
            "Xiaojuan Qi"
        ],
        "citations": 39,
        "references": 55,
        "year": 2024
    },
    {
        "title": "Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots",
        "abstract": "—The landscape of Artiﬁcial Intelligence (AI) services has been signiﬁcantly inﬂuenced by the rapid proliferation of Large Language Models (LLMs), primarily due to their remarkable proﬁciency in comprehending, generating, and completing text in a manner that mirrors human interaction. Among these services, LLM-based chatbots have gained widespread popularity due to their ability to facilitate smooth and intuitive human-machine exchanges. However, their susceptibility to jailbreak attacks — attempts by malicious users to prompt sensitive or harmful responses against service guidelines — remains a critical concern. Despite numerous efforts to expose these weak points, our research presented in this paper indicates that current strategies fall short in effectively targeting mainstream LLM chatbots. This ineffectiveness can be largely attributed to undisclosed defensive measures, implemented by service providers to thwart such exploitative attempts. Our paper presents J AILBREAKER , a comprehensive framework that offers insight into the intriguing dynamics of jail-break attacks and the countermeasures deployed against them. J AILBREAKER provides a dual-pronged contribution. Initially, we propose a novel method that utilizes time-based characteristics intrinsic to the generation process to deconstruct the defense mechanisms employed by popular LLM chatbot services. This approach, informed by time-based SQL injection techniques, allows us to unravel valuable details about the functioning of these defensive measures. Through careful manipulation of the chatbots’ time-sensitive reactions, we unravel the complex aspects of their design and establish a proof-of-concept attack to circumvent the defenses of multiple LLM chatbots such as C HAT GPT, Bard, and Bing Chat. Our second offering is an innovative method for the automatic generation of jailbreak prompts that target robustly defended LLM chatbots. The crux of this approach involves leveraging an LLM to auto-learn successful patterns. By ﬁne-tuning an LLM with jailbreak prompts, we validate the potential of automated jailbreak creation for several high-proﬁle commercial LLM chatbots. Our method generates attack prompts achieving an average success rate of 21.58%, considerably surpassing the 7.33% success rate accomplished by existing prompts. We have conscientiously reported our ﬁndings to the impacted service providers. J AILBREAKER establishes a groundbreaking approach to unveil vulnerabilities in LLMs, underscoring the need for more formidable defenses against such intrusions.",
        "authors": [
            "Gelei Deng",
            "Yi Liu",
            "Yuekang Li",
            "Kailong Wang",
            "Ying Zhang",
            "Zefeng Li",
            "Haoyu Wang",
            "Tianwei Zhang",
            "Yang Liu"
        ],
        "citations": 132,
        "references": 45,
        "year": 2023
    },
    {
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey",
        "abstract": "\n With the rapid development of online services and web applications, recommender systems (RS) have become increasingly indispensable for mitigating information overload and matching users’ information needs by providing personalized suggestions over items. Although the RS research community has made remarkable progress over the past decades, conventional recommendation models (CRM) still have some limitations,\n e.g.\n , lacking open-domain world knowledge, and difficulties in comprehending users’ underlying preferences and motivations. Meanwhile, large language models (LLM) have shown impressive general intelligence and human-like capabilities for various natural language processing (NLP) tasks, which mainly stem from their extensive open-world knowledge, logical and commonsense reasoning abilities, as well as their comprehension of human culture and society. Consequently, the emergence of LLM is inspiring the design of recommender systems and pointing out a promising research direction,\n i.e.\n , whether we can incorporate LLM and benefit from their common knowledge and capabilities to compensate for the limitations of CRM. In this paper, we conduct a comprehensive survey on this research direction, and draw a bird’s-eye view from the perspective of the whole pipeline in real-world recommender systems. Specifically, we summarize existing research works from two orthogonal aspects: where and how to adapt LLM to RS. For the “\n WHERE\n ” question, we discuss the roles that LLM could play in different stages of the recommendation pipeline,\n i.e.\n , feature engineering, feature encoder, scoring/ranking function, user interaction, and pipeline controller. For the “\n HOW\n ” question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria,\n i.e.\n , whether to tune LLM or not during training, and whether to involve conventional recommendation models for inference. Detailed analysis and general development paths are provided for both “WHERE” and “HOW” questions, respectively. Then, we highlight the key challenges in adapting LLM to RS from three aspects,\n i.e.\n , efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects. To further facilitate the research community of LLM-enhanced recommender systems, we actively maintain a GitHub repository for papers and other related resources in this rising direction\n \n 1\n \n .\n",
        "authors": [
            "Jianghao Lin",
            "Xinyi Dai",
            "Yunjia Xi",
            "Weiwen Liu",
            "Bo Chen",
            "Xiangyang Li",
            "Chenxu Zhu",
            "Huifeng Guo",
            "Yong Yu",
            "Ruiming Tang",
            "Weinan Zhang"
        ],
        "citations": 153,
        "references": 259,
        "year": 2023
    },
    {
        "title": "Large Language Model Guided Tree-of-Thought",
        "abstract": "In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",
        "authors": [
            "Jieyi Long"
        ],
        "citations": 137,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Assessing the research landscape and clinical utility of large language models: a scoping review",
        "abstract": null,
        "authors": [
            "Ye-Jean Park",
            "Abhinav Pillai",
            "Jiawen Deng",
            "Eddie Guo",
            "Mehul Gupta",
            "Mike Paget",
            "Christopher Naugler"
        ],
        "citations": 41,
        "references": 94,
        "year": 2024
    },
    {
        "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression",
        "abstract": "In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark, LongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost reduction in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua.",
        "authors": [
            "Huiqiang Jiang",
            "Qianhui Wu",
            "Xufang Luo",
            "Dongsheng Li",
            "Chin-Yew Lin",
            "Yuqing Yang",
            "Lili Qiu"
        ],
        "citations": 121,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Provable Robust Watermarking for AI-Generated Text",
        "abstract": "We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs. Code is available at https://github.com/XuandongZhao/Unigram-Watermark.",
        "authors": [
            "Xuandong Zhao",
            "P. Ananth",
            "Lei Li",
            "Yu-Xiang Wang"
        ],
        "citations": 123,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Reward Design with Language Models",
        "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning",
        "authors": [
            "Minae Kwon",
            "Sang Michael Xie",
            "Kalesha Bullard",
            "Dorsa Sadigh"
        ],
        "citations": 160,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
        "abstract": "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences: we show that, regardless of the premise, models falsely label NLI test samples as entailing when the hypothesis is attested in training data, and that entities are used as ``indices'' to access the memorized data. Second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation.",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Liang Cheng",
            "Mohammad Javad Hosseini",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "citations": 158,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution",
        "abstract": "Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furthermore, Promptbreeder is able to evolve intricate task-prompts for the challenging problem of hate speech classification.",
        "authors": [
            "Chrisantha Fernando",
            "Dylan Banarse",
            "H. Michalewski",
            "Simon Osindero",
            "Tim Rocktäschel"
        ],
        "citations": 131,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Large Language Model Alignment: A Survey",
        "abstract": "Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values. This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead. Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.",
        "authors": [
            "Tianhao Shen",
            "Renren Jin",
            "Yufei Huang",
            "Chuang Liu",
            "Weilong Dong",
            "Zishan Guo",
            "Xinwei Wu",
            "Yan Liu",
            "Deyi Xiong"
        ],
        "citations": 134,
        "references": 287,
        "year": 2023
    },
    {
        "title": "Evaluating Large Language Models at Evaluating Instruction Following",
        "abstract": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these ``LLM evaluators'', particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",
        "authors": [
            "Zhiyuan Zeng",
            "Jiatong Yu",
            "Tianyu Gao",
            "Yu Meng",
            "Tanya Goyal",
            "Danqi Chen"
        ],
        "citations": 125,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
        "abstract": "Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\\% significantly.",
        "authors": [
            "Zheng Yuan",
            "Hongyi Yuan",
            "Cheng Li",
            "Guanting Dong",
            "Chuanqi Tan",
            "Chang Zhou"
        ],
        "citations": 126,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation",
        "abstract": "The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm — Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. Our code and dataset can be found at https://github.com/jizhi-zhang/FaiRLLM.",
        "authors": [
            "Jizhi Zhang",
            "Keqin Bao",
            "Yang Zhang",
            "Wenjie Wang",
            "Fuli Feng",
            "Xiangnan He"
        ],
        "citations": 130,
        "references": 43,
        "year": 2023
    },
    {
        "title": "PAL: Program-aided Language Models",
        "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",
        "authors": [
            "Luyu Gao",
            "Aman Madaan",
            "Shuyan Zhou",
            "Uri Alon",
            "Pengfei Liu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
        ],
        "citations": 374,
        "references": 67,
        "year": 2022
    },
    {
        "title": "ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model",
        "abstract": null,
        "authors": [
            "Hanyao Huang",
            "Ou Zheng",
            "Dongdong Wang",
            "Jiayi Yin",
            "Zijin Wang",
            "Shengxuan Ding",
            "H. Yin",
            "Chuan Xu",
            "Renjie Yang",
            "Q. Zheng",
            "B. Shi"
        ],
        "citations": 131,
        "references": 111,
        "year": 2023
    },
    {
        "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
        "abstract": "To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling.Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an LLM to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones.We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. These representational harms have concerning implications for downstream applications like story generation.",
        "authors": [
            "Myra Cheng",
            "Esin Durmus",
            "Dan Jurafsky"
        ],
        "citations": 128,
        "references": 128,
        "year": 2023
    },
    {
        "title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs",
        "abstract": "We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on $\\textit{a single}$ A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: https://platypus-llm.github.io",
        "authors": [
            "Ariel N. Lee",
            "Cole J. Hunter",
            "Nataniel Ruiz"
        ],
        "citations": 126,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Implications of large language models such as ChatGPT for dental medicine.",
        "abstract": "OBJECTIVE\nThis article provides an overview of the implications of ChatGPT and other large language models (LLMs) for dental medicine.\n\n\nOVERVIEW\nChatGPT, a LLM trained on massive amounts of textual data, is adept at fulfilling various language-related tasks. Despite its impressive capabilities, ChatGPT has serious limitations, such as occasionally giving incorrect answers, producing nonsensical content, and presenting misinformation as fact. Dental practitioners, assistants, and hygienists are not likely to be significantly impacted by LLMs. However, LLMs could affect the work of administrative personnel and the provision of dental telemedicine. LLMs offer potential for clinical decision support, text summarization, efficient writing, and multilingual communication. As more people seek health information from LLMs, it is crucial to safeguard against inaccurate, outdated, and biased responses to health-related queries. LLMs pose challenges for patient data confidentiality and cybersecurity that must be tackled. In dental education, LLMs present fewer challenges than in other academic fields. LLMs can enhance academic writing fluency, but acceptable usage boundaries in science need to be established.\n\n\nCONCLUSIONS\nWhile LLMs such as ChatGPT may have various useful applications in dental medicine, they come with risks of malicious use and serious limitations, including the potential for misinformation.\n\n\nCLINICAL SIGNIFICANCE\nAlong with the potential benefits of using LLMs as an additional tool in dental medicine, it is crucial to carefully consider the limitations and potential risks inherent in such artificial intelligence technologies.",
        "authors": [
            "F. Eggmann",
            "R. Weiger",
            "N. Zitzmann",
            "M. Blatz"
        ],
        "citations": 129,
        "references": 14,
        "year": 2023
    },
    {
        "title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
        "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptRobust, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present a comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users.",
        "authors": [
            "Kaijie Zhu",
            "Jindong Wang",
            "Jiaheng Zhou",
            "Zichen Wang",
            "Hao Chen",
            "Yidong Wang",
            "Linyi Yang",
            "Weirong Ye",
            "N. Gong",
            "Yue Zhang",
            "Xingxu Xie"
        ],
        "citations": 129,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap",
        "abstract": "Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.",
        "authors": [
            "Xingyu Wu",
            "Sheng-hao Wu",
            "Jibin Wu",
            "Liang Feng",
            "Kay Chen Tan"
        ],
        "citations": 37,
        "references": 185,
        "year": 2024
    },
    {
        "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models",
        "abstract": "Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models.",
        "authors": [
            "Sheng Wang",
            "Zihao Zhao",
            "Xi Ouyang",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "citations": 126,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Task and Motion Planning with Large Language Models for Object Rearrangement",
        "abstract": "Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",
        "authors": [
            "Yan Ding",
            "Xiaohan Zhang",
            "Chris Paxton",
            "Shiqi Zhang"
        ],
        "citations": 146,
        "references": 44,
        "year": 2023
    },
    {
        "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
        "abstract": "Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.",
        "authors": [
            "Izzeddin Gur",
            "Hiroki Furuta",
            "Austin Huang",
            "Mustafa Safdari",
            "Yutaka Matsuo",
            "D. Eck",
            "Aleksandra Faust"
        ],
        "citations": 137,
        "references": 101,
        "year": 2023
    },
    {
        "title": "Automated Unit Test Improvement using Large Language Models at Meta",
        "abstract": "This paper describes Meta's TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM's test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta's Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.",
        "authors": [
            "N. Alshahwan",
            "Jubin Chheda",
            "Anastasia Finogenova",
            "Beliz Gokkaya",
            "Mark Harman",
            "Inna Harper",
            "Alexandru Marginean",
            "Shubho Sengupta",
            "Eddy Wang"
        ],
        "citations": 38,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Rethinking Interpretability in the Era of Large Language Models",
        "abstract": "Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.",
        "authors": [
            "Chandan Singh",
            "J. Inala",
            "Michel Galley",
            "Rich Caruana",
            "Jianfeng Gao"
        ],
        "citations": 38,
        "references": 156,
        "year": 2024
    },
    {
        "title": "Almanac - Retrieval-Augmented Language Models for Clinical Medicine.",
        "abstract": "BACKGROUND\nLarge language models (LLMs) have recently shown impressive zero-shot capabilities, whereby they can use auxiliary data, without the availability of task-specific training examples, to complete a variety of natural language tasks, such as summarization, dialogue generation, and question answering. However, despite many promising applications of LLMs in clinical medicine, adoption of these models has been limited by their tendency to generate incorrect and sometimes even harmful statements.\n\n\nMETHODS\nWe tasked a panel of eight board-certified clinicians and two health care practitioners with evaluating Almanac, an LLM framework augmented with retrieval capabilities from curated medical resources for medical guideline and treatment recommendations. The panel compared responses from Almanac and standard LLMs (ChatGPT-4, Bing, and Bard) versus a novel data set of 314 clinical questions spanning nine medical specialties.\n\n\nRESULTS\nAlmanac showed a significant improvement in performance compared with the standard LLMs across axes of factuality, completeness, user preference, and adversarial safety.\n\n\nCONCLUSIONS\nOur results show the potential for LLMs with access to domain-specific corpora to be effective in clinical decision-making. The findings also underscore the importance of carefully testing LLMs before deployment to mitigate their shortcomings. (Funded by the National Institutes of Health, National Heart, Lung, and Blood Institute.).",
        "authors": [
            "Cyril Zakka",
            "R. Shad",
            "Akash Chaurasia",
            "Alex R. Dalal",
            "Jennifer L. Kim",
            "M. Moor",
            "R. Fong",
            "Curran Phillips",
            "Kevin Alexander",
            "Euan A. Ashley",
            "Jack Boyd",
            "Kathleen Boyd",
            "Karen Hirsch",
            "Curtis P. Langlotz",
            "Rita Lee",
            "Joanna Melia",
            "Joanna Nelson",
            "Karim Sallam",
            "Stacey Tullis",
            "M. Vogelsong",
            "J. Cunningham",
            "W. Hiesinger"
        ],
        "citations": 38,
        "references": 17,
        "year": 2024
    },
    {
        "title": "Who's Harry Potter? Approximate Unlearning in LLMs",
        "abstract": "Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.",
        "authors": [
            "Ronen Eldan",
            "M. Russinovich"
        ],
        "citations": 124,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Large Language Models as Commonsense Knowledge for Large-Scale Task Planning",
        "abstract": "Large-scale task planning is a major challenge. Recent work exploits large language models (LLMs) directly as a policy and shows surprisingly interesting results. This paper shows that LLMs provide a commonsense model of the world in addition to a policy that acts on it. The world model and the policy can be combined in a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task planning. In our new LLM-MCTS algorithm, the LLM-induced world model provides a commonsense prior belief for MCTS to achieve effective reasoning; the LLM-induced policy acts as a heuristic to guide the search, vastly improving search efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin, for complex, novel tasks. Further experiments and analyses on multiple tasks -- multiplication, multi-hop travel planning, object rearrangement -- suggest minimum description length (MDL) as a general guiding principle: if the description length of the world model is substantially smaller than that of the policy, using LLM as a world model for model-based planning is likely better than using LLM solely as a policy.",
        "authors": [
            "Zirui Zhao",
            "W. Lee",
            "David Hsu"
        ],
        "citations": 129,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Instruction-Following Evaluation for Large Language Models",
        "abstract": "One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of\"verifiable instructions\"such as\"write in more than 400 words\"and\"mention the keyword of AI at least 3 times\". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction_following_eval",
        "authors": [
            "Jeffrey Zhou",
            "Tianjian Lu",
            "Swaroop Mishra",
            "Siddhartha Brahma",
            "Sujoy Basu",
            "Yi Luan",
            "Denny Zhou",
            "Le Hou"
        ],
        "citations": 114,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",
        "abstract": "This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.",
        "authors": [
            "Cunxiang Wang",
            "Xiaoze Liu",
            "Yuanhao Yue",
            "Xiangru Tang",
            "Tianhang Zhang",
            "Cheng Jiayang",
            "Yunzhi Yao",
            "Wenyang Gao",
            "Xuming Hu",
            "Zehan Qi",
            "Yidong Wang",
            "Linyi Yang",
            "Jindong Wang",
            "Xing Xie",
            "Zheng Zhang",
            "Yue Zhang"
        ],
        "citations": 135,
        "references": 284,
        "year": 2023
    },
    {
        "title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
        "abstract": "In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature",
        "authors": [
            "Matthew Renze",
            "Erhan Guven"
        ],
        "citations": 35,
        "references": 47,
        "year": 2024
    },
    {
        "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
        "abstract": "Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.",
        "authors": [
            "Aohan Zeng",
            "Mingdao Liu",
            "Rui Lu",
            "Bowen Wang",
            "Xiao Liu",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "citations": 116,
        "references": 53,
        "year": 2023
    },
    {
        "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
        "abstract": "The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \\url{https://github.com/OFA-Sys/ExpertLLaMA}.",
        "authors": [
            "Benfeng Xu",
            "An Yang",
            "Junyang Lin",
            "Quang Wang",
            "Chang Zhou",
            "Yongdong Zhang",
            "Zhendong Mao"
        ],
        "citations": 104,
        "references": 17,
        "year": 2023
    },
    {
        "title": "Prompting Large Language Models with Speech Recognition Abilities",
        "abstract": "Large language models (LLMs) have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLM by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audio embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% relatively in WER and perform multilingual speech recognition, despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to generate fewer embeddings. The results from these studies show that multilingual ASR is possible even when the LLM is frozen, or when strides of almost 1 second are used in the audio encoder opening up the possibility for LLMs to operate on long-form audio.",
        "authors": [
            "Yassir Fathullah",
            "Chunyang Wu",
            "Egor Lakomkin",
            "J. Jia",
            "Yuan Shangguan",
            "Ke Li",
            "Jinxi Guo",
            "Wenhan Xiong",
            "Jay Mahadeokar",
            "Ozlem Kalinli",
            "Christian Fuegen",
            "M. Seltzer"
        ],
        "citations": 107,
        "references": 26,
        "year": 2023
    },
    {
        "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
        "abstract": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.",
        "authors": [
            "Leonard Salewski",
            "Stephan Alaniz",
            "Isabel Rio-Torto",
            "Eric Schulz",
            "Zeynep Akata"
        ],
        "citations": 122,
        "references": 97,
        "year": 2023
    },
    {
        "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets",
        "abstract": "Large Language Models (LLMs) have impressive capabilities, but are prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we use high-quality datasets of simple true/false statements to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that at sufficient scale, LLMs linearly represent the truth or falsehood of factual statements. We also show that simple difference-in-mean probes generalize as well as other probing techniques while identifying directions which are more causally implicated in model outputs.",
        "authors": [
            "Samuel Marks",
            "Max Tegmark"
        ],
        "citations": 116,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
        "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
        "authors": [
            "Xiangyu Qi",
            "Kaixuan Huang",
            "Ashwinee Panda",
            "Mengdi Wang",
            "Prateek Mittal"
        ],
        "citations": 98,
        "references": 89,
        "year": 2023
    },
    {
        "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling",
        "abstract": "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.",
        "authors": [
            "Dahyun Kim",
            "Chanjun Park",
            "Sanghoon Kim",
            "Wonsung Lee",
            "Wonho Song",
            "Yunsu Kim",
            "Hyeonwoo Kim",
            "Yungi Kim",
            "Hyeonju Lee",
            "Jihoo Kim",
            "Changbae Ahn",
            "Seonghoon Yang",
            "Sukyung Lee",
            "Hyunbyung Park",
            "Gyoungjin Gim",
            "Mikyoung Cha",
            "Hwalsuk Lee",
            "Sunghun Kim"
        ],
        "citations": 100,
        "references": 53,
        "year": 2023
    },
    {
        "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance",
        "abstract": "Although large language models (LLMs) has shown great performance on natural language processing (NLP) in the financial domain, there are no publicly available financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 136K data samples to support the fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including five financial NLP tasks and one financial prediction task. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI.",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Xiao Zhang",
            "Yanzhao Lai",
            "Min Peng",
            "Alejandro Lopez-Lira",
            "Jimin Huang"
        ],
        "citations": 99,
        "references": 50,
        "year": 2023
    },
    {
        "title": "In-Context Learning Creates Task Vectors",
        "abstract": "In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the\"standard\"machine learning framework, where one uses a training set $S$ to find a best-fitting function $f(x)$ in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query $x$ and a single\"task vector\"calculated from the training set. Thus, ICL can be seen as compressing $S$ into a single task vector $\\boldsymbol{\\theta}(S)$ and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.",
        "authors": [
            "Roee Hendel",
            "Mor Geva",
            "Amir Globerson"
        ],
        "citations": 102,
        "references": 21,
        "year": 2023
    },
    {
        "title": "Large Language Models for Software Engineering: Survey and Open Problems",
        "abstract": "This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.",
        "authors": [
            "Angela Fan",
            "Beliz Gokkaya",
            "Mark Harman",
            "Mitya Lyubarskiy",
            "Shubho Sengupta",
            "Shin Yoo",
            "Jie M. Zhang"
        ],
        "citations": 128,
        "references": 236,
        "year": 2023
    },
    {
        "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration",
        "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The “decoder-only“ architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
        "authors": [
            "Jian Wu",
            "Yashesh Gaur",
            "Zhuo Chen",
            "Long Zhou",
            "Yilun Zhu",
            "Tianrui Wang",
            "Jinyu Li",
            "Shujie Liu",
            "Bo Ren",
            "Linquan Liu",
            "Yu Wu"
        ],
        "citations": 103,
        "references": 43,
        "year": 2023
    },
    {
        "title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
        "abstract": "The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.",
        "authors": [
            "Neel Guha",
            "Julian Nyarko",
            "Daniel E. Ho",
            "Christopher Ré",
            "Adam Chilton",
            "Aditya Narayana",
            "Alex Chohlas-Wood",
            "Austin M. K. Peters",
            "Brandon Waldon",
            "D. Rockmore",
            "Diego A. Zambrano",
            "Dmitry Talisman",
            "E. Hoque",
            "Faiz Surani",
            "F. Fagan",
            "Galit Sarfaty",
            "Gregory M. Dickinson",
            "Haggai Porat",
            "Jason Hegland",
            "Jessica Wu",
            "Joe Nudell",
            "Joel Niklaus",
            "John J. Nay",
            "Jonathan H. Choi",
            "K. Tobia",
            "M. Hagan",
            "Megan Ma",
            "Michael A. Livermore",
            "Nikon Rasumov-Rahe",
            "Nils Holzenberger",
            "Noam Kolt",
            "Peter Henderson",
            "Sean Rehaag",
            "Sharad Goel",
            "Shangsheng Gao",
            "Spencer Williams",
            "Sunny G. Gandhi",
            "Tomer Zur",
            "Varun J. Iyer",
            "Zehua Li"
        ],
        "citations": 103,
        "references": 59,
        "year": 2023
    },
    {
        "title": "RAGAs: Automated Evaluation of Retrieval Augmented Generation",
        "abstract": "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",
        "authors": [
            "ES Shahul",
            "Jithin James",
            "Luis Espinosa Anke",
            "S. Schockaert"
        ],
        "citations": 113,
        "references": 29,
        "year": 2023
    },
    {
        "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
        "abstract": "Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.",
        "authors": [
            "Minghao Li",
            "Feifan Song",
            "Yu Bowen",
            "Haiyang Yu",
            "Zhoujun Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "citations": 101,
        "references": 28,
        "year": 2023
    },
    {
        "title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
        "abstract": "Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction.",
        "authors": [
            "Jiayan Guo",
            "Lun Du",
            "Hengyu Liu"
        ],
        "citations": 118,
        "references": 44,
        "year": 2023
    },
    {
        "title": "LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving",
        "abstract": "Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field. Project page: https://sites.google.com/view/llm-mpc",
        "authors": [
            "Hao Sha",
            "Yao Mu",
            "Yuxuan Jiang",
            "Li Chen",
            "Chenfeng Xu",
            "Ping Luo",
            "S. Li",
            "Masayoshi Tomizuka",
            "Wei Zhan",
            "Mingyu Ding"
        ],
        "citations": 120,
        "references": 70,
        "year": 2023
    },
    {
        "title": "InferFix: End-to-End Program Repair with LLMs",
        "abstract": "Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.",
        "authors": [
            "Ma Jin",
            "Syed Shahriar",
            "Michele Tufano",
            "Xin Shi",
            "Shuai Lu",
            "Neel Sundaresan",
            "Alexey Svyatkovskiy"
        ],
        "citations": 122,
        "references": 38,
        "year": 2023
    },
    {
        "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
        "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT.",
        "authors": [
            "Gengze Zhou",
            "Yicong Hong",
            "Qi Wu"
        ],
        "citations": 98,
        "references": 85,
        "year": 2023
    },
    {
        "title": "TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents",
        "abstract": "With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.",
        "authors": [
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Zhiwei Xu",
            "Tianpeng Bao",
            "Guoqing Du",
            "Shiwei Shi",
            "Hangyu Mao",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "citations": 98,
        "references": 91,
        "year": 2023
    },
    {
        "title": "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap",
        "abstract": "The rise of powerful large language models (LLMs) brings about tremendous opportunities for innovation but also looming risks for individuals and society at large. We have reached a pivotal moment for ensuring that LLMs and LLM-infused applications are developed and deployed responsibly. However, a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs. It is paramount to pursue new approaches to provide transparency for LLMs, and years of research at the intersection of AI and human-computer interaction (HCI) highlight that we must do so with a human-centered perspective: Transparency is fundamentally about supporting appropriate human understanding, and this understanding is sought by different stakeholders with different goals in different contexts. In this new era of LLMs, we must develop and design approaches to transparency by considering the needs of stakeholders in the emerging LLM ecosystem, the novel types of LLM-infused applications being built, and the new usage patterns and challenges around LLMs, all while building on lessons learned about how people process, interact with, and make use of information. We reflect on the unique challenges that arise in providing transparency for LLMs, along with lessons learned from HCI and responsible AI research that has taken a human-centered perspective on AI transparency. We then lay out four common approaches that the community has taken to achieve transparency -- model reporting, publishing evaluation results, providing explanations, and communicating uncertainty -- and call out open questions around how these approaches may or may not be applied to LLMs. We hope this provides a starting point for discussion and a useful roadmap for future research.",
        "authors": [
            "Q. Liao",
            "J. Vaughan"
        ],
        "citations": 113,
        "references": 219,
        "year": 2023
    },
    {
        "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds",
        "abstract": "The unprecedented advancements in Large Language Models (LLMs) have shown a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM understands colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate the perceptual and generalization capabilities of PointLLM, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different methods, including human evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples. Codes, datasets, and benchmarks are available at https://github.com/OpenRobotLab/PointLLM .",
        "authors": [
            "Runsen Xu",
            "Xiaolong Wang",
            "Tai Wang",
            "Yilun Chen",
            "Jiangmiao Pang",
            "Dahua Lin"
        ],
        "citations": 104,
        "references": 74,
        "year": 2023
    },
    {
        "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
        "abstract": "We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting . Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed ﬁltering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following . So far, these attacks assumed that the adversary is directly prompting the LLM. In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs ) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viability of our attacks, we implemented speciﬁc demonstrations",
        "authors": [
            "Kai Greshake",
            "Sahar Abdelnabi",
            "Shailesh Mishra",
            "C. Endres",
            "Thorsten Holz",
            "Mario Fritz"
        ],
        "citations": 100,
        "references": 37,
        "year": 2023
    },
    {
        "title": "ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning",
        "abstract": "This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.",
        "authors": [
            "Petter Törnberg"
        ],
        "citations": 115,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve",
        "abstract": "The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low - even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4's accuracy at decoding a simple cipher is 51% when the output is a high-probability word sequence but only 13% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system - one that has been shaped by its own particular set of pressures.",
        "authors": [
            "R. Thomas McCoy",
            "Shunyu Yao",
            "Dan Friedman",
            "Matthew Hardy",
            "Thomas L. Griffiths"
        ],
        "citations": 114,
        "references": 0,
        "year": 2023
    },
    {
        "title": "ART: Automatic multi-step reasoning and tool-use for large language models",
        "abstract": "Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.",
        "authors": [
            "Bhargavi Paranjape",
            "Scott M. Lundberg",
            "Sameer Singh",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer",
            "Marco Tulio Ribeiro"
        ],
        "citations": 118,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Drive Like a Human: Rethinking Autonomous Driving with Large Language Models",
        "abstract": "In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, providing valuable insights for the development of human-like autonomous driving. The related code are available at https:/ithub.om/PJLab-ADG/DriveLikeAHuman.",
        "authors": [
            "Daocheng Fu",
            "Xin Li",
            "Licheng Wen",
            "Min Dou",
            "Pinlong Cai",
            "Botian Shi",
            "Y. Qiao"
        ],
        "citations": 116,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
        "abstract": "Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, we collect a dataset that consists of 1.9K harmful questions covering a wide range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2) SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the safety alignment of LLMs by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the utility of the baseline models (TruthfulQA, MMLU, and BBH).",
        "authors": [
            "Rishabh Bhardwaj",
            "Soujanya Poria"
        ],
        "citations": 110,
        "references": 35,
        "year": 2023
    },
    {
        "title": "An Empirical Study of the Non-determinism of ChatGPT in Code Generation",
        "abstract": "There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; nondeterministically returning very different code for the same prompt. Such non-determinism affects the correctness and consistency of the generated code, undermines developers’ trust in LLMs, and yields low reproducibility in LLM-based papers. Nevertheless, there is no work investigating how serious this non-determinism threat is.\n \n To fill this gap, this paper conducts an empirical study on the non-determinism of ChatGPT in code generation. We chose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems across three code generation benchmarks (i.e., CodeContests, APPS, and HumanEval) with three aspects of code similarities: semantic similarity, syntactic similarity, and structural similarity. Our results reveal that ChatGPT exhibits a high degree of non-determinism under the default setting: the ratio of coding tasks with zero equal test output across different requests is 75.76%, 51.00%, and 47.56% for three different code generation datasets (i.e., CodeContests, APPS, and HumanEval), respectively. In addition, we find that setting the\n temperature\n to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (\n temperature\n =1). In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.\n",
        "authors": [
            "Shuyin Ouyang",
            "J Zhang",
            "M. Harman",
            "Meng Wang"
        ],
        "citations": 100,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Eight Things to Know about Large Language Models",
        "abstract": "The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.",
        "authors": [
            "Sam Bowman"
        ],
        "citations": 101,
        "references": 127,
        "year": 2023
    },
    {
        "title": "Translating Natural Language to Planning Goals with Large-Language Models",
        "abstract": "Recent large language models (LLMs) have demonstrated remarkable performance on a variety of natural language processing (NLP) tasks, leading to intense excitement about their applicability across various domains. Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks. In this work, our central question is whether LLMs are able to translate goals specified in natural language to a structured planning language. If so, LLM can act as a natural interface between the planner and human users; the translated goal can be handed to domain-independent AI planners that are very effective at planning. Our empirical results on GPT 3.5 variants show that LLMs are much better suited towards translation rather than planning. We find that LLMs are able to leverage commonsense knowledge and reasoning to furnish missing details from under-specified goals (as is often the case in natural language). However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used. As such, these models are promising for translation to structured planning languages, but care should be taken in their use.",
        "authors": [
            "Yaqi Xie",
            "Chenyao Yu",
            "Tongyao Zhu",
            "Jinbin Bai",
            "Ze Gong",
            "Harold Soh"
        ],
        "citations": 119,
        "references": 39,
        "year": 2023
    },
    {
        "title": "GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations",
        "abstract": "As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and (2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Most open-source LLMs, e.g., CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than commercial LLMs, e.g., GPT-4, in complex games, yet the recently released Llama-3-70b-Instruct makes up for this shortcoming. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games. Detailed error profiles are provided for a better understanding of LLMs' behavior. We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs.",
        "authors": [
            "Jinhao Duan",
            "Renming Zhang",
            "James Diffenderfer",
            "B. Kailkhura",
            "Lichao Sun",
            "Elias Stengel-Eskin",
            "Mohit Bansal",
            "Tianlong Chen",
            "Kaidi Xu"
        ],
        "citations": 36,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Large Language Models in Finance: A Survey",
        "abstract": "Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI.",
        "authors": [
            "Yinheng Li",
            "Shaofei Wang",
            "Han Ding",
            "Hang Chen"
        ],
        "citations": 110,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Comparing Code Explanations Created by Students and Large Language Models",
        "abstract": "Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To evaluate LLM-created explanations, we compare them with explanations created by students in a large course (n ≈ 1000) with respect to accuracy, understandability and length. We find that LLM-created explanations, which can be produced automatically on demand, are rated as being significantly easier to understand and more accurate summaries of code than student-created explanations. We discuss the significance of this finding, and suggest how such models can be incorporated into introductory programming education.",
        "authors": [
            "Juho Leinonen",
            "Paul Denny",
            "S. Macneil",
            "Sami Sarsa",
            "Seth Bernstein",
            "Joanne Kim",
            "Andrew Tran",
            "Arto Hellas"
        ],
        "citations": 116,
        "references": 53,
        "year": 2023
    },
    {
        "title": "ChipNeMo: Domain-Adapted LLMs for Chip Design",
        "abstract": "ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: domain-adaptive tokenization, domain-adaptive continued pretraining, model alignment with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our evaluations demonstrate that domain-adaptive pretraining of language models, can lead to superior performance in domain related downstream tasks compared to their base LLaMA2 counterparts, without degradations in generic capabilities. In particular, our largest model, ChipNeMo-70B, outperforms the highly capable GPT-4 on two of our use cases, namely engineering assistant chatbot and EDA scripts generation, while exhibiting competitive performance on bug summarization and analysis. These results underscore the potential of domain-specific customization for enhancing the effectiveness of large language models in specialized applications.",
        "authors": [
            "Mingjie Liu",
            "Teodor-Dumitru Ene",
            "Robert Kirby",
            "Chris Cheng",
            "Nathaniel Pinckney",
            "Rongjian Liang",
            "Jonah Alben",
            "Himyanshu Anand",
            "Sanmitra Banerjee",
            "I. Bayraktaroglu",
            "Bonita Bhaskaran",
            "Bryan Catanzaro",
            "Arjun Chaudhuri",
            "Sharon Clay",
            "B. Dally",
            "Laura Dang",
            "Parikshit Deshpande",
            "Siddhanth Dhodhi",
            "S. Halepete",
            "Eric Hill",
            "Jiashang Hu",
            "Sumit Jain",
            "Brucek Khailany",
            "K. Kunal",
            "Xiaowei Li",
            "Hao Liu",
            "S. Oberman",
            "Sujeet Omar",
            "Sreedhar Pratty",
            "Jonathan Raiman",
            "Ambar Sarkar",
            "Zhengjiang Shao",
            "Hanfei Sun",
            "Pratik P Suthar",
            "Varun Tej",
            "Kaizhe Xu",
            "Haoxin Ren"
        ],
        "citations": 98,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?",
        "abstract": "Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.",
        "authors": [
            "Kai Sun",
            "Y. Xu",
            "Hanwen Zha",
            "Yue Liu",
            "Xinhsuai Dong"
        ],
        "citations": 101,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis",
        "abstract": "Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs' performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages. Second, instruction semantics can surprisingly be ignored when given in-context exemplars. Third, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Code will be released at: https://github.com/NJUNLP/MMT-LLM.",
        "authors": [
            "Wenhao Zhu",
            "Hongyi Liu",
            "Qingxiu Dong",
            "Jingjing Xu",
            "Lingpeng Kong",
            "Jiajun Chen",
            "Lei Li",
            "Shujian Huang"
        ],
        "citations": 110,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
        "abstract": "While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback—either produced by the LLM itself (self-correction) or some external system—are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",
        "authors": [
            "Liangming Pan",
            "Michael Stephen Saxon",
            "Wenda Xu",
            "Deepak Nathani",
            "Xinyi Wang",
            "W. Wang"
        ],
        "citations": 35,
        "references": 158,
        "year": 2024
    },
    {
        "title": "Can Large Language Model Agents Simulate Human Trust Behaviors?",
        "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.",
        "authors": [
            "Chengxing Xie",
            "Canyu Chen",
            "Feiran Jia",
            "Ziyu Ye",
            "Kai Shu",
            "Adel Bibi",
            "Ziniu Hu",
            "Philip H. S. Torr",
            "Bernard Ghanem",
            "G. Li"
        ],
        "citations": 34,
        "references": 70,
        "year": 2024
    },
    {
        "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
        "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",
        "authors": [
            "Antonia Creswell",
            "M. Shanahan",
            "I. Higgins"
        ],
        "citations": 303,
        "references": 53,
        "year": 2022
    },
    {
        "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
        "abstract": "As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.",
        "authors": [
            "Yao Fu",
            "Litu Ou",
            "Mingyu Chen",
            "Yuhao Wan",
            "Hao-Chun Peng",
            "Tushar Khot"
        ],
        "citations": 97,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Personality Traits in Large Language Models",
        "abstract": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.",
        "authors": [
            "Mustafa Safdari",
            "Gregory Serapio-Garc'ia",
            "Clé-ment Crepy",
            "Stephen Fitz",
            "P. Romero",
            "Luning Sun",
            "Marwa Abdulhai",
            "Aleksandra Faust",
            "Maja J Matari'c"
        ],
        "citations": 97,
        "references": 183,
        "year": 2023
    },
    {
        "title": "Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks",
        "abstract": "Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk",
        "authors": [
            "V. Veselovsky",
            "Manoel Horta Ribeiro",
            "Robert West"
        ],
        "citations": 110,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
        "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
        "authors": [
            "H. Trivedi",
            "Niranjan Balasubramanian",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "citations": 285,
        "references": 124,
        "year": 2022
    },
    {
        "title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
        "abstract": null,
        "authors": [
            "M. Polak",
            "Dane Morgan"
        ],
        "citations": 98,
        "references": 51,
        "year": 2023
    },
    {
        "title": "VerilogEval: Evaluating Large Language Models for Verilog Code Generation",
        "abstract": "The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.",
        "authors": [
            "Mingjie Liu",
            "N. Pinckney",
            "Brucek Khailany",
            "Haoxing Ren"
        ],
        "citations": 93,
        "references": 34,
        "year": 2023
    },
    {
        "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
        "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.",
        "authors": [
            "Haotian Sun",
            "Yuchen Zhuang",
            "Lingkai Kong",
            "Bo Dai",
            "Chao Zhang"
        ],
        "citations": 93,
        "references": 30,
        "year": 2023
    },
    {
        "title": "On the Reliability of Watermarks for Large Language Models",
        "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.",
        "authors": [
            "John Kirchenbauer",
            "Jonas Geiping",
            "Yuxin Wen",
            "Manli Shu",
            "Khalid Saifullah",
            "Kezhi Kong",
            "Kasun Fernando",
            "Aniruddha Saha",
            "Micah Goldblum",
            "T. Goldstein"
        ],
        "citations": 95,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Perspectives on Large Language Models for Relevance Judgment",
        "abstract": "When asked, large language models~(LLMs) like ChatGPT claim that they can assist with relevance judgments but it is not clear whether automated judgments can reliably be used in evaluations of retrieval systems. In this perspectives paper, we discuss possible ways for~LLMs to support relevance judgments along with concerns and issues that arise. We devise a human--machine collaboration spectrum that allows to categorize different relevance judgment strategies, based on how much humans rely on machines. For the extreme point of 'fully automated judgments', we further include a pilot experiment on whether LLM-based relevance judgments correlate with judgments from trained human assessors. We conclude the paper by providing opposing perspectives for and against the use of~LLMs for automatic relevance judgments, and a compromise perspective, informed by our analyses of the literature, our preliminary experimental evidence, and our experience as IR~researchers.",
        "authors": [
            "G. Faggioli",
            "Laura Dietz",
            "C. Clarke",
            "Gianluca Demartini",
            "Matthias Hagen",
            "C. Hauff",
            "N. Kando",
            "E. Kanoulas",
            "Martin Potthast",
            "Benno Stein",
            "Henning Wachsmuth"
        ],
        "citations": 93,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Nash Learning from Human Feedback",
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution. In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences.",
        "authors": [
            "Rémi Munos",
            "Michal Valko",
            "Daniele Calandriello",
            "M. G. Azar",
            "Mark Rowland",
            "Z. Guo",
            "Yunhao Tang",
            "Matthieu Geist",
            "Thomas Mesnard",
            "Andrea Michi",
            "Marco Selvi",
            "Sertan Girgin",
            "Nikola Momchev",
            "Olivier Bachem",
            "D. Mankowitz",
            "D. Precup",
            "Bilal Piot"
        ],
        "citations": 89,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
        "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
        "authors": [
            "Chuanyang Zheng",
            "Zhengying Liu",
            "Enze Xie",
            "Zhenguo Li",
            "Yu Li"
        ],
        "citations": 91,
        "references": 70,
        "year": 2023
    },
    {
        "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
        "abstract": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -- and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL's performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.",
        "authors": [
            "Jane Pan",
            "Tianyu Gao",
            "Howard Chen",
            "Danqi Chen"
        ],
        "citations": 94,
        "references": 31,
        "year": 2023
    },
    {
        "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
        "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.",
        "authors": [
            "Xiangru Tang",
            "Anni Zou",
            "Zhuosheng Zhang",
            "Yilun Zhao",
            "Xingyao Zhang",
            "Arman Cohan",
            "Mark B. Gerstein"
        ],
        "citations": 94,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Representation Learning with Large Language Models for Recommendation",
        "abstract": "Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, employs LLMs for user/item profiling, and aligns the semantic space of LLMs with collaborative relational signals through cross-view alignment. This work further demonstrates the theoretical foundation of incorporating textual signals through mutual information maximization, which improves the quality of representations. Our evaluation integrates RLMRec with state-of-the-art recommender models, while also analyzing its efficiency and robustness to noise data. Implementation codes are available at https://github.com/HKUDS/RLMRec.",
        "authors": [
            "Xubin Ren",
            "Wei Wei",
            "Lianghao Xia",
            "Lixin Su",
            "Suqi Cheng",
            "Junfeng Wang",
            "Dawei Yin",
            "Chao Huang"
        ],
        "citations": 89,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Large Language Models for Generative Information Extraction: A Survey",
        "abstract": "Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).",
        "authors": [
            "Derong Xu",
            "Wei Chen",
            "Wenjun Peng",
            "Chao Zhang",
            "Tong Xu",
            "Xiangyu Zhao",
            "Xian Wu",
            "Yefeng Zheng",
            "Enhong Chen"
        ],
        "citations": 92,
        "references": 298,
        "year": 2023
    },
    {
        "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
        "abstract": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -- and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL's performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.",
        "authors": [
            "Jane Pan",
            "Tianyu Gao",
            "Howard Chen",
            "Danqi Chen"
        ],
        "citations": 94,
        "references": 31,
        "year": 2023
    },
    {
        "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
        "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.",
        "authors": [
            "Xiangru Tang",
            "Anni Zou",
            "Zhuosheng Zhang",
            "Yilun Zhao",
            "Xingyao Zhang",
            "Arman Cohan",
            "Mark B. Gerstein"
        ],
        "citations": 94,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Benchmarking LLMs via Uncertainty Quantification",
        "abstract": "The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs.",
        "authors": [
            "Fanghua Ye",
            "Mingming Yang",
            "Jianhui Pang",
            "Longyue Wang",
            "Derek F. Wong",
            "Emine Yilmaz",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "citations": 31,
        "references": 88,
        "year": 2024
    },
    {
        "title": "Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models",
        "abstract": "Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants’ feedback.",
        "authors": [
            "Priyan Vaithilingam",
            "Tianyi Zhang",
            "Elena L. Glassman"
        ],
        "citations": 397,
        "references": 53,
        "year": 2022
    },
    {
        "title": "On the Risk of Misinformation Pollution with Large Language Models",
        "abstract": "In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs.",
        "authors": [
            "Yikang Pan",
            "Liangming Pan",
            "Wenhu Chen",
            "Preslav Nakov",
            "Min-Yen Kan",
            "W. Wang"
        ],
        "citations": 89,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
        "abstract": "Recent work has considered whether large language models (LLMs) can function as planners: given a task, generate a plan. We investigate whether LLMs can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain. In particular, we consider PDDL domains and use GPT-4 to synthesize Python programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program; and (2) automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback. We evaluate this approach in seven PDDL domains and compare it to four ablations and four baselines. Overall, we find that GPT-4 is a surprisingly powerful generalized planner. We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two training tasks are often sufficient for strong generalization.",
        "authors": [
            "Tom Silver",
            "Soham Dan",
            "Kavitha Srinivas",
            "J. Tenenbaum",
            "L. Kaelbling",
            "Michael Katz"
        ],
        "citations": 92,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Playing repeated games with Large Language Models",
        "abstract": "Large Language Models (LLMs) are transforming society and permeating into diverse applications. As a result, LLMs will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. Here, we propose to use behavioral game theory to study LLM's cooperation and coordination behavior. To do so, we let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-like strategies. Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioral signatures. In a large set of two players-two strategies games, we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination. We, therefore, further focus on two games from these distinct families. In the canonical iterated Prisoner's Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting after another agent has defected only once. In the Battle of the Sexes, we find that GPT-4 cannot match the behavior of the simple convention to alternate between options. We verify that these behavioral signatures are stable across robustness checks. Finally, we show how GPT-4's behavior can be modified by providing further information about the other player as well as by asking it to predict the other player's actions before making a choice. These results enrich our understanding of LLM's social behavior and pave the way for a behavioral game theory for machines.",
        "authors": [
            "Elif Akata",
            "Lion Schulz",
            "Julian Coda-Forno",
            "Seong Joon Oh",
            "M. Bethge",
            "Eric Schulz"
        ],
        "citations": 95,
        "references": 62,
        "year": 2023
    },
    {
        "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
        "abstract": "Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .",
        "authors": [
            "Chenxu Hu",
            "Jie Fu",
            "Chenzhuang Du",
            "Simian Luo",
            "J. Zhao",
            "Hang Zhao"
        ],
        "citations": 90,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
        "abstract": "Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM’s outputs for unintended purposes. In this paper, we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that—when combined with a user’s query—disrupts the attacked model’s alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model’s limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments, we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool for evaluating and enhancing alignment of LLMs with human intent. To our knowledge, this is the first automated universal black-box jailbreak attack.",
        "authors": [
            "Raz Lapid",
            "Ron Langberg",
            "Moshe Sipper"
        ],
        "citations": 85,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations",
        "abstract": "As large language models (LLMs) gain popularity among speakers of diverse languages, we believe that it is crucial to benchmark them to better understand model behaviors, failures, and limitations in languages beyond English. In this work, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national medical licensing examinations from the past five years, including the current year. Our team comprises native Japanese-speaking NLP researchers and a practicing cardiologist based in Japan. Our experiments show that GPT-4 outperforms ChatGPT and GPT-3 and passes all six years of the exams, highlighting LLMs' potential in a language that is typologically distant from English. However, our evaluation also exposes critical limitations of the current LLM APIs. First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia. Further, our analysis shows that the API costs are generally higher and the maximum context size is smaller for Japanese because of the way non-Latin scripts are currently tokenized in the pipeline. We release our benchmark as Igaku QA as well as all model outputs and exam metadata. We hope that our results and benchmark will spur progress on more diverse applications of LLMs. Our benchmark is available at https://github.com/jungokasai/IgakuQA.",
        "authors": [
            "Jungo Kasai",
            "Y. Kasai",
            "Keisuke Sakaguchi",
            "Yutaro Yamada",
            "Dragomir R. Radev"
        ],
        "citations": 86,
        "references": 82,
        "year": 2023
    },
    {
        "title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
        "abstract": "The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations. The failure explanation can further guide a language-based planner to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset with a variety of tasks and failure scenarios. We demonstrate that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.",
        "authors": [
            "Zeyi Liu",
            "Arpit Bahety",
            "Shuran Song"
        ],
        "citations": 86,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",
        "abstract": "Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method called ``red teaming'' on OpenAI's ChatGPT\\footnote{In this paper, ChatGPT refers to the version released on Dec 15th.} to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) \\textit{Bias} 2) \\textit{Reliability} 3) \\textit{Robustness} 4) \\textit{Toxicity}. In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our findings on AI ethics and harmal behaviors of ChatGPT, as well as future problems and practical design considerations for responsible LLMs. We believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.",
        "authors": [
            "Terry Yue Zhuo",
            "Yujin Huang",
            "Chunyang Chen",
            "Zhenchang Xing"
        ],
        "citations": 86,
        "references": 97,
        "year": 2023
    },
    {
        "title": "ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports",
        "abstract": null,
        "authors": [
            "Katharina Jeblick",
            "B. Schachtner",
            "Jakob Dexl",
            "Andreas Mittermeier",
            "A. T. Stüber",
            "Johanna Topalis",
            "Tobias Weber",
            "Philipp Wesp",
            "B. Sabel",
            "J. Ricke",
            "M. Ingrisch"
        ],
        "citations": 322,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Large Language Model Unlearning",
        "abstract": "We study how to perform unlearning, i.e. forgetting undesirable misbehaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.",
        "authors": [
            "Yuanshun Yao",
            "Xiaojun Xu",
            "Yang Liu"
        ],
        "citations": 79,
        "references": 55,
        "year": 2023
    },
    {
        "title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models",
        "abstract": "We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs’ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach — it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility — in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.",
        "authors": [
            "Zhao Mandi",
            "Shreeya Jain",
            "Shuran Song"
        ],
        "citations": 82,
        "references": 64,
        "year": 2023
    },
    {
        "title": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
        "abstract": "Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multi-modal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that our model achieves 76.1 driving score on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points under the same settings, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs. Code and models shall be released at https://github.com/OpenGVLab/DriveMLM.",
        "authors": [
            "Wenhai Wang",
            "Jiangwei Xie",
            "ChuanYang Hu",
            "Haoming Zou",
            "Jianan Fan",
            "Wenwen Tong",
            "Yang Wen",
            "Silei Wu",
            "Hanming Deng",
            "Zhiqi Li",
            "Hao Tian",
            "Lewei Lu",
            "Xizhou Zhu",
            "Xiaogang Wang",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "citations": 84,
        "references": 80,
        "year": 2023
    },
    {
        "title": "PALR: Personalization Aware LLMs for Recommendation",
        "abstract": "Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameters LLM for the ranking purpose. This model takes retrieval candidates in natural language format as input, with instruction which explicitly asking to select results from input candidates during inference. Our experimental results demonstrate that our solution outperforms state-of-the-art models on various sequential recommendation tasks.",
        "authors": [
            "Zheng Chen",
            "Ziyan Jiang"
        ],
        "citations": 83,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Skywork: A More Open Bilingual Foundation Model",
        "abstract": "In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves \\emph{state of the art} performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",
        "authors": [
            "Tianwen Wei",
            "Liang Zhao",
            "Lichang Zhang",
            "Bo Zhu",
            "Lijie Wang",
            "Haihua Yang",
            "Biye Li",
            "Cheng Cheng",
            "Weiwei Lü",
            "Rui Hu",
            "Chenxia Li",
            "Liu Yang",
            "Xilin Luo",
            "X. Wu",
            "Lunan Liu",
            "Wenjun Cheng",
            "Peng Cheng",
            "Jianhao Zhang",
            "Xiaoyu Zhang",
            "Lei Lin",
            "Xiaokun Wang",
            "Yutuan Ma",
            "Chuanhai Dong",
            "Yanqi Sun",
            "Yifu Chen",
            "Yongyi Peng",
            "Xiaojuan Liang",
            "Shuicheng Yan",
            "Han Fang",
            "Yahui Zhou"
        ],
        "citations": 83,
        "references": 56,
        "year": 2023
    },
    {
        "title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
        "abstract": "Graph Neural Networks (GNNs) have evolved to understand graph structures through recursive exchanges and aggregations among nodes. To enhance robustness, self-supervised learning (SSL) has become a vital tool for data augmentation. Traditional methods often depend on fine-tuning with task-specific labels, limiting their effectiveness when labeled data is scarce. Our research tackles this by advancing graph model generalization in zero-shot learning environments. Inspired by the success of large language models (LLMs), we aim to create a graph-oriented LLM capable of exceptional generalization across various datasets and tasks without relying on downstream graph data. We introduce the GraphGPT framework, which integrates LLMs with graph structural knowledge through graph instruction tuning. This framework includes a text-graph grounding component to link textual and graph structures and a dual-stage instruction tuning approach with a lightweight graph-text alignment projector. These innovations allow LLMs to comprehend complex graph structures and enhance adaptability across diverse datasets and tasks. Our framework demonstrates superior generalization in both supervised and zero-shot graph learning tasks, surpassing existing benchmarks. The open-sourced model implementation of our GraphGPT is available at https://github.com/HKUDS/GraphGPT.",
        "authors": [
            "Jiabin Tang",
            "Yuhao Yang",
            "Wei Wei",
            "Lei Shi",
            "Lixin Su",
            "Suqi Cheng",
            "Dawei Yin",
            "Chao Huang"
        ],
        "citations": 83,
        "references": 62,
        "year": 2023
    },
    {
        "title": "MemGPT: Towards LLMs as Operating Systems",
        "abstract": "Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.",
        "authors": [
            "Charles Packer",
            "Vivian Fang",
            "Shishir G. Patil",
            "Kevin Lin",
            "Sarah Wooders",
            "Joseph Gonzalez"
        ],
        "citations": 83,
        "references": 42,
        "year": 2023
    },
    {
        "title": "How to Index Item IDs for Recommendation Foundation Models",
        "abstract": "Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item as in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text and hallucinated recommendations when deciding which item(s) to recommend, creating LLM-compatible item IDs to uniquely identify each item is essential for recommendation foundation models. In this study, we systematically examine the item ID creation and indexing problem for recommendation foundation models, using P5 as an example of the backbone LLM. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as random indexing, title indexing, and independent indexing. We then propose four simple yet effective solutions, including sequential indexing, collaborative indexing, semantic (content-based) indexing, and hybrid indexing. Our study highlights the significant influence of item indexing methods on the performance of LLM-based recommendation, and our results on real-world datasets validate the effectiveness of our proposed solutions. The research also demonstrates how recent advances on language modeling and traditional IR principles such as indexing can help each other for better learning and inference. Source code and data are available at https://github.com/Wenyueh/LLM-RecSys-ID.",
        "authors": [
            "Wenyue Hua",
            "Shuyuan Xu",
            "Yingqiang Ge",
            "Yongfeng Zhang"
        ],
        "citations": 80,
        "references": 55,
        "year": 2023
    },
    {
        "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs",
        "abstract": "LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of instruction-following data. Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi-modal inputs, including image, video, and speech. Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, these LLMs give up the ability to ground specific parts of inputs, thus only constructing a coarse-grained mapping. However, explicit and informative correspondence between text and other modalities will not only improve the user experience but also help to expand the application scenario of multi-modal LLMs. Therefore, we propose BuboGPT, a multi-modal LLM with visual grounding that can perform cross-modal interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities. As a result, BuboGPT is able to point out the specific location of an object in the image, when it is generating response or description for that object. Our contributions are two-fold: 1) An off-the-shelf visual grounding module based on SAM that extracts entities in a sentence and find corresponding masks in the image. 2) A two-stage training scheme and instruction dataset to endow joint text-image-audio understanding. Our experiments show that BuboGPT achieves impressive multi-modality understanding and visual grounding abilities during the interaction with human. It performs consistently well when provided by arbitrary modality combinations (either aligned or unaligned). Our code, model and dataset are available at https://bubo-gpt.github.io .",
        "authors": [
            "Yang Zhao",
            "Zhijie Lin",
            "Daquan Zhou",
            "Zilong Huang",
            "Jiashi Feng",
            "Bingyi Kang"
        ],
        "citations": 84,
        "references": 33,
        "year": 2023
    },
    {
        "title": "Transfer Visual Prompt Generator across LLMs",
        "abstract": "While developing a new multimodal LLM (MLLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the MLLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing MLLMs for the target MLLM. In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly effective. Through extensive experiments, we demonstrate that VPGTrans helps significantly speed up the transfer learning process without compromising performance. Remarkably, it helps achieve the VPG transfer from BLIP-2 OPT$_\\text{2.7B}$ to BLIP-2 OPT$_\\text{6.7B}$ with over 10 times speed-up and 10.7% training data compared with connecting a VPG to OPT$_\\text{6.7B}$ from scratch. Further, a series of intriguing findings and potential rationales behind them are provided and discussed. Finally, we showcase the practical value of our VPGTrans approach, by customizing two novel MLLMs, including VL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs.",
        "authors": [
            "Ao Zhang",
            "Hao Fei",
            "Yuan Yao",
            "Wei Ji",
            "Li Li",
            "Zhiyuan Liu",
            "Tat-seng Chua"
        ],
        "citations": 79,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Evaluating the Moral Beliefs Encoded in LLMs",
        "abstract": "This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM\"making a choice\", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g.,\"Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g.,\"Should I stop for a pedestrian on the road?\"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g.,\"do not kill\"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models\"choose\"actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.",
        "authors": [
            "Nino Scherrer",
            "Claudia Shi",
            "Amir Feder",
            "D. Blei"
        ],
        "citations": 80,
        "references": 75,
        "year": 2023
    },
    {
        "title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
        "abstract": "Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu",
        "authors": [
            "Vitor Jeronymo",
            "L. Bonifacio",
            "H. Abonizio",
            "Marzieh Fadaee",
            "R. Lotufo",
            "Jakub Zavrel",
            "Rodrigo Nogueira"
        ],
        "citations": 81,
        "references": 10,
        "year": 2023
    },
    {
        "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
        "abstract": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, “help” from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should notlimit NLP to a small fraction of the world’s languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.",
        "authors": [
            "Ayyoob Imani",
            "Peiqin Lin",
            "Amir Hossein Kargaran",
            "Silvia Severini",
            "Masoud Jalili Sabet",
            "Nora Kassner",
            "Chunlan Ma",
            "Helmut Schmid",
            "André F. T. Martins",
            "François Yvon",
            "Hinrich Schütze"
        ],
        "citations": 80,
        "references": 91,
        "year": 2023
    },
    {
        "title": "LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs",
        "abstract": "In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing on these innovations, LLM4TS has yielded state-of-the-art results in long-term forecasting. Our model has also shown exceptional capabilities as both a robust representation learner and an effective few-shot learner, thanks to the knowledge transferred from the pre-trained LLM.",
        "authors": [
            "Ching Chang",
            "Wenjie Peng",
            "Tien-Fu Chen"
        ],
        "citations": 81,
        "references": 24,
        "year": 2023
    },
    {
        "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
        "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
        "authors": [
            "Erfan Shayegani",
            "Yue Dong",
            "Nael B. Abu-Ghazaleh"
        ],
        "citations": 80,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
        "abstract": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
        "authors": [
            "Bahare Fatemi",
            "Jonathan J. Halcrow",
            "Bryan Perozzi"
        ],
        "citations": 76,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
        "abstract": "The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors: https://github.com/MiuLab/PersonaLLM-Survey",
        "authors": [
            "Yu-Min Tseng",
            "Yu-Chao Huang",
            "Teng-Yun Hsiao",
            "Yu-Ching Hsu",
            "Jia-Yin Foo",
            "Chao-Wei Huang",
            "Yun-Nung Chen"
        ],
        "citations": 25,
        "references": 232,
        "year": 2024
    },
    {
        "title": "Leveraging Large Language Models for Decision Support in Personalized Oncology",
        "abstract": "Key Points Question Can current conversational large language models (LLMs) be used as a tool for personalized decision-making in precision oncology? Findings In this diagnostic study, treatment option identification from 4 LLMs for 10 fictional patients deviated substantially from expert recommendations. Nevertheless, LLMs correctly identified several important treatment strategies and partly provided reasonable suggestions that were not easily found by experts. Meaning These results suggest that LLMs are not yet applicable as a routine tool for aiding personalized clinical decision-making in oncology, but do improve upon existing LLM-based methods.",
        "authors": [
            "Manuela Benary",
            "Xing David Wang",
            "Max Schmidt",
            "D. Soll",
            "G. Hilfenhaus",
            "M. Nassir",
            "C. Sigler",
            "Maren Knödler",
            "U. Keller",
            "D. Beule",
            "Ulrich Keilholz",
            "Ulf Leser",
            "D. Rieke"
        ],
        "citations": 83,
        "references": 20,
        "year": 2023
    },
    {
        "title": "On the Exploitability of Instruction Tuning",
        "abstract": "Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \\textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs. Code is available at \\url{https://github.com/azshue/AutoPoison}.",
        "authors": [
            "Manli Shu",
            "Jiong Wang",
            "Chen Zhu",
            "Jonas Geiping",
            "Chaowei Xiao",
            "T. Goldstein"
        ],
        "citations": 77,
        "references": 56,
        "year": 2023
    },
    {
        "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
        "abstract": "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.",
        "authors": [
            "Xiaomeng Hu",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "citations": 75,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests",
        "abstract": "Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence. Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers’ help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on. Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students’ code and assessed the LLM-generated answers both quantitatively and qualitatively. Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57% of the time). False positives are common (40% chance for GPT-3.5). The advice that the LLMs provide on the issues is often sensible. The LLMs perform better on issues involving program logic rather than on output formatting. Model solutions are frequently provided even when the LLM is prompted not to. LLM responses to prompts in a non-English language are only slightly worse than responses to English prompts. Implications: Our results continue to highlight the utility of LLMs in programming education. At the same time, the results highlight the unreliability of LLMs: LLMs make some of the same mistakes that students do, perhaps especially when formatting output as required by automated assessment systems. Our study informs teachers interested in using LLMs as well as future efforts to customize LLMs for the needs of programming education.",
        "authors": [
            "Arto Hellas",
            "Juho Leinonen",
            "Sami Sarsa",
            "Charles Koutcheme",
            "Lilja Kujanpää",
            "Juha Sorva"
        ],
        "citations": 79,
        "references": 96,
        "year": 2023
    },
    {
        "title": "Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention",
        "abstract": "Recent large language models (LLMs) have advanced the quality of open-ended conversations with chatbots. Although LLM-driven chatbots have the potential to support public health interventions by monitoring populations at scale through empathetic interactions, their use in real-world settings is underexplored. We thus examine the case of CareCall, an open-domain chatbot that aims to support socially isolated individuals via check-up phone calls and monitoring by teleoperators. Through focus group observations and interviews with 34 people from three stakeholder groups, including the users, the teleoperators, and the developers, we found CareCall offered a holistic understanding of each individual while offloading the public health workload and helped mitigate loneliness and emotional burdens. However, our findings highlight that traits of LLM-driven chatbots led to challenges in supporting public and personal health needs. We discuss considerations of designing and deploying LLM-driven chatbots for public health intervention, including tensions among stakeholders around system expectations.",
        "authors": [
            "Eunkyung Jo",
            "Daniel A. Epstein",
            "Hyunhoon Jung",
            "Young-Ho Kim"
        ],
        "citations": 75,
        "references": 83,
        "year": 2023
    },
    {
        "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
        "abstract": "Recently, growing interest has been aroused in extending the multimodal capability of large language models (LLMs), e.g., vision-language (VL) learning, which is regarded as the next milestone of artificial general intelligence. However, existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning. In this paper, we propose a novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA). Instead of using large neural networks to connect the image encoder and LLM, MMA adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models. Meanwhile, MMA is also equipped with a routing algorithm to help LLMs achieve an automatic shift between single- and multi-modal instructions without compromising their ability of natural language understanding. To validate MMA, we apply it to a recent LLM called LLaMA and term this formed large vision-language instructed model as LaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two setups, namely multimodal science question answering and multimodal dialogue. The experimental results not only demonstrate the competitive performance and the superior training efficiency of LaVIN than existing multimodal LLMs, but also confirm its great potential as a general-purpose chatbot. More importantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4 training hours with 3.8M trainable parameters, greatly confirming the effectiveness of MMA. Our project is released at https://luogen1996.github.io/lavin.",
        "authors": [
            "Gen Luo",
            "Yiyi Zhou",
            "Tianhe Ren",
            "Shen Chen",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "citations": 76,
        "references": 53,
        "year": 2023
    },
    {
        "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
        "abstract": "Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than those generated by open-source models. While LLaMA 2 falls behind other models, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators.",
        "authors": [
            "Yapei Chang",
            "Kyle Lo",
            "Tanya Goyal",
            "Mohit Iyyer"
        ],
        "citations": 72,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal",
        "abstract": "Some argue scale is all what is needed to achieve AI, covering even causal models. We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about other SCM within their variables. We conjecture that in the cases where LLM succeed in doing causal inference, underlying was a respective meta SCM that exposed correlations between causal facts in natural language on whose data the LLM was ultimately trained. If our hypothesis holds true, then this would imply that LLMs are like parrots in that they simply recite the causal knowledge embedded in the data. Our empirical analysis provides favoring evidence that current LLMs are even weak `causal parrots.'",
        "authors": [
            "M. Zecevic",
            "Moritz Willig",
            "D. Dhami",
            "K. Kersting"
        ],
        "citations": 76,
        "references": 71,
        "year": 2023
    },
    {
        "title": "MotionGPT: Finetuned LLMs are General-Purpose Motion Generators",
        "abstract": "Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Visit our webpage at https://qiqiapink.github.io/MotionGPT/.",
        "authors": [
            "Yaqi Zhang",
            "Di Huang",
            "B. Liu",
            "Shixiang Tang",
            "Yan Lu",
            "Lu Chen",
            "Lei Bai",
            "Q. Chu",
            "Nenghai Yu",
            "Wanli Ouyang"
        ],
        "citations": 72,
        "references": 55,
        "year": 2023
    },
    {
        "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
        "abstract": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.",
        "authors": [
            "Siwon Kim",
            "Sangdoo Yun",
            "Hwaran Lee",
            "Martin Gubri",
            "Sung-Hoon Yoon",
            "Seong Joon Oh"
        ],
        "citations": 72,
        "references": 44,
        "year": 2023
    },
    {
        "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
        "abstract": "Nowadays, the quality of responses generated by different modern large language models (LLMs) is hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs for reference-free evaluation of open-ended question answering. More specifically, they use the recognized\"strongest\"LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho&MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose (1) the peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on the preferences of two answers. We conduct experiments on two benchmark datasets. We find that our approaches achieve higher accuracy and align better with human judgments. Interestingly, PR can induce a relatively accurate self-ranking of models under the anonymous setting, where each model's name is unrevealed. Our work provides space to explore evaluating models that are hard to compare for humans.",
        "authors": [
            "Ruosen Li",
            "Teerth Patel",
            "Xinya Du"
        ],
        "citations": 78,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
        "abstract": "Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.",
        "authors": [
            "Shahriar Golchin",
            "M. Surdeanu"
        ],
        "citations": 75,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Guiding Large Language Models via Directional Stimulus Prompting",
        "abstract": "We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",
        "authors": [
            "Zekun Li",
            "Baolin Peng",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao",
            "Xi Yan"
        ],
        "citations": 74,
        "references": 109,
        "year": 2023
    },
    {
        "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
        "abstract": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user’s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation.",
        "authors": [
            "John Joon Young Chung",
            "Ece Kamar",
            "Saleema Amershi"
        ],
        "citations": 82,
        "references": 56,
        "year": 2023
    },
    {
        "title": "AI chatbots not yet ready for clinical use",
        "abstract": "As large language models (LLMs) expand and become more advanced, so do the natural language processing capabilities of conversational AI, or “chatbots”. OpenAI's recent release, ChatGPT, uses a transformer-based model to enable human-like text generation and question-answering on general domain knowledge, while a healthcare-specific Large Language Model (LLM) such as GatorTron has focused on the real-world healthcare domain knowledge. As LLMs advance to achieve near human-level performances on medical question and answering benchmarks, it is probable that Conversational AI will soon be developed for use in healthcare. In this article we discuss the potential and compare the performance of two different approaches to generative pretrained transformers—ChatGPT, the most widely used general conversational LLM, and Foresight, a GPT (generative pretrained transformer) based model focused on modelling patients and disorders. The comparison is conducted on the task of forecasting relevant diagnoses based on clinical vignettes. We also discuss important considerations and limitations of transformer-based chatbots for clinical use.",
        "authors": [
            "Joshua Au Yeung",
            "Z. Kraljevic",
            "Akish Luintel",
            "Alfred Balston",
            "Esther Idowu",
            "R. Dobson",
            "J. Teo"
        ],
        "citations": 84,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Prompting GPT-3 To Be Reliable",
        "abstract": "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.",
        "authors": [
            "Chenglei Si",
            "Zhe Gan",
            "Zhengyuan Yang",
            "Shuohang Wang",
            "Jianfeng Wang",
            "Jordan L. Boyd-Graber",
            "Lijuan Wang"
        ],
        "citations": 246,
        "references": 104,
        "year": 2022
    },
    {
        "title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
        "abstract": "Large language models (LLMs) offer impressive performance in various zero-shot and few-shot tasks. However, their success in zero-shot or few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined. This paper investigates how zero-shot and few-shot performance of LLMs has changed chronologically over datasets released over time, and over LLMs released over time. Utilizing GPT-3 series models and several other recent open-sourced LLMs, and controlling for dataset difficulty, we find that datasets released prior to the LLM training data creation date perform surprisingly better than datasets released post the LLM training data creation date. This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets prior to the LLMs' training data creation date. Additionally, we utilize training data inspection, training data extraction, and a membership inference attack, which reveal further evidence of task contamination. Importantly, we find that for tasks with no possibility of task contamination, LLMs rarely demonstrate statistically significant improvements over simple majority baselines, in both zero and few-shot settings.",
        "authors": [
            "Changmao Li",
            "Jeffrey Flanigan"
        ],
        "citations": 74,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization",
        "abstract": "Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.",
        "authors": [
            "Jeonghoon Kim",
            "J. H. Lee",
            "Sungdong Kim",
            "Joonsuk Park",
            "Kang Min Yoo",
            "S. Kwon",
            "Dongsoo Lee"
        ],
        "citations": 75,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Large language models challenge the future of higher education",
        "abstract": null,
        "authors": [
            "Silvia Milano",
            "J. McGrane",
            "S. Leonelli"
        ],
        "citations": 71,
        "references": 6,
        "year": 2023
    },
    {
        "title": "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs",
        "abstract": "When we look around and perform complex tasks, how we see and selectively process what we see is crucial. How-ever, the lack of this visual search mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on important visual details, especially when handling high-resolution and visually crowded images. To address this, we introduce V*, an LLM- guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying. When combined with an MLLM, this mechanism enhances collaborative reasoning, contextual understanding, and precise visual grounding. This integration results in a new MLLM meta-architecture, named Show, sEArch, and TelL (SEAL). We further create V* Bench, a benchmark specifically designed to evaluate MLLMs in their ability to process high-resolution images and focus on visual details. Our study highlights the necessity of incorporating visual search capabilities into multimodal systems. The code is available here.",
        "authors": [
            "Penghao Wu",
            "Saining Xie"
        ],
        "citations": 68,
        "references": 60,
        "year": 2023
    },
    {
        "title": "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?",
        "abstract": "Recent advancements in GPT-4V have displayed remarkable multi-modal capabilities in processing image inputs and following open-ended instructions. Despite these advancements, there is considerable scope for enhancing open-source multi-modal LLMs, especially in terms of multi-modal understanding accuracy and instruction-following proficiency. In this paper, we conduct a comprehensive study on training GPT4-style models. We introduce Lynx a multi-modal LLM developed through a series of controlled experiments comparing various model variants. This process allowed us to identify and implement an optimal training strategy tailored for multi-modal LLMs. In addition to our model development, we propose a plug-and-play technique designed to augment the instruction-following capabilities of multi-modal LLMs. We have validated the performance of Lynx on multiple benchmarks. Results demonstrate that Lynx not only achieves strong image understanding accuracy but also excels in instruction-following tasks, paving the path for ongoing enhancements in multi-modal LLMs.",
        "authors": [
            "Yan Zeng",
            "Hanbo Zhang",
            "Jiani Zheng",
            "Jiangnan Xia",
            "Guoqiang Wei",
            "Yang Wei",
            "Yuchen Zhang",
            "Tao Kong"
        ],
        "citations": 65,
        "references": 117,
        "year": 2023
    },
    {
        "title": "Leveraging Large Language Models for Sequential Recommendation",
        "abstract": "Sequential recommendation problems have received increasing attention in research during the past few years, leading to the inception of a large variety of algorithmic approaches. In this work, we explore how large language models (LLMs), which are nowadays introducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches. Specifically, we devise and evaluate three approaches to leverage the power of LLMs in different ways. Our results from experiments on two datasets show that initializing the state-of-the-art sequential recommendation model BERT4Rec with embeddings obtained from an LLM improves NDCG by 15-20% compared to the vanilla BERT4Rec model. Furthermore, we find that a simple approach that leverages LLM embeddings for producing recommendations, can provide competitive performance by highlighting semantically related items. We publicly share the code and data of our experiments to ensure reproducibility.1",
        "authors": [
            "Jesse Harte",
            "Wouter Zorgdrager",
            "Panos Louridas",
            "Asterios Katsifodimos",
            "D. Jannach",
            "Marios Fragkoulis"
        ],
        "citations": 69,
        "references": 46,
        "year": 2023
    },
    {
        "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
        "abstract": "Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. Code will be made available at https://github.com/yuhuixu1993/qa-lora.",
        "authors": [
            "Yuhui Xu",
            "Lingxi Xie",
            "Xiaotao Gu",
            "Xin Chen",
            "Heng Chang",
            "Hengheng Zhang",
            "Zhensu Chen",
            "Xiaopeng Zhang",
            "Qi Tian"
        ],
        "citations": 69,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Knowledge Graph Prompting for Multi-Document Question Answering",
        "abstract": "The `pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or document structural relations. For graph traversal, we design an LLM-based graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for LLMs. Our code: https://github.com/YuWVandy/KG-LLM-MDQA.",
        "authors": [
            "Yu Wang",
            "Nedim Lipka",
            "Ryan A. Rossi",
            "Alexa F. Siu",
            "Ruiyi Zhang",
            "Tyler Derr"
        ],
        "citations": 69,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Planting a SEED of Vision in Large Language Model",
        "abstract": "We present SEED, an elaborate image tokenizer that empowers Large Language Models (LLMs) with the emergent ability to SEE and Draw at the same time. Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.) or generation (compared to Stable Diffusion, etc.). Despite the limitations, we remain confident in its natural capacity to unify visual and textual representations, facilitating scalable multimodal training with LLM's original recipe. In this study, we identify two crucial principles for the architecture and training of SEED that effectively ease subsequent alignment with LLMs. (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. As a result, the off-the-shelf LLM is able to perform both image-to-text and text-to-image generation by incorporating our SEED through efficient LoRA tuning. Comprehensive multimodal pretraining and instruction tuning, which may yield improved results, are reserved for future investigation. This version of SEED was trained in 5.7 days using only 64 V100 GPUs and 5M publicly available image-text pairs. Our preliminary study emphasizes the great potential of discrete visual tokens in versatile multimodal LLMs and the importance of proper image tokenizers in broader research.",
        "authors": [
            "Yuying Ge",
            "Yixiao Ge",
            "Ziyun Zeng",
            "Xintao Wang",
            "Ying Shan"
        ],
        "citations": 69,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
        "abstract": "In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}. To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools. Specifically, we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic networks, protein molecules, sequential recommender systems, social networks and knowledge graphs.",
        "authors": [
            "Jiawei Zhang"
        ],
        "citations": 67,
        "references": 71,
        "year": 2023
    },
    {
        "title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
        "abstract": "Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance. Code: https://github.com/dinobby/ReConcile",
        "authors": [
            "Justin Chih-Yao Chen",
            "Swarnadeep Saha",
            "Mohit Bansal"
        ],
        "citations": 67,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Art or Artifice? Large Language Models and the False Promise of Creativity",
        "abstract": "Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT) [64], which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose Torrance Test of Creative Writing (TTCW) to evaluate creativity as product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.",
        "authors": [
            "Tuhin Chakrabarty",
            "Philippe Laban",
            "Divyansh Agarwal",
            "S. Muresan",
            "Chien-Sheng Wu"
        ],
        "citations": 68,
        "references": 84,
        "year": 2023
    },
    {
        "title": "SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification",
        "abstract": "This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree-based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8× for distributed LLM inference and by 2.6-3.5× for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/",
        "authors": [
            "Xupeng Miao",
            "Gabriele Oliaro",
            "Zhihao Zhang",
            "Xinhao Cheng",
            "Zeyu Wang",
            "Zhengxin Zhang",
            "Rae Ying Yee Wong",
            "Alan Zhu",
            "Lijie Yang",
            "Xiaoxiang Shi",
            "Chunan Shi",
            "Zhuoming Chen",
            "Daiyaan Arfeen",
            "Reyna Abhyankar",
            "Zhihao Jia"
        ],
        "citations": 66,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Conversational Automated Program Repair",
        "abstract": "Automated Program Repair (APR) can help developers automatically generate patches for bugs. Due to the impressive performance obtained using Large Pre-Trained Language Models (LLMs) on many code related tasks, researchers have started to directly use LLMs for APR. However, prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases. To address these limitations, we propose conversational APR, a new paradigm for program repair that alternates between patch generation and validation in a conversational manner. In conversational APR, we iteratively build the input to the model by combining previously generated patches with validation feedback. As such, we leverage the long-term context window of LLMs to not only avoid generating previously incorrect patches but also incorporate validation feedback to help the model understand the semantic meaning of the program under test. We evaluate 10 different LLM including the newly developed ChatGPT model to demonstrate the improvement of conversational APR over the prior LLM for APR approach.",
        "authors": [
            "Chun Xia",
            "Lingming Zhang"
        ],
        "citations": 67,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Hypothesis Search: Inductive Reasoning with Language Models",
        "abstract": "Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding\"in context learning.\"This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be verified by running on observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100-problem subset of ARC, our automated pipeline using LLM summaries achieves 30% accuracy, outperforming the direct prompting baseline (accuracy of 17%). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to 33%. Our ablations show that both abstract hypothesis generation and concrete program representations benefit LLMs on inductive reasoning tasks.",
        "authors": [
            "Ruocheng Wang",
            "E. Zelikman",
            "Gabriel Poesia",
            "Yewen Pu",
            "Nick Haber",
            "Noah D. Goodman"
        ],
        "citations": 64,
        "references": 61,
        "year": 2023
    },
    {
        "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
        "abstract": "Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.",
        "authors": [
            "Lianghui Zhu",
            "Xinggang Wang",
            "Xinlong Wang"
        ],
        "citations": 64,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
        "abstract": "Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.",
        "authors": [
            "Ehsan Kamalloo",
            "Nouha Dziri",
            "C. Clarke",
            "Davood Rafiei"
        ],
        "citations": 64,
        "references": 51,
        "year": 2023
    },
    {
        "title": "ChatEDA: A Large Language Model Powered Autonomous Agent for EDA",
        "abstract": "The integration of a complex set of electronic design automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research article introduces ChatEDA, an autonomous agent for EDA empowered by an LLM, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the register-transfer level (RTL) to the graphic data system version II (GDSII) by effectively managing task decomposition, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other similar LLMs.",
        "authors": [
            "Haoyuan Wu",
            "Zhuolun He",
            "Xinyun Zhang",
            "Xufeng Yao",
            "Su Zheng",
            "Haisheng Zheng",
            "Bei Yu"
        ],
        "citations": 69,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Fast Distributed Inference Serving for Large Language Models",
        "abstract": "Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demands low latency for LLM inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long latency. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize latency with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi-information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactively offloads and uploads intermediate state between GPU memory and host memory for LLM inference. We build a system prototype of FastServe and experimental results show that compared to the state-of-the-art solution vLLM, FastServe improves the throughput by up to 31.4x and 17.9x under the same average and tail latency requirements, respectively.",
        "authors": [
            "Bingyang Wu",
            "Yinmin Zhong",
            "Zili Zhang",
            "Gang Huang",
            "Xuanzhe Liu",
            "Xin Jin"
        ],
        "citations": 63,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
        "abstract": "Large Language Models (LLM) are a new class of computation engines, “programmed” via prompt engineering. Researchers are still learning how to best “program” these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of “code analysis” and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly. actually helps. Prior work shows that LLM performance on code summarization benefits from embedding a few code & summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU 11Scores of 30–40 BLEU are considered “Good” to “Understandable” for natural language translation; see https://cloud.google.com/translate/automl/docs/evaluate.. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.",
        "authors": [
            "Toufique Ahmed",
            "Kunal Suresh Pai",
            "Prem Devanbu",
            "Earl T. Barr"
        ],
        "citations": 68,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Large language models show human-like content biases in transmission chain experiments",
        "abstract": "Significance Use of AI in the production of text through Large Language Models (LLMs) is widespread and growing, with potential applications in journalism, copywriting, academia, and other writing tasks. As such, it is important to understand whether text produced or summarized by LLMs exhibits biases. The studies presented here demonstrate that the LLM ChatGPT-3 reflects human biases for certain types of content in its production. The presence of these biases in LLM output has implications for its common use, as it may magnify human tendencies for content which appeals to these biases.",
        "authors": [
            "Alberto Acerbi",
            "Joseph M. Stubbersfield"
        ],
        "citations": 67,
        "references": 18,
        "year": 2023
    },
    {
        "title": "Query Rewriting for Retrieval-Augmented Large Language Models",
        "abstract": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.",
        "authors": [
            "Xinbei Ma",
            "Yeyun Gong",
            "Pengcheng He",
            "Hai Zhao",
            "Nan Duan"
        ],
        "citations": 63,
        "references": 62,
        "year": 2023
    },
    {
        "title": "ChemCrow: Augmenting large-language models with chemistry tools",
        "abstract": "Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientiﬁc applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our evaluation, including both LLM and expert human assessments, demonstrates ChemCrow’s effectiveness in automating a diverse set of chemical tasks. Surprisingly, we ﬁnd that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a signiﬁcant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers barriers for non-experts, but also fosters scientiﬁc advancement by bridging the gap between experimental and computational chemistry.",
        "authors": [
            "Andrés M Bran",
            "Sam Cox",
            "Andrew D. White",
            "Philippe Schwaller"
        ],
        "citations": 64,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective",
        "abstract": "Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs, which contributes to the development of society and human well-being. Specifically, molecule-caption translation is an important task for molecule discovery, aligning human understanding with molecular space. However, most of the existing methods heavily rely on domain experts, require excessive computational cost, or suffer from sub-optimal performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their powerful capabilities in natural language understanding, generalization, and in-context learning (ICL), which provides unprecedented opportunities to advance molecule discovery. Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges. In this work, we propose a novel LLM-based framework (MolReGPT) for molecule-caption translation, where an In-Context Few-Shot Molecule Learning paradigm is introduced to empower molecule discovery with LLMs like ChatGPT to perform their in-context learning capability without domain-specific pre-training and fine-tuning. MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to enable LLMs to learn the task knowledge from context examples. We evaluate the effectiveness of MolReGPT on molecule-caption translation, including molecule understanding and text-based molecule generation. Experimental results show that compared to fine-tuned models, MolReGPT outperforms MolT5-base and is comparable to MolT5-large without additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs via in-context learning in molecule-caption translation for advancing molecule discovery. Our work expands the scope of LLM applications, as well as providing a new paradigm for molecule discovery and design.",
        "authors": [
            "Jiatong Li",
            "Yunqing Liu",
            "Wenqi Fan",
            "Xiao Wei",
            "Hui Liu",
            "Jiliang Tang",
            "Qing Li"
        ],
        "citations": 64,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training",
        "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by using tree-search algorithms to guide multi-step reasoning. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLM. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64.",
        "authors": [
            "Xidong Feng",
            "Ziyu Wan",
            "Muning Wen",
            "Ying Wen",
            "Weinan Zhang",
            "Jun Wang"
        ],
        "citations": 64,
        "references": 61,
        "year": 2023
    },
    {
        "title": "On the Tool Manipulation Capability of Open-source Large Language Models",
        "abstract": "Recent studies on software tool manipulation with large language models (LLMs) mostly rely on closed model APIs. The industrial adoption of these models is substantially constrained due to the security and robustness risks in exposing information to closed LLM API services. In this paper, we ask can we enhance open-source LLMs to be competitive to leading closed LLM APIs in tool manipulation, with practical amount of human supervision. By analyzing common tool manipulation failures, we first demonstrate that open-source LLMs may require training with usage examples, in-context demonstration and generation style regulation to resolve failures. These insights motivate us to revisit classical methods in LLM literature, and demonstrate that we can adapt them as model alignment with programmatic data generation, system prompts and in-context demonstration retrievers to enhance open-source LLMs for tool manipulation. To evaluate these techniques, we create the ToolBench, a tool manipulation benchmark consisting of diverse software tools for real-world tasks. We demonstrate that our techniques can boost leading open-source LLMs by up to 90% success rate, showing capabilities competitive to OpenAI GPT-4 in 4 out of 8 ToolBench tasks. We show that such enhancement typically requires about one developer day to curate data for each tool, rendering a recipe with practical amount of human supervision.",
        "authors": [
            "Qiantong Xu",
            "Fenglu Hong",
            "B. Li",
            "Changran Hu",
            "Zheng Chen",
            "Jian Zhang"
        ],
        "citations": 62,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Safety Assessment of Chinese Large Language Models",
        "abstract": "With the rapid popularity of large language models such as ChatGPT and GPT-4, a growing amount of attention is paid to their safety concerns. These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information. Evaluating and enhancing their safety is particularly essential for the wide application of large language models (LLMs). To further promote the safe deployment of LLMs, we develop a Chinese LLM safety assessment benchmark. Our benchmark explores the comprehensive safety performance of LLMs from two perspectives: 8 kinds of typical safety scenarios and 6 types of more challenging instruction attacks. Our benchmark is based on a straightforward process in which it provides the test prompts and evaluates the safety of the generated responses from the evaluated model. In evaluation, we utilize the LLM's strong evaluation ability and develop it as a safety evaluator by prompting. On top of this benchmark, we conduct safety assessments and analyze 15 LLMs including the OpenAI GPT series and other well-known Chinese LLMs, where we observe some interesting findings. For example, we find that instruction attacks are more likely to expose safety issues of all LLMs. Moreover, to promote the development and deployment of safe, responsible, and ethical AI, we publicly release SafetyPrompts including 100k augmented prompts and responses by LLMs.",
        "authors": [
            "Hao Sun",
            "Zhexin Zhang",
            "Jiawen Deng",
            "Jiale Cheng",
            "Minlie Huang"
        ],
        "citations": 60,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
        "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
        "authors": [
            "Marjan Ghazvininejad",
            "Hila Gonen",
            "Luke Zettlemoyer"
        ],
        "citations": 62,
        "references": 30,
        "year": 2023
    },
    {
        "title": "On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)",
        "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) how good LLMs are by themselves in generating and validating simple plans in commonsense planning tasks (of the type that humans are generally quite good at) and (2) how good LLMs are in being a source of heuristic guidance for other agents--either AI planners or human planners--in their planning tasks. To investigate these questions in a systematic rather than anecdotal manner, we start by developing a benchmark suite based on the kinds of domains employed in the International Planning Competition. On this benchmark, we evaluate LLMs in three modes: autonomous, heuristic and human-in-the-loop. Our results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate. The heuristic and human-in-the-loop modes show slightly more promise. In addition to these results, we also make our benchmark and evaluation tools available to support investigations by research community.",
        "authors": [
            "Karthik Valmeekam",
            "S. Sreedharan",
            "Matthew Marquez",
            "Alberto Olmo Hernandez",
            "Subbarao Kambhampati"
        ],
        "citations": 62,
        "references": 51,
        "year": 2023
    },
    {
        "title": "EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education",
        "abstract": "EduChat (https://www.educhat.top/) is a large-scale language model (LLM)-based chatbot system in the education domain. Its goal is to support personalized, fair, and compassionate intelligent education, serving teachers, students, and parents. Guided by theories from psychology and education, it further strengthens educational functions such as open question answering, essay assessment, Socratic teaching, and emotional support based on the existing basic LLMs. Particularly, we learn domain-specific knowledge by pre-training on the educational corpus and stimulate various skills with tool use by fine-tuning on designed system prompts and instructions. Currently, EduChat is available online as an open-source project, with its code, data, and model parameters available on platforms (e.g., GitHub https://github.com/icalk-nlp/EduChat, Hugging Face https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its capabilities online (https://vimeo.com/851004454). This initiative aims to promote research and applications of LLMs for intelligent education.",
        "authors": [
            "Yuhao Dan",
            "Zhikai Lei",
            "Yiyang Gu",
            "Yong Li",
            "Jia-Peng Yin",
            "Jiaju Lin",
            "Linhao Ye",
            "Zhiyan Tie",
            "Yougen Zhou",
            "Yilei Wang",
            "Aimin Zhou",
            "Zeyang Zhou",
            "Qin Chen",
            "Jie Zhou",
            "Liang He",
            "Xipeng Qiu"
        ],
        "citations": 67,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Waffling around for Performance: Visual Classification with Random Words and Broad Concepts",
        "abstract": "The visual classification performance of vision-language models such as CLIP has been shown to benefit from additional semantic knowledge from large language models (LLMs) such as GPT-3. In particular, averaging over LLM-generated class descriptors, e.g. \"waffle, which has a round shape\", can notably improve generalization performance. In this work, we critically study this behavior and propose WaffleCLIP, a framework for zero-shot visual classification which simply replaces LLM-generated descriptors with random character and word descriptors. Without querying external models, we achieve comparable performance gains on a large number of visual classification tasks. This allows WaffleCLIP to both serve as a low-cost alternative, as well as a sanity check for any future LLM-based vision-language model extensions. We conduct an extensive experimental study on the impact and shortcomings of additional semantics introduced with LLM-generated descriptors, and showcase how - if available - semantic context is better leveraged by querying LLMs for high-level concepts, which we show can be done to jointly resolve potential class name ambiguities. Code is available here: https://github.com/ExplainableML/WaffleCLIP.",
        "authors": [
            "Karsten Roth",
            "Jae Myung Kim",
            "A. S. Koepke",
            "O. Vinyals",
            "C. Schmid",
            "Zeynep Akata"
        ],
        "citations": 56,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
        "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.",
        "authors": [
            "Shima Rahimi Moghaddam",
            "C. Honey"
        ],
        "citations": 65,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine",
        "abstract": null,
        "authors": [
            "Thomas Savage",
            "Ashwin Nayak",
            "Roberta Gallo",
            "E. Rangan",
            "Jonathan H. Chen"
        ],
        "citations": 66,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?",
        "abstract": "There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.",
        "authors": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "Subbarao Kambhampati"
        ],
        "citations": 66,
        "references": 13,
        "year": 2023
    },
    {
        "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples",
        "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples {without} using Natural Questions or MS MARCO to train %question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.",
        "authors": [
            "Zhuyun Dai",
            "Vincent Zhao",
            "Ji Ma",
            "Yi Luan",
            "Jianmo Ni",
            "Jing Lu",
            "A. Bakalov",
            "Kelvin Guu",
            "Keith B. Hall",
            "Ming-Wei Chang"
        ],
        "citations": 189,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Retrieval meets Long Context Large Language Models",
        "abstract": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.",
        "authors": [
            "Peng Xu",
            "Wei Ping",
            "Xianchao Wu",
            "Lawrence C. McAfee",
            "Chen Zhu",
            "Zihan Liu",
            "Sandeep Subramanian",
            "Evelina Bakhturina",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "citations": 60,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Can Artificial Intelligence Pass the American Board of Orthopaedic Surgery Examination? Orthopaedic Residents Versus ChatGPT",
        "abstract": "Abstract Background Advances in neural networks, deep learning, and artificial intelligence (AI) have progressed recently. Previous deep learning AI has been structured around domain-specific areas that are trained on dataset-specific areas of interest that yield high accuracy and precision. A new AI model using large language models (LLM) and nonspecific domain areas, ChatGPT (OpenAI), has gained attention. Although AI has demonstrated proficiency in managing vast amounts of data, implementation of that knowledge remains a challenge. Questions/purposes (1) What percentage of Orthopaedic In-Training Examination questions can a generative, pretrained transformer chatbot (ChatGPT) answer correctly? (2) How does that percentage compare with results achieved by orthopaedic residents of different levels, and if scoring lower than the 10th percentile relative to 5th-year residents is likely to correspond to a failing American Board of Orthopaedic Surgery score, is this LLM likely to pass the orthopaedic surgery written boards? (3) Does increasing question taxonomy affect the LLM’s ability to select the correct answer choices? Methods This study randomly selected 400 of 3840 publicly available questions based on the Orthopaedic In-Training Examination and compared the mean score with that of residents who took the test over a 5-year period. Questions with figures, diagrams, or charts were excluded, including five questions the LLM could not provide an answer for, resulting in 207 questions administered with raw score recorded. The LLM’s answer results were compared with the Orthopaedic In-Training Examination ranking of orthopaedic surgery residents. Based on the findings of an earlier study, a pass-fail cutoff was set at the 10th percentile. Questions answered were then categorized based on the Buckwalter taxonomy of recall, which deals with increasingly complex levels of interpretation and application of knowledge; comparison was made of the LLM’s performance across taxonomic levels and was analyzed using a chi-square test. Results ChatGPT selected the correct answer 47% (97 of 207) of the time, and 53% (110 of 207) of the time it answered incorrectly. Based on prior Orthopaedic In-Training Examination testing, the LLM scored in the 40th percentile for postgraduate year (PGY) 1s, the eighth percentile for PGY2s, and the first percentile for PGY3s, PGY4s, and PGY5s; based on the latter finding (and using a predefined cutoff of the 10th percentile of PGY5s as the threshold for a passing score), it seems unlikely that the LLM would pass the written board examination. The LLM’s performance decreased as question taxonomy level increased (it answered 54% [54 of 101] of Tax 1 questions correctly, 51% [18 of 35] of Tax 2 questions correctly, and 34% [24 of 71] of Tax 3 questions correctly; p = 0.034). Conclusion Although this general-domain LLM has a low likelihood of passing the orthopaedic surgery board examination, testing performance and knowledge are comparable to that of a first-year orthopaedic surgery resident. The LLM's ability to provide accurate answers declines with increasing question taxonomy and complexity, indicating a deficiency in implementing knowledge. Clinical Relevance Current AI appears to perform better at knowledge and interpretation-based inquires, and based on this study and other areas of opportunity, it may become an additional tool for orthopaedic learning and education.",
        "authors": [
            "Zachary C Lum"
        ],
        "citations": 64,
        "references": 15,
        "year": 2023
    },
    {
        "title": "Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities",
        "abstract": "Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, including split learning/inference, parameter-efficient fine-tuning, quantization, and parameter-sharing inference, to facilitate the efficient deployment of LLMs. This article serves as a position paper for thoroughly identifying the motivation, challenges, and pathway for empowering LLMs at the 6G edge.",
        "authors": [
            "Zhengyi Lin",
            "Guanqiao Qu",
            "Qiyuan Chen",
            "Xianhao Chen",
            "Zhe Chen",
            "Kaibin Huang"
        ],
        "citations": 63,
        "references": 23,
        "year": 2023
    },
    {
        "title": "VideoLLM: Modeling Video Sequence with Large Language Models",
        "abstract": "With the exponential growth of video data, there is an urgent need for automated technology to analyze and comprehend video content. However, existing video understanding models are often task-specific and lack a comprehensive capability of handling diverse tasks. The success of large language models (LLMs) like GPT has demonstrated their impressive abilities in sequence causal reasoning. Building upon this insight, we propose a novel framework called VideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMs from natural language processing (NLP) for video sequence understanding. VideoLLM incorporates a carefully designed Modality Encoder and Semantic Translator, which convert inputs from various modalities into a unified token sequence. This token sequence is then fed into a decoder-only LLM. Subsequently, with the aid of a simple task head, our VideoLLM yields an effective unified framework for different kinds of video understanding tasks. To evaluate the efficacy of VideoLLM, we conduct extensive experiments using multiple LLMs and fine-tuning methods. We evaluate our VideoLLM on eight tasks sourced from four different datasets. The experimental results demonstrate that the understanding and reasoning capabilities of LLMs can be effectively transferred to video understanding tasks. We release the code at https://github.com/cg1177/VideoLLM.",
        "authors": [
            "Guo Chen",
            "Yin-Dong Zheng",
            "Jiahao Wang",
            "Jilan Xu",
            "Yifei Huang",
            "Junting Pan",
            "Yi Wang",
            "Yali Wang",
            "Y. Qiao",
            "Tong Lu",
            "Limin Wang"
        ],
        "citations": 63,
        "references": 103,
        "year": 2023
    },
    {
        "title": "ChipGPT: How far are we from natural language hardware design",
        "abstract": "As large language models (LLMs) like ChatGPT exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction. To estimate the potential of the hardware design process assisted by LLMs, this work attempts to demonstrate an automated design environment that explores LLMs to generate hardware logic designs from natural language specifications. To realize a more accessible and efficient chip development flow, we present a scalable four-stage zero-code logic design framework based on LLMs without retraining or finetuning. At first, the demo, ChipGPT, begins by generating prompts for the LLM, which then produces initial Verilog programs. Second, an output manager corrects and optimizes these programs before collecting them into the final design space. Eventually, ChipGPT will search through this space to select the optimal design under the target metrics. The evaluation sheds some light on whether LLMs can generate correct and complete hardware logic designs described by natural language for some specifications. It is shown that ChipGPT improves programmability, and controllability, and shows broader design optimization space compared to prior work and native LLMs alone.",
        "authors": [
            "Kaiyan Chang",
            "Y. Wang",
            "Haimeng Ren",
            "Mengdi Wang",
            "Shengwen Liang",
            "Yinhe Han",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "citations": 56,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study",
        "abstract": "Evaluating the quality of generated text is a challenging task in natural language processing. This difﬁculty arises from the inherent complexity and diversity of text. Recently, OpenAI’s ChatGPT, a powerful large language model (LLM), has garnered signiﬁcant attention due to its impressive performance in various tasks. Therefore, we present this report to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods based on ChatGPT or similar LLMs. The experimental results prove that ChatGPT is capable to evaluate text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts using ChatGPT may lead to suboptimal results. We hope this report will provide valuable insights into selecting appropriate meth-ods for evaluating text quality with LLMs such as ChatGPT. We have released the used data 1 .",
        "authors": [
            "Yi Chen",
            "Rui Wang",
            "Haiyun Jiang",
            "Shuming Shi",
            "Rui-Lan Xu"
        ],
        "citations": 58,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Batch Prompting: Efficient Inference with Large Language Model APIs",
        "abstract": "Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Further analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Moreover, batch prompting can be applied across different reasoning methods using LLMs. Our code can be found at the site https://github.com/xlang-ai/batch-prompting.",
        "authors": [
            "Zhoujun Cheng",
            "Jungo Kasai",
            "Tao Yu"
        ],
        "citations": 57,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Privacy-Preserving Prompt Tuning for Large Language Model Services",
        "abstract": "Prompt tuning provides an efficient way for users to customize Large Language Models (LLMs) with their private data in the emerging LLM service scenario. However, the sensitive nature of private data brings the need for privacy preservation in LLM service customization. Based on prompt tuning, we propose Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy guarantees for LLM services. \\textsc{rapt} adopts a local privacy setting, allowing users to privatize their data locally with local differential privacy. As prompt tuning performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task, allowing LLMs to learn better task-dependent representations. Despite the simplicity of our framework, experiments show that RAPT achieves competitive performance across tasks while providing privacy guarantees against adversaries.",
        "authors": [
            "Yansong Li",
            "Zhixing Tan",
            "Yang Liu"
        ],
        "citations": 56,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors",
        "abstract": "Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task. We hypothesize that instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE's low incidence in instruction-tuning datasets, making up less than 1% of all tasks (Wang et al., 2022). To address this limitation, we propose QA4RE, a framework that aligns RE with question answering (QA), a predominant task in instruction-tuning datasets. Comprehensive zero-shot RE experiments over four datasets with two series of instruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework consistently improves LLM performance, strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot baselines by a large margin. Additionally, we provide thorough experiments and discussions to show the robustness, few-shot effectiveness, and strong transferability of our QA4RE framework. This work illustrates a promising way of adapting LLMs to challenging and underrepresented tasks by aligning these tasks with more common instruction-tuning tasks like QA.",
        "authors": [
            "Kai Zhang",
            "Bernal Jimenez Gutierrez",
            "Yu Su"
        ],
        "citations": 59,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Graphologue: Exploring Large Language Model Responses with Interactive Diagrams",
        "abstract": "Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented ability to synthesize text responses to diverse user questions. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can interact with the diagrams to flexibly adjust the graphical presentation and to submit context-specific prompts to obtain more information. Utilizing diagrams, Graphologue enables graphical, non-linear dialogues between humans and LLMs, facilitating information exploration, organization, and comprehension.",
        "authors": [
            "Peiling Jiang",
            "Jude Rayan",
            "Steven W. Dow",
            "Haijun Xia"
        ],
        "citations": 58,
        "references": 107,
        "year": 2023
    },
    {
        "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
        "abstract": "Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play.",
        "authors": [
            "Zelai Xu",
            "Chao Yu",
            "Fei Fang",
            "Yu Wang",
            "Yi Wu"
        ],
        "citations": 58,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning",
        "abstract": "Abstract Motivation Creating knowledge bases and ontologies is a time consuming task that relies on manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrarily complex nested knowledge schemas. Results Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against an LLM to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for matched elements. We present examples of applying SPIRES in different domains, including extraction of food recipes, multi-species cellular signaling pathways, disease treatments, multi-step drug mechanisms, and chemical to disease relationships. Current SPIRES accuracy is comparable to the mid-range of existing Relation Extraction methods, but greatly surpasses an LLM’s native capability of grounding entities with unique identifiers. SPIRES has the advantage of easy customization, flexibility, and, crucially, the ability to perform new tasks in the absence of any new training data. This method supports a general strategy of leveraging the language interpreting capabilities of LLMs to assemble knowledge bases, assisting manual knowledge curation and acquisition while supporting validation with publicly-available databases and ontologies external to the LLM. Availability and implementation SPIRES is available as part of the open source OntoGPT package: https://github.com/monarch-initiative/ontogpt.",
        "authors": [
            "J. Caufield",
            "Harshad B. Hegde",
            "Vincent Emonet",
            "N. Harris",
            "marcin p. joachimiak",
            "N. Matentzoglu",
            "Hyeongsik Kim",
            "S. Moxon",
            "J. Reese",
            "M. Haendel",
            "Peter N. Robinson",
            "C. Mungall"
        ],
        "citations": 62,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling",
        "abstract": "Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention. Due to their powerful emergent abilities, recent LLMs are considered as a possible alternative to structured knowledge bases like knowledge graphs (KGs). However, while LLMs are proficient at learning probabilistic language patterns and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance in generating texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes enhancing LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs’ factual reasoning ability, opening up new avenues for LLM research.",
        "authors": [
            "Lin F. Yang",
            "Hongyang Chen",
            "Zhao Li",
            "Xiao Ding",
            "Xindong Wu"
        ],
        "citations": 61,
        "references": 150,
        "year": 2023
    },
    {
        "title": "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
        "abstract": "With large language models (LLMs) achieving remarkable breakthroughs in NLP domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on three real-world public datasets to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension. To be highlighted, with only less than 10% training samples, few-shot ReLLa can outperform traditional CTR models that are trained on the entire training set (e.g., DCNv2, DIN, SIM).",
        "authors": [
            "Jianghao Lin",
            "Rongjie Shan",
            "Chenxu Zhu",
            "Kounianhua Du",
            "Bo Chen",
            "Shigang Quan",
            "Ruiming Tang",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "citations": 61,
        "references": 111,
        "year": 2023
    },
    {
        "title": "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling",
        "abstract": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.",
        "authors": [
            "Kolby Nottingham",
            "Prithviraj Ammanabrolu",
            "Alane Suhr",
            "Yejin Choi",
            "Hannaneh Hajishirzi",
            "Sameer Singh",
            "Roy Fox"
        ],
        "citations": 61,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs",
        "abstract": "Large language models (LLMs) are increasingly becoming all-powerful and pervasive via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased, behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously before deployment. Existing auditing tools use either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and interview research experts in safe and fair AI, to build upon the auditing tool: AdaTest [36], which is powered by a generative LLM. Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of AdaTest++, the augmented tool, we conduct user studies with participants auditing two commercial language models: OpenAI’s GPT-3 and Azure’s sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis testing. Further, with our tool, users identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown in formal audits and also those previously under-reported.",
        "authors": [
            "Charvi Rastogi",
            "Marco Tulio Ribeiro",
            "Nicholas King",
            "Saleema Amershi"
        ],
        "citations": 56,
        "references": 51,
        "year": 2023
    },
    {
        "title": "WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia",
        "abstract": "This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus. WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment. Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM. WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4, while receiving significantly higher user ratings and more favorable comments.",
        "authors": [
            "Sina J. Semnani",
            "Violet Z. Yao",
            "He Zhang",
            "M. Lam"
        ],
        "citations": 58,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models",
        "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and $95\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time ($240\\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.",
        "authors": [
            "Robin Staab",
            "Mark Vero",
            "Mislav Balunovi'c",
            "Martin T. Vechev"
        ],
        "citations": 58,
        "references": 62,
        "year": 2023
    },
    {
        "title": "WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia",
        "abstract": "This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus. WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment. Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM. WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4, while receiving significantly higher user ratings and more favorable comments.",
        "authors": [
            "Sina J. Semnani",
            "Violet Z. Yao",
            "He Zhang",
            "M. Lam"
        ],
        "citations": 58,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs",
        "abstract": "Large language models (LLMs) are increasingly becoming all-powerful and pervasive via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased, behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously before deployment. Existing auditing tools use either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and interview research experts in safe and fair AI, to build upon the auditing tool: AdaTest [36], which is powered by a generative LLM. Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of AdaTest++, the augmented tool, we conduct user studies with participants auditing two commercial language models: OpenAI’s GPT-3 and Azure’s sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis testing. Further, with our tool, users identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown in formal audits and also those previously under-reported.",
        "authors": [
            "Charvi Rastogi",
            "Marco Tulio Ribeiro",
            "Nicholas King",
            "Saleema Amershi"
        ],
        "citations": 56,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models",
        "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and $95\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time ($240\\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.",
        "authors": [
            "Robin Staab",
            "Mark Vero",
            "Mislav Balunovi'c",
            "Martin T. Vechev"
        ],
        "citations": 58,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Language Models are Realistic Tabular Data Generators",
        "abstract": "Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data's characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes.",
        "authors": [
            "V. Borisov",
            "Kathrin Seßler",
            "Tobias Leemann",
            "Martin Pawelczyk",
            "Gjergji Kasneci"
        ],
        "citations": 175,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Emotional intelligence of Large Language Models",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI. This test is an objective, performance-driven, and text-based evaluation, which requires evaluating complex emotions in realistic scenarios, providing a consistent assessment for both human and LLM capabilities. With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average Emotional Quotient (EQ) scores, with GPT-4 exceeding 89% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not rely on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/",
        "authors": [
            "Xuena Wang",
            "Xueting Li",
            "Zi Yin",
            "Yue Wu",
            "Liu Jia Department of PsychologyTsinghua Laboratory of Brain",
            "Intelligence",
            "Tsinghua University",
            "Departmentof Psychology",
            "Renmin University"
        ],
        "citations": 57,
        "references": 50,
        "year": 2023
    },
    {
        "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
        "abstract": "Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.",
        "authors": [
            "Tim Dettmers",
            "Luke Zettlemoyer"
        ],
        "citations": 181,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems",
        "abstract": "In the rapidly evolving landscape of artificial intelligence (AI), generative large language models (LLMs) stand at the forefront, revolutionizing how we interact with our data. However, the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput. This survey addresses the imperative need for efficient LLM serving methodologies from a machine learning system (MLSys) research perspective, standing at the crux of advanced AI innovations and practical system optimizations. We provide in-depth analysis, covering a spectrum of solutions, ranging from cutting-edge algorithmic modifications to groundbreaking changes in system designs. The survey aims to provide a comprehensive understanding of the current state and future directions in efficient LLM serving, offering valuable insights for researchers and practitioners in overcoming the barriers of effective LLM deployment, thereby reshaping the future of AI.",
        "authors": [
            "Xupeng Miao",
            "Gabriele Oliaro",
            "Zhihao Zhang",
            "Xinhao Cheng",
            "Hongyi Jin",
            "Tianqi Chen",
            "Zhihao Jia"
        ],
        "citations": 58,
        "references": 265,
        "year": 2023
    },
    {
        "title": "GraphText: Graph Reasoning in Text Space",
        "abstract": "Large Language Models (LLMs) have gained the ability to assimilate human knowledge and facilitate natural language interactions with both humans and other LLMs. However, despite their impressive achievements, LLMs have not made significant advancements in the realm of graph machine learning. This limitation arises because graphs encapsulate distinct relational data, making it challenging to transform them into natural language that LLMs understand. In this paper, we bridge this gap with a novel framework, GraphText, that translates graphs into natural language. GraphText derives a graph-syntax tree for each graph that encapsulates both the node attributes and inter-node relationships. Traversal of the tree yields a graph text sequence, which is then processed by an LLM to treat graph tasks as text generation tasks. Notably, GraphText offers multiple advantages. It introduces training-free graph reasoning: even without training on graph data, GraphText with ChatGPT can achieve on par with, or even surpassing, the performance of supervised-trained graph neural networks through in-context learning (ICL). Furthermore, GraphText paves the way for interactive graph reasoning, allowing both humans and LLMs to communicate with the model seamlessly using natural language. These capabilities underscore the vast, yet-to-be-explored potential of LLMs in the domain of graph machine learning.",
        "authors": [
            "Jianan Zhao",
            "Le Zhuo",
            "Yikang Shen",
            "Meng Qu",
            "Kai Liu",
            "Michael M. Bronstein",
            "Zhaocheng Zhu",
            "Jian Tang"
        ],
        "citations": 57,
        "references": 58,
        "year": 2023
    },
    {
        "title": "CodePlan: Repository-level Coding using LLMs and Planning",
        "abstract": "\n Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as\n repository-level coding\n tasks. Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it. CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs. We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2–97 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines. CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them. We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan.\n",
        "authors": [
            "Ramakrishna Bairi",
            "Atharv Sonwane",
            "Aditya Kanade",
            "C. VageeshD",
            "Arun Shankar Iyer",
            "Suresh Parthasarathy",
            "S. Rajamani",
            "B. Ashok",
            "Shashank Shet"
        ],
        "citations": 59,
        "references": 91,
        "year": 2023
    },
    {
        "title": "PromptChainer: Chaining Large Language Model Prompts through Visual Programming",
        "abstract": "While LLMs have made it possible to rapidly prototype new ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains – a key step to lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We find from pilot studies that users need support transforming data between steps of a chain, as well as debugging the chain at multiple granularities. To address these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four designers and developers, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to even more complex tasks, as well as supporting low-fi chain prototyping.",
        "authors": [
            "Tongshuang Sherry Wu",
            "Ellen Jiang",
            "Aaron Donsbach",
            "J. Gray",
            "A. Molina",
            "Michael Terry",
            "Carrie J. Cai"
        ],
        "citations": 181,
        "references": 23,
        "year": 2022
    },
    {
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "abstract": "Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single\"generic\"user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g.,\"should we raise taxes on the rich?\"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs (>70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (>65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.",
        "authors": [
            "Michiel A. Bakker",
            "Martin Chadwick",
            "Hannah Sheahan",
            "Michael Henry Tessler",
            "Lucy Campbell-Gillingham",
            "Jan Balaguer",
            "Nat McAleese",
            "Amelia Glaese",
            "John Aslanides",
            "M. Botvinick",
            "C. Summerfield"
        ],
        "citations": 183,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs’ capability to learn to reason in context.",
        "authors": [
            "Boshi Wang",
            "Sewon Min",
            "Xiang Deng",
            "Jiaming Shen",
            "You Wu",
            "Luke Zettlemoyer",
            "Huan Sun"
        ],
        "citations": 182,
        "references": 40,
        "year": 2022
    },
    {
        "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
        "abstract": "Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.",
        "authors": [
            "S. A. Jacobs",
            "Masahiro Tanaka",
            "Chengming Zhang",
            "Minjia Zhang",
            "L. Song",
            "Samyam Rajbhandari",
            "Yuxiong He"
        ],
        "citations": 54,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
        "abstract": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.",
        "authors": [
            "Yang Deng",
            "Wenqiang Lei",
            "Hongru Wang",
            "Tat-seng Chua"
        ],
        "citations": 54,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Talking about Large Language Models",
        "abstract": "Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us.",
        "authors": [
            "M. Shanahan"
        ],
        "citations": 186,
        "references": 43,
        "year": 2022
    },
    {
        "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering",
        "abstract": "Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs.",
        "authors": [
            "Sheng Liu",
            "Haotian Ye",
            "Lei Xing",
            "James Y. Zou"
        ],
        "citations": 51,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
        "abstract": "Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model’s lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA-2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.",
        "authors": [
            "Swarnadeep Saha",
            "Omer Levy",
            "Asli Celikyilmaz",
            "Mohit Bansal",
            "Jason Weston",
            "Xian Li"
        ],
        "citations": 51,
        "references": 56,
        "year": 2023
    },
    {
        "title": "LLMs4OL: Large Language Models for Ontology Learning",
        "abstract": "We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \\textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.",
        "authors": [
            "Hamed Babaei Giglou",
            "J. D’Souza",
            "S. Auer"
        ],
        "citations": 51,
        "references": 72,
        "year": 2023
    },
    {
        "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
        "abstract": "Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps.",
        "authors": [
            "Daixuan Cheng",
            "Shaohan Huang",
            "Junyu Bi",
            "Yu-Wei Zhan",
            "Jianfeng Liu",
            "Yujing Wang",
            "Hao Sun",
            "Furu Wei",
            "Denvy Deng",
            "Qi Zhang"
        ],
        "citations": 53,
        "references": 65,
        "year": 2023
    },
    {
        "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
        "abstract": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For \\textbf{ability modeling}, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For \\textbf{data}, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For \\textbf{evaluation criteria}, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate $28$ open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing LLMs and knowledge-related systems.",
        "authors": [
            "Jifan Yu",
            "Xiaozhi Wang",
            "Shangqing Tu",
            "S. Cao",
            "Daniel Zhang-li",
            "Xin Lv",
            "Hao Peng",
            "Zijun Yao",
            "Xiaohan Zhang",
            "Hanming Li",
            "Chun-yan Li",
            "Zheyuan Zhang",
            "Yushi Bai",
            "Yantao Liu",
            "Amy Xin",
            "Nianyi Lin",
            "Kaifeng Yun",
            "Linlu Gong",
            "Jianhui Chen",
            "Zhili Wu",
            "Y. Qi",
            "Weikai Li",
            "Yong Guan",
            "Kaisheng Zeng",
            "Ji Qi",
            "Hailong Jin",
            "Jinxin Liu",
            "Yuxian Gu",
            "Yu Gu",
            "Yuan Yao",
            "Ning Ding",
            "Lei Hou",
            "Zhiyuan Liu",
            "Bin Xu",
            "Jie Tang",
            "Juanzi Li"
        ],
        "citations": 55,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
        "abstract": "With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans. In other words, with better abstractive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations.",
        "authors": [
            "Chenhui Shen",
            "Liying Cheng",
            "Yang You",
            "Lidong Bing"
        ],
        "citations": 52,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training",
        "abstract": "Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective -- Black-Box Prompt Optimization (BPO) -- to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO.",
        "authors": [
            "Jiale Cheng",
            "Xiao Liu",
            "Kehan Zheng",
            "Pei Ke",
            "Hongning Wang",
            "Yuxiao Dong",
            "Jie Tang",
            "Minlie Huang"
        ],
        "citations": 51,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Large Language Models as Evolutionary Optimizers",
        "abstract": "Evolutionary algorithms (EAs) have achieved remarkable success in tackling complex combinatorial optimization problems. However, EAs often demand carefully-designed operators with the aid of domain expertise to achieve satisfactory performance. In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers. The main advantage is that it requires minimal domain knowledge and human efforts, as well as no additional training of the model. This approach is referred to as LLM-driven EA (LMEA). Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions. Then, LMEA evaluates these new solutions and include them into the population for the next generation. LMEA is equipped with a self-adaptation mechanism that controls the temperature of the LLM. This enables it to balance between exploration and exploitation and prevents the search from getting stuck in local optima. We investigate the power of LMEA on the classical traveling salesman problems (TSPs) widely used in combinatorial optimization research. Notably, the results show that LMEA performs competitively to traditional heuristics in finding high-quality solutions on TSP instances with up to 20 nodes. Additionally, we also study the effectiveness of LLM-driven crossover/mutation and the self- adaptation mechanism in evolutionary search. In summary, our results reveal the great potentials of LLMs as evolutionary optimizers for solving combinatorial problems. We hope our research shall inspire future explorations on LLM-driven EAs for complex optimization challenges.",
        "authors": [
            "Shengcai Liu",
            "Caishun Chen",
            "Xinghua Qu",
            "Ke Tang",
            "Y. Ong"
        ],
        "citations": 55,
        "references": 65,
        "year": 2023
    },
    {
        "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
        "abstract": "Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.",
        "authors": [
            "Xiaoxuan Wang",
            "Ziniu Hu",
            "Pan Lu",
            "Yanqiao Zhu",
            "Jieyu Zhang",
            "Satyen Subramaniam",
            "Arjun R. Loomba",
            "Shichang Zhang",
            "Yizhou Sun",
            "Wei Wang"
        ],
        "citations": 53,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Connecting Speech Encoder and Large Language Model for ASR",
        "abstract": "The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.",
        "authors": [
            "Wenyi Yu",
            "Changli Tang",
            "Guangzhi Sun",
            "Xianzhao Chen",
            "T. Tan",
            "Wei Li",
            "Lu Lu",
            "Zejun Ma",
            "Chao Zhang"
        ],
        "citations": 52,
        "references": 38,
        "year": 2023
    },
    {
        "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
        "abstract": "Instruction tuning has become the de facto method to equip large language models (LLMs) with the ability of following user instructions. Usually, hundreds of thousands or millions of instruction-following pairs are employed to fine-tune the foundation LLMs. Recently, some studies show that a small number of high-quality instruction data is enough. However, how to select appropriate instruction data for a given LLM is still an open problem. To address this problem, in this paper we present a model-oriented data selection (MoDS) approach, which selects instruction data based on a new criteria considering three aspects: quality, coverage and necessity. First, our approach utilizes a quality evaluation model to filter out the high-quality subset from the original instruction dataset, and then designs an algorithm to further select from the high-quality subset a seed instruction dataset with good coverage. The seed dataset is applied to fine-tune the foundation LLM to obtain an initial instruction-following LLM. Finally, we develop a necessity evaluation model to find out the instruction data which are performed badly in the initial instruction-following LLM and consider them necessary instructions to further improve the LLMs. In this way, we can get a small high-quality, broad-coverage and high-necessity subset from the original instruction datasets. Experimental results show that, the model fine-tuned with 4,000 instruction pairs selected by our approach could perform better than the model fine-tuned with the full original dataset which includes 214k instruction data.",
        "authors": [
            "Qianlong Du",
            "Chengqing Zong",
            "Jiajun Zhang"
        ],
        "citations": 55,
        "references": 35,
        "year": 2023
    },
    {
        "title": "LLM360: Towards Fully Transparent Open-Source LLMs",
        "abstract": "The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.",
        "authors": [
            "Zhengzhong Liu",
            "Aurick Qiao",
            "W. Neiswanger",
            "Hongyi Wang",
            "Bowen Tan",
            "Tianhua Tao",
            "Junbo Li",
            "Yuqi Wang",
            "Suqi Sun",
            "Omkar Pangarkar",
            "Richard Fan",
            "Yi Gu",
            "Victor Miller",
            "Yonghao Zhuang",
            "Guowei He",
            "Haonan Li",
            "Fajri Koto",
            "Liping Tang",
            "Nikhil Ranjan",
            "Zhiqiang Shen",
            "Xuguang Ren",
            "Roberto Iriondo",
            "Cun Mu",
            "Zhiting Hu",
            "Mark Schulze",
            "Preslav Nakov",
            "Timothy Baldwin",
            "Eric P. Xing"
        ],
        "citations": 55,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
        "abstract": "While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g.,\"A\") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.",
        "authors": [
            "Joshua Robinson",
            "Christopher Rytting",
            "D. Wingate"
        ],
        "citations": 155,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes",
        "abstract": "\n A long standing goal in the data management community is developing systems that input documents and output queryable tables without user effort. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using the in-context learning abilities of large language models (LLMs). We propose and evaluate Evaporate, a prototype system powered by LLMs. We identify two strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than directly processing each document with the LLM. To improve quality while maintaining low cost, we propose an extended implementation, Evaporate-Code+, which achieves better quality than direct extraction. Our insight is to generate many candidate functions and ensemble their extractions using weak supervision. Evaporate-Code+ outperforms the state-of-the art systems using a\n sublinear\n pass over the documents with the LLM. This equates to a 110X reduction in the number of documents the LLM needs to process across our 16 real-world evaluation settings.\n",
        "authors": [
            "Simran Arora",
            "Brandon Yang",
            "Sabri Eyuboglu",
            "A. Narayan",
            "Andrew Hojel",
            "Immanuel Trummer",
            "Christopher Ré"
        ],
        "citations": 55,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Level Generation Through Large Language Models",
        "abstract": "Large Language Models (LLMs) are powerful tools, capable of leveraging their training on natural language to write stories, generate code, and answer questions. But can they generate functional video game levels? Game levels, with their complex functional constraints and spatial relationships in more than one dimension, are very different from the kinds of data an LLM typically sees during training. Datasets of game levels are also hard to come by, potentially taxing the abilities of these data-hungry models. We investigate the use of LLMs to generate levels for the game Sokoban, finding that LLMs are indeed capable of doing so, and that their performance scales dramatically with dataset size. We also perform preliminary experiments on controlling LLM level generators and discuss promising areas for future work.",
        "authors": [
            "G. Todd",
            "Sam Earle",
            "Muhammad Umair Nasir",
            "M. Green",
            "Julian Togelius"
        ],
        "citations": 55,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves",
        "abstract": "Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive comparison between RaR and the popular Chain-of-Thought (CoT) methods, both theoretically and empirically. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities. Data and codes are available at https://github.com/uclaml/Rephrase-and-Respond.",
        "authors": [
            "Yihe Deng",
            "Weitong Zhang",
            "Zixiang Chen",
            "Quanquan Gu"
        ],
        "citations": 54,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Large Language Models for Generative Recommendation: A Survey and Visionary Discussions",
        "abstract": "Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields, e.g., recommender systems (RS). However, most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor), which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages, such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods, and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS tasks. We hope that this survey can provide the context and guidance needed to explore this interesting and emerging topic.",
        "authors": [
            "Lei Li",
            "Yongfeng Zhang",
            "Dugang Liu",
            "L. Chen"
        ],
        "citations": 53,
        "references": 112,
        "year": 2023
    },
    {
        "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
        "abstract": "Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",
        "authors": [
            "Karthik Valmeekam",
            "Alberto Olmo",
            "S. Sreedharan",
            "Subbarao Kambhampati"
        ],
        "citations": 147,
        "references": 36,
        "year": 2022
    },
    {
        "title": "Large Language Models are Better Reasoners with Self-Verification",
        "abstract": "Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification.",
        "authors": [
            "Yixuan Weng",
            "Minjun Zhu",
            "Fei Xia",
            "Bin Li",
            "Shizhu He",
            "Kang Liu",
            "Jun Zhao"
        ],
        "citations": 134,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
        "abstract": "Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io.",
        "authors": [
            "Boyuan Chen",
            "F. Xia",
            "Brian Ichter",
            "Kanishka Rao",
            "K. Gopalakrishnan",
            "M. Ryoo",
            "Austin Stone",
            "Daniel Kappler"
        ],
        "citations": 162,
        "references": 44,
        "year": 2022
    },
    {
        "title": "The ChatGPT (Generative Artificial Intelligence) Revolution Has Made Artificial Intelligence Approachable for Medical Professionals",
        "abstract": "In November 2022, OpenAI publicly launched its large language model (LLM), ChatGPT, and reached the milestone of having over 100 million users in only 2 months. LLMs have been shown to be useful in a myriad of health care–related tasks and processes. In this paper, I argue that attention to, public access to, and debate about LLMs have initiated a wave of products and services using generative artificial intelligence (AI), which had previously found it hard to attract physicians. This paper describes what AI tools have become available since the beginning of the ChatGPT revolution and contemplates how it they might change physicians’ perceptions about this breakthrough technology.",
        "authors": [
            "B. Meskó"
        ],
        "citations": 53,
        "references": 11,
        "year": 2023
    },
    {
        "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
        "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.",
        "authors": [
            "Minzhi Li",
            "Taiwei Shi",
            "Caleb Ziems",
            "Min-Yen Kan",
            "Nancy F. Chen",
            "Zhengyuan Liu",
            "Diyi Yang"
        ],
        "citations": 50,
        "references": 53,
        "year": 2023
    },
    {
        "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
        "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
        "authors": [
            "Xi Ye",
            "Greg Durrett"
        ],
        "citations": 145,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering",
        "abstract": "Large-scale language models (LLMs), such as ChatGPT, are capable of generating human-like responses for various downstream tasks, such as task-oriented dialogues and question answering. However, applying LLMs to medical domains remains challenging due to their inability to leverage domain-speciﬁc knowledge. In this study, we present the Large-scale Language Models Augmented with Medical Textbooks (LLM-AMT) , which integrates authoritative medical textbooks as the cornerstone of its design, enhancing its proﬁciency in the specialized domain through plug-and-play modules, comprised of a Hybrid Textbook Re-triever, supplemented by the Query Augmenter and the LLM Reader. Experimental evaluation on three open-domain medical question-answering tasks reveals a substantial enhancement in both the professionalism and accuracy of the LLM responses when utilizing LLM-AMT, exhibiting an improvement ranging from 11.4% to 13.2%. Despite being 100 × smaller, we found that medical textbooks as the retrieval corpus serves as a more valuable external knowledge source than Wikipedia in the medical domain. Our experiments show that textbook augmentation results in a performance improvement ranging from 9.7% to 12.2% over Wikipedia augmentation.",
        "authors": [
            "Yubo Wang",
            "Xueguang Ma",
            "Wenhu Chen"
        ],
        "citations": 50,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Large Language Models can Learn Rules",
        "abstract": "When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often generate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on relational reasoning, numerical reasoning and concept learning problems show that HtT improves existing prompting methods, with an absolute gain of 10-30% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.",
        "authors": [
            "Zhaocheng Zhu",
            "Yuan Xue",
            "Xinyun Chen",
            "Denny Zhou",
            "Jian Tang",
            "D. Schuurmans",
            "Hanjun Dai"
        ],
        "citations": 50,
        "references": 85,
        "year": 2023
    },
    {
        "title": "Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation",
        "abstract": "Recently, large language models (LLMs) have shown great potential in recommender systems, either improving existing recommendation models or serving as the backbone. However, there exists a large semantic gap between LLMs and recommender systems, since items to be recommended are often indexed by discrete identifiers (item ID) out of the LLM's vocabulary. In essence, LLMs capture language semantics while recommender systems imply collaborative semantics, making it difficult to sufficiently leverage the model capacity of LLMs for recommendation. To address this challenge, in this paper, we propose a new LLM-based recommendation model called LC-Rec, which can better integrate language and collaborative semantics for recommender systems. Our approach can directly generate items from the entire item set for recommendation, without relying on candidate items. Specifically, we make two major contributions in our approach. For item indexing, we design a learning-based vector quantization method with uniform semantic mapping, which can assign meaningful and non-conflicting IDs (called item indices) for items. For alignment tuning, we propose a series of specially designed tuning tasks to enhance the integration of collaborative semantics in LLMs. Our fine-tuning tasks enforce LLMs to deeply integrate language and collaborative semantics (characterized by the learned item indices), so as to achieve an effective adaptation to recommender systems. Extensive experiments demonstrate the effectiveness of our method, showing that our approach can outperform a number of competitive baselines including traditional recommenders and existing LLM-based recommenders. Our code is available at https://github.com/RUCAIBox/LC-Rec/.",
        "authors": [
            "Bowen Zheng",
            "Yupeng Hou",
            "Hongyu Lu",
            "Yu Chen",
            "Wayne Xin Zhao",
            "Ji-rong Wen"
        ],
        "citations": 46,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Label-free Node Classification on Graphs with Large Language Models (LLMS)",
        "abstract": "In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with a cost less than 1 dollar.",
        "authors": [
            "Zhikai Chen",
            "Haitao Mao",
            "Hongzhi Wen",
            "Haoyu Han",
            "Wei-dong Jin",
            "Haiyang Zhang",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "citations": 49,
        "references": 68,
        "year": 2023
    },
    {
        "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
        "abstract": "Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the\"better\"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.",
        "authors": [
            "Ryan Liu",
            "Nihar B. Shah"
        ],
        "citations": 46,
        "references": 92,
        "year": 2023
    },
    {
        "title": "LLMs cannot find reasoning errors, but can correct them!",
        "abstract": "While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023b; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we show that poor self-correction performance stems from LLMs' inability to find logical mistakes, rather than their ability to correct a known mistake. Firstly, we benchmark several state-of-the-art LLMs on their mistake-finding ability and demonstrate that they generally struggle with the task, even in highly objective, unambiguous cases. Secondly, we test the correction abilities of LLMs -- separately from mistake finding -- using a backtracking setup that feeds ground truth mistake location information to the model. We show that this boosts downstream task performance across our 5 reasoning tasks, indicating that LLMs' correction abilities are robust. Finally, we show that it is possible to obtain mistake location information without ground truth labels or in-domain training data. We train a small classifier with out-of-domain data, which exhibits stronger mistake-finding performance than prompting a large model. We release our dataset of LLM-generated logical mistakes, BIG-Bench Mistake, to enable further research into locating LLM reasoning mistakes.",
        "authors": [
            "Gladys Tyen",
            "Hassan Mansoor",
            "Peter Chen",
            "Tony Mak",
            "Victor Carbune"
        ],
        "citations": 46,
        "references": 39,
        "year": 2023
    },
    {
        "title": "CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation",
        "abstract": "Leveraging Large Language Models as Recommenders (LLMRec) has gained significant attention and introduced fresh perspectives in user preference modeling. Existing LLMRec approaches prioritize text semantics, usually neglecting the valuable collaborative information from user-item interactions in recommendations. While these text-emphasizing approaches excel in cold-start scenarios, they may yield sub-optimal performance in warm-start situations. In pursuit of superior recommendations for both cold and warm start scenarios, we introduce CoLLM, an innovative LLMRec methodology that seamlessly incorporates collaborative information into LLMs for recommendation. CoLLM captures collaborative information through an external traditional model and maps it to the input token embedding space of LLM, forming collaborative embeddings for LLM usage. Through this external integration of collaborative information, CoLLM ensures effective modeling of collaborative information without modifying the LLM itself, providing the flexibility to employ various collaborative information modeling techniques. Extensive experiments validate that CoLLM adeptly integrates collaborative information into LLMs, resulting in enhanced recommendation performance. We release the code and data at https://github.com/zyang1580/CoLLM.",
        "authors": [
            "Yang Zhang",
            "Fuli Feng",
            "Jizhi Zhang",
            "Keqin Bao",
            "Qifan Wang",
            "Xiangnan He"
        ],
        "citations": 48,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Aligning Language Models to User Opinions",
        "abstract": "An important aspect of developing LLMs that interact with humans is to align models' behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by Pew Research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling both user opinions as well as user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. In addition to the typical approach of prompting LLMs with demographics and ideology, we discover that utilizing the most relevant past opinions from individual users enables the model to predict user opinions more accurately.",
        "authors": [
            "EunJeong Hwang",
            "Bodhisattwa Prasad Majumder",
            "Niket Tandon"
        ],
        "citations": 45,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
        "abstract": "Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.",
        "authors": [
            "Yunpeng Huang",
            "Jingwei Xu",
            "Zixu Jiang",
            "Junyu Lai",
            "Zenan Li",
            "Yuan Yao",
            "Taolue Chen",
            "Lijuan Yang",
            "Zhou Xin",
            "Xiaoxing Ma"
        ],
        "citations": 43,
        "references": 255,
        "year": 2023
    },
    {
        "title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models",
        "abstract": "Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.",
        "authors": [
            "Iman Mirzadeh",
            "Keivan Alizadeh-Vahid",
            "Sachin Mehta",
            "C. C. D. Mundo",
            "Oncel Tuzel",
            "Golnoosh Samei",
            "Mohammad Rastegari",
            "Mehrdad Farajtabar"
        ],
        "citations": 44,
        "references": 84,
        "year": 2023
    },
    {
        "title": "ADaPT: As-Needed Decomposition and Planning with Language Models",
        "abstract": "Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.",
        "authors": [
            "Archiki Prasad",
            "Alexander Koller",
            "Mareike Hartmann",
            "Peter Clark",
            "Ashish Sabharwal",
            "Mohit Bansal",
            "Tushar Khot"
        ],
        "citations": 43,
        "references": 86,
        "year": 2023
    },
    {
        "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
        "abstract": "Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual cases and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. Furthermore, we show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4.",
        "authors": [
            "Yuanhao Wu",
            "Juno Zhu",
            "Siliang Xu",
            "Kashun Shum",
            "Cheng Niu",
            "Randy Zhong",
            "Juntong Song",
            "Tong Zhang"
        ],
        "citations": 44,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
        "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as\"thoughts\". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called\"Everything of Thoughts\"(XoT) to defy the law of\"Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions. We evaluate XoT on several challenging multi-solution problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches. Notably, XoT can yield multiple solutions with just one LLM call, showcasing its remarkable proficiency in addressing complex problems across diverse domains.",
        "authors": [
            "Ruomeng Ding",
            "Chaoyun Zhang",
            "Lu Wang",
            "Yong Xu",
            "Ming-Jie Ma",
            "Wei Zhang",
            "Si Qin",
            "S. Rajmohan",
            "Qingwei Lin",
            "Dongmei Zhang"
        ],
        "citations": 43,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Automated Annotation with Generative AI Requires Validation",
        "abstract": "Generative large language models (LLMs) can be a powerful tool for augmenting text annotation procedures, but their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans. To this end, we outline a workflow to harness the annotation potential of LLMs in a principled, efficient way. Using GPT-4, we validate this approach by replicating 27 annotation tasks across 11 datasets from recent social science articles in high-impact journals. We find that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task, which reinforces the necessity to validate on a task-by-task basis. We make available easy-to-use software designed to implement our workflow and streamline the deployment of LLMs for automated annotation.",
        "authors": [
            "Nicholas Pangakis",
            "Samuel Wolken",
            "Neil Fasching"
        ],
        "citations": 47,
        "references": 22,
        "year": 2023
    },
    {
        "title": "BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT",
        "abstract": "Large language models (LLMs) have performed well in providing general and extensive health suggestions in single-turn conversations, exemplified by systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning. In real-world medical consultations, doctors usually employ a series of iterative inquiries to comprehend the patient's condition thoroughly, enabling them to provide effective and personalized suggestions subsequently, which can be defined as chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health suggestions polished by ChatGPT. Experimental results demonstrate that the proposed BianQue can simultaneously balance the capabilities of both questioning and health suggestions, which will help promote the research and application of LLMs in the field of proactive health.",
        "authors": [
            "Yirong Chen",
            "Zhenyu Wang",
            "Xiaofen Xing",
            "Huimin Zheng",
            "Zhipei Xu",
            "Kai Fang",
            "Junhong Wang",
            "Sihang Li",
            "Jieling Wu",
            "Qi Liu",
            "Xiangmin Xu"
        ],
        "citations": 46,
        "references": 14,
        "year": 2023
    },
    {
        "title": "Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",
        "abstract": "Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good advisor for SLMs by providing multi-perspective instructive rationales. To instantiate this proposal, we design an adaptive rationale guidance network for fake news detection (ARG), in which SLMs selectively acquire insights on news analysis from the LLMs' rationales. We further derive a rationale-free version of ARG by distillation, namely ARG-D, which services cost-sensitive scenarios without inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and ARG-D outperform three types of baseline methods, including SLM-based, LLM-based, and combinations of small and large language models.",
        "authors": [
            "Beizhe Hu",
            "Qiang Sheng",
            "Juan Cao",
            "Yuhui Shi",
            "Yang Li",
            "Danding Wang",
            "Peng Qi"
        ],
        "citations": 47,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation",
        "abstract": "Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a\"Game-of-Thoughts\". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. Additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. Specifically, the first-order allows an LLM agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. After integrating ReCon with different LLMs, extensive experiment results from the Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around deceptive information without extra fine-tuning and data. Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.",
        "authors": [
            "Shenzhi Wang",
            "Chang Liu",
            "Zilong Zheng",
            "Siyuan Qi",
            "Shuo Chen",
            "Qisen Yang",
            "Andrew Zhao",
            "Chaofei Wang",
            "Shiji Song",
            "Gao Huang"
        ],
        "citations": 46,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Autonomous GIS: the next-generation AI-powered GIS",
        "abstract": "ABSTRACT Large Language Models (LLMs), such as ChatGPT, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval. By adopting LLM as the reasoning core, we introduce Autonomous GIS as an AI-powered geographic information system (GIS) that leverages the LLM's general abilities in natural language understanding, reasoning, and coding for addressing spatial problems with automatic spatial data collection, analysis, and visualization. We envision that autonomous GIS will need to achieve five autonomous goals: self-generating, self-organizing, self-verifying, self-executing, and self-growing. We developed a prototype system called LLM-Geo using the GPT-4 API, demonstrating what an autonomous GIS looks like and how it delivers expected results without human intervention using three case studies. For all case studies, LLM-Geo returned accurate results, including aggregated numbers, graphs, and maps.. Although still in its infancy and lacking several important modules such as logging and code testing, LLM-Geo demonstrates a potential path toward the next-generation AI-powered GIS. We advocate for the GIScience community to devote more efforts to the research and development of autonomous GIS, making spatial analysis easier, faster, and more accessible to a broader audience.",
        "authors": [
            "Zhenlong Li",
            "H. Ning"
        ],
        "citations": 48,
        "references": 96,
        "year": 2023
    },
    {
        "title": "Inference with Reference: Lossless Acceleration of Large Language Models",
        "abstract": "We propose LLMA, an LLM accelerator to losslessly speed up Large Language Model (LLM) inference with references. LLMA is motivated by the observation that there are abundant identical text spans between the decoding result by an LLM and the reference that is available in many real world scenarios (e.g., retrieved documents). LLMA first selects a text span from the reference and copies its tokens to the decoder and then efficiently checks the tokens' appropriateness as the decoding result in parallel within one decoding step. The improved computational parallelism allows LLMA to achieve over 2x speed-up for LLMs with identical generation results as greedy decoding in many practical generation scenarios where significant overlap between in-context reference and outputs exists (e.g., search engines and multi-turn conversations).",
        "authors": [
            "Nan Yang",
            "Tao Ge",
            "Liang Wang",
            "Binxing Jiao",
            "Daxin Jiang",
            "Linjun Yang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "citations": 45,
        "references": 14,
        "year": 2023
    },
    {
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "abstract": "As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and>50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference. We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++ creatively proposes: (1) Asynchronized softmax with unified max value. FlashDecoding++ introduces a unified max value technique for different partial softmax computations to avoid synchronization. (2) Flat GEMM optimization with double buffering. FlashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource adaptation. FlashDecoding++ heuristically optimizes dataflow using different hardware resource considering input dynamics. Due to the versatility of optimizations in FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on both NVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also achieves an average speedup of 1.37x compared to state-of-the-art LLM inference engines on mainstream LLMs.",
        "authors": [
            "Ke Hong",
            "Guohao Dai",
            "Jiaming Xu",
            "Qiuli Mao",
            "Xiuhong Li",
            "Jun Liu",
            "Kangdi Chen",
            "Yuhan Dong",
            "Yu Wang"
        ],
        "citations": 50,
        "references": 32,
        "year": 2023
    },
    {
        "title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
        "abstract": "Recently, Large Language Models (LLMs) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets has been conducted. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art models when they were fine-tuned only on the training set of these datasets. This suggests that pre-training on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with the performance of different LLMs may vary depending on the task. While their performance is still quite poor in comparison to the biomedical models that were fine-tuned on large training sets, our findings demonstrate that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.",
        "authors": [
            "Israt Jahan",
            "Md Tahmid Rahman Laskar",
            "Chun Peng",
            "Jimmy X. Huang"
        ],
        "citations": 44,
        "references": 126,
        "year": 2023
    },
    {
        "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
        "abstract": "While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction.",
        "authors": [
            "Yixin Liu",
            "A. R. Fabbri",
            "Jiawen Chen",
            "Yilun Zhao",
            "Simeng Han",
            "Shafiq R. Joty",
            "Pengfei Liu",
            "Dragomir R. Radev",
            "Chien-Sheng Wu",
            "Arman Cohan"
        ],
        "citations": 43,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Bias of AI-generated content: an examination of news produced by large language models",
        "abstract": null,
        "authors": [
            "Xiao Fang",
            "Shangkun Che",
            "Minjia Mao",
            "Hongzhe Zhang",
            "Ming Zhao",
            "Xiaohang Zhao"
        ],
        "citations": 44,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Retrieve Anything To Augment Large Language Models",
        "abstract": "Large language models (LLMs) face significant challenges stemming from their inherent limitations in knowledge, memory, alignment, and action. These challenges cannot be addressed by LLMs alone, but should rely on assistance from the external world, such as knowledge base, memory store, demonstration examples, and tools. Retrieval augmentation stands as a vital mechanism for bridging the gap between LLMs and the external assistance. However, conventional methods encounter two pressing issues. On the one hand, the general-purpose retrievers are not properly optimized for the retrieval augmentation of LLMs. On the other hand, the task-specific retrievers lack the required versatility, hindering their performance across the diverse retrieval augmentation scenarios. In this work, we present a novel approach, the LLM-Embedder, which comprehensively supports the diverse retrieval augmentation needs of LLMs with one unified embedding model. Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference. To address this challenge, we systematically optimize our training methodology. This includes reward formulation based on LLMs' feedback, the stabilization of knowledge distillation, multi-task fine-tuning with explicit instructions, and homogeneous in-batch negative sampling. These optimization strategies contribute to the outstanding empirical performance of the LLM-Embedder. Notably, it yields remarkable enhancements in retrieval augmentation for LLMs, surpassing both general-purpose and task-specific retrievers in various evaluation scenarios. Our checkpoint and source code are publicly available at https://github.com/FlagOpen/FlagEmbedding.",
        "authors": [
            "Peitian Zhang",
            "Shitao Xiao",
            "Zheng Liu",
            "Zhicheng Dou",
            "Jian-Yun Nie"
        ],
        "citations": 43,
        "references": 102,
        "year": 2023
    },
    {
        "title": "On Learning to Summarize with Large Language Models as References",
        "abstract": "Recent studies have found that summaries generated by large language models (LLMs) are favored by human annotators over the original reference summaries in commonly used summarization datasets. Therefore, we study an LLM-as-reference learning setting for smaller text summarization models to investigate whether their performance can be substantially improved. To this end, we use LLMs as both oracle summary generators for standard supervised fine-tuning and oracle summary evaluators for efficient contrastive learning that leverages the LLMs’ supervision signals. We conduct comprehensive experiments with source news articles and find that (1) summarization models trained under the LLM-as-reference setting achieve significant performance improvement in both LLM and human evaluations; (2) contrastive learning outperforms standard supervised fine-tuning under both low and high resource settings. Our experimental results also enable a meta-analysis of LLMs’ summary evaluation capacities under a challenging setting, showing that LLMs are not well-aligned with human evaluators. Particularly, our expert human evaluation reveals remaining nuanced performance gaps between LLMs and our fine-tuned models, which LLMs fail to capture. Thus, we call for further studies into both the potential and challenges of using LLMs in summarization model development.",
        "authors": [
            "Yixin Liu",
            "Alexander R. Fabbri",
            "Pengfei Liu",
            "Dragomir R. Radev",
            "Arman Cohan"
        ],
        "citations": 44,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
        "abstract": "Large Language Models are cognitively biased judges. Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
        "authors": [
            "Ryan Koo",
            "Minhwa Lee",
            "Vipul Raheja",
            "Jong Inn Park",
            "Zae Myung Kim",
            "Dongyeop Kang"
        ],
        "citations": 44,
        "references": 75,
        "year": 2023
    },
    {
        "title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
        "abstract": "Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/Microsoft/SmartPlay",
        "authors": [
            "Yue Wu",
            "Xuan Tang",
            "Tom M. Mitchell",
            "Yuanzhi Li"
        ],
        "citations": 50,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
        "abstract": "Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.",
        "authors": [
            "Jiashuo Sun",
            "Chengjin Xu",
            "Lumingyuan Tang",
            "Sai Wang",
            "Chen Lin",
            "Yeyun Gong",
            "Lionel M. Ni",
            "H. Shum",
            "Jian Guo"
        ],
        "citations": 40,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Where Would I Go Next? Large Language Models as Human Mobility Predictors",
        "abstract": "Accurate human mobility prediction underpins many important applications across a variety of domains, including epidemic modelling, transport planning, and emergency responses. Due to the sparsity of mobility data and the stochastic nature of people's daily activities, achieving precise predictions of people's locations remains a challenge. While recently developed large language models (LLMs) have demonstrated superior performance across numerous language-related tasks, their applicability to human mobility studies remains unexplored. Addressing this gap, this article delves into the potential of LLMs for human mobility prediction tasks. We introduce a novel method, LLM-Mob, which leverages the language understanding and reasoning capabilities of LLMs for analysing human mobility data. We present concepts of historical stays and context stays to capture both long-term and short-term dependencies in human movement and enable time-aware prediction by using time information of the prediction target. Additionally, we design context-inclusive prompts that enable LLMs to generate more accurate predictions. Comprehensive evaluations of our method reveal that LLM-Mob excels in providing accurate and interpretable predictions, highlighting the untapped potential of LLMs in advancing human mobility prediction techniques. We posit that our research marks a significant paradigm shift in human mobility modelling, transitioning from building complex domain-specific models to harnessing general-purpose LLMs that yield accurate predictions through language instructions. The code for this work is available at https://github.com/xlwang233/LLM-Mob.",
        "authors": [
            "Xinglei Wang",
            "Meng Fang",
            "Zichao Zeng",
            "Tao Cheng"
        ],
        "citations": 41,
        "references": 46,
        "year": 2023
    },
    {
        "title": "R2GenGPT: Radiology Report Generation with Frozen LLMs",
        "abstract": "Large Language Models (LLMs) have consistently showcased remarkable generalization capabilities when applied to various language tasks. Nonetheless, harnessing the full potential of LLMs for Radiology Report Generation (R2Gen) still presents a challenge, stemming from the inherent disparity in modality between LLMs and the R2Gen task. To bridge this gap effectively, we propose R2GenGPT, which is a novel solution that aligns visual features with the word embedding space of LLMs using an efficient visual alignment module. This innovative approach empowers the previously static LLM to seamlessly integrate and process image information, marking a step forward in optimizing R2Gen performance. R2GenGPT offers the following benefits. First, it attains state-of-the-art (SOTA) performance by training only the lightweight visual alignment module while freezing all the parameters of LLM. Second, it exhibits high training efficiency, as it requires the training of an exceptionally minimal number of parameters while achieving rapid convergence. By employing delta tuning, our model only trains 5M parameters (which constitute just 0.07\\% of the total parameter count) to achieve performance close to the SOTA levels. Our code is available at https://github.com/wang-zhanyu/R2GenGPT.",
        "authors": [
            "Zhanyu Wang",
            "Lingqiao Liu",
            "Lei Wang",
            "Luping Zhou"
        ],
        "citations": 40,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Understanding the Effectiveness of Large Language Models in Code Translation",
        "abstract": "—Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are actively exploring their potential to automate code translation, i.e., generating code in target PL from its equivalent in another PL. The pre-requisite for advancing the state of LLM-based code translation is to understand their limitations. To that end, we present a large-scale empirical study to investigate the ability of LLMs, including general LLMs and code LLMs, for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our analysis involves the translation of 1,700 code samples from three distinct benchmarks and real-world projects, revealing LLMs are yet to be reliably used to automate code translation—with incorrect translations ranging from 52.7% to 97.9% across the studied LLMs. Further manual investigation of unsuccessful translations among all PLs identifies 14 root causes for translation bugs. Based on the insights from the empirical study, we propose a prompt-crafting approach to provide additional context for LLMs, improving the performance of LLM-based code translation by 5.5% on average across different PLs, LLMs, and benchmarks. Our study is the first of its kind, in terms of its scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our collected extensive dataset—consisting of 1,700 code samples written in five PLs with 10K+ tests, 43K+ translated code, 1,725 manually labeled bugs, and 1,365 bug-fix pairs generated using LLMs –can help drive research in this area.",
        "authors": [
            "Rangeet Pan",
            "Ali Reza Ibrahimzada",
            "R. Krishna",
            "Divya Sankar",
            "Lambert Pouguem Wassi",
            "Michele Merler",
            "Boris Sobolev",
            "Raju Pavuluri",
            "S. Sinha",
            "Reyhaneh Jabbarvand"
        ],
        "citations": 41,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Red Teaming Language Model Detectors with Language Models",
        "abstract": "The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.",
        "authors": [
            "Zhouxing Shi",
            "Yihan Wang",
            "Fan Yin",
            "Xiangning Chen",
            "Kai-Wei Chang",
            "Cho-Jui Hsieh"
        ],
        "citations": 41,
        "references": 47,
        "year": 2023
    },
    {
        "title": "MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records",
        "abstract": "The ability of large language models (LLMs) to follow natural language instructions with human-level fluency suggests many opportunities in healthcare to reduce administrative burden and improve quality of care. However, evaluating LLMs on realistic text generation tasks for healthcare remains challenging. Existing question answering datasets for electronic health record (EHR) data fail to capture the complexity of information needs and documentation burdens experienced by clinicians. To address these challenges, we introduce MedAlign, a benchmark dataset of 983 natural language instructions for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes clinician-written reference responses for 303 instructions, and provides 276 longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality of each LLM response. We found high error rates, ranging from 35% (GPT-4) to 68% (MPT-7B-Instruct), and 8.3% drop in accuracy moving from 32k to 2k context lengths for GPT-4. Finally, we report correlations between clinician rankings and automated natural language generation metrics as a way to rank LLMs without human review. We make MedAlign available under a research data use agreement to enable LLM evaluations on tasks aligned with clinician needs and preferences.",
        "authors": [
            "S. Fleming",
            "A. Lozano",
            "W. Haberkorn",
            "Jenelle A. Jindal",
            "E. Reis",
            "Rahul Thapa",
            "Louis Blankemeier",
            "Julian Z. Genkins",
            "E. Steinberg",
            "Ashwin Nayak",
            "Birju S. Patel",
            "Chia-Chun Chiang",
            "A. Callahan",
            "Zepeng Huo",
            "S. Gatidis",
            "S. Adams",
            "Oluseyi Fayanju",
            "Shreya Shah",
            "Thomas Savage",
            "Ethan Goh",
            "A. Chaudhari",
            "N. Aghaeepour",
            "Christopher D. Sharp",
            "M. Pfeffer",
            "Percy Liang",
            "Jonathan H. Chen",
            "K. Morse",
            "E. Brunskill",
            "Jason Alan Fries",
            "N. Shah"
        ],
        "citations": 41,
        "references": 70,
        "year": 2023
    },
    {
        "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances",
        "abstract": "The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them cheaply. This paper aims to reduce the monetary cost for serving LLMs by leveraging preemptible GPU instances on modern clouds, which offer accesses to spare GPU resources at a much cheaper price than regular instances but may be preempted by the cloud provider at any time. Serving LLMs on preemptible instances requires addressing challenges induced by frequent instance preemptions and the necessity of migrating instances to handle the preemptions. This paper presents SpotServe, the first distributed LLM serving system on preemptible instances. Several key techniques of SpotServe realize fast and reliable serving of generative LLMs on cheap preemptible instances. First, SpotServe dynamically adapts the LLM parallelization configuration for dynamic instance availability and fluctuating workload, while balancing the trade-off among the overall throughput, inference latency and monetary costs. Second, to minimize the cost of migrating instances for dynamic reparallelization, the task of migrating instances is formulated as a bipartite graph matching problem in SpotServe, which uses the Kuhn-Munkres algorithm to identify an optimal migration plan that minimizes communication cost. Finally, to take advantage of the grace period offered by modern cloud platforms, we introduce stateful inference recovery, a new inference mechanism that commits inference progress at a much finer granularity and allows SpotServe to cheaply resume inference upon preemption. We evaluate SpotServe on real spot instance preemption traces and various popular LLMs and show that SpotServe can reduce the P99 tail latency by 2.4 - 9.1× compared with the best existing LLM serving systems. We also show that SpotServe can leverage the price advantage of preemptive instances, saving 54% monetary cost compared with only using on-demand instances. The code is publicly available at: https://github.com/Hsword/SpotServe.",
        "authors": [
            "Xupeng Miao",
            "Chunan Shi",
            "Jiangfei Duan",
            "Xiaoli Xi",
            "Dahua Lin",
            "Bin Cui",
            "Zhihao Jia"
        ],
        "citations": 42,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
        "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.",
        "authors": [
            "Yuheng Huang",
            "Jiayang Song",
            "Zhijie Wang",
            "Huaming Chen",
            "Lei Ma"
        ],
        "citations": 47,
        "references": 96,
        "year": 2023
    },
    {
        "title": "Taken out of context: On measuring situational awareness in LLMs",
        "abstract": "We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: https://github.com/AsaCooperStickland/situational-awareness-evals.",
        "authors": [
            "Lukas Berglund",
            "Asa Cooper Stickland",
            "Mikita Balesni",
            "Max Kaufmann",
            "Meg Tong",
            "Tomasz Korbak",
            "Daniel Kokotajlo",
            "Owain Evans"
        ],
        "citations": 46,
        "references": 63,
        "year": 2023
    },
    {
        "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
        "abstract": "Large language models~(LLMs) are instruction followers, but it can be challenging to find the best instruction for different situations, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. On each iteration of the proposed method, which we call InstructZero, a soft prompt is converted into an instruction using the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, and the performance is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our results show that InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks. Our code and data are publicly available at https://github.com/Lichang-Chen/InstructZero.",
        "authors": [
            "Lichang Chen",
            "Jiuhai Chen",
            "T. Goldstein",
            "Heng Huang",
            "Tianyi Zhou"
        ],
        "citations": 36,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community",
        "abstract": "The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop's outcomes, including the rethinking of IR's core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.",
        "authors": [
            "Qingyao Ai",
            "Ting Bai",
            "Zhao Cao",
            "Yi Chang",
            "Jiawei Chen",
            "Zhumin Chen",
            "Zhiyong Cheng",
            "Shoubin Dong",
            "Zhicheng Dou",
            "Fuli Feng",
            "Shengling Gao",
            "J. Guo",
            "Xiangnan He",
            "Yanyan Lan",
            "Chenliang Li",
            "Yiqun Liu",
            "Ziyu Lyu",
            "Weizhi Ma",
            "Jun Ma",
            "Z. Ren",
            "Pengjie Ren",
            "Zhiqiang Wang",
            "Min Wang",
            "Jirong Wen",
            "Lei Wu",
            "Xin Xin",
            "Jun Xu",
            "Dawei Yin",
            "Peng Zhang",
            "Fan Zhang",
            "Wei-na Zhang",
            "M. Zhang",
            "Xiaofei Zhu"
        ],
        "citations": 49,
        "references": 126,
        "year": 2023
    },
    {
        "title": "Video Understanding with Large Language Models: A Survey",
        "abstract": "With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.",
        "authors": [
            "Yunlong Tang",
            "Jing Bi",
            "Siting Xu",
            "Luchuan Song",
            "Susan Liang",
            "Teng Wang",
            "Daoan Zhang",
            "Jie An",
            "Jingyang Lin",
            "Rongyi Zhu",
            "A. Vosoughi",
            "Chao Huang",
            "Zeliang Zhang",
            "Feng Zheng",
            "Jianguo Zhang",
            "Ping Luo",
            "Jiebo Luo",
            "Chenliang Xu"
        ],
        "citations": 49,
        "references": 394,
        "year": 2023
    },
    {
        "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges",
        "abstract": "Misinformation such as fake news and rumors is a serious threat for information ecosystems and public trust. The emergence of large language models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double‐edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emerging question is: can we utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM‐generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions, respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM‐generated misinformation.",
        "authors": [
            "Canyu Chen",
            "Kai Shu"
        ],
        "citations": 49,
        "references": 563,
        "year": 2023
    },
    {
        "title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
        "abstract": "Recent advances in Foundation Models such as Large Language Models (LLMs) have propelled them to the forefront of Recommender Systems (RS). Despite their utility, there is a growing concern that LLMs might inadvertently perpetuate societal stereotypes, resulting in unfair recommendations. Since fairness is critical for RS as many users take it for decision-making and demand fulfillment, this paper focuses on user-side fairness for LLM-based recommendation where the users may require a recommender system to be fair on specific sensitive features such as gender or age. In this paper, we dive into the extent of unfairness exhibited by LLM-based recommender models based on both T5 and LLaMA backbones, and discuss appropriate methods for promoting equitable treatment of users in LLM-based recommendation models. We introduce a novel Counterfactually-Fair-Prompt (CFP) method towards Unbiased Foundation mOdels (UFO) for fairness-aware LLM-based recommendation. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and compared with both matching-based and sequential-based fairness-aware recommendation models. Results show that CFP achieves better recommendation performance with a high level of fairness.",
        "authors": [
            "Wenyue Hua",
            "Yingqiang Ge",
            "Shuyuan Xu",
            "Jianchao Ji",
            "Yongfeng Zhang"
        ],
        "citations": 40,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Can LLMs Effectively Leverage Graph Structural Information: When and Why",
        "abstract": "This paper studies Large Language Models (LLMs) augmented with structured data–particularly graphs–a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks with textual features. To address the “when” question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the “why” questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively related to the local homophily ratio of the node 1 .",
        "authors": [
            "Jin Huang",
            "Xingjian Zhang",
            "Qiaozhu Mei",
            "Jiaqi Ma"
        ],
        "citations": 39,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs. Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for developing future collaboration methods. Codes and data are available at https://github.com/Waste-Wood/FORD",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Yixin Cao",
            "Ting Liu",
            "Bing Qin"
        ],
        "citations": 40,
        "references": 40,
        "year": 2023
    },
    {
        "title": "A Survey on Fairness in Large Language Models",
        "abstract": "Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.",
        "authors": [
            "Yingji Li",
            "Mengnan Du",
            "Rui Song",
            "Xin Wang",
            "Y. Wang"
        ],
        "citations": 45,
        "references": 179,
        "year": 2023
    },
    {
        "title": "Large Language Model−Based Chatbot vs Surgeon-Generated Informed Consent Documentation for Common Procedures",
        "abstract": "Key Points Question Can a large language model (LLM)-based chatbot outperform surgeons in generating readable, accurate, and complete procedure-specific risks, benefits, and alternatives (RBAs) for use in informed consent? Findings This cross-sectional study of 36 RBAs for 6 commonly performed surgical procedures found that the LLM-based chatbot generated more readable, complete, and accurate consent documentation than the surgeons. Meaning These findings indicate that LLM-based chatbots are a promising tool for generating informed consent forms, easing the documentation burden on physicians while providing salient information to patients.",
        "authors": [
            "Hannah Decker",
            "Karen Trang",
            "Joel Ramirez",
            "Alexis Colley",
            "Logan Pierce",
            "Melissa Coleman",
            "Tasce Bongiovanni",
            "Genevieve B. Melton",
            "Elizabeth C. Wick"
        ],
        "citations": 48,
        "references": 31,
        "year": 2023
    },
    {
        "title": "From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data",
        "abstract": "Large Language Models (LLMs) exhibit exceptional abilities for causal analysis between concepts in numerous societally impactful domains, including medicine, science, and law. Recent research on LLM performance in various causal discovery and inference tasks has given rise to a new ladder in the classical three-stage framework of causality. In this paper, we advance the current research of LLM-driven causal discovery by proposing a novel framework that combines knowledge-based LLM causal analysis with data-driven causal structure learning. To make LLM more than a query tool and to leverage its power in discovering natural and new laws of causality, we integrate the valuable LLM expertise on existing causal mechanisms into statistical analysis of objective data to build a novel and practical baseline for causal structure learning. We introduce a universal set of prompts designed to extract causal graphs from given variables and assess the influence of LLM prior causality on recovering causal structures from data. We demonstrate the significant enhancement of LLM expertise on the quality of recovered causal structures from data, while also identifying critical challenges and issues, along with potential approaches to address them. As a pioneering study, this paper aims to emphasize the new frontier that LLMs are opening for classical causal discovery and inference, and to encourage the widespread adoption of LLM capabilities in data-driven causal analysis.",
        "authors": [
            "Taiyu Ban",
            "Lyvzhou Chen",
            "Xiangyu Wang",
            "Huanhuan Chen"
        ],
        "citations": 47,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Developing prompts from large language model for extracting clinical information from pathology and ultrasound reports in breast cancer",
        "abstract": "Purpose We aimed to evaluate the time and cost of developing prompts using large language model (LLM), tailored to extract clinical factors in breast cancer patients and their accuracy. Materials and Methods We collected data from reports of surgical pathology and ultrasound from breast cancer patients who underwent radiotherapy from 2020 to 2022. We extracted the information using the Generative Pre-trained Transformer (GPT) for Sheets and Docs extension plugin and termed this the “LLM” method. The time and cost of developing the prompts with LLM methods were assessed and compared with those spent on collecting information with “full manual” and “LLM-assisted manual” methods. To assess accuracy, 340 patients were randomly selected, and the extracted information by LLM method were compared with those collected by “full manual” method. Results Data from 2,931 patients were collected. We developed 12 prompts for Extract function and 12 for Format function to extract and standardize the information. The overall accuracy was 87.7%. For lymphovascular invasion, it was 98.2%. Developing and processing the prompts took 3.5 hours and 15 minutes, respectively. Utilizing the ChatGPT application programming interface cost US $65.8 and when factoring in the estimated wage, the total cost was US $95.4. In an estimated comparison, “LLM-assisted manual” and “LLM” methods were time- and cost-efficient compared to the “full manual” method. Conclusion Developing and facilitating prompts for LLM to derive clinical factors was efficient to extract crucial information from huge medical records. This study demonstrated the potential of the application of natural language processing using LLM model in breast cancer patients. Prompts from the current study can be re-used for other research to collect clinical information.",
        "authors": [
            "Hyeon Seok Choi",
            "Jun Yeong Song",
            "Kyung Hwan Shin",
            "Ji Hyun Chang",
            "B. Jang"
        ],
        "citations": 45,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?",
        "abstract": "Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models’ outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks. In this study, we explore the potential of LLM-based evaluators in enhancing multilingual evaluation by calibrating them against 20K human judgments across three text-generation tasks, five metrics, and eight languages. Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.",
        "authors": [
            "Rishav Hada",
            "Varun Gumma",
            "Adrian de Wynter",
            "Harshita Diddee",
            "Mohamed Ahmed",
            "M. Choudhury",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "citations": 44,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
        "abstract": "Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems. This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XLingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all.",
        "authors": [
            "Yiqiao Jin",
            "Mohit Chandra",
            "Gaurav Verma",
            "Yibo Hu",
            "Munmun De Choudhury",
            "Srijan Kumar"
        ],
        "citations": 43,
        "references": 94,
        "year": 2023
    },
    {
        "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
        "abstract": "Large language models (LLMs) can\"lie\", which we define as outputting false statements despite\"knowing\"the truth in a demonstrable sense. LLMs might\"lie\", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -- prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.",
        "authors": [
            "Lorenzo Pacchiardi",
            "A. J. Chan",
            "S. Mindermann",
            "Ilan Moscovitz",
            "Alexa Y. Pan",
            "Y. Gal",
            "Owain Evans",
            "J. Brauner"
        ],
        "citations": 44,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop",
        "abstract": "Large Language Models (LLM) are already widely used to generate content for a variety of online platforms. As we are not able to safely distinguish LLM-generated content from human-produced content, LLM-generated content is used to train the next generation of LLMs, giving rise to a self-consuming training loop. From the image generation domain we know that such a self-consuming training loop reduces both quality and diversity of images finally ending in a model collapse. However, it is unclear whether this alarming effect can also be observed for LLMs. Therefore, we present the first study investigating the self-consuming training loop for LLMs. Further, we propose a novel method based on logic expressions that allows us to unambiguously verify the correctness of LLM-generated content, which is difficult for natural language text. We find that the self-consuming training loop produces correct outputs, however, the output declines in its diversity depending on the proportion of the used generated data. Fresh data can slow down this decline, but not stop it. Given these concerning results, we encourage researchers to study methods to negate this process.",
        "authors": [
            "Martin Briesch",
            "Dominik Sobania",
            "Franz Rothlauf"
        ],
        "citations": 43,
        "references": 34,
        "year": 2023
    },
    {
        "title": "GenRec: Large Language Model for Generative Recommendation",
        "abstract": "In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recommendation tasks. Subsequently, we use these prompts to fine-tune the LLaMA backbone LLM on a dataset of user-item interactions, represented by textual data, to capture user preferences and item characteristics. Our research underscores the potential of LLM-based generative recommendation in revolutionizing the domain of recommendation systems and offers a foundational framework for future explorations in this field. We conduct extensive experiments on benchmark datasets, and the experiments shows that our GenRec has significant better results on large dataset.",
        "authors": [
            "Jianchao Ji",
            "Zelong Li",
            "Shuyuan Xu",
            "Wenyue Hua",
            "Yingqiang Ge",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "citations": 36,
        "references": 19,
        "year": 2023
    },
    {
        "title": "PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences",
        "abstract": "Extended Abstract Large Language Models (LLMs)—by way of their training and design—can be thought of as implicit computational models of humans [2] and studies are already exploring how these LLMs can be seen as effective proxies for speciﬁc human sub-populations [1]. This is largely because these models were designed to respond to prompts in a similar fashion to how a person would react—which makes them very appealing for applications like chatbots. In that context, an appealing property of LLMs is that their adaptivity to take on the character of different individuals based on certain traits, e.g. personality traits. Research shows that designing chatbots with curated personality proﬁles provides an improved personalized and engaging user experience [5]. Despite the need and clear applications, little work has been done to evaluate whether the behavior of LLM-generated personas can reﬂect certain personality traits accurately and consistently. In this work, we design a case study to address this gap. In this work, we consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and aim to answer the following questions: When GPT-3.5 (text-davinci-003) is assigned a Big Five personality type, (1) do LLM personas consistently express the assigned personality traits in personality tests and writing tasks? (2) Does assigning a gender role have an additional effect on LLM personas’ behavior? To investigate these research questions, we build upon prior work in text-based personality analysis [4] by studying the ability of LLMs to generate content with curated personality traits. Speciﬁcally, we create 10 personas (5",
        "authors": [
            "Hang Jiang",
            "Xiajie Zhang",
            "Xubo Cao",
            "Jad Kabbara",
            "Dwaipayan Roy"
        ],
        "citations": 38,
        "references": 9,
        "year": 2023
    },
    {
        "title": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models",
        "abstract": "The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \\textit{\\carb}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, \\carb~significantly enhances the accuracy of carbon footprint estimations for various LLMs. The source code is released at \\url{https://github.com/SotaroKaneda/MLCarbon}.",
        "authors": [
            "Ahmad Faiz",
            "S. Kaneda",
            "Ruhan Wang",
            "Rita Osi",
            "Parteek Sharma",
            "Fan Chen",
            "Lei Jiang"
        ],
        "citations": 40,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Large language models as tax attorneys: a case study in legal capabilities emergence",
        "abstract": "Better understanding of Large Language Models' (LLMs) legal analysis abilities can contribute to improving the efficiency of legal services, governing artificial intelligence and leveraging LLMs to identify inconsistencies in law. This paper explores LLM capabilities in applying tax law. We choose this area of law because it has a structure that allows us to set up automated validation pipelines across thousands of examples, requires logical reasoning and maths skills, and enables us to test LLM capabilities in a manner relevant to real-world economic lives of citizens and companies. Our experiments demonstrate emerging legal understanding capabilities, with improved performance in each subsequent OpenAI model release. We experiment with retrieving and using the relevant legal authority to assess the impact of providing additional legal context to LLMs. Few-shot prompting, presenting examples of question–answer pairs, is also found to significantly enhance the performance of the most advanced model, GPT-4. The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels. As LLMs continue to advance, their ability to reason about law autonomously could have significant implications for the legal profession and AI governance. This article is part of the theme issue ‘A complexity science approach to law and governance’.",
        "authors": [
            "John J. Nay",
            "David Karamardian",
            "Sarah Lawsky",
            "Wenting Tao",
            "Meghana Moorthy Bhat",
            "Raghav Jain",
            "Aaron Travis Lee",
            "Jonathan H. Choi",
            "Jungo Kasai"
        ],
        "citations": 40,
        "references": 55,
        "year": 2023
    },
    {
        "title": "A Robust Semantics-based Watermark for Large Language Model against Paraphrasing",
        "abstract": "Large language models (LLMs) have show great ability in various natural language tasks. However, there are concerns that LLMs are possible to be used improperly or even illegally. To prevent the malicious usage of LLMs, detecting LLM-generated text becomes crucial in the deployment of LLM applications. Watermarking is an effective strategy to detect the LLM-generated content by encoding a pre-defined secret watermark to facilitate the detection process. However, the majority of existing watermark methods leverage the simple hashes of precedent tokens to partition vocabulary. Such watermark can be easily eliminated by paraphrase and correspondingly the detection effectiveness will be greatly compromised. Thus, to enhance the robustness against paraphrase, we propose a semantics-based watermark framework SemaMark. It leverages the semantics as an alternative to simple hashes of tokens since the paraphrase will likely preserve the semantic meaning of the sentences. Comprehensive experiments are conducted to demonstrate the effectiveness and robustness of SemaMark under different paraphrases.",
        "authors": [
            "Jie Ren",
            "Han Xu",
            "Yiding Liu",
            "Yingqian Cui",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Jiliang Tang"
        ],
        "citations": 38,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Tabi: An Efficient Multi-Level Inference System for Large Language Models",
        "abstract": "Today's trend of building ever larger language models (LLMs), while pushing the performance of natural language processing, adds significant latency to the inference stage. We observe that due to the diminishing returns of adding parameters to LLMs, a smaller model could make the same prediction as a costly LLM for a majority of queries. Based on this observation, we design Tabi, an inference system with a multi-level inference engine that serves queries using small models and optional LLMs for demanding applications. Tabi is optimized for discriminative models (i.e., not generative LLMs) in a serving framework. Tabi uses the calibrated confidence score to decide whether to return the accurate results of small models extremely fast or re-route them to LLMs. For re-routed queries, it uses attention-based word pruning and weighted ensemble techniques to offset the system overhead and accuracy loss. We implement and evaluate Tabi with multiple tasks and models. Our result shows that Tabi achieves 21%-40% average latency reduction (with comparable tail latency) over the state-of-the-art while meeting LLM-grade high accuracy targets.",
        "authors": [
            "Yiding Wang",
            "Kai Chen",
            "Haisheng Tan",
            "Kun Guo"
        ],
        "citations": 42,
        "references": 87,
        "year": 2023
    },
    {
        "title": "ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?",
        "abstract": "ChatGPT, developed by OpenAI, is one of the largest Large Language Models (LLM) with over 175 billion parameters. ChatGPT has demonstrated the impressive capabilities of LLM, particularly in the field of natural language processing (NLP). With the emergence of the discussion and application of LLM in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM and a cross-modal encoder, an intelligent system can handle traffic data from various modalities and execute transportation operations through a single LLM. NLP, combined with cross-modal processing, is investigated with its potential applications in transportation. To demonstrate this potential, a smartphone-based crash report auto-generation and analysis framework is presented as a use case. Despite the potential benefits, challenges related to data privacy, data quality, and model bias must be considered. Overall, the use of LLM in intelligent transport systems holds promise for more efficient, intelligent, and sustainable transportation systems that improve the lives of people around the world.",
        "authors": [
            "Ou Zheng",
            "M. Abdel-Aty",
            "Dongdong Wang",
            "Zijin Wang",
            "Shengxuan Ding"
        ],
        "citations": 42,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
        "abstract": "Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the\"answer consistency\"of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40% of its cost.",
        "authors": [
            "Murong Yue",
            "Jie Zhao",
            "Min Zhang",
            "Liang Du",
            "Ziyu Yao"
        ],
        "citations": 39,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models",
        "abstract": "People are increasingly turning to large language models (LLMs) for complex information tasks like academic research or planning a move to another city. However, while they often require working in a nonlinear manner — e.g., to arrange information spatially to organize and make sense of it, current interfaces for interacting with LLMs are generally linear to support conversational interaction. To address this limitation and explore how we can support LLM-powered exploration and sensemaking, we developed Sensecape, an interactive system designed to support complex information tasks with an LLM by enabling users to (1) manage the complexity of information through multilevel abstraction and (2) switch seamlessly between foraging and sensemaking. Our within-subject user study reveals that Sensecape empowers users to explore more topics and structure their knowledge hierarchically, thanks to the externalization of levels of abstraction. We contribute implications for LLM-based workflows and interfaces for information tasks.",
        "authors": [
            "Sangho Suh",
            "Bryan Min",
            "Srishti Palani",
            "Haijun Xia"
        ],
        "citations": 38,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Large Language Models Enable Few-Shot Clustering",
        "abstract": "Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user’s intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model (LLM) can amplify an expert’s guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find that incorporating LLMs in the first two stages routinely provides significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.1",
        "authors": [
            "Vijay Viswanathan",
            "Kiril Gashteovski",
            "Carolin (Haas) Lawrence",
            "Tongshuang Sherry Wu",
            "Graham Neubig"
        ],
        "citations": 38,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
        "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
        "authors": [
            "Huao Li",
            "Yu Quan Chong",
            "Simon Stepputtis",
            "Joseph Campbell",
            "Dana Hughes",
            "Michael Lewis",
            "Katia P. Sycara"
        ],
        "citations": 37,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Enabling Conversational Interaction with Mobile UI using Large Language Models",
        "abstract": "Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.",
        "authors": [
            "Bryan Wang",
            "Gang Li",
            "Yang Li"
        ],
        "citations": 103,
        "references": 75,
        "year": 2022
    },
    {
        "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models",
        "abstract": "Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnect and task disconnect between the LLM and VQA tasks. End-to-end training on multimodal data may bridge the disconnects, but is inflexible and computationally expensive. To address this issue, we propose Img2LLM, a plug-and-play module that provides LLM prompts to enable LLMs to perform zeroshot VQA tasks without end-to-end training. We develop LLM-agnostic models describe image content as exemplar question-answer pairs, which prove to be effective LLM prompts. Img2LLM offers the following benefits: 1) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo [3] by 5.6% on VQAv2. On the challenging A-OKVQA dataset, our method outperforms few-shot methods by as much as 20%. 2) It flexibly interfaces with a wide range of LLMs to perform VQA. 3) It eliminates the need to specialize LLMs using end-to-end finetuning and serve highly specialized LLMs to end users, thereby reducing cost. Code is available via the LAVIS [28] framework at https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa.",
        "authors": [
            "Jiaxian Guo",
            "Junnan Li",
            "Dongxu Li",
            "A. M. H. Tiong",
            "Boyang Albert Li",
            "Dacheng Tao",
            "Steven C. H. Hoi"
        ],
        "citations": 93,
        "references": 73,
        "year": 2022
    },
    {
        "title": "Benchmarking Large Language Models for Automated Verilog RTL Code Generation",
        "abstract": "Automating hardware design could obviate a signif-icant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models (LLMs) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of LLMs to generate useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets collected from GitHub and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in LLMs more capable of producing syntactically correct code (25.9% overall). Further, when analyzing functional correctness, a fine-tuned open-source CodeGen LLM can outperform the state-of-the-art commercial Codex LLM (6.5% overall). We release our training/evaluation scripts and LLM checkpoints as open source contributions.",
        "authors": [
            "Shailja Thakur",
            "Baleegh Ahmad",
            "Zhenxing Fan",
            "H. Pearce",
            "Benjamin Tan",
            "R. Karri",
            "Brendan Dolan-Gavitt",
            "S. Garg"
        ],
        "citations": 94,
        "references": 15,
        "year": 2022
    },
    {
        "title": "Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",
        "abstract": "With the wide application of Large Language Models (LLMs) such as ChatGPT, how to make the contents generated by LLM accurate and credible becomes very important, especially in complex knowledge-intensive tasks. In this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering, which is a typical complex knowledge-intensive task. SearChain is a framework that deeply integrates LLM and information retrieval (IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition of the multi-hop question. Each node of the chain is a query-answer pair consisting of an IR-oriented query and the answer generated by LLM for this query. IR veriﬁes, completes, and traces the information of each node of the chain, so as to guide LLM to construct the correct chain-of-query, and ﬁnally answer the multi-hop question. SearChain makes LLM change from trying to give a answer to trying to construct the chain-of-query when faced with the multi-hop question, which can stimulate the knowledge-reasoning ability and provides the interface for IR to be deeply involved in reasoning process of LLM. IR interacts with each node of chain-of-query of LLM. It veriﬁes the information of the node and provides the unknown knowledge to LLM, which ensures the accuracy of the whole chain in the process of LLM generating the answer. Besides, the contents returned by LLM to the user include not only the ﬁnal answer but also the reasoning process for the question, that is, the chain-of-query and the supporting documents retrieved by IR for each node of the chain, which improves the credibility and traceability of the contents generated by LLM. Experimental results show SearChain outperforms related baselines on four multi-hop question-answering datasets. Besides, SearChain can distinguish which knowledge comes from LLM and which comes from IR in the generated texts.",
        "authors": [
            "Shicheng Xu",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng",
            "Tat-seng Chua"
        ],
        "citations": 33,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
        "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
        "authors": [
            "Derek Tam",
            "Anisha Mascarenhas",
            "Shiyue Zhang",
            "Sarah Kwan",
            "Mohit Bansal",
            "Colin Raffel"
        ],
        "citations": 82,
        "references": 52,
        "year": 2022
    },
    {
        "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
        "abstract": "Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.",
        "authors": [
            "Fangyu Liu",
            "Julian Martin Eisenschlos",
            "Francesco Piccinno",
            "Syrine Krichene",
            "Chenxi Pang",
            "Kenton Lee",
            "Mandar Joshi",
            "Wenhu Chen",
            "Nigel Collier",
            "Y. Altun"
        ],
        "citations": 75,
        "references": 51,
        "year": 2022
    },
    {
        "title": "What is it like to program with artificial intelligence?",
        "abstract": "Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot. In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges. Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.",
        "authors": [
            "Advait Sarkar",
            "A. Gordon",
            "Carina Negreanu",
            "Christian Poelitz",
            "Sruti Srinivasa Ragavan",
            "B. Zorn"
        ],
        "citations": 75,
        "references": 97,
        "year": 2022
    },
    {
        "title": "Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing",
        "abstract": "Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.",
        "authors": [
            "Zhe Liu",
            "Chunyang Chen",
            "Junjie Wang",
            "Xing Che",
            "Yuekai Huang",
            "Jun Hu",
            "Qing Wang"
        ],
        "citations": 77,
        "references": 92,
        "year": 2022
    },
    {
        "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment",
        "abstract": "AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind -- the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule-breaking -- inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MORALCOT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT",
        "authors": [
            "Zhijing Jin",
            "Sydney Levine",
            "Fernando Gonzalez",
            "Ojasv Kamal",
            "Maarten Sap",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "J. Tenenbaum",
            "B. Scholkopf"
        ],
        "citations": 78,
        "references": 84,
        "year": 2022
    },
    {
        "title": "Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving",
        "abstract": "Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique objectlevel multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver’s proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available1 for further exploration.",
        "authors": [
            "Long Chen",
            "Oleg Sinavski",
            "Jan Hünermann",
            "Alice Karnsund",
            "Andrew J Willmott",
            "Danny Birch",
            "Daniel Maund",
            "Jamie Shotton"
        ],
        "citations": 124,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Legal Prompt Engineering for Multilingual Legal Judgement Prediction",
        "abstract": "Legal Prompt Engineering (LPE) or Legal Prompting is a process to guide and assist a large language model (LLM) with performing a natural legal language processing (NLLP) skill. Our goal is to use LPE with LLMs over long legal documents for the Legal Judgement Prediction (LJP) task. We investigate the performance of zero-shot LPE for given facts in case-texts from the European Court of Human Rights (in English) and the Federal Supreme Court of Switzerland (in German, French and Italian). Our results show that zero-shot LPE is better compared to the baselines, but it still falls short compared to current state of the art supervised approaches. Nevertheless, the results are important, since there was 1) no explicit domain-specific data used - so we show that the transfer to the legal domain is possible for general-purpose LLMs, and 2) the LLMs where directly applied without any further training or fine-tuning - which in turn saves immensely in terms of additional computational costs.",
        "authors": [
            "Dietrich Trautmann",
            "Alina Petrova",
            "Frank Schilder"
        ],
        "citations": 67,
        "references": 30,
        "year": 2022
    },
    {
        "title": "Parallel Context Windows for Large Language Models",
        "abstract": "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (“windows”), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ ai21labs/parallel-context-windows.",
        "authors": [
            "Nir Ratner",
            "Yoav Levine",
            "Yonatan Belinkov",
            "Ori Ram",
            "Inbal Magar",
            "Omri Abend",
            "Ehud D. Karpas",
            "A. Shashua",
            "Kevin Leyton-Brown",
            "Y. Shoham"
        ],
        "citations": 57,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models",
        "abstract": "Large language models (LLMs) have shown impressive results while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial in this setting. We formulate and study Attributed QA as a key first step in the development of attributed LLMs. We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures. We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development. Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).",
        "authors": [
            "Bernd Bohnet",
            "Vinh Q. Tran",
            "Pat Verga",
            "Roee Aharoni",
            "D. Andor",
            "Livio Baldini Soares",
            "Jacob Eisenstein",
            "Kuzman Ganchev",
            "Jonathan Herzig",
            "Kai Hui",
            "T. Kwiatkowski",
            "Ji Ma",
            "Jianmo Ni",
            "Tal Schuster",
            "William W. Cohen",
            "Michael Collins",
            "Dipanjan Das",
            "Donald Metzler",
            "Slav Petrov",
            "Kellie Webster"
        ],
        "citations": 56,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals",
        "abstract": "Current language technology is ubiquitous and directly influences individuals’ lives worldwide. Given the recent trend in AI on training and constantly releasing new and powerful large language models (LLMs), there is a need to assess their biases and potential concrete consequences. While some studies have highlighted the shortcomings of these models, there is only little on the negative impact of LLMs on LGBTQIA+ individuals. In this paper, we investigated a state-of-the-art template-based approach for measuring the harmfulness of English LLMs sentence completion when the subjects belong to the LGBTQIA+ community. Our findings show that, on average, the most likely LLM-generated completion is an identity attack 13% of the time. Our results raise serious concerns about the applicability of these models in production environments.",
        "authors": [
            "Debora Nozza",
            "Federico Bianchi",
            "Anne Lauscher",
            "Dirk Hovy"
        ],
        "citations": 51,
        "references": 52,
        "year": 2022
    },
    {
        "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
        "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{https://chat.lmsys.org}.",
        "authors": [
            "Wei-Lin Chiang",
            "Lianmin Zheng",
            "Ying Sheng",
            "Anastasios Nikolas Angelopoulos",
            "Tianle Li",
            "Dacheng Li",
            "Hao Zhang",
            "Banghua Zhu",
            "Michael Jordan",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "citations": 276,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent MultiModal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify “CLIP-blind pairs”- images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",
        "authors": [
            "Shengbang Tong",
            "Zhuang Liu",
            "Yuexiang Zhai",
            "Yi Ma",
            "Yann LeCun",
            "Saining Xie"
        ],
        "citations": 177,
        "references": 66,
        "year": 2024
    },
    {
        "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
        "abstract": "Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs",
        "authors": [
            "Yi Zeng",
            "Hongpeng Lin",
            "Jingwen Zhang",
            "Diyi Yang",
            "Ruoxi Jia",
            "Weiyan Shi"
        ],
        "citations": 176,
        "references": 80,
        "year": 2024
    },
    {
        "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
        "abstract": "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks.",
        "authors": [
            "Yaowei Zheng",
            "Richong Zhang",
            "Junhao Zhang",
            "Yanhan Ye",
            "Zheyan Luo",
            "Yongqiang Ma"
        ],
        "citations": 164,
        "references": 94,
        "year": 2024
    },
    {
        "title": "TrustLLM: Trustworthiness in Large Language Models",
        "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.",
        "authors": [
            "Lichao Sun",
            "Yue Huang",
            "Haoran Wang",
            "Siyuan Wu",
            "Qihui Zhang",
            "Chujie Gao",
            "Yixin Huang",
            "Wenhan Lyu",
            "Yixuan Zhang",
            "Xiner Li",
            "Zheng Liu",
            "Yixin Liu",
            "Yijue Wang",
            "Zhikun Zhang",
            "B. Kailkhura",
            "Caiming Xiong",
            "Chaowei Xiao",
            "Chun-Yan Li",
            "Eric P. Xing",
            "Furong Huang",
            "Haodong Liu",
            "Heng Ji",
            "Hongyi Wang",
            "Huan Zhang",
            "Huaxiu Yao",
            "M. Kellis",
            "M. Zitnik",
            "Meng Jiang",
            "Mohit Bansal",
            "James Zou",
            "Jian Pei",
            "Jian Liu",
            "Jianfeng Gao",
            "Jiawei Han",
            "Jieyu Zhao",
            "Jiliang Tang",
            "Jindong Wang",
            "John Mitchell",
            "Kai Shu",
            "Kaidi Xu",
            "Kai-Wei Chang",
            "Lifang He",
            "Lifu Huang",
            "M. Backes",
            "Neil Zhenqiang Gong",
            "Philip S. Yu",
            "Pin-Yu Chen",
            "Quanquan Gu",
            "Ran Xu",
            "Rex Ying",
            "Shuiwang Ji",
            "S. Jana",
            "Tian-Xiang Chen",
            "Tianming Liu",
            "Tianying Zhou",
            "William Wang",
            "Xiang Li",
            "Xiang-Yu Zhang",
            "Xiao Wang",
            "Xingyao Xie",
            "Xun Chen",
            "Xuyu Wang",
            "Yan Liu",
            "Yanfang Ye",
            "Yinzhi Cao",
            "Yue Zhao"
        ],
        "citations": 140,
        "references": 0,
        "year": 2024
    },
    {
        "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
        "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.",
        "authors": [
            "S. Tonmoy",
            "S. M. M. Zaman",
            "Vinija Jain",
            "Anku Rani",
            "Vipula Rawte",
            "Aman Chadha",
            "Amitava Das"
        ],
        "citations": 130,
        "references": 50,
        "year": 2024
    },
    {
        "title": "RAFT: Adapting Language Model to Domain Specific RAG",
        "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a\"open-book\"in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla.",
        "authors": [
            "Tianjun Zhang",
            "Shishir G. Patil",
            "Naman Jain",
            "Sheng Shen",
            "M. Zaharia",
            "Ion Stoica",
            "Joseph E. Gonzalez"
        ],
        "citations": 115,
        "references": 57,
        "year": 2024
    },
    {
        "title": "A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges",
        "abstract": "Large Language Models (LLMs) recently demonstrated extraordinary capability in various natural language processing (NLP) tasks including language translation, text generation, question answering, etc. Moreover, LLMs are new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a plethora of research on LLMs have been appeared within a short time, it is quite impossible to track all of these and get an overview of the current state of research in this area. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews LLMs, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different training methods that have been used to train them. The paper also demonstrates the datasets utilized in the studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the paper also explores open issues and challenges to deploy LLMs in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained architectures, applications, challenges, and future goals.",
        "authors": [
            "Mohaimenul Azam Khan Raiaan",
            "Md. Saddam Hossain Mukta",
            "Kaniz Fatema",
            "Nur Mohammad Fahad",
            "Sadman Sakib",
            "Most. Marufatul Jannat Mim",
            "Jubaer Ahmad",
            "Mohammed Eunus Ali",
            "Sami Azam"
        ],
        "citations": 114,
        "references": 181,
        "year": 2024
    },
    {
        "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
        "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.",
        "authors": [
            "Duzhen Zhang",
            "Yahan Yu",
            "Chenxing Li",
            "Jiahua Dong",
            "Dan Su",
            "Chenhui Chu",
            "Dong Yu"
        ],
        "citations": 115,
        "references": 275,
        "year": 2024
    },
    {
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "abstract": "Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.",
        "authors": [
            "Arka Pal",
            "Deep Karkhanis",
            "Samuel Dooley",
            "Manley Roberts",
            "Siddartha Naidu",
            "Colin White"
        ],
        "citations": 94,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
        "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the\"lost-in-the-middle\"effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "citations": 95,
        "references": 67,
        "year": 2024
    },
    {
        "title": "AI models collapse when trained on recursively generated data",
        "abstract": null,
        "authors": [
            "Ilia Shumailov",
            "Zakhar Shumaylov",
            "Yiren Zhao",
            "Nicolas Papernot",
            "Ross Anderson",
            "Yarin Gal"
        ],
        "citations": 101,
        "references": 10,
        "year": 2024
    },
    {
        "title": "Long-context LLMs Struggle with Long In-context Learning",
        "abstract": "Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs.",
        "authors": [
            "Tianle Li",
            "Ge Zhang",
            "Quy Duc Do",
            "Xiang Yue",
            "Wenhu Chen"
        ],
        "citations": 92,
        "references": 49,
        "year": 2024
    },
    {
        "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "abstract": "Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.",
        "authors": [
            "Pratyush Maini",
            "Zhili Feng",
            "Avi Schwarzschild",
            "Zachary Chase Lipton",
            "J. Kolter"
        ],
        "citations": 85,
        "references": 44,
        "year": 2024
    },
    {
        "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
        "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model",
        "authors": [
            "Naman Jain",
            "King Han",
            "Alex Gu",
            "Wen-Ding Li",
            "Fanjia Yan",
            "Tianjun Zhang",
            "Sida I. Wang",
            "Armando Solar-Lezama",
            "Koushik Sen",
            "Ion Stoica"
        ],
        "citations": 88,
        "references": 90,
        "year": 2024
    },
    {
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
        "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.",
        "authors": [
            "Sébastien Bubeck",
            "Varun Chandrasekaran",
            "Ronen Eldan",
            "J. Gehrke",
            "Eric Horvitz",
            "Ece Kamar",
            "Peter Lee",
            "Y. Lee",
            "Yuan-Fang Li",
            "Scott M. Lundberg",
            "Harsha Nori",
            "Hamid Palangi",
            "Marco Tulio Ribeiro",
            "Yi Zhang"
        ],
        "citations": 1000,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
        "authors": [
            "Hugo Touvron",
            "Louis Martin",
            "Kevin R. Stone",
            "Peter Albert",
            "Amjad Almahairi",
            "Yasmine Babaei",
            "Nikolay Bashlykov",
            "Soumya Batra",
            "Prajjwal Bhargava",
            "Shruti Bhosale",
            "D. Bikel",
            "Lukas Blecher",
            "Cristian Cantón Ferrer",
            "Moya Chen",
            "Guillem Cucurull",
            "David Esiobu",
            "Jude Fernandes",
            "Jeremy Fu",
            "Wenyin Fu",
            "Brian Fuller",
            "Cynthia Gao",
            "Vedanuj Goswami",
            "Naman Goyal",
            "A. Hartshorn",
            "Saghar Hosseini",
            "Rui Hou",
            "Hakan Inan",
            "Marcin Kardas",
            "Viktor Kerkez",
            "Madian Khabsa",
            "Isabel M. Kloumann",
            "A. Korenev",
            "Punit Singh Koura",
            "M. Lachaux",
            "Thibaut Lavril",
            "Jenya Lee",
            "Diana Liskovich",
            "Yinghai Lu",
            "Yuning Mao",
            "Xavier Martinet",
            "Todor Mihaylov",
            "Pushkar Mishra",
            "Igor Molybog",
            "Yixin Nie",
            "Andrew Poulton",
            "Jeremy Reizenstein",
            "Rashi Rungta",
            "Kalyan Saladi",
            "Alan Schelten",
            "Ruan Silva",
            "Eric Michael Smith",
            "R. Subramanian",
            "Xia Tan",
            "Binh Tang",
            "Ross Taylor",
            "Adina Williams",
            "Jian Xiang Kuan",
            "Puxin Xu",
            "Zhengxu Yan",
            "Iliyan Zarov",
            "Yuchen Zhang",
            "Angela Fan",
            "M. Kambadur",
            "Sharan Narang",
            "Aurélien Rodriguez",
            "Robert Stojnic",
            "Sergey Edunov",
            "Thomas Scialom"
        ],
        "citations": 1000,
        "references": 131,
        "year": 2023
    },
    {
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
        "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
        "authors": [
            "Tim Dettmers",
            "Artidoro Pagnoni",
            "Ari Holtzman",
            "Luke Zettlemoyer"
        ],
        "citations": 1000,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Tower: An Open Multilingual Large Language Model for Translation-Related Tasks",
        "abstract": "While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.",
        "authors": [
            "Duarte M. Alves",
            "José P. Pombal",
            "Nuno M. Guerreiro",
            "P. Martins",
            "Joao Alves",
            "Amin Farajian",
            "Ben Peters",
            "Ricardo Rei",
            "Patrick Fernandes",
            "Sweta Agrawal",
            "Pierre Colombo",
            "J. G. D. Souza",
            "André Martins"
        ],
        "citations": 80,
        "references": 96,
        "year": 2024
    },
    {
        "title": "A Survey of Large Language Models",
        "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.",
        "authors": [
            "Wayne Xin Zhao",
            "Kun Zhou",
            "Junyi Li",
            "Tianyi Tang",
            "Xiaolei Wang",
            "Yupeng Hou",
            "Yingqian Min",
            "Beichen Zhang",
            "Junjie Zhang",
            "Zican Dong",
            "Yifan Du",
            "Chen Yang",
            "Yushuo Chen",
            "Z. Chen",
            "Jinhao Jiang",
            "Ruiyang Ren",
            "Yifan Li",
            "Xinyu Tang",
            "Zikang Liu",
            "Peiyu Liu",
            "J. Nie",
            "Ji-rong Wen"
        ],
        "citations": 1000,
        "references": 417,
        "year": 2023
    },
    {
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
        "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.",
        "authors": [
            "Stella Biderman",
            "Hailey Schoelkopf",
            "Quentin G. Anthony",
            "Herbie Bradley",
            "Kyle O'Brien",
            "Eric Hallahan",
            "Mohammad Aflah Khan",
            "Shivanshu Purohit",
            "USVSN Sai Prashanth",
            "Edward Raff",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Oskar van der Wal"
        ],
        "citations": 978,
        "references": 101,
        "year": 2023
    },
    {
        "title": "Qwen Technical Report",
        "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",
        "authors": [
            "Jinze Bai",
            "Shuai Bai",
            "Yunfei Chu",
            "Zeyu Cui",
            "Kai Dang",
            "Xiaodong Deng",
            "Yang Fan",
            "Wenhang Ge",
            "Yu Han",
            "Fei Huang",
            "Binyuan Hui",
            "Luo Ji",
            "Mei Li",
            "Junyang Lin",
            "Runji Lin",
            "Dayiheng Liu",
            "Gao Liu",
            "Chengqiang Lu",
            "K. Lu",
            "Jianxin Ma",
            "Rui Men",
            "Xingzhang Ren",
            "Xuancheng Ren",
            "Chuanqi Tan",
            "Sinan Tan",
            "Jianhong Tu",
            "Peng Wang",
            "Shijie Wang",
            "Wei Wang",
            "Shengguang Wu",
            "Benfeng Xu",
            "Jin Xu",
            "An Yang",
            "Hao Yang",
            "Jian Yang",
            "Jian Yang",
            "Shusheng Yang",
            "Yang Yao",
            "Bowen Yu",
            "Yu Bowen",
            "Hongyi Yuan",
            "Zheng Yuan",
            "Jianwei Zhang",
            "Xing Zhang",
            "Yichang Zhang",
            "Zhenru Zhang",
            "Chang Zhou",
            "Jingren Zhou",
            "Xiaohuan Zhou",
            "Tianhang Zhu"
        ],
        "citations": 1000,
        "references": 163,
        "year": 2023
    },
    {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
        "authors": [
            "Shunyu Yao",
            "Dian Yu",
            "Jeffrey Zhao",
            "Izhak Shafran",
            "T. Griffiths",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "citations": 1000,
        "references": 52,
        "year": 2023
    },
    {
        "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "abstract": "The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai",
        "authors": [
            "Nathaniel Li",
            "Alexander Pan",
            "Anjali Gopal",
            "Summer Yue",
            "Daniel Berrios",
            "Alice Gatti",
            "Justin D. Li",
            "Ann-Kathrin Dombrowski",
            "Shashwat Goel",
            "Long Phan",
            "Gabriel Mukobi",
            "Nathan Helm-Burger",
            "Rassin R. Lababidi",
            "Lennart Justen",
            "Andrew B. Liu",
            "Michael Chen",
            "Isabelle Barrass",
            "Oliver Zhang",
            "Xiaoyuan Zhu",
            "Rishub Tamirisa",
            "Bhrugu Bharathi",
            "Adam Khoja",
            "Ariel Herbert-Voss",
            "Cort B. Breuer",
            "Andy Zou",
            "Mantas Mazeika",
            "Zifan Wang",
            "Palash Oswal",
            "Weiran Liu",
            "Adam A. Hunt",
            "Justin Tienken-Harder",
            "Kevin Y. Shih",
            "Kemper Talley",
            "John Guan",
            "Russell Kaplan",
            "Ian Steneker",
            "David Campbell",
            "Brad Jokubaitis",
            "Alex Levinson",
            "Jean Wang",
            "William Qian",
            "K. Karmakar",
            "Steven Basart",
            "Stephen Fitz",
            "Mindy Levine",
            "P. Kumaraguru",
            "U. Tupakula",
            "Vijay Varadharajan",
            "Yan Shoshitaishvili",
            "Jimmy Ba",
            "K. Esvelt",
            "Alexandr Wang",
            "Dan Hendrycks"
        ],
        "citations": 76,
        "references": 89,
        "year": 2024
    },
    {
        "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
        "abstract": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
        "authors": [
            "Wenqi Fan",
            "Yujuan Ding",
            "Liang-bo Ning",
            "Shijie Wang",
            "Hengyun Li",
            "Dawei Yin",
            "Tat-Seng Chua",
            "Qing Li"
        ],
        "citations": 71,
        "references": 180,
        "year": 2024
    },
    {
        "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
        "abstract": "This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.",
        "authors": [
            "Tsendsuren Munkhdalai",
            "Manaal Faruqui",
            "Siddharth Gopal"
        ],
        "citations": 76,
        "references": 64,
        "year": 2024
    },
    {
        "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
        "abstract": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.",
        "authors": [
            "Eric Wallace",
            "Kai Xiao",
            "R. Leike",
            "Lilian Weng",
            "Jo-hannes Heidecke",
            "Alex Beutel"
        ],
        "citations": 67,
        "references": 31,
        "year": 2024
    },
    {
        "title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models",
        "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.",
        "authors": [
            "Usman Anwar",
            "Abulhair Saparov",
            "Javier Rando",
            "Daniel Paleka",
            "Miles Turpin",
            "Peter Hase",
            "Ekdeep Singh Lubana",
            "Erik Jenner",
            "Stephen Casper",
            "Oliver Sourbut",
            "Benjamin L. Edelman",
            "Zhaowei Zhang",
            "Mario Gunther",
            "Anton Korinek",
            "J. Hernández-Orallo",
            "Lewis Hammond",
            "Eric J. Bigelow",
            "Alexander Pan",
            "L. Langosco",
            "Tomasz Korbak",
            "Heidi Zhang",
            "Ruiqi Zhong",
            "Se'an 'O h'Eigeartaigh",
            "Gabriel Recchia",
            "Giulio Corsi",
            "Alan Chan",
            "Markus Anderljung",
            "Lilian Edwards",
            "Y. Bengio",
            "Danqi Chen",
            "Samuel Albanie",
            "Tegan Maharaj",
            "Jakob N. Foerster",
            "Florian Tramèr",
            "He He",
            "Atoosa Kasirzadeh",
            "Yejin Choi",
            "David Krueger"
        ],
        "citations": 73,
        "references": 0,
        "year": 2024
    },
    {
        "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
        "abstract": "We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans\"within a blink\"(e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not\"emerged\"yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception.",
        "authors": [
            "Xingyu Fu",
            "Yushi Hu",
            "Bangzheng Li",
            "Yu Feng",
            "Haoyu Wang",
            "Xudong Lin",
            "Dan Roth",
            "Noah A. Smith",
            "Wei-Chiu Ma",
            "Ranjay Krishna"
        ],
        "citations": 72,
        "references": 85,
        "year": 2024
    },
    {
        "title": "Continual Learning for Large Language Models: A Survey",
        "abstract": "Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.",
        "authors": [
            "Tongtong Wu",
            "Linhao Luo",
            "Yuan-Fang Li",
            "Shirui Pan",
            "Thuy-Trang Vu",
            "Gholamreza Haffari"
        ],
        "citations": 69,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Large Language Model guided Protocol Fuzzing",
        "abstract": "—How to find security flaws in a protocol implementation without a machine-readable specification of the protocol? Facing the internet, protocol implementations are particularly security-critical software systems where inputs must adhere to a specific structure and order that is often informally specified in hundreds of pages in natural language (RFC). Without some machine-readable version of that protocol, it is difficult to automatically generate valid test inputs for its implementation that follow the required structure and order. It is possible to partially alleviate this challenge using mutational fuzzing on a set of recorded message sequences as seed inputs. However, the set of available seeds is often quite limited and will hardly cover the great diversity of protocol states and input structures. In this paper, we explore the opportunities of systematic interaction with pre-trained large language models (LLMs), which have ingested millions of pages of human-readable protocol specifications, to draw out machine-readable information about the protocol that can be used during protocol fuzzing. We use the knowledge of the LLMs about protocol message types for well-known protocols. We also checked the LLM’s capability in detecting “states” for stateful protocol implementations by generating sequences of messages and predicting response codes. Based on these observations, we have developed an LLM-guided protocol implementation fuzzing engine. Our protocol fuzzer C HAT AFL constructs grammars for each message type in a protocol, and then mutates messages or predicts the next messages in a message sequence via interactions with LLMs. Experiments on a wide range of real-world protocols from P RO F UZZ B ENCH show significant efficacy in state and code coverage. Our LLM-guided stateful fuzzer was compared with state-of-the-art fuzzers AFLN ET and NSF UZZ . C HAT AFL covers 47.60% and 42.69% more state transitions, 29.55% and 25.75% more",
        "authors": [
            "Ruijie Meng",
            "Martin Mirchev",
            "Marcel Böhme",
            "Abhik Roychoudhury"
        ],
        "citations": 68,
        "references": 45,
        "year": 2024
    },
    {
        "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
        "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction.",
        "authors": [
            "Yejin Bang",
            "Samuel Cahyawijaya",
            "Nayeon Lee",
            "Wenliang Dai",
            "Dan Su",
            "Bryan Wilie",
            "Holy Lovenia",
            "Ziwei Ji",
            "Tiezheng Yu",
            "Willy Chung",
            "Quyet V. Do",
            "Yan Xu",
            "Pascale Fung"
        ],
        "citations": 1000,
        "references": 151,
        "year": 2023
    },
    {
        "title": "A Survey on Evaluation of Large Language Models",
        "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey",
        "authors": [
            "Yu-Chu Chang",
            "Xu Wang",
            "Jindong Wang",
            "Yuanyi Wu",
            "Kaijie Zhu",
            "Hao Chen",
            "Linyi Yang",
            "Xiaoyuan Yi",
            "Cunxiang Wang",
            "Yidong Wang",
            "Weirong Ye",
            "Yue Zhang",
            "Yi Chang",
            "Philip S. Yu",
            "Qian Yang",
            "Xingxu Xie"
        ],
        "citations": 1000,
        "references": 305,
        "year": 2023
    },
    {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
        "authors": [
            "Yunfan Gao",
            "Yun Xiong",
            "Xinyu Gao",
            "Kangxiang Jia",
            "Jinliu Pan",
            "Yuxi Bi",
            "Yi Dai",
            "Jiawei Sun",
            "Qianyu Guo",
            "Meng Wang",
            "Haofen Wang"
        ],
        "citations": 915,
        "references": 229,
        "year": 2023
    },
    {
        "title": "Baichuan 2: Open Large-scale Language Models",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.",
        "authors": [
            "Ai Ming Yang",
            "Bin Xiao",
            "Bingning Wang",
            "Borong Zhang",
            "Ce Bian",
            "Chao Yin",
            "Chenxu Lv",
            "Da Pan",
            "Dian Wang",
            "Dong Yan",
            "Fan Yang",
            "Fei Deng",
            "Feng Wang",
            "Feng Liu",
            "Guangwei Ai",
            "Guosheng Dong",
            "Hai Zhao",
            "Hang Xu",
            "Hao-Lun Sun",
            "Hongda Zhang",
            "Hui Liu",
            "Jiaming Ji",
            "Jian Xie",
            "Juntao Dai",
            "Kuncheng Fang",
            "Lei Su",
            "Liang Song",
            "Lifeng Liu",
            "Liyun Ru",
            "Luyao Ma",
            "Mang Wang",
            "Mickel Liu",
            "Mingan Lin",
            "Nuolan Nie",
            "Pei Guo",
            "Ruiyang Sun",
            "Zhang Tao",
            "Tianpeng Li",
            "Tianyu Li",
            "Wei Cheng",
            "Weipeng Chen",
            "Xiangrong Zeng",
            "Xiaochuan Wang",
            "Xiaoxi Chen",
            "Xin Men",
            "Xin Yu",
            "Xuehai Pan",
            "Yan-Bin Shen",
            "Yiding Wang",
            "Yiyu Li",
            "Youxin Jiang",
            "Yuchen Gao",
            "Yupeng Zhang",
            "Zenan Zhou",
            "Zhiying Wu"
        ],
        "citations": 602,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
        "abstract": "Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.",
        "authors": [
            "Zilong Wang",
            "Hao Zhang",
            "Chun-Liang Li",
            "Julian Martin Eisenschlos",
            "Vincent Perot",
            "Zifeng Wang",
            "Lesly Miculicich",
            "Yasuhisa Fujii",
            "Jingbo Shang",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "citations": 58,
        "references": 44,
        "year": 2024
    },
    {
        "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
        "abstract": "As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.",
        "authors": [
            "Xin Men",
            "Mingyu Xu",
            "Qingyu Zhang",
            "Bingning Wang",
            "Hongyu Lin",
            "Yaojie Lu",
            "Xianpei Han",
            "Weipeng Chen"
        ],
        "citations": 60,
        "references": 58,
        "year": 2024
    },
    {
        "title": "Exploring EFL university teachers’ beliefs in integrating ChatGPT and other large language models in language education: a study in China",
        "abstract": "ABSTRACT Nowadays, the prevalence of ChatGPT and other Large Language Models (LLMs) has posed significant challenges into the education field, particularly in English education. In response, this study aimed to investigate the beliefs of 95 EFL university teachers from Chinese universities regarding the integration of LLMs in language education, as well as the relationships between their beliefs and other factors. The study yielded several findings: (1) According to the quantitative and qualitative results, we revealed several concerns among Chinese EFL university teachers regarding LLMs integration, such as neglection of traditional learning resources, academic integrity, and excessive reliance. (2) Previous experiences with LLMs, frequency of LLMs use, and self-evaluation on stages of LLMs integration all played vital roles in shaping university teachers’ beliefs in integrating LLMs in language education. (3) No significant correlation was observed between university teachers’ beliefs in integrating LLMs in language education and the availability of IT personnel. (4) No significant correlation was observed between university teachers’ beliefs in integrating LLMs in language education their evaluation on IT infrastructure. This research has provided some insights into university teachers’ beliefs in ChatGPT and other LLMs to promote effective policies and strategies in the digital era.",
        "authors": [
            "Yang Gao",
            "Qikai Wang",
            "Xiaochen Wang"
        ],
        "citations": 60,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
        "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
        "authors": [
            "Shirui Pan",
            "Linhao Luo",
            "Yufei Wang",
            "Chen Chen",
            "Jiapu Wang",
            "Xindong Wu"
        ],
        "citations": 552,
        "references": 300,
        "year": 2023
    },
    {
        "title": "Chain-of-Thought Reasoning Without Prompting",
        "abstract": "In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding.",
        "authors": [
            "Xuezhi Wang",
            "Denny Zhou"
        ],
        "citations": 57,
        "references": 55,
        "year": 2024
    },
    {
        "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
        "abstract": "Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.",
        "authors": [
            "Shubham Toshniwal",
            "Ivan Moshkov",
            "Sean Narenthiran",
            "Daria Gitman",
            "Fei Jia",
            "Igor Gitman"
        ],
        "citations": 53,
        "references": 40,
        "year": 2024
    },
    {
        "title": "Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation",
        "abstract": null,
        "authors": [
            "Elizabeth C. Stade",
            "S. Stirman",
            "L. Ungar",
            "Cody L. Boland",
            "H. A. Schwartz",
            "D. Yaden",
            "João Sedoc",
            "Robert J. DeRubeis",
            "Robb Willer",
            "J. Eichstaedt"
        ],
        "citations": 53,
        "references": 82,
        "year": 2024
    },
    {
        "title": "Large Language Models and Games: A Survey and Roadmap",
        "abstract": "Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.",
        "authors": [
            "Roberto Gallotta",
            "Graham Todd",
            "Marvin Zammit",
            "Sam Earle",
            "Antonios Liapis",
            "Julian Togelius",
            "Georgios N. Yannakakis"
        ],
        "citations": 52,
        "references": 190,
        "year": 2024
    },
    {
        "title": "Evaluating Large Language Models in Class-Level Code Generation",
        "abstract": "Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a sim-ple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios. To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.",
        "authors": [
            "Xueying Du",
            "Mingwei Liu",
            "Kaixin Wang",
            "Hanlin Wang",
            "Junwei Liu",
            "Yixuan Chen",
            "Jiayi Feng",
            "Chaofeng Sha",
            "Xin Peng",
            "Yiling Lou"
        ],
        "citations": 51,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs",
        "abstract": "Jailbreak attacks aim to bypass the safeguards of LLMs. While researchers have studied different jailbreak attacks in depth, they have done so in isolation -- either with unaligned experiment settings or comparing a limited range of methods. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We collect 17 cutting-edge jailbreak methods, summarize their features, and establish a novel jailbreak attack taxonomy. Based on eight popular censored LLMs and 160 questions from 16 violation categories, we conduct a unified and impartial assessment of attack effectiveness as well as a comprehensive ablation study. Our extensive experimental results demonstrate that all the jailbreak attacks have a powerful effect on the LLMs. This indicates that all LLMs fail to cover all the violation categories, and they are susceptible to significant jailbreak risks, with even the well-aligned Llama3 facing a maximum attack success rate of 0.88. Additionally, we test jailbreak attacks under eight advanced external defenses and find none of the defenses could mitigate the jailbreak attacks entirely. Our study offers valuable insights for future research on jailbreak attacks and defenses and serves as a benchmark tool for researchers and practitioners to evaluate them effectively.",
        "authors": [
            "Junjie Chu",
            "Yugeng Liu",
            "Ziqing Yang",
            "Xinyue Shen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "citations": 50,
        "references": 49,
        "year": 2024
    },
    {
        "title": "TempCompass: Do Video LLMs Really Understand Videos?",
        "abstract": "Recently, there is a surge in interest surrounding video large language models (Video LLMs). However, existing benchmarks fail to provide a comprehensive feedback on the temporal perception ability of Video LLMs. On the one hand, most of them are unable to distinguish between different temporal aspects (e.g., speed, direction) and thus cannot reflect the nuanced performance on these specific aspects. On the other hand, they are limited in the diversity of task formats (e.g., only multi-choice QA), which hinders the understanding of how temporal perception performance may vary across different types of tasks. Motivated by these two problems, we propose the \\textbf{TempCompass} benchmark, which introduces a diversity of temporal aspects and task formats. To collect high-quality test data, we devise two novel strategies: (1) In video collection, we construct conflicting videos that share the same static content but differ in a specific temporal aspect, which prevents Video LLMs from leveraging single-frame bias or language priors. (2) To collect the task instructions, we propose a paradigm where humans first annotate meta-information for a video and then an LLM generates the instruction. We also design an LLM-based approach to automatically and accurately evaluate the responses from Video LLMs. Based on TempCompass, we comprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs, and reveal the discerning fact that these models exhibit notably poor temporal perception ability. Our data will be available at https://github.com/llyx97/TempCompass.",
        "authors": [
            "Yuanxin Liu",
            "Shicheng Li",
            "Yi Liu",
            "Yuxiang Wang",
            "Shuhuai Ren",
            "Lei Li",
            "Sishuo Chen",
            "Xu Sun",
            "Lu Hou"
        ],
        "citations": 49,
        "references": 39,
        "year": 2024
    },
    {
        "title": "AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs",
        "abstract": "As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs. In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs. In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend.",
        "authors": [
            "Zeyi Liao",
            "Huan Sun"
        ],
        "citations": 47,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs",
        "abstract": null,
        "authors": [
            "Li Wang",
            "Xi Chen",
            "Xiangwen Deng",
            "Hao Wen",
            "M. You",
            "Weizhi Liu",
            "Qi Li",
            "Jian Li"
        ],
        "citations": 48,
        "references": 31,
        "year": 2024
    },
    {
        "title": "Large Language Model for Vulnerability Detection: Emerging Results and Future Directions",
        "abstract": "Previous learning-based vulnerability detection methods relied on either medium-sized pretrained models or smaller neural networks from scratch. Recent advancements in Large Pre-Trained Language Models (LLMs) have showcased remarkable few-shot learning capabilities in various tasks. However, the effectiveness of LLMs in detecting software vulnerabilities is largely unexplored. This paper aims to bridge this gap by exploring how LLMs perform with various prompts, particularly focusing on two state-of-the-art LLMs: GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achieves competitive performance with the prior state-of-the-art vulnerability detection approach and GPT-4 consistently outperformed the state-of-the-art.",
        "authors": [
            "Xin Zhou",
            "Ting Zhang",
            "David Lo"
        ],
        "citations": 43,
        "references": 37,
        "year": 2024
    },
    {
        "title": "AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories",
        "abstract": "We illustrate how standard psychometric inventories originally designed for assessing noncognitive human traits can be repurposed as diagnostic tools to evaluate analogous traits in large language models (LLMs). We start from the assumption that LLMs, inadvertently yet inevitably, acquire psychological traits (metaphorically speaking) from the vast text corpora on which they are trained. Such corpora contain sediments of the personalities, values, beliefs, and biases of the countless human authors of these texts, which LLMs learn through a complex training process. The traits that LLMs acquire in such a way can potentially influence their behavior, that is, their outputs in downstream tasks and applications in which they are employed, which in turn may have real-world consequences for individuals and social groups. By eliciting LLMs’ responses to language-based psychometric inventories, we can bring their traits to light. Psychometric profiling enables researchers to study and compare LLMs in terms of noncognitive characteristics, thereby providing a window into the personalities, values, beliefs, and biases these models exhibit (or mimic). We discuss the history of similar ideas and outline possible psychometric approaches for LLMs. We demonstrate one promising approach, zero-shot classification, for several LLMs and psychometric inventories. We conclude by highlighting open challenges and future avenues of research for AI Psychometrics.",
        "authors": [
            "Max Pellert",
            "Clemens M. Lechner",
            "Claudia Wagner",
            "Beatrice Rammstedt",
            "Markus Strohmaier"
        ],
        "citations": 45,
        "references": 94,
        "year": 2024
    },
    {
        "title": "Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models",
        "abstract": "\n Do large language models (LLMs) know the law? LLMs are increasingly being used to augment legal practice, education, and research, yet their revolutionary potential is threatened by the presence of “hallucinations”—textual output that is not consistent with legal facts. We present the first systematic evidence of these hallucinations in public-facing LLMs, documenting trends across jurisdictions, courts, time periods, and cases. Using OpenAI’s ChatGPT 4 and other public models, we show that LLMs hallucinate at least 58% of the time, struggle to predict their own hallucinations, and often uncritically accept users’ incorrect legal assumptions. We conclude by cautioning against the rapid and unsupervised integration of popular LLMs into legal tasks, and we develop a typology of legal hallucinations to guide future research in this area.",
        "authors": [
            "Matthew Dahl",
            "Varun Magesh",
            "Mirac Suzgun",
            "Daniel E. Ho"
        ],
        "citations": 44,
        "references": 130,
        "year": 2024
    },
    {
        "title": "Revolutionizing Finance with LLMs: An Overview of Applications and Insights",
        "abstract": "In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields. Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively. In the financial domain, the deployment of LLMs is gaining momentum. These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction. In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks. Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions. Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks. This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry.",
        "authors": [
            "Huaqin Zhao",
            "Zheng Liu",
            "Zihao Wu",
            "Yiwei Li",
            "Tianze Yang",
            "Peng Shu",
            "Shaochen Xu",
            "Haixing Dai",
            "Lin Zhao",
            "Gengchen Mai",
            "Ninghao Liu",
            "Tianming Liu"
        ],
        "citations": 43,
        "references": 136,
        "year": 2024
    },
    {
        "title": "Knowledge Conflicts for LLMs: A Survey",
        "abstract": "This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.",
        "authors": [
            "Rongwu Xu",
            "Zehan Qi",
            "Zhijiang Guo",
            "Cunxiang Wang",
            "Hongru Wang",
            "Yue Zhang",
            "Wei Xu"
        ],
        "citations": 45,
        "references": 200,
        "year": 2024
    },
    {
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
        "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM",
        "authors": [
            "Ziyang Luo",
            "Can Xu",
            "Pu Zhao",
            "Qingfeng Sun",
            "Xiubo Geng",
            "Wenxiang Hu",
            "Chongyang Tao",
            "Jing Ma",
            "Qingwei Lin",
            "Daxin Jiang"
        ],
        "citations": 501,
        "references": 37,
        "year": 2023
    },
    {
        "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
        "abstract": "The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.",
        "authors": [
            "Biyang Guo",
            "Xin Zhang",
            "Ziyuan Wang",
            "Minqi Jiang",
            "Jinran Nie",
            "Yuxuan Ding",
            "Jianwei Yue",
            "Yupeng Wu"
        ],
        "citations": 504,
        "references": 47,
        "year": 2023
    },
    {
        "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
        "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
        "authors": [
            "Yujia Qin",
            "Shi Liang",
            "Yining Ye",
            "Kunlun Zhu",
            "Lan Yan",
            "Ya-Ting Lu",
            "Yankai Lin",
            "Xin Cong",
            "Xiangru Tang",
            "Bill Qian",
            "Sihan Zhao",
            "Runchu Tian",
            "Ruobing Xie",
            "Jie Zhou",
            "M. Gerstein",
            "Dahai Li",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "citations": 464,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Large Language Models for Robotics: Opportunities, Challenges, and Perspectives",
        "abstract": "Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.",
        "authors": [
            "Jiaqi Wang",
            "Zihao Wu",
            "Yiwei Li",
            "Hanqi Jiang",
            "Peng Shu",
            "Enze Shi",
            "Huawen Hu",
            "Chong-Yi Ma",
            "Yi-Hsueh Liu",
            "Xuhui Wang",
            "Yincheng Yao",
            "Xuan Liu",
            "Huaqin Zhao",
            "Zheng Liu",
            "Haixing Dai",
            "Lin Zhao",
            "Bao Ge",
            "Xiang Li",
            "Tianming Liu",
            "Shu Zhang"
        ],
        "citations": 41,
        "references": 118,
        "year": 2024
    },
    {
        "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
        "abstract": "The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as\"catastrophic forgetting\". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.",
        "authors": [
            "Haizhou Shi",
            "Zihao Xu",
            "Hengyi Wang",
            "Weiyi Qin",
            "Wenyuan Wang",
            "Yibin Wang",
            "Hao Wang"
        ],
        "citations": 40,
        "references": 338,
        "year": 2024
    },
    {
        "title": "Large Language Models for Education: A Survey and Outlook",
        "abstract": "The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.",
        "authors": [
            "Shen Wang",
            "Tianlong Xu",
            "Hang Li",
            "Chaoli Zhang",
            "Joleen Liang",
            "Jiliang Tang",
            "Philip S. Yu",
            "Qingsong Wen"
        ],
        "citations": 40,
        "references": 179,
        "year": 2024
    },
    {
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment",
        "abstract": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.",
        "authors": [
            "Andreas Kopf",
            "Yannic Kilcher",
            "Dimitri von Rutte",
            "Sotiris Anagnostidis",
            "Zhi Rui Tam",
            "K. Stevens",
            "Abdullah Barhoum",
            "Nguyen Minh Duc",
            "Oliver Stanley",
            "Rich'ard Nagyfi",
            "ES Shahul",
            "Sameer Suri",
            "David Glushkov",
            "Arnav Dantuluri",
            "Andrew Maguire",
            "Christoph Schuhmann",
            "Huu Nguyen",
            "A. Mattick"
        ],
        "citations": 501,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models",
        "abstract": "Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on several representative LLMs, such as LLaMA-2, BLOOM, and Mistral. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to\"steer\"the output language of LLMs by selectively activating or deactivating language-specific neurons. Our research provides important evidence to the understanding and exploration of the multilingual capabilities of LLMs.",
        "authors": [
            "Tianyi Tang",
            "Wenyang Luo",
            "Haoyang Huang",
            "Dongdong Zhang",
            "Xiaolei Wang",
            "Xin Zhao",
            "Furu Wei",
            "Ji-Rong Wen"
        ],
        "citations": 37,
        "references": 49,
        "year": 2024
    },
    {
        "title": "Large Language Models for Time Series: A Survey",
        "abstract": "Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets in diverse domains, and discusses the challenges and future opportunities of this emerging field.",
        "authors": [
            "Xiyuan Zhang",
            "Ranak Roy Chowdhury",
            "Rajesh K. Gupta",
            "Jingbo Shang"
        ],
        "citations": 37,
        "references": 97,
        "year": 2024
    },
    {
        "title": "Efficient Streaming Language Models with Attention Sinks",
        "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
        "authors": [
            "Guangxuan Xiao",
            "Yuandong Tian",
            "Beidi Chen",
            "Song Han",
            "Mike Lewis"
        ],
        "citations": 418,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
        "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.",
        "authors": [
            "Akari Asai",
            "Zeqiu Wu",
            "Yizhong Wang",
            "Avirup Sil",
            "Hannaneh Hajishirzi"
        ],
        "citations": 411,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
        "abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the\"truthfulness\"of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.",
        "authors": [
            "Kenneth Li",
            "Oam Patel",
            "Fernanda Vi'egas",
            "H. Pfister",
            "M. Wattenberg"
        ],
        "citations": 354,
        "references": 164,
        "year": 2023
    },
    {
        "title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
        "abstract": "Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of\"scientific language\", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.",
        "authors": [
            "Qiang Zhang",
            "Keyan Ding",
            "Tianwen Lyv",
            "Xinda Wang",
            "Qingyu Yin",
            "Yiwen Zhang",
            "Jing Yu",
            "Yuhao Wang",
            "Xiaotong Li",
            "Zhuoyi Xiang",
            "Zhuang Xiang",
            "Zeyuan Wang",
            "Ming Qin",
            "Mengyao Zhang",
            "Jinlu Zhang",
            "Jiyu Cui",
            "Renjun Xu",
            "Hongyan Chen",
            "Xiaohui Fan",
            "Huabin Xing",
            "Huajun Chen"
        ],
        "citations": 35,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
        "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
        "authors": [
            "Xiangyu Qi",
            "Yi Zeng",
            "Tinghao Xie",
            "Pin-Yu Chen",
            "Ruoxi Jia",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "citations": 382,
        "references": 92,
        "year": 2023
    },
    {
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
        "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.",
        "authors": [
            "Yuzhen Huang",
            "Yuzhuo Bai",
            "Zhihao Zhu",
            "Junlei Zhang",
            "Jinghan Zhang",
            "Tangjun Su",
            "Junteng Liu",
            "Chuancheng Lv",
            "Yikai Zhang",
            "Jiayi Lei",
            "Fanchao Qi",
            "Yao Fu",
            "Maosong Sun",
            "Junxian He"
        ],
        "citations": 406,
        "references": 48,
        "year": 2023
    },
    {
        "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
        "abstract": "Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.",
        "authors": [
            "Bohao Li",
            "Rui Wang",
            "Guangzhi Wang",
            "Yuying Ge",
            "Yixiao Ge",
            "Ying Shan"
        ],
        "citations": 373,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
        "authors": [
            "Yilun Du",
            "Shuang Li",
            "A. Torralba",
            "J. Tenenbaum",
            "Igor Mordatch"
        ],
        "citations": 428,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Gorilla: Large Language Model Connected with Massive APIs",
        "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu",
        "authors": [
            "Shishir G. Patil",
            "Tianjun Zhang",
            "Xin Wang",
            "Joseph E. Gonzalez"
        ],
        "citations": 387,
        "references": 48,
        "year": 2023
    },
    {
        "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
        "abstract": "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at https://github.com/THUDM/LongBench.",
        "authors": [
            "Yushi Bai",
            "Xin Lv",
            "Jiajie Zhang",
            "Hong Lyu",
            "Jiankai Tang",
            "Zhidian Huang",
            "Zhengxiao Du",
            "Xiao Liu",
            "Aohan Zeng",
            "Lei Hou",
            "Yuxiao Dong",
            "Jie Tang",
            "Juanzi Li"
        ],
        "citations": 315,
        "references": 69,
        "year": 2023
    },
    {
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.",
        "authors": [
            "Yue Wang",
            "Hung Le",
            "Akhilesh Deepak Gotmare",
            "Nghi D. Q. Bui",
            "Junnan Li",
            "Steven C. H. Hoi"
        ],
        "citations": 367,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
        "abstract": "This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide. An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai.",
        "authors": [
            "Jingfeng Yang",
            "Hongye Jin",
            "Ruixiang Tang",
            "Xiaotian Han",
            "Qizhang Feng",
            "Haoming Jiang",
            "Bing Yin",
            "Xia Hu"
        ],
        "citations": 479,
        "references": 171,
        "year": 2023
    },
    {
        "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
        "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
        "authors": [
            "Wenlong Huang",
            "Chen Wang",
            "Ruohan Zhang",
            "Yunzhu Li",
            "Jiajun Wu",
            "Li Fei-Fei"
        ],
        "citations": 375,
        "references": 146,
        "year": 2023
    },
    {
        "title": "Textbooks Are All You Need II: phi-1.5 technical report",
        "abstract": "We continue the investigation into the power of smaller Transformer-based language models as initiated by \\textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality\"data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need\"approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \\textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step\"or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \\textbf{phi-1.5} to promote further research on these urgent topics.",
        "authors": [
            "Yuan-Fang Li",
            "Sébastien Bubeck",
            "Ronen Eldan",
            "Allison Del Giorno",
            "Suriya Gunasekar",
            "Yin Tat Lee"
        ],
        "citations": 357,
        "references": 39,
        "year": 2023
    },
    {
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
        "abstract": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM",
        "authors": [
            "Haipeng Luo",
            "Qingfeng Sun",
            "Can Xu",
            "Pu Zhao",
            "Jian-Guang Lou",
            "Chongyang Tao",
            "Xiubo Geng",
            "Qingwei Lin",
            "Shifeng Chen",
            "Dongmei Zhang"
        ],
        "citations": 329,
        "references": 107,
        "year": 2023
    },
    {
        "title": "On Protecting the Data Privacy of Large Language Models (LLMs): A Survey",
        "abstract": "Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language. They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks. When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy. This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding. Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs. Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints. Finally, the discourse extends to delineate the challenges encountered and outline prospective directions for advancement in the realm of LLM privacy protection.",
        "authors": [
            "Biwei Yan",
            "Kun Li",
            "Minghui Xu",
            "Yueyan Dong",
            "Yue Zhang",
            "Zhaochun Ren",
            "Xiuzhen Cheng"
        ],
        "citations": 33,
        "references": 139,
        "year": 2024
    },
    {
        "title": "Mind2Web: Towards a Generalist Agent for the Web",
        "abstract": "We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.",
        "authors": [
            "Xiang Deng",
            "Yu Gu",
            "Boyuan Zheng",
            "Shijie Chen",
            "Samuel Stevens",
            "Boshi Wang",
            "Huan Sun",
            "Yu Su"
        ],
        "citations": 270,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
        "authors": [
            "Tianyu Gao",
            "Howard Yen",
            "Jiatong Yu",
            "Danqi Chen"
        ],
        "citations": 252,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
        "abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.",
        "authors": [
            "Stephen Casper",
            "Xander Davies",
            "Claudia Shi",
            "T. Gilbert",
            "J'er'emy Scheurer",
            "Javier Rando",
            "Rachel Freedman",
            "Tomasz Korbak",
            "David Lindner",
            "Pedro Freire",
            "Tony Wang",
            "Samuel Marks",
            "Charbel-Raphaël Ségerie",
            "Micah Carroll",
            "Andi Peng",
            "Phillip J. K. Christoffersen",
            "Mehul Damani",
            "Stewart Slocum",
            "Usman Anwar",
            "Anand Siththaranjan",
            "Max Nadeau",
            "Eric J. Michaud",
            "J. Pfau",
            "Dmitrii Krasheninnikov",
            "Xin Chen",
            "L. Langosco",
            "Peter Hase",
            "Erdem Biyik",
            "A. Dragan",
            "David Krueger",
            "Dorsa Sadigh",
            "Dylan Hadfield-Menell"
        ],
        "citations": 367,
        "references": 291,
        "year": 2023
    },
    {
        "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing",
        "abstract": "Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.",
        "authors": [
            "Ye Tian",
            "Baolin Peng",
            "Linfeng Song",
            "Lifeng Jin",
            "Dian Yu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "citations": 33,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Can AI-Generated Text be Reliably Detected?",
        "abstract": "Large Language Models (LLMs) perform impressively well in various applications. However, the potential for misuse of these models in activities such as plagiarism, generating fake news, and spamming has raised concern about their responsible use. Consequently, the reliable detection of AI-generated text has become a critical area of research. AI text detectors have shown to be effective under their specific settings. In this paper, we stress-test the robustness of these AI text detectors in the presence of an attacker. We introduce recursive paraphrasing attack to stress test a wide range of detection schemes, including the ones using the watermarking as well as neural network-based detectors, zero shot classifiers, and retrieval-based detectors. Our experiments conducted on passages, each approximately 300 tokens long, reveal the varying sensitivities of these detectors to our attacks. Our findings indicate that while our recursive paraphrasing method can significantly reduce detection rates, it only slightly degrades text quality in many cases, highlighting potential vulnerabilities in current detection systems in the presence of an attacker. Additionally, we investigate the susceptibility of watermarked LLMs to spoofing attacks aimed at misclassifying human-written text as AI-generated. We demonstrate that an attacker can infer hidden AI text signatures without white-box access to the detection method, potentially leading to reputational risks for LLM developers. Finally, we provide a theoretical framework connecting the AUROC of the best possible detector to the Total Variation distance between human and AI text distributions. This analysis offers insights into the fundamental challenges of reliable detection as language models continue to advance. Our code is publicly available at https://github.com/vinusankars/Reliability-of-AI-text-detectors.",
        "authors": [
            "Vinu Sankar Sadasivan",
            "Aounon Kumar",
            "S. Balasubramanian",
            "Wenxiao Wang",
            "S. Feizi"
        ],
        "citations": 309,
        "references": 52,
        "year": 2023
    },
    {
        "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
        "abstract": "Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.",
        "authors": [
            "Yunxiang Li",
            "Zihan Li",
            "Kai Zhang",
            "Ruilong Dan",
            "Steven Jiang",
            "You Zhang"
        ],
        "citations": 285,
        "references": 19,
        "year": 2023
    },
    {
        "title": "Faith and Fate: Limits of Transformers on Compositionality",
        "abstract": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.",
        "authors": [
            "Nouha Dziri",
            "Ximing Lu",
            "Melanie Sclar",
            "Xiang Lorraine Li",
            "Liwei Jian",
            "Bill Yuchen Lin",
            "Peter West",
            "Chandra Bhagavatula",
            "Ronan Le Bras",
            "Jena D. Hwang",
            "Soumya Sanyal",
            "S. Welleck",
            "Xiang Ren",
            "Allyson Ettinger",
            "Zaïd Harchaoui",
            "Yejin Choi"
        ],
        "citations": 265,
        "references": 81,
        "year": 2023
    },
    {
        "title": "A survey on multimodal large language models",
        "abstract": "ABSTRACT Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.",
        "authors": [
            "Shukang Yin",
            "Chaoyou Fu",
            "Sirui Zhao",
            "Ke Li",
            "Xing Sun",
            "Tong Xu",
            "Enhong Chen"
        ],
        "citations": 393,
        "references": 250,
        "year": 2023
    },
    {
        "title": "Can AI-Generated Text be Reliably Detected?",
        "abstract": "Large Language Models (LLMs) perform impressively well in various applications. However, the potential for misuse of these models in activities such as plagiarism, generating fake news, and spamming has raised concern about their responsible use. Consequently, the reliable detection of AI-generated text has become a critical area of research. AI text detectors have shown to be effective under their specific settings. In this paper, we stress-test the robustness of these AI text detectors in the presence of an attacker. We introduce recursive paraphrasing attack to stress test a wide range of detection schemes, including the ones using the watermarking as well as neural network-based detectors, zero shot classifiers, and retrieval-based detectors. Our experiments conducted on passages, each approximately 300 tokens long, reveal the varying sensitivities of these detectors to our attacks. Our findings indicate that while our recursive paraphrasing method can significantly reduce detection rates, it only slightly degrades text quality in many cases, highlighting potential vulnerabilities in current detection systems in the presence of an attacker. Additionally, we investigate the susceptibility of watermarked LLMs to spoofing attacks aimed at misclassifying human-written text as AI-generated. We demonstrate that an attacker can infer hidden AI text signatures without white-box access to the detection method, potentially leading to reputational risks for LLM developers. Finally, we provide a theoretical framework connecting the AUROC of the best possible detector to the Total Variation distance between human and AI text distributions. This analysis offers insights into the fundamental challenges of reliable detection as language models continue to advance. Our code is publicly available at https://github.com/vinusankars/Reliability-of-AI-text-detectors.",
        "authors": [
            "Vinu Sankar Sadasivan",
            "Aounon Kumar",
            "S. Balasubramanian",
            "Wenxiao Wang",
            "S. Feizi"
        ],
        "citations": 309,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Faith and Fate: Limits of Transformers on Compositionality",
        "abstract": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.",
        "authors": [
            "Nouha Dziri",
            "Ximing Lu",
            "Melanie Sclar",
            "Xiang Lorraine Li",
            "Liwei Jian",
            "Bill Yuchen Lin",
            "Peter West",
            "Chandra Bhagavatula",
            "Ronan Le Bras",
            "Jena D. Hwang",
            "Soumya Sanyal",
            "S. Welleck",
            "Xiang Ren",
            "Allyson Ettinger",
            "Zaïd Harchaoui",
            "Yejin Choi"
        ],
        "citations": 265,
        "references": 81,
        "year": 2023
    },
    {
        "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
        "abstract": "Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.",
        "authors": [
            "Yunxiang Li",
            "Zihan Li",
            "Kai Zhang",
            "Ruilong Dan",
            "Steven Jiang",
            "You Zhang"
        ],
        "citations": 285,
        "references": 19,
        "year": 2023
    },
    {
        "title": "A survey on multimodal large language models",
        "abstract": "ABSTRACT Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.",
        "authors": [
            "Shukang Yin",
            "Chaoyou Fu",
            "Sirui Zhao",
            "Ke Li",
            "Xing Sun",
            "Tong Xu",
            "Enhong Chen"
        ],
        "citations": 393,
        "references": 250,
        "year": 2023
    },
    {
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
        "abstract": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
        "authors": [
            "L. Yu",
            "Weisen Jiang",
            "Han Shi",
            "Jincheng Yu",
            "Zhengying Liu",
            "Yu Zhang",
            "James T. Kwok",
            "Zheng Li",
            "Adrian Weller",
            "Weiyang Liu"
        ],
        "citations": 236,
        "references": 84,
        "year": 2023
    },
    {
        "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
        "abstract": "Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.",
        "authors": [
            "Jiahao Yu",
            "Xingwei Lin",
            "Zheng Yu",
            "Xinyu Xing"
        ],
        "citations": 220,
        "references": 78,
        "year": 2023
    },
    {
        "title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
        "abstract": "We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.",
        "authors": [
            "M. Pourreza",
            "Davood Rafiei"
        ],
        "citations": 222,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
        "authors": [
            "Weiwei Sun",
            "Lingyong Yan",
            "Xinyu Ma",
            "Pengjie Ren",
            "Dawei Yin",
            "Z. Ren"
        ],
        "citations": 226,
        "references": 46,
        "year": 2023
    },
    {
        "title": "A Survey for In-context Learning",
        "abstract": "With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples. It has been a new trend exploring ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress, challenges, and future work in ICL. We ﬁrst present a formal deﬁnition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques of ICL, including training strategies, prompting strategies, and so on. Finally, we present the challenges of ICL and provide potential directions for further research. We hope our work can encourage more research on uncovering how ICL works and improving ICL in future work. 1",
        "authors": [
            "Qingxiu Dong",
            "Lei Li",
            "Damai Dai",
            "Ce Zheng",
            "Zhiyong Wu",
            "Baobao Chang",
            "Xu Sun",
            "Jingjing Xu",
            "Lei Li",
            "Zhifang Sui"
        ],
        "citations": 344,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Zero-Shot Information Extraction via Chatting with ChatGPT",
        "abstract": "Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.",
        "authors": [
            "Xiang Wei",
            "Xingyu Cui",
            "Ning Cheng",
            "Xiaobin Wang",
            "Xin Zhang",
            "Shen Huang",
            "Pengjun Xie",
            "Jinan Xu",
            "Yufeng Chen",
            "Meishan Zhang",
            "Yong Jiang",
            "Wenjuan Han"
        ],
        "citations": 273,
        "references": 43,
        "year": 2023
    },
    {
        "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
        "abstract": "Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially\"black boxes\"to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.",
        "authors": [
            "Zhibin Gou",
            "Zhihong Shao",
            "Yeyun Gong",
            "Yelong Shen",
            "Yujiu Yang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "citations": 274,
        "references": 144,
        "year": 2023
    },
    {
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
        "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",
        "authors": [
            "Teven Le Scao",
            "Angela Fan",
            "Christopher Akiki",
            "Ellie Pavlick",
            "Suzana Ili'c",
            "Daniel Hesslow",
            "Roman Castagn'e",
            "A. Luccioni",
            "François Yvon",
            "Matthias Gallé",
            "J. Tow",
            "Alexander M. Rush",
            "Stella Biderman",
            "Albert Webson",
            "Pawan Sasanka Ammanamanchi",
            "Thomas Wang",
            "Benoît Sagot",
            "Niklas Muennighoff",
            "Albert Villanova del Moral",
            "Olatunji Ruwase",
            "Rachel Bawden",
            "Stas Bekman",
            "Angelina McMillan-Major",
            "Iz Beltagy",
            "Huu Nguyen",
            "Lucile Saulnier",
            "Samson Tan",
            "Pedro Ortiz Suarez",
            "Victor Sanh",
            "Hugo Laurenccon",
            "Yacine Jernite",
            "Julien Launay",
            "Margaret Mitchell",
            "Colin Raffel",
            "Aaron Gokaslan",
            "Adi Simhi",
            "Aitor Soroa Etxabe",
            "Alham Fikri Aji",
            "Amit Alfassy",
            "Anna Rogers",
            "Ariel Kreisberg Nitzav",
            "Canwen Xu",
            "Chenghao Mou",
            "Chris C. Emezue",
            "Christopher Klamm",
            "Colin Leong",
            "Daniel Alexander van Strien",
            "David Ifeoluwa Adelani",
            "Dragomir R. Radev",
            "E. G. Ponferrada",
            "Efrat Levkovizh",
            "Ethan Kim",
            "Eyal Natan",
            "F. Toni",
            "Gérard Dupont",
            "Germán Kruszewski",
            "Giada Pistilli",
            "Hady ElSahar",
            "Hamza Benyamina",
            "H. Tran",
            "Ian Yu",
            "Idris Abdulmumin",
            "Isaac Johnson",
            "Itziar Gonzalez-Dios",
            "Javier de la Rosa",
            "Jenny Chim",
            "Jesse Dodge",
            "Jian Zhu",
            "Jonathan Chang",
            "Jorg Frohberg",
            "Josephine Tobing",
            "J. Bhattacharjee",
            "Khalid Almubarak",
            "Kimbo Chen",
            "Kyle Lo",
            "L. V. Werra",
            "Leon Weber",
            "Long Phan",
            "Loubna Ben Allal",
            "Ludovic Tanguy",
            "Manan Dey",
            "M. Muñoz",
            "Maraim Masoud",
            "María Grandury",
            "Mario vSavsko",
            "Max Huang",
            "Maximin Coavoux",
            "Mayank Singh",
            "Mike Tian-Jian Jiang",
            "Minh Chien Vu",
            "M. A. Jauhar",
            "Mustafa Ghaleb",
            "Nishant Subramani",
            "Nora Kassner",
            "Nurulaqilla Khamis",
            "Olivier Nguyen",
            "Omar Espejel",
            "Ona de Gibert",
            "Paulo Villegas",
            "Peter Henderson",
            "Pierre Colombo",
            "Priscilla Amuok",
            "Quentin Lhoest",
            "Rheza Harliman",
            "Rishi Bommasani",
            "R. L'opez",
            "Rui Ribeiro",
            "Salomey Osei",
            "S. Pyysalo",
            "Sebastian Nagel",
            "Shamik Bose",
            "Shamsuddeen Hassan Muhammad",
            "Shanya Sharma",
            "S. Longpre",
            "Somaieh Nikpoor",
            "S. Silberberg",
            "S. Pai",
            "S. Zink",
            "Tiago Timponi Torrent",
            "Timo Schick",
            "Tristan Thrush",
            "V. Danchev",
            "Vassilina Nikoulina",
            "Veronika Laippala",
            "Violette Lepercq",
            "V. Prabhu",
            "Zaid Alyafeai",
            "Zeerak Talat",
            "Arun Raja",
            "Benjamin Heinzerling",
            "Chenglei Si",
            "Elizabeth Salesky",
            "Sabrina J. Mielke",
            "Wilson Y. Lee",
            "Abheesht Sharma",
            "Andrea Santilli",
            "Antoine Chaffin",
            "Arnaud Stiegler",
            "Debajyoti Datta",
            "Eliza Szczechla",
            "Gunjan Chhablani",
            "Han Wang",
            "Harshit Pandey",
            "Hendrik Strobelt",
            "Jason Alan Fries",
            "Jos Rozen",
            "Leo Gao",
            "Lintang Sutawika",
            "M Saiful Bari",
            "Maged S. Al-Shaibani",
            "Matteo Manica",
            "Nihal V. Nayak",
            "Ryan Teehan",
            "Samuel Albanie",
            "Sheng Shen",
            "Srulik Ben-David",
            "Stephen H. Bach",
            "Taewoon Kim",
            "T. Bers",
            "Thibault Févry",
            "Trishala Neeraj",
            "Urmish Thakker",
            "Vikas Raunak",
            "Xiang Tang",
            "Zheng-Xin Yong",
            "Zhiqing Sun",
            "Shaked Brody",
            "Y. Uri",
            "Hadar Tojarieh",
            "Adam Roberts",
            "Hyung Won Chung",
            "Jaesung Tae",
            "Jason Phang",
            "Ofir Press",
            "Conglong Li",
            "D. Narayanan",
            "Hatim Bourfoune",
            "J. Casper",
            "Jeff Rasley",
            "Max Ryabinin",
            "Mayank Mishra",
            "Minjia Zhang",
            "Mohammad Shoeybi",
            "Myriam Peyrounette",
            "N. Patry",
            "Nouamane Tazi",
            "Omar Sanseviero",
            "Patrick von Platen",
            "Pierre Cornette",
            "Pierre Franccois Lavall'ee",
            "R. Lacroix",
            "Samyam Rajbhandari",
            "Sanchit Gandhi",
            "Shaden Smith",
            "S. Requena",
            "Suraj Patil",
            "Tim Dettmers",
            "Ahmed Baruwa",
            "Amanpreet Singh",
            "Anastasia Cheveleva",
            "Anne-Laure Ligozat",
            "Arjun Subramonian",
            "Aur'elie N'ev'eol",
            "Charles Lovering",
            "Daniel H Garrette",
            "D. Tunuguntla",
            "Ehud Reiter",
            "Ekaterina Taktasheva",
            "E. Voloshina",
            "Eli Bogdanov",
            "Genta Indra Winata",
            "Hailey Schoelkopf",
            "Jan-Christoph Kalo",
            "Jekaterina Novikova",
            "J. Forde",
            "Xiangru Tang",
            "Jungo Kasai",
            "Ken Kawamura",
            "Liam Hazan",
            "Marine Carpuat",
            "Miruna Clinciu",
            "Najoung Kim",
            "Newton Cheng",
            "O. Serikov",
            "Omer Antverg",
            "Oskar van der Wal",
            "Rui Zhang",
            "Ruochen Zhang",
            "Sebastian Gehrmann",
            "Shachar Mirkin",
            "S. Pais",
            "Tatiana Shavrina",
            "Thomas Scialom",
            "Tian Yun",
            "Tomasz Limisiewicz",
            "Verena Rieser",
            "Vitaly Protasov",
            "V. Mikhailov",
            "Yada Pruksachatkun",
            "Yonatan Belinkov",
            "Zachary Bamberger",
            "Zdenvek Kasner",
            "Zdeněk Kasner",
            "A. Pestana",
            "A. Feizpour",
            "Ammar Khan",
            "Amy Faranak",
            "A. Santos",
            "Anthony Hevia",
            "Antigona Unldreaj",
            "Arash Aghagol",
            "Arezoo Abdollahi",
            "A. Tammour",
            "A. HajiHosseini",
            "Bahareh Behroozi",
            "Benjamin Ayoade Ajibade",
            "B. Saxena",
            "Carlos Muñoz Ferrandis",
            "Danish Contractor",
            "D. Lansky",
            "Davis David",
            "Douwe Kiela",
            "D. A. Nguyen",
            "Edward Tan",
            "Emi Baylor",
            "Ezinwanne Ozoani",
            "F. Mirza",
            "Frankline Ononiwu",
            "Habib Rezanejad",
            "H.A. Jones",
            "Indrani Bhattacharya",
            "Irene Solaiman",
            "Irina Sedenko",
            "Isar Nejadgholi",
            "J. Passmore",
            "Joshua Seltzer",
            "Julio Bonis Sanz",
            "Karen Fort",
            "Lívia Dutra",
            "Mairon Samagaio",
            "Maraim Elbadri",
            "Margot Mieskes",
            "Marissa Gerchick",
            "Martha Akinlolu",
            "Michael McKenna",
            "Mike Qiu",
            "M. Ghauri",
            "Mykola Burynok",
            "Nafis Abrar",
            "Nazneen Rajani",
            "Nour Elkott",
            "N. Fahmy",
            "Olanrewaju Samuel",
            "Ran An",
            "R. Kromann",
            "Ryan Hao",
            "S. Alizadeh",
            "Sarmad Shubber",
            "Silas L. Wang",
            "Sourav Roy",
            "S. Viguier",
            "Thanh-Cong Le",
            "Tobi Oyebade",
            "T. Le",
            "Yoyo Yang",
            "Zach Nguyen",
            "Abhinav Ramesh Kashyap",
            "Alfredo Palasciano",
            "A. Callahan",
            "Anima Shukla",
            "Antonio Miranda-Escalada",
            "A. Singh",
            "Benjamin Beilharz",
            "Bo Wang",
            "C. Brito",
            "Chenxi Zhou",
            "Chirag Jain",
            "Chuxin Xu",
            "Clémentine Fourrier",
            "Daniel Le'on Perin'an",
            "Daniel Molano",
            "Dian Yu",
            "Enrique Manjavacas",
            "Fabio Barth",
            "Florian Fuhrimann",
            "Gabriel Altay",
            "Giyaseddin Bayrak",
            "Gully Burns",
            "Helena U. Vrabec",
            "I. Bello",
            "Isha Dash",
            "J. Kang",
            "John Giorgi",
            "Jonas Golde",
            "J. Posada",
            "Karthi Sivaraman",
            "Lokesh Bulchandani",
            "Lu Liu",
            "Luisa Shinzato",
            "Madeleine Hahn de Bykhovetz",
            "Maiko Takeuchi",
            "Marc Pàmies",
            "M. A. Castillo",
            "Marianna Nezhurina",
            "Mario Sanger",
            "M. Samwald",
            "Michael Cullan",
            "Michael Weinberg",
            "M. Wolf",
            "Mina Mihaljcic",
            "Minna Liu",
            "M. Freidank",
            "Myungsun Kang",
            "Natasha Seelam",
            "N. Dahlberg",
            "N. Broad",
            "N. Muellner",
            "Pascale Fung",
            "Patricia Haller",
            "Patrick Haller",
            "R. Eisenberg",
            "Robert Martin",
            "Rodrigo Canalli",
            "Rosaline Su",
            "Ruisi Su",
            "Samuel Cahyawijaya",
            "Samuele Garda",
            "Shlok S Deshmukh",
            "Shubhanshu Mishra",
            "Sid Kiblawi",
            "Simon Ott",
            "Sinee Sang-aroonsiri",
            "Srishti Kumar",
            "Stefan Schweter",
            "S. Bharati",
            "Tanmay Laud",
            "Théo Gigant",
            "Tomoya Kainuma",
            "Wojciech Kusa",
            "Yanis Labrak",
            "Yashasvi Bajaj",
            "Y. Venkatraman",
            "Yifan Xu",
            "Ying Xu",
            "Yu Xu",
            "Z. Tan",
            "Zhongli Xie",
            "Zifan Ye",
            "M. Bras",
            "Younes Belkada",
            "Thomas Wolf"
        ],
        "citations": 1000,
        "references": 172,
        "year": 2022
    },
    {
        "title": "Instruction Tuning for Large Language Models: A Survey",
        "abstract": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\\footnote{In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",
        "authors": [
            "Shengyu Zhang",
            "Linfeng Dong",
            "Xiaoya Li",
            "Sen Zhang",
            "Xiaofei Sun",
            "Shuhe Wang",
            "Jiwei Li",
            "Runyi Hu",
            "Tianwei Zhang",
            "Fei Wu",
            "Guoyin Wang"
        ],
        "citations": 394,
        "references": 173,
        "year": 2023
    },
    {
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
        "authors": [
            "Shunyu Yao",
            "Jeffrey Zhao",
            "Dian Yu",
            "Nan Du",
            "Izhak Shafran",
            "Karthik Narasimhan",
            "Yuan Cao"
        ],
        "citations": 1000,
        "references": 66,
        "year": 2022
    },
    {
        "title": "MedAlpaca - An Open-Source Collection of Medical Conversational AI Models and Training Data",
        "abstract": "As large language models (LLMs) like OpenAI's GPT series continue to make strides, we witness the emergence of artificial intelligence applications in an ever-expanding range of fields. In medicine, these LLMs hold considerable promise for improving medical workflows, diagnostics, patient care, and education. Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy. In our work, we present an innovative dataset consisting of over 160,000 entries, specifically crafted to fine-tune LLMs for effective medical applications. We investigate the impact of fine-tuning these datasets on publicly accessible pre-trained LLMs, and subsequently, we juxtapose the performance of pre-trained-only models against the fine-tuned models concerning the examinations that future medical doctors must pass to achieve certification.",
        "authors": [
            "T. Han",
            "Lisa C. Adams",
            "Jens-Michalis Papaioannou",
            "Paul Grundmann",
            "Tom Oberhauser",
            "Alexander Löser",
            "D. Truhn",
            "K. Bressem"
        ],
        "citations": 226,
        "references": 17,
        "year": 2023
    },
    {
        "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendations, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https://github.com/SAI990323/TALLRec.",
        "authors": [
            "Keqin Bao",
            "Jizhi Zhang",
            "Yang Zhang",
            "Wenjie Wang",
            "Fuli Feng",
            "Xiangnan He"
        ],
        "citations": 233,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "abstract": "Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. Code and datasets are available at https://github.com/zjunlp/EasyEdit.",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Bo Tian",
            "Siyuan Cheng",
            "Zhoubo Li",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "citations": 224,
        "references": 91,
        "year": 2023
    },
    {
        "title": "Bias and Fairness in Large Language Models: A Survey",
        "abstract": "Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
        "authors": [
            "Isabel O. Gallegos",
            "Ryan A. Rossi",
            "Joe Barrow",
            "Md. Mehrab Tanjim",
            "Sungchul Kim",
            "Franck Dernoncourt",
            "Tong Yu",
            "Ruiyi Zhang",
            "Nesreen Ahmed"
        ],
        "citations": 314,
        "references": 256,
        "year": 2023
    },
    {
        "title": "The future landscape of large language models in medicine",
        "abstract": null,
        "authors": [
            "J. Clusmann",
            "F. Kolbinger",
            "H. Muti",
            "Zunamys I. Carrero",
            "Jan-Niklas Eckardt",
            "Narmin Ghaffari Laleh",
            "C. M. L. Löffler",
            "Sophie-Caroline Schwarzkopf",
            "Michaela Unger",
            "G. P. Veldhuizen",
            "Sophia J Wagner",
            "Jakob Nikolas Kather"
        ],
        "citations": 339,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
        "abstract": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with these insights, zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators. The code and processed datasets are available at https://github.com/RUCAIBox/LLMRank.",
        "authors": [
            "Yupeng Hou",
            "Junjie Zhang",
            "Zihan Lin",
            "Hongyu Lu",
            "Ruobing Xie",
            "Julian McAuley",
            "Wayne Xin Zhao"
        ],
        "citations": 232,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
        "abstract": "With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.",
        "authors": [
            "Haoran Li",
            "Dadi Guo",
            "Wei Fan",
            "Mingshi Xu",
            "Jie Huang",
            "Yangqiu Song"
        ],
        "citations": 265,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Large Language Models Are Zero-Shot Time Series Forecasters",
        "abstract": "By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.",
        "authors": [
            "Nate Gruver",
            "Marc Finzi",
            "Shikai Qiu",
            "Andrew Gordon Wilson"
        ],
        "citations": 219,
        "references": 91,
        "year": 2023
    },
    {
        "title": "MEGA: Multilingual Evaluation of Generative AI",
        "abstract": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",
        "authors": [
            "Kabir Ahuja",
            "Rishav Hada",
            "Millicent Ochieng",
            "Prachi Jain",
            "Harshita Diddee",
            "Krithika Ramesh",
            "Samuel C. Maina",
            "T. Ganu",
            "Sameer Segal",
            "Maxamed Axmed",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "citations": 217,
        "references": 100,
        "year": 2023
    },
    {
        "title": "ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health",
        "abstract": "Large Language Models (LLMs) have recently gathered attention with the release of ChatGPT, a user-centered chatbot released by OpenAI. In this perspective article, we retrace the evolution of LLMs to understand the revolution brought by ChatGPT in the artificial intelligence (AI) field. The opportunities offered by LLMs in supporting scientific research are multiple and various models have already been tested in Natural Language Processing (NLP) tasks in this domain. The impact of ChatGPT has been huge for the general public and the research community, with many authors using the chatbot to write part of their articles and some papers even listing ChatGPT as an author. Alarming ethical and practical challenges emerge from the use of LLMs, particularly in the medical field for the potential impact on public health. Infodemic is a trending topic in public health and the ability of LLMs to rapidly produce vast amounts of text could leverage misinformation spread at an unprecedented scale, this could create an “AI-driven infodemic,” a novel public health threat. Policies to contrast this phenomenon need to be rapidly elaborated, the inability to accurately detect artificial-intelligence-produced text is an unresolved issue.",
        "authors": [
            "L. De Angelis",
            "F. Baglivo",
            "G. Arzilli",
            "G. P. Privitera",
            "P. Ferragina",
            "A. Tozzi",
            "C. Rizzo"
        ],
        "citations": 331,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
        "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
        "authors": [
            "Yecheng Jason Ma",
            "William Liang",
            "Guanzhi Wang",
            "De-An Huang",
            "O. Bastani",
            "Dinesh Jayaraman",
            "Yuke Zhu",
            "Linxi Fan",
            "A. Anandkumar"
        ],
        "citations": 201,
        "references": 73,
        "year": 2023
    },
    {
        "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
        "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \\emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \\textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\\ie \\emph{reasoning}). Specially, we propose an \\emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Zican Dong",
            "Keming Ye",
            "Wayne Xin Zhao",
            "Ji-rong Wen"
        ],
        "citations": 203,
        "references": 58,
        "year": 2023
    },
    {
        "title": "CMMLU: Measuring massive multitask language understanding in Chinese",
        "abstract": "As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models within the Chinese context.",
        "authors": [
            "Haonan Li",
            "Yixuan Zhang",
            "Fajri Koto",
            "Yifei Yang",
            "Hai Zhao",
            "Yeyun Gong",
            "Nan Duan",
            "Tim Baldwin"
        ],
        "citations": 179,
        "references": 50,
        "year": 2023
    },
    {
        "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget",
        "abstract": "Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.",
        "authors": [
            "Ilia Shumailov",
            "Zakhar Shumaylov",
            "Yiren Zhao",
            "Y. Gal",
            "Nicolas Papernot",
            "Ross Anderson"
        ],
        "citations": 236,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
        "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
        "authors": [
            "Lei Wang",
            "Wanyu Xu",
            "Yihuai Lan",
            "Zhiqiang Hu",
            "Yunshi Lan",
            "R. Lee",
            "Ee-Peng Lim"
        ],
        "citations": 236,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Explainability for Large Language Models: A Survey",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.",
        "authors": [
            "Haiyan Zhao",
            "Hanjie Chen",
            "F. Yang",
            "Ninghao Liu",
            "Huiqi Deng",
            "Hengyi Cai",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Mengnan Du"
        ],
        "citations": 272,
        "references": 218,
        "year": 2023
    },
    {
        "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
        "abstract": "The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a\"behavorial\"study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and event causality (86% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.",
        "authors": [
            "Emre Kıcıman",
            "R. Ness",
            "Amit Sharma",
            "Chenhao Tan"
        ],
        "citations": 208,
        "references": 127,
        "year": 2023
    },
    {
        "title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
        "abstract": "Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at: https://github.com/CurryTang/Graph-LLM .",
        "authors": [
            "Zhikai Chen",
            "Haitao Mao",
            "Hang Li",
            "Wei Jin",
            "Haifang Wen",
            "Xiaochi Wei",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Wenqi Fan",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "citations": 203,
        "references": 110,
        "year": 2023
    },
    {
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
        "abstract": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as\"jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. Our code is available at https://github.com/Princeton-SysML/Jailbreak_LLM.",
        "authors": [
            "Yangsibo Huang",
            "Samyak Gupta",
            "Mengzhou Xia",
            "Kai Li",
            "Danqi Chen"
        ],
        "citations": 210,
        "references": 51,
        "year": 2023
    },
    {
        "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
        "abstract": "The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.",
        "authors": [
            "Xiaogeng Liu",
            "Nan Xu",
            "Muhao Chen",
            "Chaowei Xiao"
        ],
        "citations": 176,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Challenges and Applications of Large Language Models",
        "abstract": "Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.",
        "authors": [
            "Jean Kaddour",
            "J. Harris",
            "Maximilian Mozes",
            "Herbie Bradley",
            "Roberta Raileanu",
            "R. McHardy"
        ],
        "citations": 228,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
        "abstract": "Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies.",
        "authors": [
            "Yunfan Gao",
            "Tao Sheng",
            "Youlin Xiang",
            "Yun Xiong",
            "Haofen Wang",
            "Jiawei Zhang"
        ],
        "citations": 207,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
        "abstract": "The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and the concurrent TinyLlama models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building competitive small-scale LLMs",
        "authors": [
            "Mengzhou Xia",
            "Tianyu Gao",
            "Zhiyuan Zeng",
            "Danqi Chen"
        ],
        "citations": 190,
        "references": 85,
        "year": 2023
    },
    {
        "title": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"",
        "abstract": "We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form\"A is B\", it will not automatically generalize to the reverse direction\"B is A\". This is the Reversal Curse. For instance, if a model is trained on\"Valentina Tereshkova was the first woman to travel to space\", it will not automatically be able to answer the question,\"Who was the first woman to travel to space?\". Moreover, the likelihood of the correct answer (\"Valentina Tershkova\") will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if\"A is B\"occurs,\"B is A\"is more likely to occur. It is worth noting, however, that if\"A is B\"appears in-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as\"Uriah Hawthorne is the composer of Abyssal Melodies\"and showing that they fail to correctly answer\"Who composed Abyssal Melodies?\". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as\"Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]\"and the reverse\"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly answers questions like the former 79% of the time, compared to 33% for the latter. Code available at: https://github.com/lukasberglund/reversal_curse.",
        "authors": [
            "Lukas Berglund",
            "Meg Tong",
            "Max Kaufmann",
            "Mikita Balesni",
            "Asa Cooper Stickland",
            "Tomasz Korbak",
            "Owain Evans"
        ],
        "citations": 187,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory",
        "abstract": "The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular\"ObtainDiamond\"task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the\"ObtainDiamond\"task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5% in success rate on the\"ObtainDiamond\"task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/OpenGVLab/GITM.",
        "authors": [
            "Xizhou Zhu",
            "Yuntao Chen",
            "Hao Tian",
            "Chenxin Tao",
            "Weijie Su",
            "Chenyu Yang",
            "Gao Huang",
            "Bin Li",
            "Lewei Lu",
            "Xiaogang Wang",
            "Y. Qiao",
            "Zhaoxiang Zhang",
            "Jifeng Dai"
        ],
        "citations": 180,
        "references": 28,
        "year": 2023
    },
    {
        "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
        "abstract": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo, a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.",
        "authors": [
            "Hao Wang",
            "Chi-Liang Liu",
            "Nuwa Xi",
            "Zewen Qiang",
            "Sendong Zhao",
            "Bing Qin",
            "Ting Liu"
        ],
        "citations": 160,
        "references": 13,
        "year": 2023
    },
    {
        "title": "Can Large Language Models Transform Computational Social Science?",
        "abstract": "Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.",
        "authors": [
            "Caleb Ziems",
            "William B. Held",
            "Omar Shaikh",
            "Jiaao Chen",
            "Zhehao Zhang",
            "Diyi Yang"
        ],
        "citations": 198,
        "references": 309,
        "year": 2023
    },
    {
        "title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
        "abstract": "Prompt engineering is a relatively new field of research that refers to the practice of designing, refining, and implementing prompts or instructions that guide the output of large language models (LLMs) to help in various tasks. With the emergence of LLMs, the most popular one being ChatGPT that has attracted the attention of over a 100 million users in only 2 months, artificial intelligence (AI), especially generative AI, has become accessible for the masses. This is an unprecedented paradigm shift not only because of the use of AI becoming more widespread but also due to the possible implications of LLMs in health care. As more patients and medical professionals use AI-based tools, LLMs being the most popular representatives of that group, it seems inevitable to address the challenge to improve this skill. This paper summarizes the current state of research about prompt engineering and, at the same time, aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs.",
        "authors": [
            "B. Meskó"
        ],
        "citations": 217,
        "references": 7,
        "year": 2023
    },
    {
        "title": "Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions",
        "abstract": "The integration of large language models (LLMs), such as those in the Generative Pre-trained Transformers (GPT) series, into medical education has the potential to transform learning experiences for students and elevate their knowledge, skills, and competence. Drawing on a wealth of professional and academic experience, we propose that LLMs hold promise for revolutionizing medical curriculum development, teaching methodologies, personalized study plans and learning materials, student assessments, and more. However, we also critically examine the challenges that such integration might pose by addressing issues of algorithmic bias, overreliance, plagiarism, misinformation, inequity, privacy, and copyright concerns in medical education. As we navigate the shift from an information-driven educational paradigm to an artificial intelligence (AI)–driven educational paradigm, we argue that it is paramount to understand both the potential and the pitfalls of LLMs in medical education. This paper thus offers our perspective on the opportunities and challenges of using LLMs in this context. We believe that the insights gleaned from this analysis will serve as a foundation for future recommendations and best practices in the field, fostering the responsible and effective use of AI technologies in medical education.",
        "authors": [
            "Alaa A. Abd-alrazaq",
            "Rawan AlSaad",
            "Dari Alhuwail",
            "Arfan Ahmed",
            "P. Healy",
            "Syed Latifi",
            "S. Aziz",
            "R. Damseh",
            "Sadam Alabed Alrazak",
            "Javaid Sheikh"
        ],
        "citations": 196,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Multimodal Foundation Models: From Specialists to General-Purpose Assistants",
        "abstract": "This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.",
        "authors": [
            "Chunyuan Li",
            "Zhe Gan",
            "Zhengyuan Yang",
            "Jianwei Yang",
            "Linjie Li",
            "Lijuan Wang",
            "Jianfeng Gao"
        ],
        "citations": 181,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
        "abstract": "Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at \\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.",
        "authors": [
            "Wenxuan Zhang",
            "Yue Deng",
            "Bing-Quan Liu",
            "Sinno Jialin Pan",
            "Lidong Bing"
        ],
        "citations": 200,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Academic integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond",
        "abstract": "This paper explores the academic integrity considerations of students’ use of Artificial Intelligence (AI) tools using Large Language Models (LLMs) such as ChatGPT in formal assessments. We examine the evolution of these tools, and highlight the potential ways that LLMs can support in the education of students in digital writing and beyond, including the teaching of writing and composition, the possibilities of co-creation between humans and AI, supporting EFL learners, and improving Automated Writing Evaluations (AWE). We describe and demonstrate the potential that these tools have in creating original, coherent text that can avoid detection by existing technological methods of detection and trained academic staff alike, demonstrating a major academic integrity concern related to the use of these tools by students. Analysing the various issues related to academic integrity that LLMs raise for both Higher Education Institutions (HEIs) and students, we conclude that it is not the student use of any AI tools that defines whether plagiarism or a breach of academic integrity has occurred, but whether any use is made clear by the student. Deciding whether any particular use of LLMs by students can be defined as academic misconduct is determined by the academic integrity policies of any given HEI, which must be updated to consider how these tools will be used in future educational environments.",
        "authors": [
            "Mike Perkins"
        ],
        "citations": 197,
        "references": 97,
        "year": 2023
    },
    {
        "title": "The False Promise of Imitating Proprietary LLMs",
        "abstract": "An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.",
        "authors": [
            "Arnav Gudibande",
            "Eric Wallace",
            "Charles Burton Snell",
            "Xinyang Geng",
            "Hao Liu",
            "P. Abbeel",
            "S. Levine",
            "Dawn Song"
        ],
        "citations": 186,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Large Language Models for Information Retrieval: A Survey",
        "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.",
        "authors": [
            "Yutao Zhu",
            "Huaying Yuan",
            "Shuting Wang",
            "Jiongnan Liu",
            "Wenhan Liu",
            "Chenlong Deng",
            "Zhicheng Dou",
            "Ji-rong Wen"
        ],
        "citations": 204,
        "references": 339,
        "year": 2023
    },
    {
        "title": "Recommender Systems in the Era of Large Language Models (LLMs)",
        "abstract": "With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization to various recommendation scenarios, and reasoning on their predictions, etc. Meanwhile, the development of Large Language Models (LLMs), such as ChatGPT and GPT-4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems. Therefore, in this survey, we comprehensively review LLM-empowered recommender systems from various perspectives including pre-training, fine-tuning, and prompting paradigms. More specifically, we first introduce the representative methods to learn user and item representations, leveraging LLMs as feature encoders. Then, we systematically review the emerging advanced techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss the promising future directions in this emerging field.",
        "authors": [
            "Wenqi Fan",
            "Zihuai Zhao",
            "Jiatong Li",
            "Yunqing Liu",
            "Xiaowei Mei",
            "Yiqi Wang",
            "Jiliang Tang",
            "Qing Li"
        ],
        "citations": 211,
        "references": 169,
        "year": 2023
    },
    {
        "title": "Dissociating language and thought in large language models: a cognitive perspective",
        "abstract": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --\"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.",
        "authors": [
            "Kyle Mahowald",
            "Anna A. Ivanova",
            "I. Blank",
            "N. Kanwisher",
            "J. Tenenbaum",
            "Evelina Fedorenko"
        ],
        "citations": 197,
        "references": 421,
        "year": 2023
    },
    {
        "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
        "abstract": "This paper aims to efficiently enable Large Language Models (LLMs) to use multimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have shown great potential for tool usage through sophisticated prompt engineering. Nevertheless, these models typically rely on prohibitive computational costs and publicly inaccessible data. To address these challenges, we propose the GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools. It generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts. By using the Low-Rank Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs to solve a range of visual problems, including visual comprehension and image generation. Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways. Extensive experiments demonstrate the effectiveness of our method on various language models, which not only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot capacity for unseen tools. The code and demo are available at https://github.com/StevenGrove/GPT4Tools.",
        "authors": [
            "Rui Yang",
            "Lin Song",
            "Yanwei Li",
            "Sijie Zhao",
            "Yixiao Ge",
            "Xiu Li",
            "Ying Shan"
        ],
        "citations": 165,
        "references": 72,
        "year": 2023
    },
    {
        "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
        "abstract": "Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. Our code and data will be released at https://github.com/RobustNLP/CipherChat.",
        "authors": [
            "Youliang Yuan",
            "Wenxiang Jiao",
            "Wenxuan Wang",
            "Jen-Tse Huang",
            "Pinjia He",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "citations": 158,
        "references": 69,
        "year": 2023
    },
    {
        "title": "DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model",
        "abstract": "Multimodallarge language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion. These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.",
        "authors": [
            "Zhenhua Xu",
            "Yujia Zhang",
            "Enze Xie",
            "Zhen Zhao",
            "Yong Guo",
            "Kwan-Yee. K. Wong",
            "Zhenguo Li",
            "Hengshuang Zhao"
        ],
        "citations": 161,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
        "authors": [
            "Jiawei Chen",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "citations": 176,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Code as Policies: Language Model Programs for Embodied Control",
        "abstract": "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
        "authors": [
            "Jacky Liang",
            "Wenlong Huang",
            "F. Xia",
            "Peng Xu",
            "Karol Hausman",
            "Brian Ichter",
            "Peter R. Florence",
            "Andy Zeng"
        ],
        "citations": 712,
        "references": 62,
        "year": 2022
    },
    {
        "title": "Evaluating large language models on medical evidence summarization",
        "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable successes in zero- and few-shot performance on various downstream tasks, paving the way for applications in high-stakes domains. In this study, we systematically examine the capabilities and limitations of LLMs, specifically GPT-3.5 and ChatGPT, in performing zero-shot medical evidence summarization across six clinical domains. We conduct both automatic and human evaluations, covering several dimensions of summary quality. Our study demonstrates that automatic metrics often do not strongly correlate with the quality of summaries. Furthermore, informed by our human evaluations, we define a terminology of error types for medical evidence summarization. Our findings reveal that LLMs could be susceptible to generating factually inconsistent summaries and making overly convincing or uncertain statements, leading to potential harm due to misinformation. Moreover, we find that models struggle to identify the salient information and are more error-prone when summarizing over longer textual contexts.",
        "authors": [
            "Liyan Tang",
            "Z. Sun",
            "B. Idnay",
            "J. Nestor",
            "A. Soroush",
            "P. A. Elias",
            "Z. Xu",
            "Y. Ding",
            "Greg Durrett",
            "Justin F. Rousseau",
            "C. Weng",
            "Y. Peng"
        ],
        "citations": 182,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective",
        "abstract": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.",
        "authors": [
            "Guhao Feng",
            "Yuntian Gu",
            "Bohang Zhang",
            "Haotian Ye",
            "Di He",
            "Liwei Wang"
        ],
        "citations": 151,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Can We Edit Factual Knowledge by In-Context Learning?",
        "abstract": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE.",
        "authors": [
            "Ce Zheng",
            "Lei Li",
            "Qingxiu Dong",
            "Yuxuan Fan",
            "Zhiyong Wu",
            "Jingjing Xu",
            "Baobao Chang"
        ],
        "citations": 145,
        "references": 45,
        "year": 2023
    },
    {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
        "authors": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "J. Nie",
            "Ji-rong Wen"
        ],
        "citations": 177,
        "references": 36,
        "year": 2023
    },
    {
        "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
        "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
        "authors": [
            "Shima Imani",
            "Liang Du",
            "H. Shrivastava"
        ],
        "citations": 161,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Practical and ethical challenges of large language models in education: A systematic scoping review",
        "abstract": "Educational technology innovations leveraging large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (eg, question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs‐based innovations in authentic educational contexts. To address this, we conducted a systematic scoping review of 118 peer‐reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The findings revealed 53 use cases for LLMs in automating education tasks, categorised into nine main categories: profiling/labelling, detection, grading, teaching support, prediction, knowledge representation, feedback, content generation, and recommendation. Additionally, we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency and insufficient privacy and beneficence considerations. The findings were summarised into three recommendations for future studies, including updating existing innovations with state‐of‐the‐art models (eg, GPT‐3/4), embracing the initiative of open‐sourcing models/systems, and adopting a human‐centred approach throughout the developmental process. As the intersection of AI and education is continuously evolving, the findings of this study can serve as an essential reference point for researchers, allowing them to leverage the strengths, learn from the limitations, and uncover potential research opportunities enabled by ChatGPT and other generative AI models.\nWhat is currently known about this topic\n\nGenerating and analysing text‐based content are time‐consuming and laborious tasks.\nLarge language models are capable of efficiently analysing an unprecedented amount of textual content and completing complex natural language processing and generation tasks.\nLarge language models have been increasingly used to develop educational technologies that aim to automate the generation and analysis of textual content, such as automated question generation and essay scoring.\nWhat this paper adds\n\nA comprehensive list of different educational tasks that could potentially benefit from LLMs‐based innovations through automation.\nA structured assessment of the practicality and ethicality of existing LLMs‐based innovations from seven important aspects using established frameworks.\nThree recommendations that could potentially support future studies to develop LLMs‐based innovations that are practical and ethical to implement in authentic educational contexts.\nImplications for practice and/or policy\n\nUpdating existing innovations with state‐of‐the‐art models may further reduce the amount of manual effort required for adapting existing models to different educational tasks.\nThe reporting standards of empirical research that aims to develop educational technologies using large language models need to be improved.\nAdopting a human‐centred approach throughout the developmental process could contribute to resolving the practical and ethical challenges of large language models in education.\n\n",
        "authors": [
            "Lixiang Yan",
            "Lele Sha",
            "Linxuan Zhao",
            "Yuheng Li",
            "Roberto Martínez Maldonado",
            "Guanliang Chen",
            "Xinyu Li",
            "Yueqiao Jin",
            "D. Gašević"
        ],
        "citations": 165,
        "references": 92,
        "year": 2023
    },
    {
        "title": "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach",
        "abstract": "\n In the past decades, recommender systems have attracted much attention in both research and industry communities. Existing recommendation models mainly learn the underlying user preference from historical behavior data (typically in the forms of item IDs), and then estimate the user-item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs), we develop a different recommendation paradigm, considering recommendation as\n instruction following\n by LLMs. The key idea is that the needs of a user can be expressed in natural language descriptions (called\n instructions\n ), so that LLMs can understand and further execute the instruction for fulfilling the recommendation. For this purpose, we instruction tune the 3B Flan-T5-XL, to better adapt LLMs to recommender systems. We first design a general instruction format for describing the preference, intention, and task form of a user in natural language. Then we manually design 39 instruction templates and automatically generate large amounts of user-personalized instruction data with varying types of preferences and intentions. To demonstrate the effectiveness of our approach, we instantiate the instructions into several widely studied recommendation (or search) tasks, and conduct extensive experiments with real-world datasets. Experiment results show that our approach can outperform several competitive baselines, including the powerful GPT-3.5, on these evaluation tasks. Our approach sheds light on developing user-friendly recommender systems, in which users can freely communicate with the system and obtain accurate recommendations via natural language instructions.\n",
        "authors": [
            "Junjie Zhang",
            "Ruobing Xie",
            "Yupeng Hou",
            "Wayne Xin Zhao",
            "Leyu Lin",
            "Ji-rong Wen"
        ],
        "citations": 158,
        "references": 69,
        "year": 2023
    },
    {
        "title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators",
        "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt LLMs to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with LLMs. Our experiment results on three tasks, including user input and keyword relevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing AnnoLLM. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset's high quality.",
        "authors": [
            "Xingwei He",
            "Zheng-Wen Lin",
            "Yeyun Gong",
            "Alex Jin",
            "Hang Zhang",
            "Chen Lin",
            "Jian Jiao",
            "S. Yiu",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "citations": 159,
        "references": 50,
        "year": 2023
    },
    {
        "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets",
        "abstract": "The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT's performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "M Saiful Bari",
            "Mizanur Rahman",
            "Md Amran Hossen Bhuiyan",
            "Shafiq R. Joty",
            "J. Huang"
        ],
        "citations": 158,
        "references": 232,
        "year": 2023
    },
    {
        "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision",
        "abstract": "Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.",
        "authors": [
            "Junjie Wang",
            "Yuchao Huang",
            "Chunyang Chen",
            "Zhe Liu",
            "Song Wang",
            "Qing Wang"
        ],
        "citations": 164,
        "references": 179,
        "year": 2023
    },
    {
        "title": "Auditing large language models: a three-layered approach",
        "abstract": null,
        "authors": [
            "Jakob Mokander",
            "Jonas Schuett",
            "Hannah Rose Kirk",
            "Luciano Floridi"
        ],
        "citations": 170,
        "references": 295,
        "year": 2023
    },
    {
        "title": "Using large language models in psychology",
        "abstract": null,
        "authors": [
            "Dorottya Demszky",
            "Diyi Yang",
            "David S. Yeager",
            "Christopher J. Bryan",
            "Margarett Clapper",
            "Susannah Chandhok",
            "J. Eichstaedt",
            "Cameron A. Hecht",
            "Jeremy P. Jamieson",
            "Meghann Johnson",
            "Michaela Jones",
            "Danielle Krettek-Cobb",
            "Leslie Lai",
            "Nirel JonesMitchell",
            "Desmond C. Ong",
            "C. Dweck",
            "James J. Gross",
            "James W. Pennebaker"
        ],
        "citations": 158,
        "references": 114,
        "year": 2023
    },
    {
        "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization",
        "abstract": "Text summarization has been a crucial problem in natural language processing (NLP) for several decades. It aims to condense lengthy documents into shorter versions while retaining the most critical information. Various methods have been proposed for text summarization, including extractive and abstractive summarization. The emergence of large language models (LLMs) like GPT3 and ChatGPT has recently created significant interest in using these models for text summarization tasks. Recent studies \\cite{goyal2022news, zhang2023benchmarking} have shown that LLMs-generated news summaries are already on par with humans. However, the performance of LLMs for more practical applications like aspect or query-based summaries is underexplored. To fill this gap, we conducted an evaluation of ChatGPT's performance on four widely used benchmark datasets, encompassing diverse summaries from Reddit posts, news articles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's performance is comparable to traditional fine-tuning methods in terms of Rouge scores. Moreover, we highlight some unique differences between ChatGPT-generated summaries and human references, providing valuable insights into the superpower of ChatGPT for diverse text summarization tasks. Our findings call for new directions in this area, and we plan to conduct further research to systematically examine the characteristics of ChatGPT-generated summaries through extensive human evaluation.",
        "authors": [
            "Xianjun Yang",
            "Yan Li",
            "Xinlu Zhang",
            "Haifeng Chen",
            "Wei Cheng"
        ],
        "citations": 162,
        "references": 18,
        "year": 2023
    },
    {
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
        "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",
        "authors": [
            "Wenlong Huang",
            "F. Xia",
            "Ted Xiao",
            "Harris Chan",
            "Jacky Liang",
            "Peter R. Florence",
            "Andy Zeng",
            "Jonathan Tompson",
            "Igor Mordatch",
            "Yevgen Chebotar",
            "P. Sermanet",
            "Noah Brown",
            "Tomas Jackson",
            "Linda Luu",
            "S. Levine",
            "Karol Hausman",
            "Brian Ichter"
        ],
        "citations": 738,
        "references": 112,
        "year": 2022
    },
    {
        "title": "Creation and Adoption of Large Language Models in Medicine.",
        "abstract": "Importance\nThere is increased interest in and potential benefits from using large language models (LLMs) in medicine. However, by simply wondering how the LLMs and the applications powered by them will reshape medicine instead of getting actively involved, the agency in shaping how these tools can be used in medicine is lost.\n\n\nObservations\nApplications powered by LLMs are increasingly used to perform medical tasks without the underlying language model being trained on medical records and without verifying their purported benefit in performing those tasks.\n\n\nConclusions and Relevance\nThe creation and use of LLMs in medicine need to be actively shaped by provisioning relevant training data, specifying the desired benefits, and evaluating the benefits via testing in real-world deployments.",
        "authors": [
            "N. Shah",
            "David N. Entwistle",
            "M. Pfeffer"
        ],
        "citations": 164,
        "references": 5,
        "year": 2023
    },
    {
        "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
        "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT",
        "authors": [
            "Rongjie Huang",
            "Mingze Li",
            "Dongchao Yang",
            "Jiatong Shi",
            "Xuankai Chang",
            "Zhenhui Ye",
            "Yuning Wu",
            "Zhiqing Hong",
            "Jia-Bin Huang",
            "Jinglin Liu",
            "Yixiang Ren",
            "Zhou Zhao",
            "Shinji Watanabe"
        ],
        "citations": 156,
        "references": 48,
        "year": 2023
    },
    {
        "title": "FinGPT: Open-Source Financial Large Language Models",
        "abstract": "Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data. In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT} and \\url{https://github.com/AI4Finance-Foundation/FinNLP}",
        "authors": [
            "Hongyang Yang",
            "Xiao-Yang Liu",
            "Chris Wang"
        ],
        "citations": 154,
        "references": 19,
        "year": 2023
    },
    {
        "title": "Can Language Models Solve Graph Problems in Natural Language?",
        "abstract": "Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The NLGraph benchmark and evaluation code are available at https://github.com/Arthur-Heng/NLGraph.",
        "authors": [
            "Heng Wang",
            "Shangbin Feng",
            "Tianxing He",
            "Zhaoxuan Tan",
            "Xiaochuang Han",
            "Yulia Tsvetkov"
        ],
        "citations": 132,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
        "abstract": "Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.",
        "authors": [
            "Qingyan Guo",
            "Rui Wang",
            "Junliang Guo",
            "Bei Li",
            "Kaitao Song",
            "Xu Tan",
            "Guoqing Liu",
            "Jiang Bian",
            "Yujiu Yang",
            "Tsinghua University",
            "Microsoft Research"
        ],
        "citations": 143,
        "references": 83,
        "year": 2023
    },
    {
        "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
        "abstract": "Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent\"selection bias\", namely, they prefer to select specific option IDs as answers (like\"Option A\"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.",
        "authors": [
            "Chujie Zheng",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Minlie Huang"
        ],
        "citations": 146,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "abstract": "We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player's negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model's negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game's rules or cannot incorporate AI feedback for further improvement. (2) Models' abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback, yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.",
        "authors": [
            "Yao Fu",
            "Hao-Chun Peng",
            "Tushar Khot",
            "Mirella Lapata"
        ],
        "citations": 141,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health",
        "abstract": "ChatGPT has drawn considerable attention from both the general public and domain experts with its remarkable text generation capabilities. This has subsequently led to the emergence of diverse applications in the field of biomedicine and health. In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically we explore the areas of biomedical information retrieval, question answering, medical text summarization, information extraction, and medical education, and investigate whether LLMs possess the transformative power to revolutionize these tasks or whether the distinct complexities of biomedical domain presents unique challenges. Following an extensive literature survey, we find that significant advances have been made in the field of text generation tasks, surpassing the previous state-of-the-art methods. For other applications, the advances have been modest. Overall, LLMs have not yet revolutionized biomedicine, but recent rapid progress indicates that such methods hold great potential to provide valuable means for accelerating discovery and improving health. We also find that the use of LLMs, like ChatGPT, in the fields of biomedicine and health entails various risks and challenges, including fabricated information in its generated responses, as well as legal and privacy concerns associated with sensitive patient data. We believe this survey can provide a comprehensive and timely overview to biomedical researchers and healthcare practitioners on the opportunities and challenges associated with using ChatGPT and other LLMs for transforming biomedicine and health.",
        "authors": [
            "Shubo Tian",
            "Qiao Jin",
            "Lana Yeganova",
            "Po-Ting Lai",
            "Qingqing Zhu",
            "Xiuying Chen",
            "Yifan Yang",
            "Qingyu Chen",
            "Won Kim",
            "Donald C. Comeau",
            "R. Islamaj",
            "Aadit Kapoor",
            "Xin Gao",
            "Zhiyong Lu"
        ],
        "citations": 155,
        "references": 143,
        "year": 2023
    },
    {
        "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
        "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly. In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a\"free lunch\"hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored. We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.",
        "authors": [
            "Erik Nijkamp",
            "Hiroaki Hayashi",
            "Caiming Xiong",
            "S. Savarese",
            "Yingbo Zhou"
        ],
        "citations": 143,
        "references": 31,
        "year": 2023
    },
    {
        "title": "The Robots Are Here: Navigating the Generative AI Revolution in Computing Education",
        "abstract": "Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.",
        "authors": [
            "J. Prather",
            "Paul Denny",
            "Juho Leinonen",
            "Brett A. Becker",
            "Ibrahim Albluwi",
            "Michelle Craig",
            "H. Keuning",
            "Natalie Kiesler",
            "Tobias Kohn",
            "Andrew Luxton-Reilly",
            "Stephen Macneil",
            "Andrew Petersen",
            "Raymond Pettit",
            "Brent N. Reeves",
            "Jaromír Šavelka"
        ],
        "citations": 138,
        "references": 178,
        "year": 2023
    },
    {
        "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models",
        "abstract": "Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5 different organizations (LLaMa-2, Falcon, InternLM, BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack. Besides, the single-turn English-only attack successfully transfers to multi-turn dialogue and other languages. This study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source LLMs against malicious attackers.",
        "authors": [
            "Xianjun Yang",
            "Xiao Wang",
            "Qi Zhang",
            "Linda R. Petzold",
            "William Yang Wang",
            "Xun Zhao",
            "Dahua Lin"
        ],
        "citations": 138,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf",
        "abstract": "Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf'', demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains.",
        "authors": [
            "Yuzhuang Xu",
            "Shuo Wang",
            "Peng Li",
            "Fuwen Luo",
            "Xiaolong Wang",
            "Weidong Liu",
            "Yang Liu"
        ],
        "citations": 143,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Large Language Models as General Pattern Machines",
        "abstract": "We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.",
        "authors": [
            "Suvir Mirchandani",
            "F. Xia",
            "Peter R. Florence",
            "Brian Ichter",
            "Danny Driess",
            "Montse Gonzalez Arenas",
            "Kanishka Rao",
            "Dorsa Sadigh",
            "Andy Zeng"
        ],
        "citations": 152,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Does Synthetic Data Generation of LLMs Help Clinical Text Mining?",
        "abstract": "Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37% to 63.99% for the named entity recognition task and from 75.86% to 83.59% for the relation extraction task. Furthermore, generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinical text mining.",
        "authors": [
            "Ruixiang Tang",
            "Xiaotian Han",
            "Xiaoqian Jiang",
            "Xia Hu"
        ],
        "citations": 146,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Adapted large language models can outperform medical experts in clinical text summarization.",
        "abstract": null,
        "authors": [
            "Dave Van Veen",
            "Cara Van Uden",
            "Louis Blankemeier",
            "Jean-Benoit Delbrouck",
            "Asad Aali",
            "Christian Blüthgen",
            "A. Pareek",
            "Malgorzata Polacin",
            "William Collins",
            "Neera Ahuja",
            "C. Langlotz",
            "Jason Hom",
            "S. Gatidis",
            "John M. Pauly",
            "Akshay S. Chaudhari"
        ],
        "citations": 148,
        "references": 91,
        "year": 2023
    },
    {
        "title": "A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4",
        "abstract": "Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI GPT-3 model, and the popularity of LLMs is increasing exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GPT-3 family large language models.",
        "authors": [
            "Katikapalli Subramanyam Kalyan"
        ],
        "citations": 140,
        "references": 146,
        "year": 2023
    },
    {
        "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning",
        "abstract": "In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies.",
        "authors": [
            "Lean Wang",
            "Lei Li",
            "Damai Dai",
            "Deli Chen",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Xu Sun"
        ],
        "citations": 135,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Large language models propagate race-based medicine",
        "abstract": null,
        "authors": [
            "J. Omiye",
            "Jenna Lester",
            "Simon Spichak",
            "V. Rotemberg",
            "Roxana Daneshjou"
        ],
        "citations": 151,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Gender bias and stereotypes in Large Language Models",
        "abstract": "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs’ behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women’s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person’s gender; (b) these choices align with people’s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably.",
        "authors": [
            "Hadas Kotek",
            "Rikker Dockum",
            "David Q. Sun"
        ],
        "citations": 152,
        "references": 128,
        "year": 2023
    },
    {
        "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
        "abstract": "Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.",
        "authors": [
            "Wes Gurnee",
            "Neel Nanda",
            "Matthew Pauly",
            "Katherine Harvey",
            "Dmitrii Troitskii",
            "D. Bertsimas"
        ],
        "citations": 134,
        "references": 107,
        "year": 2023
    },
    {
        "title": "A Survey on Model Compression for Large Language Models",
        "abstract": "Abstract Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.",
        "authors": [
            "Xunyu Zhu",
            "Jian Li",
            "Yong Liu",
            "Can Ma",
            "Weiping Wang"
        ],
        "citations": 132,
        "references": 201,
        "year": 2023
    },
    {
        "title": "Low-Resource Languages Jailbreak GPT-4",
        "abstract": "AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs' safety vulnerabilities. Therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.",
        "authors": [
            "Zheng-Xin Yong",
            "Cristina Menghini",
            "Stephen H. Bach"
        ],
        "citations": 130,
        "references": 53,
        "year": 2023
    },
    {
        "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
        "abstract": "Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs.",
        "authors": [
            "Zeming Chen",
            "Alejandro Hern'andez Cano",
            "Angelika Romanou",
            "Antoine Bonnet",
            "Kyle Matoba",
            "Francesco Salvi",
            "Matteo Pagliardini",
            "Simin Fan",
            "Andreas Kopf",
            "Amirkeivan Mohtashami",
            "Alexandre Sallinen",
            "Alireza Sakhaeirad",
            "Vinitra Swamy",
            "Igor Krawczuk",
            "Deniz Bayazit",
            "Axel Marmet",
            "Syrielle Montariol",
            "Mary-Anne Hartley",
            "Martin Jaggi",
            "Antoine Bosselut"
        ],
        "citations": 126,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Language is All a Graph Needs",
        "abstract": "The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data like images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, languages, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is available at https://github.com/agiresearch/InstructGLM.",
        "authors": [
            "Ruosong Ye",
            "Caiqi Zhang",
            "Runhui Wang",
            "Shuyuan Xu",
            "Yongfeng Zhang"
        ],
        "citations": 126,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval",
        "abstract": "The effectiveness of multi-stage text retrieval has been solidly demonstrated since before the era of pre-trained language models. However, most existing studies utilize models that predate recent advances in large language models (LLMs). This study seeks to explore potential improvements that state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise reranker (RankLLaMA) for both passage retrieval and document retrieval using the MS MARCO datasets. Our findings demonstrate that the effectiveness of large language models indeed surpasses that of smaller models. Additionally, since LLMs can inherently handle longer contexts, they can represent entire documents holistically, obviating the need for traditional segmenting and pooling strategies. Furthermore, evaluations on BEIR demonstrate that our RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model checkpoints from this study are available on HuggingFace.",
        "authors": [
            "Xueguang Ma",
            "Liang Wang",
            "Nan Yang",
            "Furu Wei",
            "Jimmy Lin"
        ],
        "citations": 122,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Chip-Chat: Challenges and Opportunities in Conversational Hardware Design",
        "abstract": "Modern hardware design starts with specifications provided in natural language. These are then translated by hardware engineers into appropriate Hardware Description Languages (HDLs) such as Verilog before synthesizing circuit elements. Automating this translation could reduce sources of human error from the engineering process. But, it is only recently that artificial intelligence (AI) has demonstrated capabilities for machine-based end-to-end design translations. Commercially-available instruction-tuned Large Language Models (LLMs) such as OpenAI’s ChatGPT and Google’s Bard claim to be able to produce code in a variety of programming languages; but studies examining them for hardware are still lacking. In this work, we thus explore the challenges faced and opportunities presented when leveraging these recent advances in LLMs for hardware design. Given that these ‘conversational’ LLMs perform best when used interactively, we perform a case study where a hardware engineer co-architects a novel 8-bit accumulator-based microprocessor architecture with the LLM according to real-world hardware constraints. We then sent the processor to tapeout in a Skywater 130nm shuttle, meaning that this ‘Chip-Chat’ resulted in what we believe to be the world’s first wholly-AI-written HDL for tapeout.",
        "authors": [
            "Jason Blocklove",
            "S. Garg",
            "R. Karri",
            "H. Pearce"
        ],
        "citations": 125,
        "references": 33,
        "year": 2023
    },
    {
        "title": "ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",
        "abstract": "This paper presents prompt design techniques for software engineering, in the form of patterns, to solve common problems when using large language models (LLMs), such as ChatGPT to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and simulating a web application API before it is implemented. This paper provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classifies patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, refactoring, and system design.",
        "authors": [
            "Jules White",
            "Sam Hays",
            "Quchen Fu",
            "Jesse Spencer-Smith",
            "Douglas C. Schmidt"
        ],
        "citations": 125,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Secrets of RLHF in Large Language Models Part I: PPO",
        "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.",
        "authors": [
            "Rui Zheng",
            "Shihan Dou",
            "Songyang Gao",
            "Wei Shen",
            "Wei-Yuan Shen",
            "Bing Wang",
            "Yan Liu",
            "Senjie Jin",
            "Qin Liu",
            "Limao Xiong",
            "Luyao Chen",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Nuo Xu",
            "Wen-De Lai",
            "Minghao Zhu",
            "Rongxiang Weng",
            "Wen-Chun Cheng",
            "Cheng Chang",
            "Zhangyue Yin",
            "Yuan Hua",
            "Haoran Huang",
            "Tianxiang Sun",
            "Hang Yan",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "citations": 127,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Detecting Language Model Attacks with Perplexity",
        "abstract": "A novel hack involving Large Language Models (LLMs) has emerged, leveraging adversarial suffixes to trick models into generating perilous responses. This method has garnered considerable attention from reputable media outlets such as the New York Times and Wired, thereby influencing public perception regarding the security and safety of LLMs. In this study, we advocate the utilization of perplexity as one of the means to recognize such potential attacks. The underlying concept behind these hacks revolves around appending an unusually constructed string of text to a harmful query that would otherwise be blocked. This maneuver confuses the protective mechanisms and tricks the model into generating a forbidden response. Such scenarios could result in providing detailed instructions to a malicious user for constructing explosives or orchestrating a bank heist. Our investigation demonstrates the feasibility of employing perplexity, a prevalent natural language processing metric, to detect these adversarial tactics before generating a forbidden response. By evaluating the perplexity of queries with and without such adversarial suffixes using an open-source LLM, we discovered that nearly 90 percent were above a perplexity of 1000. This contrast underscores the efficacy of perplexity for detecting this type of exploit.",
        "authors": [
            "Gabriel Alon",
            "Michael Kamfonas"
        ],
        "citations": 122,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Studying Large Language Model Generalization with Influence Functions",
        "abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.",
        "authors": [
            "R. Grosse",
            "Juhan Bae",
            "Cem Anil",
            "Nelson Elhage",
            "Alex Tamkin",
            "Amirhossein Tajdini",
            "Benoit Steiner",
            "Dustin Li",
            "Esin Durmus",
            "Ethan Perez",
            "Evan Hubinger",
            "Kamil.e Lukovsiut.e",
            "Karina Nguyen",
            "Nicholas Joseph",
            "Sam McCandlish",
            "Jared Kaplan",
            "Sam Bowman"
        ],
        "citations": 119,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
        "abstract": "By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. These results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented LLMs. Resources are available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Renze Lou",
            "Yu Su"
        ],
        "citations": 116,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph",
        "abstract": "Large language models (LLMs) have made signiﬁcant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs’ ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufﬁcient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring additional training costs.",
        "authors": [
            "Jiashuo Sun",
            "Chengjin Xu",
            "Lumingyuan Tang",
            "Sai Wang",
            "Chen Lin",
            "Yeyun Gong",
            "H. Shum",
            "Jian Guo"
        ],
        "citations": 111,
        "references": 66,
        "year": 2023
    },
    {
        "title": "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation",
        "abstract": "In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. Third, we find that generating the entire class all at once (i.e. holistic generation strategy) is the best generation strategy only for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and compositional) is better strategies for the other models with limited ability of understanding long instructions and utilizing the middle information. Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes. Our benchmark is available at https://github.com/FudanSELab/ClassEval.",
        "authors": [
            "Xueying Du",
            "Mingwei Liu",
            "Kaixin Wang",
            "Hanlin Wang",
            "Junwei Liu",
            "Yixuan Chen",
            "Jiayi Feng",
            "Chaofeng Sha",
            "Xin Peng",
            "Yiling Lou"
        ],
        "citations": 98,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
        "authors": [
            "Shizhe Diao",
            "Pengcheng Wang",
            "Yong Lin",
            "Xiang Liu",
            "Tong Zhang"
        ],
        "citations": 99,
        "references": 91,
        "year": 2023
    },
    {
        "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
        "abstract": "Large language models (LLMs) have succeeded significantly in various applications but remain susceptible to adversarial jailbreaks that void their safety guardrails. Previous attempts to exploit these vulnerabilities often rely on high-cost computational extrapolations, which may not be practical or efficient. In this paper, inspired by the authority influence demonstrated in the Milgram experiment, we present a lightweight method to take advantage of the LLMs' personification capabilities to construct $\\textit{a virtual, nested scene}$, allowing it to realize an adaptive way to escape the usage control in a normal scenario. Empirically, the contents induced by our approach can achieve leading harmfulness rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open-source and closed-source LLMs, $\\textit{e.g.}$, Llama-2, Llama-3, GPT-3.5, GPT-4, and GPT-4o. The code and data are available at: https://github.com/tmlr-group/DeepInception.",
        "authors": [
            "Xuan Li",
            "Zhanke Zhou",
            "Jianing Zhu",
            "Jiangchao Yao",
            "Tongliang Liu",
            "Bo Han"
        ],
        "citations": 99,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
        "abstract": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
        "authors": [
            "Hongxin Zhang",
            "Weihua Du",
            "Jiaming Shan",
            "Qinhong Zhou",
            "Yilun Du",
            "J. Tenenbaum",
            "Tianmin Shu",
            "Chuang Gan"
        ],
        "citations": 118,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Do Large Language Models Know What They Don't Know?",
        "abstract": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.",
        "authors": [
            "Zhangyue Yin",
            "Qiushi Sun",
            "Qipeng Guo",
            "Jiawen Wu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "citations": 118,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
        "abstract": "Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.",
        "authors": [
            "Yubo Ma",
            "Yixin Cao",
            "YongChing Hong",
            "Aixin Sun"
        ],
        "citations": 115,
        "references": 75,
        "year": 2023
    },
    {
        "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
        "abstract": "While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations. Augmenting LLMs with domain-specific tools such as database utilities can facilitate easier and more precise access to specialized knowledge. In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions. Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs by in-context learning and an augmented decoding algorithm that can detect and execute API calls. Experimental results show that GeneGPT achieves state-of-the-art performance on eight tasks in the GeneTuring benchmark with an average score of 0.83, largely surpassing retrieval-augmented LLMs such as the new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1) API demonstrations have good cross-task generalizability and are more useful than documentations for in-context learning; (2) GeneGPT can generalize to longer chains of API calls and answer multi-hop questions in GeneHop, a novel dataset introduced in this work; (3) Different types of errors are enriched in different tasks, providing valuable insights for future improvements.",
        "authors": [
            "Qiao Jin",
            "Yifan Yang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "citations": 117,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models",
        "abstract": "The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.",
        "authors": [
            "Paula Maddigan",
            "Teo Susnjak"
        ],
        "citations": 115,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Computing Education in the Era of Generative AI",
        "abstract": "Challenges and opportunities faced by computing educators and students adapting to LLMs capable of generating accurate source code from natural-language problem descriptions.",
        "authors": [
            "Paul Denny",
            "J. Prather",
            "Brett A. Becker",
            "James Finnie-Ansley",
            "Arto Hellas",
            "Juho Leinonen",
            "Andrew Luxton-Reilly",
            "B. Reeves",
            "E. Santos",
            "Sami Sarsa"
        ],
        "citations": 120,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
        "abstract": "In this paper, we present an innovative process-oriented math process reward model called \\textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \\textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \\textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\\%$\\to$84.1\\% on GSM8K and 28.6\\%$\\to$33.0\\% on MATH). The accuracy can be further enhanced to 89.1\\% and 43.5\\% on GSM8K and MATH with the verification of Math-Shepherd, respectively. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.",
        "authors": [
            "Peiyi Wang",
            "Lei Li",
            "Zhihong Shao",
            "R. Xu",
            "Damai Dai",
            "Yifei Li",
            "Deli Chen",
            "Y.Wu",
            "Zhifang Sui"
        ],
        "citations": 97,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation",
        "abstract": "Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs' QA and judgemental capabilities. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.",
        "authors": [
            "Ruiyang Ren",
            "Yuhao Wang",
            "Yingqi Qu",
            "Wayne Xin Zhao",
            "J. Liu",
            "Hao Tian",
            "Huaqin Wu",
            "Ji-rong Wen",
            "Haifeng Wang"
        ],
        "citations": 98,
        "references": 34,
        "year": 2023
    },
    {
        "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback",
        "abstract": "To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural language feedback. (b) Better single-turn performance does not guarantee better multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.",
        "authors": [
            "Xingyao Wang",
            "Zihan Wang",
            "Jiateng Liu",
            "Yangyi Chen",
            "Lifan Yuan",
            "Hao Peng",
            "Heng Ji"
        ],
        "citations": 104,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
        "abstract": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
        "authors": [
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "citations": 107,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Language Models Represent Space and Time",
        "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual\"space neurons\"and\"time neurons\"that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.",
        "authors": [
            "Wes Gurnee",
            "Max Tegmark"
        ],
        "citations": 103,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions",
        "abstract": "Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks. While there has been a growing interest in Auto-GPT stypled agents, questions remain regarding the effectiveness and flexibility of Auto-GPT in solving real-world decision-making tasks. Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties. In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate real-world scenarios. Our aim is to gain deeper insights into this problem and understand the adaptability of GPT-based agents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme. This approach enables lightweight supervised learning without requiring fine-tuning of the foundational LLMs. We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.",
        "authors": [
            "Hui Yang",
            "Sifu Yue",
            "Yunzhong He"
        ],
        "citations": 101,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
        "abstract": "Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answering tasks.",
        "authors": [
            "Haoyang Huang",
            "Tianyi Tang",
            "Dongdong Zhang",
            "Wayne Xin Zhao",
            "Ting Song",
            "Yan Xia",
            "Furu Wei"
        ],
        "citations": 113,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
        "abstract": "Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).",
        "authors": [
            "Erfan Shayegani",
            "Md Abdullah Al Mamun",
            "Yu Fu",
            "Pedram Zaree",
            "Yue Dong",
            "Nael B. Abu-Ghazaleh"
        ],
        "citations": 104,
        "references": 183,
        "year": 2023
    },
    {
        "title": "Leveraging Large Language Models for Mental Health Prediction via Online Text Data",
        "abstract": "The recent technology boost of large language models (LLMs) has empowered a variety of applications. However, there is very little research on understanding and improving LLMs’ capability for the mental health domain. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction tasks via online text data. We conduct a wide range of experiments, covering zero-shot prompting, few-shot prompting, and instruction finetuning. The results indicate the promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned model, Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7% on balanced accuracy and performs on par with the state-of-the-art task-specific model. We summarize our findings into a set of action guidelines for future researchers, engineers, and practitioners on how to empower LLMs with better mental health domain knowledge and become an expert in mental health prediction tasks.",
        "authors": [
            "Xuhai Xu",
            "Bingsheng Yao",
            "Yuanzhe Dong",
            "Hong Yu",
            "James A. Hendler",
            "A. Dey",
            "Dakuo Wang"
        ],
        "citations": 101,
        "references": 92,
        "year": 2023
    },
    {
        "title": "A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",
        "abstract": "The utilization of large language models (LLMs) in the Healthcare domain has generated both excitement and concern due to their ability to effectively respond to freetext queries with certain professional knowledge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other. Then we summarize related Healthcare training data, training methods, optimization strategies, and usage. Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty. Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a shift from model-centered methodologies to data-centered methodologies. Also, we determine that the biggest obstacle of using LLMs in Healthcare are fairness, accountability, transparency and ethics.",
        "authors": [
            "Kai He",
            "Rui Mao",
            "Qika Lin",
            "Yucheng Ruan",
            "Xiang Lan",
            "Mengling Feng",
            "Erik Cambria"
        ],
        "citations": 108,
        "references": 421,
        "year": 2023
    },
    {
        "title": "On the assessment of generative AI in modeling tasks: an experience report with ChatGPT and UML",
        "abstract": null,
        "authors": [
            "J. Cámara",
            "J. Troya",
            "Lola Burgueño",
            "Antonio Vallecillo"
        ],
        "citations": 99,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Efficient Large Language Models: A Survey",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding and language generation, and thus have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain the repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient LLMs research and inspire them to contribute to this important and exciting field.",
        "authors": [
            "Zhongwei Wan",
            "Xin Wang",
            "Che Liu",
            "Samiul Alam",
            "Yu Zheng",
            "Jiachen Liu",
            "Zhongnan Qu",
            "Shen Yan",
            "Yi Zhu",
            "Quanlu Zhang",
            "Mosharaf Chowdhury",
            "Mi Zhang"
        ],
        "citations": 102,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Better Zero-Shot Reasoning with Role-Play Prompting",
        "abstract": "Modern large language models (LLMs) exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs’ reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks. Our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%. Upon further comparison with the Zero-Shot-CoT technique, which prompts the model to “think step by step”, our study demonstrates that role-play prompting acts as a more effective trigger for the CoT process.This highlights its potential to augment the reasoning capabilities of LLMs. We release our code at https://github.com/NKU-HLT/Role-Play-Prompting.",
        "authors": [
            "Aobo Kong",
            "Shiwan Zhao",
            "Hao Chen",
            "Qicheng Li",
            "Yong Qin",
            "Ruiqi Sun",
            "Xiaoxia Zhou"
        ],
        "citations": 100,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
        "abstract": "Purpose We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. Methods We developed an exam consisting of 100 radiation oncology physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. The performance of ChatGPT (GPT-4) was further explored by being asked to explain first, then answer. The deductive reasoning capability of ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct answer with “None of the above choices is the correct answer.”). A majority vote analysis was used to approximate how well each group could score when working together. Results ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on average, with improved accuracy when prompted to explain before answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups or Bard (LaMDA). In evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. Conclusion This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.",
        "authors": [
            "J. Holmes",
            "Zheng Liu",
            "Lian-Cheng Zhang",
            "Yuzhen Ding",
            "T. Sio",
            "L. Mcgee",
            "J. Ashman",
            "Xiang Li",
            "Tianming Liu",
            "Jiajian Shen",
            "W. Liu"
        ],
        "citations": 107,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities in generalizing to new tasks in a zero-shot or few-shot manner. However, the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question. Traditionally, Collaborative Filtering (CF) has been the most effective method for these tasks, predominantly relying on the extensive volume of rating data. In contrast, LLMs typically demand considerably less data while maintaining an exhaustive world knowledge about each item, such as movies or products. In this paper, we conduct a thorough examination of both CF and LLMs within the classic task of user rating prediction, which involves predicting a user's rating for a candidate item based on their past ratings. We investigate various LLMs in different sizes, ranging from 250M to 540B parameters and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We conduct comprehensive analysis to compare between LLMs and strong CF methods, and find that zero-shot LLMs lag behind traditional recommender models that have the access to user interaction data, indicating the importance of user interaction data. However, through fine-tuning, LLMs achieve comparable or even better performance with only a small fraction of the training data, demonstrating their potential through data efficiency.",
        "authors": [
            "Wang-Cheng Kang",
            "Jianmo Ni",
            "Nikhil Mehta",
            "M. Sathiamoorthy",
            "Lichan Hong",
            "Ed H. Chi",
            "D. Cheng"
        ],
        "citations": 98,
        "references": 44,
        "year": 2023
    },
    {
        "title": "A Bibliometric Review of Large Language Models Research from 2017 to 2023",
        "abstract": "Large language models (LLMs), such as OpenAI’s Generative Pre-trained Transformer (GPT), are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks. LLMs have become a highly sought-after research area because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains, including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of LLMs research. Overall, this paper offers valuable insights into the current state, impact, and potential of LLMs research and its applications.",
        "authors": [
            "Lizhou Fan",
            "Lingyao Li",
            "Zihui Ma",
            "Sanggyu Lee",
            "Huizi Yu",
            "Libby Hemphill"
        ],
        "citations": 111,
        "references": 86,
        "year": 2023
    },
    {
        "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
        "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs’ resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4 , 032 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets, with 567 , 084 test samples in total. Our findings demonstrate that contemporary LLMs are vulnerable to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. We make our code, prompts, and methodologies to generate adversarial prompts publicly accessible, thereby enabling and encouraging collaborative exploration in this pivotal field: https://github.com/microsoft/promptbench .",
        "authors": [
            "Kaijie Zhu",
            "Jindong Wang",
            "Jiaheng Zhou",
            "Zichen Wang",
            "Hao Chen",
            "Yidong Wang",
            "Linyi Yang",
            "Weirong Ye",
            "Neil Zhenqiang Gong",
            "Yue Zhang",
            "Xing Xie"
        ],
        "citations": 99,
        "references": 125,
        "year": 2023
    },
    {
        "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
        "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Jiaying Lu",
            "Chengyuan Deng",
            "Can Zheng",
            "Junxiang Wang",
            "Tanmoy Chowdhury",
            "Yun-Qing Li",
            "Hejie Cui",
            "Xuchao Zhang",
            "Tian-yu Zhao",
            "Amit Panalkar",
            "Wei Cheng",
            "Haoyu Wang",
            "Yanchi Liu",
            "Zhengzhang Chen",
            "Haifeng Chen",
            "Chris White",
            "Quanquan Gu",
            "Jian Pei",
            "Carl Yang",
            "Liang Zhao"
        ],
        "citations": 100,
        "references": 331,
        "year": 2023
    },
    {
        "title": "Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of ChatGPT and other large language models in scholarly peer review",
        "abstract": null,
        "authors": [
            "Mohammad Hosseini",
            "S. Horbach"
        ],
        "citations": 112,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration",
        "abstract": "Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.",
        "authors": [
            "Ping Yu",
            "Hua Xu",
            "Xia Hu",
            "Chao Deng"
        ],
        "citations": 97,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Magicoder: Source Code Is All You Need",
        "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-I NSTRUCT , a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse , realistic , and controllable data. The orthogonality of OSS-I NSTRUCT and other data generation methods like Evol-Instruct further enables us to build an enhanced Magicoder S . Both Magicoder and Magicoder S substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, Magicoder S -CL-7B based on C ODE L LAMA even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-I NSTRUCT opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references.",
        "authors": [
            "Yuxiang Wei",
            "Zhe Wang",
            "Jiawei Liu",
            "Yifeng Ding",
            "Lingming Zhang"
        ],
        "citations": 90,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Automatic Chain of Thought Prompting in Large Language Models",
        "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like\"Let's think step by step\"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the\"Let's think step by step\"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot",
        "authors": [
            "Zhuosheng Zhang",
            "Aston Zhang",
            "Mu Li",
            "Alexander J. Smola"
        ],
        "citations": 477,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
        "abstract": "A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.",
        "authors": [
            "Viet Dac Lai",
            "Chien Van Nguyen",
            "Nghia Trung Ngo",
            "Thuat Nguyen",
            "Franck Dernoncourt",
            "Ryan A. Rossi",
            "Thien Huu Nguyen"
        ],
        "citations": 89,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Making LLaMA SEE and Draw with SEED Tokenizer",
        "abstract": "The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce SEED, an elaborate image tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant.",
        "authors": [
            "Yuying Ge",
            "Sijie Zhao",
            "Ziyun Zeng",
            "Yixiao Ge",
            "Chen Li",
            "Xintao Wang",
            "Ying Shan"
        ],
        "citations": 87,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.",
        "authors": [
            "Zhijing Jin",
            "Jiarui Liu",
            "Zhiheng Lyu",
            "Spencer Poff",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Mona T. Diab",
            "B. Scholkopf"
        ],
        "citations": 87,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
        "abstract": "This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs’ problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io",
        "authors": [
            "Logesh Kumar Umapathi",
            "Ankit Pal",
            "Malaikannan Sankarasubbu"
        ],
        "citations": 87,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
        "abstract": "Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call\"EmotionPrompt\"that combines the original prompt with emotional stimuli), e.g., 8.00% relative performance improvement in Instruction Induction and 115% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.",
        "authors": [
            "Cheng Li",
            "Jindong Wang",
            "Kaijie Zhu",
            "Yixuan Zhang",
            "Wenxin Hou",
            "Jianxun Lian",
            "Xingxu Xie"
        ],
        "citations": 87,
        "references": 40,
        "year": 2023
    },
    {
        "title": "HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models",
        "abstract": "Large language models (LLMs), after being aligned with vision models and integrated into vision-language models (VLMs), can bring impressive improvement in image reasoning tasks. This was shown by the recently released GPT-4V(ison), LLaVA-1.5, etc. However, the strong language prior in these SOTA LVLMs can be a double-edged sword: they may ignore the image context and solely rely on the (even contradictory) language prior for reasoning. In contrast, the vision modules in VLMs are weaker than LLMs and may result in misleading visual representations, which are then translated to confident mistakes by LLMs. To study these two types of VLM mistakes, i.e., language hallucination and visual illusion , we curated “H ALLUSION B ENCH 1 ,” an image-context reasoning benchmark that is still challenging to even GPT-4V and LLaVA-1.5. We provide a detailed analysis of examples in H ALLUSION B ENCH , which sheds novel insights on the illusion or hallucination of VLMs and how to improve them in the future. The benchmark and codebase will be released at https://github.com/tianyi-lab/HallusionBench.",
        "authors": [
            "Fuxiao Liu",
            "Tianrui Guan",
            "Xiyang Wu",
            "Zongxia Li",
            "Lichang Chen",
            "Yaser Yacoob",
            "Dinesh Manocha",
            "Tianyi Zhou"
        ],
        "citations": 93,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources",
        "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.Code and data are available at https://github.com/OpenLMLab/LOMO.",
        "authors": [
            "Kai Lv",
            "Yuqing Yang",
            "Tengxiao Liu",
            "Qinghui Gao",
            "Qipeng Guo",
            "Xipeng Qiu"
        ],
        "citations": 92,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
        "abstract": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty measures, applying them to *selective NLG* where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple measure for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.",
        "authors": [
            "Zhen Lin",
            "Shubhendu Trivedi",
            "Jimeng Sun"
        ],
        "citations": 88,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Embracing Large Language Models for Medical Applications: Opportunities and Challenges",
        "abstract": "Large language models (LLMs) have the potential to revolutionize the field of medicine by, among other applications, improving diagnostic accuracy and supporting clinical decision-making. However, the successful integration of LLMs in medicine requires addressing challenges and considerations specific to the medical domain. This viewpoint article provides a comprehensive overview of key aspects for the successful implementation of LLMs in medicine, including transfer learning, domain-specific fine-tuning, domain adaptation, reinforcement learning with expert input, dynamic training, interdisciplinary collaboration, education and training, evaluation metrics, clinical validation, ethical considerations, data privacy, and regulatory frameworks. By adopting a multifaceted approach and fostering interdisciplinary collaboration, LLMs can be developed, validated, and integrated into medical practice responsibly, effectively, and ethically, addressing the needs of various medical disciplines and diverse patient populations. Ultimately, this approach will ensure that LLMs enhance patient care and improve overall health outcomes for all.",
        "authors": [
            "Mert Karabacak",
            "Konstantinos Margetis"
        ],
        "citations": 93,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models",
        "abstract": "The escalating debate on AI’s capabilities warrants developing reliable metrics to assess machine “intelligence.” Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs’ N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.",
        "authors": [
            "Natalie Shapira",
            "Mosh Levy",
            "S. Alavi",
            "Xuhui Zhou",
            "Yejin Choi",
            "Yoav Goldberg",
            "Maarten Sap",
            "Vered Shwartz"
        ],
        "citations": 89,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
        "abstract": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \\textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \\textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.",
        "authors": [
            "Qingyu Lu",
            "Baopu Qiu",
            "Liang Ding",
            "Liping Xie",
            "Dacheng Tao"
        ],
        "citations": 94,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
        "abstract": "With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to be able to identify risks through the evaluation of\"dangerous capabilities\"in order to responsibly deploy LLMs. In this work, we collect the first open-source dataset to evaluate safeguards in LLMs, and deploy safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We annotate and assess the responses of six popular LLMs to these instructions. Based on our annotation, we proceed to train several BERT-like classifiers, and find that these small classifiers can achieve results that are comparable with GPT-4 on automatic safety evaluation. Warning: this paper contains example data that may be offensive, harmful, or biased.",
        "authors": [
            "Yuxia Wang",
            "Haonan Li",
            "Xudong Han",
            "Preslav Nakov",
            "Timothy Baldwin"
        ],
        "citations": 85,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.",
        "authors": [
            "Pouya Pezeshkpour",
            "Estevam Hruschka"
        ],
        "citations": 94,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Knowledge Editing for Large Language Models: A Survey",
        "abstract": "\n Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently,\n Knowledge-based Model Editing\n (KME), also known as\n Knowledge Editing\n or\n Model Editing\n , has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.\n",
        "authors": [
            "Song Wang",
            "Yaochen Zhu",
            "Haochen Liu",
            "Zaiyi Zheng",
            "Chen Chen",
            "Jundong Li"
        ],
        "citations": 88,
        "references": 163,
        "year": 2023
    },
    {
        "title": "Large Language Models on Graphs: A Comprehensive Survey",
        "abstract": "Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.",
        "authors": [
            "Bowen Jin",
            "Gang Liu",
            "Chi Han",
            "Meng Jiang",
            "Heng Ji",
            "Jiawei Han"
        ],
        "citations": 94,
        "references": 262,
        "year": 2023
    },
    {
        "title": "Document-Level Machine Translation with Large Language Models",
        "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM.",
        "authors": [
            "Longyue Wang",
            "Chenyang Lyu",
            "Tianbo Ji",
            "Zhirui Zhang",
            "Dian Yu",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "citations": 95,
        "references": 56,
        "year": 2023
    },
    {
        "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
        "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.",
        "authors": [
            "Yuhui Li",
            "Fangyun Wei",
            "Jinjing Zhao",
            "Chao Zhang",
            "Hongyang Zhang"
        ],
        "citations": 85,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Multilingual Jailbreak Challenges in Large Language Models",
        "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \\textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at \\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.",
        "authors": [
            "Yue Deng",
            "Wenxuan Zhang",
            "Sinno Jialin Pan",
            "Lidong Bing"
        ],
        "citations": 85,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
        "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.",
        "authors": [
            "O. Ovadia",
            "Menachem Brief",
            "Moshik Mishaeli",
            "Oren Elisha"
        ],
        "citations": 86,
        "references": 63,
        "year": 2023
    },
    {
        "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models",
        "abstract": "In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex domains, they often face the need to follow longer user prompts or generate longer texts. In these situations, the length generalization failure of LLMs on long sequences becomes more prominent. Most pre-training schemes truncate training sequences to a fixed length. LLMs often struggle to generate fluent and coherent texts after longer contexts, even with relative positional encoding specifically designed to cope with this problem. Common solutions such as finetuning on longer corpora often involve daunting hardware and time costs and require careful training process design. To more efficiently extrapolate existing LLMs’ generation quality to longer texts, we theoretically and empirically investigate the main out-of-distribution (OOD) factors contributing to this problem. Inspired by this diagnosis, we propose a simple yet effective solution for on-the-fly length generalization, LM-Infinite. It involves only a Λ-shaped attention mask (to avoid excessive attended tokens) and a distance limit (to avoid unseen distances) while requiring no parameter updates or learning. We find it applicable to a variety of LLMs using relative-position encoding methods. LM-Infinite is computationally efficient with O(n) time and space, and demonstrates consistent text generation fluency and quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup. We will make the codes publicly available following publication.",
        "authors": [
            "Chi Han",
            "Qifan Wang",
            "Wenhan Xiong",
            "Yu Chen",
            "Heng Ji",
            "Sinong Wang"
        ],
        "citations": 80,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Can the ChatGPT and other large language models with internet-connected database solve the questions and concerns of patient with prostate cancer and help democratize medical knowledge?",
        "abstract": null,
        "authors": [
            "Lingxuan Zhu",
            "W. Mou",
            "R. Chen"
        ],
        "citations": 86,
        "references": 7,
        "year": 2023
    },
    {
        "title": "VeriGen: A Large Language Model for Verilog Code Generation",
        "abstract": "In this study, we explore the capability of Large Language Models (LLMs) to automate hardware design by automatically completing partial Verilog code, a common language for designing and modeling digital systems. We fine-tune pre-existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite, featuring a custom problem set and testing benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase. Upon testing with a more diverse and complex problem set, we find that the fine-tuned model shows competitive performance against state-of-the-art gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41% improvement in generating syntactically correct Verilog code across various problem categories compared to its pre-trained counterpart, highlighting the potential of smaller, in-house LLMs in hardware design automation. We release our training/evaluation scripts and LLM checkpoints as open-source contributions.",
        "authors": [
            "Shailja Thakur",
            "Baleegh Ahmad",
            "H. Pearce",
            "Benjamin Tan",
            "Brendan Dolan-Gavitt",
            "R. Karri",
            "S. Garg"
        ],
        "citations": 84,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods",
        "abstract": "– Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called “machine psychology”. The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behavioral patterns discovered in LLMs are to be interpreted. In sum, machine psychology aims to discover emergent abilities in LLMs that cannot be detected by most traditional natural language processing benchmarks.",
        "authors": [
            "Thilo Hagendorff"
        ],
        "citations": 84,
        "references": 102,
        "year": 2023
    },
    {
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "abstract": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code.We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
        "authors": [
            "Kechi Zhang",
            "Zhuo Li",
            "Jia Li",
            "Ge Li",
            "Zhi Jin"
        ],
        "citations": 78,
        "references": 42,
        "year": 2023
    },
    {
        "title": "MGTBench: Benchmarking Machine-Generated Text Detection",
        "abstract": "Nowadays, powerful large language models (LLMs) such as ChatGPT have demonstrated revolutionary power in a variety of tasks. Consequently, the detection of machine-generated texts (MGTs) is becoming increasingly crucial as LLMs become more advanced and prevalent. These models have the ability to generate human-like language, making it challenging to discern whether a text is authored by a human or a machine. This raises concerns regarding authenticity, accountability, and potential bias. However, existing methods for detecting MGTs are evaluated using different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework that encompasses various methodologies. Furthermore, it remains unclear how existing detection methods would perform against powerful LLMs. In this paper, we fill this gap by proposing the first benchmark framework for MGT detection against powerful LLMs, named MGTBench. Extensive evaluations on public datasets with curated texts generated by various powerful LLMs such as ChatGPT-turbo and Claude demonstrate the effectiveness of different detection methods. Our ablation study shows that a larger number of words in general leads to better performance and most detection methods can achieve similar performance with much fewer training samples. Moreover, we delve into a more challenging task: text attribution. Our findings indicate that the model-based detection methods still perform well in the text attribution task. To investigate the robustness of different detection methods, we consider three adversarial attacks, namely paraphrasing, random spacing, and adversarial perturbations. We discover that these attacks can significantly diminish detection effectiveness, underscoring the critical need for the development of more robust detection methods.",
        "authors": [
            "Xinlei He",
            "Xinyue Shen",
            "Z. Chen",
            "M. Backes",
            "Yang Zhang"
        ],
        "citations": 79,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Leveraging Large Language Models in Conversational Recommender Systems",
        "abstract": "A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture powered by LLMs. For improved personalization, we describe how an LLM can consume interpretable natural language user profiles and use them to modulate session-level context. To overcome conversational data limitations in the absence of an existing production CRS, we propose techniques for building a controllable LLM-based user simulator to generate synthetic conversations. As a proof of concept we introduce RecLLM, a large-scale CRS for YouTube videos built on LaMDA, and demonstrate its fluency and diverse functionality through some illustrative example conversations.",
        "authors": [
            "Luke Friedman",
            "Sameer Ahuja",
            "David Allen",
            "Zhenning Tan",
            "Hakim Sidahmed",
            "Changbo Long",
            "Jun Xie",
            "Gabriel Schubiner",
            "Ajay Patel",
            "Harsh Lara",
            "Brian Chu",
            "Zexiang Chen",
            "Manoj Tiwari"
        ],
        "citations": 78,
        "references": 118,
        "year": 2023
    },
    {
        "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
        "abstract": "Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.",
        "authors": [
            "Taicheng Guo",
            "Kehan Guo",
            "B. Nan",
            "Zhengwen Liang",
            "Zhichun Guo",
            "N. Chawla",
            "O. Wiest",
            "Xiangliang Zhang"
        ],
        "citations": 82,
        "references": 77,
        "year": 2023
    },
    {
        "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
        "abstract": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
        "authors": [
            "Martin Pawelczyk",
            "Seth Neel",
            "Himabindu Lakkaraju"
        ],
        "citations": 76,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning",
        "abstract": "Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.",
        "authors": [
            "Mingyang Geng",
            "Shangwen Wang",
            "Dezun Dong",
            "Hao Wang",
            "Ge Li",
            "Zhi Jin",
            "Xiaoguang Mao",
            "Xiangke Liao"
        ],
        "citations": 83,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "abstract": "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user’s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
        "authors": [
            "Jinheon Baek",
            "Alham Fikri Aji",
            "Amir Saffari"
        ],
        "citations": 82,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Towards Building The Federatedgpt: Federated Instruction Tuning",
        "abstract": "While \"instruction-tuned\" generative large language models (LLMs) have demonstrated an impressive ability to generalize to new tasks, the training phases heavily rely on large amounts of diverse and high-quality instruction data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. To tackle this issue, our study introduces a new approach called Federated Instruction Tuning (FedIT), which leverages federated learning (FL) as the learning framework for the instruction tuning of LLMs. This marks the first exploration of FL-based instruction tuning for LLMs. This is especially important since text data is predominantly generated by end users. For example, collecting extensive amounts of everyday user conversations can be a useful approach to improving the generalizability of LLMs, allowing them to generate authentic and natural responses. Therefore, it is imperative to design and adapt FL approaches to effectively leverage these users’ diverse instructions stored on local devices while mitigating concerns related to the data sensitivity and the cost of data transmission. In this study, we leverage extensive qualitative analysis, including the prevalent GPT-4 auto-evaluation to illustrate how our FedIT framework enhances the performance of LLMs. Utilizing diverse instruction sets on the client side, FedIT outperforms centralized training with only limited local instructions.",
        "authors": [
            "Jianyi Zhang",
            "Saeed Vahidian",
            "Martin Kuo",
            "Chunyuan Li",
            "Ruiyi Zhang",
            "Guoyin Wang",
            "Yiran Chen"
        ],
        "citations": 79,
        "references": 103,
        "year": 2023
    },
    {
        "title": "Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning",
        "abstract": "In recent years, pre-trained large language models (LLMs) have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. Current understandings of the underlying mechanisms by which this capability arises from regular language model pretraining objectives remain disconnected from the real-world LLMs. This study aims to examine the in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs as latent variable models. On this premise, we propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs. We demonstrate significant improvement over baselines, averaged over eight GPT models on eight real-world text classification datasets. We also demonstrate the real-world usefulness of our algorithm on GSM8K, a math word problem dataset. Our empirical findings support our hypothesis that LLMs implicitly infer a latent variable containing task information.",
        "authors": [
            "Xinyi Wang",
            "Wanrong Zhu",
            "Michael Stephen Saxon",
            "William Yang Wang"
        ],
        "citations": 77,
        "references": 52,
        "year": 2023
    },
    {
        "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
        "abstract": "Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards\"true long-context understanding\".",
        "authors": [
            "Jiaqi Li",
            "Mengmeng Wang",
            "Zilong Zheng",
            "Muhan Zhang"
        ],
        "citations": 76,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Query2doc: Query Expansion with Large Language Models",
        "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Furu Wei"
        ],
        "citations": 77,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization",
        "abstract": "While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs' capability and safety. Our code is available at \\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.",
        "authors": [
            "Zhexin Zhang",
            "Junxiao Yang",
            "Pei Ke",
            "Minlie Huang"
        ],
        "citations": 82,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
        "abstract": "During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful “copilots” in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI “copilots” (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50 bugs, respectively, surpassing the best-performing baseline by 14 and 16 bugs fixed. More importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given the same generation budget.",
        "authors": [
            "Yuxiang Wei",
            "Chun Xia",
            "Lingming Zhang"
        ],
        "citations": 77,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
        "abstract": "In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.",
        "authors": [
            "Yingdong Hu",
            "Fanqi Lin",
            "Tong Zhang",
            "Li Yi",
            "Yang Gao"
        ],
        "citations": 74,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
        "abstract": "Large language models (LLMs) are used to generate content for a wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like ChatGPT or search engines like Bing. This intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe, inaccurate or toxic outputs. While alignment techniques like reinforcement learning with human feedback (RLHF) and red-teaming can mitigate some safety concerns and improve model capabilities, it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values. Different people may legitimately disagree on their preferences for language and conversational norms, as well as on values or ideologies which guide their communication. Personalising LLMs through micro-level preference learning processes may result in models that are better aligned with each user. However, there are several normative challenges in defining the bounds of a societally-acceptable and safe degree of personalisation. In this paper, we ask how, and in what ways, LLMs should be personalised. First, we review literature on current paradigms for aligning LLMs with human feedback, and identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in who we are really aligning to. Second, we present a taxonomy of benefits and risks associated with personalised LLMs, for individuals and society at large. Finally, we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment, while restraining unsafe and undesirable LLM-behaviours within (supra-)national and organisational bounds.",
        "authors": [
            "Hannah Rose Kirk",
            "Bertie Vidgen",
            "Paul Röttger",
            "Scott A. Hale"
        ],
        "citations": 80,
        "references": 257,
        "year": 2023
    },
    {
        "title": "Large Language Models for Robotics: A Survey",
        "abstract": "The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention. LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy. Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning. We first provide an overview of the background and development of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and recent advancements in robotics models based on LLMs. We then delve into the various techniques used in the model, including those employed in perception, decision-making, control, and interaction. Finally, we explore the applications of LLMs in robotics and some potential challenges they may face in the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics is one of the promising but challenging paths to achieve this.",
        "authors": [
            "Fanlong Zeng",
            "Wensheng Gan",
            "Yongheng Wang",
            "Ning Liu",
            "Philip S. Yu"
        ],
        "citations": 92,
        "references": 149,
        "year": 2023
    },
    {
        "title": "Do Large Language Models Understand Chemistry? A Conversation with ChatGPT",
        "abstract": "Large language models (LLMs) have promised a revolution in answering complex questions using the ChatGPT model. Its application in chemistry is still in its infancy. This viewpoint addresses the question of how well ChatGPT understands chemistry by posing five simple tasks in different subareas of chemistry.",
        "authors": [
            "Cayque Monteiro Castro Nascimento",
            "A. S. Pimentel"
        ],
        "citations": 91,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Large language models in health care: Development, applications, and challenges",
        "abstract": "Abstract Recently, the emergence of ChatGPT, an artificial intelligence chatbot developed by OpenAI, has attracted significant attention due to its exceptional language comprehension and content generation capabilities, highlighting the immense potential of large language models (LLMs). LLMs have become a burgeoning hotspot across many fields, including health care. Within health care, LLMs may be classified into LLMs for the biomedical domain and LLMs for the clinical domain based on the corpora used for pre‐training. In the last 3 years, these domain‐specific LLMs have demonstrated exceptional performance on multiple natural language processing tasks, surpassing the performance of general LLMs as well. This not only emphasizes the significance of developing dedicated LLMs for the specific domains, but also raises expectations for their applications in health care. We believe that LLMs may be used widely in preconsultation, diagnosis, and management, with appropriate development and supervision. Additionally, LLMs hold tremendous promise in assisting with medical education, medical writing and other related applications. Likewise, health care systems must recognize and address the challenges posed by LLMs.",
        "authors": [
            "Rui Yang",
            "Ting Fang Tan",
            "Wei Lu",
            "A. J. Thirunavukarasu",
            "D. Ting",
            "Nan Liu"
        ],
        "citations": 95,
        "references": 67,
        "year": 2023
    },
    {
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
        "abstract": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.",
        "authors": [
            "Ning Miao",
            "Y. Teh",
            "Tom Rainforth"
        ],
        "citations": 84,
        "references": 24,
        "year": 2023
    },
    {
        "title": "Understanding Social Reasoning in Language Models with Language Models",
        "abstract": "As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.",
        "authors": [
            "Kanishk Gandhi",
            "Jan-Philipp Franken",
            "Tobias Gerstenberg",
            "Noah D. Goodman"
        ],
        "citations": 72,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
        "abstract": "Large Language Models(LLMs) have demonstrated remarkable performance across various natural language processing tasks; however, how to comprehensively and accurately assess their performance becomes an urgent issue to be addressed. This paper introduces GAOKAO-Bench, an intuitive benchmark that employs questions from the Chinese GAOKAO examination as test samples, including both subjective and objective questions. To align with human examination methods, we design a method based on zero-shot settings to evaluate the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and ERNIE-Bot.Our findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across various subjects. We also use LLMs to grade the subjective questions, and find that model scores achieve a moderate level of consistency with human scores. In conclusion, this research contributes a robust evaluation benchmark for future large language models and offers valuable insights into the advantages and limitations of such models.",
        "authors": [
            "Xiaotian Zhang",
            "Chun-yan Li",
            "Yi Zong",
            "Zhengyu Ying",
            "Liang He",
            "Xipeng Qiu"
        ],
        "citations": 71,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "abstract": "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user’s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
        "authors": [
            "Jinheon Baek",
            "Alham Fikri Aji",
            "Amir Saffari"
        ],
        "citations": 70,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",
        "abstract": "Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.",
        "authors": [
            "Maximilian Mozes",
            "Xuanli He",
            "Bennett Kleinberg",
            "L. D. Griffin"
        ],
        "citations": 70,
        "references": 222,
        "year": 2023
    },
    {
        "title": "Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles",
        "abstract": "The future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities. Autonomous vehicles of the future will not only transport passengers but also interact and adapt to their desires, making the journey comfortable, efficient, and pleasant. In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles' decision-making processes. By integrating LLMs' natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles. The proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving technologies.",
        "authors": [
            "Can Cui",
            "Yunsheng Ma",
            "Xu Cao",
            "Wenqian Ye",
            "Ziran Wang"
        ],
        "citations": 74,
        "references": 35,
        "year": 2023
    },
    {
        "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "abstract": "Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs-despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies. In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of LLaMA-a recent state-of-the-art LLM-developed by Meta AI on two generations of popular GPUs (NVIDIA V100 & A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. We present the results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. To our knowledge, our work is the one of the first to study LLM inference performance from the perspective of computational and energy resources at this scale.",
        "authors": [
            "S. Samsi",
            "Dan Zhao",
            "Joseph McDonald",
            "Baolin Li",
            "Adam Michaleas",
            "Michael Jones",
            "William Bergeron",
            "J. Kepner",
            "Devesh Tiwari",
            "V. Gadepally"
        ],
        "citations": 74,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models",
        "abstract": "Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs’ sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15% to 48% performance gain in accuracy and F1 score.",
        "authors": [
            "Boyu Zhang",
            "Hongyang Yang",
            "Tianyu Zhou",
            "Muhammad Ali Babar",
            "Xiao-Yang Liu"
        ],
        "citations": 75,
        "references": 36,
        "year": 2023
    },
    {
        "title": "An Iterative Optimizing Framework for Radiology Report Summarization With ChatGPT",
        "abstract": "The “Impression” section of a radiology report is a critical basis for communication between radiologists and other physicians. Typically written by radiologists, this part is derived from the “Findings” section, which can be laborious and error-prone. Although deep-learning-based models, such as bidirectional encoder representation from transformers (BERT), have achieved promising results in automatic impression generation (AIG), such models often require substantial amounts of medical data and have poor generalization performance. Recently, large language models (LLMs) like Chat Generative Pre-trained Transformer (ChatGPT) have shown strong generalization capabilities and performance, but their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, leveraging the contextual learning capabilities of LLMs through our dynamic prompt and iterative optimization algorithm to accomplish the AIG task. ImpressionGPT initially employs a small amount of domain-specific data to create a dynamic prompt, extracting contextual semantic information closely related to the test data. Subsequently, the iterative optimization algorithm automatically evaluates the output of LLMs and provides optimization suggestions, continuously refining the output results. The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database (MIMIC-CXR) and Open Access Biomedical Image Search Engine (OpenI) datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains.",
        "authors": [
            "Chong Ma",
            "Zihao Wu",
            "Jiaqi Wang",
            "Shaochen Xu",
            "Yaonai Wei",
            "Zheng Liu",
            "Lei Guo",
            "Xiaoya Cai",
            "Shu Zhang",
            "Tuo Zhang",
            "Dajiang Zhu",
            "Dinggang Shen",
            "Tianming Liu",
            "Xiang Li"
        ],
        "citations": 81,
        "references": 48,
        "year": 2023
    },
    {
        "title": "LLMs Accelerate Annotation for Medical Information Extraction",
        "abstract": "The unstructured nature of clinical notes within electronic health records often conceals vital patient-related information, making it challenging to access or interpret. To uncover this hidden information, specialized Natural Language Processing (NLP) models are required. However, training these models necessitates large amounts of labeled data, a process that is both time-consuming and costly when relying solely on human experts for annotation. In this paper, we propose an approach that combines Large Language Models (LLMs) with human expertise to create an efficient method for generating ground truth labels for medical text annotation. By utilizing LLMs in conjunction with human annotators, we significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets. We rigorously evaluate our method on a medical information extraction task, demonstrating that our approach not only substantially cuts down on human intervention but also maintains high accuracy. The results highlight the potential of using LLMs to improve the utilization of unstructured clinical data, allowing for the swift deployment of tailored NLP solutions in healthcare.",
        "authors": [
            "Akshay Goel",
            "Almog Gueta",
            "Omry Gilon",
            "Chang Liu",
            "Sofia Erell",
            "Lan Huong Nguyen",
            "Xiaohong Hao",
            "Bolous Jaber",
            "Shashir Reddy",
            "Rupesh Kartha",
            "Jean Steiner",
            "Itay Laish",
            "Amir Feder"
        ],
        "citations": 71,
        "references": 40,
        "year": 2023
    },
    {
        "title": "14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon",
        "abstract": "Large-language models (LLMs) such as GPT-4 caught the interest of many scientists. Recent studies suggested that these models could be useful in chemistry and materials science. To explore these possibilities, we organized a hackathon. This article chronicles the projects built as part of this hackathon. Participants employed LLMs for various applications, including predicting properties of molecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data, and developing new educational applications. The diverse topics and the fact that working prototypes could be generated in less than two days highlight that LLMs will profoundly impact the future of our fields. The rich collection of ideas and projects also indicates that the applications of LLMs are not limited to materials science and chemistry but offer potential benefits to a wide range of scientific disciplines.",
        "authors": [
            "K. Jablonka",
            "Qianxiang Ai",
            "Alexander Al-Feghali",
            "S. Badhwar",
            "Joshua D. Bocarsly Andres M Bran",
            "S. Bringuier",
            "L. Brinson",
            "K. Choudhary",
            "Defne Çirci",
            "Sam Cox",
            "W. D. Jong",
            "Matthew L. Evans",
            "Nicolas Gastellu",
            "Jérôme Genzling",
            "M. Gil",
            "Ankur Gupta",
            "Zhi Hong",
            "A. Imran",
            "S. Kruschwitz",
            "A. Labarre",
            "Jakub L'ala",
            "Tao Liu",
            "Steven Ma",
            "Sauradeep Majumdar",
            "G. Merz",
            "N. Moitessier",
            "E. Moubarak",
            "B. Mouriño",
            "B. Pelkie",
            "M. Pieler",
            "M. C. Ramos",
            "Bojana Rankovi'c",
            "Samuel G. Rodriques",
            "J. N. Sanders",
            "P. Schwaller",
            "Marcus Schwarting",
            "Jia-Xin Shi",
            "B. Smit",
            "Benn Smith",
            "J. V. Heck",
            "C. Volker",
            "Logan T. Ward",
            "S. Warren",
            "B. Weiser",
            "Sylvester Zhang",
            "Xiaoqi Zhang",
            "Ghezal Ahmad Jan Zia",
            "Aristana Scourtas",
            "K. Schmidt",
            "Ian T. Foster",
            "Andrew D. White",
            "B. Blaiszik"
        ],
        "citations": 70,
        "references": 155,
        "year": 2023
    },
    {
        "title": "The Impact of Multimodal Large Language Models on Health Care’s Future",
        "abstract": "When large language models (LLMs) were introduced to the public at large in late 2022 with ChatGPT (OpenAI), the interest was unprecedented, with more than 1 billion unique users within 90 days. Until the introduction of Generative Pre-trained Transformer 4 (GPT-4) in March 2023, these LLMs only contained a single mode—text. As medicine is a multimodal discipline, the potential future versions of LLMs that can handle multimodality—meaning that they could interpret and generate not only text but also images, videos, sound, and even comprehensive documents—can be conceptualized as a significant evolution in the field of artificial intelligence (AI). This paper zooms in on the new potential of generative AI, a new form of AI that also includes tools such as LLMs, through the achievement of multimodal inputs of text, images, and speech on health care’s future. We present several futuristic scenarios to illustrate the potential path forward as multimodal LLMs (M-LLMs) could represent the gateway between health care professionals and using AI for medical purposes. It is important to point out, though, that despite the unprecedented potential of generative AI in the form of M-LLMs, the human touch in medicine remains irreplaceable. AI should be seen as a tool that can augment health care professionals rather than replace them. It is also important to consider the human aspects of health care—empathy, understanding, and the doctor-patient relationship—when deploying AI.",
        "authors": [
            "B. Meskó"
        ],
        "citations": 70,
        "references": 8,
        "year": 2023
    },
    {
        "title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
        "abstract": "There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.",
        "authors": [
            "Kaya Stechly",
            "Matthew Marquez",
            "Subbarao Kambhampati"
        ],
        "citations": 71,
        "references": 14,
        "year": 2023
    },
    {
        "title": "Towards Reasoning in Large Language Models: A Survey",
        "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",
        "authors": [
            "Jie Huang",
            "K. Chang"
        ],
        "citations": 463,
        "references": 132,
        "year": 2022
    },
    {
        "title": "The Role of Large Language Models in Medical Education: Applications and Implications",
        "abstract": "Large language models (LLMs) such as ChatGPT have sparked extensive discourse within the medical education community, spurring both excitement and apprehension. Written from the perspective of medical students, this editorial offers insights gleaned through immersive interactions with ChatGPT, contextualized by ongoing research into the imminent role of LLMs in health care. Three distinct positive use cases for ChatGPT were identified: facilitating differential diagnosis brainstorming, providing interactive practice cases, and aiding in multiple-choice question review. These use cases can effectively help students learn foundational medical knowledge during the preclinical curriculum while reinforcing the learning of core Entrustable Professional Activities. Simultaneously, we highlight key limitations of LLMs in medical education, including their insufficient ability to teach the integration of contextual and external information, comprehend sensory and nonverbal cues, cultivate rapport and interpersonal interaction, and align with overarching medical education and patient care goals. Through interacting with LLMs to augment learning during medical school, students can gain an understanding of their strengths and weaknesses. This understanding will be pivotal as we navigate a health care landscape increasingly intertwined with LLMs and artificial intelligence.",
        "authors": [
            "Conrad W Safranek",
            "A. Sidamon-Eristoff",
            "Aidan Gilson",
            "David Chartash"
        ],
        "citations": 81,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Knowledge Distillation of Large Language Models",
        "abstract": "Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.",
        "authors": [
            "Yuxian Gu",
            "Li Dong",
            "Furu Wei",
            "Minlie Huang"
        ],
        "citations": 65,
        "references": 91,
        "year": 2023
    },
    {
        "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
        "abstract": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.",
        "authors": [
            "Thuat Nguyen",
            "Chien Van Nguyen",
            "Viet Dac Lai",
            "Hieu Man",
            "Nghia Trung Ngo",
            "Franck Dernoncourt",
            "Ryan Rossi",
            "Thien Huu Nguyen"
        ],
        "citations": 67,
        "references": 53,
        "year": 2023
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities",
        "abstract": null,
        "authors": [
            "Yuqi Zhu",
            "Xiaohan Wang",
            "Jing Chen",
            "Shuofei Qiao",
            "Yixin Ou",
            "Yunzhi Yao",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "citations": 68,
        "references": 98,
        "year": 2023
    },
    {
        "title": "An Empirical Study on Information Extraction using Large Language Models",
        "abstract": "Human-like large language models (LLMs), especially the most powerful and popular ones in OpenAI's GPT family, have proven to be very helpful for many natural language processing (NLP) related tasks. Therefore, various attempts have been made to apply LLMs to information extraction (IE), which is a fundamental NLP task that involves extracting information from unstructured plain text. To demonstrate the latest representative progress in LLMs' information extraction ability, we assess the information extraction ability of GPT-4 (the latest version of GPT at the time of writing this paper) from four perspectives: Performance, Evaluation Criteria, Robustness, and Error Types. Our results suggest a visible performance gap between GPT-4 and state-of-the-art (SOTA) IE methods. To alleviate this problem, considering the LLMs' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other LLMs and NLP tasks. Rich experiments show our methods' effectiveness and some of their remaining issues in improving GPT-4's information extraction ability.",
        "authors": [
            "Ridong Han",
            "T. Peng",
            "Chaohao Yang",
            "Benyou Wang",
            "Lu Liu",
            "Xiang Wan"
        ],
        "citations": 68,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Structured Chain-of-Thought Prompting for Code Generation",
        "abstract": "\n Large Language Models (LLMs) have shown impressive abilities in code generation. Chain-of-Thought (CoT) prompting is the state-of-the-art approach to utilizing LLMs. CoT prompting asks LLMs first to generate CoTs (\n i.e.,\n intermediate natural language reasoning steps) and then output the code. However, the accuracy of CoT prompting still can not satisfy practical applications. For example, gpt-3.5-turbo with CoT prompting only achieves 53.29% Pass@1 in HumanEval.\n \n \n In this paper, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation named SCoT prompting. Our motivation is that human developers follow structured programming. Developers use three programming structures (\n i.e.,\n sequential, branch, and loop) to design and implement structured programs. Thus, we ask LLMs to use three programming structures to generate SCoTs (structured reasoning steps) before outputting the final code. Compared to CoT prompting, SCoT prompting explicitly introduces programming structures and unlocks the structured programming thinking of LLMs. We apply SCoT prompting to two LLMs (\n i.e.,\n gpt-4-turbo, gpt-3.5-turbo, and DeepSeek Coder-Instruct-{1.3B, 6.7B, 33B}) and evaluate it on three benchmarks (\n i.e.,\n HumanEval, MBPP, and MBCPP). SCoT prompting outperforms CoT prompting by up to 13.79% in Pass@1. SCoT prompting is robust to examples and achieves substantial improvements. The human evaluation also shows human developers prefer programs from SCoT prompting.\n",
        "authors": [
            "Jia Li♂",
            "Ge Li",
            "Yongming Li",
            "Zhi Jin"
        ],
        "citations": 68,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
        "abstract": "Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",
        "authors": [
            "Shashank Gupta",
            "Vaishnavi Shrivastava",
            "A. Deshpande",
            "A. Kalyan",
            "Peter Clark",
            "Ashish Sabharwal",
            "Tushar Khot"
        ],
        "citations": 66,
        "references": 38,
        "year": 2023
    },
    {
        "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities",
        "abstract": "Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs’ vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.",
        "authors": [
            "Yi Wu",
            "Nan Jiang",
            "H. Pham",
            "Thibaud Lutellier",
            "Jordan Davis",
            "Lin Tan",
            "Petr Babkin",
            "Sameena Shah"
        ],
        "citations": 65,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "abstract": "Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the “Lasswell Communication Model” proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs’ zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "authors": [
            "Yiming Wang",
            "Zhuosheng Zhang",
            "Rui Wang"
        ],
        "citations": 66,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding",
        "abstract": "This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose “Skeleton-of-Thought” (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel . Not only does SoT provide considerable speed-up (up to 2.39 × across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.",
        "authors": [
            "Xuefei Ning",
            "Zinan Lin",
            "Zixuan Zhou",
            "Huazhong Yang",
            "Yu Wang"
        ],
        "citations": 64,
        "references": 132,
        "year": 2023
    },
    {
        "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
        "abstract": "Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation. Then, we propose to replace the subject in question statements to enable flexible queries and assessments on different subjects from LLMs. Finally, we re-formulate the question instructions in a manner of correctness evaluation to facilitate LLMs to generate clearer responses. The proposed framework enables LLMs to flexibly assess personalities of different groups of people. We further propose three evaluation metrics to measure the consistency, robustness, and fairness of assessment results from state-of-the-art LLMs including ChatGPT and GPT-4. Our experiments reveal ChatGPT's ability to assess human personalities, and the average results demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT.",
        "authors": [
            "Haocong Rao",
            "Cyril Leung",
            "C. Miao"
        ],
        "citations": 67,
        "references": 95,
        "year": 2023
    },
    {
        "title": "The ethics of disclosing the use of artificial intelligence tools in writing scholarly manuscripts",
        "abstract": "In this article, we discuss ethical issues related to using and disclosing artificial intelligence (AI) tools, such as ChatGPT and other systems based on large language models (LLMs), to write or edit scholarly manuscripts. Some journals, such as Science, have banned the use of LLMs because of the ethical problems they raise concerning responsible authorship. We argue that this is not a reasonable response to the moral conundrums created by the use of LLMs because bans are unenforceable and would encourage undisclosed use of LLMs. Furthermore, LLMs can be useful in writing, reviewing and editing text, and promote equity in science. Others have argued that LLMs should be mentioned in the acknowledgments since they do not meet all the authorship criteria. We argue that naming LLMs as authors or mentioning them in the acknowledgments are both inappropriate forms of recognition because LLMs do not have free will and therefore cannot be held morally or legally responsible for what they do. Tools in general, and software in particular, are usually cited in-text, followed by being mentioned in the references. We provide suggestions to improve APA Style for referencing ChatGPT to specifically indicate the contributor who used LLMs (because interactions are stored on personal user accounts), the used version and model (because the same version could use different language models and generate dissimilar responses, e.g., ChatGPT May 12 Version GPT3.5 or GPT4), and the time of usage (because LLMs evolve fast and generate dissimilar responses over time). We recommend that researchers who use LLMs: (1) disclose their use in the introduction or methods section to transparently describe details such as used prompts and note which parts of the text are affected, (2) use in-text citations and references (to recognize their used applications and improve findability and indexing), and (3) record and submit their relevant interactions with LLMs as supplementary material or appendices.",
        "authors": [
            "Mohammad Hosseini",
            "D. Resnik",
            "Kristi L. Holmes"
        ],
        "citations": 68,
        "references": 52,
        "year": 2023
    },
    {
        "title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge",
        "abstract": "Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specific needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide an understanding of the advantages and limitations of LLMs in medicine. Overall, in this review, we address the following questions: 1) What are the practices for developing medical LLMs 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities for LLMs in medicine and serve as a practical resource. We also maintain a regularly updated list of practical guides on medical LLMs at https://github.com/AI-in-Health/MedLLMsPracticalGuide",
        "authors": [
            "Hongjian Zhou",
            "Boyang Gu",
            "Xinyu Zou",
            "Yiru Li",
            "Sam S. Chen",
            "Peilin Zhou",
            "Junling Liu",
            "Y. Hua",
            "Chengfeng Mao",
            "Xian Wu",
            "Zheng Li",
            "Fenglin Liu"
        ],
        "citations": 63,
        "references": 432,
        "year": 2023
    }
]