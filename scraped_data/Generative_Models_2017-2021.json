[
    {
        "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
        "abstract": "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.",
        "authors": [
            "Gautier Izacard",
            "Edouard Grave"
        ],
        "citations": 979,
        "references": 35,
        "year": 2020
    },
    {
        "title": "Improved Techniques for Training Score-Based Generative Models",
        "abstract": "Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories.",
        "authors": [
            "Yang Song",
            "Stefano Ermon"
        ],
        "citations": 992,
        "references": 32,
        "year": 2020
    },
    {
        "title": "Disease variant prediction with deep generative models of evolutionary data",
        "abstract": null,
        "authors": [
            "J. Frazer",
            "Pascal Notin",
            "M. Dias",
            "Aidan N. Gomez",
            "Joseph K. Min",
            "Kelly P. Brock",
            "Y. Gal",
            "D. Marks"
        ],
        "citations": 446,
        "references": 46,
        "year": 2021
    },
    {
        "title": "Solving Inverse Problems in Medical Imaging with Score-Based Generative Models",
        "abstract": "Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.",
        "authors": [
            "Yang Song",
            "Liyue Shen",
            "Lei Xing",
            "Stefano Ermon"
        ],
        "citations": 437,
        "references": 52,
        "year": 2021
    },
    {
        "title": "Skilful precipitation nowcasting using deep generative models of radar",
        "abstract": null,
        "authors": [
            "Suman V. Ravuri",
            "Karel Lenc",
            "M. Willson",
            "D. Kangin",
            "Rémi R. Lam",
            "Piotr Wojciech Mirowski",
            "Megan Fitzsimons",
            "M. Athanassiadou",
            "Sheleem Kashem",
            "Sam Madge",
            "R. Prudden",
            "Amol Mandhane",
            "Aidan Clark",
            "Andrew Brock",
            "K. Simonyan",
            "R. Hadsell",
            "Nial H. Robinson",
            "Ellen Clancy",
            "A. Arribas",
            "S. Mohamed"
        ],
        "citations": 640,
        "references": 90,
        "year": 2021
    },
    {
        "title": "Improving Language Understanding by Generative Pre-Training",
        "abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",
        "authors": [
            "Alec Radford",
            "Karthik Narasimhan"
        ],
        "citations": 1000,
        "references": 73,
        "year": 2018
    },
    {
        "title": "Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation",
        "abstract": "This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.",
        "authors": [
            "Emmanuel Bengio",
            "Moksh Jain",
            "Maksym Korablyov",
            "Doina Precup",
            "Y. Bengio"
        ],
        "citations": 262,
        "references": 50,
        "year": 2021
    },
    {
        "title": "Learning Representations and Generative Models for 3D Point Clouds",
        "abstract": "Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.",
        "authors": [
            "Panos Achlioptas",
            "Olga Diamanti",
            "Ioannis Mitliagkas",
            "L. Guibas"
        ],
        "citations": 1000,
        "references": 59,
        "year": 2017
    },
    {
        "title": "Design Guidelines for Prompt Engineering Text-to-Image Generative Models",
        "abstract": "Text-to-image generative models are a new and powerful way to generate visual artwork. However, the open-ended nature of text as interaction is double-edged; while users can input anything and have access to an infinite range of generations, they also must engage in brute-force trial and error with the text prompt when the result quality is poor. We conduct a study exploring what prompt keywords and model hyperparameters can help produce coherent outputs. In particular, we study prompts structured to include subject and style keywords and investigate success and failure modes of these prompts. Our evaluation of 5493 generations over the course of five experiments spans 51 abstract and concrete subjects as well as 51 abstract and figurative styles. From this evaluation, we present design guidelines that can help people produce better outcomes from text-to-image generative models.",
        "authors": [
            "Vivian Liu",
            "Lydia B. Chilton"
        ],
        "citations": 398,
        "references": 47,
        "year": 2021
    },
    {
        "title": "PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models",
        "abstract": "The primary aim of single-image super-resolution is to construct a high-resolution (HR) image from a corresponding low-resolution (LR) input. In previous approaches, which have generally been supervised, the training objective typically measures a pixel-wise average distance between the super-resolved (SR) and HR images. Optimizing such metrics often leads to blurring, especially in high variance (detailed) regions. We propose an alternative formulation of the super-resolution problem based on creating realistic SR images that downscale correctly. We present a novel super-resolution algorithm addressing this problem, PULSE (Photo Upsampling via Latent Space Exploration), which generates high-resolution, realistic images at resolutions previously unseen in the literature. It accomplishes this in an entirely self-supervised fashion and is not confined to a specific degradation operator used during training, unlike previous methods (which require training on databases of LR-HR image pairs for supervised learning). Instead of starting with the LR image and slowly adding detail, PULSE traverses the high-resolution natural image manifold, searching for images that downscale to the original LR image. This is formalized through the “downscaling loss,” which guides exploration through the latent space of a generative model. By leveraging properties of high-dimensional Gaussians, we restrict the search space to guarantee that our outputs are realistic. PULSE thereby generates super-resolved images that both are realistic and downscale correctly. We show extensive experimental results demonstrating the efficacy of our approach in the domain of face super-resolution (also known as face hallucination). Our method outperforms state-of-the-art methods in perceptual quality at higher resolutions and scale factors than previously possible.",
        "authors": [
            "Sachit Menon",
            "Alexandru Damian",
            "Shijia Hu",
            "Nikhil Ravi",
            "C. Rudin"
        ],
        "citations": 512,
        "references": 30,
        "year": 2020
    },
    {
        "title": "A Variational Perspective on Diffusion-Based Generative Models and Score Matching",
        "abstract": "Discrete-time diffusion-based generative models and score matching methods have shown promising results in modeling high-dimensional image data. Recently, Song et al. (2021) show that diffusion processes that transform data into noise can be reversed via learning the score function, i.e. the gradient of the log-density of the perturbed data. They propose to plug the learned score function into an inverse formula to define a generative diffusion process. Despite the empirical success, a theoretical underpinning of this procedure is still lacking. In this work, we approach the (continuous-time) generative diffusion directly and derive a variational framework for likelihood estimation, which includes continuous-time normalizing flows as a special case, and can be seen as an infinitely deep variational autoencoder. Under this framework, we show that minimizing the score-matching loss is equivalent to maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed by Song et al. (2021), bridging the theoretical gap.",
        "authors": [
            "Chin-Wei Huang",
            "Jae Hyun Lim",
            "Aaron C. Courville"
        ],
        "citations": 174,
        "references": 61,
        "year": 2021
    },
    {
        "title": "FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models",
        "abstract": "A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.",
        "authors": [
            "Will Grathwohl",
            "Ricky T. Q. Chen",
            "J. Bettencourt",
            "I. Sutskever",
            "D. Duvenaud"
        ],
        "citations": 808,
        "references": 24,
        "year": 2018
    },
    {
        "title": "Reliable Fidelity and Diversity Metrics for Generative Models",
        "abstract": "Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Frechet Inception Distance (FID) score. Because it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics. Code: this https URL.",
        "authors": [
            "Muhammad Ferjad Naeem",
            "Seong Joon Oh",
            "Youngjung Uh",
            "Yunjey Choi",
            "Jaejun Yoo"
        ],
        "citations": 338,
        "references": 25,
        "year": 2020
    },
    {
        "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
        "abstract": "In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at this https URL",
        "authors": [
            "Pouya Samangouei",
            "Maya Kabkab",
            "R. Chellappa"
        ],
        "citations": 1000,
        "references": 25,
        "year": 2018
    },
    {
        "title": "Inverse molecular design using machine learning: Generative models for matter engineering",
        "abstract": "The discovery of new materials can bring enormous societal and technological progress. In this context, exploring completely the large space of potential materials is computationally intractable. Here, we review methods for achieving inverse design, which aims to discover tailored materials from the starting point of a particular desired functionality. Recent advances from the rapidly growing field of artificial intelligence, mostly from the subfield of machine learning, have resulted in a fertile exchange of ideas, where approaches to inverse molecular design are being proposed and employed at a rapid pace. Among these, deep generative models have been applied to numerous classes of materials: rational design of prospective drugs, synthetic routes to organic compounds, and optimization of photovoltaics and redox flow batteries, as well as a variety of other solid-state materials.",
        "authors": [
            "Benjamín Sánchez-Lengeling",
            "Alán Aspuru-Guzik"
        ],
        "citations": 1000,
        "references": 98,
        "year": 2018
    },
    {
        "title": "Protein design and variant prediction using autoregressive generative models",
        "abstract": null,
        "authors": [
            "Jung-Eun Shin",
            "Adam J. Riesselman",
            "Aaron W. Kollasch",
            "Conor McMahon",
            "Elana Simon",
            "C. Sander",
            "A. Manglik",
            "A. Kruse",
            "D. Marks"
        ],
        "citations": 277,
        "references": 110,
        "year": 2021
    },
    {
        "title": "Towards Accurate Generative Models of Video: A New Metric & Challenges",
        "abstract": "Recent advances in deep generative models have lead to remarkable progress in synthesizing high quality images. Following their successful application in image processing and representation learning, an important next step is to consider videos. Learning generative models of video is a much harder task, requiring a model to capture the temporal dynamics of a scene, in addition to the visual presentation of objects. While recent attempts at formulating generative models of video have had some success, current progress is hampered by (1) the lack of qualitative metrics that consider visual quality, temporal coherence, and diversity of samples, and (2) the wide gap between purely synthetic video data sets and challenging real-world data sets in terms of complexity. To this extent we propose Fr\\'{e}chet Video Distance (FVD), a new metric for generative models of video, and StarCraft 2 Videos (SCV), a benchmark of game play from custom starcraft 2 scenarios that challenge the current capabilities of generative models of video. We contribute a large-scale human study, which confirms that FVD correlates well with qualitative human judgment of generated videos, and provide initial benchmark results on SCV.",
        "authors": [
            "Thomas Unterthiner",
            "Sjoerd van Steenkiste",
            "Karol Kurach",
            "Raphaël Marinier",
            "Marcin Michalski",
            "S. Gelly"
        ],
        "citations": 533,
        "references": 53,
        "year": 2018
    },
    {
        "title": "How Faithful is your Synthetic Data? Sample-level Metrics for Evaluating and Auditing Generative Models",
        "abstract": "Devising domain- and model-agnostic evaluation metrics for generative models is an important and as yet unresolved problem. Most existing metrics, which were tailored solely to the image synthesis setup, exhibit a limited capacity for diagnosing the different modes of failure of generative models across broader application domains. In this paper, we introduce a 3-dimensional evaluation metric, ($\\alpha$-Precision, $\\beta$-Recall, Authenticity), that characterizes the fidelity, diversity and generalization performance of any generative model in a domain-agnostic fashion. Our metric unifies statistical divergence measures with precision-recall analysis, enabling sample- and distribution-level diagnoses of model fidelity and diversity. We introduce generalization as an additional, independent dimension (to the fidelity-diversity trade-off) that quantifies the extent to which a model copies training data -- a crucial performance indicator when modeling sensitive data with requirements on privacy. The three metric components correspond to (interpretable) probabilistic quantities, and are estimated via sample-level binary classification. The sample-level nature of our metric inspires a novel use case which we call model auditing, wherein we judge the quality of individual samples generated by a (black-box) model, discarding low-quality samples and hence improving the overall model performance in a post-hoc manner.",
        "authors": [
            "A. Alaa",
            "B. V. Breugel",
            "Evgeny S. Saveliev",
            "M. Schaar"
        ],
        "citations": 160,
        "references": 51,
        "year": 2021
    },
    {
        "title": "Generative Models for Graph-Based Protein Design",
        "abstract": "Engineered proteins offer the potential to solve many problems in biomedicine, energy, and materials science, but creating designs that succeed is difficult in practice. A significant aspect of this challenge is the complex coupling between protein sequence and 3D structure, with the task of finding a viable design often referred to as the inverse protein folding problem. We develop relational language models for protein sequences that directly condition on a graph specification of the target structure. Our approach efficiently captures the complex dependencies in proteins by focusing on those that are long-range in sequence but local in 3D space. Our framework significantly improves in both speed and robustness over conventional and deep-learning-based methods for structure-based protein sequence design, and takes a step toward rapid and targeted biomolecular design with the aid of deep generative models.",
        "authors": [
            "John Ingraham",
            "Vikas K. Garg",
            "R. Barzilay",
            "T. Jaakkola"
        ],
        "citations": 456,
        "references": 51,
        "year": 2019
    },
    {
        "title": "Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed",
        "abstract": "Iterative generative models, such as noise conditional score networks and denoising diffusion probabilistic models, produce high quality samples by gradually denoising an initial noise vector. However, their denoising process has many steps, making them 2-3 orders of magnitude slower than other generative models such as GANs and VAEs. In this paper, we establish a novel connection between knowledge distillation and image generation with a technique that distills a multi-step denoising process into a single step, resulting in a sampling speed similar to other single-step generative models. Our Denoising Student generates high quality samples comparable to GANs on the CIFAR-10 and CelebA datasets, without adversarial training. We demonstrate that our method scales to higher resolutions through experiments on 256 x 256 LSUN. Code and checkpoints are available at https://github.com/tcl9876/Denoising_Student",
        "authors": [
            "Eric Luhman",
            "Troy Luhman"
        ],
        "citations": 221,
        "references": 47,
        "year": 2021
    },
    {
        "title": "Do Deep Generative Models Know What They Don't Know?",
        "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",
        "authors": [
            "Eric T. Nalisnick",
            "Akihiro Matsukawa",
            "Y. Teh",
            "Dilan Görür",
            "Balaji Lakshminarayanan"
        ],
        "citations": 723,
        "references": 39,
        "year": 2018
    },
    {
        "title": "Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design",
        "abstract": "Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to state-of-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models. Our implementation is available at this https URL",
        "authors": [
            "Jonathan Ho",
            "Xi Chen",
            "A. Srinivas",
            "Yan Duan",
            "P. Abbeel"
        ],
        "citations": 432,
        "references": 40,
        "year": 2019
    },
    {
        "title": "Improved Precision and Recall Metric for Assessing Generative Models",
        "abstract": "The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations.",
        "authors": [
            "T. Kynkäänniemi",
            "Tero Karras",
            "S. Laine",
            "J. Lehtinen",
            "Timo Aila"
        ],
        "citations": 712,
        "references": 38,
        "year": 2019
    },
    {
        "title": "Deep Generative Models in Engineering Design: A Review",
        "abstract": "\n Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative Machine Learning to design engineering may enable such automated design synthesis and is a research subject of great importance. We present a review and analysis of Deep Generative Learning models in engineering design. Deep Generative Models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), feedforward Neural Networks (NNs) and certain Deep Reinforcement Learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis. The prevalence of DGMs in Engineering Design has skyrocketed since 2016. Anticipating continued growth, we conduct a review of recent advances with the hope of benefitting researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature. In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported development of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously. In our discussion we identify possible solution pathways as key areas on which to target future work.",
        "authors": [
            "Lyle Regenwetter",
            "A. Nobari",
            "Faez Ahmed"
        ],
        "citations": 157,
        "references": 191,
        "year": 2021
    },
    {
        "title": "Learning Deep Generative Models of Graphs",
        "abstract": "Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learning generative models over arbitrary graphs, and opens new directions for moving away from restrictions of vector- and sequence-like knowledge representations, toward more expressive and flexible relational data structures.",
        "authors": [
            "Yujia Li",
            "O. Vinyals",
            "Chris Dyer",
            "Razvan Pascanu",
            "P. Battaglia"
        ],
        "citations": 633,
        "references": 43,
        "year": 2018
    },
    {
        "title": "Compressed Sensing using Generative Models",
        "abstract": "The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model G : ℝk → ℝn. Our main theorem is that, if G is L-Lipschitz, then roughly O(k log L) random Gaussian measurements suffice for an l2/l2 recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use 5-10x fewer measurements than Lasso for the same accuracy.",
        "authors": [
            "Ashish Bora",
            "A. Jalal",
            "Eric Price",
            "A. Dimakis"
        ],
        "citations": 773,
        "references": 45,
        "year": 2017
    },
    {
        "title": "Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations",
        "abstract": null,
        "authors": [
            "Payel Das",
            "Tom Sercu",
            "Kahini Wadhawan",
            "Inkit Padhi",
            "Sebastian Gehrmann",
            "F. Cipcigan",
            "Vijil Chenthamarakshan",
            "H. Strobelt",
            "Cicero dos Santos",
            "Pin-Yu Chen",
            "Yi Yan Yang",
            "Jeremy P. K. Tan",
            "J. Hedrick",
            "J. Crain",
            "A. Mojsilovic"
        ],
        "citations": 242,
        "references": 86,
        "year": 2021
    },
    {
        "title": "Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?",
        "abstract": "While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to 7.5% and 6.7% in $\\ell_{\\infty}$ and $\\ell_2$ threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by 7.6% on the CIFAR-10 dataset. We further demonstrate that different generative models bring a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact of individual generative models and further provide a deeper understanding of why current state-of-the-art in diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.",
        "authors": [
            "Vikash Sehwag",
            "Saeed Mahloujifar",
            "Tinashe Handina",
            "Sihui Dai",
            "Chong Xiang",
            "M. Chiang",
            "Prateek Mittal"
        ],
        "citations": 118,
        "references": 80,
        "year": 2021
    },
    {
        "title": "Generative Models as a Data Source for Multiview Representation Learning",
        "abstract": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple\"views\"of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more\"model zoos\"proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.",
        "authors": [
            "Ali Jahanian",
            "Xavier Puig",
            "Yonglong Tian",
            "Phillip Isola"
        ],
        "citations": 118,
        "references": 90,
        "year": 2021
    },
    {
        "title": "Generative Models as Distributions of Functions",
        "abstract": "Generative models are typically trained on grid-like data such as images. As a result, the size of these models usually scales directly with the underlying grid resolution. In this paper, we abandon discretized grids and instead parameterize individual data points by continuous functions. We then build generative models by learning distributions over such functions. By treating data points as functions, we can abstract away from the specific type of data we train on and construct models that are agnostic to discretization. To train our model, we use an adversarial approach with a discriminator that acts on continuous signals. Through experiments on a wide variety of data modalities including images, 3D shapes and climate data, we demonstrate that our model can learn rich distributions of functions independently of data type and resolution.",
        "authors": [
            "Emilien Dupont",
            "Y. Teh",
            "A. Doucet"
        ],
        "citations": 93,
        "references": 83,
        "year": 2021
    },
    {
        "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
        "abstract": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
        "authors": [
            "Dingfan Chen",
            "Ning Yu",
            "Yang Zhang",
            "Mario Fritz"
        ],
        "citations": 373,
        "references": 101,
        "year": 2019
    },
    {
        "title": "Assessing Generative Models via Precision and Recall",
        "abstract": "Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.",
        "authors": [
            "Mehdi S. M. Sajjadi",
            "Olivier Bachem",
            "Mario Lucic",
            "O. Bousquet",
            "S. Gelly"
        ],
        "citations": 532,
        "references": 25,
        "year": 2018
    },
    {
        "title": "Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization",
        "abstract": "Training deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semi-supervised learning, which exploits more widely available unlabeled data to complement small labeled data sets. In this paper, we propose a novel framework for discriminative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative adversarial network that captures the joint image-label distribution and is trained efficiently using a large set of un-labeled images supplemented with only few labeled ones. We build our architecture on top of StyleGAN2 [45], augmented with a label synthesis branch. Image labeling at test time is achieved by first embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the inferred embedding. We evaluate our approach in two important domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain performance compared to several baselines, and are the first to showcase extreme out-of-domain generalization, such as transferring from CT to MRI in medical imaging, and photographs of real faces to paintings, sculptures, and even cartoons and animal faces. Project Page: https://nv-tlabs.github.io/semanticGAN/",
        "authors": [
            "Daiqing Li",
            "Junlin Yang",
            "Karsten Kreis",
            "A. Torralba",
            "S. Fidler"
        ],
        "citations": 164,
        "references": 112,
        "year": 2021
    },
    {
        "title": "Improving Transferability of Adversarial Patches on Face Recognition with Generative Models",
        "abstract": "Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well.",
        "authors": [
            "Zihao Xiao",
            "Xianfeng Gao",
            "Chilin Fu",
            "Yinpeng Dong",
            "Wei-zhe Gao",
            "Xiaolu Zhang",
            "Jun Zhou",
            "Jun Zhu"
        ],
        "citations": 100,
        "references": 30,
        "year": 2021
    },
    {
        "title": "The Synthesizability of Molecules Proposed by Generative Models",
        "abstract": "The discovery of functional molecules is an expensive and time-consuming process, exemplified by the rising costs of small molecule therapeutic discovery. One class of techniques of growing interest for early-stage drug discovery is de novo molecular generation and optimization, catalyzed by the development of new deep learning approaches. These techniques can suggest novel molecular structures intended to maximize a multi-objective function, e.g., suitability as a therapeutic against a particular target, without relying on brute-force exploration of a chemical space. However, the utility of these approaches is stymied by ignorance of synthesizability. To highlight the severity of this issue, we use a data-driven computer-aided synthesis planning program to quantify how often molecules proposed by state-of-the-art generative models cannot be readily synthesized. Our analysis demonstrates that there are several tasks for which these models generate unrealistic molecular structures despite performing well on popular quantitative benchmarks. Synthetic complexity heuristics can successfully bias generation toward synthetically-tractable chemical space, although doing so necessarily detracts from the primary objective. This analysis suggests that to improve the utility of these models in real discovery workflows, new algorithm development is warranted.",
        "authors": [
            "Wenhao Gao",
            "Connor W. Coley"
        ],
        "citations": 231,
        "references": 51,
        "year": 2020
    },
    {
        "title": "Generative Models for De Novo Drug Design.",
        "abstract": "Artificial intelligence (AI) is booming. Among various AI approaches, generative models have received much attention in recent years. Inspired by these successes, researchers are now applying generative model techniques to de novo drug design, which has been considered as the \"holy grail\" of drug discovery. In this Perspective, we first focus on describing models such as recurrent neural network, autoencoder, generative adversarial network, transformer, and hybrid models with reinforcement learning. Next, we summarize the applications of generative models to drug design, including generating various compounds to expand the compound library and designing compounds with specific properties, and we also list a few publicly available molecular design tools based on generative models which can be used directly to generate molecules. In addition, we also introduce current benchmarks and metrics frequently used for generative models. Finally, we discuss the challenges and prospects of using generative models to aid drug design.",
        "authors": [
            "X. Tong",
            "Xiaohong Liu",
            "Xiaoqin Tan",
            "Xutong Li",
            "Jiaxin Jiang",
            "Zhaoping Xiong",
            "Tingyang Xu",
            "Hualiang Jiang",
            "Nan Qiao",
            "Mingyue Zheng"
        ],
        "citations": 115,
        "references": 90,
        "year": 2021
    },
    {
        "title": "Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data",
        "abstract": "Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake detection, that is agnostic to the evolution of generative models, by introducing artificial fingerprints into the models.Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a surprising discovery on the transferability of such fingerprints from training data to generative models, which in turn appears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and attribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model inventions and their possible misuses, which makes it independent of the current arms race.",
        "authors": [
            "Ning Yu",
            "Vladislav Skripniuk",
            "Sahar Abdelnabi",
            "Mario Fritz"
        ],
        "citations": 181,
        "references": 62,
        "year": 2020
    },
    {
        "title": "Understanding Failures in Out-of-Distribution Detection with Deep Generative Models",
        "abstract": "Deep generative models (dgms) seem a natural fit for detecting out-of-distribution (ood) inputs, but such models have been shown to assign higher probabilities or densities to ood images than images from the training distribution. In this work, we explain why this behavior should be attributed to model misestimation. We first prove that no method can guarantee performance beyond random chance without assumptions on which out-distributions are relevant. We then interrogate the typical set hypothesis, the claim that relevant out-distributions can lie in high likelihood regions of the data distribution, and that ood detection should be defined based on the data distribution's typical set. We highlight the consequences implied by assuming support overlap between in- and out-distributions, as well as the arbitrariness of the typical set for ood detection. Our results suggest that estimation error is a more plausible explanation than the misalignment between likelihood-based ood detection and out-distributions of interest, and we illustrate how even minimal estimation error can lead to ood detection failures, yielding implications for future work in deep generative modeling and ood detection.",
        "authors": [
            "Lily H. Zhang",
            "Mark Goldstein",
            "R. Ranganath"
        ],
        "citations": 91,
        "references": 31,
        "year": 2021
    },
    {
        "title": "Structure-based de novo drug design using 3D deep generative models",
        "abstract": "Deep generative models are attracting much attention in the field of de novo molecule design. Compared to traditional methods, deep generative models can be trained in a fully data-driven way with little requirement for expert knowledge. Although many models have been developed to generate 1D and 2D molecular structures, 3D molecule generation is less explored, and the direct design of drug-like molecules inside target binding sites remains challenging. In this work, we introduce DeepLigBuilder, a novel deep learning-based method for de novo drug design that generates 3D molecular structures in the binding sites of target proteins. We first developed Ligand Neural Network (L-Net), a novel graph generative model for the end-to-end design of chemically and conformationally valid 3D molecules with high drug-likeness. Then, we combined L-Net with Monte Carlo tree search to perform structure-based de novo drug design tasks. In the case study of inhibitor design for the main protease of SARS-CoV-2, DeepLigBuilder suggested a list of drug-like compounds with novel chemical structures, high predicted affinity, and similar binding features to those of known inhibitors. The current version of L-Net was trained on drug-like compounds from ChEMBL, which could be easily extended to other molecular datasets with desired properties based on users' demands and applied in functional molecule generation. Merging deep generative models with atomic-level interaction evaluation, DeepLigBuilder provides a state-of-the-art model for structure-based de novo drug design and lead optimization.",
        "authors": [
            "Yibo Li",
            "Jianfeng Pei",
            "L. Lai"
        ],
        "citations": 100,
        "references": 77,
        "year": 2021
    },
    {
        "title": "Intermediate Layer Optimization for Inverse Problems using Deep Generative Models",
        "abstract": "We propose Intermediate Layer Optimization (ILO), a novel optimization algorithm for solving inverse problems with deep generative models. Instead of optimizing only over the initial latent code, we progressively change the input layer obtaining successively more expressive generators. To explore the higher dimensional spaces, our method searches for latent codes that lie within a small $l_1$ ball around the manifold induced by the previous layer. Our theoretical analysis shows that by keeping the radius of the ball relatively small, we can improve the established error bound for compressed sensing with deep generative models. We empirically show that our approach outperforms state-of-the-art methods introduced in StyleGAN-2 and PULSE for a wide range of inverse problems including inpainting, denoising, super-resolution and compressed sensing.",
        "authors": [
            "Giannis Daras",
            "Joseph Dean",
            "A. Jalal",
            "A. Dimakis"
        ],
        "citations": 78,
        "references": 74,
        "year": 2021
    },
    {
        "title": "Estimation of Thermodynamic Observables in Lattice Field Theories with Deep Generative Models.",
        "abstract": "In this Letter, we demonstrate that applying deep generative machine learning models for lattice field theory is a promising route for solving problems where Markov chain Monte Carlo (MCMC) methods are problematic. More specifically, we show that generative models can be used to estimate the absolute value of the free energy, which is in contrast to existing MCMC-based methods, which are limited to only estimate free energy differences. We demonstrate the effectiveness of the proposed method for two-dimensional ϕ^{4} theory and compare it to MCMC-based methods in detailed numerical experiments.",
        "authors": [
            "K. Nicoli",
            "Christopher J. Anders",
            "L. Funcke",
            "T. Hartung",
            "K. Jansen",
            "P. Kessel",
            "Shinichi Nakajima",
            "Paolo Stornati"
        ],
        "citations": 82,
        "references": 52,
        "year": 2021
    },
    {
        "title": "Molecular design in drug discovery: a comprehensive review of deep generative models",
        "abstract": "Deep generative models have been an upsurge in the deep learning community since they were proposed. These models are designed for generating new synthetic data including images, videos and texts by fitting the data approximate distributions. In the last few years, deep generative models have shown superior performance in drug discovery especially de novo molecular design. In this study, deep generative models are reviewed to witness the recent advances of de novo molecular design for drug discovery. In addition, we divide those models into two categories based on molecular representations in silico. Then these two classical types of models are reported in detail and discussed about both pros and cons. We also indicate the current challenges in deep generative models for de novo molecular design. De novo molecular design automatically is promising but a long road to be explored.",
        "authors": [
            "Yu Cheng",
            "Yongshun Gong",
            "Yuansheng Liu",
            "Bosheng Song",
            "Q. Zou"
        ],
        "citations": 88,
        "references": 95,
        "year": 2021
    },
    {
        "title": "Don't Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence",
        "abstract": "Although machine learning models trained on massive data have led to break-throughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efficient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for control-ling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images. Project page:https://nv-tlabs.github.io/DP-Sinkhorn.",
        "authors": [
            "Tianshi Cao",
            "A. Bie",
            "Arash Vahdat",
            "S. Fidler",
            "Karsten Kreis"
        ],
        "citations": 63,
        "references": 58,
        "year": 2021
    },
    {
        "title": "Reverse Engineering of Generative Models: Inferring Model Hyperparameters From Generated Images",
        "abstract": "State-of-the-art (SOTA) Generative Models (GMs) can synthesize photo-realistic images that are hard for humans to distinguish from genuine photos. Identifying and understanding manipulated media are crucial to mitigate the social concerns on the potential misuse of GMs. We propose to perform reverse engineering of GMs to infer model hyperparameters from the images generated by these models. We define a novel problem, “model parsing”, as estimating GM network architectures and training loss functions by examining their generated images – a task seemingly impossible for human beings. To tackle this problem, we propose a framework with two components: a Fingerprint Estimation Network (FEN), which estimates a GM fingerprint from a generated image by training with four constraints to encourage the fingerprint to have desired properties, and a Parsing Network (PN), which predicts network architecture and loss functions from the estimated fingerprints. To evaluate our approach, we collect a fake image dataset with 100 K images generated by 116 different GMs. Extensive experiments show encouraging results in parsing the hyperparameters of the unseen models. Finally, our fingerprint estimation can be leveraged for deepfake detection and image attribution, as we show by reporting SOTA results on both the deepfake detection (Celeb-DF) and image attribution benchmarks.",
        "authors": [
            "Vishal Asnani",
            "Xi Yin",
            "Tal Hassner",
            "Xiaoming Liu"
        ],
        "citations": 60,
        "references": 159,
        "year": 2021
    },
    {
        "title": "Generating 3D molecules conditional on receptor binding sites with deep generative models",
        "abstract": "The goal of structure-based drug discovery is to find small molecules that bind to a given target protein. Deep learning has been used to generate drug-like molecules with certain cheminformatic properties, but has not yet been applied to generating 3D molecules predicted to bind to proteins by sampling the conditional distribution of protein–ligand binding interactions. In this work, we describe for the first time a deep learning system for generating 3D molecular structures conditioned on a receptor binding site. We approach the problem using a conditional variational autoencoder trained on an atomic density grid representation of cross-docked protein–ligand structures. We apply atom fitting and bond inference procedures to construct valid molecular conformations from generated atomic densities. We evaluate the properties of the generated molecules and demonstrate that they change significantly when conditioned on mutated receptors. We also explore the latent space learned by our generative model using sampling and interpolation techniques. This work opens the door for end-to-end prediction of stable bioactive molecules from protein structures with deep learning.",
        "authors": [
            "Matthew Ragoza",
            "Tomohide Masuda",
            "D. Koes"
        ],
        "citations": 93,
        "references": 8,
        "year": 2021
    },
    {
        "title": "MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning",
        "abstract": "Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. Building on Frozen, we train a series of VL models that autoregressively generate text from arbitrary combinations of visual and textual input. The pretraining is entirely end-to-end using a single language modeling objective, simplifying optimization compared to previous approaches. Importantly, the language model weights remain unchanged during training, allowing for transfer of encyclopedic knowledge and in-context learning abilities from language pretraining. MAGMA outperforms Frozen on open-ended generative tasks, achieving state of the art results on the OKVQA benchmark and competitive results on a range of other popular VL benchmarks, while pretraining on 0.2% of the number of samples used to train SimVLM.",
        "authors": [
            "C. Eichenberg",
            "Sid Black",
            "Samuel Weinbach",
            "Letitia Parcalabescu",
            "A. Frank"
        ],
        "citations": 97,
        "references": 63,
        "year": 2021
    },
    {
        "title": "Enhancing Generative Models via Quantum Correlations",
        "abstract": "Generative modeling using samples drawn from the probability distribution constitutes a powerful approach for unsupervised machine learning. Quantum mechanical systems can produce probability distributions that exhibit quantum correlations which are diﬃcult to capture using classical models. We show theoretically that such quantum correlations provide a powerful resource for generative modeling. In particular, we provide an unconditional proof of separation in expressive power between a class of widely-used generative models, known as Bayesian networks, and its minimal quantum extension. We show that this expressivity advantage is associated with quantum nonlocality and quantum contextuality. Furthermore, we numerically test this separation on standard machine learning data sets and show that it holds for practical problems. The possibility of quantum advantage demonstrated in this work not only sheds light on the design of useful quantum machine learning protocols but also provides inspiration to draw on ideas from quantum foundations to improve purely classical algorithms.",
        "authors": [
            "Xun Gao",
            "Eric R. Anschuetz",
            "Sheng-Tao Wang",
            "J. Cirac",
            "M. Lukin"
        ],
        "citations": 67,
        "references": 88,
        "year": 2021
    },
    {
        "title": "A Systematic Survey on Deep Generative Models for Graph Generation",
        "abstract": "Graphs are important data representations for describing objects and their relationships, which appear in a wide diversity of real-world scenarios. As one of a critical problem in this area, graph generation considers learning the distributions of given graphs and generating more novel graphs. Owing to their wide range of applications, generative models for graphs, which have a rich history, however, are traditionally hand-crafted and only capable of modeling a few statistical properties of graphs. Recent advances in deep generative models for graph generation is an important step towards improving the fidelity of generated graphs and paves the way for new kinds of applications. This article provides an extensive overview of the literature in the field of deep generative models for graph generation. First, the formal definition of deep generative models for the graph generation and the preliminary knowledge are provided. Second, taxonomies of deep generative models for both unconditional and conditional graph generation are proposed respectively; the existing works of each are compared and analyzed. After that, an overview of the evaluation metrics in this specific domain is provided. Finally, the applications that deep graph generation enables are summarized and five promising future research directions are highlighted.",
        "authors": [
            "Xiaojie Guo",
            "Liang Zhao"
        ],
        "citations": 136,
        "references": 168,
        "year": 2020
    },
    {
        "title": "Controlling generative models with continuous factors of variations",
        "abstract": "Recent deep generative models are able to provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent work has shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like the position or scale of the object in the image. Our method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations of the generated image, such as translation, zoom or color variations. We demonstrate the effectiveness of our method qualitatively and quantitatively, both for GANs and variational auto-encoders.",
        "authors": [
            "Antoine Plumerault",
            "H. Borgne",
            "C. Hudelot"
        ],
        "citations": 124,
        "references": 34,
        "year": 2020
    },
    {
        "title": "Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models",
        "abstract": "Learning generative models that span multiple data modalities, such as vision and language, is often motivated by the desire to learn more useful, generalisable representations that faithfully capture common underlying factors between the modalities. In this work, we characterise successful learning of such models as the fulfilment of four criteria: i) implicit latent decomposition into shared and private subspaces, ii) coherent joint generation over all modalities, iii) coherent cross-generation across individual modalities, and iv) improved model learning for individual modalities through multi-modal integration. Here, we propose a mixture-of-experts multi-modal variational autoencoder (MMVAE) for learning of generative models on different sets of modalities, including a challenging image language dataset, and demonstrate its ability to satisfy all four criteria, both qualitatively and quantitatively.",
        "authors": [
            "Yuge Shi",
            "Siddharth Narayanaswamy",
            "Brooks Paige",
            "Philip H. S. Torr"
        ],
        "citations": 248,
        "references": 59,
        "year": 2019
    },
    {
        "title": "De Novo Drug Design Using Reinforcement Learning with Graph-Based Deep Generative Models",
        "abstract": "Machine learning provides effective computational tools for exploring the chemical space via deep generative models. Here, we propose a new reinforcement learning scheme to fine-tune graph-based deep generative models for de novo molecular design tasks. We show how our computational framework can successfully guide a pretrained generative model toward the generation of molecules with a specific property profile, even when such molecules are not present in the training set and unlikely to be generated by the pretrained model. We explored the following tasks: generating molecules of decreasing/increasing size, increasing drug-likeness, and increasing bioactivity. Using the proposed approach, we achieve a model which generates diverse compounds with predicted DRD2 activity for 95% of sampled molecules, outperforming previously reported methods on this metric.",
        "authors": [
            "S. R. Atance",
            "Juan Viguera Diez",
            "O. Engkvist",
            "S. Olsson",
            "Rocío Mercado"
        ],
        "citations": 61,
        "references": 48,
        "year": 2021
    },
    {
        "title": "Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES",
        "abstract": "Inverse design allows the generation of molecules with desirable physical quantities using property optimization. Deep generative models have recently been applied to tackle inverse design, as they possess the ability to optimize molecular properties directly through structure modification using gradients. While the ability to carry out direct property optimizations is promising, the use of generative deep learning models to solve practical problems requires large amounts of data and is very time-consuming. In this work, we propose STONED – a simple and efficient algorithm to perform interpolation and exploration in the chemical space, comparable to deep generative models. STONED bypasses the need for large amounts of data and training times by using string modifications in the SELFIES molecular representation. First, we achieve non-trivial performance on typical benchmarks for generative models without any training. Additionally, we demonstrate applications in high-throughput virtual screening for the design of drugs, photovoltaics, and the construction of chemical paths, allowing for both property and structure-based interpolation in the chemical space. Overall, we anticipate our results to be a stepping stone for developing more sophisticated inverse design models and benchmarking tools, ultimately helping generative models achieve wider adoption.",
        "authors": [
            "AkshatKumar Nigam",
            "R. Pollice",
            "Mario Krenn",
            "Gabriel dos Passos Gomes",
            "Alán Aspuru-Guzik"
        ],
        "citations": 105,
        "references": 59,
        "year": 2020
    },
    {
        "title": "imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose",
        "abstract": "We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.",
        "authors": [
            "Thiemo Alldieck",
            "Hongyi Xu",
            "C. Sminchisescu"
        ],
        "citations": 106,
        "references": 56,
        "year": 2021
    },
    {
        "title": "A comprehensive survey and analysis of generative models in machine learning",
        "abstract": null,
        "authors": [
            "GM Harshvardhan",
            "M. Gourisaria",
            "M. Pandey",
            "S. Rautaray"
        ],
        "citations": 299,
        "references": 89,
        "year": 2020
    },
    {
        "title": "Data augmentation for enhancing EEG-based emotion recognition with deep generative models",
        "abstract": "Objective. The data scarcity problem in emotion recognition from electroencephalography (EEG) leads to difficulty in building an affective model with high accuracy using machine learning algorithms or deep neural networks. Inspired by emerging deep generative models, we propose three methods for augmenting EEG training data to enhance the performance of emotion recognition models. Approach. Our proposed methods are based on two deep generative models, variational autoencoder (VAE) and generative adversarial network (GAN), and two data augmentation ways, full and partial usage strategies. For the full usage strategy, all of the generated data are augmented to the training dataset without judging the quality of the generated data, while for the partial usage, only high-quality data are selected and appended to the training dataset. These three methods are called conditional Wasserstein GAN (cWGAN), selective VAE (sVAE), and selective WGAN (sWGAN). Main results. To evaluate the effectiveness of these proposed methods, we perform a systematic experimental study on two public EEG datasets for emotion recognition, namely, SEED and DEAP. We first generate realistic-like EEG training data in two forms: power spectral density and differential entropy. Then, we augment the original training datasets with a different number of generated realistic-like EEG data. Finally, we train support vector machines and deep neural networks with shortcut layers to build affective models using the original and augmented training datasets. The experimental results demonstrate that our proposed data augmentation methods based on generative models outperform the existing data augmentation approaches such as conditional VAE, Gaussian noise, and rotational data augmentation. We also observe that the number of generated data should be less than 10 times of the original training dataset to achieve the best performance. Significance. The augmented training datasets produced by our proposed sWGAN method significantly enhance the performance of EEG-based emotion recognition models.",
        "authors": [
            "Yun Luo",
            "Li-Zhen Zhu",
            "Zi-Yu Wan",
            "Bao-Liang Lu"
        ],
        "citations": 110,
        "references": 75,
        "year": 2020
    },
    {
        "title": "Multimodal Generative Models for Scalable Weakly-Supervised Learning",
        "abstract": "Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations. Previous work have proposed generative models to handle multi-modal input. However, these models either do not learn a joint distribution or require complex additional computations to handle missing data. Here, we introduce a multimodal variational autoencoder that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities, thereby enabling weakly-supervised learning. We apply our method on four datasets and show that we match state-of-the-art performance using many fewer parameters. In each case our approach yields strong weakly-supervised results. We then consider a case study of learning image transformations---edge detection, colorization, facial landmark segmentation, etc.---as a set of modalities. We find appealing results across this range of tasks.",
        "authors": [
            "Mike Wu",
            "Noah D. Goodman"
        ],
        "citations": 349,
        "references": 40,
        "year": 2018
    },
    {
        "title": "Responsible Disclosure of Generative Models Using Scalable Fingerprinting",
        "abstract": "Over the past five years, deep generative models have achieved a qualitative new level of performance. Generated data has become difficult, if not impossible, to be distinguished from real data. While there are plenty of use cases that benefit from this technology, there are also strong concerns on how this new technology can be misused to spoof sensors, generate deep fakes, and enable misinformation at scale. Unfortunately, current deep fake detection methods are not sustainable, as the gap between real and fake continues to close. In contrast, our work enables a responsible disclosure of such state-of-the-art generative models, that allows researchers and companies to fingerprint their models, so that the generated samples containing a fingerprint can be accurately detected and attributed to a source. Our technique achieves this by an efficient and scalable ad-hoc generation of a large population of models with distinct fingerprints. Our recommended operation point uses a 128-bit fingerprint which in principle results in more than $10^{36}$ identifiable models. Experimental results show that our method fulfills key properties of a fingerprinting mechanism and achieves effectiveness in deep fake detection and attribution.",
        "authors": [
            "Ning Yu",
            "Vladislav Skripniuk",
            "Dingfan Chen",
            "Larry S. Davis",
            "Mario Fritz"
        ],
        "citations": 81,
        "references": 93,
        "year": 2020
    },
    {
        "title": "Adversarial purification with Score-based generative models",
        "abstract": "While adversarial training is considered as a standard defense method against adversarial attacks for image classifiers, adversarial purification, which purifies attacked images into clean images with a standalone purification model, has shown promises as an alternative defense method. Recently, an Energy-Based Model (EBM) trained with Markov-Chain Monte-Carlo (MCMC) has been highlighted as a purification model, where an attacked image is purified by running a long Markov-chain using the gradients of the EBM. Yet, the practicality of the adversarial purification using an EBM remains questionable because the number of MCMC steps required for such purification is too large. In this paper, we propose a novel adversarial purification method based on an EBM trained with Denoising Score-Matching (DSM). We show that an EBM trained with DSM can quickly purify attacked images within a few steps. We further introduce a simple yet effective randomized purification scheme that injects random noises into images before purification. This process screens the adversarial perturbations imposed on images by the random noises and brings the images to the regime where the EBM can denoise well. We show that our purification method is robust against various attacks and demonstrate its state-of-the-art performances.",
        "authors": [
            "Jongmin Yoon",
            "S. Hwang",
            "Juho Lee"
        ],
        "citations": 126,
        "references": 50,
        "year": 2021
    },
    {
        "title": "On the Frequency Bias of Generative Models",
        "abstract": "The key objective of Generative Adversarial Networks (GANs) is to generate new data with the same statistics as the provided training data. However, multiple recent works show that state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an elevated amount of high frequencies in the spectral statistics which makes it straightforward to distinguish real and generated images. Explanations for this phenomenon are controversial: While most works attribute the artifacts to the generator, other works point to the discriminator. We take a sober look at those explanations and provide insights on what makes proposed measures against high-frequency artifacts effective. To achieve this, we first independently assess the architectures of both the generator and discriminator and investigate if they exhibit a frequency bias that makes learning the distribution of high-frequency content particularly problematic. Based on these experiments, we make the following four observations: 1) Different upsampling operations bias the generator towards different spectral properties. 2) Checkerboard artifacts introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able to compensate for these artifacts. 3) The discriminator does not struggle with detecting high frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling operations in the discriminator can impair the quality of the training signal it provides. In light of these findings, we analyze proposed measures against high-frequency artifacts in state-of-the-art GAN training but find that none of the existing approaches can fully resolve spectral artifacts yet. Our results suggest that there is great potential in improving the discriminator and that this could be key to match the distribution of the training data more closely.",
        "authors": [
            "Katja Schwarz",
            "Yiyi Liao",
            "Andreas Geiger"
        ],
        "citations": 68,
        "references": 43,
        "year": 2021
    },
    {
        "title": "De novo molecular design and generative models.",
        "abstract": null,
        "authors": [
            "Joshua Meyers",
            "Benedek Fabian",
            "Nathan Brown"
        ],
        "citations": 149,
        "references": 67,
        "year": 2021
    },
    {
        "title": "On Memorization in Probabilistic Deep Generative Models",
        "abstract": "Recent advances in deep generative models have led to impressive results in a variety of application domains. Motivated by the possibility that deep learning models might memorize part of the input data, there have been increased efforts to understand how memorization arises. In this work, we extend a recently proposed measure of memorization for supervised learning (Feldman, 2019) to the unsupervised density estimation problem and adapt it to be more computationally efficient. Next, we present a study that demonstrates how memorization can occur in probabilistic deep generative models such as variational autoencoders. This reveals that the form of memorization to which these models are susceptible differs fundamentally from mode collapse and overfitting. Furthermore, we show that the proposed memorization score measures a phenomenon that is not captured by commonly-used nearest neighbor tests. Finally, we discuss several strategies that can be used to limit memorization in practice. Our work thus provides a framework for understanding problematic memorization in probabilistic generative models.",
        "authors": [
            "G. V. D. Burg",
            "Christopher K. I. Williams"
        ],
        "citations": 52,
        "references": 80,
        "year": 2021
    },
    {
        "title": "Learning Generative Models of Textured 3D Meshes from Real-World Images",
        "abstract": "Recent advances in differentiable rendering have sparked an interest in learning generative models of textured 3D meshes from image collections. These models natively disentangle pose and appearance, enable downstream applications in computer graphics, and improve the ability of generative models to understand the concept of image formation. Although there has been prior work on learning such models from collections of 2D images, these approaches require a delicate pose estimation step that exploits annotated keypoints, thereby restricting their applicability to a few specific datasets. In this work, we propose a GAN framework for generating textured triangle meshes without relying on such annotations. We show that the performance of our approach is on par with prior work that relies on ground-truth keypoints, and more importantly, we demonstrate the generality of our method by setting new baselines on a larger set of categories from ImageNet–for which keypoints are not available–without any class-specific hyperparameter tuning. We release our code at https://github.com/dariopavllo/textured-3d-gan",
        "authors": [
            "Dario Pavllo",
            "Jonas Köhler",
            "T. Hofmann",
            "Aurélien Lucchi"
        ],
        "citations": 49,
        "references": 58,
        "year": 2021
    },
    {
        "title": "The Gaussian equivalence of generative models for learning with shallow neural networks",
        "abstract": "Understanding the impact of data structure on the computational tractability of learning is a key challenge for the theory of neural networks. Many theoretical works do not explicitly model training data, or assume that inputs are drawn component-wise independently from some simple probability distribution. Here, we go beyond this simple paradigm by studying the performance of neural networks trained on data drawn from pre-trained generative models. This is possible due to a Gaussian equivalence stating that the key metrics of interest, such as the training and test errors, can be fully captured by an appropriately chosen Gaussian model. We provide three strands of rigorous, analytical and numerical evidence corroborating this equivalence. First, we establish rigorous conditions for the Gaussian equivalence to hold in the case of single-layer generative models, as well as deterministic rates for convergence in distribution. Second, we leverage this equivalence to derive a closed set of equations describing the generalisation performance of two widely studied machine learning problems: two-layer neural networks trained using one-pass stochastic gradient descent, and full-batch pre-learned features or kernel methods. Finally, we perform experiments demonstrating how our theory applies to deep, pre-trained generative models. These results open a viable path to the theoretical study of machine learning models with realistic data.",
        "authors": [
            "Sebastian Goldt",
            "Bruno Loureiro",
            "G. Reeves",
            "Florent Krzakala",
            "M. M'ezard",
            "Lenka Zdeborov'a"
        ],
        "citations": 87,
        "references": 96,
        "year": 2020
    },
    {
        "title": "Inverse design of nanoporous crystalline reticular materials with deep generative models",
        "abstract": null,
        "authors": [
            "Zhenpeng Yao",
            "Benjamín Sánchez-Lengeling",
            "N. Bobbitt",
            "Benjamin J. Bucior",
            "S. Kumar",
            "Sean P. Collins",
            "Thomas Burns",
            "T. Woo",
            "O. Farha",
            "R. Snurr",
            "Alán Aspuru-Guzik"
        ],
        "citations": 241,
        "references": 107,
        "year": 2020
    },
    {
        "title": "Exploring Generative Models with Middle School Students",
        "abstract": "Applications of generative models such as Generative Adversarial Networks (GANs) have made their way to social media platforms that children frequently interact with. While GANs are associated with ethical implications pertaining to children, such as the generation of Deepfakes, there are negligible efforts to educate middle school children about generative AI. In this work, we present a generative models learning trajectory (LT), educational materials, and interactive activities for young learners with a focus on GANs, creation and application of machine-generated media, and its ethical implications. The activities were deployed in four online workshops with 72 students (grades 5-9). We found that these materials enabled children to gain an understanding of what generative models are, their technical components and potential applications, and benefits and harms, while reflecting on their ethical implications. Learning from our findings, we propose an improved learning trajectory for complex socio-technical systems.",
        "authors": [
            "Safinah Ali",
            "Daniella DiPaola",
            "Irene A. Lee",
            "Jenna Hong",
            "C. Breazeal"
        ],
        "citations": 42,
        "references": 56,
        "year": 2021
    },
    {
        "title": "Deep Generative Models for 3D Linker Design",
        "abstract": "Rational compound design remains a challenging problem for both computational methods and medicinal chemists. Computational generative methods have begun to show promising results for the design problem. However, they have not yet used the power of three-dimensional (3D) structural information. We have developed a novel graph-based deep generative model that combines state-of-the-art machine learning techniques with structural knowledge. Our method (“DeLinker”) takes two fragments or partial structures and designs a molecule incorporating both. The generation process is protein-context-dependent, utilizing the relative distance and orientation between the partial structures. This 3D information is vital to successful compound design, and we demonstrate its impact on the generation process and the limitations of omitting such information. In a large-scale evaluation, DeLinker designed 60% more molecules with high 3D similarity to the original molecule than a database baseline. When considering the more relevant problem of longer linkers with at least five atoms, the outperformance increased to 200%. We demonstrate the effectiveness and applicability of this approach on a diverse range of design problems: fragment linking, scaffold hopping, and proteolysis targeting chimera (PROTAC) design. As far as we are aware, this is the first molecular generative model to incorporate 3D structural information directly in the design process. The code is available at https://github.com/oxpig/DeLinker.",
        "authors": [
            "F. Imrie",
            "A. Bradley",
            "M. Schaar",
            "C. Deane"
        ],
        "citations": 165,
        "references": 50,
        "year": 2020
    },
    {
        "title": "Input complexity and out-of-distribution detection with likelihood-based generative models",
        "abstract": "Likelihood-based generative models are a promising resource to detect out-of-distribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, we pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. We report a set of experiments supporting this hypothesis, and use an estimate of input complexity to derive an efficient and parameter-free OOD score, which can be seen as a likelihood-ratio, akin to Bayesian model comparison. We find such score to perform comparably to, or even better than, existing OOD detection approaches under a wide range of data sets, models, model sizes, and complexity estimates.",
        "authors": [
            "J. Serrà",
            "David Álvarez",
            "V. Gómez",
            "Olga Slizovskaia",
            "José F. Núñez",
            "J. Luque"
        ],
        "citations": 262,
        "references": 32,
        "year": 2019
    },
    {
        "title": "Identifiable Deep Generative Models via Sparse Decoding",
        "abstract": "We develop the sparse VAE for unsupervised representation learning on high-dimensional data. The sparse VAE learns a set of latent factors (representations) which summarize the associations in the observed data features. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. As examples, in ratings data each movie is only described by a few genres; in text data each word is only applicable to a few topics; in genomics, each gene is active in only a few biological processes. We prove such sparse deep generative models are identifiable: with infinite data, the true model parameters can be learned. (In contrast, most deep generative models are not identifiable.) We empirically study the sparse VAE with both simulated and real data. We find that it recovers meaningful latent factors and has smaller heldout reconstruction error than related methods.",
        "authors": [
            "Gemma E. Moran",
            "Dhanya Sridhar",
            "Yixin Wang",
            "D. Blei"
        ],
        "citations": 37,
        "references": 53,
        "year": 2021
    },
    {
        "title": "Estimating the success of re-identifications in incomplete datasets using generative models",
        "abstract": null,
        "authors": [
            "Luc Rocher",
            "J. Hendrickx",
            "Y. de Montjoye"
        ],
        "citations": 563,
        "references": 74,
        "year": 2019
    },
    {
        "title": "Learning Generative Models of 3D Structures",
        "abstract": "3D models of objects and scenes are critical to many academic disciplines and industrial applications. Of particular interest is the emerging opportunity for 3D graphics to serve artificial intelligence: computer vision systems can benefit from synthetically‐generated training data rendered from virtual 3D scenes, and robots can be trained to navigate in and interact with real‐world environments by first acquiring skills in simulated ones. One of the most promising ways to achieve this is by learning and applying generative models of 3D content: computer programs that can synthesize new 3D shapes and scenes. To allow users to edit and manipulate the synthesized 3D content to achieve their goals, the generative model should also be structure‐aware: it should express 3D shapes and scenes using abstractions that allow manipulation of their high‐level structure. This state‐of‐the‐art report surveys historical work and recent progress on learning structure‐aware generative models of 3D shapes and scenes. We present fundamental representations of 3D shape and scene geometry and structures, describe prominent methodologies including probabilistic models, deep generative models, program synthesis, and neural networks for structured data, and cover many recent methods for structure‐aware synthesis of 3D shapes and indoor scenes.",
        "authors": [
            "S. Chaudhuri",
            "Daniel Ritchie",
            "Kai Xu",
            "Haotong Zhang"
        ],
        "citations": 81,
        "references": 177,
        "year": 2020
    },
    {
        "title": "Generative chemistry: drug discovery with deep learning generative models",
        "abstract": null,
        "authors": [
            "Yuemin Bian",
            "X. Xie"
        ],
        "citations": 83,
        "references": 156,
        "year": 2020
    },
    {
        "title": "Protein sequence design with deep generative models",
        "abstract": null,
        "authors": [
            "Zachary Wu",
            "Kadina E. Johnston",
            "F. Arnold",
            "Kevin Kaichuang Yang"
        ],
        "citations": 124,
        "references": 86,
        "year": 2021
    },
    {
        "title": "Critical Points in Quantum Generative Models",
        "abstract": "One of the most important properties of neural networks is the clustering of local minima of the loss function near the global minimum, enabling efficient training. Though generative models implemented on quantum computers are known to be more expressive than their traditional counterparts, it has empirically been observed that these models experience a transition in the quality of their local minima. Namely, below some critical number of parameters, all local minima are far from the global minimum in function value; above this critical parameter count, all local minima are good approximators of the global minimum. Furthermore, for a certain class of quantum generative models, this transition has empirically been observed to occur at parameter counts exponentially large in the problem size, meaning practical training of these models is out of reach. Here, we give the first proof of this transition in trainability, specializing to this latter class of quantum generative model. We use techniques inspired by those used to study the loss landscapes of classical neural networks. We also verify that our analytic results hold experimentally even at modest model sizes.",
        "authors": [
            "Eric R. Anschuetz"
        ],
        "citations": 39,
        "references": 49,
        "year": 2021
    },
    {
        "title": "Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining",
        "abstract": "Many important problems in science and engineering, such as drug design, involve optimizing an expensive black-box objective function over a complex, high-dimensional, and structured input space. Although machine learning techniques have shown promise in solving such problems, existing approaches substantially lack sample efficiency. We introduce an improved method for efficient black-box optimization, which performs the optimization in the low-dimensional, continuous latent manifold learned by a deep generative model. In contrast to previous approaches, we actively steer the generative model to maintain a latent manifold that is highly useful for efficiently optimizing the objective. We achieve this by periodically retraining the generative model on the data points queried along the optimization trajectory, as well as weighting those data points according to their objective function value. This weighted retraining can be easily implemented on top of existing methods, and is empirically shown to significantly improve their efficiency and performance on synthetic and real-world optimization problems.",
        "authors": [
            "Austin Tripp",
            "Erik A. Daxberger",
            "José Miguel Hernández-Lobato"
        ],
        "citations": 124,
        "references": 81,
        "year": 2020
    },
    {
        "title": "Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions",
        "abstract": "Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to evaluate and compare them. In this paper, we enumerate the desirable criteria for such a comparison metric and provide an overview of the status quo of graph generative model comparison in use today, which predominantly relies on the maximum mean discrepancy (MMD). We perform a systematic evaluation of MMD in the context of graph generative model comparison, highlighting some of the challenges and pitfalls researchers inadvertently may encounter. After conducting a thorough analysis of the behaviour of MMD on synthetically-generated perturbed graphs as well as on recently-proposed graph generative models, we are able to provide a suitable procedure to mitigate these challenges and pitfalls. We aggregate our findings into a list of practical recommendations for researchers to use when evaluating graph generative models.",
        "authors": [
            "Leslie O’Bray",
            "Max Horn",
            "Bastian Alexander Rieck",
            "Karsten M. Borgwardt"
        ],
        "citations": 35,
        "references": 51,
        "year": 2021
    },
    {
        "title": "Classification Accuracy Score for Conditional Generative Models",
        "abstract": "Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes---variational autoencoders, autoregressive models, and generative adversarial networks (GANs)---to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), reveals some surprising results not identified by traditional metrics and constitute our contributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1 and Top-5 accuracy decrease by 27.9\\% and 41.6\\%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore, in order to facilitate better diagnoses of generative models, we open-source the proposed metric.",
        "authors": [
            "Suman V. Ravuri",
            "O. Vinyals"
        ],
        "citations": 215,
        "references": 47,
        "year": 2019
    },
    {
        "title": "Adversarial Attacks Against Deep Generative Models on Data: A Survey",
        "abstract": "Deep generative models have gained much attention given their ability to generate data for applications as varied as healthcare to financial technology to surveillance, and many more - the most popular models being generative adversarial networks (GANs) and variational auto-encoders (VAEs). Yet, as with all machine learning models, ever is the concern over security breaches and privacy leaks and deep generative models are no exception. In fact, these models have advanced so rapidly in recent years that work on their security is still in its infancy. In an attempt to audit the current and future threats against these models, and to provide a roadmap for defense preparations in the short term, we prepared this comprehensive and specialized survey on the security and privacy preservation of GANs and VAEs. Our focus is on the inner connection between attacks and model architectures and, more specifically, on five components of deep generative models: the training data, the latent code, the generators/decoders of GANs/VAEs, the discriminators/encoders of GANs/VAEs, and the generated data. For each model, component and attack, we review the current research progress and identify the key challenges. The paper concludes with a discussion of possible future attacks and research directions in the field.",
        "authors": [
            "Hui Sun",
            "Tianqing Zhu",
            "Zhiqiu Zhang",
            "Dawei Jin",
            "P. Xiong",
            "Wanlei Zhou"
        ],
        "citations": 35,
        "references": 132,
        "year": 2021
    },
    {
        "title": "From Generative Models to Generative Passages: A Computational Approach to (Neuro) Phenomenology",
        "abstract": null,
        "authors": [
            "M. Ramstead",
            "C. Hesp",
            "L. Sandved-Smith",
            "J. Mago",
            "M. Lifshitz",
            "G. Pagnoni",
            "Ryan Smith",
            "G. Dumas",
            "A. Lutz",
            "K. Friston",
            "Axel Constant"
        ],
        "citations": 41,
        "references": 168,
        "year": 2021
    },
    {
        "title": "Deep generative models of genetic variation capture the effects of mutations",
        "abstract": null,
        "authors": [
            "Adam J. Riesselman",
            "John Ingraham",
            "D. Marks"
        ],
        "citations": 487,
        "references": 78,
        "year": 2018
    },
    {
        "title": "Learning Generative Models with Sinkhorn Divergences",
        "abstract": "The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.",
        "authors": [
            "Aude Genevay",
            "G. Peyré",
            "Marco Cuturi"
        ],
        "citations": 585,
        "references": 38,
        "year": 2017
    },
    {
        "title": "A Non-Parametric Test to Detect Data-Copying in Generative Models",
        "abstract": "Detecting overfitting in generative models is an important challenge in machine learning. In this work, we formalize a form of overfitting that we call {\\em{data-copying}} -- where the generative model memorizes and outputs training samples or small variations thereof. We provide a three sample non-parametric test for detecting data-copying that uses the training set, a separate sample from the target distribution, and a generated sample from the model, and study the performance of our test on several canonical models and datasets. \nFor code \\& examples, visit this https URL",
        "authors": [
            "Casey Meehan",
            "Kamalika Chaudhuri",
            "S. Dasgupta"
        ],
        "citations": 54,
        "references": 23,
        "year": 2020
    },
    {
        "title": "The neural coding framework for learning generative models",
        "abstract": null,
        "authors": [
            "Alexander Ororbia",
            "Daniel Kifer"
        ],
        "citations": 58,
        "references": 118,
        "year": 2020
    },
    {
        "title": "Generative Models for Active Vision",
        "abstract": "The active visual system comprises the visual cortices, cerebral attention networks, and oculomotor system. While fascinating in its own right, it is also an important model for sensorimotor networks in general. A prominent approach to studying this system is active inference—which assumes the brain makes use of an internal (generative) model to predict proprioceptive and visual input. This approach treats action as ensuring sensations conform to predictions (i.e., by moving the eyes) and posits that visual percepts are the consequence of updating predictions to conform to sensations. Under active inference, the challenge is to identify the form of the generative model that makes these predictions—and thus directs behavior. In this paper, we provide an overview of the generative models that the brain must employ to engage in active vision. This means specifying the processes that explain retinal cell activity and proprioceptive information from oculomotor muscle fibers. In addition to the mechanics of the eyes and retina, these processes include our choices about where to move our eyes. These decisions rest upon beliefs about salient locations, or the potential for information gain and belief-updating. A key theme of this paper is the relationship between “looking” and “seeing” under the brain's implicit generative model of the visual world.",
        "authors": [
            "Thomas Parr",
            "Noor Sajid",
            "Lancelot Da Costa",
            "M. Berk Mirza",
            "K. Friston"
        ],
        "citations": 30,
        "references": 198,
        "year": 2021
    },
    {
        "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models",
        "abstract": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.",
        "authors": [
            "Benjamin Hilprecht",
            "Martin Härterich",
            "Daniel Bernau"
        ],
        "citations": 176,
        "references": 32,
        "year": 2019
    },
    {
        "title": "Identifiable Generative Models for Missing Not at Random Data Imputation",
        "abstract": "Real-world datasets often have missing values associated with complex generative processes, where the cause of the missingness may not be fully observed. This is known as missing not at random (MNAR) data. However, many imputation methods do not take into account the missingness mechanism, resulting in biased imputation values when MNAR data is present. Although there are a few methods that have considered the MNAR scenario, their model's identifiability under MNAR is generally not guaranteed. That is, model parameters can not be uniquely determined even with infinite data samples, hence the imputation results given by such models can still be biased. This issue is especially overlooked by many modern deep generative models. In this work, we fill in this gap by systematically analyzing the identifiability of generative models under MNAR. Furthermore, we propose a practical deep generative model which can provide identifiability guarantees under mild assumptions, for a wide range of MNAR mechanisms. Our method demonstrates a clear advantage for tasks on both synthetic data and multiple real-world scenarios with MNAR data.",
        "authors": [
            "Chao Ma",
            "Cheng Zhang"
        ],
        "citations": 29,
        "references": 65,
        "year": 2021
    },
    {
        "title": "Generative models for inverse design of inorganic solid materials",
        "abstract": "Overwhelming evidence has been accumulating that materials informatics can provide a novel solution for materials discovery. While the conventional approach to innovation relies mainly on experimentation, the generative models stemming from the field of machine learning can realize the long-held dream of inverse design, where properties are mapped to the chemical structures. In this review, we introduce the general aspects of inverse materials design and provide a brief overview of two generative models, variational autoencoder and generative adversarial network, which can be utilized to generate and optimize inorganic solid materials according to their properties. Reversible representation schemes for generative models are compared between molecular and crystalline structures, and challenges in regard to the latter are also discussed. Finally, we summarize the recent application of generative models in the exploration of chemical space with compositional and configurational degrees of freedom, and potential future directions are speculatively outlined.",
        "authors": [
            "Litao Chen",
            "Wentao Zhang",
            "Zhiwei Nie",
            "Shunning Li",
            "Feng Pan"
        ],
        "citations": 24,
        "references": 100,
        "year": 2021
    },
    {
        "title": "Deep Generative Design: Integration of Topology Optimization and Generative Models",
        "abstract": "\n Deep learning has recently been applied to various research areas of design optimization. This study presents the need and effectiveness of adopting deep learning for generative design (or design exploration) research area. This work proposes an artificial intelligent (AI)-based deep generative design framework that is capable of generating numerous design options which are not only aesthetic but also optimized for engineering performance. The proposed framework integrates topology optimization and generative models (e.g., generative adversarial networks (GANs)) in an iterative manner to explore new design options, thus generating a large number of designs starting from limited previous design data. In addition, anomaly detection can evaluate the novelty of generated designs, thus helping designers choose among design options. The 2D wheel design problem is applied as a case study for validation of the proposed framework. The framework manifests better aesthetics, diversity, and robustness of generated designs than previous generative design methods.",
        "authors": [
            "Sangeun Oh",
            "Yongsu Jung",
            "Seongsin Kim",
            "Ikjin Lee",
            "Namwoo Kang"
        ],
        "citations": 286,
        "references": 52,
        "year": 2019
    },
    {
        "title": "Enhancing scientific discoveries in molecular biology with deep generative models",
        "abstract": "Generative models provide a well‐established statistical framework for evaluating uncertainty and deriving conclusions from large data sets especially in the presence of noise, sparsity, and bias. Initially developed for computer vision and natural language processing, these models have been shown to effectively summarize the complexity that underlies many types of data and enable a range of applications including supervised learning tasks, such as assigning labels to images; unsupervised learning tasks, such as dimensionality reduction; and out‐of‐sample generation, such as de novo image synthesis. With this early success, the power of generative models is now being increasingly leveraged in molecular biology, with applications ranging from designing new molecules with properties of interest to identifying deleterious mutations in our genomes and to dissecting transcriptional variability between single cells. In this review, we provide a brief overview of the technical notions behind generative models and their implementation with deep learning techniques. We then describe several different ways in which these models can be utilized in practice, using several recent applications in molecular biology as examples.",
        "authors": [
            "Romain Lopez",
            "Adam Gayoso",
            "N. Yosef"
        ],
        "citations": 60,
        "references": 178,
        "year": 2020
    },
    {
        "title": "Latent Space Refinement for Deep Generative Models",
        "abstract": "Deep generative models are becoming widely used across science and industry for a variety of purposes. A common challenge is achieving a precise implicit or explicit representation of the data probability density. Recent proposals have suggested using classifier weights to refine the learned density of deep generative models. We extend this idea to all types of generative models and show how latent space refinement via iterated generative modeling can circumvent topological obstructions and improve precision. This methodology also applies to cases were the target model is non-differentiable and has many internal latent dimensions which must be marginalized over before refinement. We demonstrate our Latent Space Refinement (LaSeR) protocol on a variety of examples, focusing on the combinations of Normalizing Flows and Generative Adversarial Networks.",
        "authors": [
            "R. Winterhalder",
            "Marco Bellagente",
            "B. Nachman"
        ],
        "citations": 26,
        "references": 96,
        "year": 2021
    },
    {
        "title": "Measuring Fairness in Generative Models",
        "abstract": "Deep generative models have made much progress in improving training stability and quality of generated data. Recently there has been increased interest in the fairness of deep-generated data. Fairness is important in many applications, e.g. law enforcement, as biases will affect efficacy. Central to fair data generation are the fairness metrics for the assessment and evaluation of different generative models. In this paper, we first review fairness metrics proposed in previous works and highlight potential weaknesses. We then discuss a performance benchmark framework along with the assessment of alternative metrics.",
        "authors": [
            "Christopher T. H. Teo",
            "Ngai-Man Cheung"
        ],
        "citations": 19,
        "references": 22,
        "year": 2021
    },
    {
        "title": "Refining Deep Generative Models via Discriminator Gradient Flow",
        "abstract": "Deep generative modeling has seen impressive advances in recent years, to the point where it is now commonplace to see simulated samples (e.g., images) that closely resemble real-world data. However, generation quality is generally inconsistent for any given model and can vary dramatically between samples. We introduce Discriminator Gradient flow (DGflow), a new technique that improves generated samples via the gradient flow of entropy-regularized f-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, our technique avoids wasteful sample rejection used by previous methods (DRS&MH-GAN). Compared to existing works that focus on specific GAN variants, we show our refinement approach can be applied to GANs with vector-valued critics and even other deep generative models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate that DGflow leads to significant improvement in the quality of generated samples for a variety of generative models, outperforming the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling (DDLS) methods.",
        "authors": [
            "Abdul Fatir Ansari",
            "Ming Liang Ang",
            "Harold Soh"
        ],
        "citations": 47,
        "references": 53,
        "year": 2020
    },
    {
        "title": "Uncertainty-Aware Deep Classifiers Using Generative Models",
        "abstract": "Deep neural networks are often ignorant about what they do not know and overconfident when they make uninformed predictions. Some recent approaches quantify classification uncertainty directly by training the model to output high uncertainty for the data samples close to class boundaries or from the outside of the training distribution. These approaches use an auxiliary data set during training to represent out-of-distribution samples. However, selection or creation of such an auxiliary data set is non-trivial, especially for high dimensional data such as images. In this work we develop a novel neural network model that is able to express both aleatoric and epistemic uncertainty to distinguish decision boundary and out-of-distribution regions of the feature space. To this end, variational autoencoders and generative adversarial networks are incorporated to automatically generate out-of-distribution exemplars for training. Through extensive analysis, we demonstrate that the proposed approach provides better estimates of uncertainty for in- and out-of-distribution samples, and adversarial examples on well-known data sets against state-of-the-art approaches including recent Bayesian approaches for neural networks and anomaly detection methods.",
        "authors": [
            "M. Sensoy",
            "Lance M. Kaplan",
            "Federico Cerutti",
            "Maryam Saleki"
        ],
        "citations": 70,
        "references": 34,
        "year": 2020
    },
    {
        "title": "Generative Models for Effective ML on Private, Decentralized Datasets",
        "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data - of representative samples, of outliers, of misclassifications - is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning or refining human-provided labels. However, manual data inspection is problematic for privacy sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models - trained using federated methods and with formal differential privacy guarantees - can be used effectively to debug many commonly occurring data issues even when the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.",
        "authors": [
            "S. Augenstein",
            "H. B. McMahan",
            "Daniel Ramage",
            "Swaroop Indra Ramaswamy",
            "P. Kairouz",
            "Mingqing Chen",
            "Rajiv Mathews",
            "B. A. Y. Arcas"
        ],
        "citations": 176,
        "references": 57,
        "year": 2019
    },
    {
        "title": "Few-shot Learning with Multilingual Generative Language Models",
        "abstract": "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.",
        "authors": [
            "Xi Victoria Lin",
            "Todor Mihaylov",
            "Mikel Artetxe",
            "Tianlu Wang",
            "Shuohui Chen",
            "Daniel Simig",
            "Myle Ott",
            "Naman Goyal",
            "Shruti Bhosale",
            "Jingfei Du",
            "Ramakanth Pasunuru",
            "Sam Shleifer",
            "Punit Singh Koura",
            "Vishrav Chaudhary",
            "Brian O'Horo",
            "Jeff Wang",
            "Luke Zettlemoyer",
            "Zornitsa Kozareva",
            "Mona T. Diab",
            "Ves Stoyanov",
            "Xian Li"
        ],
        "citations": 262,
        "references": 76,
        "year": 2021
    },
    {
        "title": "Theoretically Informed Generative Models Can Advance the Psychological and Brain Sciences: Lessons from the Reliability Paradox",
        "abstract": "Theories of individual differences are foundational to psychological and brain sciences, yet they are traditionally developed and tested using superficial summaries of data (e.g., mean response times) that are both (1) disconnected from our otherwise rich conceptual theories of behavior, and (2) contaminated with measurement error. Traditional approaches therefore lack the flexibility required to test increasingly complex theories of behavior. To resolve this theory-description gap, we present the generative modeling approach, which involves using background knowledge to formally specify how behavior is generated within people, and in turn how generative processes vary across people. Generative modeling shifts our focus away from estimating descriptive statistical “effects” toward estimating psychologically interpretable parameters, while simultaneously accounting for measurement error that would otherwise attenuate individual difference correlations. We demonstrate the utility of generative models in the context of the “reliability paradox”, a phenomenon wherein highly replicable group effects (e.g., Stroop effect) fail to capture individual differences (e.g., low test-retest reliability). Simulations and empirical data from the Implicit Association Test, and Stroop, Flanker, Posner, and Delay Discounting tasks show that generative models yield (1) more theoretically informative parameters, and (2) higher test-retest estimates relative to traditional approaches, illustrating their potential for enhancing theory development.",
        "authors": [
            "Nathaniel Haines",
            "Peter D. Kvam",
            "L. Irving",
            "C. Smith",
            "Theodore P. Beauchaine",
            "M. Pitt",
            "W. Ahn",
            "Brandon M. Turner"
        ],
        "citations": 45,
        "references": 0,
        "year": 2020
    },
    {
        "title": "Comparative Study of Deep Generative Models on Chemical Space Coverage",
        "abstract": "In recent years, deep molecular generative models have emerged as promising methods for de novo molecular design. Thanks to the rapid advance of deep learning techniques, deep learning architectures such as recurrent neural networks, variational autoencoders, and adversarial networks have been successfully employed for constructing generative models. Recently, quite a few metrics have been proposed to evaluate these deep generative models. However, many of these metrics cannot evaluate the chemical space coverage of sampled molecules. This work presents a novel and complementary metric for evaluating deep molecular generative models. The metric is based on the chemical space coverage of a reference dataset-GDB-13. The performance of seven different molecular generative models was compared by calculating what fraction of the structures, ring systems, and functional groups could be reproduced from the largely unseen reference set when using only a small fraction of GDB-13 for training. The results show that the performance of the generative models studied varies significantly using the benchmark metrics introduced herein, such that the generalization capabilities of the generative models can be clearly differentiated. In addition, the coverages of GDB-13 ring systems and functional groups were compared between the models. Our study provides a useful new metric that can be used for evaluating and comparing generative models.",
        "authors": [
            "J. Zhang",
            "Rocío Mercado",
            "O. Engkvist",
            "Hongming Chen"
        ],
        "citations": 43,
        "references": 49,
        "year": 2020
    },
    {
        "title": "Real-time crash prediction on expressways using deep generative models",
        "abstract": null,
        "authors": [
            "Qing Cai",
            "M. Abdel-Aty",
            "Jinghui Yuan",
            "Jaeyoung Lee",
            "Yina Wu"
        ],
        "citations": 132,
        "references": 45,
        "year": 2020
    },
    {
        "title": "Improving the Fairness of Deep Generative Models without Retraining",
        "abstract": "Generative Adversarial Networks (GANs) have recently advanced face synthesis by learning the underlying distribution of observed data. However, it will lead to a biased image generation due to the imbalanced training data or the mode collapse issue. Prior work typically addresses the fairness of data generation by balancing the training data that correspond to the concerned attributes. In this work, we propose a simple yet effective method to improve the fairness of image generation for a pre-trained GAN model without retraining. We utilize the recent work of GAN interpretation to identify the directions in the latent space corresponding to the target attributes, and then manipulate a set of latent codes with balanced attribute distributions over output images. We learn a Gaussian Mixture Model (GMM) to fit a distribution of the latent code set, which supports the sampling of latent codes for producing images with a more fair attribute distribution. Experiments show that our method can substantially improve the fairness of image generation, outperforming potential baselines both quantitatively and qualitatively. The images generated from our method are further applied to reveal and quantify the biases in commercial face classifiers and face super-resolution model.",
        "authors": [
            "Shuhan Tan",
            "Yujun Shen",
            "Bolei Zhou"
        ],
        "citations": 56,
        "references": 33,
        "year": 2020
    },
    {
        "title": "LOGAN: Membership Inference Attacks Against Generative Models",
        "abstract": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator’s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.",
        "authors": [
            "Jamie Hayes",
            "Luca Melis",
            "G. Danezis",
            "Emiliano De Cristofaro"
        ],
        "citations": 476,
        "references": 86,
        "year": 2017
    },
    {
        "title": "Fréchet ChemNet Distance: A Metric for Generative Models for Molecules in Drug Discovery",
        "abstract": "The new wave of successful generative models in machine learning has increased the interest in deep learning driven de novo drug design. However, method comparison is difficult because of various flaws of the currently employed evaluation metrics. We propose an evaluation metric for generative models called Fréchet ChemNet distance (FCD). The advantage of the FCD over previous metrics is that it can detect whether generated molecules are diverse and have similar chemical and biological properties as real molecules.",
        "authors": [
            "Kristina Preuer",
            "Philipp Renz",
            "Thomas Unterthiner",
            "Sepp Hochreiter",
            "G. Klambauer"
        ],
        "citations": 304,
        "references": 29,
        "year": 2018
    },
    {
        "title": "Constructing Unrestricted Adversarial Examples with Generative Models",
        "abstract": "Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small norm-bounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Specifically, we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.",
        "authors": [
            "Yang Song",
            "Rui Shu",
            "Nate Kushman",
            "Stefano Ermon"
        ],
        "citations": 284,
        "references": 54,
        "year": 2018
    },
    {
        "title": "DCTRGAN: improving the precision of generative models with reweighting",
        "abstract": "Significant advances in deep learning have led to more widely used and precise neural network-based generative models such as Generative Adversarial Networks (GANS). We introduce a post-hoc correction to deep generative models to further improve their fidelity, based on the Deep neural networks using the Classification for Tuning and Reweighting (DCTR) protocol. The correction takes the form of a reweighting function that can be applied to generated examples when making predictions from the simulation. We illustrate this approach using GANS trained on standard multimodal probability densities as well as calorimeter simulations from high energy physics. We show that the weighted GAN examples significantly improve the accuracy of the generated samples without a large loss in statistical power. This approach could be applied to any generative model and is a promising refinement method for high energy physics applications and beyond.",
        "authors": [
            "S. Diefenbacher",
            "E. Eren",
            "G. Kasieczka",
            "A. Korol",
            "Benjamin Nachman",
            "David Shih"
        ],
        "citations": 41,
        "references": 90,
        "year": 2020
    },
    {
        "title": "Winning Lottery Tickets in Deep Generative Models",
        "abstract": "The lottery ticket hypothesis suggests that sparse, sub-networks of a given neural network, if initialized properly, can be trained to reach comparable or even better performance to that of the original network. Prior works in lottery tickets have primarily focused on the supervised learning setup, with several papers proposing effective ways of finding winning tickets in classification problems. In this paper, we confirm the existence of winning tickets in deep generative models such as GANs and VAEs. We show that the popular iterative magnitude pruning approach (with late resetting) can be used with generative losses to find the winning tickets. This approach effectively yields tickets with sparsity up to 99% for AutoEncoders, 93% for VAEs and 89% for GANs on CIFAR and Celeb-A datasets. We also demonstrate the transferability of winning tickets across different generative models (GANs and VAEs) sharing the same architecture, suggesting that winning tickets have inductive biases that could help train a wide range of deep generative models. Furthermore, we show the practical benefits of lottery tickets in generative models by detecting tickets at very early stages in training called early-bird tickets. Through early-bird tickets, we can achieve up to 88% reduction in floating-point operations (FLOPs) and 54% reduction in training time, making it possible to train large-scale generative models over tight resource constraints. These results out-perform existing early pruning methods like SNIP (Lee, Ajanthan, and Torr 2019) and GraSP(Wang, Zhang, and Grosse 2020). Our findings shed light towards existence of proper network initializations that could improve convergence and stability of generative models.",
        "authors": [
            "N. Kalibhat",
            "Y. Balaji",
            "S. Feizi"
        ],
        "citations": 41,
        "references": 41,
        "year": 2020
    },
    {
        "title": "Invertible generative models for inverse problems: mitigating representation error and dataset bias",
        "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging -- for example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors. Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting. Given a trained generative model, we study the empirical risk formulation of the desired inverse problem under a regularization that promotes high likelihood images, either directly by penalization or algorithmically by initialization. For compressive sensing, invertible priors can yield higher accuracy than sparsity priors across almost all undersampling ratios, and due to their lack of representation error, invertible priors can yield better reconstructions than GAN priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images. We additionally compare performance for compressive sensing to unlearned methods, such as the deep decoder, and we establish theoretical bounds on expected recovery error in the case of a linear invertible model.",
        "authors": [
            "Muhammad Asim",
            "Ali Ahmed",
            "Paul Hand"
        ],
        "citations": 143,
        "references": 36,
        "year": 2019
    },
    {
        "title": "Detecting Out-of-Distribution Inputs to Deep Generative Models Using Typicality",
        "abstract": "Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data (Nalisnick et al., 2019; Choi et al., 2019). We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density. In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed. To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods. The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated. We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. (2019).",
        "authors": [
            "Eric T. Nalisnick",
            "Akihiro Matsukawa",
            "Y. Teh",
            "Balaji Lakshminarayanan"
        ],
        "citations": 124,
        "references": 65,
        "year": 2019
    },
    {
        "title": "Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models",
        "abstract": "Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.",
        "authors": [
            "Sam Bond-Taylor",
            "Adam Leach",
            "Yang Long",
            "Chris G. Willcocks"
        ],
        "citations": 410,
        "references": 276,
        "year": 2021
    },
    {
        "title": "Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models",
        "abstract": "Multimodal learning for generative models often refers to the learning of abstract concepts from the commonality of information in multiple modalities, such as vision and language. While it has proven effective for learning generalisable representations, the training of such models often requires a large amount of \"related\" multimodal data that shares commonality, which can be expensive to come by. To mitigate this, we develop a novel contrastive framework for generative model learning, allowing us to train the model not just by the commonality between modalities, but by the distinction between \"related\" and \"unrelated\" multimodal data. We show in experiments that our method enables data-efficient multimodal learning on challenging datasets for various multimodal VAE models. We also show that under our proposed framework, the generative model can accurately identify related samples from unrelated ones, making it possible to make use of the plentiful unlabeled, unpaired multimodal data.",
        "authors": [
            "Yuge Shi",
            "Brooks Paige",
            "Philip H. S. Torr",
            "N. Siddharth"
        ],
        "citations": 36,
        "references": 25,
        "year": 2020
    },
    {
        "title": "Further Analysis of Outlier Detection with Deep Generative Models",
        "abstract": "The recent, counter-intuitive discovery that deep generative models (DGMs) can frequently assign a higher likelihood to outliers has implications for both outlier detection applications as well as our overall understanding of generative modeling. In this work, we present a possible explanation for this phenomenon, starting from the observation that a model's typical set and high-density region may not conincide. From this vantage point we propose a novel outlier test, the empirical success of which suggests that the failure of existing likelihood-based outlier tests does not necessarily imply that the corresponding generative model is uncalibrated. We also conduct additional experiments to help disentangle the impact of low-level texture versus high-level semantics in differentiating outliers. In aggregate, these results suggest that modifications to the standard evaluation practices and benchmarks commonly applied in the literature are needed.",
        "authors": [
            "Ziyu Wang",
            "B. Dai",
            "D. Wipf",
            "Jun Zhu"
        ],
        "citations": 36,
        "references": 37,
        "year": 2020
    },
    {
        "title": "Statistical guarantees for generative models without domination",
        "abstract": "In this paper, we introduce a convenient framework for studying (adversarial) generative models from a statistical perspective. It consists in modeling the generative device as a smooth transformation of the unit hypercube of a dimension that is much smaller than that of the ambient space and measuring the quality of the generative model by means of an integral probability metric. In the particular case of integral probability metric defined through a smoothness class, we establish a risk bound quantifying the role of various parameters. In particular, it clearly shows the impact of dimension reduction on the error of the generative model.",
        "authors": [
            "Nicolas Schreuder",
            "Victor-Emmanuel Brunel",
            "A. Dalalyan"
        ],
        "citations": 31,
        "references": 39,
        "year": 2020
    },
    {
        "title": "Theoretical guarantees for sampling and inference in generative models with latent diffusions",
        "abstract": "We introduce and study a class of probabilistic generative models, where the latent object is a finite-dimensional diffusion process on a finite time interval and the observed variable is drawn conditionally on the terminal point of the diffusion. We make the following contributions: \nWe provide a unified viewpoint on both sampling and variational inference in such generative models through the lens of stochastic control. \nWe quantify the expressiveness of diffusion-based generative models. Specifically, we show that one can efficiently sample from a wide class of terminal target distributions by choosing the drift of the latent diffusion from the class of multilayer feedforward neural nets, with the accuracy of sampling measured by the Kullback-Leibler divergence to the target distribution. \nFinally, we present and analyze a scheme for unbiased simulation of generative models with latent diffusions and provide bounds on the variance of the resulting estimators. This scheme can be implemented as a deep generative model with a random number of layers.",
        "authors": [
            "Belinda Tzen",
            "M. Raginsky"
        ],
        "citations": 89,
        "references": 47,
        "year": 2019
    },
    {
        "title": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
        "abstract": "Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.",
        "authors": [
            "Yang Song",
            "Taesup Kim",
            "Sebastian Nowozin",
            "Stefano Ermon",
            "Nate Kushman"
        ],
        "citations": 753,
        "references": 48,
        "year": 2017
    },
    {
        "title": "Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting",
        "abstract": "A learned generative model often produces biased statistics relative to the underlying data distribution. A standard technique to correct this bias is importance sampling, where samples from the model are weighted by the likelihood ratio under model and true distributions. When the likelihood ratio is unknown, it can be estimated by training a probabilistic classifier to distinguish samples from the two distributions. We employ this likelihood-free importance weighting method to correct for the bias in generative models. We find that this technique consistently improves standard goodness-of-fit metrics for evaluating the sample quality of state-of-the-art deep generative models, suggesting reduced bias. Finally, we demonstrate its utility on representative applications in a) data augmentation for classification using generative adversarial networks, and b) model-based policy evaluation using off-policy data.",
        "authors": [
            "Aditya Grover",
            "Jiaming Song",
            "Alekh Agarwal",
            "Kenneth Tran",
            "Ashish Kapoor",
            "E. Horvitz",
            "Stefano Ermon"
        ],
        "citations": 120,
        "references": 67,
        "year": 2019
    },
    {
        "title": "Detecting Out-of-Distribution Inputs to Deep Generative Models Using a Test for Typicality",
        "abstract": "Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data. We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density. In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed. To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods. The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated. We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. (2019).",
        "authors": [
            "Eric T. Nalisnick",
            "Akihiro Matsukawa",
            "Y. Teh",
            "Balaji Lakshminarayanan"
        ],
        "citations": 86,
        "references": 44,
        "year": 2019
    },
    {
        "title": "HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models",
        "abstract": "Generative models often use human evaluations to measure the perceived quality of their outputs. Automated metrics are noisy indirect proxies, because they rely on heuristics or pretrained embeddings. However, up until now, direct human evaluation strategies have been ad-hoc, neither standardized nor validated. Our work establishes a gold standard human benchmark for generative realism. We construct Human eYe Perceptual Evaluation (HYPE) a human benchmark that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) able to produce separable model performances, and (4) efficient in cost and time. We introduce two variants: one that measures visual perception under adaptive time constraints to determine the threshold at which a model's outputs appear real (e.g. 250ms), and the other a less expensive variant that measures human error rate on fake and real images sans time constraints. We test HYPE across six state-of-the-art generative adversarial networks and two sampling techniques on conditional and unconditional image generation using four datasets: CelebA, FFHQ, CIFAR-10, and ImageNet. We find that HYPE can track model improvements across training epochs, and we confirm via bootstrap sampling that HYPE rankings are consistent and replicable.",
        "authors": [
            "Sharon Zhou",
            "Mitchell L. Gordon",
            "Ranjay Krishna",
            "Austin Narcomey",
            "Li Fei-Fei",
            "Michael S. Bernstein"
        ],
        "citations": 112,
        "references": 62,
        "year": 2019
    },
    {
        "title": "Sliced Wasserstein Generative Models",
        "abstract": "In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.",
        "authors": [
            "Jiqing Wu",
            "Zhiwu Huang",
            "Dinesh Acharya",
            "Wen Li",
            "Janine Thoma",
            "D. Paudel",
            "L. Gool"
        ],
        "citations": 117,
        "references": 48,
        "year": 2019
    },
    {
        "title": "AmbientGAN: Generative models from lossy measurements",
        "abstract": "Generative models provide a way to model structure in complex distributions and have been shown to be useful for many tasks of practical interest. However, cur-rent techniques for training generative models require access to fully-observed samples. In many settings, it is expensive or even impossible to obtain fully-observed samples, but economical to obtain partial, noisy observations. We consider the task of learning an implicit generative model given only lossy measurements of samples from the distribution of interest. We show that the true underlying distribution can be provably recovered even in the presence of per-sample information loss for a class of measurement models. Based on this, we propose a new method of training Generative Adversarial Networks (GANs) which we call AmbientGAN. On three benchmark datasets, and for various measurement models, we demonstrate substantial qualitative and quantitative improvements. Generative models trained with our method can obtain 2 - 4 x higher inception scores than the baselines.",
        "authors": [
            "Ashish Bora",
            "Eric Price",
            "A. Dimakis"
        ],
        "citations": 184,
        "references": 34,
        "year": 2018
    },
    {
        "title": "Reconstructing quantum states with generative models",
        "abstract": null,
        "authors": [
            "J. Carrasquilla",
            "G. Torlai",
            "R. Melko",
            "L. Aolita"
        ],
        "citations": 274,
        "references": 64,
        "year": 2018
    },
    {
        "title": "The Gaussian equivalence of generative models for learning with two-layer neural networks",
        "abstract": "Understanding the impact of data structure on learning in neural networks remains a key challenge for the theory of neural networks. Many theoretical works on neural networks do not explicitly model training data, or assume that inputs are drawn independently from some factorised probability distribution. Here, we go beyond the simple i.i.d. modelling paradigm by studying neural networks trained on data drawn from structured generative models. We make three contributions: First, we establish rigorous conditions under which a class of generative models shares key statistical properties with an appropriately chosen Gaussian feature model. Second, we use this Gaussian equivalence theorem (GET) to derive a closed set of equations that describe the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent on data drawn from a large class of generators. We complement our theoretical results by experiments demonstrating how our theory applies to deep, pre-trained generative models.",
        "authors": [
            "Sebastian Goldt",
            "Bruno Loureiro",
            "G. Reeves",
            "M. M'ezard",
            "Florent Krzakala",
            "Lenka Zdeborov'a"
        ],
        "citations": 32,
        "references": 91,
        "year": 2020
    },
    {
        "title": "Structural Patterns and Generative Models of Real-world Hypergraphs",
        "abstract": "Graphs have been utilized as a powerful tool to model pairwise relationships between people or objects. Such structure is a special type of a broader concept referred to as hypergraph, in which each hyperedge may consist of an arbitrary number of nodes, rather than just two. A large number of real-world datasets are of this form - for example, lists of recipients of emails sent from an organization, users participating in a discussion thread or subject labels tagged in an online question. However, due to complex representations and lack of adequate tools, little attention has been paid to exploring the underlying patterns in these interactions. In this work, we empirically study a number of real-world hypergraph datasets across various domains. In order to enable thorough investigations, we introduce the multi-level decomposition method, which represents each hypergraph by a set of pairwise graphs. Each pairwise graph, which we refer to as a k-level decomposed graph, captures the interactions between pairs of subsets of k nodes. We empirically find that at each decomposition level, the investigated hypergraphs obey five structural properties. These properties serve as criteria for evaluating how realistic a hypergraph is, and establish a foundation for the hypergraph generation problem. We also propose a hypergraph generator that is remarkably simple but capable of fulfilling these evaluation metrics, which are hardly achieved by other baseline generator models.",
        "authors": [
            "Do Manh Tuan",
            "Se-eun Yoon",
            "Bryan Hooi",
            "Kijung Shin"
        ],
        "citations": 66,
        "references": 54,
        "year": 2020
    },
    {
        "title": "Learning Generative Models across Incomparable Spaces",
        "abstract": "Generative Adversarial Networks have shown remarkable success in learning a distribution that faithfully recovers a reference distribution in its entirety. However, in some cases, we may want to only learn some aspects (e.g., cluster or manifold structure), while modifying others (e.g., style, orientation or dimension). In this work, we propose an approach to learn generative models across such incomparable spaces, and demonstrate how to steer the learned distribution towards target properties. A key component of our model is the Gromov-Wasserstein distance, a notion of discrepancy that compares distributions relationally rather than absolutely. While this framework subsumes current generative models in identically reproducing distributions, its inherent flexibility allows application to tasks in manifold learning, relational learning and cross-domain learning.",
        "authors": [
            "Charlotte Bunne",
            "David Alvarez-Melis",
            "A. Krause",
            "S. Jegelka"
        ],
        "citations": 108,
        "references": 58,
        "year": 2019
    },
    {
        "title": "Deep generative models for galaxy image simulations",
        "abstract": "\n Image simulations are essential tools for preparing and validating the analysis of current and future wide-field optical surveys. However, the galaxy models used as the basis for these simulations are typically limited to simple parametric light profiles, or use a fairly limited amount of available space-based data. In this work, we propose a methodology based on deep generative models to create complex models of galaxy morphologies that may meet the image simulation needs of upcoming surveys. We address the technical challenges associated with learning this morphology model from noisy and point spread function (PSF)-convolved images by building a hybrid Deep Learning/physical Bayesian hierarchical model for observed images, explicitly accounting for the PSF and noise properties. The generative model is further made conditional on physical galaxy parameters, to allow for sampling new light profiles from specific galaxy populations. We demonstrate our ability to train and sample from such a model on galaxy postage stamps from the HST/ACS COSMOS survey, and validate the quality of the model using a range of second- and higher order morphology statistics. Using this set of statistics, we demonstrate significantly more realistic morphologies using these deep generative models compared to conventional parametric models. To help make these generative models practical tools for the community, we introduce galsim-hub, a community-driven repository of generative models, and a framework for incorporating generative models within the galsim image simulation software.",
        "authors": [
            "F. Lanusse",
            "R. Mandelbaum",
            "Siamak Ravanbakhsh",
            "Chun-Liang Li",
            "P. Freeman",
            "B. Póczos"
        ],
        "citations": 29,
        "references": 62,
        "year": 2020
    },
    {
        "title": "Clustering Analysis via Deep Generative Models With Mixture Models",
        "abstract": "Clustering is a fundamental problem that frequently arises in many fields, such as pattern recognition, data mining, and machine learning. Although various clustering algorithms have been developed in the past, traditional clustering algorithms with shallow structures cannot excavate the interdependence of complex data features in latent space. Recently, deep generative models, such as autoencoder (AE), variational AE (VAE), and generative adversarial network (GAN), have achieved remarkable success in many unsupervised applications thanks to their capabilities for learning promising latent representations from original data. In this work, first we propose a novel clustering approach based on both Wasserstein GAN with gradient penalty (WGAN-GP) and VAE with a Gaussian mixture prior. By combining the WGAN-GP with VAE, the generator of WGAN-GP is formulated by drawing samples from the probabilistic decoder of VAE. Moreover, to provide more robust clustering and generation performance when outliers are encountered in data, a variant of the proposed deep generative model is developed based on a Student’s-t mixture prior. The effectiveness of our deep generative models is validated though experiments on both clustering analysis and samples generation. Through the comparison with other state-of-art clustering approaches based on deep generative models, the proposed approach can provide more stable training of the model, improve the accuracy of clustering, and generate realistic samples.",
        "authors": [
            "L. Yang",
            "Wentao Fan",
            "N. Bouguila"
        ],
        "citations": 25,
        "references": 0,
        "year": 2020
    },
    {
        "title": "Flow-based Deep Generative Models",
        "abstract": "In this report, we investigate the ﬂow-based deep generative models. We ﬁrst compare different generative models, especially generative adversarial networks (GANs), variational autoencoders (VAEs) and ﬂow-based generative models. We then survey different normalizing ﬂow models, including non-linear independent components estimation (NICE), real-valued non-volume preserving (RealNVP) transformations, generative ﬂow with invertible 1 × 1 convolutions (Glow), masked autoregressive ﬂow (MAF) and inverse autoregressive ﬂow (IAF). Finally, we conduct experiments on generating MNIST handwritten digits using NICE and RealNVP to examine the effectiveness of ﬂow-based models. Source code is available at https://github.com/salu133445/flows .",
        "authors": [
            "Jiarui Xu",
            "Hao-Wen Dong"
        ],
        "citations": 22,
        "references": 16,
        "year": 2020
    },
    {
        "title": "Event generation and statistical sampling for physics with deep generative models and a density information buffer",
        "abstract": null,
        "authors": [
            "S. Otten",
            "S. Caron",
            "Wieske de Swart",
            "Melissa van Beekveld",
            "L. Hendriks",
            "C. V. van Leeuwen",
            "Damian Podareanu",
            "R. Ruiz de austri",
            "R. Verheyen"
        ],
        "citations": 105,
        "references": 67,
        "year": 2019
    },
    {
        "title": "Generative models, linguistic communication and active inference",
        "abstract": null,
        "authors": [
            "Karl J. Friston",
            "Thomas Parr",
            "Y. Yufik",
            "Noor Sajid",
            "E. Holmes"
        ],
        "citations": 69,
        "references": 180,
        "year": 2020
    },
    {
        "title": "Flow-based generative models for Markov chain Monte Carlo in lattice field theory",
        "abstract": "A Markov chain update scheme using a machine-learned flow-based generative model is proposed for Monte Carlo sampling in lattice field theories. The generative model may be optimized (trained) to produce samples from a distribution approximating the desired Boltzmann distribution determined by the lattice action of the theory being studied. Training the model systematically improves autocorrelation times in the Markov chain, even in regions of parameter space where standard Markov chain Monte Carlo algorithms exhibit critical slowing down in producing decorrelated updates. Moreover, the model may be trained without existing samples from the desired distribution. The algorithm is compared with HMC and local Metropolis sampling for ϕ4 theory in two dimensions.",
        "authors": [
            "M. S. Albergo",
            "G. Kanwar",
            "P. Shanahan"
        ],
        "citations": 200,
        "references": 52,
        "year": 2019
    },
    {
        "title": "Generative Models",
        "abstract": null,
        "authors": [
            "Thomas P. Trappenberg"
        ],
        "citations": 86,
        "references": 31,
        "year": 2019
    },
    {
        "title": "Deep Generative Models",
        "abstract": null,
        "authors": [
            "N. Siddharth",
            "Brooks Paige",
            "Alban Desmaison",
            "Frank Wood"
        ],
        "citations": 42,
        "references": 55,
        "year": 2020
    },
    {
        "title": "Scalable Reversible Generative Models with Free-form Continuous Dynamics",
        "abstract": "A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson’s trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on highdimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.",
        "authors": [
            "Will Grathwohl"
        ],
        "citations": 115,
        "references": 20,
        "year": 2018
    },
    {
        "title": "Randomized SMILES strings improve the quality of molecular generative models",
        "abstract": null,
        "authors": [
            "Josep Arús‐Pous",
            "Simon Johansson",
            "Oleksii Prykhodko",
            "E. Bjerrum",
            "C. Tyrchan",
            "J. Reymond",
            "Hongming Chen",
            "O. Engkvist"
        ],
        "citations": 235,
        "references": 57,
        "year": 2019
    },
    {
        "title": "Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models",
        "abstract": "We present a new, fast and flexible pipeline for indoor scene synthesis that is based on deep convolutional generative models. Our method operates on a top-down image-based representation, and inserts objects iteratively into the scene by predict their category, location, orientation and size with separate neural network modules. Our pipeline naturally supports automatic completion of partial scenes, as well as synthesis of complete scenes, without any modifications. Our method is significantly faster than the previous image-based method, and generates results that outperforms it and other state-of-the-art deep generative scene models in terms of faithfulness to training data and perceived visual quality.",
        "authors": [
            "Daniel Ritchie",
            "Kai Wang",
            "Yu-An Lin"
        ],
        "citations": 137,
        "references": 35,
        "year": 2018
    },
    {
        "title": "Statistical Inference for Generative Models with Maximum Mean Discrepancy",
        "abstract": "While likelihood-based inference and its variants provide a statistically efficient and widely applicable approach to parametric inference, their application to models involving intractable likelihoods poses challenges. In this work, we study a class of minimum distance estimators for intractable generative models, that is, statistical models for which the likelihood is intractable, but simulation is cheap. The distance considered, maximum mean discrepancy (MMD), is defined through the embedding of probability measures into a reproducing kernel Hilbert space. We study the theoretical properties of these estimators, showing that they are consistent, asymptotically normal and robust to model misspecification. A main advantage of these estimators is the flexibility offered by the choice of kernel, which can be used to trade-off statistical efficiency and robustness. On the algorithmic side, we study the geometry induced by MMD on the parameter space and use this to introduce a novel natural gradient descent-like algorithm for efficient implementation of these estimators. We illustrate the relevance of our theoretical results on several classes of models including a discrete-time latent Markov process and two multivariate stochastic differential equation models.",
        "authors": [
            "François‐Xavier Briol",
            "A. Barp",
            "A. Duncan",
            "M. Girolami"
        ],
        "citations": 64,
        "references": 117,
        "year": 2019
    },
    {
        "title": "Generative Models from the perspective of Continual Learning",
        "abstract": "Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge. Our code is available online1.",
        "authors": [
            "Timothée Lesort",
            "Hugo Caselles-Dupré",
            "M. G. Ortiz",
            "A. Stoian",
            "David Filliat"
        ],
        "citations": 147,
        "references": 37,
        "year": 2018
    },
    {
        "title": "Probabilistic harmonization and annotation of single‐cell transcriptomics data with deep generative models",
        "abstract": "As single-cell transcriptomics becomes a mainstream technology, the natural next step is to integrate the accumulating data in order to achieve a common ontology of cell types and states. However, owing to various nuisance factors of variation, it is not straightforward how to compare gene expression levels across data sets and how to automatically assign cell type labels in a new data set based on existing annotations. In this manuscript, we demonstrate that our previously developed method, scVI, provides an effective and fully probabilistic approach for joint representation and analysis of cohorts of single-cell RNA-seq data sets, while accounting for uncertainty caused by biological and measurement noise. We also introduce single-cell ANnotation using Variational Inference (scANVI), a semi-supervised variant of scVI designed to leverage any available cell state annotations — for instance when only one data set in a cohort is annotated, or when only a few cells in a single data set can be labeled using marker genes. We demonstrate that scVI and scANVI compare favorably to the existing methods for data integration and cell state annotation in terms of accuracy, scalability, and adaptability to challenging settings such as a hierarchical structure of cell state labels. We further show that different from existing methods, scVI and scANVI represent the integrated datasets with a single generative model that can be directly used for any probabilistic decision making task, using differential expression as our case study. scVI and scANVI are available as open source software and can be readily used to facilitate cell state annotation and help ensure consistency and reproducibility across studies.",
        "authors": [
            "Chenling A. Xu",
            "Romain Lopez",
            "Edouard Mehlman",
            "J. Regier",
            "Michael I. Jordan",
            "N. Yosef"
        ],
        "citations": 287,
        "references": 126,
        "year": 2019
    },
    {
        "title": "Learning and Querying Fast Generative Models for Reinforcement Learning",
        "abstract": "A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed generative models that learn and operate on compact state representations, so-called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment from raw pixels. The computational speed-up of state-space models while maintaining high accuracy makes their application in RL feasible: We demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game MSPACMAN, demonstrating the potential of using learned environment models for planning.",
        "authors": [
            "Lars Buesing",
            "T. Weber",
            "S. Racanière",
            "S. Eslami",
            "Danilo Jimenez Rezende",
            "David P. Reichert",
            "Fabio Viola",
            "F. Besse",
            "Karol Gregor",
            "D. Hassabis",
            "D. Wierstra"
        ],
        "citations": 128,
        "references": 36,
        "year": 2018
    },
    {
        "title": "Causal deconvolution by algorithmic generative models",
        "abstract": null,
        "authors": [
            "H. Zenil",
            "N. Kiani",
            "Allan A. Zea",
            "J. Tegnér"
        ],
        "citations": 80,
        "references": 52,
        "year": 2019
    },
    {
        "title": "Analyzing the Training Processes of Deep Generative Models",
        "abstract": "Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.",
        "authors": [
            "Mengchen Liu",
            "Jiaxin Shi",
            "Kelei Cao",
            "Jun Zhu",
            "Shixia Liu"
        ],
        "citations": 149,
        "references": 52,
        "year": 2018
    },
    {
        "title": "Generative Models",
        "abstract": null,
        "authors": [
            "Cao Xiao",
            "Jimeng Sun"
        ],
        "citations": 0,
        "references": 58,
        "year": 2021
    },
    {
        "title": "Content and misrepresentation in hierarchical generative models",
        "abstract": null,
        "authors": [
            "Alex B. Kiefer",
            "J. Hohwy"
        ],
        "citations": 94,
        "references": 95,
        "year": 2018
    },
    {
        "title": "GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation",
        "abstract": "Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.",
        "authors": [
            "Minsuk Kahng",
            "Nikhil Thorat",
            "Duen Horng Chau",
            "F. Viégas",
            "M. Wattenberg"
        ],
        "citations": 160,
        "references": 52,
        "year": 2018
    },
    {
        "title": "Emergence of Object Segmentation in Perturbed Generative Models",
        "abstract": "We introduce a novel framework to build a model that can learn how to segment objects from a collection of images without any human annotation. Our method builds on the observation that the location of object segments can be perturbed locally relative to a given background without affecting the realism of a scene. Our approach is to first train a generative model of a layered scene. The layered representation consists of a background image, a foreground image and the mask of the foreground. A composite image is then obtained by overlaying the masked foreground image onto the background. The generative model is trained in an adversarial fashion against a discriminator, which forces the generative model to produce realistic composite images. To force the generator to learn a representation where the foreground layer corresponds to an object, we perturb the output of the generative model by introducing a random shift of both the foreground image and mask relative to the background. Because the generator is unaware of the shift before computing its output, it must produce layered representations that are realistic for any such random perturbation. Finally, we learn to segment an image by defining an autoencoder consisting of an encoder, which we train, and the pre-trained generator as the decoder, which we freeze. The encoder maps an image to a feature vector, which is fed as input to the generator to give a composite image matching the original input image. Because the generator outputs an explicit layered representation of the scene, the encoder learns to detect and segment objects. We demonstrate this framework on real images of several object categories.",
        "authors": [
            "Adam Bielski",
            "P. Favaro"
        ],
        "citations": 97,
        "references": 35,
        "year": 2019
    },
    {
        "title": "Deep generative models: Survey",
        "abstract": "Generative models have found their way to the forefront of deep learning the last decade and so far, it seems that the hype will not fade away any time soon. In this paper, we give an overview of the most important building blocks of most recent revolutionary deep generative models such as RBM, DBM, DBN, VAE and GAN. We will also take a look at three of state-of-the-art generative models, namely PixelRNN, DRAW and NADE. We will delve into their unique architectures, the learning procedures and their potential and limitations. We will also review some of the known issues that arise when trying to design and train deep generative architectures using shallow ones and how different models deal with these issues. This paper is not meant to be a comprehensive study of these models, but rather a starting point for those who bear an interest in the field.",
        "authors": [
            "Achraf Oussidi",
            "Azeddine Elhassouny"
        ],
        "citations": 145,
        "references": 39,
        "year": 2018
    },
    {
        "title": "The Anatomy of Inference: Generative Models and Brain Structure",
        "abstract": "To infer the causes of its sensations, the brain must call on a generative (predictive) model. This necessitates passing local messages between populations of neurons to update beliefs about hidden variables in the world beyond its sensory samples. It also entails inferences about how we will act. Active inference is a principled framework that frames perception and action as approximate Bayesian inference. This has been successful in accounting for a wide range of physiological and behavioral phenomena. Recently, a process theory has emerged that attempts to relate inferences to their neurobiological substrates. In this paper, we review and develop the anatomical aspects of this process theory. We argue that the form of the generative models required for inference constrains the way in which brain regions connect to one another. Specifically, neuronal populations representing beliefs about a variable must receive input from populations representing the Markov blanket of that variable. We illustrate this idea in four different domains: perception, planning, attention, and movement. In doing so, we attempt to show how appealing to generative models enables us to account for anatomical brain architectures. Ultimately, committing to an anatomical theory of inference ensures we can form empirical hypotheses that can be tested using neuroimaging, neuropsychological, and electrophysiological experiments.",
        "authors": [
            "Thomas Parr",
            "Karl J. Friston"
        ],
        "citations": 137,
        "references": 231,
        "year": 2018
    },
    {
        "title": "Bias and Generalization in Deep Generative Models: An Empirical Study",
        "abstract": "In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images. Inspired by experimental methods from cognitive psychology, we probe each learning algorithm with carefully designed training datasets to characterize when and how existing models generate novel attributes and their combinations. We identify similarities to human psychology and verify that these patterns are consistent across commonly used models and architectures.",
        "authors": [
            "Shengjia Zhao",
            "Hongyu Ren",
            "Arianna Yuan",
            "Jiaming Song",
            "Noah D. Goodman",
            "Stefano Ermon"
        ],
        "citations": 130,
        "references": 33,
        "year": 2018
    },
    {
        "title": "Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis",
        "abstract": "In recent years, Generative Adversarial Networks have achieved impressive results in photorealistic image synthesis. This progress nurtures hopes that one day the classical rendering pipeline can be replaced by efficient models that are learned directly from images. However, current image synthesis models operate in the 2D domain where disentangling 3D properties such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable and controllable representation. Our key hypothesis is that the image generation process should be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional. We define the new task of 3D controllable image synthesis and propose an approach for solving it by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt. changes in viewpoint or object pose. We further evaluate various 3D representations in terms of their usefulness for this challenging task.",
        "authors": [
            "Yiyi Liao",
            "Katja Schwarz",
            "L. Mescheder",
            "Andreas Geiger"
        ],
        "citations": 153,
        "references": 52,
        "year": 2019
    },
    {
        "title": "A quantum machine learning algorithm based on generative models",
        "abstract": "We propose a quantum learning algorithm for a quantum generative model and prove its advantages compared with classical models. Quantum computing and artificial intelligence, combined together, may revolutionize future technologies. A significant school of thought regarding artificial intelligence is based on generative models. Here, we propose a general quantum algorithm for machine learning based on a quantum generative model. We prove that our proposed model is more capable of representing probability distributions compared with classical generative models and has exponential speedup in learning and inference at least for some instances if a quantum computer cannot be efficiently simulated classically. Our result opens a new direction for quantum machine learning and offers a remarkable example where a quantum algorithm shows exponential improvement over classical algorithms in an important application field.",
        "authors": [
            "Xun Gao",
            "Z.-Y. Zhang",
            "Z.-Y. Zhang",
            "Luming Duan",
            "Luming Duan"
        ],
        "citations": 115,
        "references": 41,
        "year": 2018
    },
    {
        "title": "Understanding the Limitations of Conditional Generative Models",
        "abstract": "Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties: (1) Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled in-distribution inputs. Our theoretical result reveals that it is impossible to guarantee detectability of adversarially-perturbed inputs even for near-optimal generative classifiers. Experimentally, we find that while we are able to train robust models for MNIST, robustness completely breaks down on CIFAR10. We relate this failure to various undesirable model properties that can be traced to the maximum likelihood training objective. Despite being a common choice in the literature, our results indicate that likelihood-based conditional generative models may are surprisingly ineffective for robust classification.",
        "authors": [
            "Ethan Fetaya",
            "J. Jacobsen",
            "Will Grathwohl",
            "R. Zemel"
        ],
        "citations": 50,
        "references": 33,
        "year": 2019
    },
    {
        "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models",
        "abstract": "Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation— describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.",
        "authors": [
            "Rundi Wu",
            "Chang Xiao",
            "Changxi Zheng"
        ],
        "citations": 123,
        "references": 51,
        "year": 2021
    },
    {
        "title": "Generative Models for Automatic Chemical Design",
        "abstract": null,
        "authors": [
            "Daniel Schwalbe-Koda",
            "Rafael Gómez-Bombarelli"
        ],
        "citations": 76,
        "references": 236,
        "year": 2019
    },
    {
        "title": "Counterfactuals uncover the modular structure of deep generative models",
        "abstract": "Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement is too restrictive and propose instead a non-statistical framework that relies on counterfactual manipulations to uncover a modular structure of the network composed of disentangled groups of internal variables. Experiments with a variety of generative models trained on complex image datasets show the obtained modules can be used to design targeted interventions. This opens the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.",
        "authors": [
            "M. Besserve",
            "Rémy Sun",
            "B. Scholkopf"
        ],
        "citations": 104,
        "references": 49,
        "year": 2018
    },
    {
        "title": "Deep Verifier Networks: Verification of Deep Discriminative Models with Deep Generative Models",
        "abstract": "AI Safety is a major concern in many deep learning applications such as autonomous driving. Given a trained deep learning model, an important natural problem is how to reliably verify the model's prediction. In this paper, we propose a novel framework --- deep verifier networks (DVN) to detect unreliable inputs or predictions of deep discriminative models, using separately trained deep generative models. Our proposed model is based on conditional variational auto-encoders with disentanglement constraints to separate the label information from the latent representation. We give both intuitive and theoretical justifications for the model. Our verifier network is trained independently with the prediction model, which eliminates the need of retraining the verifier network for a new model. We test the verifier network on both out-of-distribution detection and adversarial example detection problems, as well as anomaly detection problems in structured prediction tasks such as image caption generation. We achieve state-of-the-art results in all of these problems.",
        "authors": [
            "Tong Che",
            "Xiaofeng Liu",
            "Site Li",
            "Yubin Ge",
            "Ruixiang Zhang",
            "Caiming Xiong",
            "Yoshua Bengio"
        ],
        "citations": 51,
        "references": 68,
        "year": 2019
    },
    {
        "title": "Multimeasurement Generative Models",
        "abstract": "We formally map the problem of sampling from an unknown distribution with a density in $\\mathbb{R}^d$ to the problem of learning and sampling a smoother density in $\\mathbb{R}^{Md}$ obtained by convolution with a fixed factorial kernel: the new density is referred to as M-density and the kernel as multimeasurement noise model (MNM). The M-density in $\\mathbb{R}^{Md}$ is smoother than the original density in $\\mathbb{R}^d$, easier to learn and sample from, yet for large $M$ the two problems are mathematically equivalent since clean data can be estimated exactly given a multimeasurement noisy observation using the Bayes estimator. To formulate the problem, we derive the Bayes estimator for Poisson and Gaussian MNMs in closed form in terms of the unnormalized M-density. This leads to a simple least-squares objective for learning parametric energy and score functions. We present various parametrization schemes of interest including one in which studying Gaussian M-densities directly leads to multidenoising autoencoders--this is the first theoretical connection made between denoising autoencoders and empirical Bayes in the literature. Samples in $\\mathbb{R}^d$ are obtained by walk-jump sampling (Saremi&Hyvarinen, 2019) via underdamped Langevin MCMC (walk) to sample from M-density and the multimeasurement Bayes estimation (jump). We study permutation invariant Gaussian M-densities on MNIST, CIFAR-10, and FFHQ-256 datasets, and demonstrate the effectiveness of this framework for realizing fast-mixing stable Markov chains in high dimensions.",
        "authors": [
            "Saeed Saremi",
            "R. Srivastava"
        ],
        "citations": 7,
        "references": 66,
        "year": 2021
    },
    {
        "title": "Discrete Flows: Invertible Generative Models of Discrete Data",
        "abstract": "While normalizing flows have led to significant advances in modeling high-dimensional continuous distributions, their applicability to discrete distributions remains unknown. In this paper, we show that flows can in fact be extended to discrete events---and under a simple change-of-variables formula not requiring log-determinant-Jacobian computations. Discrete flows have numerous applications. We consider two flow architectures: discrete autoregressive flows that enable bidirectionality, allowing, for example, tokens in text to depend on both left-to-right and right-to-left contexts in an exact language model; and discrete bipartite flows that enable efficient non-autoregressive generation as in RealNVP. Empirically, we find that discrete autoregressive flows outperform autoregressive baselines on synthetic discrete distributions, an addition task, and Potts models; and bipartite flows can obtain competitive performance with autoregressive baselines on character-level language modeling for Penn Tree Bank and text8.",
        "authors": [
            "Dustin Tran",
            "Keyon Vafa",
            "Kumar Krishna Agrawal",
            "Laurent Dinh",
            "Ben Poole"
        ],
        "citations": 112,
        "references": 43,
        "year": 2019
    },
    {
        "title": "Dataless training of generative models for the inverse design of metasurfaces",
        "abstract": "Metasurfaces are subwavelength-structured artificial media that can shape and localize electromagnetic waves in unique ways. The inverse design of metasurfaces is a non-convex optimization problem in a high dimensional space, making global optimization a huge challenge. We present a new type of global optimization algorithm, based on the training of a generative neural network without a training set, which can produce high-performance metasurfaces. Instead of directly optimizing devices one at a time, we reframe the optimization as the training of a generator that iteratively enhances the probability of generating high-performance devices. The loss function used for backpropagation is defined as a function of generated patterns and their efficiency gradients, which are calculated by the adjoint variable method using the forward and adjoint electromagnetic simulations. We observe that distributions of devices generated by the network continuously shift towards high-performance design space regions over the course of optimization. Upon training completion, the best-generated devices have efficiencies comparable to or exceeding the best devices designed using standard topology optimization. We envision that our proposed global optimization algorithm generally applies to other gradient-based optimization problems in optics, mechanics and electronics.",
        "authors": [
            "Jiaqi Jiang",
            "Jonathan A. Fan"
        ],
        "citations": 88,
        "references": 38,
        "year": 2019
    },
    {
        "title": "A path towards quantum advantage in training deep generative models with quantum annealers",
        "abstract": "The development of quantum-classical hybrid (QCH) algorithms is critical to achieve state-of-the-art computational models. A QCH variational autoencoder (QVAE) was introduced in reference [] by some of the authors of this paper. QVAE consists of a classical auto-encoding structure realized by traditional deep neural networks to perform inference to and generation from, a discrete latent space. The latent generative process is formalized as thermal sampling from a quantum Boltzmann machine (QBM). This setup allows quantum-assisted training of deep generative models by physically simulating the generative process with quantum annealers. In this paper, we have successfully employed D-Wave quantum annealers as Boltzmann samplers to perform quantum-assisted, end-to-end training of QVAE. The hybrid structure of QVAE allows us to deploy current-generation quantum annealers in QCH generative models to achieve competitive performance on datasets such as MNIST. The results presented in this paper suggest that commercially available quantum annealers can be deployed, in conjunction with well-crafted classical deep neutral networks, to achieve competitive results in unsupervised and semisupervised tasks on large-scale datasets. We also provide evidence that our setup is able to exploit large latent-space QBMs, which develop slowly mixing modes. This expressive latent space results in slow and inefficient classical sampling and paves the way to achieve quantum advantage with quantum annealing in realistic sampling applications.",
        "authors": [
            "Walter Winci",
            "L. Buffoni",
            "Hossein Sadeghi",
            "Amir Khoshaman",
            "E. Andriyash",
            "Mohammad H. Amin"
        ],
        "citations": 58,
        "references": 74,
        "year": 2019
    },
    {
        "title": "Improving Missing Data Imputation with Deep Generative Models",
        "abstract": "Datasets with missing values are very common on industry applications, and they can have a negative impact on machine learning models. Recent studies introduced solutions to the problem of imputing missing values based on deep generative models. Previous experiments with Generative Adversarial Networks and Variational Autoencoders showed interesting results in this domain, but it is not clear which method is preferable for different use cases. The goal of this work is twofold: we present a comparison between missing data imputation solutions based on deep generative models, and we propose improvements over those methodologies. We run our experiments using known real life datasets with different characteristics, removing values at random and reconstructing them with several imputation techniques. Our results show that the presence or absence of categorical variables can alter the selection of the best model, and that some models are more stable than others after similar runs with different random number generator seeds.",
        "authors": [
            "R. Camino",
            "Christian Hammerschmidt",
            "R. State"
        ],
        "citations": 52,
        "references": 36,
        "year": 2019
    },
    {
        "title": "Learning Localized Generative Models for 3D Point Clouds via Graph Convolution",
        "abstract": "Point clouds are an important type of geometric data and have widespread use in computer graphics and vision. However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space. Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation. This paper studies the unsupervised problem of a generative model exploiting graph convolution. We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator. The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry. We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.",
        "authors": [
            "D. Valsesia",
            "Giulia Fracastoro",
            "E. Magli"
        ],
        "citations": 177,
        "references": 27,
        "year": 2018
    },
    {
        "title": "Glow: Generative Flow with Invertible 1x1 Convolutions",
        "abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at this https URL",
        "authors": [
            "Diederik P. Kingma",
            "Prafulla Dhariwal"
        ],
        "citations": 1000,
        "references": 29,
        "year": 2018
    },
    {
        "title": "Modeling Sparse Deviations for Compressed Sensing using Generative Models",
        "abstract": "In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model. A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined. We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals. Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior. Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.",
        "authors": [
            "Manik Dhar",
            "Aditya Grover",
            "Stefano Ermon"
        ],
        "citations": 77,
        "references": 49,
        "year": 2018
    },
    {
        "title": "Conditional molecular design with deep generative models",
        "abstract": "Although machine learning has been successfully used to propose novel molecules that satisfy desired properties, it is still challenging to explore a large chemical space efficiently. In this paper, we present a conditional molecular design method that facilitates generating new molecules with desired properties. The proposed model, which simultaneously performs both property prediction and molecule generation, is built as a semisupervised variational autoencoder trained on a set of existing molecules with only a partial annotation. We generate new molecules with desired properties by sampling from the generative distribution estimated by the model. We demonstrate the effectiveness of the proposed model by evaluating it on drug-like molecules. The model improves the performance of property prediction by exploiting unlabeled molecules and efficiently generates novel molecules fulfilling various target conditions.",
        "authors": [
            "Seokho Kang",
            "Kyunghyun Cho"
        ],
        "citations": 172,
        "references": 51,
        "year": 2018
    },
    {
        "title": "Inverting Deep Generative models, One layer at a time",
        "abstract": "We study the problem of inverting a deep generative model with ReLU activations. Inversion corresponds to finding a latent code vector that explains observed measurements as much as possible. In most prior works this is performed by attempting to solve a non-convex optimization problem involving the generator. In this paper we obtain several novel theoretical results for the inversion problem. \nWe show that for the realizable case, single layer inversion can be performed exactly in polynomial time, by solving a linear program. Further, we show that for multiple layers, inversion is NP-hard and the pre-image set can be non-convex. \nFor generative models of arbitrary depth, we show that exact recovery is possible in polynomial time with high probability, if the layers are expanding and the weights are randomly selected. Very recent work analyzed the same problem for gradient descent inversion. Their analysis requires significantly higher expansion (logarithmic in the latent dimension) while our proposed algorithm can provably reconstruct even with constant factor expansion. We also provide provable error bounds for different norms for reconstructing noisy observations. Our empirical validation demonstrates that we obtain better reconstructions when the latent dimension is large.",
        "authors": [
            "Qi Lei",
            "A. Jalal",
            "I. Dhillon",
            "A. Dimakis"
        ],
        "citations": 49,
        "references": 27,
        "year": 2019
    },
    {
        "title": "On the evaluation of generative models in music",
        "abstract": null,
        "authors": [
            "Li-Chia Yang",
            "Alexander Lerch"
        ],
        "citations": 137,
        "references": 68,
        "year": 2018
    },
    {
        "title": "Latent Space Oddity: on the Curvature of Deep Generative Models",
        "abstract": "Deep generative models provide a systematic way to learn nonlinear data distributions, through a set of latent variables and a nonlinear \"generator\" function that maps latent points into the input space. The nonlinearity of the generator imply that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalize to other deep generative models.",
        "authors": [
            "Georgios Arvanitidis",
            "L. K. Hansen",
            "Søren Hauberg"
        ],
        "citations": 246,
        "references": 20,
        "year": 2017
    },
    {
        "title": "Deep Generative Models for Distribution-Preserving Lossy Compression",
        "abstract": "We propose and study the problem of distribution-preserving lossy compression. Motivated by recent advances in extreme image compression which allow to maintain artifact-free reconstructions even at very low bitrates, we propose to optimize the rate-distortion tradeoff under the constraint that the reconstructed samples follow the distribution of the training data. The resulting compression system recovers both ends of the spectrum: On one hand, at zero bitrate it learns a generative model of the data, and at high enough bitrates it achieves perfect reconstruction. Furthermore, for intermediate bitrates it smoothly interpolates between learning a generative model of the training data and perfectly reconstructing the training samples. We study several methods to approximately solve the proposed optimization problem, including a novel combination of Wasserstein GAN and Wasserstein Autoencoder, and present an extensive theoretical and empirical characterization of the proposed compression systems.",
        "authors": [
            "M. Tschannen",
            "E. Agustsson",
            "Mario Lucic"
        ],
        "citations": 120,
        "references": 46,
        "year": 2018
    },
    {
        "title": "AdaGAN: Boosting Generative Models",
        "abstract": "Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.",
        "authors": [
            "I. Tolstikhin",
            "S. Gelly",
            "O. Bousquet",
            "Carl-Johann Simon-Gabriel",
            "B. Scholkopf"
        ],
        "citations": 219,
        "references": 23,
        "year": 2017
    },
    {
        "title": "Adversarial Examples for Generative Models",
        "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.",
        "authors": [
            "Jernej Kos",
            "Ian Fischer",
            "D. Song"
        ],
        "citations": 265,
        "references": 28,
        "year": 2017
    },
    {
        "title": "Generative Models",
        "abstract": null,
        "authors": [
            "Sim-Hui Tee"
        ],
        "citations": 0,
        "references": 107,
        "year": 2020
    },
    {
        "title": "Deep Generative Models with Learnable Knowledge Constraints",
        "abstract": "The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified {\\it a priori}, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.",
        "authors": [
            "Zhiting Hu",
            "Zichao Yang",
            "R. Salakhutdinov",
            "Xiaodan Liang",
            "Lianhui Qin",
            "Haoye Dong",
            "E. Xing"
        ],
        "citations": 75,
        "references": 63,
        "year": 2018
    },
    {
        "title": "Deep Generative Models for Molecular Science",
        "abstract": "Generative deep machine learning models now rival traditional quantum‐mechanical computations in predicting properties of new structures, and they come with a significantly lower computational cost, opening new avenues in computational molecular science. In the last few years, a variety of deep generative models have been proposed for modeling molecules, which differ in both their model structure and choice of input features. We review these recent advances within deep generative models for predicting molecular properties, with particular focus on models based on the probabilistic autoencoder (or variational autoencoder, VAE) approach in which the molecular structure is embedded in a latent vector space from which its properties can be predicted and its structure can be restored.",
        "authors": [
            "P. B. Jørgensen",
            "Mikkel N. Schmidt",
            "O. Winther"
        ],
        "citations": 68,
        "references": 35,
        "year": 2018
    },
    {
        "title": "Auto-encoder-based generative models for data augmentation on regression problems",
        "abstract": null,
        "authors": [
            "H. Ohno"
        ],
        "citations": 35,
        "references": 51,
        "year": 2019
    },
    {
        "title": "Learning Implicit Generative Models by Matching Perceptual Features",
        "abstract": "Perceptual features (PFs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. However, the efficacy of PFs as key source of information for learning generative models is not well studied. We investigate here the use of PFs in the context of learning implicit generative models through moment matching (MM). More specifically, we propose a new effective MM approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained ConvNets. Our proposed approach improves upon existing MM methods by: (1) breaking away from the problematic min/max game of adversarial learning; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. Our experimental results demonstrate that, due to the expressiveness of PFs from pretrained deep ConvNets, our method achieves state-of-the-art results for challenging benchmarks.",
        "authors": [
            "C. D. Santos",
            "Youssef Mroueh",
            "Inkit Padhi",
            "Pierre L. Dognin"
        ],
        "citations": 27,
        "references": 44,
        "year": 2019
    },
    {
        "title": "Exploring the GDB-13 chemical space using deep generative models",
        "abstract": null,
        "authors": [
            "Josep Arús‐Pous",
            "T. Blaschke",
            "Silas Ulander",
            "J. Reymond",
            "Hongming Chen",
            "O. Engkvist"
        ],
        "citations": 136,
        "references": 53,
        "year": 2018
    },
    {
        "title": "Advances and challenges in deep generative models for de novo molecule generation",
        "abstract": "The de novo molecule generation problem involves generating novel or modified molecular structures with desirable properties. Taking advantage of the great representation learning ability of deep learning models, deep generative models, which differ from discriminative models in their traditional machine learning approach, provide the possibility of generation of desirable molecules directly. Although deep generative models have been extensively discussed in the machine learning community, a specific investigation of the computational issues related to deep generative models for de novo molecule generation is needed. A concise and insightful discussion of recent advances in applying deep generative models for de novo molecule generation is presented, with particularly emphasizing the most important challenges for successful application of deep generative models in this specific area.",
        "authors": [
            "Dongyu Xue",
            "Yukang Gong",
            "Zhao-Yi Yang",
            "Guohui Chuai",
            "Sheng Qu",
            "Ai-Zong Shen",
            "Jing Yu",
            "Qi Liu"
        ],
        "citations": 68,
        "references": 48,
        "year": 2018
    },
    {
        "title": "Group Anomaly Detection using Deep Generative Models",
        "abstract": null,
        "authors": [
            "Raghavendra Chalapathy",
            "E. Toth",
            "S. Chawla"
        ],
        "citations": 58,
        "references": 33,
        "year": 2018
    },
    {
        "title": "Adaptive Density Estimation for Generative Models",
        "abstract": "Unsupervised learning of generative models has seen tremendous progress over recent years, in particular due to generative adversarial networks (GANs), variational autoencoders, and flow-based models. GANs have dramatically improved sample quality, but suffer from two drawbacks: (i) they mode-drop, \\ie, do not cover the full support of the train data, and (ii) they do not allow for likelihood evaluations on held-out data. In contrast likelihood-based training encourages models to cover the full support of the train data, but yields poorer samples. These mutual shortcomings can in principle be addressed by training generative latent variable models in a hybrid adversarial-likelihood manner. However, we show that commonly made parametric assumptions create a conflict between them, making successful hybrid models non trivial. As a solution, we propose the use of deep invertible transformations in the latent variable decoder. This approach allows for likelihood computations in image space, is more efficient than fully invertible models, and can take full advantage of adversarial training. We show that our model significantly improves over existing hybrid models: offering GAN-like samples, IS and FID scores that are competitive with fully adversarial models and improved likelihood scores.",
        "authors": [
            "Thomas Lucas",
            "K. Shmelkov",
            "Alahari Karteek",
            "C. Schmid",
            "Jakob Verbeek"
        ],
        "citations": 31,
        "references": 55,
        "year": 2019
    },
    {
        "title": "Diffusion Models Beat GANs on Image Synthesis",
        "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
        "authors": [
            "Prafulla Dhariwal",
            "Alex Nichol"
        ],
        "citations": 1000,
        "references": 81,
        "year": 2021
    },
    {
        "title": "Mathematical Generative Models",
        "abstract": null,
        "authors": [
            "Kumiko Tanaka-Ishii"
        ],
        "citations": 0,
        "references": 16,
        "year": 2021
    },
    {
        "title": "Learning Neurosymbolic Generative Models via Program Synthesis",
        "abstract": "Significant strides have been made toward designing better generative models in recent years. Despite this progress, however, state-of-the-art approaches are still largely unable to capture complex global structure in data. For example, images of buildings typically contain spatial patterns such as windows repeating at regular intervals; state-of-the-art generative methods can't easily reproduce these structures. We propose to address this problem by incorporating programs representing global structure into the generative model---e.g., a 2D for-loop may represent a configuration of windows. Furthermore, we propose a framework for learning these models by leveraging program synthesis to generate training data. On both synthetic and real-world data, we demonstrate that our approach is substantially better than the state-of-the-art at both generating and completing images that contain global structure.",
        "authors": [
            "Halley Young",
            "O. Bastani",
            "M. Naik"
        ],
        "citations": 30,
        "references": 33,
        "year": 2019
    },
    {
        "title": "Deep Generative Models in the Real-World: An Open Challenge from Medical Imaging",
        "abstract": "Recent advances in deep learning led to novel generative modeling techniques that achieve unprecedented quality in generated samples and performance in learning complex distributions in imaging data. These new models in medical image computing have important applications that form clinically relevant and very challenging unsupervised learning problems. In this paper, we explore the feasibility of using state-of-the-art auto-encoder-based deep generative models, such as variational and adversarial auto-encoders, for one such task: abnormality detection in medical imaging. We utilize typical, publicly available datasets with brain scans from healthy subjects and patients with stroke lesions and brain tumors. We use the data from healthy subjects to train different auto-encoder based models to learn the distribution of healthy images and detect pathologies as outliers. Models that can better learn the data distribution should be able to detect outliers more accurately. We evaluate the detection performance of deep generative models and compare them with non-deep learning based approaches to provide a benchmark of the current state of research. We conclude that abnormality detection is a challenging task for deep generative models and large room exists for improvement. In order to facilitate further research, we aim to provide carefully pre-processed imaging data available to the research community.",
        "authors": [
            "Xiaoran Chen",
            "Nick Pawlowski",
            "Martin Rajchl",
            "Ben Glocker",
            "E. Konukoglu"
        ],
        "citations": 49,
        "references": 46,
        "year": 2018
    },
    {
        "title": "Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance",
        "abstract": "Minimum expected distance estimation (MEDE) algorithms have been widely used for probabilistic models with intractable likelihood functions and they have become increasingly popular due to their use in implicit generative modeling (e.g. Wasserstein generative adversarial networks, Wasserstein autoencoders). Emerging from computational optimal transport, the Sliced-Wasserstein (SW) distance has become a popular choice in MEDE thanks to its simplicity and computational benefits. While several studies have reported empirical success on generative modeling with SW, the theoretical properties of such estimators have not yet been established. In this study, we investigate the asymptotic properties of estimators that are obtained by minimizing SW. We first show that convergence in SW implies weak convergence of probability measures in general Wasserstein spaces. Then we show that estimators obtained by minimizing SW (and also an approximate version of SW) are asymptotically consistent. We finally prove a central limit theorem, which characterizes the asymptotic distribution of the estimators and establish a convergence rate of $\\sqrt{n}$, where $n$ denotes the number of observed data points. We illustrate the validity of our theory on both synthetic data and neural networks.",
        "authors": [
            "Kimia Nadjahi",
            "Alain Durmus",
            "Umut Simsekli",
            "R. Badeau"
        ],
        "citations": 60,
        "references": 44,
        "year": 2019
    },
    {
        "title": "Cosmological N-body simulations: a challenge for scalable generative models",
        "abstract": null,
        "authors": [
            "Nathanael Perraudin",
            "Ankit Srivastava",
            "Aurélien Lucchi",
            "T. Kacprzak",
            "T. Hofmann",
            "A. Réfrégier"
        ],
        "citations": 32,
        "references": 72,
        "year": 2019
    },
    {
        "title": "Performing Co-membership Attacks Against Deep Generative Models",
        "abstract": "In this paper we propose a new membership attack method called co-membership attacks against deep generative models including Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). Specifically, membership attack aims to check whether a given instance x was used in the training data or not. A co-membership attack checks whether the given bundle of n instances were in the training, with the prior knowledge that the bundle was either entirely used in the training or none at all. Successful membership attacks can compromise the privacy of training data when the generative model is published. Our main idea is to cast membership inference of target data x as the optimization of another neural network (called the attacker network) to search for the latent encoding to reproduce x. The final reconstruction error is used directly to conclude whether x was in the training data or not. We conduct extensive experiments on a variety of datasets and generative models showing that: our attacker network outperforms prior membership attacks; co-membership attacks can be substantially more powerful than single attacks; and VAEs are more susceptible to membership attacks compared to GANs.",
        "authors": [
            "Kin Sum Liu",
            "Chaowei Xiao",
            "Bo Li",
            "Jie Gao"
        ],
        "citations": 54,
        "references": 28,
        "year": 2018
    },
    {
        "title": "The Riemannian Geometry of Deep Generative Models",
        "abstract": "Deep generative models learn a mapping from a low-dimensional latent space to a high-dimensional data space. Under certain regularity conditions, these models parameterize nonlinear manifolds in the data space. In this paper, we investigate the Riemannian geometry of these generated manifolds. First, we develop efficient algorithms for computing geodesic curves, which provide an intrinsic notion of distance between points on the manifold. Second, we develop an algorithm for parallel translation of a tangent vector along a path on the manifold. We show how parallel translation can be used to generate analogies, i.e., to transport a change in one data point into a semantically similar change of another data point. Our experiments on real image data show that the manifolds learned by deep generative models, while nonlinear, are surprisingly close to zero curvature. The practical implication is that linear paths in the latent space closely approximate geodesics on the generated manifold.",
        "authors": [
            "Hang Shao",
            "Abhishek Kumar",
            "P. Fletcher"
        ],
        "citations": 166,
        "references": 25,
        "year": 2017
    },
    {
        "title": "Conditional Generative Models are not Robust",
        "abstract": "Class-conditional generative models are an increasingly popular approach to achieve robust classification. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based conditional generative models from a theoretical and practical perspective. Our theoretical result reveals that it is impossible to guarantee detectability of adversarial examples even for near-optimal generative classifiers. Experimentally, we show that naively trained conditional generative models have poor discriminative performance, making them unsuitable for classification. This is related to overlooked issues with training conditional generative models and we show methods to improve performance. Finally, we analyze the robustness of our proposed conditional generative models on MNIST and CIFAR10. While we are able to train robust models for MNIST, robustness completely breaks down on CIFAR10. This lack of robustness is related to various undesirable model properties maximum likelihood fails to penalize. Our results indicate that likelihood may fundamentally be at odds with robust classification on challenging problems.",
        "authors": [
            "Ethan Fetaya",
            "J. Jacobsen",
            "R. Zemel"
        ],
        "citations": 15,
        "references": 29,
        "year": 2019
    },
    {
        "title": "Learning the Structure of Generative Models without Labeled Data",
        "abstract": "Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the ℓ 1-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100× faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.",
        "authors": [
            "Stephen H. Bach",
            "Bryan D. He",
            "Alexander J. Ratner",
            "C. Ré"
        ],
        "citations": 157,
        "references": 37,
        "year": 2017
    },
    {
        "title": "Physics-informed deep generative models",
        "abstract": "We consider the application of deep generative models in propagating uncertainty through complex physical systems. Specifically, we put forth an implicit variational inference formulation that constrains the generative model output to satisfy given physical laws expressed by partial differential equations. Such physics-informed constraints provide a regularization mechanism for effectively training deep probabilistic models for modeling physical systems in which the cost of data acquisition is high and training data-sets are typically small. This provides a scalable framework for characterizing uncertainty in the outputs of physical systems due to randomness in their inputs or noise in their observations. We demonstrate the effectiveness of our approach through a canonical example in transport dynamics.",
        "authors": [
            "Yibo Yang",
            "P. Perdikaris"
        ],
        "citations": 56,
        "references": 32,
        "year": 2018
    },
    {
        "title": "Characterizing Bias in Classifiers using Generative Models",
        "abstract": "Models that are learned from real-world data are often biased because the data used to train them is biased. This can propagate systemic human biases that exist and ultimately lead to inequitable treatment of people, especially minorities. To characterize bias in learned classifiers, existing approaches rely on human oracles labeling real-world examples to identify the \"blind spots\" of the classifiers; these are ultimately limited due to the human labor required and the finite nature of existing image examples. We propose a simulation-based approach for interrogating classifiers using generative adversarial models in a systematic manner. We incorporate a progressive conditional generative model for synthesizing photo-realistic facial images and Bayesian Optimization for an efficient interrogation of independent facial image classification systems. We show how this approach can be used to efficiently characterize racial and gender biases in commercial systems.",
        "authors": [
            "Daniel J. McDuff",
            "Shuang Ma",
            "Yale Song",
            "Ashish Kapoor"
        ],
        "citations": 43,
        "references": 49,
        "year": 2019
    },
    {
        "title": "Differentially Private Data Generative Models",
        "abstract": "Deep neural networks (DNNs) have recently been widely adopted in various applications, and such success is largely due to a combination of algorithmic breakthroughs, computation resource improvements, and access to a large amount of data. However, the large-scale data collections required for deep learning often contain sensitive information, therefore raising many privacy concerns. Prior research has shown several successful attacks in inferring sensitive training data information, such as model inversion, membership inference, and generative adversarial networks (GAN) based leakage attacks against collaborative deep learning. In this paper, to enable learning efficiency as well as to generate data with privacy guarantees and high utility, we propose a differentially private autoencoder-based generative model (DP-AuGM) and a differentially private variational autoencoder-based generative model (DP-VaeGM). We evaluate the robustness of two proposed models. We show that DP-AuGM can effectively defend against the model inversion, membership inference, and GAN-based attacks. We also show that DP-VaeGM is robust against the membership inference attack. We conjecture that the key to defend against the model inversion and GAN-based attacks is not due to differential privacy but the perturbation of training data. Finally, we demonstrate that both DP-AuGM and DP-VaeGM can be easily integrated with real-world machine learning applications, such as machine learning as a service and federated learning, which are otherwise threatened by the membership inference attack and the GAN-based attack, respectively.",
        "authors": [
            "Qingrong Chen",
            "Chong Xiang",
            "Minhui Xue",
            "Bo Li",
            "N. Borisov",
            "Dali Kaafar",
            "Haojin Zhu"
        ],
        "citations": 76,
        "references": 64,
        "year": 2018
    },
    {
        "title": "Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models",
        "abstract": "Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial for the cross-modal retrieval performance. Unlike existing image-text retrieval approaches that embed image-text pairs as single feature vectors in a common representational space, we propose to incorporate generative processes into the cross-modal feature embedding, through which we are able to learn not only the global abstract features but also the local grounded features. Extensive experiments show that our framework can well match images and sentences with complex content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset.",
        "authors": [
            "Jiuxiang Gu",
            "Jianfei Cai",
            "Shafiq R. Joty",
            "Li Niu",
            "G. Wang"
        ],
        "citations": 351,
        "references": 41,
        "year": 2017
    },
    {
        "title": "Metrics for Deep Generative Models",
        "abstract": "Neural samplers such as variational autoencoders (VAEs) or generative adversarial networks (GANs) approximate distributions by transforming samples from a simple random source---the latent space---to samples from a more complex distribution represented by a dataset. While the manifold hypothesis implies that the density induced by a dataset contains large regions of low density, the training criterions of VAEs and GANs will make the latent space densely covered. Consequently points that are separated by low-density regions in observation space will be pushed together in latent space, making stationary distances poor proxies for similarity. We transfer ideas from Riemannian geometry to this setting, letting the distance between two points be the shortest path on a Riemannian manifold induced by the transformation. The method yields a principled distance measure, provides a tool for visual inspection of deep generative models, and an alternative to linear interpolation in latent space. In addition, it can be applied for robot movement generalization using previously learned skills. The method is evaluated on a synthetic dataset with known ground truth; on a simulated robot arm dataset; on human motion capture data; and on a generative model of handwritten digits.",
        "authors": [
            "Nutan Chen",
            "Alexej Klushyn",
            "Richard Kurle",
            "Xueyan Jiang",
            "Justin Bayer",
            "Patrick van der Smagt"
        ],
        "citations": 110,
        "references": 36,
        "year": 2017
    },
    {
        "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
        "abstract": "We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.",
        "authors": [
            "Yang Song",
            "Stefano Ermon"
        ],
        "citations": 1000,
        "references": 68,
        "year": 2019
    },
    {
        "title": "Generating and designing DNA with deep generative models",
        "abstract": "We propose generative neural network methods to generate DNA sequences and tune them to have desired properties. We present three approaches: creating synthetic DNA sequences using a generative adversarial network; a DNA-based variant of the activation maximization (\"deep dream\") design method; and a joint procedure which combines these two approaches together. We show that these tools capture important structures of the data and, when applied to designing probes for protein binding microarrays, allow us to generate new sequences whose properties are estimated to be superior to those found in the training data. We believe that these results open the door for applying deep generative models to advance genomics research.",
        "authors": [
            "N. Killoran",
            "Leo J. Lee",
            "Andrew Delong",
            "D. Duvenaud",
            "B. Frey"
        ],
        "citations": 141,
        "references": 40,
        "year": 2017
    },
    {
        "title": "On Unifying Deep Generative Models",
        "abstract": "Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transferred techniques.",
        "authors": [
            "Zhiting Hu",
            "Zichao Yang",
            "R. Salakhutdinov",
            "E. Xing"
        ],
        "citations": 126,
        "references": 66,
        "year": 2017
    },
    {
        "title": "Histopathology stain-color normalization using deep generative models",
        "abstract": "Performance of designed CAD algorithms for histopathology image analysis is affected by the amount of variations in the samples such as color and intensity of stained images. Stain-color normalization is a well-studied technique for compensating such effects at the input of CAD systems. In this paper, we introduce unsupervised generative neural networks for performing stain-color normalization. For color normalization in stained hematoxylin and eosin (H&E) images, we present three methods based on three frameworks for deep generative models: variational auto-encoder (VAE), generative adversarial networks (GAN) and deep convolutional Gaussian mixture models (DCGMM). Our contribution is defining the color normalization as a learning generative model that is able to generate various color copies of the input image through a nonlinear parametric transformation. In contrast to earlier generative models proposed for stain-color normalization, our approach does not need any labels for data or any other assumptions about the H&E image content. Furthermore, our models learn a parametric transformation during training and can convert the color information of an input image to resemble any arbitrary reference image. This property is essential in time-critical CAD systems in case of changing the reference image, since our approach does not need retraining in contrast to other proposed generative models for stain-color normalization. Experiments on histopathological H&E images with high staining variations, collected from different laboratories, show that our proposed models outperform quantitatively state-of-the-art methods in the measure of color constancy with at least 10-15%, while the converted images are visually in agreement with this performance improvement.",
        "authors": [
            "F. G. Zanjani",
            "S. Zinger",
            "B. Bejnordi",
            "J. Laak"
        ],
        "citations": 43,
        "references": 35,
        "year": 2018
    },
    {
        "title": "Flexible and accurate inference and learning for deep generative models",
        "abstract": "We introduce a new approach to learning in hierarchical latent-variable generative models called the “distributed distributional code Helmholtz machine”, which emphasises flexibility and accuracy in the inferential process. Like the original Helmholtz machine and later variational autoencoder algorithms (but unlike adver- sarial methods) our approach learns an explicit inference or “recognition” model to approximate the posterior distribution over the latent variables. Unlike these earlier methods, it employs a posterior representation that is not limited to a narrow tractable parametrised form (nor is it represented by samples). To train the genera- tive and recognition models we develop an extended wake-sleep algorithm inspired by the original Helmholtz machine. This makes it possible to learn hierarchical latent models with both discrete and continuous variables, where an accurate poste- rior representation is essential. We demonstrate that the new algorithm outperforms current state-of-the-art methods on synthetic, natural image patch and the MNIST data sets.",
        "authors": [
            "Eszter Vértes",
            "M. Sahani"
        ],
        "citations": 44,
        "references": 25,
        "year": 2018
    },
    {
        "title": "Recent Trends in Deep Generative Models: a Review",
        "abstract": "With the recent improvements in computation power and high scale datasets, many interesting studies have been presented based on discriminative models such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) for various classification problems. These models have achieved current state-of-the-art results in almost all applications of computer vision but not sufficient sampling out-of-data, understanding of data distribution. By pioneers of the deep learning community, generative adversarial training is defined as the most exciting topic of computer vision field nowadays. With the influence of these views and potential usages of generative models, many kinds of researches were conducted using generative models especially Generative Adversarial Network (GAN) and Autoencoder (AE) based models with an increasing trend. In this study, a comprehensive review of generative models with defining relations among them is presented for a better understanding of GANs and AEs by pointing the importance of generative models.",
        "authors": [
            "C. G. Turhan",
            "H. Ş. Bilge"
        ],
        "citations": 44,
        "references": 64,
        "year": 2018
    },
    {
        "title": "Learning Disentangled Representations with Semi-Supervised Deep Generative Models",
        "abstract": "Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.",
        "authors": [
            "Siddharth Narayanaswamy",
            "Brooks Paige",
            "Jan-Willem van de Meent",
            "Alban Desmaison",
            "Noah D. Goodman",
            "Pushmeet Kohli",
            "Frank D. Wood",
            "Philip H. S. Torr"
        ],
        "citations": 354,
        "references": 37,
        "year": 2017
    },
    {
        "title": "GHUM & GHUML: Generative 3D Human Shape and Articulated Pose Models",
        "abstract": "We present a statistical, articulated 3D human shape modeling pipeline, within a fully trainable, modular, deep learning framework. Given high-resolution complete 3D body scans of humans, captured in various poses, together with additional closeups of their head and facial expressions, as well as hand articulation, and given initial, artist designed, gender neutral rigged quad-meshes, we train all model parameters including non-linear shape spaces based on variational auto-encoders, pose-space deformation correctives, skeleton joint center predictors, and blend skinning functions, in a single consistent learning loop. The models are simultaneously trained with all the 3d dynamic scan data (over 60,000 diverse human configurations in our new dataset) in order to capture correlations and ensure consistency of various components. Models support facial expression analysis, as well as body (with detailed hand) shape and pose estimation. We provide fully train-able generic human models of different resolutions- the moderate-resolution GHUM consisting of 10,168 vertices and the low-resolution GHUML(ite) of 3,194 vertices–, run comparisons between them, analyze the impact of different components and illustrate their reconstruction from image data. The models will be available for research.",
        "authors": [
            "Hongyi Xu",
            "Eduard Gabriel Bazavan",
            "Andrei Zanfir",
            "W. Freeman",
            "R. Sukthankar",
            "C. Sminchisescu"
        ],
        "citations": 296,
        "references": 44,
        "year": 2020
    },
    {
        "title": "CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training",
        "abstract": "We propose an adversarial training procedure for learning a causal implicit generative model for a given causal graph. We show that adversarial training can be used to learn a generative model with true observational and interventional distributions if the generator architecture is consistent with the given causal graph. We consider the application of generating faces based on given binary labels where the dependency structure between the labels is preserved with a causal graph. This problem can be seen as learning a causal implicit generative model for the image and labels. We devise a two-stage procedure for this problem. First we train a causal implicit generative model over binary labels using a neural network consistent with a causal graph as the generator. We empirically show that WassersteinGAN can be used to output discrete labels. Later, we propose two new conditional GAN architectures, which we call CausalGAN and CausalBEGAN. We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels. The conditional GAN combined with a trained causal implicit generative model for the labels is then a causal implicit generative model over the labels and the generated image. We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.",
        "authors": [
            "Murat Kocaoglu",
            "Christopher Snyder",
            "A. Dimakis",
            "S. Vishwanath"
        ],
        "citations": 243,
        "references": 37,
        "year": 2017
    },
    {
        "title": "Generative Models for Fast Calorimeter Simulation: the LHCb case>",
        "abstract": "Simulation is one of the key components in high energy physics. Historically it relies on the Monte Carlo methods which require a tremendous amount of computation resources. These methods may have difficulties with the expected High Luminosity Large Hadron Collider (HL-LHC) needs, so the experiments are in urgent need of new fast simulation techniques. We introduce a new Deep Learning framework based on Generative Adversarial Networks which can be faster than traditional simulation methods by 5 orders of magnitude with reasonable simulation accuracy. This approach will allow physicists to produce a sufficient amount of simulated data needed by the next HL-LHC experiments using limited computing resources.",
        "authors": [
            "V. Chekalina",
            "Elena Orlova",
            "Fedor Ratnikov",
            "Dmitry Ulyanov",
            "Andrey Ustyuzhanin",
            "Egor Zakharov"
        ],
        "citations": 65,
        "references": 14,
        "year": 2018
    },
    {
        "title": "Skill Rating for Generative Models",
        "abstract": "We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.",
        "authors": [
            "Catherine Olsson",
            "Surya Bhupatiraju",
            "Tom B. Brown",
            "Augustus Odena",
            "I. Goodfellow"
        ],
        "citations": 32,
        "references": 37,
        "year": 2018
    },
    {
        "title": "Learning Hierarchical Features from Deep Generative Models",
        "abstract": "Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with some existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that does not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no taskspecific regularization.",
        "authors": [
            "Shengjia Zhao",
            "Jiaming Song",
            "Stefano Ermon"
        ],
        "citations": 105,
        "references": 27,
        "year": 2017
    },
    {
        "title": "Geodesic Clustering in Deep Generative Models",
        "abstract": "Deep generative models are tremendously successful in learning low-dimensional latent representations that well-describe the data. These representations, however, tend to much distort relationships between points, i.e. pairwise distances tend to not reflect semantic similarities well. This renders unsupervised tasks, such as clustering, difficult when working with the latent representations. We demonstrate that taking the geometry of the generative model into account is sufficient to make simple clustering algorithms work well over latent representations. Leaning on the recent finding that deep generative models constitute stochastically immersed Riemannian manifolds, we propose an efficient algorithm for computing geodesics (shortest paths) and computing distances in the latent space, while taking its distortion into account. We further propose a new architecture for modeling uncertainty in variational autoencoders, which is essential for understanding the geometry of deep generative models. Experiments show that the geodesic distance is very likely to reflect the internal structure of the data.",
        "authors": [
            "Tao Yang",
            "Georgios Arvanitidis",
            "Dongmei Fu",
            "Xiaogang Li",
            "Søren Hauberg"
        ],
        "citations": 26,
        "references": 36,
        "year": 2018
    },
    {
        "title": "Generative models for clinical applications in computational psychiatry.",
        "abstract": "Despite the success of modern neuroimaging techniques in furthering our understanding of cognitive and pathophysiological processes, translation of these advances into clinically relevant tools has been virtually absent until now. Neuromodeling represents a powerful framework for overcoming this translational deadlock, and the development of computational models to solve clinical problems has become a major scientific goal over the last decade, as reflected by the emergence of clinically oriented neuromodeling fields like Computational Psychiatry, Computational Neurology, and Computational Psychosomatics. Generative models of brain physiology and connectivity in the human brain play a key role in this endeavor, striving for computational assays that can be applied to neuroimaging data from individual patients for differential diagnosis and treatment prediction. In this review, we focus on dynamic causal modeling (DCM) and its use for Computational Psychiatry. DCM is a widely used generative modeling framework for functional magnetic resonance imaging (fMRI) and magneto-/electroencephalography (M/EEG) data. This article reviews the basic concepts of DCM, revisits examples where it has proven valuable for addressing clinically relevant questions, and critically discusses methodological challenges and recent methodological advances. We conclude this review with a more general discussion of the promises and pitfalls of generative models in Computational Psychiatry and highlight the path that lies ahead of us. This article is categorized under: Neuroscience > Computation Neuroscience > Clinical Neuroscience.",
        "authors": [
            "S. Frässle",
            "Yu Yao",
            "Dario Schöbi",
            "Eduardo A. Aponte",
            "J. Heinzle",
            "K. Stephan"
        ],
        "citations": 31,
        "references": 162,
        "year": 2018
    },
    {
        "title": "Efficient generative modeling of protein sequences using simple autoregressive models",
        "abstract": null,
        "authors": [
            "J. Trinquier",
            "Guido Uguzzoni",
            "A. Pagnani",
            "F. Zamponi",
            "M. Weigt"
        ],
        "citations": 62,
        "references": 101,
        "year": 2021
    },
    {
        "title": "Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models",
        "abstract": "\n \n Adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. Implicit models such as generative adversarial networks (GAN) often generate better samples compared to explicit models trained by maximum likelihood. Yet, GANs sidestep the characterization of an explicit density which makes quantitative evaluations challenging. To bridge this gap, we propose Flow-GANs, a generative adversarial network for which we can perform exact likelihood evaluation, thus supporting both adversarial and maximum likelihood training. When trained adversarially, Flow-GANs generate high-quality samples but attain extremely poor log-likelihood scores, inferior even to a mixture model memorizing the training data; the opposite is true when trained by maximum likelihood. Results on MNIST and CIFAR-10 demonstrate that hybrid training can attain high held-out likelihoods while retaining visual fidelity in the generated samples.\n \n",
        "authors": [
            "Aditya Grover",
            "Manik Dhar",
            "Stefano Ermon"
        ],
        "citations": 199,
        "references": 48,
        "year": 2017
    },
    {
        "title": "LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks",
        "abstract": "Generative models are increasingly used to artificially generate various kinds of data, including high-quality images and videos. These models are used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, the data used to train these models is often sensitive, thus prompting the need to evaluate information leakage from producing synthetic samples with generative models---specifically, whether an adversary can infer information about the data used to train the models. In this paper, we present the first membership inference attack on generative models. To mount the attack, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that are part of training datasets by relying on the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of dataset samples. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones succeeds with 80% accuracy. Finally, we discuss the sensitivity of our attacks to different training parameters, and their robustness against mitigation strategies, finding that successful defenses often result in significant worse performances of the generative models in terms of training stability and/or sample quality.",
        "authors": [
            "Jamie Hayes",
            "Luca Melis",
            "G. Danezis",
            "Emiliano De Cristofaro"
        ],
        "citations": 101,
        "references": 48,
        "year": 2017
    },
    {
        "title": "Denoising Diffusion Implicit Models",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.",
        "authors": [
            "Jiaming Song",
            "Chenlin Meng",
            "Stefano Ermon"
        ],
        "citations": 1000,
        "references": 47,
        "year": 2020
    },
    {
        "title": "Zero-Shot Learning via Class-Conditioned Deep Generative Models",
        "abstract": "\n \n We present a deep generative model for Zero-Shot Learning (ZSL). Unlike most existing methods for this problem, that represent each class as a point (via a semantic embedding), we represent each seen/unseen class using a class-specific latent-space distribution, conditioned on class attributes. We use these latent-space distributions as a prior for a supervised variational autoencoder (VAE), which also facilitates learning highly discriminative feature representations for the inputs. The entire framework is learned end-to-end using only the seen-class training data. At test time, the label for an unseen-class test input is the class that maximizes the VAE lower bound. We further extend the model to a (i) semi-supervised/transductive setting by leveraging unlabeled unseen-class data via an unsupervised learning module, and (ii) few-shot learning where we also have a small number of labeled inputs from the unseen classes. We compare our model with several state-of-the-art methods through a comprehensive set of experiments on a variety of benchmark data sets.\n \n",
        "authors": [
            "Wenlin Wang",
            "Yunchen Pu",
            "V. Verma",
            "Kai Fan",
            "Yizhe Zhang",
            "Changyou Chen",
            "Piyush Rai",
            "L. Carin"
        ],
        "citations": 144,
        "references": 51,
        "year": 2017
    },
    {
        "title": "Learning Universal Adversarial Perturbations with Generative Models",
        "abstract": "Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it's generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks.",
        "authors": [
            "Jamie Hayes",
            "G. Danezis"
        ],
        "citations": 144,
        "references": 59,
        "year": 2017
    },
    {
        "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models",
        "abstract": "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal \"realism\" constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function. Code with dedicated cloud instance has been made publicly available (this https URL).",
        "authors": [
            "Jesse Engel",
            "M. Hoffman",
            "Adam Roberts"
        ],
        "citations": 135,
        "references": 34,
        "year": 2017
    },
    {
        "title": "Generative models for network neuroscience: prospects and promise",
        "abstract": "Network neuroscience is the emerging discipline concerned with investigating the complex patterns of interconnections found in neural systems, and identifying principles with which to understand them. Within this discipline, one particularly powerful approach is network generative modelling, in which wiring rules are algorithmically implemented to produce synthetic network architectures with the same properties as observed in empirical network data. Successful models can highlight the principles by which a network is organized and potentially uncover the mechanisms by which it grows and develops. Here, we review the prospects and promise of generative models for network neuroscience. We begin with a primer on network generative models, with a discussion of compressibility and predictability, and utility in intuiting mechanisms, followed by a short history on their use in network science, broadly. We then discuss generative models in practice and application, paying particular attention to the critical need for cross-validation. Next, we review generative models of biological neural networks, both at the cellular and large-scale level, and across a variety of species including Caenorhabditis elegans, Drosophila, mouse, rat, cat, macaque and human. We offer a careful treatment of a few relevant distinctions, including differences between generative models and null models, sufficiency and redundancy, inferring and claiming mechanism, and functional and structural connectivity. We close with a discussion of future directions, outlining exciting frontiers both in empirical data collection efforts as well as in method and theory development that, together, further the utility of the generative network modelling approach for network neuroscience.",
        "authors": [
            "Richard F. Betzel",
            "D. Bassett"
        ],
        "citations": 94,
        "references": 243,
        "year": 2017
    },
    {
        "title": "Interpretable dimensionality reduction of single cell transcriptome data with deep generative models",
        "abstract": null,
        "authors": [
            "Jiarui Ding",
            "A. Condon",
            "Sohrab P. Shah"
        ],
        "citations": 299,
        "references": 71,
        "year": 2017
    },
    {
        "title": "Noise Estimation for Generative Diffusion Models",
        "abstract": "Generative diffusion models have emerged as leading models in speech and image generation. However, in order to perform well with a small number of denoising steps, a costly tuning of the set of noise parameters is needed. In this work, we present a simple and versatile learning scheme that can step-by-step adjust those noise parameters, for any given number of steps, while the previous work needs to retune for each number separately. Furthermore, without modifying the weights of the diffusion model, we are able to significantly improve the synthesis results, for a small number of steps. Our approach comes at a negligible computation cost.",
        "authors": [
            "Robin San Roman",
            "Eliya Nachmani",
            "Lior Wolf"
        ],
        "citations": 92,
        "references": 33,
        "year": 2021
    },
    {
        "title": "Learning Hierarchical Features from Generative Models",
        "abstract": "Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no task specific regularization or prior knowledge.",
        "authors": [
            "Shengjia Zhao",
            "Jiaming Song",
            "Stefano Ermon"
        ],
        "citations": 69,
        "references": 29,
        "year": 2017
    },
    {
        "title": "Inference in generative models using the Wasserstein distance",
        "abstract": "In purely generative models, one can simulate data given parameters but not necessarily evaluate the likelihood. We use Wasserstein distances between empirical distributions of observed data and empirical distributions of synthetic data drawn from such models to estimate their parameters. Previous interest in the Wasserstein distance for statistical inference has been mainly theoretical, due to computational limitations. Thanks to recent advances in numerical transport, the computation of these distances has become feasible, up to controllable approximation errors. We leverage these advances to propose point estimators and quasi-Bayesian distributions for parameter inference, first for independent data. For dependent data, we extend the approach by using delay reconstruction and residual reconstruction techniques. For large data sets, we propose an alternative distance using the Hilbert space-filling curve, which computation scales as nlogn where n is the size of the data. We provide a theoretical study of the proposed estimators, and adaptive Monte Carlo algorithms to approximate them. The approach is illustrated on four examples: a quantile g-and-k distribution, a toggle switch model from systems biology, a Lotka-Volterra model for plankton population sizes and a L\\'evy-driven stochastic volatility model.",
        "authors": [
            "Espen Bernton",
            "P. Jacob",
            "Mathieu Gerber",
            "C. Robert"
        ],
        "citations": 76,
        "references": 79,
        "year": 2017
    },
    {
        "title": "Variational Memory Addressing in Generative Models",
        "abstract": "Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory. This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups. Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets. Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process. To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning. The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations. We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory",
        "authors": [
            "J. Bornschein",
            "A. Mnih",
            "Daniel Zoran",
            "Danilo Jimenez Rezende"
        ],
        "citations": 63,
        "references": 28,
        "year": 2017
    },
    {
        "title": "Generative models",
        "abstract": "This chapter presents an introduction to the important topic of building generative models. These are models that are aimed to understand the variety of a class such as cars or trees. A generative mode should be able to generate feature vectors for instances of the class they represent, and such models should therefore be able to characterize the class with all its variations. The subject is discussed both in a Bayesian and in a deep learning context, and also within a supervised and unsupervised context. This area is related to important algorithms such as k-means clustering, expectation maximization (EM), naïve Bayes, generative adversarial networks (GANs), and variational autoencoders (VAE).",
        "authors": [],
        "citations": 0,
        "references": 0,
        "year": 2019
    },
    {
        "title": "Sinkhorn-AutoDiff: Tractable Wasserstein Learning of Generative Models",
        "abstract": "The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles both these issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.",
        "authors": [
            "Aude Genevay",
            "G. Peyré",
            "Marco Cuturi"
        ],
        "citations": 48,
        "references": 8,
        "year": 2017
    },
    {
        "title": "The Robust Manifold Defense: Adversarial Training using Generative Models",
        "abstract": "We propose a new type of attack for finding adversarial examples for image classifiers. Our method exploits spanners, i.e. deep neural networks whose input space is low-dimensional and whose output range approximates the set of images of interest. Spanners may be generators of GANs or decoders of VAEs. The key idea in our attack is to search over latent code pairs to find ones that generate nearby images with different classifier outputs. We argue that our attack is stronger than searching over perturbations of real images. Moreover, we show that our stronger attack can be used to reduce the accuracy of Defense-GAN to 3\\%, resolving an open problem from the well-known paper by Athalye et al. We combine our attack with normal adversarial training to obtain the most robust known MNIST classifier, significantly improving the state of the art against PGD attacks. Our formulation involves solving a min-max problem, where the min player sets the parameters of the classifier and the max player is running our attack, and is thus searching for adversarial examples in the {\\em low-dimensional} input space of the spanner. \nAll code and models are available at \\url{this https URL}",
        "authors": [
            "Andrew Ilyas",
            "A. Jalal",
            "Eirini Asteri",
            "C. Daskalakis",
            "A. Dimakis"
        ],
        "citations": 171,
        "references": 55,
        "year": 2017
    },
    {
        "title": "Generative Models of Visually Grounded Imagination",
        "abstract": "It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C's). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of Suzuki et.al. and the BiVCCA method of Wang et.al.) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset.",
        "authors": [
            "Ramakrishna Vedantam",
            "Ian S. Fischer",
            "Jonathan Huang",
            "K. Murphy"
        ],
        "citations": 135,
        "references": 65,
        "year": 2017
    },
    {
        "title": "Generative models for fast simulation",
        "abstract": "Machine Learning techniques have been used in different applications by the HEP community: in this talk, we discuss the case of detector simulation. The need for simulated events, expected in the future for LHC experiments and their High Luminosity upgrades, is increasing dramatically and requires new fast simulation solutions. We will present results of several studies on the application of computer vision techniques to the simulation of detectors, such as calorimeters. We will also describe a new R&D activity, within the GeantV project, aimed at providing a configurable tool capable of training a neural network to reproduce the detector response and replace standard Monte Carlo simulation. This represents a generic approach in the sense that such a network could be designed and trained to simulate any kind of detector and, eventually, the whole data processing chain in order to get, directly in one step, the final reconstructed quantities, in just a small fraction of time. We will present the first three-dimensional images of energy showers in a high granularity calorimeter, obtained using Generative Adversarial Networks.",
        "authors": [
            "Sofia Vallecorsa"
        ],
        "citations": 41,
        "references": 23,
        "year": 2018
    },
    {
        "title": "Domain Randomization and Generative Models for Robotic Grasping",
        "abstract": "Deep learning-based robotic grasping has made significant progress thanks to algorithmic improvements and increased data availability. However, state-of-the-art models are often trained on as few as hundreds or thousands of unique object instances, and as a result generalization can be a challenge. In this work, we explore a novel data generation pipeline for training a deep neural network to perform grasp planning that applies the idea of domain randomization to object synthesis. We generate millions of unique, unrealistic procedurally generated objects, and train a deep neural network to perform grasp planning on these objects. Since the distribution of successful grasps for a given object can be highly multimodal, we propose an autoregressive grasp planning model that maps sensor inputs of a scene to a probability distribution over possible grasps. This model allows us to sample grasps efficiently at test time (or avoid sampling entirely). We evaluate our model architecture and data generation pipeline in simulation and the real world. We find we can achieve a >90% success rate on previously unseen realistic objects at test time in simulation despite having only been trained on random objects. We also demonstrate an 80% success rate on real-world grasp attempts despite having only been trained on random simulated objects.",
        "authors": [
            "Joshua Tobin",
            "Wojciech Zaremba",
            "P. Abbeel"
        ],
        "citations": 168,
        "references": 57,
        "year": 2017
    },
    {
        "title": "Augmented Normalizing Flows: Bridging the Gap Between Generative Flows and Latent Variable Models",
        "abstract": "In this work, we propose a new family of generative flows on an augmented data space, with an aim to improve expressivity without drastically increasing the computational cost of sampling and evaluation of a lower bound on the likelihood. Theoretically, we prove the proposed flow can approximate a Hamiltonian ODE as a universal transport map. Empirically, we demonstrate state-of-the-art performance on standard benchmarks of flow-based generative modeling.",
        "authors": [
            "Chin-Wei Huang",
            "Laurent Dinh",
            "Aaron C. Courville"
        ],
        "citations": 86,
        "references": 92,
        "year": 2020
    },
    {
        "title": "Regularising Inverse Problems with Generative Machine Learning Models",
        "abstract": null,
        "authors": [
            "Margaret Duff",
            "Neill D. F. Campbell",
            "Matthias Joachim Ehrhardt"
        ],
        "citations": 27,
        "references": 130,
        "year": 2021
    },
    {
        "title": "StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation",
        "abstract": "Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.",
        "authors": [
            "Yunjey Choi",
            "Min-Je Choi",
            "M. Kim",
            "Jung-Woo Ha",
            "Sunghun Kim",
            "J. Choo"
        ],
        "citations": 1000,
        "references": 34,
        "year": 2017
    },
    {
        "title": "Semi-Supervised Generation with Cluster-aware Generative Models",
        "abstract": "Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.",
        "authors": [
            "Lars Maaløe",
            "Marco Fraccaro",
            "O. Winther"
        ],
        "citations": 28,
        "references": 33,
        "year": 2017
    },
    {
        "title": "Semi-supervised Learning with Deep Generative Models for Asset Failure Prediction",
        "abstract": "This work presents a novel semi-supervised learning approach for data-driven modeling of asset failures when health status is only partially known in historical data. We combine a generative model parameterized by deep neural networks with non-linear embedding technique. It allows us to build prognostic models with the limited amount of health status information for the precise prediction of future asset reliability. The proposed method is evaluated on a publicly available dataset for remaining useful life (RUL) estimation, which shows significant improvement even when a fraction of the data with known health status is as sparse as 1% of the total. Our study suggests that the non-linear embedding based on a deep generative model can efficiently regularize a complex model with deep architectures while achieving high prediction accuracy that is far less sensitive to the availability of health status information.",
        "authors": [
            "A. Yoon",
            "Taehoon Lee",
            "Yongsub Lim",
            "Deokwoo Jung",
            "Philgyun Kang",
            "Dongwon Kim",
            "Keuntae Park",
            "Yongjin Choi"
        ],
        "citations": 59,
        "references": 31,
        "year": 2017
    },
    {
        "title": "How Novelists Use Generative Language Models: An Exploratory User Study",
        "abstract": "Generative language models are garnering interest as creative tools. We present a user study to explore how fiction writers use generative language models during their writing process. We had four professional novelists complete various writing tasks while having access to a generative language model that either finishes their sentence or generates the next paragraph of text. We report the primary ways that novelists interact with these models, including: to generate ideas for describing scenes and characters, to create antagonistic suggestions that force them to hone their descriptive language, and as a constraint tool for challenging their writing practice. We identify six criteria for evaluating creative writing assistants, and propose design guidelines for future co-writing tools.",
        "authors": [
            "Alex Calderwood",
            "Vivian Qiu",
            "K. Gero",
            "Lydia B. Chilton"
        ],
        "citations": 78,
        "references": 16,
        "year": 2020
    },
    {
        "title": "Improving Generative Imagination in Object-Centric World Models",
        "abstract": "The remarkable recent advances in object-centric generative world models raise a few questions. First, while many of the recent achievements are indispensable for making a general and versatile world model, it is quite unclear how these ingredients can be integrated into a unified framework. Second, despite using generative objectives, abilities for object detection and tracking are mainly investigated, leaving the crucial ability of temporal imagination largely under question. Third, a few key abilities for more faithful temporal imagination such as multimodal uncertainty and situation-awareness are missing. In this paper, we introduce Generative Structured World Models (G-SWM). The G-SWM achieves the versatile world modeling not only by unifying the key properties of previous models in a principled framework but also by achieving two crucial new abilities, multimodal uncertainty and situation-awareness. Our thorough investigation on the temporal generation ability in comparison to the previous models demonstrates that G-SWM achieves the versatility with the best or comparable performance for all experiment settings including a few complex settings that have not been tested before.",
        "authors": [
            "Zhixuan Lin",
            "Yi-Fu Wu",
            "Skand Peri",
            "Bofeng Fu",
            "Jindong Jiang",
            "Sungjin Ahn"
        ],
        "citations": 71,
        "references": 52,
        "year": 2020
    },
    {
        "title": "IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models",
        "abstract": "This paper provides a unified account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a query-document pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards fitting the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an attacker to the current discriminative model, generates difficult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the unified framework takes advantage of both schools of thinking: (i) the generative model learns to fit the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as much as 23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.",
        "authors": [
            "Jun Wang",
            "Lantao Yu",
            "Weinan Zhang",
            "Yu Gong",
            "Yinghui Xu",
            "Benyou Wang",
            "P. Zhang",
            "Dell Zhang"
        ],
        "citations": 581,
        "references": 56,
        "year": 2017
    },
    {
        "title": "Predictive and generative machine learning models for photonic crystals",
        "abstract": "Abstract The prediction and design of photonic features have traditionally been guided by theory-driven computational methods, spanning a wide range of direct solvers and optimization techniques. Motivated by enormous advances in the field of machine learning, there has recently been a growing interest in developing complementary data-driven methods for photonics. Here, we demonstrate several predictive and generative data-driven approaches for the characterization and inverse design of photonic crystals. Concretely, we built a data set of 20,000 two-dimensional photonic crystal unit cells and their associated band structures, enabling the training of supervised learning models. Using these data set, we demonstrate a high-accuracy convolutional neural network for band structure prediction, with orders-of-magnitude speedup compared to conventional theory-driven solvers. Separately, we demonstrate an approach to high-throughput inverse design of photonic crystals via generative adversarial networks, with the design goal of substantial transverse-magnetic band gaps. Our work highlights photonic crystals as a natural application domain and test bed for the development of data-driven tools in photonics and the natural sciences.",
        "authors": [
            "T. Christensen",
            "Charlotte Loh",
            "S. Picek",
            "D. Jakobović",
            "Li Jing",
            "Sophie Fisher",
            "V. Ceperic",
            "J. Joannopoulos",
            "M. Soljačić"
        ],
        "citations": 74,
        "references": 67,
        "year": 2020
    },
    {
        "title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
        "abstract": "In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.",
        "authors": [
            "G. L. Guimaraes",
            "Benjamín Sánchez-Lengeling",
            "Pedro Luis Cunha Farias",
            "Alán Aspuru-Guzik"
        ],
        "citations": 501,
        "references": 49,
        "year": 2017
    },
    {
        "title": "Alias-Free Generative Adversarial Networks",
        "abstract": "We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.",
        "authors": [
            "Tero Karras",
            "M. Aittala",
            "S. Laine",
            "Erik Härkönen",
            "Janne Hellsten",
            "J. Lehtinen",
            "Timo Aila"
        ],
        "citations": 1000,
        "references": 74,
        "year": 2021
    },
    {
        "title": "Stochastic Generative Models",
        "abstract": "Modelling: You assume — usually an implausible assumption, taken literally — that both the training data and the new examples (the test data) are samples generated independently by some stochastic (probabilistic) process Q within some space Φ of possible processes. Often the possible processes in Φ are all structurally the same and vary only in numerical parameters, but that is not always the case.",
        "authors": [],
        "citations": 0,
        "references": 0,
        "year": 2019
    },
    {
        "title": "Shaping Belief States with Generative Environment Models for RL",
        "abstract": "When agents interact with a complex environment, they must form and maintain beliefs about the relevant aspects of that environment. We propose a way to efficiently train expressive generative models in complex environments. We show that a predictive algorithm with an expressive generative model can form stable belief-states in visually rich and dynamic 3D environments. More precisely, we show that the learned representation captures the layout of the environment as well as the position and orientation of the agent. Our experiments show that the model substantially improves data-efficiency on a number of reinforcement learning (RL) tasks compared with strong model-free baseline agents. We find that predicting multiple steps into the future (overshooting), in combination with an expressive generative model, is critical for stable representations to emerge. In practice, using expressive generative models in RL is computationally expensive and we propose a scheme to reduce this computational burden, allowing us to build agents that are competitive with model-free baselines.",
        "authors": [
            "Karol Gregor",
            "Danilo Jimenez Rezende",
            "F. Besse",
            "Yan Wu",
            "Hamza Merzic",
            "Aäron van den Oord"
        ],
        "citations": 114,
        "references": 69,
        "year": 2019
    },
    {
        "title": "Avoiding Latent Variable Collapse With Generative Skip Models",
        "abstract": "Variational autoencoders learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as \"latent variable collapse,\" especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior \"collapses\" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.",
        "authors": [
            "Adji B. Dieng",
            "Yoon Kim",
            "Alexander M. Rush",
            "D. Blei"
        ],
        "citations": 169,
        "references": 36,
        "year": 2018
    },
    {
        "title": "Generating geologically realistic 3D reservoir facies models using deep learning of sedimentary architecture with generative adversarial networks",
        "abstract": null,
        "authors": [
            "Tuanfeng Zhang",
            "P. Tilke",
            "Emilien Dupont",
            "Lingchen Zhu",
            "Lin Liang",
            "William J. Bailey"
        ],
        "citations": 111,
        "references": 23,
        "year": 2019
    },
    {
        "title": "Physics-Based Generative Adversarial Models for Image Restoration and Beyond",
        "abstract": "We present an algorithm to directly solve numerous image restoration problems (e.g., image deblurring, image dehazing, and image deraining). These problems are ill-posed, and the common assumptions for existing methods are usually based on heuristic image priors. In this paper, we show that these problems can be solved by generative models with adversarial learning. However, a straightforward formulation based on a straightforward generative adversarial network (GAN) does not perform well in these tasks, and some structures of the estimated images are usually not preserved well. Motivated by an interesting observation that the estimated results should be consistent with the observed inputs under the physics models, we propose an algorithm that guides the estimation process of a specific task within the GAN framework. The proposed model is trained in an end-to-end fashion and can be applied to a variety of image restoration and low-level vision problems. Extensive experiments demonstrate that the proposed method performs favorably against state-of-the-art algorithms.",
        "authors": [
            "Jin-shan Pan",
            "Yang Liu",
            "Jiangxin Dong",
            "Jiawei Zhang",
            "Jimmy S. J. Ren",
            "Jinhui Tang",
            "Yu-Wing Tai",
            "Ming-Hsuan Yang"
        ],
        "citations": 137,
        "references": 75,
        "year": 2018
    },
    {
        "title": "SCALOR: Generative World Models with Scalable Object Representations",
        "abstract": "Scalability in terms of object density in a scene is a primary challenge in unsupervised sequential object-oriented representation learning. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models. Additionally, we introduce a background module that allows SCALOR to model complex dynamic backgrounds as well as many foreground objects in the scene. We demonstrate that SCALOR can deal with crowded scenes containing up to a hundred objects while jointly modeling complex dynamic backgrounds. Importantly, SCALOR is the ﬁrst unsupervised object representation model shown to work for natural scenes containing several tens of moving objects.",
        "authors": [
            "Jindong Jiang",
            "Sepehr Janghorbani",
            "Gerard de Melo",
            "Sungjin Ahn"
        ],
        "citations": 126,
        "references": 27,
        "year": 2019
    },
    {
        "title": "Geometric Disentanglement for Generative Latent Shape Models",
        "abstract": "Representing 3D shapes is a fundamental problem in artiﬁcial intelligence, which has numerous applications within computer vision and graphics. One avenue that has recently begun to be explored is the use of latent representations of generative models. However, it remains an open problem to learn a generative model of shapes that is interpretable and easily manipulated, particularly in the absence of supervised labels. In this paper, we propose an unsupervised approach to partitioning the latent space of a variational autoencoder for 3D point clouds in a natural way, using only geometric information, that builds upon prior work utilizing generative adversarial models of point sets. Our method makes use of tools from spectral geometry to separate intrinsic and extrinsic shape information, and then considers several hierarchical disentanglement penalties for dividing the latent space in this manner. We also propose a novel disentanglement penalty that penalizes the predicted change in the latent representation of the output,with respect to the latent variables of the initial shape. We show that the resulting latent representation exhibits intuitive and interpretable behaviour, enabling tasks such as pose transfer that cannot easily be performed by models with an entangled representation.",
        "authors": [
            "Tristan Aumentado-Armstrong",
            "Stavros Tsogkas",
            "A. Jepson",
            "Sven J. Dickinson"
        ],
        "citations": 52,
        "references": 68,
        "year": 2019
    },
    {
        "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
        "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
        "authors": [
            "Jungil Kong",
            "Jaehyeon Kim",
            "Jaekyoung Bae"
        ],
        "citations": 1000,
        "references": 28,
        "year": 2020
    },
    {
        "title": "Understanding Posterior Collapse in Generative Latent Variable Models",
        "abstract": "Posterior collapse in Variational Autoencoders (VAEs) arises when the variational distribution closely matches the uninformative prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We identify how local maxima can emerge from the marginal log-likelihood of pPCA, which yields similar local maxima for the evidence lower bound (ELBO). We show that training a linear VAE with variational inference recovers a uniquely identiﬁable global maximum corresponding to the principal component directions. We provide empirical evidence that the presence of local maxima causes posterior collapse in non-linear VAEs. Our ﬁndings help to explain a wide range of heuristic approaches in the literature that attempt to diminish the eﬀect of the KL term in the ELBO to alleviate posterior collapse.",
        "authors": [
            "James Lucas",
            "G. Tucker",
            "R. Grosse",
            "Mohammad Norouzi"
        ],
        "citations": 139,
        "references": 31,
        "year": 2019
    },
    {
        "title": "Zero-shot Text Classification With Generative Language Models",
        "abstract": "This work investigates the use of natural language to enable zero-shot model adaptation to new tasks. We use text and metadata from social commenting platforms as a source for a simple pretraining task. We then provide the language model with natural language descriptions of classification tasks as input and train it to generate the correct answer in natural language via a language modeling objective. This allows the model to generalize to new classification tasks without the need for multiple multitask classification heads. We show the zero-shot performance of these generative language models, trained with weak supervision, on six benchmark text classification datasets from the torchtext library. Despite no access to training data, we achieve up to a 45% absolute improvement in classification accuracy over random or majority class baselines. These results show that natural language can serve as simple and powerful descriptors for task adaptation. We believe this points the way to new metalearning strategies for text problems.",
        "authors": [
            "Raul Puri",
            "Bryan Catanzaro"
        ],
        "citations": 95,
        "references": 25,
        "year": 2019
    },
    {
        "title": "Discovering Interpretable Representations for Both Deep Generative and Discriminative Models",
        "abstract": "Interpretability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretability. However, this may reduce accuracy, and is not applicable to already trained models. We propose two interpretability frameworks. First, we provide an interpretable lens for an existing model. We use a generative model which takes as input the representation in an existing (generative or discriminative) model, weakly supervised by limited side information. Applying a flexible and invertible transformation to the input leads to an interpretable representation with no loss in accuracy. We extend the approach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what \"interpretable\" means. Our second framework relies on joint optimization for a representation which is both maximally informative about the side information and maximally compressive about the non-interpretable data factors. This leads to a novel perspective on the relationship between compression and regularization. We also propose a new interpretability evaluation metric based on our framework. Empirically, we achieve state-of-the-art results on three datasets using the two proposed algorithms.",
        "authors": [
            "T. Adel",
            "Zoubin Ghahramani",
            "Adrian Weller"
        ],
        "citations": 89,
        "references": 39,
        "year": 2018
    },
    {
        "title": "Improved Denoising Diffusion Probabilistic Models",
        "abstract": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion",
        "authors": [
            "Alex Nichol",
            "Prafulla Dhariwal"
        ],
        "citations": 1000,
        "references": 47,
        "year": 2021
    },
    {
        "title": "Are generative deep models for novelty detection truly better?",
        "abstract": "Many deep models have been recently proposed for anomaly detection. This paper presents comparison of selected generative deep models and classical anomaly detection methods on an extensive number of non--image benchmark datasets. We provide statistical comparison of the selected models, in many configurations, architectures and hyperparamaters. We arrive to conclusion that performance of the generative models is determined by the process of selection of their hyperparameters. Specifically, performance of the deep generative models deteriorates with decreasing amount of anomalous samples used in hyperparameter selection. In practical scenarios of anomaly detection, none of the deep generative models systematically outperforms the kNN.",
        "authors": [
            "V. Škvára",
            "T. Pevný",
            "V. Šmídl"
        ],
        "citations": 38,
        "references": 24,
        "year": 2018
    },
    {
        "title": "Deep Generative Markov State Models",
        "abstract": "We propose a deep generative Markov State Model (DeepGenMSM) learning framework for inference of metastable dynamical systems and prediction of trajectories. After unsupervised training on time series data, the model contains (i) a probabilistic encoder that maps from high-dimensional configuration space to a small-sized vector indicating the membership to metastable (long-lived) states, (ii) a Markov chain that governs the transitions between metastable states and facilitates analysis of the long-time dynamics, and (iii) a generative part that samples the conditional distribution of configurations in the next time step. The model can be operated in a recursive fashion to generate trajectories to predict the system evolution from a defined starting state and propose new configurations. The DeepGenMSM is demonstrated to provide accurate estimates of the long-time kinetics and generate valid distributions for molecular dynamics (MD) benchmark systems. Remarkably, we show that DeepGenMSMs are able to make long time-steps in molecular configuration space and generate physically realistic structures in regions that were not seen in training data.",
        "authors": [
            "Hao Wu",
            "Andreas Mardt",
            "Luca Pasquali",
            "F. Noé"
        ],
        "citations": 56,
        "references": 32,
        "year": 2018
    },
    {
        "title": "Visually-Aware Fashion Recommendation and Design with Generative Image Models",
        "abstract": "Building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved (i.e., fashion styles). Recent work has shown that approaches to 'visual' recommendation (e.g. clothing, art, etc.) can be made more accurate by incorporating visual signals directly into the recommendation objective, using 'off-the-shelf' feature representations derived from deep networks. Here, we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning 'fashion aware' image representations directly, i.e., by training the image representation (from the pixel level) and the recommender system jointly; this contribution is related to recent work using Siamese CNNs, though we are able to show improvements over state-of-the-art recommendation techniques such as BPR and variants that make use of pretrained visual features. Furthermore, we show that our model can be used generatively, i.e., given a user and a product category, we can generate new images (i.e., clothing items) that are most consistent with their personal taste. This represents a first step towards building systems that go beyond recommending existing items from a product corpus, but which can be used to suggest styles and aid the design of new products.",
        "authors": [
            "Wang-Cheng Kang",
            "Chen Fang",
            "Zhaowen Wang",
            "Julian McAuley"
        ],
        "citations": 241,
        "references": 42,
        "year": 2017
    },
    {
        "title": "Score-based Generative Modeling in Latent Space",
        "abstract": "Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .",
        "authors": [
            "Arash Vahdat",
            "Karsten Kreis",
            "J. Kautz"
        ],
        "citations": 590,
        "references": 110,
        "year": 2021
    },
    {
        "title": "Tackling the Generative Learning Trilemma with Denoising Diffusion GANs",
        "abstract": "A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$\\times$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively. Project page and code can be found at https://nvlabs.github.io/denoising-diffusion-gan",
        "authors": [
            "Zhisheng Xiao",
            "Karsten Kreis",
            "Arash Vahdat"
        ],
        "citations": 473,
        "references": 96,
        "year": 2021
    },
    {
        "title": "Generative Pretraining From Pixels",
        "abstract": "Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full ﬁne-tuning, matching the top supervised pre-trained models. An even larger model trained on a mix-ture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",
        "authors": [
            "Mark Chen",
            "Alec Radford",
            "Jeff Wu",
            "Heewoo Jun",
            "Prafulla Dhariwal",
            "D. Luan",
            "I. Sutskever"
        ],
        "citations": 1000,
        "references": 79,
        "year": 2020
    },
    {
        "title": "Wasserstein Learning of Deep Generative Point Process Models",
        "abstract": "Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.",
        "authors": [
            "Shuai Xiao",
            "Mehrdad Farajtabar",
            "X. Ye",
            "Junchi Yan",
            "Xiaokang Yang",
            "Le Song",
            "H. Zha"
        ],
        "citations": 163,
        "references": 33,
        "year": 2017
    },
    {
        "title": "Learning Implicit Fields for Generative Shape Modeling",
        "abstract": "We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.",
        "authors": [
            "Zhiqin Chen",
            "Hao Zhang"
        ],
        "citations": 1000,
        "references": 51,
        "year": 2018
    },
    {
        "title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields",
        "abstract": "Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects’ shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.",
        "authors": [
            "Michael Niemeyer",
            "Andreas Geiger"
        ],
        "citations": 907,
        "references": 103,
        "year": 2020
    },
    {
        "title": "Parametrization and generation of geological models with generative adversarial networks",
        "abstract": "One of the main challenges in the parametrization of geological models is the ability to capture complex geological structures often observed in the subsurface. In recent years, generative adversarial networks (GAN) were proposed as an efficient method for the generation and parametrization of complex data, showing state-of-the-art performances in challenging computer vision tasks such as reproducing natural images (handwritten digits, human faces, etc.). In this work, we study the application of Wasserstein GAN for the parametrization of geological models. The effectiveness of the method is assessed for uncertainty propagation tasks using several test cases involving different permeability patterns and subsurface flow problems. Results show that GANs are able to generate samples that preserve the multipoint statistical features of the geological models both visually and quantitatively. The generated samples reproduce both the geological structures and the flow statistics of the reference geology.",
        "authors": [
            "Shing Chan",
            "A. Elsheikh"
        ],
        "citations": 65,
        "references": 21,
        "year": 2017
    },
    {
        "title": "The Partial Information Decomposition of Generative Neural Network Models",
        "abstract": "In this work we study the distributed representations learnt by generative neural network models. In particular, we investigate the properties of redundant and synergistic information that groups of hidden neurons contain about the target variable. To this end, we use an emerging branch of information theory called partial information decomposition (PID) and track the informational properties of the neurons through training. We find two differentiated phases during the training process: a first short phase in which the neurons learn redundant information about the target, and a second phase in which neurons start specialising and each of them learns unique information about the target. We also find that in smaller networks individual neurons learn more specific information about certain features of the input, suggesting that learning pressure can encourage disentangled representations.",
        "authors": [
            "T. M. S. Tax",
            "P. Mediano",
            "M. Shanahan"
        ],
        "citations": 69,
        "references": 38,
        "year": 2017
    },
    {
        "title": "Generative Temporal Models with Memory",
        "abstract": "We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations. A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future. To create such models, we introduce Generative Temporal Models augmented with external memory systems. They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation. We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently. This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs.",
        "authors": [
            "Mevlana Gemici",
            "Chia-Chun Hung",
            "Adam Santoro",
            "Greg Wayne",
            "S. Mohamed",
            "Danilo Jimenez Rezende",
            "David Amos",
            "T. Lillicrap"
        ],
        "citations": 56,
        "references": 46,
        "year": 2017
    },
    {
        "title": "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery",
        "abstract": null,
        "authors": [
            "T. Schlegl",
            "Philipp Seeböck",
            "S. Waldstein",
            "U. Schmidt-Erfurth",
            "G. Langs"
        ],
        "citations": 1000,
        "references": 20,
        "year": 2017
    },
    {
        "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis",
        "abstract": "We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches how-ever fall short in two ways: first, they may lack an under-lying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (π-GAN or pi-GAN), for high-quality 3D-aware image synthesis. π-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.",
        "authors": [
            "Eric Chan",
            "M. Monteiro",
            "Petr Kellnhofer",
            "Jiajun Wu",
            "Gordon Wetzstein"
        ],
        "citations": 801,
        "references": 75,
        "year": 2020
    },
    {
        "title": "Robust Compressed Sensing MRI with Deep Generative Priors",
        "abstract": "The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep generative priors can be powerful tools for solving inverse problems. However, to date this framework has been empirically successful only on certain datasets (for example, human faces and MNIST digits), and it is known to perform poorly on out-of-distribution samples. In this paper, we present the first successful application of the CSGM framework on clinical MRI data. We train a generative prior on brain scans from the fastMRI dataset, and show that posterior sampling via Langevin dynamics achieves high quality reconstructions. Furthermore, our experiments and theory show that posterior sampling is robust to changes in the ground-truth distribution and measurement process. Our code and models are available at: \\url{https://github.com/utcsilab/csgm-mri-langevin}.",
        "authors": [
            "A. Jalal",
            "Marius Arvinte",
            "Giannis Daras",
            "E. Price",
            "A. Dimakis",
            "Jonathan I. Tamir"
        ],
        "citations": 280,
        "references": 97,
        "year": 2021
    },
    {
        "title": "MolGAN: An implicit generative model for small molecular graphs",
        "abstract": "eep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is pos-sible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuris-tics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforce-ment learning objective to encourage the genera-tion of molecules with specific desired chemical properties. In experiments on the QM9 chemi-cal database, we demonstrate that our model is capable of generating close to 100% valid com-pounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, al-beit being susceptible to mode collapse.",
        "authors": [
            "Nicola De Cao",
            "Thomas Kipf"
        ],
        "citations": 856,
        "references": 48,
        "year": 2018
    },
    {
        "title": "Continual Learning with Deep Generative Replay",
        "abstract": "Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (\"generator\") and a task solving model (\"solver\"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.",
        "authors": [
            "Hanul Shin",
            "Jung Kwon Lee",
            "Jaehong Kim",
            "Jiwon Kim"
        ],
        "citations": 1000,
        "references": 36,
        "year": 2017
    },
    {
        "title": "Joint Discriminative and Generative Learning for Person Re-Identification",
        "abstract": "Person re-identification (re-id) remains challenging due to significant intra-class variations across different cameras. Recently, there has been a growing interest in using generative models to augment training data and enhance the invariance to input changes. The generative pipelines in existing methods, however, stay relatively separate from the discriminative re-id learning stages. Accordingly, re-id models are often trained in a straightforward manner on the generated data. In this paper, we seek to improve learned re-id embeddings by better leveraging the generated data. To this end, we propose a joint learning framework that couples re-id learning and data generation end-to-end. Our model involves a generative module that separately encodes each person into an appearance code and a structure code, and a discriminative module that shares the appearance encoder with the generative module. By switching the appearance or structure codes, the generative module is able to generate high-quality cross-id composed images, which are online fed back to the appearance encoder and used to improve the discriminative module. The proposed joint learning framework renders significant improvement over the baseline without using generated data, leading to the state-of-the-art performance on several benchmark datasets.",
        "authors": [
            "Zhedong Zheng",
            "Xiaodong Yang",
            "Zhiding Yu",
            "Liang Zheng",
            "Yi Yang",
            "J. Kautz"
        ],
        "citations": 720,
        "references": 63,
        "year": 2019
    },
    {
        "title": "A Unified Generative Framework for Aspect-based Sentiment Analysis",
        "abstract": "Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation. Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an end-to-end framework. Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks.",
        "authors": [
            "Hang Yan",
            "Junqi Dai",
            "Tuo Ji",
            "Xipeng Qiu",
            "Zheng Zhang"
        ],
        "citations": 244,
        "references": 52,
        "year": 2021
    },
    {
        "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis",
        "abstract": "Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks.",
        "authors": [
            "Kundan Kumar",
            "Rithesh Kumar",
            "T. Boissiere",
            "L. Gestin",
            "Wei Zhen Teoh",
            "Jose M. R. Sotelo",
            "A. D. Brébisson",
            "Yoshua Bengio",
            "Aaron C. Courville"
        ],
        "citations": 896,
        "references": 51,
        "year": 2019
    },
    {
        "title": "Time-series Generative Adversarial Networks",
        "abstract": "A good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks (GANs) into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time, supervised models for sequence prediction - which allow finer control over network dynamics - are inherently deterministic. We propose a novel framework for generating realistic time-series data that combines the flexibility of the unsupervised paradigm with the control afforded by supervised training. Through a learned embedding space jointly optimized with both supervised and adversarial objectives, we encourage the network to adhere to the dynamics of the training data during sampling. Empirically, we evaluate the ability of our method to generate realistic samples using a variety of real and synthetic time-series datasets. Qualitatively and quantitatively, we find that the proposed framework consistently and significantly outperforms state-of-the-art benchmarks with respect to measures of similarity and predictive ability.",
        "authors": [
            "Jinsung Yoon",
            "Daniel Jarrett",
            "M. Schaar"
        ],
        "citations": 815,
        "references": 34,
        "year": 2019
    },
    {
        "title": "GeDi: Generative Discriminator Guided Sequence Generation",
        "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.",
        "authors": [
            "Ben Krause",
            "Akhilesh Deepak Gotmare",
            "Bryan McCann",
            "N. Keskar",
            "Shafiq R. Joty",
            "R. Socher",
            "Nazneen Rajani"
        ],
        "citations": 370,
        "references": 64,
        "year": 2020
    },
    {
        "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
        "abstract": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
        "authors": [
            "Jaehyeon Kim",
            "Sungwon Kim",
            "Jungil Kong",
            "Sungroh Yoon"
        ],
        "citations": 433,
        "references": 38,
        "year": 2020
    },
    {
        "title": "CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",
        "abstract": "Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., dog, frisbee, catch, throw); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., “a man throws a frisbee and his dog catches it”). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 77k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance (31.6% v.s. 63.5% in SPICE metric). Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA (76.9% to 78.4 in dev accuracy) by generating additional context.",
        "authors": [
            "Bill Yuchen Lin",
            "Ming Shen",
            "Wangchunshu Zhou",
            "Pei Zhou",
            "Chandra Bhagavatula",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "citations": 332,
        "references": 94,
        "year": 2020
    },
    {
        "title": "Scaling Laws for Autoregressive Generative Modeling",
        "abstract": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. \nThe cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. \nWe find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
        "authors": [
            "T. Henighan",
            "J. Kaplan",
            "Mor Katz",
            "Mark Chen",
            "Christopher Hesse",
            "Jacob Jackson",
            "Heewoo Jun",
            "Tom B. Brown",
            "Prafulla Dhariwal",
            "Scott Gray",
            "Chris Hallacy",
            "Benjamin Mann",
            "Alec Radford",
            "A. Ramesh",
            "Nick Ryder",
            "Daniel M. Ziegler",
            "John Schulman",
            "Dario Amodei",
            "Sam McCandlish"
        ],
        "citations": 349,
        "references": 36,
        "year": 2020
    },
    {
        "title": "EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning",
        "abstract": "Over the last few years, deep learning techniques have yielded significant improvements in image inpainting. However, many of these techniques fail to reconstruct reasonable structures as they are commonly over-smoothed and/or blurry. This paper develops a new approach for image inpainting that does a better job of reproducing filled regions exhibiting fine details. We propose a two-stage adversarial model EdgeConnect that comprises of an edge generator followed by an image completion network. The edge generator hallucinates edges of the missing region (both regular and irregular) of the image, and the image completion network fills in the missing regions using hallucinated edges as a priori. We evaluate our model end-to-end over the publicly available datasets CelebA, Places2, and Paris StreetView, and show that it outperforms current state-of-the-art techniques quantitatively and qualitatively. Code and models available at: this https URL",
        "authors": [
            "Kamyar Nazeri",
            "Eric Ng",
            "Tony Joseph",
            "F. Qureshi",
            "Mehran Ebrahimi"
        ],
        "citations": 655,
        "references": 61,
        "year": 2019
    },
    {
        "title": "On the \"steerability\" of generative adversarial networks",
        "abstract": "An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise - these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by \"steering\" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page: this https URL",
        "authors": [
            "Ali Jahanian",
            "Lucy Chai",
            "Phillip Isola"
        ],
        "citations": 385,
        "references": 34,
        "year": 2019
    },
    {
        "title": "Differentially Private Generative Adversarial Network",
        "abstract": "Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.",
        "authors": [
            "Liyang Xie",
            "Kaixiang Lin",
            "Shu Wang",
            "Fei Wang",
            "Jiayu Zhou"
        ],
        "citations": 462,
        "references": 38,
        "year": 2018
    },
    {
        "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion",
        "abstract": "Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered “velocities” that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efﬁcient synthesis from CLD-based diffusion models. We ﬁnd that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD signiﬁcantly outperforms solvers such as Euler–Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. VPSDE. We leave the study of CLD with maximum likelihood training for high-dimensional (image) datasets to future work.",
        "authors": [
            "Tim Dockhorn",
            "Arash Vahdat",
            "Karsten Kreis"
        ],
        "citations": 209,
        "references": 112,
        "year": 2021
    },
    {
        "title": "Recent Progress on Generative Adversarial Networks (GANs): A Survey",
        "abstract": "Generative adversarial network (GANs) is one of the most important research avenues in the field of artificial intelligence, and its outstanding data generation capacity has received wide attention. In this paper, we present the recent progress on GANs. First, the basic theory of GANs and the differences among different generative models in recent years were analyzed and summarized. Then, the derived models of GANs are classified and introduced one by one. Third, the training tricks and evaluation metrics were given. Fourth, the applications of GANs were introduced. Finally, the problem, we need to address, and future directions were discussed.",
        "authors": [
            "Zhaoqing Pan",
            "Weijie Yu",
            "Xiaokai Yi",
            "Asifullah Khan",
            "Feng Yuan",
            "Yuhui Zheng"
        ],
        "citations": 431,
        "references": 80,
        "year": 2019
    },
    {
        "title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation",
        "abstract": "We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at this https URL or this https URL.",
        "authors": [
            "Junho Kim",
            "Minjae Kim",
            "Hyeonwoo Kang",
            "Kwanghee Lee"
        ],
        "citations": 515,
        "references": 46,
        "year": 2019
    },
    {
        "title": "Watch Your Up-Convolution: CNN Based Generative Deep Neural Networks Are Failing to Reproduce Spectral Distributions",
        "abstract": "Generative convolutional deep neural networks, e.g. popular GAN architectures, are relying on convolution based up-sampling methods to produce non-scalar outputs like images or video sequences. In this paper, we show that common up-sampling methods, i.e. known as up-convolution or transposed convolution, are causing the inability of such models to reproduce spectral distributions of natural training data correctly. This effect is independent of the underlying architecture and we show that it can be used to easily detect generated data like deepfakes with up to 100% accuracy on public benchmarks. To overcome this drawback of current generative models, we propose to add a novel spectral regularization term to the training optimization objective. We show that this approach not only allows to train spectral consistent GANs that are avoiding high frequency errors. Also, we show that a correct approximation of the frequency spectrum has positive effects on the training stability and output quality of generative networks.",
        "authors": [
            "Ricard Durall",
            "M. Keuper",
            "J. Keuper"
        ],
        "citations": 298,
        "references": 63,
        "year": 2020
    },
    {
        "title": "Variational Diffusion Models",
        "abstract": "Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at https://github.com/google-research/vdm .",
        "authors": [
            "Diederik P. Kingma",
            "Tim Salimans",
            "Ben Poole",
            "Jonathan Ho"
        ],
        "citations": 925,
        "references": 54,
        "year": 2021
    },
    {
        "title": "Residual Flows for Invertible Generative Modeling",
        "abstract": "Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a \"Russian roulette\" estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.",
        "authors": [
            "Ricky T. Q. Chen",
            "Jens Behrmann",
            "D. Duvenaud",
            "J. Jacobsen"
        ],
        "citations": 357,
        "references": 53,
        "year": 2019
    },
    {
        "title": "Medical Image Synthesis for Data Augmentation and Anonymization using Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "Hoo-Chang Shin",
            "Neil A. Tenenholtz",
            "Jameson K. Rogers",
            "C. Schwarz",
            "M. Senjem",
            "J. Gunter",
            "K. Andriole",
            "Mark H. Michalski"
        ],
        "citations": 515,
        "references": 22,
        "year": 2018
    },
    {
        "title": "An introduction to deep generative modeling",
        "abstract": "Deep generative models (DGM) are neural networks with many hidden layers trained to approximate complicated, high‐dimensional probability distributions using samples. When trained successfully, we can use the DGM to estimate the likelihood of each observation and to create new samples from the underlying distribution. Developing DGMs has become one of the most hotly researched fields in artificial intelligence in recent years. The literature on DGMs has become vast and is growing rapidly. Some advances have even reached the public sphere, for example, the recent successes in generating realistic‐looking images, voices, or movies; so‐called deep fakes. Despite these successes, several mathematical and practical issues limit the broader use of DGMs: given a specific dataset, it remains challenging to design and train a DGM and even more challenging to find out why a particular model is or is not effective. To help advance the theoretical understanding of DGMs, we introduce DGMs and provide a concise mathematical framework for modeling the three most popular approaches: normalizing flows, variational autoencoders, and generative adversarial networks. We illustrate the advantages and disadvantages of these basic approaches using numerical experiments. Our goal is to enable and motivate the reader to contribute to this proliferating research area. Our presentation also emphasizes relations between generative modeling and optimal transport.",
        "authors": [
            "Lars Ruthotto",
            "E. Haber"
        ],
        "citations": 203,
        "references": 54,
        "year": 2021
    },
    {
        "title": "Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks",
        "abstract": null,
        "authors": [
            "V. Sandfort",
            "K. Yan",
            "P. Pickhardt",
            "R. Summers"
        ],
        "citations": 509,
        "references": 22,
        "year": 2019
    },
    {
        "title": "GraphGAN: Graph Representation Learning with Generative Adversarial Nets",
        "abstract": "\n \n The goal of graph representation learning is to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in the graph, and discriminative models that predict the probability of edge existence between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying above two classes of methods, in which the generative model and discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces \"fake\" samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, when considering the implementation of generative model, we propose a novel graph softmax to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including link prediction, node classification, and recommendation, over state-of-the-art baselines.\n \n",
        "authors": [
            "Hongwei Wang",
            "Jia Wang",
            "Jialin Wang",
            "Miao Zhao",
            "Weinan Zhang",
            "Fuzheng Zhang",
            "Xing Xie",
            "M. Guo"
        ],
        "citations": 592,
        "references": 36,
        "year": 2017
    },
    {
        "title": "The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks",
        "abstract": "This paper studies model-inversion attacks, in which the access to a model is abused to infer information about the training data. Since its first introduction by~\\cite{fredrikson2014privacy}, such attacks have raised serious concerns given that training data usually contain privacy sensitive information. Thus far, successful model-inversion attacks have only been demonstrated on simple models, such as linear regression and logistic regression. Previous attempts to invert neural networks, even the ones with simple architectures, have failed to produce convincing results. Here we present a novel attack method, termed the \\emph{generative model-inversion attack}, which can invert deep neural networks with high success rates. Rather than reconstructing private training data from scratch, we leverage partial public information, which can be very generic, to learn a distributional prior via generative adversarial networks (GANs) and use it to guide the inversion process. Moreover, we theoretically prove that a model's predictive power and its vulnerability to inversion attacks are indeed two sides of the same coin---highly predictive models are able to establish a strong correlation between features and labels, which coincides exactly with what an adversary exploits to mount the attacks. Our extensive experiments demonstrate that the proposed attack improves identification accuracy over the existing work by about $75\\%$ for reconstructing face images from a state-of-the-art face recognition classifier. We also show that differential privacy, in its canonical form, is of little avail to defend against our attacks.",
        "authors": [
            "Yuheng Zhang",
            "R. Jia",
            "Hengzhi Pei",
            "Wenxiao Wang",
            "Bo Li",
            "D. Song"
        ],
        "citations": 359,
        "references": 29,
        "year": 2019
    },
    {
        "title": "Permutation Invariant Graph Generation via Score-Based Generative Modeling",
        "abstract": "Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.",
        "authors": [
            "Chenhao Niu",
            "Yang Song",
            "Jiaming Song",
            "Shengjia Zhao",
            "Aditya Grover",
            "Stefano Ermon"
        ],
        "citations": 223,
        "references": 45,
        "year": 2020
    },
    {
        "title": "Structured Denoising Diffusion Models in Discrete State-Spaces",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.",
        "authors": [
            "Jacob Austin",
            "Daniel D. Johnson",
            "Jonathan Ho",
            "Daniel Tarlow",
            "Rianne van den Berg"
        ],
        "citations": 695,
        "references": 71,
        "year": 2021
    },
    {
        "title": "SMPLicit: Topology-aware Generative Model for Clothed People",
        "abstract": "In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.",
        "authors": [
            "Enric Corona",
            "Albert Pumarola",
            "G. Alenyà",
            "Gerard Pons-Moll",
            "F. Moreno-Noguer"
        ],
        "citations": 174,
        "references": 71,
        "year": 2021
    },
    {
        "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
        "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. \nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.",
        "authors": [
            "David Bau",
            "Jun-Yan Zhu",
            "Hendrik Strobelt",
            "Bolei Zhou",
            "J. Tenenbaum",
            "W. Freeman",
            "A. Torralba"
        ],
        "citations": 447,
        "references": 7,
        "year": 2018
    },
    {
        "title": "Equivariant Flows: exact likelihood generative learning for symmetric densities",
        "abstract": "Normalizing flows are exact-likelihood generative neural networks which approximately transform samples from a simple prior distribution to samples of the probability distribution of interest. Recent work showed that such generative models can be utilized in statistical mechanics to sample equilibrium states of many-body systems in physics and chemistry. To scale and generalize these results, it is essential that the natural symmetries in the probability density - in physics defined by the invariances of the target potential - are built into the flow. We provide a theoretical sufficient criterion showing that the distribution generated by equivariant normalizing flows is invariant with respect to these symmetries by design. Furthermore, we propose building blocks for flows which preserve symmetries which are usually found in physical/chemical many-body particle systems. Using benchmark systems motivated from molecular physics, we demonstrate that those symmetry preserving flows can provide better generalization capabilities and sampling efficiency.",
        "authors": [
            "Jonas Köhler",
            "Leon Klein",
            "F. Noé"
        ],
        "citations": 235,
        "references": 50,
        "year": 2020
    },
    {
        "title": "Data Synthesis based on Generative Adversarial Networks",
        "abstract": "\n Privacy is an important concern for our society where sharing data with partners or releasing data to the public is a frequent occurrence. Some of the techniques that are being used to achieve privacy are to remove identifiers, alter quasi-identifiers, and perturb values. Unfortunately, these approaches suffer from two limitations. First, it has been shown that private information can still be leaked if attackers possess some background knowledge or other information sources. Second, they do not take into account the adverse impact these methods will have on the utility of the released data. In this paper, we propose a method that meets both requirements. Our method, called\n table-GAN\n , uses generative adversarial networks (GANs) to synthesize fake tables that are statistically similar to the original table yet do not incur information leakage. We show that the machine learning models trained using our synthetic tables exhibit performance that is similar to that of models trained using the original table for unknown testing cases. We call this property\n model compatibility\n . We believe that anonymization/perturbation/synthesis methods without model compatibility are of little value. We used four real-world datasets from four different domains for our experiments and conducted indepth comparisons with state-of-the-art anonymization, perturbation, and generation techniques. Throughout our experiments, only our method consistently shows balance between privacy level and model compatibility.\n",
        "authors": [
            "Noseong Park",
            "Mahmoud Mohammadi",
            "Kshitij Gorde",
            "S. Jajodia",
            "Hongkyu Park",
            "Youngmin Kim"
        ],
        "citations": 423,
        "references": 31,
        "year": 2018
    },
    {
        "title": "NeRF-VAE: A Geometry Aware 3D Scene Generative Model",
        "abstract": "We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.",
        "authors": [
            "Adam R. Kosiorek",
            "Heiko Strathmann",
            "Daniel Zoran",
            "Pol Moreno",
            "R. Schneider",
            "Sovna Mokr'a",
            "Danilo Jimenez Rezende"
        ],
        "citations": 130,
        "references": 54,
        "year": 2021
    },
    {
        "title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations",
        "abstract": "Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, however, do not account for component interactions. Hence, neither of them allows for principled sampling of novel scenes. Here we present GENESIS, the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes by capturing relationships between scene components. GENESIS parameterises a spatial GMM over images which is decoded from a set of object-centric latent variables that are either inferred sequentially in an amortised fashion or sampled from an autoregressive prior. We train GENESIS on several publicly available datasets and evaluate its performance on scene generation, decomposition, and semi-supervised learning.",
        "authors": [
            "Martin Engelcke",
            "Adam R. Kosiorek",
            "Oiwi Parker Jones",
            "I. Posner"
        ],
        "citations": 296,
        "references": 60,
        "year": 2019
    },
    {
        "title": "Generative adversarial network for road damage detection",
        "abstract": "Machine learning can produce promising results when sufficient training data are available; however, infrastructure inspections typically do not provide sufficient training data for road damage. Given the differences in the environment, the type of road damage and the degree of its progress can vary from structure to structure. The use of generative models, such as a generative adversarial network (GAN) or a variational autoencoder, makes it possible to generate a pseudoimage that cannot be distinguished from a real one. Combining a progressive growing GAN along with Poisson blending artificially generates road damage images that can be used as new training data to improve the accuracy of road damage detection. The addition of a synthesized road damage image to the training data improves the F‐measure by 5% and 2% when the number of original images is small and relatively large, respectively. All of the results and the new Road Damage Dataset 2019 are publicly available (https://github.com/sekilab/RoadDamageDetector).",
        "authors": [
            "Hiroya Maeda",
            "Takehiro Kashiyama",
            "Y. Sekimoto",
            "Toshikazu Seto",
            "Hiroshi Omata"
        ],
        "citations": 201,
        "references": 66,
        "year": 2020
    },
    {
        "title": "Generate to Adapt: Aligning Domains Using Generative Adversarial Networks",
        "abstract": "Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS.",
        "authors": [
            "S. Sankaranarayanan",
            "Y. Balaji",
            "C. Castillo",
            "R. Chellappa"
        ],
        "citations": 637,
        "references": 44,
        "year": 2017
    },
    {
        "title": "A Survey on Generative Adversarial Networks: Variants, Applications, and Training",
        "abstract": "The Generative Models have gained considerable attention in unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to their outstanding data generation capability. Many GAN models have been proposed, and several practical applications have emerged in various domains of computer vision and machine learning. Despite GANs excellent success, there are still obstacles to stable training. The problems are Nash equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GANs. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We discuss (I) the original GAN model and its modified versions, (II) a detailed analysis of various GAN applications in different domains, and (III) a detailed study about the various GAN training obstacles as well as training solutions. Finally, we reveal several issues as well as research outlines to the topic.",
        "authors": [
            "Abdul Jabbar",
            "Xi Li",
            "Bourahla Omar"
        ],
        "citations": 239,
        "references": 264,
        "year": 2020
    },
    {
        "title": "Learning Neural Generative Dynamics for Molecular Conformation Generation",
        "abstract": "We study how to generate molecule conformations (\\textit{i.e.}, 3D structures) from a molecular graph. Traditional methods, such as molecular dynamics, sample conformations via computationally expensive simulations. Recently, machine learning methods have shown great potential by training on a large collection of conformation data. Challenges arise from the limited model capacity for capturing complex distributions of conformations and the difficulty in modeling long-range dependencies between atoms. Inspired by the recent progress in deep generative models, in this paper, we propose a novel probabilistic framework to generate valid and diverse conformations given a molecular graph. We propose a method combining the advantages of both flow-based and energy-based models, enjoying: (1) a high model capacity to estimate the multimodal conformation distribution; (2) explicitly capturing the complex long-range dependencies between atoms in the observation space. Extensive experiments demonstrate the superior performance of the proposed method on several benchmarks, including conformation generation and distance modeling tasks, with a significant improvement over existing generative models for molecular conformation sampling.",
        "authors": [
            "Minkai Xu",
            "Shitong Luo",
            "Y. Bengio",
            "Jian Peng",
            "Jian Tang"
        ],
        "citations": 109,
        "references": 44,
        "year": 2021
    },
    {
        "title": "Regularizing Generative Adversarial Networks under Limited Data",
        "abstract": "Recent years have witnessed the rapid progress of generative adversarial networks (GANs). However, the success of the GAN models hinges on a large amount of training data. This work proposes a regularization approach for training robust GAN models on limited data. We theoretically show a connection between the regularized loss and an f-divergence called LeCam-divergence, which we find is more robust under limited training data. Extensive experiments on several benchmark datasets demonstrate that the proposed regularization scheme 1) improves the generalization performance and stabilizes the learning dynamics of GAN models under limited training data, and 2) complements the recent data augmentation methods. These properties facilitate training GAN models to achieve state-of-theart performance when only limited training data of the ImageNet benchmark is available. The source code is available at https://github.com/google/lecam-gan.",
        "authors": [
            "Hung-Yu Tseng",
            "Lu Jiang",
            "Ce Liu",
            "Ming-Hsuan Yang",
            "Weilong Yang"
        ],
        "citations": 132,
        "references": 88,
        "year": 2021
    },
    {
        "title": "Generative Adversarial Networks (GANs)",
        "abstract": "Generative Adversarial Networks (GANs) is a novel class of deep generative models that has recently gained significant attention. GANs learn complex and high-dimensional distributions implicitly over images, audio, and data. However, there exist major challenges in training of GANs, i.e., mode collapse, non-convergence, and instability, due to inappropriate design of network architectre, use of objective function, and selection of optimization algorithm. Recently, to address these challenges, several solutions for better design and optimization of GANs have been investigated based on techniques of re-engineered network architectures, new objective functions, and alternative optimization algorithms. To the best of our knowledge, there is no existing survey that has particularly focused on the broad and systematic developments of these solutions. In this study, we perform a comprehensive survey of the advancements in GANs design and optimization solutions proposed to handle GANs challenges. We first identify key research issues within each design and optimization technique and then propose a new taxonomy to structure solutions by key research issues. In accordance with the taxonomy, we provide a detailed discussion on different GANs variants proposed within each solution and their relationships. Finally, based on the insights gained, we present promising research directions in this rapidly growing field.",
        "authors": [
            "Divya Saxena",
            "Jiannong Cao"
        ],
        "citations": 239,
        "references": 257,
        "year": 2020
    },
    {
        "title": "DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis",
        "abstract": "Synthesizing high-resolution realistic images from text descriptions is a challenging task. Almost all existing text-to-image methods employ stacked generative adversarial networks as the backbone, utilize cross-modal attention mechanisms to fuse text and image features, and use extra networks to ensure text-image semantic consistency. The existing text-to-image models have three problems: 1) For the backbone, there are multiple generators and discriminators stacked for generating different scales of images making the training process slow and inefficient. 2) For semantic consistency, the existing models employ extra networks to ensure the semantic consistency increasing the training complexity and bringing an additional computational cost. 3) For the text-image feature fusion method, cross-modal attention is only applied a few times during the generation process due to its computational cost impeding fusing the text and image features deeply. To solve these limitations, we propose 1) a novel simplified text-to-image backbone which is able to synthesize high-quality images directly by one pair of generator and discriminator, 2) a novel regularization method called Matching-Aware zero-centered Gradient Penalty which promotes the generator to synthesize more realistic and text-image semantic consistent images without introducing extra networks, 3) a novel fusion module called Deep Text-Image Fusion Block which can exploit the semantics of text descriptions effectively and fuse text and image features deeply during the generation process. Compared with the previous text-to-image models, our DF-GAN is simpler and more efficient and achieves better performance. Extensive experiments and ablation studies on both Caltech-UCSD Birds 200 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models.",
        "authors": [
            "Ming Tao",
            "H. Tang",
            "Songsong Wu",
            "N. Sebe",
            "Fei Wu",
            "Xiaoyuan Jing"
        ],
        "citations": 201,
        "references": 57,
        "year": 2020
    },
    {
        "title": "Gradient Inversion with Generative Image Prior",
        "abstract": "Federated Learning (FL) is a distributed learning framework, in which the local data never leaves clients devices to preserve privacy, and the server trains models on the data via accessing only the gradients of those local data. Without further privacy mechanisms such as differential privacy, this leaves the system vulnerable against an attacker who inverts those gradients to reveal clients sensitive data. However, a gradient is often insufficient to reconstruct the user data without any prior knowledge. By exploiting a generative model pretrained on the data distribution, we demonstrate that data privacy can be easily breached. Further, when such prior knowledge is unavailable, we investigate the possibility of learning the prior from a sequence of gradients seen in the process of FL training. We experimentally show that the prior in a form of generative model is learnable from iterative interactions in FL. Our findings strongly suggest that additional mechanisms are necessary to prevent privacy leakage in FL.",
        "authors": [
            "Jinwoo Jeon",
            "Jaechang Kim",
            "Kangwook Lee",
            "Sewoong Oh",
            "Jungseul Ok"
        ],
        "citations": 135,
        "references": 35,
        "year": 2021
    },
    {
        "title": "MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment",
        "abstract": "\n \n Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/.\n \n",
        "authors": [
            "Hao-Wen Dong",
            "Wen-Yi Hsiao",
            "Li-Chia Yang",
            "Yi-Hsuan Yang"
        ],
        "citations": 517,
        "references": 35,
        "year": 2017
    },
    {
        "title": "Multi-objective de novo drug design with conditional graph generative model",
        "abstract": null,
        "authors": [
            "Yibo Li",
            "L. Zhang",
            "Zhenming Liu"
        ],
        "citations": 320,
        "references": 73,
        "year": 2018
    },
    {
        "title": "Graphite: Iterative Generative Modeling of Graphs",
        "abstract": "Graphs are a fundamental abstraction for modeling relational data. However, graphs are discrete and combinatorial in nature, and learning representations suitable for machine learning tasks poses statistical and computational challenges. In this work, we propose Graphite, an algorithmic framework for unsupervised learning of representations over nodes in large graphs using deep latent variable generative models. Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding. On a wide variety of synthetic and benchmark datasets, Graphite outperforms competing approaches for the tasks of density estimation, link prediction, and node classification. Finally, we derive a theoretical connection between message passing in graph neural networks and mean-field variational inference.",
        "authors": [
            "Aditya Grover",
            "Aaron Zweig",
            "Stefano Ermon"
        ],
        "citations": 289,
        "references": 84,
        "year": 2018
    },
    {
        "title": "High‐Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks",
        "abstract": "High‐throughput screening has become one of the major strategies for the discovery of novel functional materials. However, its effectiveness is severely limited by the lack of sufficient and diverse materials in current materials repositories such as the open quantum materials database (OQMD). Recent progress in deep learning have enabled generative strategies that learn implicit chemical rules for creating hypothetical materials with new compositions and structures. However, current materials generative models have difficulty in generating structurally diverse, chemically valid, and stable materials. Here we propose CubicGAN, a generative adversarial network (GAN) based deep neural network model for large scale generative design of novel cubic materials. When trained on 375 749 ternary materials from the OQMD database, the authors show that the model is able to not only rediscover most of the currently known cubic materials but also generate hypothetical materials of new structure prototypes. A total of 506 such materials have been verified by phonon dispersion calculation. Considering the importance of cubic materials in wide applications such as solar panels, the GAN model provides a promising approach to significantly expand existing materials repositories, enabling the discovery of new functional materials via screening. The new crystal structures discovered are freely accessible at www.carolinamatdb.org.",
        "authors": [
            "Yong Zhao",
            "Mohammed Al-fahdi",
            "Ming Hu",
            "E. Siriwardane",
            "Yuqi Song",
            "Alireza Nasiri",
            "Jianjun Hu"
        ],
        "citations": 97,
        "references": 65,
        "year": 2021
    },
    {
        "title": "Learning to Dress 3D People in Generative Clothing",
        "abstract": "Three-dimensional human body models are widely used in the analysis of human pose and motion. Existing models, however, are learned from minimally-clothed 3D scans and thus do not generalize to the complexity of dressed people in common images and videos. Additionally, current models lack the expressive power needed to represent the complex non-linear geometry of pose-dependent clothing shapes. To address this, we learn a generative 3D mesh model of clothed people from 3D scans with varying pose and clothing. Specifically, we train a conditional Mesh-VAE-GAN to learn the clothing deformation from the SMPL body model, making clothing an additional term in SMPL. Our model is conditioned on both pose and clothing type, giving the ability to draw samples of clothing to dress different body shapes in a variety of styles and poses. To preserve wrinkle detail, our Mesh-VAE-GAN extends patchwise discriminators to 3D meshes. Our model, named CAPE, represents global shape and fine local structure, effectively extending the SMPL body model to clothing. To our knowledge, this is the first generative model that directly dresses 3D human body meshes and generalizes to different poses. The model, code and data are available for research purposes at https://cape.is.tue.mpg.de.",
        "authors": [
            "Qianli Ma",
            "Jinlong Yang",
            "Anurag Ranjan",
            "S. Pujades",
            "Gerard Pons-Moll",
            "Siyu Tang",
            "Michael J. Black"
        ],
        "citations": 327,
        "references": 99,
        "year": 2019
    },
    {
        "title": "GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction",
        "abstract": "In the past few years, a lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of Deep Convolutional Neural Networks (DCNNs). In the most recent works, differentiable renderers were employed in order to learn the relationship between the facial identity features and the parameters of a 3D morphable model for shape and texture. The texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction of the state-of-the-art methods is still not capable of modeling textures in high fidelity. In this paper, we take a radically different approach and harness the power of Generative Adversarial Networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful generator of facial texture in UV space. Then, we revisit the original 3D Morphable Models (3DMMs) fitting approaches making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. We optimize the parameters with the supervision of pretrained deep identity features through our end-to-end differentiable framework. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.",
        "authors": [
            "Baris Gecer",
            "Stylianos Ploumpis",
            "I. Kotsia",
            "S. Zafeiriou"
        ],
        "citations": 327,
        "references": 52,
        "year": 2019
    },
    {
        "title": "Evolutionary Generative Adversarial Networks",
        "abstract": "Generative adversarial networks (GANs) have been effective for learning generative models for real-world data. However, accompanied with the generative tasks becoming more and more challenging, existing GANs (GAN and its variants) tend to suffer from different training problems such as instability and mode collapse. In this paper, we propose a novel GAN framework called evolutionary GANs (E-GANs) for stable GAN training and improved generative performance. Unlike existing GANs, which employ a predefined adversarial objective function alternately training a generator and a discriminator, we evolve a population of generators to play the adversarial game with the discriminator. Different adversarial training objectives are employed as mutation operations and each individual (i.e., generator candidature) are updated based on these mutations. Then, we devise an evaluation mechanism to measure the quality and diversity of generated samples, such that only well-performing generator(s) are preserved and used for further training. In this way, E-GAN overcomes the limitations of an individual adversarial training objective and always preserves the well-performing offspring, contributing to progress in, and the success of GANs. Experiments on several datasets demonstrate that E-GAN achieves convincing generative performance and reduces the training problems inherent in existing GANs.",
        "authors": [
            "Chaoyue Wang",
            "Chang Xu",
            "Xin Yao",
            "D. Tao"
        ],
        "citations": 270,
        "references": 82,
        "year": 2018
    },
    {
        "title": "Generative Language Modeling for Automated Theorem Proving",
        "abstract": "We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.",
        "authors": [
            "Stanislas Polu",
            "I. Sutskever"
        ],
        "citations": 247,
        "references": 48,
        "year": 2020
    },
    {
        "title": "Generative Adversarial Networks for Crystal Structure Prediction",
        "abstract": "The constant demand for novel functional materials calls for efficient strategies to accelerate the materials discovery, and crystal structure prediction is one of the most fundamental tasks along that direction. In addressing this challenge, generative models can offer new opportunities since they allow for the continuous navigation of chemical space via latent spaces. In this work, we employ a crystal representation that is inversion-free based on unit cell and fractional atomic coordinates and build a generative adversarial network for crystal structures. The proposed model is applied to generate the Mg–Mn–O ternary materials with the theoretical evaluation of their photoanode properties for high-throughput virtual screening (HTVS). The proposed generative HTVS framework predicts 23 new crystal structures with reasonable calculated stability and band gap. These findings suggest that the generative model can be an effective way to explore hidden portions of the chemical space, an area that is usually unreachable when conventional substitution-based discovery is employed.",
        "authors": [
            "Sungwon Kim",
            "Juhwan Noh",
            "Geun Ho Gu",
            "A. Aspuru‐Guzik",
            "Yousung Jung"
        ],
        "citations": 158,
        "references": 80,
        "year": 2020
    },
    {
        "title": "Density estimation using deep generative neural networks",
        "abstract": "Significance Density estimation is among the most fundamental problems in statistics. It is notoriously difficult to estimate the density of high-dimensional data due to the “curse of dimensionality.” Here, we introduce a new general-purpose density estimator based on deep generative neural networks. By modeling data normally distributed around a manifold of reduced dimension, we show how the power of bidirectional generative neural networks (e.g., cycleGAN) can be exploited for explicit evaluation of the data density. Simulation and real data experiments suggest that our method is effective in a wide range of problems. This approach should be helpful in many applications where an accurate density estimator is needed. Density estimation is one of the fundamental problems in both statistics and machine learning. In this study, we propose Roundtrip, a computational framework for general-purpose density estimation based on deep generative neural networks. Roundtrip retains the generative power of deep generative models, such as generative adversarial networks (GANs) while it also provides estimates of density values, thus supporting both data generation and density estimation. Unlike previous neural density estimators that put stringent conditions on the transformation from the latent space to the data space, Roundtrip enables the use of much more general mappings where target density is modeled by learning a manifold induced from a base density (e.g., Gaussian distribution). Roundtrip provides a statistical framework for GAN models where an explicit evaluation of density values is feasible. In numerical experiments, Roundtrip exceeds state-of-the-art performance in a diverse range of density estimation tasks.",
        "authors": [
            "Qiao Liu",
            "Jiaze Xu",
            "R. Jiang",
            "Wing Hong Wong"
        ],
        "citations": 87,
        "references": 36,
        "year": 2021
    },
    {
        "title": "Generative Deep Learning for Targeted Compound Design",
        "abstract": "In the past few years, de novo molecular design has increasingly been using generative models from the emergent field of Deep Learning, proposing novel compounds that are likely to possess desired properties or activities. De novo molecular design finds applications in different fields ranging from drug discovery and materials sciences to biotechnology. A panoply of deep generative models, including architectures as Recurrent Neural Networks, Autoencoders, and Generative Adversarial Networks, can be trained on existing data sets and provide for the generation of novel compounds. Typically, the new compounds follow the same underlying statistical distributions of properties exhibited on the training data set Additionally, different optimization strategies, including transfer learning, Bayesian optimization, reinforcement learning, and conditional generation, can direct the generation process toward desired aims, regarding their biological activities, synthesis processes or chemical features. Given the recent emergence of these technologies and their relevance, this work presents a systematic and critical review on deep generative models and related optimization methods for targeted compound design, and their applications.",
        "authors": [
            "Tiago Sousa",
            "João Correia",
            "Vítor Pereira",
            "Miguel Rocha"
        ],
        "citations": 89,
        "references": 89,
        "year": 2021
    },
    {
        "title": "PolyGen: An Autoregressive Generative Model of 3D Meshes",
        "abstract": "Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.",
        "authors": [
            "C. Nash",
            "Yaroslav Ganin",
            "A. Eslami",
            "P. Battaglia"
        ],
        "citations": 222,
        "references": 41,
        "year": 2020
    },
    {
        "title": "BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling",
        "abstract": "With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.",
        "authors": [
            "Lars Maaløe",
            "Marco Fraccaro",
            "Valentin Liévin",
            "O. Winther"
        ],
        "citations": 210,
        "references": 64,
        "year": 2019
    },
    {
        "title": "Learning Generative Vision Transformer with Energy-Based Latent Space for Saliency Prediction",
        "abstract": "Vision transformer networks have shown superiority in many computer vision tasks. In this paper, we take a step further by proposing a novel generative vision transformer with latent variables following an informative energy-based prior for salient object detection. Both the vision transformer network and the energy-based prior model are jointly trained via Markov chain Monte Carlo-based maximum likelihood estimation, in which the sampling from the intractable posterior and prior distributions of the latent variables are performed by Langevin dynamics. Further, with the generative vision transformer, we can easily obtain a pixel-wise uncertainty map from an image, which indicates the model confidence in predicting saliency from the image. Different from the existing generative models which define the prior distribution of the latent variables as a simple isotropic Gaussian distribution, our model uses an energy-based informative prior which can be more expressive to capture the latent space of the data. We apply the proposed framework to both RGB and RGB-D salient object detection tasks. Extensive experimental results show that our framework can achieve not only accurate saliency predictions but also meaningful uncertainty maps that are consistent with the human perception.",
        "authors": [
            "Jing Zhang",
            "Jianwen Xie",
            "Nick Barnes",
            "Ping Li"
        ],
        "citations": 83,
        "references": 100,
        "year": 2021
    },
    {
        "title": "CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields",
        "abstract": "Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. While this leads to impressive 3D consistency, the camera needs to be modelled as well and we show in this work that these methods are sensitive to the choice of prior camera distributions. Current approaches assume fixed intrinsics and predefined priors over camera pose ranges, and parameter tuning is typically required for real-world data. If the data distribution is not matched, results degrade significantly. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.",
        "authors": [
            "Michael Niemeyer",
            "Andreas Geiger"
        ],
        "citations": 66,
        "references": 78,
        "year": 2021
    },
    {
        "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm",
        "abstract": "Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.",
        "authors": [
            "Laria Reynolds",
            "Kyle McDonell"
        ],
        "citations": 733,
        "references": 26,
        "year": 2021
    },
    {
        "title": "Generative adversarial networks: introduction and outlook",
        "abstract": "Recently, generative adversarial networks U+0028 GANs U+0029 have become a research focus of artificial intelligence. Inspired by two-player zero-sum game, GANs comprise a generator and a discriminator, both trained under the adversarial learning idea. The goal of GANs is to estimate the potential distribution of real data samples and generate new samples from that distribution. Since their initiation, GANs have been widely studied due to their enormous prospect for applications, including image and vision computing, speech and language processing, etc. In this review paper, we summarize the state of the art of GANs and look into the future. Firstly, we survey GANs U+02BC proposal background, theoretic and implementation models, and application fields. Then, we discuss GANs U+02BC advantages and disadvantages, and their development trends. In particular, we investigate the relation between GANs and parallel intelligence, with the conclusion that GANs have a great potential in parallel systems research in terms of virtual-real interaction and integration. Clearly, GANs can provide substantial algorithmic support for parallel intelligence.",
        "authors": [
            "Kunfeng Wang",
            "Chao Gou",
            "Y. Duan",
            "Yilun Lin",
            "Xinhu Zheng",
            "Fei-yue Wang"
        ],
        "citations": 469,
        "references": 65,
        "year": 2017
    },
    {
        "title": "CIAGAN: Conditional Identity Anonymization Generative Adversarial Networks",
        "abstract": "The unprecedented increase in the usage of computer vision technology in society goes hand in hand with an increased concern in data privacy. In many real-world scenarios like people tracking or action recognition, it is important to be able to process the data while taking careful consideration in protecting people's identity. We propose and develop CIAGAN, a model for image and video anonymization based on conditional generative adversarial networks. Our model is able to remove the identifying characteristics of faces and bodies while producing high-quality images and videos that can be used for any computer vision task, such as detection or tracking. Unlike previous methods, we have full control over the de-identification (anonymization) procedure, ensuring both anonymization as well as diversity. We compare our method to several baselines and achieve state-of-the-art results. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/ciagan.",
        "authors": [
            "Maxim Maximov",
            "Ismail Elezi",
            "L. Leal-Taix'e"
        ],
        "citations": 157,
        "references": 46,
        "year": 2020
    },
    {
        "title": "Generative Speech Coding with Predictive Variance Regularization",
        "abstract": "The recent emergence of machine-learning based generative models for speech suggests a significant reduction in bit rate for speech codecs is possible. However, the performance of generative models deteriorates significantly with the distortions present in real-world input signals. We argue that this deterioration is due to the sensitivity of the maximum likelihood criterion to outliers and the ineffectiveness of modeling a sum of independent signals with a single autoregressive model. We introduce predictive-variance regularization to reduce the sensitivity to outliers, resulting in a significant increase in performance. We show that noise reduction to remove unwanted signals can significantly increase performance. We provide extensive subjective performance evaluations that show that our system based on generative modeling provides state-of-the-art coding performance at 3 kb/s for real-world speech signals at reasonable computational complexity.",
        "authors": [
            "W. Kleijn",
            "Andrew Storus",
            "Michael Chinen",
            "Tom Denton",
            "Felicia S. C. Lim",
            "Alejandro Luebs",
            "J. Skoglund",
            "Hengchin Yeh"
        ],
        "citations": 64,
        "references": 32,
        "year": 2021
    },
    {
        "title": "Generative Modeling with Optimal Transport Maps",
        "abstract": "With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a powerful tool for large-scale generative modeling tasks. In these tasks, OT cost is typically used as the loss for training GANs. In contrast to this approach, we show that the OT map itself can be used as a generative model, providing comparable performance. Previous analogous approaches consider OT maps as generative models only in the latent spaces due to their poor performance in the original high-dimensional ambient space. In contrast, we apply OT maps directly in the ambient space, e.g., a space of high-dimensional images. First, we derive a min-max optimization algorithm to efficiently compute OT maps for the quadratic cost (Wasserstein-2 distance). Next, we extend the approach to the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. We evaluate the algorithm on image generation and unpaired image restoration tasks. In particular, we consider denoising, colorization, and inpainting, where the optimality of the restoration map is a desired attribute, since the output (restored) image is expected to be close to the input (degraded) one.",
        "authors": [
            "Litu Rout",
            "Alexander Korotin",
            "Evgeny Burnaev"
        ],
        "citations": 65,
        "references": 70,
        "year": 2021
    },
    {
        "title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
        "abstract": "Generative commonsense reasoning which aims to empower machines to generate sentences with the capacity of reasoning over a set of concepts is a critical bottleneck for text generation. Even the state-of-the-art pre-trained language generation models struggle at this task and often produce implausible and anomalous sentences. One reason is that they rarely consider incorporating the knowledge graph which can provide rich relational information among the commonsense concepts. To promote the ability of commonsense reasoning for text generation, we propose a novel knowledge graph augmented pre-trained language generation model KG-BART, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output. Moreover, KG-BART can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets. Experiments on benchmark CommonGen dataset verify the effectiveness of our proposed approach by comparing with several strong pre-trained language generation models, particularly KG-BART outperforms BART by 5.80, 4.60, in terms of BLEU-3, 4. Moreover, we also show that the generated context by our model can work as background scenarios to benefit downstream commonsense QA tasks.",
        "authors": [
            "Ye Liu",
            "Yao Wan",
            "Lifang He",
            "Hao Peng",
            "Philip S. Yu"
        ],
        "citations": 175,
        "references": 40,
        "year": 2020
    },
    {
        "title": "Rewriting a Deep Generative Model",
        "abstract": null,
        "authors": [
            "David Bau",
            "Steven Liu",
            "Tongzhou Wang",
            "Jun-Yan Zhu",
            "A. Torralba"
        ],
        "citations": 126,
        "references": 88,
        "year": 2020
    },
    {
        "title": "Leveraging Seen and Unseen Semantic Relationships for Generative Zero-Shot Learning",
        "abstract": null,
        "authors": [
            "Maunil R. Vyas",
            "Hemanth Venkateswara",
            "S. Panchanathan"
        ],
        "citations": 126,
        "references": 47,
        "year": 2020
    },
    {
        "title": "Stabilizing Training of Generative Adversarial Networks through Regularization",
        "abstract": "Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",
        "authors": [
            "Kevin Roth",
            "Aurélien Lucchi",
            "Sebastian Nowozin",
            "Thomas Hofmann"
        ],
        "citations": 433,
        "references": 31,
        "year": 2017
    },
    {
        "title": "druGAN: An Advanced Generative Adversarial Autoencoder Model for de Novo Generation of New Molecules with Desired Molecular Properties in Silico.",
        "abstract": "Deep generative adversarial networks (GANs) are the emerging technology in drug discovery and biomarker development. In our recent work, we demonstrated a proof-of-concept of implementing deep generative adversarial autoencoder (AAE) to identify new molecular fingerprints with predefined anticancer properties. Another popular generative model is the variational autoencoder (VAE), which is based on deep neural architectures. In this work, we developed an advanced AAE model for molecular feature extraction problems, and demonstrated its advantages compared to VAE in terms of (a) adjustability in generating molecular fingerprints; (b) capacity of processing very large molecular data sets; and (c) efficiency in unsupervised pretraining for regression model. Our results suggest that the proposed AAE model significantly enhances the capacity and efficiency of development of the new molecules with specific anticancer properties using the deep generative models.",
        "authors": [
            "Artur Kadurin",
            "S. Nikolenko",
            "Kuzma Khrabrov",
            "A. Aliper",
            "A. Zhavoronkov"
        ],
        "citations": 447,
        "references": 28,
        "year": 2017
    },
    {
        "title": "Generative Adversarial Perturbations",
        "abstract": "In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.",
        "authors": [
            "Omid Poursaeed",
            "Isay Katsman",
            "Bicheng Gao",
            "Serge J. Belongie"
        ],
        "citations": 333,
        "references": 64,
        "year": 2017
    },
    {
        "title": "Learning Graph Representation With Generative Adversarial Nets",
        "abstract": "Graph representation learning aims to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in a graph, and discriminative models that predict the probability of edge between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying the above two classes of methods, in which the generative and the discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces “fake” samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, we propose a novel graph softmax as the implementation of the generative model to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including graph reconstruction, link prediction, node classification, recommendation, and visualization, over state-of-the-art baselines.",
        "authors": [
            "Hongwei Wang",
            "Jialin Wang",
            "Jia Wang",
            "Miao Zhao",
            "Weinan Zhang",
            "Fuzheng Zhang",
            "Wenjie Li",
            "Xing Xie",
            "M. Guo"
        ],
        "citations": 65,
        "references": 0,
        "year": 2021
    },
    {
        "title": "Deep Fluids: A Generative Network for Parameterized Fluid Simulations",
        "abstract": "This paper presents a novel generative model to synthesize fluid simulations from a set of reduced parameters. A convolutional neural network is trained on a collection of discrete, parameterizable fluid simulation velocity fields. Due to the capability of deep learning architectures to learn representative features of the data, our generative model is able to accurately approximate the training data set, while providing plausible interpolated in‐betweens. The proposed generative model is optimized for fluids by a novel loss function that guarantees divergence‐free velocity fields at all times. In addition, we demonstrate that we can handle complex parameterizations in reduced spaces, and advance simulations in time by integrating in the latent space with a second network. Our method models a wide variety of fluid behaviors, thus enabling applications such as fast construction of simulations, interpolation of fluids with different parameters, time re‐sampling, latent space simulations, and compression of fluid simulation data. Reconstructed velocity fields are generated up to 700× faster than re‐simulating the data with the underlying CPU solver, while achieving compression rates of up to 1300×.",
        "authors": [
            "Byungsoo Kim",
            "V. C. Azevedo",
            "N. Thürey",
            "Theodore Kim",
            "M. Gross",
            "B. Solenthaler"
        ],
        "citations": 371,
        "references": 63,
        "year": 2018
    },
    {
        "title": "Label-Efficient Semantic Segmentation with Diffusion Models",
        "abstract": "Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision.",
        "authors": [
            "Dmitry Baranchuk",
            "Ivan Rubachev",
            "A. Voynov",
            "Valentin Khrulkov",
            "Artem Babenko"
        ],
        "citations": 434,
        "references": 39,
        "year": 2021
    },
    {
        "title": "SMILES-based deep generative scaffold decorator for de-novo drug design",
        "abstract": null,
        "authors": [
            "Josep Arús‐Pous",
            "Atanas Patronov",
            "E. Bjerrum",
            "C. Tyrchan",
            "J. Reymond",
            "Hongming Chen",
            "O. Engkvist"
        ],
        "citations": 132,
        "references": 65,
        "year": 2020
    },
    {
        "title": "GuacaMol: Benchmarking Models for De Novo Molecular Design",
        "abstract": "De novo design seeks to generate molecules with required property profiles by virtual design-make-test cycles. With the emergence of deep learning and neural generative models in many application areas, models for molecular design based on neural networks appeared recently and show promising results. However, the new models have not been profiled on consistent tasks, and comparative studies to well-established algorithms have only seldom been performed. To standardize the assessment of both classical and neural models for de novo molecular design, we propose an evaluation framework, GuacaMol, based on a suite of standardized benchmarks. The benchmark tasks encompass measuring the fidelity of the models to reproduce the property distribution of the training sets, the ability to generate novel molecules, the exploration and exploitation of chemical space, and a variety of single and multiobjective optimization tasks. The benchmarking open-source Python code and a leaderboard can be found on https://benevolent.ai/guacamol .",
        "authors": [
            "Nathan Brown",
            "Marco Fiscato",
            "Marwin H. S. Segler",
            "Alain C. Vaucher"
        ],
        "citations": 634,
        "references": 97,
        "year": 2018
    },
    {
        "title": "MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation",
        "abstract": "Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting.",
        "authors": [
            "Li-Chia Yang",
            "Szu-Yu Chou",
            "Yi-Hsuan Yang"
        ],
        "citations": 447,
        "references": 42,
        "year": 2017
    },
    {
        "title": "FairGAN: Fairness-aware Generative Adversarial Networks",
        "abstract": "Fairness-aware learning is increasingly important in data mining. Discrimination prevention aims to prevent discrimination in the training data before it is used to conduct predictive analysis. In this paper, we focus on fair data generation that ensures the generated data is discrimination free. Inspired by generative adversarial networks (GAN), we present fairness-aware generative adversarial networks, called FairGAN, which are able to learn a generator producing fair data and also preserving good data utility. Compared with the naive fair data generation models, FairGAN further ensures the classifiers which are trained on generated data can achieve fair classification on real data. Experiments on a real dataset show the effectiveness of FairGAN.",
        "authors": [
            "Depeng Xu",
            "Shuhan Yuan",
            "Lu Zhang",
            "Xintao Wu"
        ],
        "citations": 287,
        "references": 29,
        "year": 2018
    },
    {
        "title": "PacGAN: The Power of Two Samples in Generative Adversarial Networks",
        "abstract": "Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why recently-proposed approaches mitigate mode collapse. We propose a principled approach to handle mode collapse called packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing—in particular the seminal result of (Blackwell, 1953)—to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well.",
        "authors": [
            "Zinan Lin",
            "A. Khetan",
            "G. Fanti",
            "Sewoong Oh"
        ],
        "citations": 312,
        "references": 77,
        "year": 2017
    },
    {
        "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images",
        "abstract": "We present a hierarchical VAE that, for the first time, outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that VAEs can actually implement autoregressive models, and other, more efficient generative models, if made sufficiently deep. Despite this, autoregressive models have traditionally outperformed VAEs. We test if insufficient depth explains the performance gap by by scaling a VAE to greater stochastic depth than previously explored and evaluating it on CIFAR-10, ImageNet, and FFHQ. We find that, in comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. We visualize the generative process and show the VAEs learn efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.",
        "authors": [
            "R. Child"
        ],
        "citations": 316,
        "references": 41,
        "year": 2020
    },
    {
        "title": "Shape-Based Generative Modeling for de Novo Drug Design",
        "abstract": "In this work, we propose a machine learning approach to generate novel molecules starting from a seed compound, its three-dimensional (3D) shape, and its pharmacophoric features. The pipeline draws inspiration from generative models used in image analysis and represents a first example of the de novo design of lead-like molecules guided by shape-based features. A variational autoencoder is used to perturb the 3D representation of a compound, followed by a system of convolutional and recurrent neural networks that generate a sequence of SMILES tokens. The generative design of novel scaffolds and functional groups can cover unexplored regions of chemical space that still possess lead-like properties.",
        "authors": [
            "Miha Škalič",
            "J. Jiménez",
            "Davide Sabbadin",
            "G. D. Fabritiis"
        ],
        "citations": 171,
        "references": 60,
        "year": 2019
    },
    {
        "title": "Quantum generative adversarial networks",
        "abstract": "Quantum machine learning is expected to be one of the first potential general-purpose applications of near-term quantum devices. A major recent breakthrough in classical machine learning is the notion of generative adversarial training, where the gradients of a discriminator model are used to train a separate generative model. In this work and a companion paper, we extend adversarial training to the quantum domain and show how to construct generative adversarial networks using quantum circuits. Furthermore, we also show how to compute gradients -- a key element in generative adversarial network training -- using another quantum circuit. We give an example of a simple practical circuit ansatz to parametrize quantum machine learning models and perform a simple numerical experiment to demonstrate that quantum generative adversarial networks can be trained successfully.",
        "authors": [
            "Pierre-Luc Dallaire-Demers",
            "N. Killoran"
        ],
        "citations": 327,
        "references": 35,
        "year": 2018
    },
    {
        "title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
        "abstract": "Generative models are becoming a tool of choice for exploring the molecular space. These models learn on a large training dataset and produce novel molecular structures with similar properties. Generated structures can be utilized for virtual screening or training semi-supervized predictive models in the downstream tasks. While there are plenty of generative models, it is unclear how to compare and rank them. In this work, we introduce a benchmarking platform called Molecular Sets (MOSES) to standardize training and comparison of molecular generative models. MOSES provides training and testing datasets, and a set of metrics to evaluate the quality and diversity of generated structures. We have implemented and compared several molecular generation models and suggest to use our results as reference points for further advancements in generative chemistry research. The platform and source code are available at https://github.com/molecularsets/moses.",
        "authors": [
            "Daniil Polykovskiy",
            "Alexander Zhebrak",
            "Benjamín Sánchez-Lengeling",
            "Sergey Golovanov",
            "Oktai Tatanov",
            "Stanislav Belyaev",
            "Rauf Kurbanov",
            "A. Artamonov",
            "V. Aladinskiy",
            "M. Veselov",
            "Artur Kadurin",
            "S. Nikolenko",
            "Alán Aspuru-Guzik",
            "A. Zhavoronkov"
        ],
        "citations": 576,
        "references": 124,
        "year": 2018
    },
    {
        "title": "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning",
        "abstract": "Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory/power scenarios (e.g., mobile). In this paper, we propose an effective way to fine-tune multiple down-stream generation tasks simultaneously using a single, large pretrained model. The experiments on five diverse language generation tasks show that by just using an additional 2-3% parameters for each task, our model can maintain or even improve the performance of fine-tuning the whole model.",
        "authors": [
            "Zhaojiang Lin",
            "Andrea Madotto",
            "Pascale Fung"
        ],
        "citations": 144,
        "references": 57,
        "year": 2020
    },
    {
        "title": "SDM-NET: Deep Generative Network for Structured Deformable Mesh",
        "abstract": "We introduce SDM-NET, a deep generative neural network which produces structured deformable meshes. Specifically, the network is trained to generate a spatial arrangement of closed, deformable mesh parts, which respect the global part structure of a shape collection, e.g., chairs, airplanes, etc. Our key observation is that while the overall structure of a 3D shape can be complex, the shape can usually be decomposed into a set of parts, each homeomorphic to a box, and the finer-scale geometry of the part can be recovered by deforming the box. The architecture of SDM-NET is that of a two-level variational autoencoder (VAE). At the part level, a PartVAE learns a deformable model of part geometries. At the structural level, we train a Structured Parts VAE (SP-VAE), which jointly learns the part structure of a shape collection and the part geometries, ensuring a coherence between global shape structure and surface details. Through extensive experiments and comparisons with the state-of-the-art deep generative models of shapes, we demonstrate the superiority of SDM-NET in generating meshes with visual quality, flexible topology, and meaningful structures, which benefit shape interpolation and other subsequently modeling tasks.",
        "authors": [
            "Lin Gao"
        ],
        "citations": 185,
        "references": 63,
        "year": 2019
    },
    {
        "title": "Recurrent World Models Facilitate Policy Evolution",
        "abstract": "A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io",
        "authors": [
            "David R Ha",
            "J. Schmidhuber"
        ],
        "citations": 833,
        "references": 101,
        "year": 2018
    },
    {
        "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
        "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, “How can we know when language models know, with confidence, the answer to a particular query?” We examine this question from the point of view of calibration, the property of a probabilistic model’s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models—T5, BART, and GPT-2—and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
        "authors": [
            "Zhengbao Jiang",
            "J. Araki",
            "Haibo Ding",
            "Graham Neubig"
        ],
        "citations": 351,
        "references": 77,
        "year": 2020
    },
    {
        "title": "SalGAN: Visual Saliency Prediction with Generative Adversarial Networks",
        "abstract": "We introduce SalGAN, a deep convolutional neural network for visual saliency prediction trained with adversarial examples. The first stage of the network consists of a generator model whose weights are learned by back-propagation computed from a binary cross entropy (BCE) loss over downsampled versions of the saliency maps. The resulting prediction is processed by a discriminator network trained to solve a binary classification task between the saliency maps generated by the generative stage and the ground truth ones. Our experiments show how adversarial training allows reaching state-of-the-art performance across different metrics when combined with a widely-used loss function like BCE. Our results can be reproduced with the source code and trained models available at https://imatge-upc.github. io/saliency-salgan-2017/.",
        "authors": [
            "Junting Pan",
            "C. Canton-Ferrer",
            "Kevin McGuinness",
            "N. O’Connor",
            "Jordi Torres",
            "E. Sayrol",
            "Xavier Giro-i-Nieto"
        ],
        "citations": 393,
        "references": 34,
        "year": 2017
    },
    {
        "title": "Generative language modeling for antibody design",
        "abstract": "Discovery and optimization of monoclonal antibodies for therapeutic applications relies on large sequence libraries, but is hindered by developability issues such as low solubility, low thermal stability, high aggregation, and high immunogenicity. Generative language models, trained on millions of protein sequences, are a powerful tool for on-demand generation of realistic, diverse sequences. We present Immunoglobulin Language Model (IgLM), a deep generative language model for creating synthetic libraries by re-designing variable-length spans of antibody sequences. IgLM formulates antibody design as an autoregressive sequence generation task based on text-infilling in natural language. We trained IgLM on 558M antibody heavy- and light-chain variable sequences, conditioning on each sequence’s chain type and species-of-origin. We demonstrate that IgLM can generate full-length heavy and light chain sequences from a variety of species, as well as infilled CDR loop libraries with improved developability profiles. IgLM is a powerful tool for antibody design and should be useful in a variety of applications.",
        "authors": [
            "Richard W. Shuai",
            "Jeffrey A. Ruffolo",
            "Jeffrey J. Gray"
        ],
        "citations": 72,
        "references": 54,
        "year": 2021
    },
    {
        "title": "Diversity-Sensitive Conditional Generative Adversarial Networks",
        "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative Adversarial Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.",
        "authors": [
            "Dingdong Yang",
            "Seunghoon Hong",
            "Y. Jang",
            "Tianchen Zhao",
            "Honglak Lee"
        ],
        "citations": 206,
        "references": 42,
        "year": 2019
    },
    {
        "title": "Voice Conversion from Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks",
        "abstract": "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.",
        "authors": [
            "Chin-Cheng Hsu",
            "Hsin-Te Hwang",
            "Yi-Chiao Wu",
            "Yu Tsao",
            "H. Wang"
        ],
        "citations": 311,
        "references": 25,
        "year": 2017
    },
    {
        "title": "Synthesizing Tabular Data using Generative Adversarial Networks",
        "abstract": "Generative adversarial networks (GANs) implicitly learn the probability distribution of a dataset and can draw samples from the distribution. This paper presents, Tabular GAN (TGAN), a generative adversarial network which can generate tabular data like medical or educational records. Using the power of deep neural networks, TGAN generates high-quality and fully synthetic tables while simultaneously generating discrete and continuous variables. When we evaluate our model on three datasets, we find that TGAN outperforms conventional statistical generative models in both capturing the correlation between columns and scaling up for large datasets.",
        "authors": [
            "L. Xu",
            "K. Veeramachaneni"
        ],
        "citations": 225,
        "references": 50,
        "year": 2018
    },
    {
        "title": "Generative Low-bitwidth Data Free Quantization",
        "abstract": null,
        "authors": [
            "Shoukai Xu",
            "Haokun Li",
            "Bohan Zhuang",
            "Jing Liu",
            "Jiezhang Cao",
            "Chuangrun Liang",
            "Mingkui Tan"
        ],
        "citations": 112,
        "references": 59,
        "year": 2020
    },
    {
        "title": "DeepPrivacy: A Generative Adversarial Network for Face Anonymization",
        "abstract": null,
        "authors": [
            "Håkon Hukkelås",
            "R. Mester",
            "F. Lindseth"
        ],
        "citations": 199,
        "references": 34,
        "year": 2019
    },
    {
        "title": "Fair Generative Modeling via Weak Supervision",
        "abstract": "Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6% over baselines for comparable image generation using generative adversarial networks.",
        "authors": [
            "Aditya Grover",
            "Kristy Choi",
            "Rui Shu",
            "Stefano Ermon"
        ],
        "citations": 129,
        "references": 89,
        "year": 2019
    },
    {
        "title": "Score-Based Generative Classifiers",
        "abstract": "The tremendous success of generative models in recent years raises the question whether they can also be used to perform classification. Generative models have been used as adversarially robust classifiers on simple datasets such as MNIST, but this robustness has not been observed on more complex datasets like CIFAR-10. Additionally, on natural image datasets, previous results have suggested a trade-off between the likelihood of the data and classification accuracy. In this work, we investigate score-based generative models as classifiers for natural images. We show that these models not only obtain competitive likelihood values but simultaneously achieve state-of-the-art classification accuracy for generative classifiers on CIFAR-10. Nevertheless, we find that these models are only slightly, if at all, more robust than discriminative baseline models on out-of-distribution tasks based on common image corruptions. Similarly and contrary to prior results, we find that score-based are prone to worst-case distribution shifts in the form of adversarial perturbations. Our work highlights that score-based generative models are closing the gap in classification accuracy compared to standard discriminative models. While they do not yet deliver on the promise of adversarial and out-of-domain robustness, they provide a different approach to classification that warrants further research.",
        "authors": [
            "Roland S. Zimmermann",
            "Lukas Schott",
            "Yang Song",
            "Benjamin A. Dunn",
            "David A. Klindt"
        ],
        "citations": 57,
        "references": 54,
        "year": 2021
    },
    {
        "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression",
        "abstract": "Symbolic regression is the task of identifying a mathematical expression that best fits a provided dataset of input and output values. Due to the richness of the space of mathematical expressions, symbolic regression is generally a challenging problem. While conventional approaches based on genetic evolution algorithms have been used for decades, deep learning-based methods are relatively new and an active research area. In this work, we present SymbolicGPT, a novel transformer-based language model for symbolic regression. This model exploits the advantages of probabilistic language models like GPT, including strength in performance and flexibility. Through comprehensive experiments, we show that our model performs strongly compared to competing models with respect to the accuracy, running time, and data efficiency.",
        "authors": [
            "Mojtaba Valipour",
            "Bowen You",
            "Maysum Panju",
            "A. Ghodsi"
        ],
        "citations": 76,
        "references": 28,
        "year": 2021
    },
    {
        "title": "Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling",
        "abstract": "Integrating physics models within machine learning models holds considerable promise toward learning robust models with improved interpretability and abilities to extrapolate. In this work, we focus on the integration of incomplete physics models into deep generative models. In particular, we introduce an architecture of variational autoencoders (VAEs) in which a part of the latent space is grounded by physics. A key technical challenge is to strike a balance between the incomplete physics and trainable components such as neural networks for ensuring that the physics part is used in a meaningful manner. To this end, we propose a regularized learning method that controls the effect of the trainable components and preserves the semantics of the physics-based latent variables as intended. We not only demonstrate generative performance improvements over a set of synthetic and real-world datasets, but we also show that we learn robust models that can consistently extrapolate beyond the training distribution in a meaningful manner. Moreover, we show that we can control the generative process in an interpretable manner.",
        "authors": [
            "Naoya Takeishi",
            "Alexandros Kalousis"
        ],
        "citations": 49,
        "references": 118,
        "year": 2021
    },
    {
        "title": "Transformer models for text-based emotion detection: a review of BERT-based approaches",
        "abstract": null,
        "authors": [
            "F. A. Acheampong",
            "Henry Nunoo-Mensah",
            "Wenyu Chen"
        ],
        "citations": 360,
        "references": 113,
        "year": 2021
    },
    {
        "title": "Moser Flow: Divergence-based Generative Modeling on Manifolds",
        "abstract": "We are interested in learning generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. Current extensions of existing (Euclidean) generative models are restricted to specific geometries and typically suffer from high computational costs. We introduce Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNF). MF also produces a CNF via a solution to the change-of-variable formula, however differently from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Furthermore, representing the model density explicitly as the divergence of a NN rather than as a solution of an ODE facilitates learning high fidelity densities. Theoretically, we prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, we demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences.",
        "authors": [
            "N. Rozen",
            "Aditya Grover",
            "Maximilian Nickel",
            "Y. Lipman"
        ],
        "citations": 47,
        "references": 37,
        "year": 2021
    },
    {
        "title": "World Models",
        "abstract": "We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/",
        "authors": [
            "David R Ha",
            "J. Schmidhuber"
        ],
        "citations": 925,
        "references": 175,
        "year": 2018
    },
    {
        "title": "SynSigGAN: Generative Adversarial Networks for Synthetic Biomedical Signal Generation",
        "abstract": "Simple Summary This paper proposes a novel generative adversarial networks model, SynSigGAN, to generate any kind of synthetic biomedical signals. The generation of synthetic signals eliminates confidentiality concerns and accessibility problem of medical data. Synthetic data can be utilized for training medical students and machine learning models for the advancement and automation of healthcare systems. Our proposed model performs significantly better than existing models with a high correlation coefficient that measures the generated synthetic signals’ similarity with the original signals. Abstract Automating medical diagnosis and training medical students with real-life situations requires the accumulation of large dataset variants covering all aspects of a patient’s condition. For preventing the misuse of patient’s private information, datasets are not always publicly available. There is a need to generate synthetic data that can be trained for the advancement of public healthcare without intruding on patient’s confidentiality. Currently, rules for generating synthetic data are predefined and they require expert intervention, which limits the types and amount of synthetic data. In this paper, we propose a novel generative adversarial networks (GAN) model, named SynSigGAN, for automating the generation of any kind of synthetic biomedical signals. We have used bidirectional grid long short-term memory for the generator network and convolutional neural network for the discriminator network of the GAN model. Our model can be applied in order to create new biomedical synthetic signals while using a small size of the original signal dataset. We have experimented with our model for generating synthetic signals for four kinds of biomedical signals (electrocardiogram (ECG), electroencephalogram (EEG), electromyography (EMG), photoplethysmography (PPG)). The performance of our model is superior wheen compared to other traditional models and GAN models, as depicted by the evaluation metric. Synthetic biomedical signals generated by our approach have been tested while using other models that could classify each signal significantly with high accuracy.",
        "authors": [
            "Debapriya Hazra",
            "Y. Byun"
        ],
        "citations": 113,
        "references": 39,
        "year": 2020
    },
    {
        "title": "Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval",
        "abstract": "In this paper, we propose a novel deep generative approach to cross-modal retrieval to learn hash functions in the absence of paired training samples through the cycle consistency loss. Our proposed approach employs adversarial training scheme to learn a couple of hash functions enabling translation between modalities while assuming the underlying semantic relationship. To induce the hash codes with semantics to the input-output pair, cycle consistency loss is further delved into the adversarial training to strengthen the correlation between the inputs and corresponding outputs. Our approach is generative to learn hash functions, such that the learned hash codes can maximally correlate each input–output correspondence and also regenerate the inputs so as to minimize the information loss. The learning to hash embedding is thus performed to jointly optimize the parameters of the hash functions across modalities as well as the associated generative models. Extensive experiments on a variety of large-scale cross-modal data sets demonstrate that our proposed method outperforms the state of the arts.",
        "authors": [
            "Lin Wu",
            "Yang Wang",
            "Ling Shao"
        ],
        "citations": 224,
        "references": 67,
        "year": 2018
    },
    {
        "title": "WAIC, but Why? Generative Ensembles for Robust Anomaly Detection",
        "abstract": "Machine learning models encounter Out-of-Distribution (OoD) errors when the data seen at test time are generated from a different stochastic generator than the one used to generate the training data. One proposal to scale OoD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OoD errors, and even assign large likelihoods to samples from other datasets. To mitigate this problem, we propose Generative Ensembles, which robustify density-based OoD detection by way of estimating epistemic uncertainty of the likelihood model. We present a puzzling observation in need of an explanation -- although likelihood measures cannot account for the typical set of a distribution, and therefore should not be suitable on their own for OoD detection, WAIC performs surprisingly well in practice.",
        "authors": [
            "Hyun-Jae Choi",
            "Eric Jang",
            "Alexander A. Alemi"
        ],
        "citations": 255,
        "references": 40,
        "year": 2018
    },
    {
        "title": "Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay",
        "abstract": "This paper proposes two novel knowledge transfer techniques for class-incremental learning (CIL). First, we propose data-free generative replay (DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples from a generative model. In the conventional generative replay, the generative model is pre-trained for old data and shared in extra memory for later incremental learning. In our proposed DF-GR, we train a generative model from scratch without using any training data, based on the pre-trained classification model from the past, so we curtail the cost of sharing pre-trained generative models. Second, we introduce dual-teacher information distillation (DT-ID) for knowledge distillation from two teachers to one student. In CIL, we use DT-ID to learn new classes incrementally based on the pre-trained model for old classes and another model (pre-)trained on the new data for new classes. We implemented the proposed schemes on top of one of the state-of-the-art CIL methods and showed the performance improvement on CIFAR-100 and ImageNet datasets.",
        "authors": [
            "Yoojin Choi",
            "Mostafa El-Khamy",
            "Jungwon Lee"
        ],
        "citations": 40,
        "references": 33,
        "year": 2021
    },
    {
        "title": "MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets",
        "abstract": "We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Compared to the original IWAE, our algorithm does not induce any additional computational overhead due to the missing data. We also develop Monte Carlo techniques for single and multiple imputation using a DLVM trained on an incomplete data set. We illustrate our approach by training a convolutional DLVM on incomplete static binarisations of MNIST. Moreover, on various continuous data sets, we show that MIWAE provides extremely accurate single imputations, and is highly competitive with state-of-the-art methods.",
        "authors": [
            "Pierre-Alexandre Mattei",
            "J. Frellsen"
        ],
        "citations": 209,
        "references": 58,
        "year": 2019
    },
    {
        "title": "A Data-Driven Graph Generative Model for Temporal Interaction Networks",
        "abstract": "Deep graph generative models have recently received a surge of attention due to its superiority of modeling realistic graphs in a variety of domains, including biology, chemistry, and social science. Despite the initial success, most, if not all, of the existing works are designed for static networks. Nonetheless, many realistic networks are intrinsically dynamic and presented as a collection of system logs (i.e., timestamped interactions/edges between entities), which pose a new research direction for us: how can we synthesize realistic dynamic networks by directly learning from the system logs? In addition, how can we ensure the generated graphs preserve both the structural and temporal characteristics of the real data? To address these challenges, we propose an end-to-end deep generative framework named TagGen. In particular, we start with a novel sampling strategy for jointly extracting structural and temporal context information from temporal networks. On top of that, TagGen parameterizes a bi-level self-attention mechanism together with a family of local operations to generate temporal random walks. At last, a discriminator gradually selects generated temporal random walks, that are plausible in the input data, and feeds them to an assembling module for generating temporal networks. The experimental results in seven real-world data sets across a variety of metrics demonstrate that (1) TagGen outperforms all baselines in the temporal interaction network generation problem, and (2) TagGen significantly boosts the performance of the prediction models in the tasks of anomaly detection and link prediction.",
        "authors": [
            "Dawei Zhou",
            "Lecheng Zheng",
            "Jiawei Han",
            "Jingrui He"
        ],
        "citations": 92,
        "references": 46,
        "year": 2020
    },
    {
        "title": "Optimizing the Latent Space of Generative Networks",
        "abstract": "Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.",
        "authors": [
            "Piotr Bojanowski",
            "Armand Joulin",
            "David Lopez-Paz",
            "Arthur Szlam"
        ],
        "citations": 402,
        "references": 62,
        "year": 2017
    },
    {
        "title": "FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow",
        "abstract": "Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.",
        "authors": [
            "Xuezhe Ma",
            "Chunting Zhou",
            "Xian Li",
            "Graham Neubig",
            "E. Hovy"
        ],
        "citations": 186,
        "references": 36,
        "year": 2019
    },
    {
        "title": "NeVAE: A Deep Generative Model for Molecular Graphs",
        "abstract": "Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with molecular graphs due to their unique characteristics—their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we propose NeVAE, a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. In addition, by using masking, the decoder is able to guarantee a set of valid properties in the generated molecules. Experiments reveal that our model can discover plausible, diverse and novel molecules more effectively than several state of the art methods. Moreover, by utilizing Bayesian optimization over the continuous latent representation of molecules our model finds, we can also find molecules that maximize certain desirable properties more effectively than alternatives.",
        "authors": [
            "Bidisha Samanta",
            "A. De",
            "G. Jana",
            "P. Chattaraj",
            "Niloy Ganguly",
            "Manuel Gomez Rodriguez"
        ],
        "citations": 204,
        "references": 66,
        "year": 2018
    },
    {
        "title": "Scalable Deep Generative Modeling for Sparse Graphs",
        "abstract": "Learning graph generative models is a challenging task for deep learning and has wide applicability to a range of domains like chemistry, biology and social science. However current deep neural methods suffer from limited scalability: for a graph with $n$ nodes and $m$ edges, existing deep neural methods require $\\Omega(n^2)$ complexity by building up the adjacency matrix. On the other hand, many real world graphs are actually sparse in the sense that $m\\ll n^2$. Based on this, we develop a novel autoregressive model, named BiGG, that utilizes this sparsity to avoid generating the full adjacency matrix, and importantly reduces the graph generation time complexity to $O((n + m)\\log n)$. Furthermore, during training this autoregressive model can be parallelized with $O(\\log n)$ synchronization stages, which makes it much more efficient than other autoregressive models that require $\\Omega(n)$. Experiments on several benchmarks show that the proposed approach not only scales to orders of magnitude larger graphs than previously possible with deep autoregressive graph generative models, but also yields better graph generation quality.",
        "authors": [
            "H. Dai",
            "Azade Nazi",
            "Yujia Li",
            "Bo Dai",
            "D. Schuurmans"
        ],
        "citations": 72,
        "references": 45,
        "year": 2020
    },
    {
        "title": "An empirical study on evaluation metrics of generative adversarial networks",
        "abstract": "Evaluating generative adversarial networks (GANs) is inherently challenging. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the problem of how to evaluate the evaluation metrics. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. With a series of carefully designed experiments, we comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far they are from learning the target distribution.",
        "authors": [
            "Qiantong Xu",
            "Gao Huang",
            "Yang Yuan",
            "Chuan Guo",
            "Yu Sun",
            "Felix Wu",
            "Kilian Q. Weinberger"
        ],
        "citations": 252,
        "references": 33,
        "year": 2018
    },
    {
        "title": "Release Strategies and the Social Impacts of Language Models",
        "abstract": "Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.",
        "authors": [
            "Irene Solaiman",
            "Miles Brundage",
            "Jack Clark",
            "Amanda Askell",
            "Ariel Herbert-Voss",
            "Jeff Wu",
            "Alec Radford",
            "Jasmine Wang"
        ],
        "citations": 528,
        "references": 99,
        "year": 2019
    },
    {
        "title": "Imitating driver behavior with generative adversarial networks",
        "abstract": "The ability to accurately predict and simulate human driving behavior is critical for the development of intelligent transportation systems. Traditional modeling methods have employed simple parametric models and behavioral cloning. This paper adopts a method for overcoming the problem of cascading errors inherent in prior approaches, resulting in realistic behavior that is robust to trajectory perturbations. We extend Generative Adversarial Imitation Learning to the training of recurrent policies, and we demonstrate that our model rivals rule-based controllers and maximum likelihood models in realistic highway simulations. Our model both reproduces emergent behavior of human drivers, such as lane change rate, while maintaining realistic control over long time horizons.",
        "authors": [
            "Alex Kuefler",
            "Jeremy Morton",
            "T. Wheeler",
            "Mykel J. Kochenderfer"
        ],
        "citations": 388,
        "references": 42,
        "year": 2017
    },
    {
        "title": "VideoFlow: A Flow-Based Generative Model for Video",
        "abstract": "Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. In particular, learning predictive models of videos offers an especially appealing mechanism to enable a rich understanding of the physical world: videos of real-world interactions are plentiful and readily available, and a model that can predict future video frames can not only capture useful representations of the world, but can be useful in its own right, for problems such as model-based robotic control. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally (as in the case of pixel-level autoregressive models), or do not directly optimize the likelihood of the data. In this work, we propose a model for video prediction based on normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows. We describe an approach for modeling the latent space dynamics, and demonstrate that flow-based generative models offer a viable and competitive approach to generative modeling of video.",
        "authors": [
            "Manoj Kumar",
            "M. Babaeizadeh",
            "D. Erhan",
            "Chelsea Finn",
            "S. Levine",
            "Laurent Dinh",
            "Durk Kingma"
        ],
        "citations": 126,
        "references": 56,
        "year": 2019
    },
    {
        "title": "Constrained crystals deep convolutional generative adversarial network for the inverse design of crystal structures",
        "abstract": null,
        "authors": [
            "Teng Long",
            "N. Fortunato",
            "I. Opahle",
            "Yixuan Zhang",
            "I. Samathrakis",
            "Chen Shen",
            "O. Gutfleisch",
            "Hongbin Zhang"
        ],
        "citations": 88,
        "references": 46,
        "year": 2020
    },
    {
        "title": "Stochastic Seismic Waveform Inversion Using Generative Adversarial Networks as a Geological Prior",
        "abstract": null,
        "authors": [
            "L. Mosser",
            "O. Dubrule",
            "M. Blunt"
        ],
        "citations": 197,
        "references": 85,
        "year": 2018
    },
    {
        "title": "Model-Free Renewable Scenario Generation Using Generative Adversarial Networks",
        "abstract": "Scenario generation is an important step in the operation and planning of power systems with high renewable penetrations. In this work, we proposed a data-driven approach for scenario generation using generative adversarial networks, which is based on two interconnected deep neural networks. Compared with existing methods based on probabilistic models that are often hard to scale or sample from, our method is data- driven, and captures renewable energy production patterns in both temporal and spatial dimensions for a large number of correlated resources. For validation, we use wind and solar times- series data from NREL integration data sets. We demonstrate that the proposed method is able to generate realistic wind and photovoltaic power profiles with full diversity of behaviors. We also illustrate how to generate scenarios based on different conditions of interest by using labeled data during training. For example, scenarios can be conditioned on weather events (e.g. high wind day, intense ramp events or large forecasts errors) or time of the year (e, g. solar generation for a day in July). Because of the feedforward nature of the neural networks, scenarios can be generated extremely efficiently without sophisticated sampling techniques.",
        "authors": [
            "Yize Chen",
            "Yishen Wang",
            "D. Kirschen",
            "Baosen Zhang"
        ],
        "citations": 394,
        "references": 36,
        "year": 2017
    },
    {
        "title": "MD-GAN: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets",
        "abstract": "A recent technical breakthrough in the domain of machine learning is the discovery and the multiple applications of Generative Adversarial Networks (GANs). Those generative models are computationally demanding, as a GAN is composed of two deep neural networks, and because it trains on large datasets. A GAN is generally trained on a single server. In this paper, we address the problem of distributing GANs so that they are able to train over datasets that are spread on multiple workers. MD-GAN is exposed as the first solution for this problem: we propose a novel learning procedure for GANs so that they fit this distributed setup. We then compare the performance of MD-GAN to an adapted version of federated learning to GANs, using the MNIST, CIFAR10 and CelebA datasets. MD-GAN exhibits a reduction by a factor of two of the learning complexity on each worker node, while providing better or identical performances with the adaptation of federated learning. We finally discuss the practical implications of distributing GANs.",
        "authors": [
            "Corentin Hardy",
            "E. L. Merrer",
            "B. Sericola"
        ],
        "citations": 171,
        "references": 46,
        "year": 2018
    },
    {
        "title": "Parameterized quantum circuits as machine learning models",
        "abstract": "Hybrid quantum–classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.",
        "authors": [
            "Marcello Benedetti",
            "Erika Lloyd",
            "Stefan H. Sack",
            "Mattia Fiorentini"
        ],
        "citations": 780,
        "references": 134,
        "year": 2019
    },
    {
        "title": "Generative Recurrent Networks for De Novo Drug Design",
        "abstract": "Generative artificial intelligence models present a fresh approach to chemogenomics and de novo drug design, as they provide researchers with the ability to narrow down their search of the chemical space and focus on regions of interest. We present a method for molecular de novo design that utilizes generative recurrent neural networks (RNN) containing long short‐term memory (LSTM) cells. This computational model captured the syntax of molecular representation in terms of SMILES strings with close to perfect accuracy. The learned pattern probabilities can be used for de novo SMILES generation. This molecular design concept eliminates the need for virtual compound library enumeration. By employing transfer learning, we fine‐tuned the RNN′s predictions for specific molecular targets. This approach enables virtual compound design without requiring secondary or external activity prediction, which could introduce error or unwanted bias. The results obtained advocate this generative RNN‐LSTM system for high‐impact use cases, such as low‐data drug discovery, fragment based molecular design, and hit‐to‐lead optimization for diverse drug targets.",
        "authors": [
            "Anvita Gupta",
            "A. T. Müller",
            "Berend J. H. Huisman",
            "Jens A Fuchs",
            "P. Schneider",
            "G. Schneider"
        ],
        "citations": 376,
        "references": 32,
        "year": 2017
    },
    {
        "title": "Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities",
        "abstract": null,
        "authors": [
            "Guo-Jun Qi"
        ],
        "citations": 341,
        "references": 53,
        "year": 2017
    },
    {
        "title": "Commonsense for Generative Multi-Hop Question Answering Tasks",
        "abstract": "Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model’s performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show that our background knowledge enhancements are generalizable and improve performance on QAngaroo-WikiHop, another multi-hop reasoning dataset.",
        "authors": [
            "Lisa Bauer",
            "Yicheng Wang",
            "Mohit Bansal"
        ],
        "citations": 177,
        "references": 45,
        "year": 2018
    },
    {
        "title": "Semantic Object Accuracy for Generative Text-to-Image Synthesis",
        "abstract": "Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g., whether an image generated from “a car driving down the street” contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.",
        "authors": [
            "T. Hinz",
            "Stefan Heinrich",
            "Stefan Wermter"
        ],
        "citations": 151,
        "references": 67,
        "year": 2019
    },
    {
        "title": "Generative Code Modeling with Graphs",
        "abstract": "Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. The generative procedure interleaves grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.",
        "authors": [
            "Marc Brockschmidt",
            "Miltiadis Allamanis",
            "Alexander L. Gaunt",
            "Oleksandr Polozov"
        ],
        "citations": 174,
        "references": 29,
        "year": 2018
    },
    {
        "title": "AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks",
        "abstract": "The compression of Generative Adversarial Networks (GANs) has lately drawn attention, due to the increasing demand for deploying GANs into mobile devices for numerous applications such as image translation, enhancement and editing. However, compared to the substantial efforts to compressing other deep models, the research on compressing GANs (usually the generators) remains at its infancy stage. Existing GAN compression algorithms are limited to handling specific GAN architectures and losses. Inspired by the recent success of AutoML in deep compression, we introduce AutoML to GAN compression and develop an AutoGAN-Distiller (AGD) framework. Starting with a specifically designed efficient search space, AGD performs an end-to-end discovery for new efficient generators, given the target computational resource constraints. The search is guided by the original GAN model via knowledge distillation, therefore fulfilling the compression. AGD is fully automatic, standalone (i.e., needing no trained discriminators), and generically applicable to various GAN models. We evaluate AGD in two representative GAN tasks: image translation and super resolution. Without bells and whistles, AGD yields remarkably lightweight yet more competitive compressed models, that largely outperform existing alternatives. Our codes and pretrained models are available at this https URL.",
        "authors": [
            "Y. Fu",
            "Wuyang Chen",
            "Haotao Wang",
            "Haoran Li",
            "Yingyan Lin",
            "Zhangyang Wang"
        ],
        "citations": 87,
        "references": 70,
        "year": 2020
    },
    {
        "title": "PixelSNAIL: An Improved Autoregressive Generative Model",
        "abstract": "Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \\times 32$ ImageNet (3.80 bits per dim). Our implementation is available at this https URL",
        "authors": [
            "Xi Chen",
            "Nikhil Mishra",
            "Mostafa Rohaninejad",
            "P. Abbeel"
        ],
        "citations": 254,
        "references": 38,
        "year": 2017
    },
    {
        "title": "A Deep Generative Framework for Paraphrase Generation",
        "abstract": "\n \n Paraphrase generation is an important problem in NLP, especially in question answering, information retrieval, information extraction, conversation systems, to name a few. In this paper, we address the problem of generating paraphrases automatically. Our proposed method is based on a combination of deep generative models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases, given an input sentence. Traditional VAEs when combined with recurrent neural networks can generate free text but they are not suitable for paraphrase generation for a given sentence. We address this problem by conditioning the both, encoder and decoder sides of VAE, on the original sentence, so that it can generate the given sentence's paraphrases. Unlike most existing models, our model is simple, modular and can generate multiple paraphrases, for a given sentence. Quantitative evaluation of the proposed method on a benchmark paraphrase dataset demonstrates its efficacy, and its performance improvement over the state-of-the-art methods by a significant margin, whereas qualitative human evaluation indicate that the generated paraphrases are well-formed, grammatically correct, and are relevant to the input sentence. Furthermore, we evaluate our method on a newly released question paraphrase dataset, and establish a new baseline for future research.\n \n",
        "authors": [
            "Ankush Gupta",
            "Arvind Agarwal",
            "Prawaan Singh",
            "Piyush Rai"
        ],
        "citations": 252,
        "references": 37,
        "year": 2017
    },
    {
        "title": "Unsupervised Generative Modeling Using Matrix Product States",
        "abstract": "Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard datasets including the Bars and Stripes, random binary patterns and the MNIST handwritten digits to illustrate the abilities, features and drawbacks of our model over popular generative models such as Hopfield model, Boltzmann machines and generative adversarial networks. Our work sheds light on many interesting directions of future exploration on the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to be realized on quantum devices.",
        "authors": [
            "Zhaoyu Han",
            "Jun Wang",
            "H. Fan",
            "Lei Wang",
            "Pan Zhang"
        ],
        "citations": 247,
        "references": 74,
        "year": 2017
    },
    {
        "title": "Machine Learning for Stochastic Parameterization: Generative Adversarial Networks in the Lorenz '96 Model",
        "abstract": "Stochastic parameterizations account for uncertainty in the representation of unresolved subgrid processes by sampling from the distribution of possible subgrid forcings. Some existing stochastic parameterizations utilize data‐driven approaches to characterize uncertainty, but these approaches require significant structural assumptions that can limit their scalability. Machine learning models, including neural networks, are able to represent a wide range of distributions and build optimized mappings between a large number of inputs and subgrid forcings. Recent research on machine learning parameterizations has focused only on deterministic parameterizations. In this study, we develop a stochastic parameterization using the generative adversarial network (GAN) machine learning framework. The GAN stochastic parameterization is trained and evaluated on output from the Lorenz '96 model, which is a common baseline model for evaluating both parameterization and data assimilation techniques. We evaluate different ways of characterizing the input noise for the model and perform model runs with the GAN parameterization at weather and climate time scales. Some of the GAN configurations perform better than a baseline bespoke parameterization at both time scales, and the networks closely reproduce the spatiotemporal correlations and regimes of the Lorenz '96 system. We also find that, in general, those models which produce skillful forecasts are also associated with the best climate simulations.",
        "authors": [
            "D. Gagne",
            "H. Christensen",
            "A. Subramanian",
            "A. Monahan"
        ],
        "citations": 133,
        "references": 102,
        "year": 2019
    },
    {
        "title": "On Training Sample Memorization: Lessons from Benchmarking Generative Modeling with a Large-scale Competition",
        "abstract": "Many recent developments on generative models for natural images have relied on heuristically-motivated metrics that can be easily gamed by memorizing a small sample from the true distribution or training a model directly to improve the metric. In this work, we critically evaluate the gameability of these metrics by designing and deploying a generative modeling competition. Our competition received over 11000 submitted models. The competitiveness between participants allowed us to investigate both intentional and unintentional memorization in generative modeling. To detect intentional memorization, we propose the \"Memorization-Informed Frechet Inception Distance\" (MiFID) as a new memorization-aware metric and design benchmark procedures to ensure that winning submissions made genuine improvements in perceptual quality. Furthermore, we manually inspect the code for the 1000 top-performing models to understand and label different forms of memorization. Our analysis reveals that unintentional memorization is a serious and common issue in popular generative models. The generated images and our memorization labels of those models as well as code to compute MiFID are released to facilitate future studies on benchmarking generative models.",
        "authors": [
            "C. Bai",
            "Hsuan-Tien Lin",
            "Colin Raffel",
            "Wendy Kan"
        ],
        "citations": 34,
        "references": 49,
        "year": 2021
    },
    {
        "title": "A Review: Generative Adversarial Networks",
        "abstract": "Deep learning has achieved great success in the field of artificial intelligence, and many deep learning models have been developed. Generative Adversarial Networks (GAN) is one of the deep learning model, which was proposed based on zero-sum game theory and has become a new research hotspot. The significance of the model variation is to obtain the data distribution through unsupervised learning and to generate more realistic/actual data. Currently, GANs have been widely studied due to the enormous application prospect, including image and vision computing, video and language processing, etc. In this paper, the background of the GAN, theoretic models and extensional variants of GANs are introduced, where the variants can further optimize the original GAN or change the basic structures. Then the typical applications of GANs are explained. Finally the existing problems of GANs are summarized and the future work of GANs models are given.",
        "authors": [
            "Liang Gonog",
            "Yimin Zhou"
        ],
        "citations": 144,
        "references": 36,
        "year": 2019
    },
    {
        "title": "Recent Advances of Generative Adversarial Networks in Computer Vision",
        "abstract": "The appearance of generative adversarial networks (GAN) provides a new approach and framework for computer vision. Compared with traditional machine learning algorithms, GAN works via adversarial training concept and is more powerful in both feature learning and representation. GAN also exhibits some problems, such as non-convergence, model collapse, and uncontrollability due to high degree of freedom. How to improve the theory of GAN and apply it to computer-vision-related tasks have now attracted much research efforts. In this paper, recently proposed GAN models and their applications in computer vision are systematically reviewed. In particular, we firstly survey the history and development of generative algorithms, the mechanism of GAN, its fundamental network structures, and theoretical analysis of the original GAN. Classical GAN algorithms are then compared comprehensively in terms of the mechanism, visual results of generated samples, and Frechet Inception Distance. These networks are further evaluated from network construction, performance, and applicability aspects by extensive experiments conducted over public datasets. After that, several typical applications of GAN in computer vision, including high-quality samples generation, style transfer, and image translation, are examined. Finally, some existing problems of GAN are summarized and discussed and potential future research topics are forecasted.",
        "authors": [
            "Yangjie Cao",
            "Li-Li Jia",
            "Yongxia Chen",
            "Nan Lin",
            "Cong Yang",
            "Bo Zhang",
            "Zhi Liu",
            "Xue-Xiang Li",
            "Honghua Dai"
        ],
        "citations": 131,
        "references": 93,
        "year": 2019
    },
    {
        "title": "Generative Adversarial Networks and Conditional Random Fields for Hyperspectral Image Classification",
        "abstract": "In this paper, we address the hyperspectral image (HSI) classification task with a generative adversarial network and conditional random field (GAN-CRF)-based framework, which integrates a semisupervised deep learning and a probabilistic graphical model, and make three contributions. First, we design four types of convolutional and transposed convolutional layers that consider the characteristics of HSIs to help with extracting discriminative features from limited numbers of labeled HSI samples. Second, we construct semisupervised generative adversarial networks (GANs) to alleviate the shortage of training samples by adding labels to them and implicitly reconstructing real HSI data distribution through adversarial training. Third, we build dense conditional random fields (CRFs) on top of the random variables that are initialized to the softmax predictions of the trained GANs and are conditioned on HSIs to refine classification maps. This semisupervised framework leverages the merits of discriminative and generative models through a game-theoretical approach. Moreover, even though we used very small numbers of labeled training HSI samples from the two most challenging and extensively studied datasets, the experimental results demonstrated that spectral–spatial GAN-CRF (SS-GAN-CRF) models achieved top-ranking accuracy for semisupervised HSI classification.",
        "authors": [
            "Zilong Zhong",
            "Jonathan Li",
            "David A Clausi",
            "A. Wong"
        ],
        "citations": 103,
        "references": 40,
        "year": 2019
    },
    {
        "title": "Wasserstein-2 Generative Networks",
        "abstract": "Generative Adversarial Networks training is not easy due to the minimax nature of the optimization objective. In this paper, we propose a novel end-to-end algorithm for training generative models which uses a non-minimax objective simplifying model training. The proposed algorithm uses the approximation of Wasserstein-2 distance by Input Convex Neural Networks. From the theoretical side, we estimate the properties of the generative mapping fitted by the algorithm. From the practical side, we conduct computational experiments which confirm the efficiency of our algorithm in various applied problems: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.",
        "authors": [
            "Alexander Korotin",
            "Vage Egiazarian",
            "Arip Asadulaev",
            "E. Burnaev"
        ],
        "citations": 95,
        "references": 51,
        "year": 2019
    },
    {
        "title": "GIF: Generative Interpretable Faces",
        "abstract": "Photo-realistic visualization and animation of expressive human faces have been a long standing challenge. 3D face modeling methods provide parametric control but generates unrealistic images, on the other hand, generative 2D models like GANs (Generative Adversarial Networks) output photo-realistic face images, but lack explicit control. Recent methods gain partial control, either by attempting to disentangle different factors in an unsupervised manner, or by adding control post hoc to a pre-trained model. Unconditional GANs, however, may entangle factors that are hard to undo later. We condition our generative model on pre-defined control parameters to encourage disentanglement in the generation process. Specifically, we condition StyleGAN2 on FLAME, a generative 3D face model. While conditioning on FLAME parameters yields unsatisfactory results, we find that conditioning on rendered FLAME geometry and photometric details works well. This gives us a generative 2D face model named GIF (Generative Interpretable Faces) that offers FLAME’s parametric control. Here, interpretable refers to the semantic meaning of different parameters. Given FLAME parameters for shape, pose, expressions, parameters for appearance, lighting, and an additional style vector, GIF outputs photo-realistic face images. We perform an AMT based perceptual study to quantitatively and qualitatively evaluate how well GIF follows its conditioning. The code, data, and trained model are publicly available for research purposes at http://gif.is.tue.mpg.de.",
        "authors": [
            "Partha Ghosh",
            "Pravir Singh Gupta",
            "Roy Uziel",
            "Anurag Ranjan",
            "Michael J. Black",
            "Timo Bolkart"
        ],
        "citations": 74,
        "references": 76,
        "year": 2020
    },
    {
        "title": "Gotta Go Fast When Generating Data with Score-Based Models",
        "abstract": "Score-based (denoising diffusion) generative models have recently gained a lot of success in generating realistic and diverse data. These approaches define a forward diffusion process for transforming data to noise and generate data by reversing it (thereby going from noise to data). Unfortunately, current score-based models generate data very slowly due to the sheer number of score network evaluations required by numerical SDE solvers. In this work, we aim to accelerate this process by devising a more efficient SDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which uses a fixed step size. We found that naively replacing it with other SDE solvers fares poorly - they either result in low-quality samples or become slower than EM. To get around this issue, we carefully devise an SDE solver with adaptive step sizes tailored to score-based generative models piece by piece. Our solver requires only two score function evaluations, rarely rejects samples, and leads to high-quality samples. Our approach generates data 2 to 10 times faster than EM while achieving better or equal sample quality. For high-resolution images, our method leads to significantly higher quality samples than all other methods tested. Our SDE solver has the benefit of requiring no step size tuning.",
        "authors": [
            "Alexia Jolicoeur-Martineau",
            "Ke Li",
            "Remi Piche-Taillefer",
            "Tal Kachman",
            "Ioannis Mitliagkas"
        ],
        "citations": 194,
        "references": 41,
        "year": 2021
    },
    {
        "title": "Ig-VAE: Generative modeling of protein structure by direct 3D coordinate generation",
        "abstract": "While deep learning models have seen increasing applications in protein science, few have been implemented for protein backbone generation—an important task in structure-based problems such as active site and interface design. We present a new approach to building class-specific backbones, using a variational auto-encoder to directly generate the 3D coordinates of immunoglobulins. Our model is torsion- and distance-aware, learns a high-resolution embedding of the dataset, and generates novel, high-quality structures compatible with existing design tools. We show that the Ig-VAE can be used to create a computational model of a SARS-CoV2-RBD binder via latent space sampling. We further demonstrate that the model’s generative prior is a powerful tool for guiding computational protein design, motivating a new paradigm under which backbone design is solved as constrained optimization problem in the latent space of a generative model.",
        "authors": [
            "Raphael R. Eguchi",
            "Christian A. Choe",
            "Po-Ssu Huang"
        ],
        "citations": 97,
        "references": 66,
        "year": 2020
    },
    {
        "title": "Distribution Augmentation for Generative Modeling",
        "abstract": "We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmentation functions to data and, importantly, conditions the generative model on the speciﬁc function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures and problem domains.",
        "authors": [
            "Heewoo Jun",
            "R. Child",
            "Mark Chen",
            "John Schulman",
            "A. Ramesh",
            "Alec Radford",
            "I. Sutskever"
        ],
        "citations": 61,
        "references": 41,
        "year": 2020
    },
    {
        "title": "Video Generative Adversarial Networks: A Review",
        "abstract": "With the increasing interest in the content creation field in multiple sectors such as media, education, and entertainment, there is an increased trend in the papers that use AI algorithms to generate content such as images, videos, audio, and text. Generative Adversarial Networks (GANs) is one of the promising models that synthesizes data samples that are similar to real data samples. While the variations of GANs models in general have been covered to some extent in several survey papers, to the best of our knowledge, this is the first paper that reviews the state-of-the-art video GANs models. This paper first categorizes GANs review papers into general GANs review papers, image GANs review papers, and special field GANs review papers such as anomaly detection, medical imaging, or cybersecurity. The paper then summarizes the main improvements in GANs that are not necessarily applied in the video domain in the first run but have been adopted in multiple video GANs variations. Then, a comprehensive review of video GANs models are provided under two main divisions based on existence of a condition. The conditional models are then further classified according to the provided condition into audio, text, video, and image. The paper concludes with the main challenges and limitations of the current video GANs models.",
        "authors": [
            "Nuha Aldausari",
            "A. Sowmya",
            "Nadine Marcus",
            "Gelareh Mohammadi"
        ],
        "citations": 89,
        "references": 138,
        "year": 2020
    },
    {
        "title": "A review of generative adversarial networks and its application in cybersecurity",
        "abstract": null,
        "authors": [
            "C. Yinka-banjo",
            "O. Ugot"
        ],
        "citations": 106,
        "references": 53,
        "year": 2019
    },
    {
        "title": "Variational Quantum Generators: Generative Adversarial Quantum Machine Learning for Continuous Distributions",
        "abstract": "A hybrid quantum–classical approach to model continuous classical probability distributions using a variational quantum circuit is proposed. The architecture of this quantum generator consists of a quantum circuit that encodes a classical random variable into a quantum state and a parameterized quantum circuit trained to mimic the target distribution. The model allows for easy interfacing with a classical function, such as a neural network, and is trained using an adversarial learning approach. It is shown that the quantum generator is able to learn using either a classical neural network or a variational quantum circuit as the discriminator model. This implementation takes advantage of automatic differentiation tools to perform the optimization of the variational circuits employed. The framework presented here for the design and implementation of the variational quantum generators can serve as a blueprint for designing hybrid quantum–classical models for other machine learning tasks.",
        "authors": [
            "J. Romero",
            "Alán Aspuru-Guzik"
        ],
        "citations": 149,
        "references": 68,
        "year": 2019
    },
    {
        "title": "Generative Interventions for Causal Learning",
        "abstract": "We introduce a framework for learning robust visual representations that generalize to new viewpoints, backgrounds, and scene contexts. Discriminative models often learn naturally occurring spurious correlations, which cause them to fail on images outside of the training distribution. In this paper, we show that we can steer generative models to manufacture interventions on features caused by confounding factors. Experiments, visualizations, and theoretical results show this method learns robust representations more consistent with the underlying causal relationships. Our approach improves performance on multiple datasets demanding out-of-distribution generalization, and we demonstrate state-of-the-art performance generalizing from ImageNet to ObjectNet dataset.",
        "authors": [
            "Chengzhi Mao",
            "Amogh Gupta",
            "Augustine Cha",
            "Hongya Wang",
            "Junfeng Yang",
            "Carl Vondrick"
        ],
        "citations": 59,
        "references": 65,
        "year": 2020
    },
    {
        "title": "MelNet: A Generative Model for Audio in the Frequency Domain",
        "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.",
        "authors": [
            "Sean Vasquez",
            "M. Lewis"
        ],
        "citations": 129,
        "references": 54,
        "year": 2019
    },
    {
        "title": "Generative Dual Adversarial Network for Generalized Zero-Shot Learning",
        "abstract": "This paper studies the problem of generalized zero-shot learning which requires the model to train on image-label pairs from some seen classes and test on the task of classifying new images from both seen and unseen classes. In this paper, we propose a novel model that provides a unified framework for three different approaches: visual->semantic mapping, semantic->visual mapping, and metric learning. Specifically, our proposed model consists of a feature generator that can generate various visual features given class embeddings as input, a regressor that maps each visual feature back to its corresponding class embedding, and a discriminator that learns to evaluate the closeness of an image feature and a class embedding. All three components are trained under the combination of cyclic consistency loss and dual adversarial loss. Experimental results show that our model not only preserves higher accuracy in classifying images from seen classes, but also performs better than existing state-of-the-art models in in classifying images from unseen classes.",
        "authors": [
            "He Huang",
            "Chang-Dong Wang",
            "Philip S. Yu",
            "Chang-Dong Wang"
        ],
        "citations": 215,
        "references": 44,
        "year": 2018
    },
    {
        "title": "A Generative Model of People in Clothing",
        "abstract": "We present the first image-based generative model of people in clothing for the full body. We sidestep the commonly used complex graphics rendering pipeline and the need for high-quality 3D scans of dressed people. Instead, we learn generative models from a large image database. The main challenge is to cope with the high variance in human pose, shape and appearance. For this reason, pure image-based approaches have not been considered so far. We show that this challenge can be overcome by splitting the generating process in two parts. First, we learn to generate a semantic segmentation of the body and clothing. Second, we learn a conditional model on the resulting segments that creates realistic images. The full model is differentiable and can be conditioned on pose, shape or color. The result are samples of people in different clothing items and styles. The proposed model can generate entirely new people with realistic clothing. In several experiments we present encouraging results that suggest an entirely data-driven approach to people generation is possible.",
        "authors": [
            "Christoph Lassner",
            "Gerard Pons-Moll",
            "Peter Gehler"
        ],
        "citations": 228,
        "references": 57,
        "year": 2017
    },
    {
        "title": "Synthesizing electronic health records using improved generative adversarial networks",
        "abstract": "Objective\nThe aim of this study was to generate synthetic electronic health records (EHRs). The generated EHR data will be more realistic than those generated using the existing medical Generative Adversarial Network (medGAN) method.\n\n\nMaterials and Methods\nWe modified medGAN to obtain two synthetic data generation models-designated as medical Wasserstein GAN with gradient penalty (medWGAN) and medical boundary-seeking GAN (medBGAN)-and compared the results obtained using the three models. We used 2 databases: MIMIC-III and National Health Insurance Research Database (NHIRD), Taiwan. First, we trained the models and generated synthetic EHRs by using these three 3 models. We then analyzed and compared the models' performance by using a few statistical methods (Kolmogorov-Smirnov test, dimension-wise probability for binary data, and dimension-wise average count for count data) and 2 machine learning tasks (association rule mining and prediction).\n\n\nResults\nWe conducted a comprehensive analysis and found our models were adequately efficient for generating synthetic EHR data. The proposed models outperformed medGAN in all cases, and among the 3 models, boundary-seeking GAN (medBGAN) performed the best.\n\n\nDiscussion\nTo generate realistic synthetic EHR data, the proposed models will be effective in the medical industry and related research from the viewpoint of providing better services. Moreover, they will eliminate barriers including limited access to EHR data and thus accelerate research on medical informatics.\n\n\nConclusion\nThe proposed models can adequately learn the data distribution of real EHRs and efficiently generate realistic synthetic EHRs. The results show the superiority of our models over the existing model.",
        "authors": [
            "M. K. Baowaly",
            "Chia-Ching Lin",
            "Chao-Lin Liu",
            "Kuan-Ta Chen"
        ],
        "citations": 179,
        "references": 51,
        "year": 2018
    },
    {
        "title": "A Small-Sample Wind Turbine Fault Detection Method With Synthetic Fault Data Using Generative Adversarial Nets",
        "abstract": "The limited fault information caused by small fault data samples is a major problem in wind turbine (WT) fault detection. This paper proposes a small-sample WT fault detection method with the synthetic fault data using generative adversarial nets (GANs). First, based on prior knowledge, a rough fault data generation process is developed to transform the normal data to the rough fault data. Second, a rough fault data refiner is developed by GANs to make the rough fault data more similar with the real fault data. Moreover, to make the generated data better suited to the WT conditions, GANs are improved in both the generative model and the discriminative model. Third, artificial intelligence (AI)-based WT fault detection models can be well trained by using only the generated data in the condition of small fault data sample. Finally, three groups of generated data evaluation experiments and four groups of WT fault detection comparative experiments are conducted using real WT data collected from a wind farm in northern China. The results indicate that the method proposed in this paper is effective.",
        "authors": [
            "Jinhai Liu",
            "Fuming Qu",
            "Xiaowei Hong",
            "Hua-guang Zhang"
        ],
        "citations": 132,
        "references": 39,
        "year": 2019
    },
    {
        "title": "Symbolic Music Generation with Diffusion Models",
        "abstract": "Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in continuous domains such as images and audio. However, due to their Langevin-inspired sampling mechanisms, their application to discrete and sequential data has been limited. In this work, we present a technique for training diffusion models on sequential data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We apply this technique to modeling symbolic music and show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.",
        "authors": [
            "Gautam Mittal",
            "Jesse Engel",
            "Curtis Hawthorne",
            "Ian Simon"
        ],
        "citations": 172,
        "references": 60,
        "year": 2021
    },
    {
        "title": "An Introduction to Image Synthesis with Generative Adversarial Nets",
        "abstract": "There has been a drastic growth of research in Generative Adversarial Nets (GANs) in the past few years. Proposed in 2014, GAN has been applied to various applications such as computer vision and natural language processing, and achieves impressive performance. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some evaluation metrics as well as possible future research directions in image synthesis with GAN.",
        "authors": [
            "He Huang",
            "Philip S. Yu",
            "Changhu Wang"
        ],
        "citations": 180,
        "references": 105,
        "year": 2018
    },
    {
        "title": "VFlow: More Expressive Generative Flows with Variational Data Augmentation",
        "abstract": "Generative flows are promising tractable models for density modeling that define probabilistic distributions with invertible transformations. However, tractability imposes architectural constraints on generative flows, making them less expressive than other types of generative models. In this work, we study a previously overlooked constraint that all the intermediate representations must have the same dimensionality with the original data due to invertibility, limiting the width of the network. We tackle this constraint by augmenting the data with some extra dimensions and jointly learning a generative flow for augmented data as well as the distribution of augmented dimensions under a variational inference framework. Our approach, VFlow, is a generalization of generative flows and therefore always performs better. Combining with existing generative flows, VFlow achieves a new state-of-the-art 2.98 bits per dimension on the CIFAR-10 dataset and is more compact than previous models to reach similar modeling quality.",
        "authors": [
            "Jianfei Chen",
            "Cheng Lu",
            "Biqi Chenli",
            "Jun Zhu",
            "Tian Tian"
        ],
        "citations": 61,
        "references": 38,
        "year": 2020
    },
    {
        "title": "VAEM: a Deep Generative Model for Heterogeneous Mixed Type Data",
        "abstract": "Deep generative models often perform poorly in real-world applications due to the heterogeneity of natural data sets. Heterogeneity arises from data containing different types of features (categorical, ordinal, continuous, etc.) and features of the same type having different marginal distributions. We propose an extension of variational autoencoders (VAEs) called VAEM to handle such heterogeneous data. VAEM is a deep generative model that is trained in a two stage manner such that the first stage provides a more uniform representation of the data to the second stage, thereby sidestepping the problems caused by heterogeneous data. We provide extensions of VAEM to handle partially observed data, and demonstrate its performance in data generation, missing data prediction and sequential feature selection tasks. Our results show that VAEM broadens the range of real-world applications where deep generative models can be successfully deployed.",
        "authors": [
            "Chao Ma",
            "Sebastian Tschiatschek",
            "José Miguel Hernández-Lobato",
            "Richard E. Turner",
            "Cheng Zhang"
        ],
        "citations": 60,
        "references": 33,
        "year": 2020
    },
    {
        "title": "FloWaveNet : A Generative Flow for Raw Audio",
        "abstract": "Most modern text-to-speech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in its practical application due to its ancestral sampling scheme. The recently suggested Parallel WaveNet and ClariNet have achieved real-time audio synthesis capability by incorporating inverse autoregressive flow for parallel sampling. However, these approaches require a two-stage training pipeline with a well-trained teacher network and can only produce natural sound by using probability distillation along with auxiliary loss terms. We propose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single-stage training procedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently parallel due to the characteristics of generative flow. The model can efficiently sample raw audio in real-time, with clarity comparable to previous two-stage parallel models. The code and samples for all models, including our FloWaveNet, are publicly available.",
        "authors": [
            "Sungwon Kim",
            "Sang-gil Lee",
            "Jongyoon Song",
            "Jaehyeon Kim",
            "Sungroh Yoon"
        ],
        "citations": 166,
        "references": 17,
        "year": 2018
    },
    {
        "title": "Spatial interpolation using conditional generative adversarial neural networks",
        "abstract": "ABSTRACT Spatial interpolation is a traditional geostatistical operation that aims at predicting the attribute values of unobserved locations given a sample of data defined on point supports. However, the continuity and heterogeneity underlying spatial data are too complex to be approximated by classic statistical models. Deep learning models, especially the idea of conditional generative adversarial networks (CGANs), provide us with a perspective for formalizing spatial interpolation as a conditional generative task. In this article, we design a novel deep learning architecture named conditional encoder-decoder generative adversarial neural networks (CEDGANs) for spatial interpolation, therein combining the encoder-decoder structure with adversarial learning to capture deep representations of sampled spatial data and their interactions with local structural patterns. A case study on elevations in China demonstrates the ability of our model to achieve outstanding interpolation results compared to benchmark methods. Further experiments uncover the learned spatial knowledge in the model’s hidden layers and test the potential to generalize our adversarial interpolation idea across domains. This work is an endeavor to investigate deep spatial knowledge using artificial intelligence. The proposed model can benefit practical scenarios and enlighten future research in various geographical applications related to spatial prediction.",
        "authors": [
            "Di Zhu",
            "Ximeng Cheng",
            "Fan Zhang",
            "Xin Yao",
            "Yong Gao",
            "Yu Liu"
        ],
        "citations": 118,
        "references": 56,
        "year": 2019
    },
    {
        "title": "Tea With Milk? A Hierarchical Generative Framework of Sequential Event Comprehension",
        "abstract": "To make sense of the world around us, we must be able to segment a continual stream of sensory inputs into discrete events. In this review, I propose that in order to comprehend events, we engage hierarchical generative models that \"reverse engineer\" the intentions of other agents as they produce sequential action in real time. By generating probabilistic predictions for upcoming events, generative models ensure that we are able to keep up with the rapid pace at which perceptual inputs unfold. By tracking our certainty about other agents' goals and the magnitude of prediction errors at multiple temporal scales, generative models enable us to detect event boundaries by inferring when a goal has changed. Moreover, by adapting flexibly to the broader dynamics of the environment and our own comprehension goals, generative models allow us to optimally allocate limited resources. Finally, I argue that we use generative models not only to comprehend events but also to produce events (carry out goal-relevant sequential action) and to continually learn about new events from our surroundings. Taken together, this hierarchical generative framework provides new insights into how the human brain processes events so effortlessly while highlighting the fundamental links between event comprehension, production, and learning.",
        "authors": [
            "G. Kuperberg"
        ],
        "citations": 51,
        "references": 202,
        "year": 2020
    },
    {
        "title": "Generative and Discriminative Text Classification with Recurrent Neural Networks",
        "abstract": "We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors (e.g., they account for conditional dependencies across words in a document), they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts---the same pattern that Ng & Jordan (2001) proved holds for linear classification models that make more naive conditional independence assumptions. Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models.",
        "authors": [
            "Dani Yogatama",
            "Chris Dyer",
            "Wang Ling",
            "Phil Blunsom"
        ],
        "citations": 191,
        "references": 19,
        "year": 2017
    },
    {
        "title": "Diffusion Models for Implicit Image Segmentation Ensembles",
        "abstract": "Diffusion models have shown impressive performance for generative modelling of images. In this paper, we present a novel semantic segmentation method based on diffusion models. By modifying the training and sampling scheme, we show that diffusion models can perform lesion segmentation of medical images. To generate an image specific segmentation, we train the model on the ground truth segmentation, and use the image as a prior during training and in every step during the sampling process. With the given stochastic sampling process, we can generate a distribution of segmentation masks. This property allows us to compute pixel-wise uncertainty maps of the segmentation, and allows an implicit ensemble of segmentations that increases the segmentation performance. We evaluate our method on the BRATS2020 dataset for brain tumor segmentation. Compared to state-of-the-art segmentation models, our approach yields good segmentation results and, additionally, detailed uncertainty maps.",
        "authors": [
            "Julia Wolleb",
            "Robin Sandkühler",
            "Florentin Bieder",
            "Philippe Valmaggia",
            "P. Cattin"
        ],
        "citations": 229,
        "references": 38,
        "year": 2021
    },
    {
        "title": "Generative Neurosymbolic Machines",
        "abstract": "Reconciling symbolic and distributed representations is a crucial challenge that can potentially resolve the limitations of current deep learning. Remarkable advances in this direction have been achieved recently via generative object-centric representation models. While learning a recognition model that infers object-centric symbolic representations like bounding boxes from raw images in an unsupervised way, no such model can provide another important ability of a generative model, i.e., generating (sampling) according to the structure of learned world density. In this paper, we propose Generative Neurosymbolic Machines, a generative model that combines the benefits of distributed and symbolic representations to support both structured representations of symbolic components and density-based generation. These two crucial properties are achieved by a two-layer latent hierarchy with the global distributed latent for flexible density modeling and the structured symbolic latent map. To increase the model flexibility in this hierarchical structure, we also propose the StructDRAW prior. In experiments, we show that the proposed model significantly outperforms the previous structured representation models as well as the state-of-the-art non-structured generative models in terms of both structure accuracy and image generation quality.",
        "authors": [
            "Jindong Jiang",
            "Sungjin Ahn"
        ],
        "citations": 67,
        "references": 51,
        "year": 2020
    },
    {
        "title": "Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "Gihyun Kwon",
            "Chihye Han",
            "Dae-Shik Kim"
        ],
        "citations": 113,
        "references": 19,
        "year": 2019
    },
    {
        "title": "BézierSketch: A generative model for scalable vector sketches",
        "abstract": null,
        "authors": [
            "Ayan Das",
            "Yongxin Yang",
            "Timothy M. Hospedales",
            "T. Xiang",
            "Yi-Zhe Song"
        ],
        "citations": 53,
        "references": 37,
        "year": 2020
    },
    {
        "title": "Bayesian Generative Active Deep Learning",
        "abstract": "© 36th International Conference on Machine Learning, ICML 2019. All rights reserved. Deep learning models have demonstrated outstanding performance in several problems, but their training process tends to require immense amounts of computational and human resources for training and labeling, constraining the types of problems that can be tackled. Therefore, the design of effective training methods that require small labeled training sets is an important research direction that will allow a more effective use of resources. Among current approaches designed to address this issue, two are particularly interesting: data augmentation and active learning. Data augmentation achieves this goal by artificially generating new training points, while active learning relies on the selection of the \"most informative\" subset of unlabeled training samples to be labelled by an oracle. Although successful in practice, data augmentation can waste computational resources because it indiscriminately generates samples that are not guaranteed to be informative, and active learning selects a small subset of informative samples (from a large un-annotated set) that may be insufficient for the training process. In this paper, we propose a Bayesian generative active deep learning approach that combines active learning with data augmentation - we provide theoretical and empirical evidence (MNIST, CIFAR-{10,100}, and SVHN) that our approach has more efficient training and better classification results than data augmentation and active learning.",
        "authors": [
            "Toan Tran",
            "Thanh-Toan Do",
            "I. Reid",
            "G. Carneiro"
        ],
        "citations": 128,
        "references": 37,
        "year": 2019
    },
    {
        "title": "Generative modeling for protein structures",
        "abstract": "Analyzing the structure and function of proteins is a key part of understanding biology at the molecular and cellular level. In addition, a major engineering challenge is to design new proteins in a principled and methodical way. Current computational modeling methods for protein design are slow and often require human oversight and intervention. Here, we apply Generative Adversarial Networks (GANs) to the task of generating protein structures, toward application in fast de novo protein design. We encode protein structures in terms of pairwise distances between alpha-carbons on the protein backbone, which eliminates the need for the generative model to learn translational and rotational symmetries. We then introduce a convex formulation of corruption-robust 3D structure recovery to fold the protein structures from generated pairwise distance maps, and solve these problems using the Alternating Direction Method of Multipliers. We test the effectiveness of our models by predicting completions of corrupted protein structures and show that the method is capable of quickly producing structurally plausible solutions.",
        "authors": [
            "N. Anand",
            "Po-Ssu Huang"
        ],
        "citations": 156,
        "references": 44,
        "year": 2018
    },
    {
        "title": "CapsuleGAN: Generative Adversarial Capsule Network",
        "abstract": null,
        "authors": [
            "Ayush Jaiswal",
            "Wael AbdAlmageed",
            "P. Natarajan"
        ],
        "citations": 156,
        "references": 20,
        "year": 2018
    },
    {
        "title": "DeLiGAN: Generative Adversarial Networks for Diverse and Limited Data",
        "abstract": "A class of recent approaches for generating images, called Generative Adversarial Networks (GAN), have been used to generate impressively realistic images of objects, bedrooms, handwritten digits and a variety of other image modalities. However, typical GAN-based approaches require large amounts of training data to capture the diversity across the image modality. In this paper, we propose DeLiGAN &#x2013; a novel GAN-based architecture for diverse and limited training data scenarios. In our approach, we reparameterize the latent generative space as a mixture model and learn the mixture models parameters along with those of GAN. This seemingly simple modification to the GAN framework is surprisingly effective and results in models which enable diversity in generated samples although trained with limited data. In our work, we show that DeLiGAN can generate images of handwritten digits, objects and hand-drawn sketches, all using limited amounts of data. To quantitatively characterize intra-class diversity of generated samples, we also introduce a modified version of inception-score, a measure which has been found to correlate well with human assessment of generated samples.",
        "authors": [
            "Swaminathan Gurumurthy",
            "Ravi Kiran Sarvadevabhatla",
            "R. Venkatesh Babu"
        ],
        "citations": 258,
        "references": 26,
        "year": 2017
    },
    {
        "title": "Evolutionary Multiobjective Optimization Driven by Generative Adversarial Networks (GANs)",
        "abstract": "Recently, increasing works have been proposed to drive evolutionary algorithms using machine-learning models. Usually, the performance of such model-based evolutionary algorithms is highly dependent on the training qualities of the adopted models. Since it usually requires a certain amount of data (i.e., the candidate solutions generated by the algorithms) for model training, the performance deteriorates rapidly with the increase of the problem scales due to the curse of dimensionality. To address this issue, we propose a multiobjective evolutionary algorithm driven by the generative adversarial networks (GANs). At each generation of the proposed algorithm, the parent solutions are first classified into real and fake samples to train the GANs; then the offspring solutions are sampled by the trained GANs. Thanks to the powerful generative ability of the GANs, our proposed algorithm is capable of generating promising offspring solutions in high-dimensional decision space with limited training data. The proposed algorithm is tested on ten benchmark problems with up to 200 decision variables. The experimental results on these test problems demonstrate the effectiveness of the proposed algorithm.",
        "authors": [
            "Cheng He",
            "Shihua Huang",
            "Ran Cheng",
            "K. Tan",
            "Yaochu Jin"
        ],
        "citations": 116,
        "references": 79,
        "year": 2019
    },
    {
        "title": "Generating Datasets with Pretrained Language Models",
        "abstract": "To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.",
        "authors": [
            "Timo Schick",
            "Hinrich Schütze"
        ],
        "citations": 219,
        "references": 58,
        "year": 2021
    },
    {
        "title": "A Non-Parametric Generative Model for Human Trajectories",
        "abstract": "Modeling human mobility and synthesizing realistic trajectories play a fundamental role in urban planning and privacy-preserving location data analysis.  Due to its high dimensionality and also the diversity of its applications, existing trajectory generative models do not preserve the geometric (and more importantly) semantic features of human mobility, especially for longer trajectories. In this paper, we propose and evaluate a novel non-parametric generative model for location trajectories that tries to capture the statistical features of human mobility {\\em as a whole}.  This is in contrast with existing models that generate trajectories in a sequential manner.  We design a new representation of locations, and use generative adversarial networks to produce data points in that representation space which will be then transformed to a time-series location trajectory form.  We evaluate our method on realistic location trajectories and compare our synthetic traces with multiple existing methods on how they preserve geographic and semantic features of real traces at both aggregated and individual levels.  The empirical results prove the capability of our model in preserving the utility of real data.",
        "authors": [
            "Kun Ouyang",
            "R. Shokri",
            "David S. Rosenblum",
            "Wenzhuo Yang"
        ],
        "citations": 109,
        "references": 25,
        "year": 2018
    },
    {
        "title": "Diagnosing and Enhancing VAE Models",
        "abstract": "Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning. Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019). The code for our model is available at this https URL TwoStageVAE.",
        "authors": [
            "B. Dai",
            "D. Wipf"
        ],
        "citations": 358,
        "references": 45,
        "year": 2019
    },
    {
        "title": "Variational Approaches for Auto-Encoding Generative Adversarial Networks",
        "abstract": "Auto-encoding generative adversarial networks (GANs) combine the standard GAN algorithm, which discriminates between real and model-generated data, with a reconstruction loss given by an auto-encoder. Such models aim to prevent mode collapse in the learned generative model by ensuring that it is grounded in all the available training data. In this paper, we develop a principle upon which auto-encoders can be combined with generative adversarial networks by exploiting the hierarchical structure of the generative model. The underlying principle shows that variational inference can be used a basic tool for learning, but with the in- tractable likelihood replaced by a synthetic likelihood, and the unknown posterior distribution replaced by an implicit distribution; both synthetic likelihoods and implicit posterior distributions can be learned using discriminators. This allows us to develop a natural fusion of variational auto-encoders and generative adversarial networks, combining the best of both these methods. We describe a unified objective for optimization, discuss the constraints needed to guide learning, connect to the wide range of existing work, and use a battery of tests to systematically and quantitatively assess the performance of our method.",
        "authors": [
            "Mihaela Rosca",
            "Balaji Lakshminarayanan",
            "David Warde-Farley",
            "S. Mohamed"
        ],
        "citations": 254,
        "references": 49,
        "year": 2017
    },
    {
        "title": "Synthesizing 3D Shapes via Modeling Multi-view Depth Maps and Silhouettes with Deep Generative Networks",
        "abstract": "We study the problem of learning generative models of 3D shapes. Voxels or 3D parts have been widely used as the underlying representations to build complex 3D shapes, however, voxel-based representations suffer from high memory requirements, and parts-based models require a large collection of cached or richly parametrized parts. We take an alternative approach: learning a generative model over multi-view depth maps or their corresponding silhouettes, and using a deterministic rendering function to produce 3D shapes from these images. A multi-view representation of shapes enables generation of 3D models with fine details, as 2D depth maps and silhouettes can be modeled at a much higher resolution than 3D voxels. Moreover, our approach naturally brings the ability to recover the underlying 3D representation from depth maps of one or a few viewpoints. Experiments show that our framework can generate 3D shapes with variations and details. We also demonstrate that our model has out-of-sample generalization power for real-world tasks with occluded objects.",
        "authors": [
            "Amir Arsalan Soltani",
            "Haibin Huang",
            "Jiajun Wu",
            "Tejas D. Kulkarni",
            "J. Tenenbaum"
        ],
        "citations": 196,
        "references": 32,
        "year": 2017
    },
    {
        "title": "Generative Compression",
        "abstract": "Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. We describe the concept of generative compression, the compression of data using generative models, and suggest that it is a direction worth pursuing to produce more accurate and visually pleasing reconstructions at deeper compression levels for both image and video data. We also show that generative compression is orders- of-magnitude more robust to bit errors (e.g., from noisy channels) than traditional variable-length coding schemes.",
        "authors": [
            "Shibani Santurkar",
            "D. Budden",
            "N. Shavit"
        ],
        "citations": 184,
        "references": 54,
        "year": 2017
    },
    {
        "title": "Emerging Convolutions for Generative Normalizing Flows",
        "abstract": "Generative flows are attractive because they admit exact likelihood optimization and efficient image synthesis. Recently, Kingma & Dhariwal (2018) demonstrated with Glow that generative flows are capable of generating high quality images. We generalize the 1 x 1 convolutions proposed in Glow to invertible d x d convolutions, which are more flexible since they operate on both channel and spatial axes. We propose two methods to produce invertible convolutions that have receptive fields identical to standard convolutions: Emerging convolutions are obtained by chaining specific autoregressive convolutions, and periodic convolutions are decoupled in the frequency domain. Our experiments show that the flexibility of d x d convolutions significantly improves the performance of generative flow models on galaxy images, CIFAR10 and ImageNet.",
        "authors": [
            "Emiel Hoogeboom",
            "Rianne van den Berg",
            "Max Welling"
        ],
        "citations": 97,
        "references": 33,
        "year": 2019
    },
    {
        "title": "Quantum Wasserstein Generative Adversarial Networks",
        "abstract": "The study of quantum generative models is well-motivated, not only because of its importance in quantum machine learning and quantum chemistry but also because of the perspective of its implementation on near-term quantum machines. Inspired by previous studies on the adversarial training of classical and quantum generative models, we propose the first design of quantum Wasserstein Generative Adversarial Networks (WGANs), which has been shown to improve the robustness and the scalability of the adversarial training of quantum generative models even on noisy quantum hardware. Specifically, we propose a definition of the Wasserstein semimetric between quantum data, which inherits a few key theoretical merits of its classical counterpart. We also demonstrate how to turn the quantum Wasserstein semimetric into a concrete design of quantum WGANs that can be efficiently implemented on quantum machines. Our numerical study, via classical simulation of quantum systems, shows the more robust and scalable numerical performance of our quantum WGANs over other quantum GAN proposals. As a surprising application, our quantum WGAN has been used to generate a 3-qubit quantum circuit of ~50 gates that well approximates a 3-qubit 1-d Hamiltonian simulation circuit that requires over 10k gates using standard techniques.",
        "authors": [
            "Shouvanik Chakrabarti",
            "Yiming Huang",
            "Tongyang Li",
            "S. Feizi",
            "Xiaodi Wu"
        ],
        "citations": 79,
        "references": 50,
        "year": 2019
    },
    {
        "title": "Training Normalizing Flows with the Information Bottleneck for Competitive Generative Classification",
        "abstract": "The Information Bottleneck (IB) objective uses information theory to formulate a task-performance versus robustness trade-off. It has been successfully applied in the standard discriminative classification setting. We pose the question whether the IB can also be used to train generative likelihood models such as normalizing flows. Since normalizing flows use invertible network architectures (INNs), they are information-preserving by construction. This seems contradictory to the idea of a bottleneck. In this work, firstly, we develop the theory and methodology of IB-INNs, a class of conditional normalizing flows where INNs are trained using the IB objective: Introducing a small amount of {\\em controlled} information loss allows for an asymptotically exact formulation of the IB, while keeping the INN's generative capabilities intact. Secondly, we investigate the properties of these models experimentally, specifically used as generative classifiers. This model class offers advantages such as improved uncertainty quantification and out-of-distribution detection, but traditional generative classifier solutions suffer considerably in classification accuracy. We find the trade-off parameter in the IB controls a mix of generative capabilities and accuracy close to standard classifiers. Empirically, our uncertainty estimates in this mixed regime compare favourably to conventional generative and discriminative classifiers.",
        "authors": [
            "Lynton Ardizzone",
            "Radek Mackowiak",
            "C. Rother",
            "Ullrich Kothe"
        ],
        "citations": 49,
        "references": 50,
        "year": 2020
    },
    {
        "title": "Learning Feature-to-Feature Translator by Alternating Back-Propagation for Generative Zero-Shot Learning",
        "abstract": "We investigate learning feature-to-feature translator networks by alternating back-propagation as a general-purpose solution to zero-shot learning (ZSL) problems. It is a generative model-based ZSL framework. In contrast to models based on generative adversarial networks (GAN) or variational autoencoders (VAE) that require auxiliary networks to assist the training, our model consists of a single conditional generator that maps class-level semantic features and Gaussian white noise vectors accounting for instance-level latent factors to visual features, and is trained by maximum likelihood estimation. The training process is a simple yet effective alternating back-propagation process that iterates the following two steps: (i) the inferential back-propagation to infer the latent noise vector of each observed example, and (ii) the learning back-propagation to update the model parameters. We show that, with slight modifications, our model is capable of learning from incomplete visual features for ZSL. We conduct extensive comparisons with existing generative ZSL methods on five benchmarks, demonstrating the superiority of our method in not only ZSL performance but also convergence speed and computational cost. Specifically, our model outperforms the existing state-of-the-art methods by a remarkable margin up to 3.1% and 4.0% in ZSL and generalized ZSL settings, respectively.",
        "authors": [
            "Yizhe Zhu",
            "Jianwen Xie",
            "Bingchen Liu",
            "A. Elgammal"
        ],
        "citations": 82,
        "references": 71,
        "year": 2019
    },
    {
        "title": "Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution",
        "abstract": "Video super-resolution (VSR) has become one of the most critical problems in video processing. In the deep learning literature, recent works have shown the benefits of using adversarial-based and perceptual losses to improve the performance on various image restoration tasks; however, these have yet to be applied for video super-resolution. In this paper, we propose a generative adversarial network (GAN)-based formulation for VSR. We introduce a new generator network optimized for the VSR problem, named VSRResNet, along with new discriminator architecture to properly guide VSRResNet during the GAN training. We further enhance our VSR GAN formulation with two regularizers, a distance loss in feature-space and pixel-space, to obtain our final VSRResFeatGAN model. We show that pre-training our generator with the mean-squared-error loss only quantitatively surpasses the current state-of-the-art VSR models. Finally, we employ the PercepDist metric to compare the state-of-the-art VSR models. We show that this metric more accurately evaluates the perceptual quality of SR solutions obtained from neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we show that our proposed model, the VSRResFeatGAN model, outperforms the current state-of-the-art SR models, both quantitatively and qualitatively.",
        "authors": [
            "Alice Lucas",
            "Santiago L&#x00F3;pez-Tapia",
            "R. Molina",
            "A. Katsaggelos"
        ],
        "citations": 162,
        "references": 32,
        "year": 2018
    },
    {
        "title": "Implicit Generation and Generalization with Energy Based Models",
        "abstract": "Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale EBM training through a MCMC framework to modern architectures. We show that MCMC sampling on EBMs generate significantly better samples than other likelihood models (with significantly lower generation cost and parameters), so much so that they are competitive with GANs. We show that EBMs are good likelihood models, able to both reliably restore test CIFAR-10 images and interconvert between classes of CIFAR-10 images. Finally show that EBMs generalize well. On CIFAR10, we achieve better out of distribution generalization than other state of the art generative models (such as assigning high likelihood to CIFAR-10 images than SVHN images). For time series modeling, EBMs generalize much better for long term time series prediction than corresponding feed-forward networks. Compositionaly, we find EBMs generalize to effectively generate samples when jointly conditioned on independent conditional EBMs for different latents.",
        "authors": [
            "Yilun Du",
            "Igor Mordatch"
        ],
        "citations": 275,
        "references": 27,
        "year": 2018
    },
    {
        "title": "Detecting Overfitting of Deep Generative Networks via Latent Recovery",
        "abstract": "State of the art deep generative networks have achieved such realism that they can be suspected of memorizing training images. It is why it is not uncommon to include visualizations of training set nearest neighbors, to suggest generated images are not simply memorized. We argue this is not sufficient and motivates studying overfitting of deep generators with more scrutiny. We address this question by i) showing how simple losses are highly effective at reconstructing images for deep generators ii) analyzing the statistics of reconstruction errors for training versus validation images. Using this methodology, we show that pure GAN models appear to generalize well, in contrast with those using hybrid adversarial losses, which are amongst the most widely applied generative methods. We also show that standard GAN evaluation metrics fail to capture memorization for some deep generators. Finally, we note the ramifications of memorization on data privacy. Considering the already widespread application of generative networks, we provide a step in the right direction towards the important yet incomplete picture of generative overfitting.",
        "authors": [
            "Ryan Webster",
            "J. Rabin",
            "Loïc Simon",
            "F. Jurie"
        ],
        "citations": 96,
        "references": 44,
        "year": 2019
    },
    {
        "title": "MaCow: Masked Convolutional Generative Flow",
        "abstract": "Flow-based generative models, conceptually attractive due to tractability of both the exact log-likelihood computation and latent-variable inference, and efficiency of both training and sampling, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. Despite their computational efficiency, the density estimation performance of flow-based generative models significantly falls behind those of state-of-the-art autoregressive models. In this work, we introduce masked convolutional generative flow (MaCow), a simple yet effective architecture of generative flow using masked convolution. By restricting the local connectivity in a small kernel, MaCow enjoys the properties of fast and stable training, and efficient sampling, while achieving significant improvements over Glow for density estimation on standard image benchmarks, considerably narrowing the gap to autoregressive models.",
        "authors": [
            "Xuezhe Ma",
            "E. Hovy"
        ],
        "citations": 66,
        "references": 38,
        "year": 2019
    },
    {
        "title": "CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning",
        "abstract": "It is known that the inconsistent distribution and representation of different modalities, such as image and text, cause the heterogeneity gap that makes it challenging to correlate such heterogeneous data. Generative adversarial networks (GANs) have shown its strong ability of modeling data distribution and learning discriminative representation, existing GANs-based works mainly focus on generative problem to generate new data. We have different goal, aim to correlate heterogeneous data, by utilizing the power of GANs to model cross-modal joint distribution. Thus, we propose Cross-modal GANs to learn discriminative common representation for bridging heterogeneity gap. The main contributions are: (1) Cross-modal GANs architecture is proposed to model joint distribution over data of different modalities. The inter-modality and intra-modality correlation can be explored simultaneously in generative and discriminative models. Both of them beat each other to promote cross-modal correlation learning. (2) Cross-modal convolutional autoencoders with weight-sharing constraint are proposed to form generative model. They can not only exploit cross-modal correlation for learning common representation, but also preserve reconstruction information for capturing semantic consistency within each modality. (3) Cross-modal adversarial mechanism is proposed, which utilizes two kinds of discriminative models to simultaneously conduct intra-modality and inter-modality discrimination. They can mutually boost to make common representation more discriminative by adversarial training process. To the best of our knowledge, our proposed CM-GANs approach is the first to utilize GANs to perform cross-modal common representation learning. Experiments are conducted to verify the performance of our proposed approach on cross-modal retrieval paradigm, compared with 10 methods on 3 cross-modal datasets.",
        "authors": [
            "Yuxin Peng",
            "Jinwei Qi",
            "Yuxin Yuan"
        ],
        "citations": 235,
        "references": 46,
        "year": 2017
    },
    {
        "title": "Generative Poisoning Attack Method Against Neural Networks",
        "abstract": "Poisoning attack is identified as a severe security threat to machine learning algorithms. In many applications, for example, deep neural network (DNN) models collect public data as the inputs to perform re-training, where the input data can be poisoned. Although poisoning attack against support vector machines (SVM) has been extensively studied before, there is still very limited knowledge about how such attack can be implemented on neural networks (NN), especially DNNs. In this work, we first examine the possibility of applying traditional gradient-based method (named as the direct gradient method) to generate poisoned data against NNs by leveraging the gradient of the target model w.r.t. the normal data. We then propose a generative method to accelerate the generation rate of the poisoned data: an auto-encoder (generator) used to generate poisoned data is updated by a reward function of the loss, and the target NN model (discriminator) receives the poisoned data to calculate the loss w.r.t. the normal data. Our experiment results show that the generative method can speed up the poisoned data generation rate by up to 239.38x compared with the direct gradient method, with slightly lower model accuracy degradation. A countermeasure is also designed to detect such poisoning attack methods by checking the loss of the target model.",
        "authors": [
            "Chaofei Yang",
            "Qing Wu",
            "Hai Helen Li",
            "Yiran Chen"
        ],
        "citations": 213,
        "references": 19,
        "year": 2017
    },
    {
        "title": "From Deterministic to Generative: Multimodal Stochastic RNNs for Video Captioning",
        "abstract": "Video captioning, in essential, is a complex natural process, which is affected by various uncertainties stemming from video content, subjective judgment, and so on. In this paper, we build on the recent progress in using encoder-decoder framework for video captioning and address what we find to be a critical deficiency of the existing methods that most of the decoders propagate deterministic hidden states. Such complex uncertainty cannot be modeled efficiently by the deterministic models. In this paper, we propose a generative approach, referred to as multimodal stochastic recurrent neural networks (MS-RNNs), which models the uncertainty observed in the data using latent stochastic variables. Therefore, MS-RNN can improve the performance of video captioning and generate multiple sentences to describe a video considering different random factors. Specifically, a multimodal long short-term memory (LSTM) is first proposed to interact with both visual and textual features to capture a high-level representation. Then, a backward stochastic LSTM is proposed to support uncertainty propagation by introducing latent variables. Experimental results on the challenging data sets, microsoft video description and microsoft research video-to-text, show that our proposed MS-RNN approach outperforms the state-of-the-art video captioning benchmarks.",
        "authors": [
            "Jingkuan Song",
            "Yuyu Guo",
            "Lianli Gao",
            "Xuelong Li",
            "A. Hanjalic",
            "Heng Tao Shen"
        ],
        "citations": 213,
        "references": 76,
        "year": 2017
    },
    {
        "title": "Language models can learn complex molecular distributions",
        "abstract": null,
        "authors": [
            "Daniel Flam-Shepherd",
            "Kevin Zhu",
            "A. Aspuru‐Guzik"
        ],
        "citations": 124,
        "references": 69,
        "year": 2021
    },
    {
        "title": "Biases in Generative Art: A Causal Look from the Lens of Art History",
        "abstract": "With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.",
        "authors": [
            "Ramya Srinivasan",
            "Kanji Uchino"
        ],
        "citations": 59,
        "references": 77,
        "year": 2020
    },
    {
        "title": "Quantum versus classical generative modelling in finance",
        "abstract": "Finding a concrete use case for quantum computers in the near term is still an open question, with machine learning typically touted as one of the first fields which will be impacted by quantum technologies. In this work, we investigate and compare the capabilities of quantum versus classical models for the task of generative modelling in machine learning. We use a real world financial dataset consisting of correlated currency pairs and compare two models in their ability to learn the resulting distribution—a restricted Boltzmann machine, and a quantum circuit Born machine. We provide extensive numerical results indicating that the simulated Born machine always at least matches the performance of the Boltzmann machine in this task, and demonstrates superior performance as the model scales. We perform experiments on both simulated and physical quantum chips using the Rigetti QCSTM platform, and also are able to partially train the largest instance to date of a quantum circuit Born machine on quantum hardware. Finally, by studying the entanglement capacity of the training Born machines, we find that entanglement typically plays a role in the problem instances which demonstrate an advantage over the Boltzmann machine.",
        "authors": [
            "Brian Coyle",
            "Maxwell P. Henderson",
            "Justin Chan Jin Le",
            "N. Kumar",
            "M. Paini",
            "E. Kashefi"
        ],
        "citations": 54,
        "references": 85,
        "year": 2020
    },
    {
        "title": "Deep Generative Endmember Modeling: An Application to Unsupervised Spectral Unmixing",
        "abstract": "Endmember (EM) spectral variability can greatly impact the performance of standard hyperspectral image analysis algorithms. Extended parametric models have been successfully applied to account for the EM spectral variability. However, these models still lack the compromise between flexibility and low-dimensional representation that is necessary to properly explore the fact that spectral variability is often confined to a low-dimensional manifold in real scenes. In this article we propose to learn a spectral variability model directly from the observed data, instead of imposing it a priori. This is achieved through a deep generative EM model, which is estimated using a variational autoencoder (VAE). The encoder and decoder that compose the generative model are trained using pure pixel information extracted directly from the observed image, what allows for an unsupervised formulation. The proposed EM model is applied to the solution of a spectral unmixing problem, which we cast as an alternating nonlinear least-squares problem that is solved iteratively with respect to the abundances and to the low-dimensional representations of the EMs in the latent space of the deep generative model. Simulations using both synthetic and real data indicate that the proposed strategy can outperform the competing state-of-the-art algorithms.",
        "authors": [
            "R. Borsoi",
            "Tales Imbiriba",
            "J. Bermudez"
        ],
        "citations": 89,
        "references": 66,
        "year": 2019
    },
    {
        "title": "AdaBalGAN: An Improved Generative Adversarial Network With Imbalanced Learning for Wafer Defective Pattern Recognition",
        "abstract": "Identification of the defective patterns of the wafer maps can provide insights for the quality control in the semiconductor wafer fabrication systems (SWFSs). In real SWFSs, the collected wafer maps are usually imbalanced from the defective types, which will result in misidentification. In this paper, a novel deep learning model called adaptive balancing generative adversarial network (AdaBalGAN) is proposed for the defective pattern recognition (DPR) of wafer maps with imbalanced data. In addition, a categorical generative adversarial network is improved to generate simulated wafer maps in high fidelity and classify the patterns with high accuracy for all defective categories. Taking consideration of the various learning abilities of the DPR model for different patterns into account, an adaptive generative controller is designed to balance the number of samples of each defective type according to the classification accuracy. The experiment results indicated that the proposed AdaBalGAN model outperforms conventional models with higher accuracy and stability for the DPR of wafer maps. Further results of comparative experiments revealed that the proposed adaptive generative mechanism can enhance and balance the recognition accuracy for all categories in the DPR of wafer maps.",
        "authors": [
            "Junliang Wang",
            "Zhengliang Yang",
            "Jie Zhang",
            "Qihua Zhang",
            "W. Chien"
        ],
        "citations": 99,
        "references": 49,
        "year": 2019
    },
    {
        "title": "The Generative Adversarial Brain",
        "abstract": "The idea that the brain learns generative models of the world has been widely promulgated. Most approaches have assumed that the brain learns an explicit density model that assigns a probability to each possible state of the world. However, explicit density models are difficult to learn, requiring approximate inference techniques that may find poor solutions. An alternative approach is to learn an implicit density model that can sample from the generative model without evaluating the probabilities of those samples. The implicit model can be trained to fool a discriminator into believing that the samples are real. This is the idea behind generative adversarial algorithms, which have proven adept at learning realistic generative models. This paper develops an adversarial framework for probabilistic computation in the brain. It first considers how generative adversarial algorithms overcome some of the problems that vex prior theories based on explicit density models. It then discusses the psychological and neural evidence for this framework, as well as how the breakdown of the generator and discriminator could lead to delusions observed in some mental disorders.",
        "authors": [
            "S. Gershman"
        ],
        "citations": 58,
        "references": 84,
        "year": 2019
    },
    {
        "title": "Conditional Image Generation with Score-Based Diffusion Models",
        "abstract": "Score-based diffusion models have emerged as one of the most promising frameworks for deep generative modelling. In this work we conduct a systematic comparison and theoretical analysis of different approaches to learning conditional probability distributions with score-based diffusion models. In particular, we prove results which provide a theoretical justification for one of the most successful estimators of the conditional score. Moreover, we introduce a multi-speed diffusion framework, which leads to a new estimator for the conditional score, performing on par with previous state-of-the-art approaches. Our theoretical and experimental findings are accompanied by an open source library MSDiff which allows for application and further research of multi-speed diffusion models.",
        "authors": [
            "Georgios Batzolis",
            "Jan Stanczuk",
            "C. Schonlieb",
            "Christian Etmann"
        ],
        "citations": 147,
        "references": 30,
        "year": 2021
    },
    {
        "title": "Learning Stable Deep Dynamics Models",
        "abstract": "Deep networks are commonly used to model dynamical systems, predicting how the state of a system will evolve over time (either autonomously or in response to control inputs). Despite the predictive power of these systems, it has been difficult to make formal claims about the basic properties of the learned systems. In this paper, we propose an approach for learning dynamical systems that are guaranteed to be stable over the entire state space. The approach works by jointly learning a dynamics model and Lyapunov function that guarantees non-expansiveness of the dynamics under the learned Lyapunov function. We show that such learning systems are able to model simple dynamical systems and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion.",
        "authors": [
            "J. Z. Kolter",
            "Gaurav Manek"
        ],
        "citations": 173,
        "references": 21,
        "year": 2020
    },
    {
        "title": "Binary Generative Adversarial Networks for Image Retrieval",
        "abstract": "\n \n The most striking successes in image retrieval using deep hashing have mostly involved discriminative models, which require labels. In this paper, we use binary generative adversarial networks (BGAN) to embed images to binary codes in an unsupervised way. By restricting the input noise variable of generative adversarial networks (GAN) to be binary and conditioned on the features of each input image, BGAN can simultaneously learn a binary representation per image, and generate an image plausibly similar to the original one. In the proposed framework, we address two main problems: 1) how to directly generate binary codes without relaxation? 2) how to equip the binary representation with the ability of accurate image retrieval? We resolve these problems by proposing new sign-activation strategy and a loss function steering the learning process, which consists of new models for adversarial loss, a content loss, and a neighborhood structure loss. Experimental results on standard datasets (CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly outperforms existing hashing methods by up to 107% in terms of mAP (See Table 2).\n \n",
        "authors": [
            "Jingkuan Song"
        ],
        "citations": 187,
        "references": 58,
        "year": 2017
    },
    {
        "title": "Generative Modeling for Small-Data Object Detection",
        "abstract": "This paper explores object detection in the small data regime, where only a limited number of annotated bounding boxes are available due to data rarity and annotation expense. This is a common challenge today with machine learning being applied to many new tasks where obtaining training data is more challenging, e.g. in medical images with rare diseases that doctors sometimes only see once in their life-time. In this work we explore this problem from a generative modeling perspective by learning to generate new images with associated bounding boxes, and using these for training an object detector. We show that simply training previously proposed generative models does not yield satisfactory performance due to them optimizing for image realism rather than object detection accuracy. To this end we develop a new model with a novel unrolling mechanism that jointly optimizes the generative model and a detector such that the generated images improve the performance of the detector. We show this method outperforms the state of the art on two challenging datasets, disease detection and small data pedestrian detection, improving the average precision on NIH Chest X-ray by a relative 20% and localization accuracy by a relative 50%.",
        "authors": [
            "Lanlan Liu",
            "Michael Muelly",
            "Jia Deng",
            "Tomas Pfister",
            "Li-Jia Li"
        ],
        "citations": 58,
        "references": 31,
        "year": 2019
    },
    {
        "title": "A Generative Model of Urban Activities from Cellular Data",
        "abstract": "Activity-based travel demand models are becoming essential tools used in transportation planning and regional development scenario evaluation. They describe travel itineraries of individual travelers, namely, what activities they are participating in, when they perform these activities, and how they choose to travel to the activity locales. However, data collection for activity-based models is performed through travel surveys that are infrequent, expensive, and reflect the changes in transportation with significant delays. Thanks to the ubiquitous cell phone data, we see an opportunity to substantially complement these surveys with data extracted from network carrier mobile phone usage logs, such as call detail records (CDRs). In this paper, we develop input–output hidden Markov models to infer travelers’ activity patterns from CDRs. We apply the model to the data collected by a major network carrier serving millions of users in the San Francisco Bay Area. Our approach delivers an end-to-end actionable solution to the practitioners in the form of a modular and interpretable activity-based travel demand model. It is experimentally validated with three independent data sources: aggregated statistics from travel surveys, a set of collected ground truth activities, and the results of a traffic micro-simulation informed with the travel plans synthesized from the developed generative model.",
        "authors": [
            "Mogeng Yin",
            "M. Sheehan",
            "Sidney A. Feygin",
            "Jean-François Paiement",
            "A. Pozdnoukhov"
        ],
        "citations": 136,
        "references": 34,
        "year": 2018
    },
    {
        "title": "Molecular Generative Model Based On Adversarially Regularized Autoencoder",
        "abstract": "Deep generative models are attracting great attention as a new promising approach for molecular design. A variety of models reported so far are based on either variational autoencoder (VAE) or generative adversarial network (GAN), but they have limitations such as low validity and uniqueness. Here we propose a new type of model based on an adversarially regularized autoencoder (ARAE). It basically uses latent variables like VAE, but the distribution of the latent variables is estimated by adversarial training like in GAN. The latter is intended to avoid both insufficiently flexible approximation of posterior distribution in VAE and difficulty in handling discrete variables in GAN. Our benchmark study showed that ARAE indeed outperformed conventional models in terms of validity, uniqueness, and novelty per generated molecule. We also demonstrated successful conditional generation of drug-like molecules with ARAE for both cases of single and multiple properties control. As a potential real-world application, we could generate EGFR inhibitors sharing the scaffolds of known active molecules while satisfying drug-like conditions simultaneously.",
        "authors": [
            "S. Hong",
            "Jaechang Lim",
            "Seongok Ryu",
            "W. Kim"
        ],
        "citations": 60,
        "references": 45,
        "year": 2019
    },
    {
        "title": "Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets",
        "abstract": "This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ( i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.",
        "authors": [
            "Zhen Yang",
            "Wei Chen",
            "Feng Wang",
            "Bo Xu"
        ],
        "citations": 167,
        "references": 39,
        "year": 2017
    },
    {
        "title": "Blind Image Deconvolution Using Deep Generative Priors",
        "abstract": "This article proposes a novel approach to regularize the ill-posed and non-linear blind image deconvolution (blind deblurring) using deep generative networks as priors. We employ two separate pretrained generative networks — given lower-dimensional Gaussian vectors as input, one of the generative models samples from the distribution of sharp images, while the other from that of the blur kernels. To deblur, we find a sharp image and a blur kernel in the range of the respective generators that best explain the blurred image. Our experiments show promising deblurring results on images even under large blurs, and heavy measurement noise. Generative models often manifest a representation error to fit arbitrary samples from the learned distribution. This may be due to multiple factors such as mode collapse, architectural choices, or training caveats. To improve the generalizability of the proposed approach, we present a modification of the proposed scheme that governs the deblurring process under both generative, and classical priors. Training generative models is computationally expensive on larger and more diverse image datasets. Our experiments also show that even an untrained structured (convolutional) network acts as an image prior. We leverage this fact to deblur diverse/complex images for which a trained generative network might not be available.",
        "authors": [
            "Muhammad Asim",
            "Fahad Shamshad",
            "Ali Ahmed"
        ],
        "citations": 91,
        "references": 66,
        "year": 2018
    },
    {
        "title": "Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative Model",
        "abstract": "In this paper we study generative modeling via autoencoders while using the elegant geometric properties of the optimal transport (OT) problem and the Wasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE), which are generative models that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or defining a closed-form for the distribution. In short, we regularize the autoencoder loss with the sliced-Wasserstein distance between the distribution of the encoded training samples and a predefined samplable distribution. We show that the proposed formulation has an efficient numerical solution that provides similar capabilities to Wasserstein Autoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an embarrassingly simple implementation.",
        "authors": [
            "Soheil Kolouri",
            "Charles E. Martin",
            "G. Rohde"
        ],
        "citations": 91,
        "references": 42,
        "year": 2018
    },
    {
        "title": "Few-shot Generative Modelling with Generative Matching Networks",
        "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. We develop a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks. By conditioning on the additional input dataset, our model can instantly learn new concepts that were not available in the training data but conform to a similar generative process. The proposed framework does not explicitly restrict diversity of the conditioning data and also does not require an extensive inference procedure for training or adaptation. Our experiments on the Omniglot dataset demonstrate that Generative Matching Networks significantly improve predictive performance on the fly as more additional data is available and outperform existing state of the art conditional generative models.",
        "authors": [
            "Sergey Bartunov",
            "D. Vetrov"
        ],
        "citations": 89,
        "references": 26,
        "year": 2018
    },
    {
        "title": "Generative Adversarial Networks: A Literature Review",
        "abstract": "The Generative Adversarial Networks, as one of the most creative deep learning models in recent years, has achieved great success in computer vision and natural language processing. It uses the game theory to generate the best sample in generator and discriminator. Recently, many deep learning models have been applied to the security field. Along with the idea of “generative” and “adversarial”, researchers are trying to apply Generative Adversarial Networks to the security field. This paper presents the development of Generative Adversarial Networks. We review traditional generation models and typical Generative Adversarial Networks models, analyze the application of their models in natural language processing and computer vision. To emphasize that Generative Adversarial Networks models are feasible to be used in security, we separately review the contributions that their defenses in information security, cyber security and artificial intelligence security. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.",
        "authors": [
            "Jieren Cheng",
            "Yue Yang",
            "Xiangyan Tang",
            "Naixue Xiong",
            "Yuan Zhang",
            "Feifei Lei"
        ],
        "citations": 37,
        "references": 118,
        "year": 2020
    },
    {
        "title": "Are Generative Classifiers More Robust to Adversarial Attacks?",
        "abstract": "There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers which only models the conditional distribution of the labels given the inputs. In this abstract we propose deep Bayes classifier that improves the classical naive Bayes with conditional deep generative models, and verifies its robustness against a number of existing attacks. We further developed a detection method for adversarial examples based on conditional deep generative models. Our initial results on MNIST suggest that deep Bayes classifiers might be more robust when compared with deep discriminative classifiers, and the proposed detection method achieves high detection rates against two commonly used attacks.",
        "authors": [
            "Yingzhen Li",
            "John Bradshaw",
            "Yash Sharma"
        ],
        "citations": 77,
        "references": 61,
        "year": 2018
    },
    {
        "title": "Differentially Private Mixture of Generative Neural Networks",
        "abstract": "Generative models are used in an increasing number of applications that rely on large amounts of contextually rich information about individuals. Owing to possible privacy violations, however, publishing or sharing generative models is not always viable. In this paper, we introduce a novel solution for privately releasing generative models and entire high-dimensional datasets produced by these models. We model the generator distribution of the training data by a mixture of k generative neural networks. These are trained together and collectively learn the generator distribution of a dataset. Data is divided into k clusters, using a novel differentially private kernel k-means, then each cluster is given to separate generative neural networks, such as Restricted Boltzmann Machines or Variational Autoencoders, which are trained only on their own cluster using differentially private gradient descent. We evaluate our approach using the MNIST dataset and a large Call Detail Records dataset, showing that it produces realistic synthetic samples, which can also be used to accurately compute arbitrary number of counting queries.",
        "authors": [
            "G. Ács",
            "Luca Melis",
            "C. Castelluccia",
            "Emiliano De Cristofaro"
        ],
        "citations": 116,
        "references": 61,
        "year": 2017
    },
    {
        "title": "Continual Learning in Generative Adversarial Nets",
        "abstract": "Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling.",
        "authors": [
            "Ari Seff",
            "Alex Beatson",
            "Daniel Suo",
            "Han Liu"
        ],
        "citations": 125,
        "references": 19,
        "year": 2017
    },
    {
        "title": "Deep Generative Modeling of LiDAR Data",
        "abstract": "Building models capable of generating structured output is a key challenge for AI and robotics. While generative models have been explored on many types of data, little work has been done on synthesizing lidar scans, which play a key role in robot mapping and localization. In this work, we show that one can adapt deep generative models for this task by unravelling lidar scans into a 2D point map. Our approach can generate high quality samples, while simultaneously learning a meaningful latent representation of the data. We demonstrate significant improvements against state-of-the-art point cloud generation methods. Furthermore, we propose a novel data representation that augments the 2D signal with absolute positional information. We show that this helps robustness to noisy and imputed input; the learned model can recover the underlying lidar scan from seemingly uninformative data.",
        "authors": [
            "Lucas Caccia",
            "H. V. Hoof",
            "Aaron C. Courville",
            "Joelle Pineau"
        ],
        "citations": 64,
        "references": 40,
        "year": 2018
    },
    {
        "title": "Deep Generative Modeling for Scene Synthesis via Hybrid Representations",
        "abstract": "We present a deep generative scene modeling technique for indoor environments. Our goal is to train a generative model using a feed-forward neural network that maps a prior distribution (e.g., a normal distribution) to the distribution of primary objects in indoor scenes. We introduce a 3D object arrangement representation that models the locations and orientations of objects, based on their size and shape attributes. Moreover, our scene representation is applicable for 3D objects with different multiplicities (repetition counts), selected from a database. We show a principled way to train this model by combining discriminative losses for both a 3D object arrangement representation and a 2D image-based representation. We demonstrate the effectiveness of our scene representation and the network training method on benchmark datasets. We also show the applications of this generative model in scene interpolation and scene completion.",
        "authors": [
            "Zaiwei Zhang",
            "Zhenpei Yang",
            "Chongyang Ma",
            "Linjie Luo",
            "Alexander G. Huth",
            "E. Vouga",
            "Qi-Xing Huang"
        ],
        "citations": 110,
        "references": 107,
        "year": 2018
    },
    {
        "title": "Scalable Unbalanced Optimal Transport using Generative Adversarial Networks",
        "abstract": "Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner. In addition, we propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs. We also provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018), and perform numerical experiments demonstrating how this methodology can be applied to population modeling.",
        "authors": [
            "Karren D. Yang",
            "Caroline Uhler"
        ],
        "citations": 69,
        "references": 40,
        "year": 2018
    },
    {
        "title": "A Deep Generative Model for Graph Layout",
        "abstract": "Different layouts can characterize different aspects of the same graph. Finding a “good” layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.",
        "authors": [
            "Oh-Hyun Kwon",
            "K. Ma"
        ],
        "citations": 46,
        "references": 90,
        "year": 2019
    },
    {
        "title": "Misc-GAN: A Multi-scale Generative Model for Graphs",
        "abstract": "Characterizing and modeling the distribution of a particular family of graphs are essential for the studying real-world networks in a broad spectrum of disciplines, ranging from market-basket analysis to biology, from social science to neuroscience. However, it is unclear how to model these complex graph organizations and learn generative models from an observed graph. The key challenges stem from the non-unique, high-dimensional nature of graphs, as well as graph community structures at different granularity levels. In this paper, we propose a multi-scale graph generative model named Misc-GAN, which models the underlying distribution of graph structures at different levels of granularity, and then “transfers” such hierarchical distribution from the graphs in the domain of interest, to a unique graph representation. The empirical results on seven real data sets demonstrate the effectiveness of the proposed framework.",
        "authors": [
            "Dawei Zhou",
            "Lecheng Zheng",
            "Jiejun Xu",
            "Jingrui He"
        ],
        "citations": 45,
        "references": 55,
        "year": 2019
    },
    {
        "title": "Generative Ensembles for Robust Anomaly Detection",
        "abstract": "Deep generative models are capable of learning probability distributions over large, high-dimensional datasets such as images, video and natural language. Generative models trained on samples from $p(x)$ ought to assign low likelihoods to out-of-distribution (OoD) samples from $q(x)$, making them suitable for anomaly detection applications. We show that in practice, likelihood models are themselves susceptible to OoD errors, and even assign large likelihoods to images from other natural datasets. To mitigate these issues, we propose Generative Ensembles, a model-independent technique for OoD detection that combines density-based anomaly detection with uncertainty estimation. Our method outperforms ODIN and VIB baselines on image datasets, and achieves comparable performance to a classification model on the Kaggle Credit Fraud dataset.",
        "authors": [
            "Hyun-Jae Choi",
            "Eric Jang"
        ],
        "citations": 80,
        "references": 27,
        "year": 2018
    },
    {
        "title": "Towards the Automatic Anime Characters Creation with Generative Adversarial Networks",
        "abstract": "Automatic generation of facial images has been well studied after the Generative Adversarial Network (GAN) came out. There exists some attempts applying the GAN model to the problem of generating facial images of anime characters, but none of the existing work gives a promising result. In this work, we explore the training of GAN models specialized on an anime facial image dataset. We address the issue from both the data and the model aspect, by collecting a more clean, well-suited dataset and leverage proper, empirical application of DRAGAN. With quantitative analysis and case studies we demonstrate that our efforts lead to a stable and high-quality model. Moreover, to assist people with anime character design, we build a website (this http URL) with our pre-trained model available online, which makes the model easily accessible to general public.",
        "authors": [
            "Yanghua Jin",
            "Jiakai Zhang",
            "Minjun Li",
            "Yingtao Tian",
            "Huachun Zhu",
            "Zhihao Fang"
        ],
        "citations": 184,
        "references": 29,
        "year": 2017
    },
    {
        "title": "KERMIT: Generative Insertion-Based Modeling for Sequences",
        "abstract": "We present KERMIT, a simple insertion-based approach to generative modeling for sequences and sequence pairs. KERMIT models the joint distribution and its decompositions (i.e., marginals and conditionals) using a single neural network and, unlike much prior work, does not rely on a prespecified factorization of the data distribution. During training, one can feed KERMIT paired data $(x, y)$ to learn the joint distribution $p(x, y)$, and optionally mix in unpaired data $x$ or $y$ to refine the marginals $p(x)$ or $p(y)$. During inference, we have access to the conditionals $p(x \\mid y)$ and $p(y \\mid x)$ in both directions. We can also sample from the joint distribution or the marginals. The model supports both serial fully autoregressive decoding and parallel partially autoregressive decoding, with the latter exhibiting an empirically logarithmic runtime. We demonstrate through experiments in machine translation, representation learning, and zero-shot cloze question answering that our unified approach is capable of matching or exceeding the performance of dedicated state-of-the-art systems across a wide range of tasks without the need for problem-specific architectural adaptation.",
        "authors": [
            "William Chan",
            "Nikita Kitaev",
            "Kelvin Guu",
            "Mitchell Stern",
            "Jakob Uszkoreit"
        ],
        "citations": 65,
        "references": 28,
        "year": 2019
    },
    {
        "title": "Federated Generative Privacy",
        "abstract": "We propose FedGP, a framework for privacy-preserving data release in the federated learning setting. We use generative adversarial networks, generator components of which are trained by FedAvg algorithm, to draw private artificial data samples and empirically assess the risk of information disclosure. Our experiments show that FedGP is able to generate labeled data of high quality to successfully train and validate supervised models. Finally, we demonstrate that our approach significantly reduces vulnerability of such models to model inversion attacks.",
        "authors": [
            "Aleksei Triastcyn",
            "B. Faltings"
        ],
        "citations": 62,
        "references": 56,
        "year": 2019
    },
    {
        "title": "Shape Inpainting Using 3D Generative Adversarial Network and Recurrent Convolutional Networks",
        "abstract": "Recent advances in convolutional neural networks have shown promising results in 3D shape completion. But due to GPU memory limitations, these methods can only produce low-resolution outputs. To inpaint 3D models with semantic plausibility and contextual details, we introduce a hybrid framework that combines a 3D Encoder-Decoder Generative Adversarial Network (3D-ED-GAN) and a Longterm Recurrent Convolutional Network (LRCN). The 3DED- GAN is a 3D convolutional neural network trained with a generative adversarial paradigm to fill missing 3D data in low-resolution. LRCN adopts a recurrent neural network architecture to minimize GPU memory usage and incorporates an Encoder-Decoder pair into a Long Shortterm Memory Network. By handling the 3D model as a sequence of 2D slices, LRCN transforms a coarse 3D shape into a more complete and higher resolution volume. While 3D-ED-GAN captures global contextual structure of the 3D shape, LRCN localizes the fine-grained details. Experimental results on both real-world and synthetic data show reconstructions from corrupted models result in complete and high-resolution 3D objects.",
        "authors": [
            "Weiyue Wang",
            "Qiangui Huang",
            "Suya You",
            "Chao Yang",
            "U. Neumann"
        ],
        "citations": 163,
        "references": 31,
        "year": 2017
    },
    {
        "title": "Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models",
        "abstract": "The ﬁeld of language modelling has been largely dominated by autoregressive models, for which sampling is inherently difﬁcult to parallelize. This paper introduces two new classes of generative models for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion . Argmax Flows are deﬁned by a composition of a continuous distribution (such as a normalizing ﬂow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our models perform competitively on language modelling and modelling of image segmentation maps.",
        "authors": [
            "Emiel Hoogeboom",
            "Didrik Nielsen",
            "P. Jaini",
            "Patrick Forr'e",
            "M. Welling"
        ],
        "citations": 71,
        "references": 40,
        "year": 2021
    },
    {
        "title": "Augmenting The Size of EEG datasets Using Generative Adversarial Networks",
        "abstract": "Electroencephalography (EEG) is one of the most promising methods in the field of Brain-Computer Interfaces (BCIs) due to its rich time-domain resolution and the availability of advanced and portable sensor technology. One of the major challenges for EEG signal analysis is the small size of its datasets as it is usually demanding for human subjects to perform lengthy experiments. Consequently, this challenge can limit the performance of EEG signal classification models. In this paper, we propose a novel generative adversarial network (GAN) model that can learn the statistical characteristics of the EEG signal and augment its datasets size to enhance the performance of classification models. Results show that the proposed model significantly outperforms other generative models on the utilized EEG dataset. Furthermore, it significantly enhances the performance of classification models working on small size EEG datasets after augmenting them with generated samples.",
        "authors": [
            "Sherif M. Abdelfattah",
            "Ghodai M. Abdelrahman",
            "Min Wang"
        ],
        "citations": 78,
        "references": 23,
        "year": 2018
    },
    {
        "title": "Fast and Accurate Simulation of Particle Detectors Using Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "P. Musella",
            "F. Pandolfi"
        ],
        "citations": 89,
        "references": 45,
        "year": 2018
    },
    {
        "title": "Learning and Inference on Generative Adversarial Quantum Circuits",
        "abstract": "Quantum mechanics is inherently probabilistic in light of Born's rule. Using quantum circuits as probabilistic generative models for classical data exploits their superior expressibility and efficient direct sampling ability. However, training of quantum circuits can be more challenging compared to classical neural networks due to lack of efficient differentiable learning algorithm. We devise an adversarial quantum-classical hybrid training scheme via coupling a quantum circuit generator and a classical neural network discriminator together. After training, the quantum circuit generative model can infer missing data with quadratic speed up via amplitude amplification. We numerically simulate the learning and inference of generative adversarial quantum circuit using the prototypical Bars-and-Stripes dataset. Generative adversarial quantum circuits is a fresh approach to machine learning which may enjoy the practically useful quantum advantage on near-term quantum devices.",
        "authors": [
            "J. Zeng",
            "Y. Wu",
            "Jin-Guo Liu",
            "Lei Wang",
            "Jiangping Hu"
        ],
        "citations": 73,
        "references": 31,
        "year": 2018
    },
    {
        "title": "Chemical language models enable navigation in sparsely populated chemical space",
        "abstract": null,
        "authors": [
            "M. Skinnider",
            "R. Stacey",
            "D. Wishart",
            "L. Foster"
        ],
        "citations": 79,
        "references": 83,
        "year": 2021
    },
    {
        "title": "CosmoGAN: creating high-fidelity weak lensing convergence maps using Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "M. Mustafa",
            "D. Bard",
            "W. Bhimji",
            "Z. Lukic",
            "Rami Al-Rfou",
            "J. Kratochvil"
        ],
        "citations": 118,
        "references": 74,
        "year": 2017
    },
    {
        "title": "Task-Aware Compressed Sensing with Generative Adversarial Networks",
        "abstract": "\n \n In recent years, neural network approaches have been widely adopted for machine learning tasks, with applications in computer vision. More recently, unsupervised generative models based on neural networks have been successfully applied to model data distributions via low-dimensional latent spaces. In this paper, we use Generative Adversarial Networks (GANs) to impose structure in compressed sensing problems, replacing the usual sparsity constraint. We propose to train the GANs in a task-aware fashion, specifically for reconstruction tasks. We also show that it is possible to train our model without using any (or much) non-compressed data. Finally, we show that the latent space of the GAN carries discriminative information and can further be regularized to generate input features for general inference tasks. We demonstrate the effectiveness of our method on a variety of reconstruction and classification problems.\n \n",
        "authors": [
            "Maya Kabkab",
            "Pouya Samangouei",
            "R. Chellappa"
        ],
        "citations": 74,
        "references": 41,
        "year": 2018
    },
    {
        "title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model",
        "abstract": "We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses (\"I don't know\", \"I can't tell\"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it cannot be deployed to have real conversations with users. \nOur work aims to achieve the best of both worlds -- the practical usefulness of G and the strong performance of D -- via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -- specifically, an RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10). The source code can be downloaded from this https URL",
        "authors": [
            "Jiasen Lu",
            "A. Kannan",
            "Jianwei Yang",
            "Devi Parikh",
            "Dhruv Batra"
        ],
        "citations": 135,
        "references": 53,
        "year": 2017
    },
    {
        "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets",
        "abstract": "We study the problem of semi-supervised question answering—utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",
        "authors": [
            "Zhilin Yang",
            "Junjie Hu",
            "R. Salakhutdinov",
            "William W. Cohen"
        ],
        "citations": 151,
        "references": 48,
        "year": 2017
    },
    {
        "title": "Monge-Ampère Flow for Generative Modeling",
        "abstract": "We present a deep generative model, named Monge-Ampere flow, which builds on continuous-time gradient flow arising from the Monge-Ampere equation in optimal transport theory. The generative map from the latent space to the data space follows a dynamical system, where a learnable potential function guides a compressible fluid to flow towards the target density distribution. Training of the model amounts to solving an optimal control problem. The Monge-Ampere flow has tractable likelihoods and supports efficient sampling and inference. One can easily impose symmetry constraints in the generative model by designing suitable scalar potential functions. We apply the approach to unsupervised density estimation of the MNIST dataset and variational calculation of the two-dimensional Ising model at the critical point. This approach brings insights and techniques from Monge-Ampere equation, optimal transport, and fluid dynamics into reversible flow-based generative models.",
        "authors": [
            "Linfeng Zhang",
            "E. Weinan",
            "Lei Wang"
        ],
        "citations": 59,
        "references": 60,
        "year": 2018
    },
    {
        "title": "Autoregressive Quantile Networks for Generative Modeling",
        "abstract": "We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.",
        "authors": [
            "Georg Ostrovski",
            "Will Dabney",
            "R. Munos"
        ],
        "citations": 84,
        "references": 46,
        "year": 2018
    },
    {
        "title": "Neural 3D Morphable Models: Spiral Convolutional Networks for 3D Shape Representation Learning and Generation",
        "abstract": "Generative models for 3D geometric data arise in many important applications in 3D computer vision and graphics. In this paper, we focus on 3D deformable shapes that share a common topological structure, such as human faces and bodies. Morphable Models and their variants, despite their linear formulation, have been widely used for shape representation, while most of the recently proposed nonlinear approaches resort to intermediate representations, such as 3D voxel grids or 2D views. In this work, we introduce a novel graph convolutional operator, acting directly on the 3D mesh, that explicitly models the inductive bias of the fixed underlying graph. This is achieved by enforcing consistent local orderings of the vertices of the graph, through the spiral operator, thus breaking the permutation invariance property that is adopted by all the prior work on Graph Neural Networks. Our operator comes by construction with desirable properties (anisotropic, topology-aware, lightweight, easy-to-optimise), and by using it as a building block for traditional deep generative architectures, we demonstrate state-of-the-art results on a variety of 3D shape datasets compared to the linear Morphable Model and other graph convolutional operators.",
        "authors": [
            "Giorgos Bouritsas",
            "Sergiy Bokhnyak",
            "Stylianos Ploumpis",
            "M. Bronstein",
            "S. Zafeiriou"
        ],
        "citations": 156,
        "references": 53,
        "year": 2019
    },
    {
        "title": "Mixture Models",
        "abstract": null,
        "authors": [
            "Kevin P. Murphy"
        ],
        "citations": 231,
        "references": 19,
        "year": 2019
    },
    {
        "title": "Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk",
        "abstract": "We examine the theoretical properties of enforcing priors provided by generative deep neural networks via empirical risk minimization. In particular we consider two models, one in which the task is to invert a generative neural network given access to its last layer and another in which the task is to invert a generative neural network given only compressive linear observations of its last layer. We establish that in both cases, in suitable regimes of network layer sizes and a randomness assumption on the network weights, that the non-convex objective function given by empirical risk minimization does not have any spurious stationary points. That is, we establish that with high probability, at any point away from small neighborhoods around two scalar multiples of the desired solution, there is a descent direction. Hence, there are no local minima, saddle points, or other stationary points outside these neighborhoods. These results constitute the first theoretical guarantees which establish the favorable global geometry of these non-convex optimization problems, and they bridge the gap between the empirical success of enforcing deep generative priors and a rigorous understanding of non-linear inverse problems.",
        "authors": [
            "Paul Hand",
            "V. Voroninski"
        ],
        "citations": 137,
        "references": 80,
        "year": 2017
    },
    {
        "title": "TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial Network",
        "abstract": "In this work, we present the Text Conditioned Auxiliary Classifier Generative Adversarial Network, (TAC-GAN) a text to image Generative Adversarial Network (GAN) for synthesizing images from their text descriptions. Former approaches have tried to condition the generative process on the textual data; but allying it to the usage of class information, known to diversify the generated samples and improve their structural coherence, has not been explored. We trained the presented TAC-GAN model on the Oxford-102 dataset of flowers, and evaluated the discriminability of the generated images with Inception-Score, as well as their diversity using the Multi-Scale Structural Similarity Index (MS-SSIM). Our approach outperforms the state-of-the-art models, i.e., its inception score is 3.45, corresponding to a relative increase of 7.8% compared to the recently introduced StackGan. A comparison of the mean MS-SSIM scores of the training and generated samples per class shows that our approach is able to generate highly diverse images with an average MS-SSIM of 0.14 over all generated classes.",
        "authors": [
            "Ayushman Dash",
            "J. Gamboa",
            "Sheraz Ahmed",
            "M. Liwicki",
            "Muhammad Zeshan Afzal"
        ],
        "citations": 139,
        "references": 28,
        "year": 2017
    },
    {
        "title": "Structure-aware Generative Network for 3D-Shape Modeling",
        "abstract": "We present SAGNet, a structure-aware generative model for 3D shapes. Given a set of segmented objects of a certain class, the geometry of their parts and the pairwise relationships between them (the structure) are jointly learned and embedded in a latent space by an autoencoder. The encoder intertwines the geometry and structure features into a single latent code, while the decoder disentangles the features and reconstructs the geometry and structure of the 3D model. Our autoencoder consists of two branches, one for the structure and one for the geometry. The key idea is that during the analysis, the two branches exchange information between them, thereby learning the dependencies between structure and geometry and encoding two augmented features, which are then fused into a single latent code. This explicit intertwining of information enables separately controlling the geometry and the structure of the generated models. We evaluate the performance of our method and conduct an ablation study. We explicitly show that encoding of shapes accounts for both similarities in structure and geometry. A variety of quality results generated by SAGNet are presented.",
        "authors": [
            "Zhijie Wu",
            "Xiang Wang",
            "Di Lin",
            "D. Lischinski",
            "D. Cohen-Or",
            "Hui Huang"
        ],
        "citations": 66,
        "references": 49,
        "year": 2018
    },
    {
        "title": "Text2Action: Generative Adversarial Synthesis from Language to Action",
        "abstract": "In this paper, we propose a generative model which learns the relationship between language and human action in order to generate a human action sequence given a sentence describing human behavior. The proposed generative model is a generative adversarial network (GAN), which is based on the sequence to sequence (SEQ2SEQ) model. Using the proposed generative network, we can synthesize various actions for a robot or a virtual agent using a text encoder recurrent neural network (RNN) and an action decoder RNN. The proposed generative network is trained from 29,770 pairs of actions and sentence annotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video dataset. We demonstrate that the network can generate human-like actions which can be transferred to a Baxter robot, such that the robot performs an action based on a provided sentence. Results show that the proposed generative network correctly models the relationship between language and action and can generate a diverse set of actions from the same sentence.",
        "authors": [
            "Hyemin Ahn",
            "Timothy Ha",
            "Yunho Choi",
            "Hwiyeon Yoo",
            "Songhwai Oh"
        ],
        "citations": 124,
        "references": 20,
        "year": 2017
    },
    {
        "title": "Comparative Study on Generative Adversarial Networks",
        "abstract": "In recent years, there have been tremendous advancements in the field of machine learning. These advancements have been made through both academic as well as industrial research. Lately, a fair amount of research has been dedicated to the usage of generative models in the field of computer vision and image classification. These generative models have been popularized through a new framework called Generative Adversarial Networks. Moreover, many modified versions of this framework have been proposed in the last two years. We study the original model proposed by Goodfellow et al. as well as modifications over the original model and provide a comparative analysis of these models.",
        "authors": [
            "Saifuddin Hitawala"
        ],
        "citations": 55,
        "references": 12,
        "year": 2018
    },
    {
        "title": "Deep Generative Video Compression",
        "abstract": "The usage of deep generative models for image compression has led to impressive performance gains over classical codecs while neural video compression is still in its infancy. Here, we propose an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video. Our approach builds upon variational autoencoder (VAE) models for sequential data and combines them with recent work on neural image compression. The approach jointly learns to transform the original sequence into a lower-dimensional representation as well as to discretize and entropy code this representation according to predictions of the sequential VAE. Rate-distortion evaluations on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved when training the model on specialized content.",
        "authors": [
            "Salvator Lombardo",
            "Jun Han",
            "Christopher Schroers",
            "S. Mandt"
        ],
        "citations": 56,
        "references": 52,
        "year": 2018
    },
    {
        "title": "An Online Learning Approach to Generative Adversarial Networks",
        "abstract": "We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1 . On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real world tasks our approach exhibits improved stability and performance compared to standard GAN training.",
        "authors": [
            "Paulina Grnarova",
            "K. Levy",
            "Aurélien Lucchi",
            "Thomas Hofmann",
            "Andreas Krause"
        ],
        "citations": 87,
        "references": 40,
        "year": 2017
    },
    {
        "title": "Extending Neural Generative Conversational Model using External Knowledge Sources",
        "abstract": "The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.",
        "authors": [
            "Prasanna Parthasarathi",
            "Joelle Pineau"
        ],
        "citations": 68,
        "references": 26,
        "year": 2018
    },
    {
        "title": "Modular Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "Bo Zhao",
            "B. Chang",
            "Zequn Jie",
            "L. Sigal"
        ],
        "citations": 77,
        "references": 33,
        "year": 2018
    },
    {
        "title": "Synthetic to Real Adaptation with Generative Correlation Alignment Networks",
        "abstract": "Synthetic images rendered from 3D CAD models are useful for augmenting training data for object recognition algorithms. However, the generated images are nonphotorealistic and do not match real image statistics. This leads to a large domain discrepancy, causing models trained on synthetic data to perform poorly on real domains. Recent work has shown the great potential of deep convolutional neural networks to generate realistic images, but has not utilized generative models to address synthetictoreal domain adaptation. In this work, we propose a Deep Generative Correlation Alignment Network (DGCAN) to synthesize images using a novel domain adaption algorithm. DGCAN leverages a shape preserving loss and a low level statistic matching loss to minimize the domain discrepancy between synthetic and real images in deep feature space. Experimentally, we show training off-the-shelf classifiers on the newly generated data can significantly boost performance when testing on the real image domains (PASCAL VOC 2007 benchmark and Office dataset), improving upon several existing methods.",
        "authors": [
            "Xingchao Peng",
            "Kate Saenko"
        ],
        "citations": 91,
        "references": 50,
        "year": 2017
    },
    {
        "title": "A Generative Model for Volume Rendering",
        "abstract": "We present a technique to synthesize and analyze volume-rendered images using generative models. We use the Generative Adversarial Network (GAN) framework to compute a model from a large collection of volume renderings, conditioned on (1) viewpoint and (2) transfer functions for opacity and color. Our approach facilitates tasks for volume analysis that are challenging to achieve using existing rendering techniques such as ray casting or texture-based methods. We show how to guide the user in transfer function editing by quantifying expected change in the output image. Additionally, the generative model transforms transfer functions into a view-invariant latent space specifically designed to synthesize volume-rendered images. We use this space directly for rendering, enabling the user to explore the space of volume-rendered images. As our model is independent of the choice of volume rendering process, we show how to analyze volume-rendered images produced by direct and global illumination lighting, for a variety of volume datasets.",
        "authors": [
            "M. Berger",
            "Jixian Li",
            "J. Levine"
        ],
        "citations": 76,
        "references": 61,
        "year": 2017
    },
    {
        "title": "SMPLpix: Neural Avatars from 3D Human Models",
        "abstract": "Recent advances in deep generative models have led to an unprecedented level of realism for synthetically generated images of humans. However, one of the remaining fundamental limitations of these models is the ability to flexibly control the generative process, e.g. change the camera and human pose while retaining the subject identity. At the same time, deformable human body models like SMPL [34] and its successors provide full control over pose and shape, but rely on classic computer graphics pipelines for rendering. Such rendering pipelines require explicit mesh rasterization that (a) does not have the potential to fix artifacts or lack of realism in the original 3D geometry and (b) until recently, were not fully incorporated into deep learning frameworks. In this work, we propose to bridge the gap between classic geometry-based rendering and the latest generative networks operating in pixel space. We train a network that directly converts a sparse set of 3D mesh vertices into photorealistic images, alleviating the need for traditional rasterization mechanism. We train our model on a large corpus of human 3D models and corresponding real photos, and show the advantage over conventional differentiable renderers both in terms of the level of photorealism and rendering efficiency.",
        "authors": [
            "S. Prokudin",
            "Michael J. Black",
            "J. Romero"
        ],
        "citations": 76,
        "references": 54,
        "year": 2020
    },
    {
        "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models",
        "abstract": "Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work \\citep{jang2016categorical, maddison2016concrete} has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \\emph{unbiased} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.",
        "authors": [
            "G. Tucker",
            "A. Mnih",
            "Chris J. Maddison",
            "John Lawson",
            "Jascha Narain Sohl-Dickstein"
        ],
        "citations": 278,
        "references": 30,
        "year": 2017
    },
    {
        "title": "Generative Modeling of Multimodal Multi-Human Behavior",
        "abstract": "This work presents a methodology for modeling and predicting human behavior in settings with $N$ humans interacting in highly multimodal scenarios (i.e. where there are many possible highly-distinct futures). A motivating example includes robots interacting with humans in crowded environments, such as self-driving cars operating alongside human-driven vehicles or human-robot collaborative bin packing in a warehouse. Our approach to model human behavior in such uncertain environments is to model humans in the scene as nodes in a graphical model, with edges encoding relationships between them. For each human, we learn a multimodal probability distribution over future actions from a dataset of multi-human interactions. Learning such distributions is made possible by recent advances in the theory of conditional variational autoencoders and deep learning approximations of probabilistic graphical models. Specifically, we learn action distributions conditioned on interaction history, neighboring human behavior, and candidate future agent behavior in order to take into account response dynamics. We demonstrate the performance of such a modeling approach in modeling basketball player trajectories, a highly multimodal, multi-human scenario which serves as a proxy for many robotic applications.",
        "authors": [
            "B. Ivanovic",
            "E. Schmerling",
            "Karen Leung",
            "M. Pavone"
        ],
        "citations": 68,
        "references": 33,
        "year": 2018
    },
    {
        "title": "Medical Image Synthesis with Generative Adversarial Networks for Tissue Recognition",
        "abstract": "This paper presents an adversarial learning-based approach to synthesize medical images for medical image tissue recognition. The performance of medical image recognition models highly depends on the representativeness and sufficiency of training samples. The high expense of collecting large amounts of practical medical images leads to a demand of synthesizing image samples. In this research, generative adversarial networks (GANs), which consist of a generative network and a discriminative network, are applied to develop a medical image synthesis model. Specifically, deep convolutional GANs (DCGANs), Wasserstein GANs (WGANs), and boundary equilibrium GANs (BEGANs) are implemented and compared to synthesize medical images in this research. Convolutional neural networks (CNNs) are applied in the GAN models, which can capture feature representations that describe a high level of image semantic information. Then synthetic images are generated by employing the generative network mapping from random noise. The effectiveness of the generative network is validated by a discriminative network, which is trained to detect the synthetic images from real images. Through a minimax two-player game, the generative and discriminative networks can train each other. The generated synthetic images are used to train a CNN classification model for tissue recognition. Through the experiments with the synthetic images, the tissue recognition accuracy achieves 98.83%, which reveals the effectiveness and applicability of synthesizing medical images through the GAN models.",
        "authors": [
            "Qianqian Zhang",
            "Haifeng Wang",
            "Hongya Lu",
            "Daehan Won",
            "S. Yoon"
        ],
        "citations": 49,
        "references": 20,
        "year": 2018
    },
    {
        "title": "Entropy and mutual information in models of deep neural networks",
        "abstract": "We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) we show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.",
        "authors": [
            "Marylou Gabrié",
            "Andre Manoel",
            "Clément Luneau",
            "Jean Barbier",
            "N. Macris",
            "Florent Krzakala",
            "L. Zdeborová"
        ],
        "citations": 170,
        "references": 80,
        "year": 2018
    },
    {
        "title": "Geometrical Insights for Implicit Generative Modeling",
        "abstract": null,
        "authors": [
            "L. Bottou",
            "Martín Arjovsky",
            "David Lopez-Paz",
            "M. Oquab"
        ],
        "citations": 49,
        "references": 61,
        "year": 2017
    },
    {
        "title": "Inferring Generative Model Structure with Static Analysis",
        "abstract": "Obtaining enough labeled data to robustly train complex discriminative models is a major bottleneck in the machine learning pipeline. A popular solution is combining multiple sources of weak supervision using generative models. The structure of these models affects training label quality, but is difficult to learn without any ground truth labels. We instead rely on these weak supervision sources having some structure by virtue of being encoded programmatically. We present Coral, a paradigm that infers generative model structure by statically analyzing the code for these heuristics, thus reducing the data required to learn structure significantly. We prove that Coral's sample complexity scales quasilinearly with the number of heuristics and number of relations found, improving over the standard sample complexity, which is exponential in n for identifying nth degree relations. Experimentally, Coral matches or outperforms traditional structure learning approaches by up to 3.81 F1 points. Using Coral to model dependencies instead of assuming independence results in better performance than a fully supervised model by 3.07 accuracy points when heuristics are used to label radiology data without ground truth labels.",
        "authors": [
            "P. Varma",
            "Bryan D. He",
            "Payal Bajaj",
            "Nishith Khandwala",
            "I. Banerjee",
            "D. Rubin",
            "Christopher Ré"
        ],
        "citations": 57,
        "references": 51,
        "year": 2017
    },
    {
        "title": "Interactive 3D Modeling with a Generative Adversarial Network",
        "abstract": "We propose the idea of using a generative adversarial network (GAN) to assist users in designing real-world shapes with a simple interface. Users edit a voxel grid with a Minecraft-like interface. Yet they can execute a SNAP command at any time, which transforms their rough model into a desired shape that is both similar and realistic. They can edit and snap until they are satisfied with the result. The advantage of this approach is to assist novice users to create 3D models characteristic of the training data by only specifying rough edits. Our key contribution is to create a suitable projection operator around a 3D-GAN that maps an arbitrary 3D voxel input to a latent vector in the shape manifold of the generator that is both similar in shape to the input but also realistic. Experiments show our method is promising for computer-assisted interactive modeling.",
        "authors": [
            "Jerry Liu",
            "F. Yu",
            "T. Funkhouser"
        ],
        "citations": 86,
        "references": 38,
        "year": 2017
    },
    {
        "title": "Interactive 3D Modeling with a Generative Adversarial Network",
        "abstract": "We propose the idea of using a generative adversarial network (GAN) to assist users in designing real-world shapes with a simple interface. Users edit a voxel grid with a Minecraft-like interface. Yet they can execute a SNAP command at any time, which transforms their rough model into a desired shape that is both similar and realistic. They can edit and snap until they are satisfied with the result. The advantage of this approach is to assist novice users to create 3D models characteristic of the training data by only specifying rough edits. Our key contribution is to create a suitable projection operator around a 3D-GAN that maps an arbitrary 3D voxel input to a latent vector in the shape manifold of the generator that is both similar in shape to the input but also realistic. Experiments show our method is promising for computer-assisted interactive modeling.",
        "authors": [
            "Jerry Liu",
            "F. Yu",
            "T. Funkhouser"
        ],
        "citations": 86,
        "references": 38,
        "year": 2017
    },
    {
        "title": "Cross-domain Generative Learning for Fine-Grained Sketch-Based Image Retrieval",
        "abstract": "The key challenge for learning a ﬁne-grained sketch-based image retrieval (FG-SBIR) model is to bridge the domain gap between photo and sketch. Existing models learn a deep joint embedding space with discriminative losses where a photo and a sketch can be compared. In this paper, we propose a novel discriminative-generative hybrid model by introducing a generative task of cross-domain image synthesis. This task enforces the learned embedding space to preserve all the domain invariant information that is useful for cross-domain reconstruction, thus explicitly reducing the domain gap as opposed to existing models. Extensive experiments on the largest FG-SBIR dataset Sketchy [19] show that the proposed model signiﬁcantly outperforms state-of-the-art discriminative FG-SBIR models.",
        "authors": [
            "Kaiyue Pang",
            "Yi-Zhe Song",
            "Tony Xiang",
            "Timothy M. Hospedales"
        ],
        "citations": 69,
        "references": 32,
        "year": 2017
    },
    {
        "title": "Causal Generative Neural Networks",
        "abstract": "We introduce CGNN, a framework to learn functional causal models as generative neural networks. These networks are trained using backpropagation to minimize the maximum mean discrepancy to the observed data. Unlike previous approaches, CGNN leverages both conditional independences and distributional asymmetries to seamlessly discover bivariate and multivariate causal structures, with or without hidden variables. CGNN does not only estimate the causal structure, but a full and differentiable generative model of the data. Throughout an extensive variety of experiments, we illustrate the competitive esults of CGNN w.r.t state-of-the-art alternatives in observational causal discovery on both simulated and real data, in the tasks of cause-effect inference, v-structure identification, and multivariate causal discovery.",
        "authors": [
            "Olivier Goudet",
            "Diviyan Kalainathan",
            "Philippe Caillou",
            "Isabelle M Guyon",
            "David Lopez-Paz",
            "M. Sebag"
        ],
        "citations": 58,
        "references": 50,
        "year": 2017
    },
    {
        "title": "A Bayesian Data Augmentation Approach for Learning Deep Models",
        "abstract": "Data augmentation is an essential part of the training process applied to deep learning models. The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed. Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm --- generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned above --- the results also show that our approach produces better classification results than similar GAN models.",
        "authors": [
            "Toan Tran",
            "Trung T. Pham",
            "G. Carneiro",
            "L. Palmer",
            "I. Reid"
        ],
        "citations": 223,
        "references": 32,
        "year": 2017
    },
    {
        "title": "Text Generation Based on Generative Adversarial Nets with Latent Variable",
        "abstract": null,
        "authors": [
            "Heng Wang",
            "Zengchang Qin",
            "T. Wan"
        ],
        "citations": 39,
        "references": 28,
        "year": 2017
    },
    {
        "title": "An Information-Theoretic Analysis of Deep Latent-Variable Models",
        "abstract": "We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family.",
        "authors": [
            "Alexander A. Alemi",
            "Ben Poole",
            "Ian S. Fischer",
            "Joshua V. Dillon",
            "R. Saurous",
            "K. Murphy"
        ],
        "citations": 79,
        "references": 63,
        "year": 2017
    },
    {
        "title": "Designing Random Graph Models Using Variational Autoencoders With Applications to Chemical Design",
        "abstract": "Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with graphs due to their unique characteristics--their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we propose a variational autoencoder for graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, the decoder is able to guarantee a set of local structural and functional properties in the generated graphs. Experiments reveal that our model is able to learn and mimic the generative process of several well-known random graph models and can be used to create new molecules more effectively than several state of the art methods.",
        "authors": [
            "Bidisha Samanta",
            "A. De",
            "Niloy Ganguly",
            "M. Gomez-Rodriguez"
        ],
        "citations": 52,
        "references": 54,
        "year": 2018
    },
    {
        "title": "A Compass for Navigating Sharing Economy Business Models",
        "abstract": "The sharing economy has emerged in recent years as a disruptive approach to traditional business models. Drawing on a multi-year research program and a design-based methodology, this article introduces a framework and generative tool called the Sharing Business Model Compass. As an actionable framework, the Compass helps elucidate the multiple, innovative forms sharing economy businesses are adopting. As a generative tool, it enables entrepreneurs, investors, incubators, and incumbents interested in entering the sharing economy to create, present, and evolve a compelling sharing business model as well as evaluate its extent of robustness.",
        "authors": [
            "P. Muñoz",
            "Boyd Cohen"
        ],
        "citations": 78,
        "references": 38,
        "year": 2018
    },
    {
        "title": "Improved Training of Wasserstein GANs",
        "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
        "authors": [
            "Ishaan Gulrajani",
            "Faruk Ahmed",
            "Martín Arjovsky",
            "Vincent Dumoulin",
            "Aaron C. Courville"
        ],
        "citations": 1000,
        "references": 37,
        "year": 2017
    },
    {
        "title": "Image Segmentation Using Deep Learning: A Survey",
        "abstract": "Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of deep learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.",
        "authors": [
            "Shervin Minaee",
            "Yuri Boykov",
            "F. Porikli",
            "A. Plaza",
            "N. Kehtarnavaz",
            "Demetri Terzopoulos"
        ],
        "citations": 1000,
        "references": 205,
        "year": 2020
    },
    {
        "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Frechet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
        "authors": [
            "M. Heusel",
            "Hubert Ramsauer",
            "Thomas Unterthiner",
            "Bernhard Nessler",
            "Sepp Hochreiter"
        ],
        "citations": 1000,
        "references": 61,
        "year": 2017
    },
    {
        "title": "Deep One-Class Classification",
        "abstract": "Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objec-tive. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GT-SRB stop signs.",
        "authors": [
            "Lukas Ruff",
            "Nico Görnitz",
            "Lucas Deecke",
            "Shoaib Ahmed Siddiqui",
            "Robert A. Vandermeulen",
            "Alexander Binder",
            "Emmanuel Müller",
            "M. Kloft"
        ],
        "citations": 1000,
        "references": 58,
        "year": 2018
    },
    {
        "title": "Joint Autoregressive and Hierarchical Priors for Learned Image Compression",
        "abstract": "Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate–distortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.",
        "authors": [
            "David C. Minnen",
            "J. Ballé",
            "G. Toderici"
        ],
        "citations": 1000,
        "references": 44,
        "year": 2018
    },
    {
        "title": "Analyzing and Improving the Image Quality of StyleGAN",
        "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
        "authors": [
            "Tero Karras",
            "S. Laine",
            "M. Aittala",
            "Janne Hellsten",
            "J. Lehtinen",
            "Timo Aila"
        ],
        "citations": 1000,
        "references": 53,
        "year": 2019
    },
    {
        "title": "Deep Anomaly Detection with Outlier Exposure",
        "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.",
        "authors": [
            "Dan Hendrycks",
            "Mantas Mazeika",
            "Thomas G. Dietterich"
        ],
        "citations": 1000,
        "references": 54,
        "year": 2018
    },
    {
        "title": "Neural Ordinary Differential Equations",
        "abstract": "We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",
        "authors": [
            "T. Chen",
            "Yulia Rubanova",
            "J. Bettencourt",
            "D. Duvenaud"
        ],
        "citations": 1000,
        "references": 64,
        "year": 2018
    },
    {
        "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
        "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
        "authors": [
            "Andrew Brock",
            "Jeff Donahue",
            "K. Simonyan"
        ],
        "citations": 1000,
        "references": 60,
        "year": 2018
    },
    {
        "title": "Mutual Information Neural Estimation",
        "abstract": "We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.",
        "authors": [
            "Mohamed Ishmael Belghazi",
            "A. Baratin",
            "Sai Rajeswar",
            "Sherjil Ozair",
            "Yoshua Bengio",
            "R. Devon Hjelm",
            "Aaron C. Courville"
        ],
        "citations": 1000,
        "references": 61,
        "year": 2018
    },
    {
        "title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications",
        "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at this https URL Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.",
        "authors": [
            "Tim Salimans",
            "A. Karpathy",
            "Xi Chen",
            "Diederik P. Kingma"
        ],
        "citations": 894,
        "references": 22,
        "year": 2017
    },
    {
        "title": "A survey on Image Data Augmentation for Deep Learning",
        "abstract": null,
        "authors": [
            "Connor Shorten",
            "T. Khoshgoftaar"
        ],
        "citations": 1000,
        "references": 142,
        "year": 2019
    },
    {
        "title": "Interpreting the Latent Space of GANs for Semantic Face Editing",
        "abstract": "Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.",
        "authors": [
            "Yujun Shen",
            "Jinjin Gu",
            "Xiaoou Tang",
            "Bolei Zhou"
        ],
        "citations": 1000,
        "references": 47,
        "year": 2019
    },
    {
        "title": "Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations",
        "abstract": "Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.",
        "authors": [
            "V. Sitzmann",
            "Michael Zollhoefer",
            "Gordon Wetzstein"
        ],
        "citations": 1000,
        "references": 79,
        "year": 2019
    },
    {
        "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction",
        "abstract": "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
        "authors": [
            "Antoine Bosselut",
            "Hannah Rashkin",
            "Maarten Sap",
            "Chaitanya Malaviya",
            "Asli Celikyilmaz",
            "Yejin Choi"
        ],
        "citations": 863,
        "references": 38,
        "year": 2019
    },
    {
        "title": "Image Transformer",
        "abstract": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. We propose another extension of self-attention allowing it to efficiently take advantage of the two-dimensional nature of images. While conceptually simple, our generative models trained on two image data sets are competitive with or significantly outperform the current state of the art in autoregressive image generation on two different data sets, CIFAR-10 and ImageNet. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published autoregressive super-resolution models. Images they generate fool human observers three times more often than the previous state of the art.",
        "authors": [
            "Niki Parmar",
            "Ashish Vaswani",
            "Jakob Uszkoreit",
            "Lukasz Kaiser",
            "Noam M. Shazeer",
            "Alexander Ku",
            "Dustin Tran"
        ],
        "citations": 1000,
        "references": 27,
        "year": 2018
    },
    {
        "title": "Normalizing Flows: An Introduction and Review of Current Methods",
        "abstract": "Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.",
        "authors": [
            "I. Kobyzev",
            "S. Prince",
            "Marcus A. Brubaker"
        ],
        "citations": 1000,
        "references": 120,
        "year": 2020
    },
    {
        "title": "Are GANs Created Equal? A Large-Scale Study",
        "abstract": "Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \\cite{goodfellow2014generative}.",
        "authors": [
            "Mario Lucic",
            "Karol Kurach",
            "Marcin Michalski",
            "S. Gelly",
            "O. Bousquet"
        ],
        "citations": 976,
        "references": 28,
        "year": 2017
    },
    {
        "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation",
        "abstract": "Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.",
        "authors": [
            "Judy Hoffman",
            "Eric Tzeng",
            "Taesung Park",
            "Jun-Yan Zhu",
            "Phillip Isola",
            "Kate Saenko",
            "Alexei A. Efros",
            "Trevor Darrell"
        ],
        "citations": 1000,
        "references": 53,
        "year": 2017
    },
    {
        "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.",
        "authors": [
            "Yao Lu",
            "Max Bartolo",
            "Alastair Moore",
            "Sebastian Riedel",
            "Pontus Stenetorp"
        ],
        "citations": 994,
        "references": 32,
        "year": 2021
    },
    {
        "title": "Cross-lingual Language Model Pretraining",
        "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",
        "authors": [
            "Guillaume Lample",
            "Alexis Conneau"
        ],
        "citations": 1000,
        "references": 52,
        "year": 2019
    },
    {
        "title": "Generating Focused Molecule Libraries for Drug Discovery with Recurrent Neural Networks",
        "abstract": "In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active toward a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target. Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria), it reproduced 28% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.",
        "authors": [
            "Marwin H. S. Segler",
            "T. Kogej",
            "C. Tyrchan",
            "M. Waller"
        ],
        "citations": 1000,
        "references": 76,
        "year": 2017
    },
    {
        "title": "GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders",
        "abstract": null,
        "authors": [
            "M. Simonovsky",
            "N. Komodakis"
        ],
        "citations": 792,
        "references": 42,
        "year": 2018
    },
    {
        "title": "Grammar Variational Autoencoder",
        "abstract": "Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.",
        "authors": [
            "Matt J. Kusner",
            "Brooks Paige",
            "José Miguel Hernández-Lobato"
        ],
        "citations": 788,
        "references": 46,
        "year": 2017
    },
    {
        "title": "MINE: Mutual Information Neural Estimation",
        "abstract": "This paper presents a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size. MINE is back-propable and we prove that it is strongly consistent. We illustrate a handful of applications in which MINE is succesfully applied to enhance the property of generative models in both unsupervised and supervised settings. We apply our framework to estimate the information bottleneck, and apply it in tasks related to supervised classification problems. Our results demonstrate substantial added flexibility and improvement in these settings.",
        "authors": [
            "Ishmael Belghazi",
            "Sai Rajeswar",
            "A. Baratin",
            "R. Devon Hjelm",
            "Aaron C. Courville"
        ],
        "citations": 603,
        "references": 64,
        "year": 2018
    },
    {
        "title": "Variational Continual Learning",
        "abstract": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "authors": [
            "Cuong V Nguyen",
            "Yingzhen Li",
            "T. Bui",
            "Richard E. Turner"
        ],
        "citations": 696,
        "references": 45,
        "year": 2017
    },
    {
        "title": "Variational Autoencoders for Collaborative Filtering",
        "abstract": "We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
        "authors": [
            "Dawen Liang",
            "R. G. Krishnan",
            "M. Hoffman",
            "Tony Jebara"
        ],
        "citations": 1000,
        "references": 54,
        "year": 2018
    },
    {
        "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",
        "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.",
        "authors": [
            "Jaehyeon Kim",
            "Jungil Kong",
            "Juhee Son"
        ],
        "citations": 748,
        "references": 45,
        "year": 2021
    },
    {
        "title": "Generating Diverse High-Fidelity Images with VQ-VAE-2",
        "abstract": "We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.",
        "authors": [
            "Ali Razavi",
            "Aäron van den Oord",
            "O. Vinyals"
        ],
        "citations": 1000,
        "references": 40,
        "year": 2019
    },
    {
        "title": "VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning",
        "abstract": "Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.",
        "authors": [
            "Akash Srivastava",
            "Lazar Valkov",
            "Chris Russell",
            "Michael U Gutmann",
            "Charles Sutton"
        ],
        "citations": 651,
        "references": 23,
        "year": 2017
    },
    {
        "title": "Adversarial Learning for Neural Dialogue Generation",
        "abstract": "We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines",
        "authors": [
            "Jiwei Li",
            "Will Monroe",
            "Tianlin Shi",
            "Sébastien Jean",
            "Alan Ritter",
            "Dan Jurafsky"
        ],
        "citations": 888,
        "references": 52,
        "year": 2017
    },
    {
        "title": "Modeling Tabular data using Conditional GAN",
        "abstract": "Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design TGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. TGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.",
        "authors": [
            "Lei Xu",
            "Maria Skoularidou",
            "Alfredo Cuesta-Infante",
            "K. Veeramachaneni"
        ],
        "citations": 1000,
        "references": 32,
        "year": 2019
    },
    {
        "title": "Likelihood Ratios for Out-of-Distribution Detection",
        "abstract": "Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.",
        "authors": [
            "Jie Jessie Ren",
            "Peter J. Liu",
            "Emily Fertig",
            "Jasper Snoek",
            "R. Poplin",
            "M. DePristo",
            "Joshua V. Dillon",
            "Balaji Lakshminarayanan"
        ],
        "citations": 670,
        "references": 64,
        "year": 2019
    },
    {
        "title": "VAE with a VampPrior",
        "abstract": "Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call \"Variational Mixture of Posteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.",
        "authors": [
            "J. Tomczak",
            "M. Welling"
        ],
        "citations": 602,
        "references": 50,
        "year": 2017
    },
    {
        "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks",
        "abstract": "This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \nIn experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.",
        "authors": [
            "Nicholas Carlini",
            "Chang Liu",
            "Ú. Erlingsson",
            "Jernej Kos",
            "D. Song"
        ],
        "citations": 1000,
        "references": 72,
        "year": 2018
    },
    {
        "title": "NVAE: A Deep Hierarchical Variational Autoencoder",
        "abstract": "Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\\times$256 pixels. The source code is available at this https URL .",
        "authors": [
            "Arash Vahdat",
            "J. Kautz"
        ],
        "citations": 827,
        "references": 82,
        "year": 2020
    },
    {
        "title": "Invertible Residual Networks",
        "abstract": "We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.",
        "authors": [
            "Jens Behrmann",
            "D. Duvenaud",
            "J. Jacobsen"
        ],
        "citations": 596,
        "references": 55,
        "year": 2018
    },
    {
        "title": "VITON: An Image-Based Virtual Try-on Network",
        "abstract": "We present an image-based VIirtual Try-On Network (VITON) without using 3D information in any form, which seamlessly transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. Conditioned upon a new clothing-agnostic yet descriptive person representation, our framework first generates a coarse synthesized image with the target clothing item overlaid on that same person in the same pose. We further enhance the initial blurry clothing area with a refinement network. The network is trained to learn how much detail to utilize from the target clothing item, and where to apply to the person in order to synthesize a photo-realistic image in which the target item deforms naturally with clear visual patterns. Experiments on our newly collected Zalando dataset demonstrate its promise in the image-based virtual try-on task over state-of-the-art generative models.1",
        "authors": [
            "Xintong Han",
            "Zuxuan Wu",
            "Zhe Wu",
            "Ruichi Yu",
            "L. Davis"
        ],
        "citations": 529,
        "references": 53,
        "year": 2017
    },
    {
        "title": "Pay Less Attention with Lightweight and Dynamic Convolutions",
        "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.",
        "authors": [
            "Felix Wu",
            "Angela Fan",
            "Alexei Baevski",
            "Yann Dauphin",
            "Michael Auli"
        ],
        "citations": 588,
        "references": 63,
        "year": 2019
    },
    {
        "title": "Deep reinforcement learning for de novo drug design",
        "abstract": "We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties. We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks—generative and predictive—that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo–generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.",
        "authors": [
            "Mariya Popova",
            "O. Isayev",
            "Alexander Tropsha"
        ],
        "citations": 950,
        "references": 83,
        "year": 2017
    },
    {
        "title": "Fast Underwater Image Enhancement for Improved Visual Perception",
        "abstract": "In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.",
        "authors": [
            "M. Islam",
            "Youya Xia",
            "Junaed Sattar"
        ],
        "citations": 751,
        "references": 56,
        "year": 2019
    },
    {
        "title": "CogView: Mastering Text-to-Image Generation via Transformers",
        "abstract": "Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",
        "authors": [
            "Ming Ding",
            "Zhuoyi Yang",
            "Wenyi Hong",
            "Wendi Zheng",
            "Chang Zhou",
            "Da Yin",
            "Junyang Lin",
            "Xu Zou",
            "Zhou Shao",
            "Hongxia Yang",
            "Jie Tang"
        ],
        "citations": 683,
        "references": 58,
        "year": 2021
    },
    {
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
        "abstract": "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.",
        "authors": [
            "Swaroop Mishra",
            "Daniel Khashabi",
            "Chitta Baral",
            "Hannaneh Hajishirzi"
        ],
        "citations": 657,
        "references": 54,
        "year": 2021
    },
    {
        "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders",
        "abstract": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",
        "authors": [
            "Jesse Engel",
            "Cinjon Resnick",
            "Adam Roberts",
            "S. Dieleman",
            "Mohammad Norouzi",
            "D. Eck",
            "K. Simonyan"
        ],
        "citations": 587,
        "references": 37,
        "year": 2017
    },
    {
        "title": "Neural Spline Flows",
        "abstract": "A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.",
        "authors": [
            "Conor Durkan",
            "Artur Bekasov",
            "Iain Murray",
            "G. Papamakarios"
        ],
        "citations": 686,
        "references": 66,
        "year": 2019
    },
    {
        "title": "PointFlow: 3D Point Cloud Generation With Continuous Normalizing Flows",
        "abstract": "As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code is available at https://github.com/stevenygd/PointFlow.",
        "authors": [
            "Guandao Yang",
            "Xun Huang",
            "Zekun Hao",
            "Ming-Yu Liu",
            "Serge J. Belongie",
            "Bharath Hariharan"
        ],
        "citations": 617,
        "references": 58,
        "year": 2019
    },
    {
        "title": "Large Scale Adversarial Representation Learning",
        "abstract": "Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",
        "authors": [
            "Jeff Donahue",
            "K. Simonyan"
        ],
        "citations": 533,
        "references": 42,
        "year": 2019
    },
    {
        "title": "Adversarially Learned One-Class Classifier for Novelty Detection",
        "abstract": "Novelty detection is the process of identifying the observation(s) that differ in some respect from the training observations (the target class). In reality, the novelty class is often absent during training, poorly sampled or not well defined. Therefore, one-class classifiers can efficiently model such problems. However, due to the unavailability of data from the novelty class, training an end-to-end deep network is a cumbersome task. In this paper, inspired by the success of generative adversarial networks for training deep models in unsupervised and semi-supervised settings, we propose an end-to-end architecture for one-class classification. Our architecture is composed of two deep networks, each of which trained by competing with each other while collaborating to understand the underlying concept in the target class, and then classify the testing samples. One network works as the novelty detector, while the other supports it by enhancing the inlier samples and distorting the outliers. The intuition is that the separability of the enhanced inliers and distorted outliers is much better than deciding on the original samples. The proposed framework applies to different related applications of anomaly and outlier detection in images and videos. The results on MNIST and Caltech-256 image datasets, along with the challenging UCSD Ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art methods.",
        "authors": [
            "M. Sabokrou",
            "Mohammad Khalooei",
            "M. Fathy",
            "E. Adeli"
        ],
        "citations": 664,
        "references": 52,
        "year": 2018
    },
    {
        "title": "A Note on the Inception Score",
        "abstract": "Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.",
        "authors": [
            "Shane T. Barratt",
            "Rishi Sharma"
        ],
        "citations": 643,
        "references": 28,
        "year": 2018
    },
    {
        "title": "Topic Modeling in Embedding Spaces",
        "abstract": "Abstract Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word’s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.",
        "authors": [
            "Adji B. Dieng",
            "Francisco J. R. Ruiz",
            "D. Blei"
        ],
        "citations": 558,
        "references": 58,
        "year": 2019
    },
    {
        "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting",
        "abstract": "Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.",
        "authors": [
            "Yen-Chun Chen",
            "Mohit Bansal"
        ],
        "citations": 574,
        "references": 64,
        "year": 2018
    },
    {
        "title": "PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees",
        "abstract": "Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modifies the Private Aggregation of Teacher Ensembles (PATE) framework and applies it to GANs. Our modified framework (which we call PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that PATE-GAN consistently outperforms the stateof-the-art method with respect to this and other notions of synthetic data quality.",
        "authors": [
            "James Jordon",
            "Jinsung Yoon",
            "M. Schaar"
        ],
        "citations": 590,
        "references": 32,
        "year": 2018
    },
    {
        "title": "Communication dynamics in complex brain networks",
        "abstract": null,
        "authors": [
            "Andrea Avena-Koenigsberger",
            "B. Mišić",
            "O. Sporns"
        ],
        "citations": 646,
        "references": 208,
        "year": 2017
    },
    {
        "title": "Unifying Vision-and-Language Tasks via Text Generation",
        "abstract": "Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5",
        "authors": [
            "Jaemin Cho",
            "Jie Lei",
            "Hao Tan",
            "Mohit Bansal"
        ],
        "citations": 500,
        "references": 84,
        "year": 2021
    },
    {
        "title": "Efficient GAN-Based Anomaly Detection",
        "abstract": "Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "authors": [
            "Houssam Zenati",
            "Chuan-Sheng Foo",
            "Bruno Lecouat",
            "Gaurav Manek",
            "V. Chandrasekhar"
        ],
        "citations": 546,
        "references": 21,
        "year": 2018
    },
    {
        "title": "Generating Natural Adversarial Examples",
        "abstract": "Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.",
        "authors": [
            "Zhengli Zhao",
            "Dheeru Dua",
            "Sameer Singh"
        ],
        "citations": 585,
        "references": 35,
        "year": 2017
    },
    {
        "title": "Prototype Augmentation and Self-Supervision for Incremental Learning",
        "abstract": "Despite the impressive performance in many individual tasks, deep neural networks suffer from catastrophic forgetting when learning new tasks incrementally. Recently, various incremental learning methods have been proposed, and some approaches achieved acceptable performance relying on stored data or complex generative models. However, storing data from previous tasks is limited by memory or privacy issues, and generative models are usually unstable and inefficient in training. In this paper, we propose a simple non-exemplar based method named PASS, to address the catastrophic forgetting problem in incremental learning. On the one hand, we propose to memorize one class-representative prototype for each old class and adopt prototype augmentation (protoAug) in the deep feature space to maintain the decision boundary of previous tasks. On the other hand, we employ self-supervised learning (SSL) to learn more generalizable and transferable features for other tasks, which demonstrates the effectiveness of SSL in incremental learning. Experimental results on benchmark datasets show that our approach significantly outperforms non-exemplar based methods, and achieves comparable performance compared to exemplar based approaches.",
        "authors": [
            "Fei Zhu",
            "Xu-Yao Zhang",
            "Chuan Wang",
            "Fei Yin",
            "Cheng-Lin Liu"
        ],
        "citations": 297,
        "references": 55,
        "year": 2021
    },
    {
        "title": "Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis",
        "abstract": "We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.",
        "authors": [
            "Tianchang Shen",
            "Jun Gao",
            "K. Yin",
            "Ming-Yu Liu",
            "S. Fidler"
        ],
        "citations": 390,
        "references": 72,
        "year": 2021
    },
    {
        "title": "3D Shape Generation and Completion through Point-Voxel Diffusion",
        "abstract": "We propose a novel approach for probabilistic generative modeling of 3D shapes. Unlike most existing models that learn to deterministically translate a latent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified, probabilistic formulation for unconditional shape generation and conditional, multi-modal shape completion. PVD marries denoising diffusion models with the hybrid, point-voxel representation of 3D shapes. It can be viewed as a series of denoising steps, reversing the diffusion process from observed point cloud data to Gaussian noise, and is trained by optimizing a variational lower bound to the (conditional) likelihood function. Experiments demonstrate that PVD is capable of synthesizing high-fidelity shapes, completing partial point clouds, and generating multiple completion results from single-view depth scans of real objects.",
        "authors": [
            "Linqi Zhou",
            "Yilun Du",
            "Jiajun Wu"
        ],
        "citations": 456,
        "references": 49,
        "year": 2021
    },
    {
        "title": "HoloGAN: Unsupervised Learning of 3D Representations From Natural Images",
        "abstract": "We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.",
        "authors": [
            "Thu Nguyen-Phuoc",
            "Chuan Li",
            "Lucas Theis",
            "Christian Richardt",
            "Yong-Liang Yang"
        ],
        "citations": 494,
        "references": 66,
        "year": 2019
    },
    {
        "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech",
        "abstract": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.",
        "authors": [
            "Vadim Popov",
            "Ivan Vovk",
            "Vladimir Gogoryan",
            "Tasnima Sadekova",
            "Mikhail Kudinov"
        ],
        "citations": 464,
        "references": 36,
        "year": 2021
    },
    {
        "title": "In Ictu Oculi: Exposing AI Created Fake Videos by Detecting Eye Blinking",
        "abstract": "The new developments in deep generative networks have significantly improve the quality and efficiency in generating realistically-looking fake face videos. In this work, we describe a new method to expose fake face videos generated with deep neural network models. Our method is based on detection of eye blinking in the videos, which is a physiological signal that is not well presented in the synthesized fake videos. Our method is evaluated over benchmarks of eye-blinking detection datasets and shows promising performance on detecting videos generated with DNN based software DeepFake.",
        "authors": [
            "Yuezun Li",
            "Ming-Ching Chang",
            "Siwei Lyu"
        ],
        "citations": 616,
        "references": 36,
        "year": 2018
    },
    {
        "title": "Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One",
        "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x,y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may beused and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, andout-of-distribution detection while also enabling our models to generate samplesrivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and presentan approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-artin both generative and discriminative learning within one hybrid model.",
        "authors": [
            "Will Grathwohl",
            "Kuan-Chieh Jackson Wang",
            "J. Jacobsen",
            "D. Duvenaud",
            "Mohammad Norouzi",
            "Kevin Swersky"
        ],
        "citations": 500,
        "references": 52,
        "year": 2019
    },
    {
        "title": "GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation",
        "abstract": "Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68% chemically valid molecules even without chemical knowledge rules and 100% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization.",
        "authors": [
            "Chence Shi",
            "Minkai Xu",
            "Zhaocheng Zhu",
            "Weinan Zhang",
            "Ming Zhang",
            "Jian Tang"
        ],
        "citations": 397,
        "references": 51,
        "year": 2020
    },
    {
        "title": "TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents",
        "abstract": "We introduce a new approach to generative data-driven dialogue systems (e.g. chatbots) called TransferTransfo which is a combination of a Transfer learning based training scheme and a high-capacity Transformer model. Fine-tuning is performed by using a multi-task objective which combines several unsupervised prediction tasks. The resulting fine-tuned model shows strong improvements over the current state-of-the-art end-to-end conversational models like memory augmented seq2seq and information-retrieval models. On the privately held PERSONA-CHAT dataset of the Conversational Intelligence Challenge 2, this approach obtains a new state-of-the-art, with respective perplexity, Hits@1 and F1 metrics of 16.28 (45 % absolute improvement), 80.7 (46 % absolute improvement) and 19.5 (20 % absolute improvement).",
        "authors": [
            "Thomas Wolf",
            "Victor Sanh",
            "Julien Chaumond",
            "Clement Delangue"
        ],
        "citations": 485,
        "references": 18,
        "year": 2019
    },
    {
        "title": "DDSP: Differentiable Digital Signal Processing",
        "abstract": "Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library will be made available upon paper acceptance and we encourage further contributions from the community and domain experts.",
        "authors": [
            "Jesse Engel",
            "Lamtharn Hantrakul",
            "Chenjie Gu",
            "Adam Roberts"
        ],
        "citations": 346,
        "references": 41,
        "year": 2020
    },
    {
        "title": "A Variational U-Net for Conditional Appearance and Shape Generation",
        "abstract": "Deep generative models have demonstrated great performance in image synthesis. However, results deteriorate in case of spatial deformations, since they generate images of objects directly, rather than modeling the intricate interplay of their inherent shape and appearance. We present a conditional U-Net [30] for shape-guided image generation, conditioned on the output of a variational autoencoder for appearance. The approach is trained end-to-end on images, without requiring samples of the same object with varying pose or appearance. Experiments show that the model enables conditional image generation and transfer. Therefore, either shape or appearance can be retained from a query image, while freely altering the other. Moreover, appearance can be sampled due to its stochastic latent representation, while preserving shape. In quantitative and qualitative experiments on COCO [20], DeepFashion [21, 23], shoes [43], Market-1501 [47] and handbags [49] the approach demonstrates significant improvements over the state-of-the-art.",
        "authors": [
            "Patrick Esser",
            "E. Sutter",
            "B. Ommer"
        ],
        "citations": 410,
        "references": 53,
        "year": 2018
    },
    {
        "title": "Swapping Autoencoder for Deep Image Manipulation",
        "abstract": "Deep generative models have become increasingly effective at producing realistic images from randomly sampled seeds, but using such models for controllable manipulation of existing images remains challenging. We propose the Swapping Autoencoder, a deep model designed specifically for image manipulation, rather than random sampling. The key idea is to encode an image with two independent components and enforce that any swapped combination maps to a realistic image. In particular, we encourage the components to represent structure and texture, by enforcing one component to encode co-occurrent patch statistics across different parts of an image. As our method is trained with an encoder, finding the latent codes for a new input image becomes trivial, rather than cumbersome. As a result, it can be used to manipulate real input images in various ways, including texture swapping, local and global editing, and latent code vector arithmetic. Experiments on multiple datasets show that our model produces better results and is substantially more efficient compared to recent generative models.",
        "authors": [
            "Taesung Park",
            "Jun-Yan Zhu",
            "Oliver Wang",
            "Jingwan Lu",
            "Eli Shechtman",
            "Alexei A. Efros",
            "Richard Zhang"
        ],
        "citations": 315,
        "references": 101,
        "year": 2020
    },
    {
        "title": "F-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning",
        "abstract": "When labeled training data is scarce, a promising data augmentation approach is to generate visual features of unknown classes using their attributes. To learn the class conditional distribution of CNN features, these models rely on pairs of image features and class attributes. Hence, they can not make use of the abundance of unlabeled data samples. In this paper, we tackle any-shot learning problems i.e. zero-shot and few-shot, in a unified feature generating framework that operates in both inductive and transductive learning settings. We develop a conditional generative model that combines the strength of VAE and GANs and in addition, via an unconditional discriminator, learns the marginal feature distribution of unlabeled images. We empirically show that our model learns highly discriminative CNN features for five datasets, i.e. CUB, SUN, AWA and ImageNet, and establish a new state-of-the-art in any-shot learning, i.e. inductive and transductive (generalized) zero- and few-shot learning settings. We also demonstrate that our learned features are interpretable: we visualize them by inverting them back to the pixel space and we explain them by generating textual arguments of why they are associated with a certain label.",
        "authors": [
            "Yongqin Xian",
            "Saurabh Sharma",
            "B. Schiele",
            "Zeynep Akata"
        ],
        "citations": 461,
        "references": 64,
        "year": 2019
    },
    {
        "title": "VideoGPT: Video Generation using VQ-VAE and Transformers",
        "abstract": "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html",
        "authors": [
            "Wilson Yan",
            "Yunzhi Zhang",
            "P. Abbeel",
            "A. Srinivas"
        ],
        "citations": 415,
        "references": 77,
        "year": 2021
    },
    {
        "title": "Geometric GAN",
        "abstract": "Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.",
        "authors": [
            "Jae Hyun Lim",
            "J. C. Ye"
        ],
        "citations": 495,
        "references": 23,
        "year": 2017
    },
    {
        "title": "InfoVAE: Information Maximizing Variational Autoencoders",
        "abstract": "A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.",
        "authors": [
            "Shengjia Zhao",
            "Jiaming Song",
            "Stefano Ermon"
        ],
        "citations": 427,
        "references": 43,
        "year": 2017
    },
    {
        "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions",
        "abstract": "Generative flows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of flows and diffusion for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood.",
        "authors": [
            "Emiel Hoogeboom",
            "Didrik Nielsen",
            "P. Jaini",
            "Patrick Forr'e",
            "M. Welling"
        ],
        "citations": 333,
        "references": 39,
        "year": 2021
    },
    {
        "title": "Learning to Generate Reviews and Discovering Sentiment",
        "abstract": "We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.",
        "authors": [
            "Alec Radford",
            "R. Józefowicz",
            "I. Sutskever"
        ],
        "citations": 496,
        "references": 61,
        "year": 2017
    },
    {
        "title": "Efficient Graph Generation with Graph Recurrent Attention Networks",
        "abstract": "We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. Our code is released at: \\url{https://github.com/lrjconan/GRAN}.",
        "authors": [
            "Renjie Liao",
            "Yujia Li",
            "Yang Song",
            "Shenlong Wang",
            "C. Nash",
            "William L. Hamilton",
            "D. Duvenaud",
            "R. Urtasun",
            "R. Zemel"
        ],
        "citations": 305,
        "references": 40,
        "year": 2019
    },
    {
        "title": "Label-Consistent Backdoor Attacks",
        "abstract": "Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.",
        "authors": [
            "Alexander Turner",
            "Dimitris Tsipras",
            "A. Madry"
        ],
        "citations": 352,
        "references": 39,
        "year": 2019
    },
    {
        "title": "SDEdit: Image Synthesis and Editing with Stochastic Differential Equations",
        "abstract": "We introduce a new image editing and synthesis framework, Stochastic Differential Editing (SDEdit), based on a recent generative model using stochastic differential equations (SDEs). Given an input image with user edits ( e.g ., hand-drawn color strokes), we ﬁrst add noise to the input according to an SDE, and subsequently denoise it by simulating the reverse SDE to gradually increase its likelihood under the prior. Our method does not require task-speciﬁc loss function designs, which are critical components for recent image editing methods based on GAN inversion. Compared to conditional GANs, we do not need to collect new datasets of original and edited images for new applications. Therefore, our method can quickly adapt to various editing tasks at test time without re-training models. Our approach achieves strong performance on a wide range of applications, including image synthesis and editing guided by stroke paintings and image compositing.",
        "authors": [
            "Chenlin Meng",
            "Yang Song",
            "Jiaming Song",
            "Jiajun Wu",
            "Jun-Yan Zhu",
            "Stefano Ermon"
        ],
        "citations": 280,
        "references": 74,
        "year": 2021
    },
    {
        "title": "State of the Art on Neural Rendering",
        "abstract": "Efficient rendering of photo‐realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo‐realistic images from hand‐crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo‐realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state‐of‐the‐art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photorealistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs. implicit control, generalization, and stochastic vs. deterministic synthesis. The second half of this state‐of‐the‐art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free‐viewpoint video, and the creation of photo‐realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.",
        "authors": [
            "A. Tewari",
            "Ohad Fried",
            "Justus Thies",
            "V. Sitzmann",
            "Stephen Lombardi",
            "Kalyan Sunkavalli",
            "Ricardo Martin-Brualla",
            "T. Simon",
            "Jason M. Saragih",
            "M. Nießner",
            "Rohit Pandey",
            "S. Fanello",
            "Gordon Wetzstein",
            "Jun-Yan Zhu",
            "C. Theobalt",
            "Maneesh Agrawala",
            "Eli Shechtman",
            "Dan B. Goldman",
            "Michael Zollhofer"
        ],
        "citations": 439,
        "references": 232,
        "year": 2020
    },
    {
        "title": "MaskGAN: Better Text Generation via Filling in the ______",
        "abstract": "Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.",
        "authors": [
            "W. Fedus",
            "I. Goodfellow",
            "Andrew M. Dai"
        ],
        "citations": 463,
        "references": 38,
        "year": 2018
    },
    {
        "title": "MoFlow: An Invertible Flow Model for Generating Molecular Graphs",
        "abstract": "Generating molecular graphs with desired chemical properties driven by deep graph generative models provides a very promising way to accelerate drug discovery process. Such graph generative models usually consist of two steps: learning latent representations and generation of molecular graphs. However, to generate novel and chemically-valid molecular graphs from latent representations is very challenging because of the chemical constraints and combinatorial complexity of molecular graphs. In this paper, we propose MoFlow, a flow-based graph generative model to learn invertible mappings between molecular graphs and their latent representations. To generate molecular graphs, our MoFlow first generates bonds (edges) through a Glow based model, then generates atoms (nodes) given bonds by a novel graph conditional flow, and finally assembles them into a chemically valid molecular graph with a posthoc validity correction. Our MoFlow has merits including exact and tractable likelihood training, efficient one-pass embedding and generation, chemical validity guarantees, 100% reconstruction of training data, and good generalization ability. We validate our model by four tasks: molecular graph generation and reconstruction, visualization of the continuous latent space, property optimization, and constrained property optimization. Our MoFlow achieves state-of-the-art performance, which implies its potential efficiency and effectiveness to explore large chemical space for drug discovery.",
        "authors": [
            "Chengxi Zang",
            "Fei Wang"
        ],
        "citations": 257,
        "references": 37,
        "year": 2020
    },
    {
        "title": "From Variational to Deterministic Autoencoders",
        "abstract": "Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without forcing it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data, we introduce an ex-post density estimation step that can be readily applied also to existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. \\footnote{An implementation is available at: \\url{this https URL}}",
        "authors": [
            "Partha Ghosh",
            "Mehdi S. M. Sajjadi",
            "Antonio Vergari",
            "Michael J. Black",
            "B. Scholkopf"
        ],
        "citations": 260,
        "references": 65,
        "year": 2019
    },
    {
        "title": "Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN",
        "abstract": null,
        "authors": [
            "Weiwei Hu",
            "Ying Tan"
        ],
        "citations": 434,
        "references": 29,
        "year": 2017
    },
    {
        "title": "InfoVAE: Balancing Learning and Inference in Variational Autoencoders",
        "abstract": "A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (Info-VAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics",
        "authors": [
            "Shengjia Zhao",
            "Jiaming Song",
            "Stefano Ermon"
        ],
        "citations": 275,
        "references": 33,
        "year": 2019
    },
    {
        "title": "The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs",
        "abstract": "Developing safe human-robot interaction systems is a necessary step towards the widespread integration of autonomous agents in society. A key component of such systems is the ability to reason about the many potential futures (e.g. trajectories) of other agents in the scene. Towards this end, we present the Trajectron, a graph-structured model that predicts many potential future trajectories of multiple agents simultaneously in both highly dynamic and multimodal scenarios (i.e. where the number of agents in the scene is time-varying and there are many possible highly-distinct futures for each agent). It combines tools from recurrent sequence modeling and variational deep generative modeling to produce a distribution of future trajectories for each agent in a scene. We demonstrate the performance of our model on several datasets, obtaining state-of-the-art results on standard trajectory prediction metrics as well as introducing a new metric for comparing models that output distributions.",
        "authors": [
            "B. Ivanovic",
            "M. Pavone"
        ],
        "citations": 377,
        "references": 59,
        "year": 2018
    },
    {
        "title": "Learning Factorized Multimodal Representations",
        "abstract": "Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities provides additional valuable information, there are two key challenges to address when learning from multimodal data: 1) models must learn the complex intra-modal and cross-modal interactions for prediction and 2) models must be robust to unexpected missing or noisy modalities during testing. In this paper, we propose to optimize for a joint generative-discriminative objective across multimodal data and labels. We introduce a model that factorizes representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors. Multimodal discriminative factors are shared across all modalities and contain joint multimodal features required for discriminative tasks such as sentiment prediction. Modality-specific generative factors are unique for each modality and contain the information required for generating data. Experimental results show that our model is able to learn meaningful multimodal representations that achieve state-of-the-art or competitive performance on six multimodal datasets. Our model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance. Lastly, we interpret our factorized representations to understand the interactions that influence multimodal learning.",
        "authors": [
            "Yao-Hung Hubert Tsai",
            "P. Liang",
            "Amir Zadeh",
            "Louis-Philippe Morency",
            "R. Salakhutdinov"
        ],
        "citations": 364,
        "references": 81,
        "year": 2018
    },
    {
        "title": "ResViT: Residual Vision Transformers for Multimodal Medical Image Synthesis",
        "abstract": "Generative adversarial models with convolutional neural network (CNN) backbones have recently been established as state-of-the-art in numerous medical image synthesis tasks. However, CNNs are designed to perform local processing with compact filters, and this inductive bias compromises learning of contextual features. Here, we propose a novel generative adversarial approach for medical image synthesis, ResViT, that leverages the contextual sensitivity of vision transformers along with the precision of convolution operators and realism of adversarial learning. ResViT’s generator employs a central bottleneck comprising novel aggregated residual transformer (ART) blocks that synergistically combine residual convolutional and transformer modules. Residual connections in ART blocks promote diversity in captured representations, while a channel compression module distills task-relevant information. A weight sharing strategy is introduced among ART blocks to mitigate computational burden. A unified implementation is introduced to avoid the need to rebuild separate synthesis models for varying source-target modality configurations. Comprehensive demonstrations are performed for synthesizing missing sequences in multi-contrast MRI, and CT images from MRI. Our results indicate superiority of ResViT against competing CNN- and transformer-based methods in terms of qualitative observations and quantitative metrics.",
        "authors": [
            "Onat Dalmaz",
            "Mahmut Yurt",
            "Tolga Cukur"
        ],
        "citations": 277,
        "references": 119,
        "year": 2021
    },
    {
        "title": "Syntax-Directed Variational Autoencoder for Structured Data",
        "abstract": "Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, e.g., computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing stochastic lazy attributes. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only syntactically valid, but also semantically reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.",
        "authors": [
            "H. Dai",
            "Yingtao Tian",
            "Bo Dai",
            "S. Skiena",
            "Le Song"
        ],
        "citations": 315,
        "references": 28,
        "year": 2018
    },
    {
        "title": "GANSynth: Adversarial Neural Audio Synthesis",
        "abstract": "Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.",
        "authors": [
            "Jesse Engel",
            "Kumar Krishna Agrawal",
            "Shuo Chen",
            "Ishaan Gulrajani",
            "Chris Donahue",
            "Adam Roberts"
        ],
        "citations": 374,
        "references": 40,
        "year": 2019
    },
    {
        "title": "A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose",
        "abstract": "While deep learning reshaped the classical motion capture pipeline with feed-forward networks, generative models are required to recover fine alignment via iterative refinement. Unfortunately, the existing models are usually hand-crafted or learned in controlled conditions, only applicable to limited domains. We propose a method to learn a generative neural body model from unlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We equip them with a skeleton to apply to time-varying and articulated motion. A key insight is that implicit models require the inverse of the forward kinematics used in explicit surface models. Our reparameterization defines spatial latent variables relative to the pose of body parts and thereby overcomes ill-posed inverse operations with an overparameterization. This enables learning volumetric body shape and appearance from scratch while jointly refining the articulated pose; all without ground truth labels for appearance, pose, or 3D shape on the input videos. When used for novel-view-synthesis and motion capture, our neural model improves accuracy on diverse datasets. Project website: https://lemonatsu.github.io/anerf/ .",
        "authors": [
            "Shih-Yang Su",
            "F. Yu",
            "Michael Zollhoefer",
            "Helge Rhodin"
        ],
        "citations": 230,
        "references": 80,
        "year": 2021
    },
    {
        "title": "Discovering Discrete Latent Topics with Neural Variational Inference",
        "abstract": "Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.",
        "authors": [
            "Yishu Miao",
            "Edward Grefenstette",
            "Phil Blunsom"
        ],
        "citations": 278,
        "references": 35,
        "year": 2017
    },
    {
        "title": "Graph theory methods: applications in brain networks",
        "abstract": "Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks.",
        "authors": [
            "O. Sporns"
        ],
        "citations": 405,
        "references": 77,
        "year": 2018
    },
    {
        "title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions",
        "abstract": "Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.",
        "authors": [
            "Zichao Yang",
            "Zhiting Hu",
            "R. Salakhutdinov",
            "Taylor Berg-Kirkpatrick"
        ],
        "citations": 377,
        "references": 42,
        "year": 2017
    },
    {
        "title": "Graph theory methods: applications in brain networks",
        "abstract": "Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks.",
        "authors": [
            "O. Sporns"
        ],
        "citations": 405,
        "references": 77,
        "year": 2018
    },
    {
        "title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions",
        "abstract": "Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.",
        "authors": [
            "Zichao Yang",
            "Zhiting Hu",
            "R. Salakhutdinov",
            "Taylor Berg-Kirkpatrick"
        ],
        "citations": 377,
        "references": 42,
        "year": 2017
    },
    {
        "title": "The graphical brain: Belief propagation and active inference",
        "abstract": "This paper considers functional integration in the brain from a computational perspective. We ask what sort of neuronal message passing is mandated by active inference—and what implications this has for context-sensitive connectivity at microscopic and macroscopic levels. In particular, we formulate neuronal processing as belief propagation under deep generative models. Crucially, these models can entertain both discrete and continuous states, leading to distinct schemes for belief updating that play out on the same (neuronal) architecture. Technically, we use Forney (normal) factor graphs to elucidate the requisite message passing in terms of its form and scheduling. To accommodate mixed generative models (of discrete and continuous states), one also has to consider link nodes or factors that enable discrete and continuous representations to talk to each other. When mapping the implicit computational architecture onto neuronal connectivity, several interesting features emerge. For example, Bayesian model averaging and comparison, which link discrete and continuous states, may be implemented in thalamocortical loops. These and other considerations speak to a computational connectome that is inherently state dependent and self-organizing in ways that yield to a principled (variational) account. We conclude with simulations of reading that illustrate the implicit neuronal message passing, with a special focus on how discrete (semantic) representations inform, and are informed by, continuous (visual) sampling of the sensorium. Author Summary This paper considers functional integration in the brain from a computational perspective. We ask what sort of neuronal message passing is mandated by active inference—and what implications this has for context-sensitive connectivity at microscopic and macroscopic levels. In particular, we formulate neuronal processing as belief propagation under deep generative models that can entertain both discrete and continuous states. This leads to distinct schemes for belief updating that play out on the same (neuronal) architecture. Technically, we use Forney (normal) factor graphs to characterize the requisite message passing, and link this formal characterization to canonical microcircuits and extrinsic connectivity in the brain.",
        "authors": [
            "Karl J. Friston",
            "Thomas Parr",
            "B. de Vries"
        ],
        "citations": 316,
        "references": 118,
        "year": 2017
    },
    {
        "title": "DLow: Diversifying Latent Flows for Diverse Human Motion Prediction",
        "abstract": null,
        "authors": [
            "Ye Yuan",
            "Kris M. Kitani"
        ],
        "citations": 219,
        "references": 80,
        "year": 2020
    },
    {
        "title": "Transferring GANs: generating images from limited data",
        "abstract": null,
        "authors": [
            "Yaxing Wang",
            "Chenshen Wu",
            "Luis Herranz",
            "Joost van de Weijer",
            "Abel Gonzalez-Garcia",
            "B. Raducanu"
        ],
        "citations": 276,
        "references": 55,
        "year": 2018
    },
    {
        "title": "Texture Fields: Learning Texture Representations in Function Space",
        "abstract": "In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",
        "authors": [
            "Michael Oechsle",
            "L. Mescheder",
            "Michael Niemeyer",
            "Thilo Strauss",
            "Andreas Geiger"
        ],
        "citations": 289,
        "references": 51,
        "year": 2019
    },
    {
        "title": "Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory",
        "abstract": "Schr\\\"odinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory - a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimization principle that inherits the same SB optimality yet without losing applications of modern generative training techniques, and we show that the resulting training algorithm achieves comparable results on generating realistic images on MNIST, CelebA, and CIFAR10. Our code is available at https://github.com/ghliu/SB-FBSDE.",
        "authors": [
            "T. Chen",
            "Guan-Horng Liu",
            "Evangelos A. Theodorou"
        ],
        "citations": 142,
        "references": 73,
        "year": 2021
    },
    {
        "title": "StyleGAN-NADA",
        "abstract": "Can a generative model be trained to produce images from a specific domain, guided only by a text prompt, without seeing any image? In other words: can an image generator be trained \"blindly\"? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or infeasible to reach with existing methods. We conduct an extensive set of experiments across a wide range of domains. These demonstrate the effectiveness of our approach, and show that our models preserve the latent-space structure that makes generative models appealing for downstream tasks. Code and videos available at: stylegan-nada.github.io/",
        "authors": [
            "Rinon Gal",
            "Or Patashnik",
            "Haggai Maron",
            "Amit H. Bermano",
            "Gal Chechik",
            "D. Cohen-Or"
        ],
        "citations": 196,
        "references": 79,
        "year": 2021
    },
    {
        "title": "Generating Sentences by Editing Prototypes",
        "abstract": "We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.",
        "authors": [
            "Kelvin Guu",
            "Tatsunori B. Hashimoto",
            "Yonatan Oren",
            "Percy Liang"
        ],
        "citations": 310,
        "references": 35,
        "year": 2017
    },
    {
        "title": "De novo generation of hit-like molecules from gene expression signatures using artificial intelligence",
        "abstract": null,
        "authors": [
            "O. Méndez-Lucio",
            "B. Baillif",
            "Djork-Arné Clevert",
            "D. Rouquié",
            "J. Wichard"
        ],
        "citations": 270,
        "references": 73,
        "year": 2018
    },
    {
        "title": "Active Inference, Curiosity and Insight",
        "abstract": "This article offers a formal account of curiosity and insight in terms of active (Bayesian) inference. It deals with the dual problem of inferring states of the world and learning its statistical structure. In contrast to current trends in machine learning (e.g., deep learning), we focus on how people attain insight and understanding using just a handful of observations, which are solicited through curious behavior. We use simulations of abstract rule learning and approximate Bayesian inference to show that minimizing (expected) variational free energy leads to active sampling of novel contingencies. This epistemic behavior closes explanatory gaps in generative models of the world, thereby reducing uncertainty and satisfying curiosity. We then move from epistemic learning to model selection or structure learning to show how abductive processes emerge when agents test plausible hypotheses about symmetries (i.e., invariances or rules) in their generative models. The ensuing Bayesian model reduction evinces mechanisms associated with sleep and has all the hallmarks of “aha” moments. This formulation moves toward a computational account of consciousness in the pre-Cartesian sense of sharable knowledge (i.e., con: “together”; scire: “to know”).",
        "authors": [
            "Karl J. Friston",
            "Marco Lin",
            "C. Frith",
            "G. Pezzulo",
            "J. Hobson",
            "S. Ondobaka"
        ],
        "citations": 264,
        "references": 115,
        "year": 2017
    },
    {
        "title": "Adversarial Video Generation on Complex Datasets",
        "abstract": "Generative models of natural images have progressed towards high fidelity samples by the strong leveraging of scale. We attempt to carry this success to the field of video modeling by showing that large Generative Adversarial Networks trained on the complex Kinetics-600 dataset are able to produce video samples of substantially higher complexity and fidelity than previous work. Our proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. We evaluate on the related tasks of video synthesis and video prediction, and achieve new state-of-the-art Frechet Inception Distance for prediction for Kinetics-600, as well as state-of-the-art Inception Score for synthesis on the UCF-101 dataset, alongside establishing a strong baseline for synthesis on Kinetics-600.",
        "authors": [
            "Aidan Clark",
            "Jeff Donahue",
            "K. Simonyan"
        ],
        "citations": 230,
        "references": 74,
        "year": 2019
    },
    {
        "title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
        "abstract": "Application of deep learning techniques for de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in natural language processing, such as Transformers, for molecular design in general. Inspired by generative pre-training (GPT) models that have been shown to be successful in generating meaningful text, we train a transformer-decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, MolGPT, performs on par with other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique, and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to control multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties by conditioning the generation on scaffold SMILES strings of desired scaffolds and property values. Using saliency maps, we highlight the interpretability of the generative process of the model.",
        "authors": [
            "Viraj Bagal",
            "Rishal Aggarwal",
            "P. K. Vinod",
            "U. Priyakumar"
        ],
        "citations": 220,
        "references": 53,
        "year": 2021
    },
    {
        "title": "Unconstrained Scene Generation with Locally Conditioned Radiance Fields",
        "abstract": "We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.",
        "authors": [
            "Terrance Devries",
            "M. Bautista",
            "Nitish Srivastava",
            "Graham W. Taylor",
            "J. Susskind"
        ],
        "citations": 144,
        "references": 63,
        "year": 2021
    },
    {
        "title": "Likelihood Regret: An Out-of-Distribution Detection Score For Variational Auto-encoder",
        "abstract": "Deep probabilistic generative models enable modeling the likelihoods of very high dimensional data. An important application of generative modeling should be the ability to detect out-of-distribution (OOD) samples by setting a threshold on the likelihood. However, a recent study shows that probabilistic generative models can, in some cases, assign higher likelihoods on certain types of OOD samples, making the OOD detection rules based on likelihood threshold problematic. To address this issue, several OOD detection methods have been proposed for deep generative models. In this paper, we make the observation that some of these methods fail when applied to generative models based on Variational Auto-encoders (VAE). As an alternative, we propose Likelihood Regret, an efficient OOD score for VAEs. We benchmark our proposed method over existing approaches, and empirical results suggest that our method obtains the best overall OOD detection performances compared with other OOD method applied on VAE.",
        "authors": [
            "Zhisheng Xiao",
            "Qing Yan",
            "Y. Amit"
        ],
        "citations": 173,
        "references": 55,
        "year": 2020
    },
    {
        "title": "Recipes for Safety in Open-domain Chatbots",
        "abstract": "Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.",
        "authors": [
            "Jing Xu",
            "Da Ju",
            "Margaret Li",
            "Y-Lan Boureau",
            "J. Weston",
            "Emily Dinan"
        ],
        "citations": 220,
        "references": 75,
        "year": 2020
    },
    {
        "title": "End-to-End Adversarial Retinal Image Synthesis",
        "abstract": "In medical image analysis applications, the availability of the large amounts of annotated data is becoming increasingly critical. However, annotated medical data is often scarce and costly to obtain. In this paper, we address the problem of synthesizing retinal color images by applying recent techniques based on adversarial learning. In this setting, a generative model is trained to maximize a loss function provided by a second model attempting to classify its output into real or synthetic. In particular, we propose to implement an adversarial autoencoder for the task of retinal vessel network synthesis. We use the generated vessel trees as an intermediate stage for the generation of color retinal images, which is accomplished with a generative adversarial network. Both models require the optimization of almost everywhere differentiable loss functions, which allows us to train them jointly. The resulting model offers an end-to-end retinal image synthesis system capable of generating as many retinal images as the user requires, with their corresponding vessel networks, by sampling from a simple probability distribution that we impose to the associated latent space. We show that the learned latent space contains a well-defined semantic structure, implying that we can perform calculations in the space of retinal images, e.g., smoothly interpolating new data points between two retinal images. Visual and quantitative results demonstrate that the synthesized images are substantially different from those in the training set, while being also anatomically consistent and displaying a reasonable visual quality.",
        "authors": [
            "P. Costa",
            "A. Galdran",
            "Maria Inês Meyer",
            "M. Niemeijer",
            "M. Abràmoff",
            "A. Mendonça",
            "A. Campilho"
        ],
        "citations": 336,
        "references": 38,
        "year": 2018
    },
    {
        "title": "Multi-Objective Molecule Generation using Interpretable Substructures",
        "abstract": "Drug discovery aims to find novel compounds with specified chemical property profiles. In terms of generative modeling, the goal is to learn to sample molecules in the intersection of multiple property constraints. This task becomes increasingly challenging when there are many property constraints. We propose to offset this complexity by composing molecules from a vocabulary of substructures that we call molecular rationales. These rationales are identified from molecules as substructures that are likely responsible for each property of interest. We then learn to expand rationales into a full molecule using graph generative models. Our final generative model composes molecules as mixtures of multiple rationale completions, and this mixture is fine-tuned to preserve the properties of interest. We evaluate our model on various drug design tasks and demonstrate significant improvements over state-of-the-art baselines in terms of accuracy, diversity, and novelty of generated compounds.",
        "authors": [
            "Wengong Jin",
            "R. Barzilay",
            "T. Jaakkola"
        ],
        "citations": 170,
        "references": 50,
        "year": 2020
    },
    {
        "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space",
        "abstract": "When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model, Optimus. A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks. We hope that our first pre-trained big VAE language model itself and results can help the NLP community renew the interests of deep generative models in the era of large-scale pre-training, and make these principled methods more practical.",
        "authors": [
            "Chunyuan Li",
            "Xiang Gao",
            "Yuan Li",
            "Xiujun Li",
            "Baolin Peng",
            "Yizhe Zhang",
            "Jianfeng Gao"
        ],
        "citations": 169,
        "references": 69,
        "year": 2020
    },
    {
        "title": "Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning",
        "abstract": "Models trained in the context of continual learning (CL) should be able to learn from a stream of data over an undefined period of time. The main challenges herein are: 1) maintaining old knowledge while simultaneously benefiting from it when learning new tasks, and 2) guaranteeing model scalability with a growing amount of data to learn from. In order to tackle these challenges, we introduce Dynamic Generative Memory (DGM) - synaptic plasticity driven framework for continual learning. DGM relies on conditional generative adversarial networks with learnable connection plasticity realized with neural masking. Specifically, we evaluate two variants of neural masking: applied to (i) layer activations and (ii) to connection weights directly. Furthermore, we propose a dynamic network expansion mechanism that ensures sufficient model capacity to accommodate for continually incoming tasks. The amount of added capacity is determined dynamically from the learned binary mask. We evaluate DGM in the continual class-incremental setup on visual classification tasks.",
        "authors": [
            "O. Ostapenko",
            "M. Puscas",
            "T. Klein",
            "P. Jähnichen",
            "Moin Nabi"
        ],
        "citations": 271,
        "references": 36,
        "year": 2019
    },
    {
        "title": "Contrastive Learning Inverts the Data Generating Process",
        "abstract": "Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.",
        "authors": [
            "Roland S. Zimmermann",
            "Yash Sharma",
            "Steffen Schneider",
            "M. Bethge",
            "Wieland Brendel"
        ],
        "citations": 186,
        "references": 64,
        "year": 2021
    },
    {
        "title": "Semi-Amortized Variational Autoencoders",
        "abstract": "Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.",
        "authors": [
            "Yoon Kim",
            "Sam Wiseman",
            "Andrew C. Miller",
            "D. Sontag",
            "Alexander M. Rush"
        ],
        "citations": 238,
        "references": 69,
        "year": 2018
    },
    {
        "title": "TweepFake: About detecting deepfake tweets",
        "abstract": "The recent advances in language modeling significantly improved the generative capabilities of deep neural models: in 2019 OpenAI released GPT-2, a pre-trained language model that can autonomously generate coherent, non-trivial and human-like text samples. Since then, ever more powerful text generative models have been developed. Adversaries can exploit these tremendous generative capabilities to enhance social bots that will have the ability to write plausible deepfake messages, hoping to contaminate public debate. To prevent this, it is crucial to develop deepfake social media messages detection systems. However, to the best of our knowledge no one has ever addressed the detection of machine-generated texts on social networks like Twitter or Facebook. With the aim of helping the research in this detection field, we collected the first dataset of real deepfake tweets, TweepFake. It is real in the sense that each deepfake tweet was actually posted on Twitter. We collected tweets from a total of 23 bots, imitating 17 human accounts. The bots are based on various generation techniques, i.e., Markov Chains, RNN, RNN+Markov, LSTM, GPT-2. We also randomly selected tweets from the humans imitated by the bots to have an overall balanced dataset of 25,572 tweets (half human and half bots generated). The dataset is publicly available on Kaggle. Lastly, we evaluated 13 deepfake text detection methods (based on various state-of-the-art approaches) to both demonstrate the challenges that Tweepfake poses and create a solid baseline of detection techniques. We hope that TweepFake can offer the opportunity to tackle the deepfake detection on social media messages as well.",
        "authors": [
            "T. Fagni",
            "F. Falchi",
            "Margherita Gambini",
            "Antonio Martella",
            "Maurizio Tesconi"
        ],
        "citations": 185,
        "references": 71,
        "year": 2020
    },
    {
        "title": "Unsupervised State Representation Learning in Atari",
        "abstract": "State representation learning, or the ability to capture latent generative factors of an environment, is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations without supervision from rewards is a challenging open problem. We introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state variables. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods. The code associated with this work is available at this https URL",
        "authors": [
            "Ankesh Anand",
            "Evan Racah",
            "Sherjil Ozair",
            "Yoshua Bengio",
            "Marc-Alexandre Côté",
            "R. Devon Hjelm"
        ],
        "citations": 241,
        "references": 88,
        "year": 2019
    },
    {
        "title": "Adversarial vulnerability for any classifier",
        "abstract": "Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.",
        "authors": [
            "Alhussein Fawzi",
            "Hamza Fawzi",
            "Omar Fawzi"
        ],
        "citations": 242,
        "references": 53,
        "year": 2018
    },
    {
        "title": "ClothFlow: A Flow-Based Model for Clothed Person Generation",
        "abstract": "We present ClothFlow, an appearance-flow-based generative model to synthesize clothed person for posed-guided person image generation and virtual try-on. By estimating a dense flow between source and target clothing regions, ClothFlow effectively models the geometric changes and naturally transfers the appearance to synthesize novel images as shown in Figure 1. We achieve this with a three-stage framework: 1) Conditioned on a target pose, we first estimate a person semantic layout to provide richer guidance to the generation process. 2) Built on two feature pyramid networks, a cascaded flow estimation network then accurately estimates the appearance matching between corresponding clothing regions. The resulting dense flow warps the source image to flexibly account for deformations. 3) Finally, a generative network takes the warped clothing regions as inputs and renders the target view. We conduct extensive experiments on the DeepFashion dataset for pose-guided person image generation and on the VITON dataset for the virtual try-on task. Strong qualitative and quantitative results validate the effectiveness of our method.",
        "authors": [
            "Xintong Han",
            "Weilin Huang",
            "Xiaojun Hu",
            "Matthew R. Scott"
        ],
        "citations": 220,
        "references": 50,
        "year": 2019
    },
    {
        "title": "Video Generation From Text",
        "abstract": "\n \n Generating videos from text has proven to be a significant challenge for existing generative models. We tackle this problem by training a conditional generative model to extract both static and dynamic information from text. This is manifested in a hybrid framework, employing a Variational Autoencoder (VAE) and a Generative Adversarial Network (GAN). The static features, called \"gist,\" are used to sketch text-conditioned background color and object layout structure. Dynamic features are considered by transforming input text into an image filter. To obtain a large amount of data for training the deep-learning model, we develop a method to automatically create a matched text-video corpus from publicly available online videos. Experimental results show that the proposed framework generates plausible and diverse short-duration smooth videos, while accurately reflecting the input text information. It significantly outperforms baseline models that directly adapt text-to-image generation procedures to produce videos. Performance is evaluated both visually and by adapting the inception score used to evaluate image generation in GANs.\n \n",
        "authors": [
            "Yitong Li",
            "Martin Renqiang Min",
            "Dinghan Shen",
            "David Edwin Carlson",
            "L. Carin"
        ],
        "citations": 251,
        "references": 41,
        "year": 2017
    },
    {
        "title": "Visual Object Networks: Image Generation with Disentangled 3D Representations",
        "abstract": "Recent progress in deep generative models has led to tremendous breakthroughs in image generation. While being able to synthesize photorealistic images, existing models lack an understanding of our underlying 3D world. Different from previous works built on 2D datasets and models, we present a new generative model, Visual Object Networks (VONs), synthesizing natural images of objects with a disentangled 3D representation. Inspired by classic graphics rendering pipelines, we unravel the image formation process into three conditionally independent factors---shape, viewpoint, and texture---and present an end-to-end adversarial learning framework that jointly models 3D shape and 2D texture. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then renders the object's 2.5D sketches (i.e., silhouette and depth map) from its shape under a sampled viewpoint. Finally, it learns to add realistic textures to these 2.5D sketches to generate realistic images. The VON not only generates images that are more realistic than the state-of-the-art 2D image synthesis methods but also enables many 3D operations such as changing the viewpoint of a generated image, shape and texture editing, linear interpolation in texture and shape space, and transferring appearance across different objects and viewpoints.",
        "authors": [
            "Jun-Yan Zhu",
            "Zhoutong Zhang",
            "Chengkai Zhang",
            "Jiajun Wu",
            "A. Torralba",
            "J. Tenenbaum",
            "Bill Freeman"
        ],
        "citations": 242,
        "references": 64,
        "year": 2018
    },
    {
        "title": "Image Generation From Small Datasets via Batch Statistics Adaptation",
        "abstract": "Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. Our method makes it possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain. Code is available at github.com/nogu-atsu/small-dataset-image-generation",
        "authors": [
            "Atsuhiro Noguchi",
            "T. Harada"
        ],
        "citations": 196,
        "references": 43,
        "year": 2019
    },
    {
        "title": "Adaptive Monte Carlo augmented with normalizing flows",
        "abstract": "Significance Monte Carlo methods, tools for sampling data from probability distributions, are widely used in the physical sciences, applied mathematics, and Bayesian statistics. Nevertheless, there are many situations in which it is computationally prohibitive to use Monte Carlo due to slow “mixing” between modes of a distribution unless hand-tuned algorithms are used to accelerate the scheme. Machine learning techniques based on generative models offer a compelling alternative to the challenge of designing efficient schemes for a specific system. Here, we formalize Monte Carlo augmented with normalizing flows and show that, with limited prior data and a physically inspired algorithm, we can substantially accelerate sampling with generative models.",
        "authors": [
            "Marylou Gabri'e",
            "Grant M. Rotskoff",
            "E. Vanden-Eijnden"
        ],
        "citations": 112,
        "references": 69,
        "year": 2021
    },
    {
        "title": "Perfection Not Required? Human-AI Partnerships in Code Translation",
        "abstract": "Generative models have become adept at producing artifacts such as images, videos, and prose at human-like levels of proficiency. New generative techniques, such as unsupervised neural machine translation (NMT), have recently been applied to the task of generating source code, translating it from one programming language to another. The artifacts produced in this way may contain imperfections, such as compilation or logical errors. We examine the extent to which software engineers would tolerate such imperfections and explore ways to aid the detection and correction of those errors. Using a design scenario approach, we interviewed 11 software engineers to understand their reactions to the use of an NMT model in the context of application modernization, focusing on the task of translating source code from one language to another. Our three-stage scenario sparked discussions about the utility and desirability of working with an imperfect AI system, how acceptance of that system’s outputs would be established, and future opportunities for generative AI in application modernization. Our study highlights how UI features such as confidence highlighting and alternate translations help software engineers work with and better understand generative NMT models.",
        "authors": [
            "Justin D. Weisz",
            "Michael J. Muller",
            "Stephanie Houde",
            "John T. Richards",
            "Steven I. Ross",
            "Fernando Martinez",
            "Mayank Agarwal",
            "Kartik Talamadupula"
        ],
        "citations": 108,
        "references": 102,
        "year": 2021
    },
    {
        "title": "Improved Techniques for Training Single-Image GANs",
        "abstract": "Recently there has been an interest in the potential of learning generative models from a single image, as opposed to from a large dataset. This task is of significance, as it means that generative models can be used in domains where collecting a large dataset is not feasible. However, training a model capable of generating realistic images from only a single sample is a difficult problem. In this work, we conduct a number of experiments to understand the challenges of training these methods and propose some best practices that we found allowed us to generate improved results over previous work. One key piece is that, unlike prior single image generation methods, we concurrently train several stages in a sequential multi-stage manner, allowing us to learn models with fewer stages of increasing image resolution. Compared to a recent state of the art baseline, our model is up to six times faster to train, has fewer parameters, and can better capture the global structure of images.",
        "authors": [
            "T. Hinz",
            "Matthew Fisher",
            "Oliver Wang",
            "Stefan Wermter"
        ],
        "citations": 133,
        "references": 61,
        "year": 2020
    },
    {
        "title": "High Fidelity Speech Synthesis with Adversarial Networks",
        "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Frechet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at this https URL.",
        "authors": [
            "Mikolaj Binkowski",
            "Jeff Donahue",
            "S. Dieleman",
            "Aidan Clark",
            "Erich Elsen",
            "Norman Casagrande",
            "Luis C. Cobo",
            "K. Simonyan"
        ],
        "citations": 232,
        "references": 58,
        "year": 2019
    },
    {
        "title": "MineGAN: Effective Knowledge Transfer From GANs to Target Domains With Few Images",
        "abstract": "One of the attractive characteristics of deep neural networks is their ability to transfer knowledge obtained in one domain to other related domains. As a result, high-quality networks can be trained in domains with relatively little training data. This property has been extensively studied for discriminative networks but has received significantly less attention for generative models. Given the often enormous effort required to train GANs, both computationally as well as in the dataset collection, the re-use of pretrained GANs is a desirable objective. We propose a novel knowledge transfer method for generative models based on mining the knowledge that is most beneficial to a specific target domain, either from a single or multiple pretrained GANs. This is done using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain. Mining effectively steers GAN sampling towards suitable regions of the latent space, which facilitates the posterior finetuning and avoids pathologies of other methods such as mode collapse and lack of flexibility. We perform experiments on several complex datasets using various GAN architectures (BigGAN, Progressive GAN) and show that the proposed method, called MineGAN, effectively transfers knowledge to domains with few target images, outperforming existing methods. In addition, MineGAN can successfully transfer knowledge from multiple pretrained GANs. Our code is available at: \\url{https://github.com/yaxingwang/MineGAN}.",
        "authors": [
            "Yaxing Wang",
            "Abel Gonzalez-Garcia",
            "David Berga",
            "Luis Herranz",
            "F. Khan",
            "Joost van de Weijer"
        ],
        "citations": 182,
        "references": 41,
        "year": 2019
    },
    {
        "title": "Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning",
        "abstract": null,
        "authors": [
            "Jike Wang",
            "Chang-Yu Hsieh",
            "Mingyang Wang",
            "Xiaorui Wang",
            "Zhenxing Wu",
            "Dejun Jiang",
            "B. Liao",
            "Xujun Zhang",
            "Bo Yang",
            "Qiaojun He",
            "Dongsheng Cao",
            "Xi Chen",
            "Tingjun Hou"
        ],
        "citations": 104,
        "references": 60,
        "year": 2021
    },
    {
        "title": "Differentiable Learning of Quantum Circuit Born Machine",
        "abstract": "Quantum circuit Born machines are generative models which represent the probability distribution of classical dataset as quantum pure states. Computational complexity considerations of the quantum sampling problem suggest that the quantum circuits exhibit stronger expressibility compared to classical neural networks. One can efficiently draw samples from the quantum circuits via projective measurements on qubits. However, similar to the leading implicit generative models in deep learning, such as the generative adversarial networks, the quantum circuits cannot provide the likelihood of the generated samples, which poses a challenge to the training. We devise an efficient gradient-based learning algorithm for the quantum circuit Born machine by minimizing the kerneled maximum mean discrepancy loss. We simulated generative modeling of the BARS-AND-STRIPES dataset and Gaussian mixture distributions using deep quantum circuits. Our experiments show the importance of circuit depth and the gradient-based optimization algorithm. The proposed learning algorithm is runnable on near-term quantum device and can exhibit quantum advantages for probabilistic generative modeling.",
        "authors": [
            "Jin-Guo Liu",
            "Lei Wang"
        ],
        "citations": 215,
        "references": 63,
        "year": 2018
    },
    {
        "title": "Graph networks for molecular design",
        "abstract": "Deep learning methods applied to chemistry can be used to accelerate the discovery of new molecules. This work introduces GraphINVENT, a platform developed for graph-based molecular design using graph neural networks (GNNs). GraphINVENT uses a tiered deep neural network architecture to probabilistically generate new molecules a single bond at a time. All models implemented in GraphINVENT can quickly learn to build molecules resembling the training set molecules without any explicit programming of chemical rules. The models have been benchmarked using the MOSES distribution-based metrics, showing how GraphINVENT models compare well with state-of-the-art generative models. This work compares six different GNN-based generative models in GraphINVENT, and shows that ultimately the gated-graph neural network performs best against the metrics considered here.",
        "authors": [
            "Rocío Mercado",
            "T. Rastemo",
            "Edvard Lindelöf",
            "G. Klambauer",
            "O. Engkvist",
            "Hongming Chen",
            "Esben Jannik Bjerrum"
        ],
        "citations": 153,
        "references": 78,
        "year": 2020
    },
    {
        "title": "Don’t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training",
        "abstract": "Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases. We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. We demonstrate the efficacy of our approach across several dialogue tasks.",
        "authors": [
            "Margaret Li",
            "Stephen Roller",
            "Ilia Kulikov",
            "S. Welleck",
            "Y-Lan Boureau",
            "Kyunghyun Cho",
            "J. Weston"
        ],
        "citations": 172,
        "references": 29,
        "year": 2019
    },
    {
        "title": "Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules",
        "abstract": "Deep learning has proven to yield fast and accurate predictions of quantum-chemical properties to accelerate the discovery of novel molecules and materials. As an exhaustive exploration of the vast chemical space is still infeasible, we require generative models that guide our search towards systems with desired properties. While graph-based models have previously been proposed, they are restricted by a lack of spatial information such that they are unable to recognize spatial isomerism and non-bonded interactions. Here, we introduce a generative neural network for 3d point sets that respects the rotational invariance of the targeted structures. We apply it to the generation of molecules and demonstrate its ability to approximate the distribution of equilibrium structures using spatial metrics as well as established measures from chemoinformatics. As our model is able to capture the complex relationship between 3d geometry and electronic properties, we bias the distribution of the generator towards molecules with a small HOMO-LUMO gap - an important property for the design of organic solar cells.",
        "authors": [
            "N. Gebauer",
            "M. Gastegger",
            "Kristof T. Schütt"
        ],
        "citations": 182,
        "references": 65,
        "year": 2019
    },
    {
        "title": "Lifelong GAN: Continual Learning for Conditional Image Generation",
        "abstract": "Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.",
        "authors": [
            "Mengyao Zhai",
            "Lei Chen",
            "Frederick Tung",
            "Jiawei He",
            "Megha Nawhal",
            "Greg Mori"
        ],
        "citations": 166,
        "references": 42,
        "year": 2019
    },
    {
        "title": "Learned motion matching",
        "abstract": "In this paper we present a learned alternative to the Motion Matching algorithm which retains the positive properties of Motion Matching but additionally achieves the scalability of neural-network-based generative models. Although neural-network-based generative models for character animation are capable of learning expressive, compact controllers from vast amounts of animation data, methods such as Motion Matching still remain a popular choice in the games industry due to their flexibility, predictability, low preprocessing time, and visual quality - all properties which can sometimes be difficult to achieve with neural-network-based methods. Yet, unlike neural networks, the memory usage of such methods generally scales linearly with the amount of data used, resulting in a constant trade-off between the diversity of animation which can be produced and real world production budgets. In this work we combine the benefits of both approaches and, by breaking down the Motion Matching algorithm into its individual steps, show how learned, scalable alternatives can be used to replace each operation in turn. Our final model has no need to store animation data or additional matching meta-data in memory, meaning it scales as well as existing generative models. At the same time, we preserve the behavior of Motion Matching, retaining the quality, control, and quick iteration time which are so important in the industry.",
        "authors": [
            "Daniel Holden",
            "O. Kanoun",
            "Maksym Perepichka",
            "T. Popa"
        ],
        "citations": 124,
        "references": 58,
        "year": 2020
    },
    {
        "title": "Preventing Posterior Collapse with delta-VAEs",
        "abstract": "Due to the phenomenon of\"posterior collapse,\"current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed $\\delta$-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet $32\\times 32$.",
        "authors": [
            "Ali Razavi",
            "Aäron van den Oord",
            "Ben Poole",
            "O. Vinyals"
        ],
        "citations": 166,
        "references": 51,
        "year": 2019
    },
    {
        "title": "DaST: Data-Free Substitute Training for Adversarial Attacks",
        "abstract": "Machine learning models are vulnerable to adversarial examples. For the black-box setting, current substitute attacks need pre-trained models to generate adversarial examples. However, pre-trained models are hard to obtain in real-world tasks. In this paper, we propose a data-free substitute training method (DaST) to obtain substitute models for adversarial black-box attacks without the requirement of any real data. To achieve this, DaST utilizes specially designed generative adversarial networks (GANs) to train the substitute models. In particular, we design a multi-branch architecture and label-control loss for the generative model to deal with the uneven distribution of synthetic samples. The substitute model is then trained by the synthetic samples generated by the generative model, which are labeled by the attacked model subsequently. The experiments demonstrate the substitute models produced by DaST can achieve competitive performance compared with the baseline models which are trained by the same train set with attacked models. Additionally, to evaluate the practicability of the proposed method on the real-world task, we attack an online machine learning model on the Microsoft Azure platform. The remote model misclassifies 98.35% of the adversarial examples crafted by our method. To the best of our knowledge, we are the first to train a substitute model for adversarial attacks without any real data.",
        "authors": [
            "Mingyi Zhou",
            "Jing Wu",
            "Yipeng Liu",
            "Shuaicheng Liu",
            "Ce Zhu"
        ],
        "citations": 134,
        "references": 41,
        "year": 2020
    },
    {
        "title": "A Survey on Adversarial Recommender Systems",
        "abstract": "Latent-factor models (LFM) based on collaborative filtering (CF), such as matrix factorization (MF) and deep CF methods, are widely used in modern recommender systems (RS) due to their excellent performance and recommendation accuracy. However, success has been accompanied with a major new arising challenge: Many applications of machine learning (ML) are adversarial in nature [146]. In recent years, it has been shown that these methods are vulnerable to adversarial examples, i.e., subtle but non-random perturbations designed to force recommendation models to produce erroneous outputs. The goal of this survey is two-fold: (i) to present recent advances on adversarial machine learning (AML) for the security of RS (i.e., attacking and defense recommendation models) and (ii) to show another successful application of AML in generative adversarial networks (GANs) for generative applications, thanks to their ability for learning (high-dimensional) data distributions. In this survey, we provide an exhaustive literature review of 76 articles published in major RS and ML journals and conferences. This review serves as a reference for the RS community working on the security of RS or on generative models using GANs to improve their quality.",
        "authors": [
            "Yashar Deldjoo",
            "T. D. Noia",
            "Felice Antonio Merra"
        ],
        "citations": 162,
        "references": 193,
        "year": 2020
    },
    {
        "title": "Memory Replay GANs: learning to generate images from new categories without forgetting",
        "abstract": "Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories.",
        "authors": [
            "Chenshen Wu",
            "Luis Herranz",
            "Xialei Liu",
            "Yaxing Wang",
            "Joost van de Weijer",
            "B. Raducanu"
        ],
        "citations": 188,
        "references": 33,
        "year": 2018
    },
    {
        "title": "GANobfuscator: Mitigating Information Leakage Under GAN via Differential Privacy",
        "abstract": "By learning generative models of semantic-rich data distributions from samples, generative adversarial network (GAN) has recently attracted intensive research interests due to its excellent empirical performance as a generative model. The model is used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, GANs can easily remember training samples due to the high model complexity of deep networks. When GANs are applied to private or sensitive data, the concentration of distribution may divulge some critical information. It consequently requires new technological advances to mitigate the information leakage under GANs. To address this issue, we propose GANobfuscator, a differentially private GAN, which can achieve differential privacy under GANs by adding carefully designed noise to gradients during the learning procedure. With GANobfuscator, analysts are able to generate an unlimited amount of synthetic data for arbitrary analysis tasks without disclosing the privacy of training data. Moreover, we theoretically prove that GANobfuscator can provide strict privacy guarantee with differential privacy. In addition, we develop a gradient-pruning strategy for GANobfuscator to improve the scalability and stability of data training. Through extensive experimental evaluation on benchmark datasets, we demonstrate that GANobfuscator can produce high-quality generated data and retain desirable utility under practical privacy budgets.",
        "authors": [
            "Chugui Xu",
            "Ju Ren",
            "Deyu Zhang",
            "Yaoxue Zhang",
            "Zhan Qin",
            "K. Ren"
        ],
        "citations": 174,
        "references": 0,
        "year": 2019
    },
    {
        "title": "Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders",
        "abstract": "Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For example, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model. In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity. We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints. Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.",
        "authors": [
            "Tengfei Ma",
            "Jie Chen",
            "Cao Xiao"
        ],
        "citations": 201,
        "references": 45,
        "year": 2018
    },
    {
        "title": "Integer Discrete Flows and Lossless Compression",
        "abstract": "Lossless compression methods shorten the expected representation size of data without loss of information, using a statistical model. Flow-based models are attractive in this setting because they admit exact likelihood optimization, which is equivalent to minimizing the expected number of bits per message. However, conventional flows assume continuous data, which may lead to reconstruction errors when quantized for compression. For that reason, we introduce a flow-based generative model for ordinal discrete data called Integer Discrete Flow (IDF): a bijective integer map that can learn rich transformations on high-dimensional data. As building blocks for IDFs, we introduce a flexible transformation layer called integer discrete coupling. Our experiments show that IDFs are competitive with other flow-based generative models. Furthermore, we demonstrate that IDF based compression achieves state-of-the-art lossless compression rates on CIFAR10, ImageNet32, and ImageNet64. To the best of our knowledge, this is the first lossless compression method that uses invertible neural networks.",
        "authors": [
            "Emiel Hoogeboom",
            "Jorn W. T. Peters",
            "Rianne van den Berg",
            "M. Welling"
        ],
        "citations": 154,
        "references": 41,
        "year": 2019
    },
    {
        "title": "Anomaly Detection of Time Series With Smoothness-Inducing Sequential Variational Auto-Encoder",
        "abstract": "Deep generative models have demonstrated their effectiveness in learning latent representation and modeling complex dependencies of time series. In this article, we present a smoothness-inducing sequential variational auto-encoder (VAE) (SISVAE) model for the robust estimation and anomaly detection of multidimensional time series. Our model is based on VAE, and its backbone is fulfilled by a recurrent neural network to capture latent temporal structures of time series for both the generative model and the inference model. Specifically, our model parameterizes mean and variance for each time-stamp with flexible neural networks, resulting in a nonstationary model that can work without the assumption of constant noise as commonly made by existing Markov models. However, such flexibility may cause the model fragile to anomalies. To achieve robust density estimation which can also benefit detection tasks, we propose a smoothness-inducing prior over possible estimations. The proposed prior works as a regularizer that places penalty at nonsmooth reconstructions. Our model is learned efficiently with a novel stochastic gradient variational Bayes estimator. In particular, we study two decision criteria for anomaly detection: reconstruction probability and reconstruction error. We show the effectiveness of our model on both synthetic data sets and public real-world benchmarks.",
        "authors": [
            "Longyuan Li",
            "Junchi Yan",
            "Haiyang Wang",
            "Yaohui Jin"
        ],
        "citations": 121,
        "references": 62,
        "year": 2020
    },
    {
        "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step",
        "abstract": "Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.",
        "authors": [
            "W. Fedus",
            "Mihaela Rosca",
            "Balaji Lakshminarayanan",
            "Andrew M. Dai",
            "S. Mohamed",
            "I. Goodfellow"
        ],
        "citations": 206,
        "references": 26,
        "year": 2017
    },
    {
        "title": "A Probabilistic Formulation of Unsupervised Text Style Transfer",
        "abstract": "We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art.",
        "authors": [
            "Junxian He",
            "Xinyi Wang",
            "Graham Neubig",
            "Taylor Berg-Kirkpatrick"
        ],
        "citations": 124,
        "references": 37,
        "year": 2020
    },
    {
        "title": "The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement",
        "abstract": null,
        "authors": [
            "William S. Peebles",
            "John Peebles",
            "Jun-Yan Zhu",
            "Alexei A. Efros",
            "A. Torralba"
        ],
        "citations": 111,
        "references": 49,
        "year": 2020
    },
    {
        "title": "GAN Memory with No Forgetting",
        "abstract": "Seeking to address the fundamental issue of memory in lifelong learning, we propose a GAN memory that is capable of realistically remembering a stream of generative processes with \\emph{no} forgetting. Our GAN memory is based on recognizing that one can modulate the ``style'' of a GAN model to form perceptually-distant targeted generation. Accordingly, we propose to do sequential style modulations atop a well-behaved base GAN model, to form sequential targeted generative models, while simultaneously benefiting from the transferred base knowledge. Experiments demonstrate the superiority of our method over existing approaches and its effectiveness in alleviating catastrophic forgetting for lifelong classification problems.",
        "authors": [
            "Yulai Cong",
            "Miaoyun Zhao",
            "Jianqiao Li",
            "Sijia Wang",
            "L. Carin"
        ],
        "citations": 112,
        "references": 101,
        "year": 2020
    },
    {
        "title": "Adversarial score matching and improved sampling for image generation",
        "abstract": "Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has recently found success in generative modeling. The approach works by first training a neural network to estimate the score of a distribution, and then using Langevin dynamics to sample from the data distribution assumed by the score network. Despite the convincing visual quality of samples, this method appears to perform worse than Generative Adversarial Networks (GANs) under the Frechet Inception Distance, a standard metric for generative models. \nWe show that this apparent gap vanishes when denoising the final Langevin samples using the score network. In addition, we propose two improvements to DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we elevate score matching methods and obtain results competitive with state-of-the-art image generation on CIFAR-10.",
        "authors": [
            "Alexia Jolicoeur-Martineau",
            "Remi Piche-Taillefer",
            "Rémi Tachet des Combes",
            "Ioannis Mitliagkas"
        ],
        "citations": 117,
        "references": 60,
        "year": 2020
    },
    {
        "title": "SPG-Net: Segmentation Prediction and Guidance Network for Image Inpainting",
        "abstract": "In this paper, we focus on image inpainting task, aiming at recovering the missing area of an incomplete image given the context information. Recent development in deep generative models enables an efficient end-to-end framework for image synthesis and inpainting tasks, but existing methods based on generative models don't exploit the segmentation information to constrain the object shapes, which usually lead to blurry results on the boundary. To tackle this problem, we propose to introduce the semantic segmentation information, which disentangles the inter-class difference and intra-class variation for image inpainting. This leads to much clearer recovered boundary between semantically different regions and better texture within semantically consistent segments. Our model factorizes the image inpainting process into segmentation prediction (SP-Net) and segmentation guidance (SG-Net) as two steps, which predict the segmentation labels in the missing area first, and then generate segmentation guided inpainting results. Experiments on multiple public datasets show that our approach outperforms existing methods in optimizing the image inpainting quality, and the interactive segmentation guidance provides possibilities for multi-modal predictions of image inpainting.",
        "authors": [
            "Yuhang Song",
            "Chao Yang",
            "Yeji Shen",
            "Peng Wang",
            "Qin Huang",
            "C.-C. Jay Kuo"
        ],
        "citations": 171,
        "references": 44,
        "year": 2018
    },
    {
        "title": "SiCloPe: Silhouette-Based Clothed People",
        "abstract": "We introduce a new silhouette-based representation for modeling clothed human bodies using deep generative models. Our method can reconstruct a complete and textured 3D model of a person wearing clothes from a single input picture. Inspired by the visual hull algorithm, our implicit representation uses 2D silhouettes and 3D joints of a body pose to describe the immense shape complexity and variations of clothed people. Given a segmented 2D silhouette of a person and its inferred 3D joints from the input picture, we first synthesize consistent silhouettes from novel view points around the subject. The synthesized silhouettes which are the most consistent with the input segmentation are fed into a deep visual hull algorithm for robust 3D shape prediction. We then infer the texture of the subject's back view using the frontal image and segmentation mask as input to a conditional generative adversarial network. Our experiments demonstrate that our silhouette-based model is an effective representation and the appearance of the back view can be predicted reliably using an image-to-image translation network. While classic methods based on parametric models often fail for single-view images of subjects with challenging clothing, our approach can still produce successful results, which are comparable to those obtained from multi-view input.",
        "authors": [
            "Ryota Natsume",
            "Shunsuke Saito",
            "Zeng Huang",
            "Weikai Chen",
            "Chongyang Ma",
            "Hao Li",
            "S. Morishima"
        ],
        "citations": 173,
        "references": 63,
        "year": 2018
    },
    {
        "title": "Entangled Conditional Adversarial Autoencoder for de Novo Drug Discovery.",
        "abstract": "Modern computational approaches and machine learning techniques accelerate the invention of new drugs. Generative models can discover novel molecular structures within hours, while conventional drug discovery pipelines require months of work. In this article, we propose a new generative architecture, entangled conditional adversarial autoencoder, that generates molecular structures based on various properties, such as activity against a specific protein, solubility, or ease of synthesis. We apply the proposed model to generate a novel inhibitor of Janus kinase 3, implicated in rheumatoid arthritis, psoriasis, and vitiligo. The discovered molecule was tested in vitro and showed good activity and selectivity.",
        "authors": [
            "Daniil Polykovskiy",
            "Alexander Zhebrak",
            "D. Vetrov",
            "Y. Ivanenkov",
            "V. Aladinskiy",
            "Polina Mamoshina",
            "M. Bozdaganyan",
            "A. Aliper",
            "A. Zhavoronkov",
            "Artur Kadurin"
        ],
        "citations": 202,
        "references": 61,
        "year": 2018
    },
    {
        "title": "High-Fidelity Image Generation With Fewer Labels",
        "abstract": "Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels and outperform it using 20% of the labels.",
        "authors": [
            "Mario Lucic",
            "M. Tschannen",
            "Marvin Ritter",
            "Xiaohua Zhai",
            "Olivier Bachem",
            "S. Gelly"
        ],
        "citations": 152,
        "references": 45,
        "year": 2019
    },
    {
        "title": "Compressed Sensing with Deep Image Prior and Learned Regularization",
        "abstract": "We propose a novel method for compressed sensing recovery using untrained deep generative models. Our method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. We show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods. Unlike various learned approaches based on generative models, our method does not require pre-training over large datasets. We further introduce a novel learned regularization technique, which incorporates prior information on the network weights. This reduces reconstruction error, especially for noisy measurements. Finally, we prove that single-layer DIP networks with constant fraction over-parameterization will perfectly fit any signal through gradient descent, despite being a non-convex problem. This theoretical result provides justification for early stopping.",
        "authors": [
            "Dave Van Veen",
            "A. Jalal",
            "Eric Price",
            "S. Vishwanath",
            "A. Dimakis"
        ],
        "citations": 177,
        "references": 90,
        "year": 2018
    },
    {
        "title": "A tale of two densities: active inference is enactive inference",
        "abstract": "The aim of this article is to clarify how best to interpret some of the central constructs that underwrite the free-energy principle (FEP) – and its corollary, active inference – in theoretical neuroscience and biology: namely, the role that generative models and variational densities play in this theory. We argue that these constructs have been systematically misrepresented in the literature, because of the conflation between the FEP and active inference, on the one hand, and distinct (albeit closely related) Bayesian formulations, centred on the brain – variously known as predictive processing, predictive coding or the prediction error minimisation framework. More specifically, we examine two contrasting interpretations of these models: a structural representationalist interpretation and an enactive interpretation. We argue that the structural representationalist interpretation of generative and recognition models does not do justice to the role that these constructs play in active inference under the FEP. We propose an enactive interpretation of active inference – what might be called enactive inference. In active inference under the FEP, the generative and recognition models are best cast as realising inference and control – the self-organising, belief-guided selection of action policies – and do not have the properties ascribed by structural representationalists.",
        "authors": [
            "M. Ramstead",
            "Michael D. Kirchhoff",
            "Karl J. Friston"
        ],
        "citations": 151,
        "references": 65,
        "year": 2019
    },
    {
        "title": "APE-GAN: Adversarial Perturbation Elimination with GAN",
        "abstract": "Although Deep Neural Networks could achieve state-of-the-art performance while recongnizing images, they often suffer a tremendous defeat from adversarial examples–inputs generated by utilizing imperceptible but intentional perturbations to samples from the datasets. So far, very few methods have provided a significant defense to adversarial examples. In this paper, an effective framework based Generative Adversarial Nets(GAN) is proposed to defense against the adversarial examples. The essense of the model is to eliminate the adversarial perturbations being highly aligned with the weight vectors of nueral models. Extensive experiments on benchmark datasets MNIST, CIFAR10 and ImageNet indicate that our framework is able to defense against adversarial examples effectively.",
        "authors": [
            "Guoqing Jin",
            "Shiwei Shen",
            "Dongming Zhang",
            "Feng Dai",
            "Yongdong Zhang"
        ],
        "citations": 207,
        "references": 28,
        "year": 2017
    },
    {
        "title": "Recurrent Neural Network Model for Constructive Peptide Design",
        "abstract": "We present a generative long short-term memory (LSTM) recurrent neural network (RNN) for combinatorial de novo peptide design. RNN models capture patterns in sequential data and generate new data instances from the learned context. Amino acid sequences represent a suitable input for these machine-learning models. Generative models trained on peptide sequences could therefore facilitate the design of bespoke peptide libraries. We trained RNNs with LSTM units on pattern recognition of helical antimicrobial peptides and used the resulting model for de novo sequence generation. Of these sequences, 82% were predicted to be active antimicrobial peptides compared to 65% of randomly sampled sequences with the same amino acid distribution as the training set. The generated sequences also lie closer to the training data than manually designed amphipathic helices. The results of this study showcase the ability of LSTM RNNs to construct new amino acid sequences within the applicability domain of the model and motivate their prospective application to peptide and protein design without the need for the exhaustive enumeration of sequence libraries.",
        "authors": [
            "A. T. Müller",
            "J. Hiss",
            "G. Schneider"
        ],
        "citations": 169,
        "references": 0,
        "year": 2018
    },
    {
        "title": "Learning Plannable Representations with Causal InfoGAN",
        "abstract": "In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans – a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.",
        "authors": [
            "Thanard Kurutach",
            "Aviv Tamar",
            "Ge Yang",
            "Stuart J. Russell",
            "P. Abbeel"
        ],
        "citations": 174,
        "references": 63,
        "year": 2018
    },
    {
        "title": "The secret life of predictive brains: what’s spontaneous activity for?",
        "abstract": "Brains at rest generate dynamical activity that is highly structured in space and time. We suggest that spontaneous activity, as in rest or dreaming, underlies top-down dynamics of generative models. During active tasks, generative models provide top-down predictive signals for perception, cognition, and action. When the brain is at rest and stimuli are weak or absent, top-down dynamics optimize the generative models for future interactions by maximizing the entropy of explanations and minimizing model complexity. Spontaneous fluctuations of correlated activity within and across brain regions may reflect transitions between 'generic priors' of the generative model: low dimensional latent variables and connectivity patterns of the most common perceptual, motor, cognitive, and interoceptive states. Even at rest, brains are proactive and predictive.",
        "authors": [
            "G. Pezzulo",
            "M. Zorzi",
            "M. Corbetta"
        ],
        "citations": 103,
        "references": 0,
        "year": 2020
    },
    {
        "title": "Learning to See Physics via Visual De-animation",
        "abstract": "We introduce a paradigm for understanding physical scenes without human annotations. At the core of our system is a physical world representation that is first recovered by a perception module and then utilized by physics and graphics engines. During training, the perception module and the generative models learn by visual de-animation --- interpreting and reconstructing the visual information stream. During testing, the system first recovers the physical world state, and then uses the generative models for reasoning and future prediction. Even more so than forward simulation, inverting a physics or graphics engine is a computationally hard problem; we overcome this challenge by using a convolutional inversion network. Our system quickly recognizes the physical world state from appearance and motion cues, and has the flexibility to incorporate both differentiable and non-differentiable physics and graphics engines. We evaluate our system on both synthetic and real datasets involving multiple physical scenes, and demonstrate that our system performs well on both physical state estimation and reasoning problems. We further show that the knowledge learned on the synthetic dataset generalizes to constrained real images.",
        "authors": [
            "Jiajun Wu",
            "Erika Lu",
            "Pushmeet Kohli",
            "Bill Freeman",
            "J. Tenenbaum"
        ],
        "citations": 198,
        "references": 47,
        "year": 2017
    },
    {
        "title": "A Large-Scale Study on Regularization and Normalization in GANs",
        "abstract": "Generative adversarial networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant number of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of \"tricks\". The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, as well as neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We discuss and evaluate common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.",
        "authors": [
            "Karol Kurach",
            "Mario Lucic",
            "Xiaohua Zhai",
            "Marcin Michalski",
            "S. Gelly"
        ],
        "citations": 171,
        "references": 37,
        "year": 2018
    },
    {
        "title": "Robust Imitation of Diverse Behaviors",
        "abstract": "Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.",
        "authors": [
            "Ziyun Wang",
            "J. Merel",
            "Scott E. Reed",
            "Nando de Freitas",
            "Greg Wayne",
            "N. Heess"
        ],
        "citations": 189,
        "references": 45,
        "year": 2017
    },
    {
        "title": "Transferable Adversarial Attacks for Image and Video Object Detection",
        "abstract": "Identifying adversarial examples is beneficial for understanding deep networks and developing robust models. However, existing attacking methods for image object detection have two limitations: weak transferability---the generated adversarial examples often have a low success rate to attack other kinds of detection methods, and high computation cost---they need much time to deal with video data, where many frames need polluting. To address these issues, we present a generative method to obtain adversarial images and videos, thereby significantly reducing the processing time. To enhance transferability, we manipulate the feature maps extracted by a feature network, which usually constitutes the basis of object detectors. Our method is based on the Generative Adversarial Network (GAN) framework, where we combine a high-level class loss and a low-level feature loss to jointly train the adversarial example generator. Experimental results on PASCAL VOC and ImageNet VID datasets show that our method efficiently generates image and video adversarial examples, and more importantly, these adversarial examples have better transferability, therefore being able to simultaneously attack two kinds of  representative object detection models: proposal based models like Faster-RCNN and regression based models like SSD.",
        "authors": [
            "Xingxing Wei",
            "Siyuan Liang",
            "Xiaochun Cao",
            "Jun Zhu"
        ],
        "citations": 204,
        "references": 45,
        "year": 2018
    },
    {
        "title": "RSGAN: face swapping and editing using face and hair representation in latent spaces",
        "abstract": "This abstract introduces a generative neural network for face swapping and editing face images. We refer to this network as \"region-separative generative adversarial network (RSGAN)\". In existing deep generative models such as Variational autoencoder (VAE) and Generative adversarial network (GAN), training data must represent what the generative models synthesize. For example, image inpainting is achieved by training images with and without holes. However, it is difficult or even impossible to prepare a dataset which includes face images both before and after face swapping because faces of real people cannot be swapped without surgical operations. We tackle this problem by training the network so that it synthesizes synthesize a natural face image from an arbitrary pair of face and hair appearances. In addition to face swapping, the proposed network can be applied to other editing applications, such as visual attribute editing and random face parts synthesis.",
        "authors": [
            "Ryota Natsume",
            "Tatsuya Yatagawa",
            "S. Morishima"
        ],
        "citations": 161,
        "references": 52,
        "year": 2018
    },
    {
        "title": "Graph Normalizing Flows",
        "abstract": "We introduce graph normalizing flows: a new, reversible graph neural network model for prediction and generation. On supervised tasks, graph normalizing flows perform similarly to message passing neural networks, but at a significantly reduced memory footprint, allowing them to scale to larger graphs. In the unsupervised case, we combine graph normalizing flows with a novel graph auto-encoder to create a generative model of graph structures. Our model is permutation-invariant, generating entire graphs with a single feed-forward pass, and achieves competitive results with the state-of-the art auto-regressive models, while being better suited to parallel computing architectures.",
        "authors": [
            "Jenny Liu",
            "Aviral Kumar",
            "Jimmy Ba",
            "J. Kiros",
            "Kevin Swersky"
        ],
        "citations": 144,
        "references": 30,
        "year": 2019
    },
    {
        "title": "Semi-Supervised and Task-Driven Data Augmentation",
        "abstract": null,
        "authors": [
            "K. Chaitanya",
            "Neerav Karani",
            "C. Baumgartner",
            "O. Donati",
            "Anton S. Becker",
            "E. Konukoglu"
        ],
        "citations": 135,
        "references": 21,
        "year": 2019
    },
    {
        "title": "Learning Independent Causal Mechanisms",
        "abstract": "Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the first place. From the point of view of causal modeling, the structure of each distribution is induced by physical mechanisms that give rise to dependences between observables. Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a particular entailed data distribution, lending themselves to transfer between problems. We develop an algorithm to recover a set of independent (inverse) mechanisms from a set of transformed data points. The approach is unsupervised and based on a set of experts that compete for data generated by the mechanisms, driving specialization. We analyze the proposed method in a series of experiments on image data. Each expert learns to map a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss implications for transfer learning and links to recent trends in generative modeling.",
        "authors": [
            "Giambattista Parascandolo",
            "Mateo Rojas-Carulla",
            "Niki Kilbertus",
            "B. Scholkopf"
        ],
        "citations": 175,
        "references": 24,
        "year": 2017
    },
    {
        "title": "The GAN Landscape: Losses, Architectures, Regularization, and Normalization",
        "abstract": "Generative adversarial networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant amount of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of \"tricks\". The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, and neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We reproduce the current state of the art and go beyond fairly exploring the GAN landscape. We discuss common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.",
        "authors": [
            "Karol Kurach",
            "Mario Lucic",
            "Xiaohua Zhai",
            "Marcin Michalski",
            "S. Gelly"
        ],
        "citations": 151,
        "references": 42,
        "year": 2018
    },
    {
        "title": "Feedback GAN for DNA optimizes protein functions",
        "abstract": null,
        "authors": [
            "Anvita Gupta",
            "James Zou"
        ],
        "citations": 159,
        "references": 29,
        "year": 2019
    },
    {
        "title": "Quantum variational autoencoder",
        "abstract": "Variational autoencoders (VAEs) are powerful generative models with the salient ability to perform inference. Here, we introduce a quantum variational autoencoder (QVAE): a VAE whose latent generative process is implemented as a quantum Boltzmann machine (QBM). We show that our model can be trained end-to-end by maximizing a well-defined loss-function: a ‘quantum’ lower-bound to a variational approximation of the log-likelihood. We use quantum Monte Carlo (QMC) simulations to train and evaluate the performance of QVAEs. To achieve the best performance, we first create a VAE platform with discrete latent space generated by a restricted Boltzmann machine. Our model achieves state-of-the-art performance on the MNIST dataset when compared against similar approaches that only involve discrete variables in the generative process. We consider QVAEs with a smaller number of latent units to be able to perform QMC simulations, which are computationally expensive. We show that QVAEs can be trained effectively in regimes where quantum effects are relevant despite training via the quantum bound. Our findings open the way to the use of quantum computers to train QVAEs to achieve competitive performance for generative models. Placing a QBM in the latent space of a VAE leverages the full potential of current and next-generation quantum computers as sampling devices.",
        "authors": [
            "Amir Khoshaman",
            "W. Vinci",
            "Brandon Denis",
            "E. Andriyash",
            "Hossein Sadeghi",
            "Mohammad H. Amin"
        ],
        "citations": 155,
        "references": 76,
        "year": 2018
    },
    {
        "title": "Error Bounds of Imitating Policies and Environments for Reinforcement Learning",
        "abstract": "In sequential decision-making, imitation learning (IL) trains a policy efficiently by mimicking expert demonstrations. Various imitation methods were proposed and empirically evaluated, meanwhile, their theoretical understandings need further studies, among which the compounding error in long-horizon decisions is a major issue. In this paper, we first analyze the value gap between the expert policy and imitated policies by two imitation methods, behavioral cloning (BC) and generative adversarial imitation. The results support that generative adversarial imitation can reduce the compounding error compared to BC. Furthermore, we establish the lower bounds of IL under two settings, suggesting the significance of environment interactions in IL. By considering the environment transition model as a dual agent, IL can also be used to learn the environment model. Therefore, based on the bounds of imitating policies, we further analyze the performance of imitating environments. The results show that environment models can be more effectively imitated by generative adversarial imitation than BC. Particularly, we obtain a policy evaluation error that is linear with the effective planning horizon w.r.t. the model bias, suggesting a novel application of adversarial imitation for model-based reinforcement learning (MBRL). We hope these results could inspire future advances in IL and MBRL.",
        "authors": [
            "Tian Xu",
            "Ziniu Li",
            "Yang Yu"
        ],
        "citations": 107,
        "references": 70,
        "year": 2020
    },
    {
        "title": "Auto-Encoding Sequential Monte Carlo",
        "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",
        "authors": [
            "T. Le",
            "Maximilian Igl",
            "Tom Rainforth",
            "Tom Jin",
            "Frank Wood"
        ],
        "citations": 151,
        "references": 42,
        "year": 2017
    },
    {
        "title": "Conditional GAN with Discriminative Filter Generation for Text-to-Video Synthesis",
        "abstract": "Developing conditional generative models for text-to-video synthesis is an extremely challenging yet an important topic of research in machine learning. In this work, we address this problem by introducing Text-Filter conditioning Generative Adversarial Network (TFGAN), a conditional GAN model with a novel multi-scale text-conditioning scheme that improves text-video associations. By combining the proposed conditioning scheme with a deep GAN architecture, TFGAN generates high quality videos from text on challenging real-world video datasets. In addition, we construct a synthetic dataset of text-conditioned moving shapes to systematically evaluate our conditioning scheme. Extensive experiments demonstrate that TFGAN significantly outperforms existing approaches, and can also generate videos of novel categories not seen during training.",
        "authors": [
            "Y. Balaji",
            "Martin Renqiang Min",
            "Bing Bai",
            "R. Chellappa",
            "H. Graf"
        ],
        "citations": 122,
        "references": 22,
        "year": 2019
    },
    {
        "title": "Discrete Point Flow Networks for Efficient Point Cloud Generation",
        "abstract": null,
        "authors": [
            "Roman Klokov",
            "Edmond Boyer",
            "J. Verbeek"
        ],
        "citations": 96,
        "references": 57,
        "year": 2020
    },
    {
        "title": "3-D Inorganic Crystal Structure Generation and Property Prediction via Representation Learning",
        "abstract": "Generative models have been successfully used to synthesize completely novel images, text, music, and speech. As such, they present an exciting opportunity for the design of new materials for functional applications. So far, generative deep-learning methods applied to molecular and drug discovery have yet to produce stable and novel 3-D crystal structures across multiple material classes. To that end, we, herein, present an autoencoder-based generative deep-representation learning pipeline for geometrically optimized 3-D crystal structures that simultaneously predicts the values of eight target properties. The system is highly general, as demonstrated through creation of novel materials from three separate material classes: binary alloys, ternary perovskites, and Heusler compounds. Comparison of these generated structures to those optimized via electronic-structure calculations shows that our generated materials are valid and geometrically optimized.",
        "authors": [
            "Callum J Court",
            "Batuhan Yildirim",
            "Apoorv Jain",
            "J. Cole"
        ],
        "citations": 93,
        "references": 80,
        "year": 2020
    },
    {
        "title": "CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training",
        "abstract": "We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in fine-grained categories, such as faces of a specific person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the fine-grained category label fed into the resulting generative model, we can generate images in a specific category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classifier network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the GAN training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, flowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with fine-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.",
        "authors": [
            "Jianmin Bao",
            "Dong Chen",
            "Fang Wen",
            "Houqiang Li",
            "Gang Hua"
        ],
        "citations": 193,
        "references": 50,
        "year": 2017
    },
    {
        "title": "VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation",
        "abstract": "Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive models, or do not directly optimize the likelihood of the data. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions. We describe an approach for modeling the latent space dynamics, and demonstrate that flow-based generative models offer a viable and competitive approach to generative modelling of video.",
        "authors": [
            "Manoj Kumar",
            "M. Babaeizadeh",
            "D. Erhan",
            "Chelsea Finn",
            "S. Levine",
            "Laurent Dinh",
            "Durk Kingma"
        ],
        "citations": 126,
        "references": 59,
        "year": 2019
    },
    {
        "title": "Latent Normalizing Flows for Discrete Sequences",
        "abstract": "Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.",
        "authors": [
            "Zachary M. Ziegler",
            "Alexander M. Rush"
        ],
        "citations": 119,
        "references": 40,
        "year": 2019
    },
    {
        "title": "Invertible Zero-Shot Recognition Flows",
        "abstract": null,
        "authors": [
            "Yuming Shen",
            "Jie Qin",
            "Lei Huang"
        ],
        "citations": 90,
        "references": 74,
        "year": 2020
    },
    {
        "title": "Weakly Supervised Sequence Tagging from Noisy Rules",
        "abstract": "We propose a framework for training sequence tagging models with weak supervision consisting of multiple heuristic rules of unknown accuracy. In addition to supporting rules that vote on tags in the output sequence, we introduce a new type of weak supervision, called linking rules, that vote on how sequence elements should be grouped into spans with the same tag. These rules are an alternative to candidate span generators that require significantly more human effort. To estimate the accuracies of the rules and combine their conflicting outputs into training data, we introduce a new type of generative model, linked hidden Markov models (linked HMMs), and prove they are generically identifiable (up to a tag permutation) without any observed training labels. We find that linked HMMs provide an average 7 F1 point boost on benchmark named entity recognition tasks versus generative models that assume the tags are i.i.d. Further, neural sequence taggers trained with these structure-aware generative models outperform comparable state-of-the-art approaches to weak supervision by an average of 2.6 F1 points.",
        "authors": [
            "Esteban Safranchik",
            "Shiying Luo",
            "Stephen H. Bach"
        ],
        "citations": 84,
        "references": 44,
        "year": 2020
    },
    {
        "title": "PaDGAN: Learning to Generate High-Quality Novel Designs",
        "abstract": "\n Deep generative models are proven to be a useful tool for automatic design synthesis and design space exploration. When applied in engineering design, existing generative models face three challenges: (1) generated designs lack diversity and do not cover all areas of the design space, (2) it is difficult to explicitly improve the overall performance or quality of generated designs, and (3) existing models generally do not generate novel designs, outside the domain of the training data. In this article, we simultaneously address these challenges by proposing a new determinantal point process-based loss function for probabilistic modeling of diversity and quality. With this new loss function, we develop a variant of the generative adversarial network, named “performance augmented diverse generative adversarial network” (PaDGAN), which can generate novel high-quality designs with good coverage of the design space. By using three synthetic examples and one real-world airfoil design example, we demonstrate that PaDGAN can generate diverse and high-quality designs. In comparison to a vanilla generative adversarial network, on average, it generates samples with a 28% higher mean quality score with larger diversity and without the mode collapse issue. Unlike typical generative models that usually generate new designs by interpolating within the boundary of training data, we show that PaDGAN expands the design space boundary outside the training data towards high-quality regions. The proposed method is broadly applicable to many tasks including design space exploration, design optimization, and creative solution recommendation.",
        "authors": [
            "Wei Chen",
            "Faez Ahmed"
        ],
        "citations": 58,
        "references": 49,
        "year": 2021
    },
    {
        "title": "Learning Latent Subspaces in Variational Autoencoders",
        "abstract": "Variational autoencoders (VAEs) are widely used deep generative models capable of learning unsupervised latent representations of data. Such representations are often difficult to interpret or control. We consider the problem of unsupervised learning of features correlated to specific labels in a dataset. We propose a VAE-based generative model which we show is capable of extracting features correlated to binary labels in the data and structuring it in a latent subspace which is easy to interpret. Our model, the Conditional Subspace VAE (CSVAE), uses mutual information minimization to learn a low-dimensional latent subspace associated with each label that can easily be inspected and independently manipulated. We demonstrate the utility of the learned representations for attribute manipulation tasks on both the Toronto Face and CelebA datasets.",
        "authors": [
            "Jack Klys",
            "Jake Snell",
            "R. Zemel"
        ],
        "citations": 133,
        "references": 28,
        "year": 2018
    },
    {
        "title": "Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias",
        "abstract": "Facial analysis models are increasingly used in applications that have serious impacts on people's lives, ranging from authentication to surveillance tracking. It is therefore critical to develop techniques that can reveal unintended biases in facial classifiers to help guide the ethical use of facial analysis technology. This work proposes a framework called \\textit{image counterfactual sensitivity analysis}, which we explore as a proof-of-concept in analyzing a smiling attribute classifier trained on faces of celebrities. The framework utilizes counterfactuals to examine how a classifier's prediction changes if a face characteristic slightly changes. We leverage recent advances in generative adversarial networks to build a realistic generative model of face images that affords controlled manipulation of specific image characteristics. We then introduce a set of metrics that measure the effect of manipulating a specific property on the output of the trained classifier. Empirically, we find several different factors of variation that affect the predictions of the smiling classifier. This proof-of-concept demonstrates potential ways generative models can be leveraged for fine-grained analysis of bias and fairness.",
        "authors": [
            "Emily L. Denton",
            "B. Hutchinson",
            "Margaret Mitchell",
            "Timnit Gebru"
        ],
        "citations": 127,
        "references": 46,
        "year": 2019
    },
    {
        "title": "Get The Point of My Utterance! Learning Towards Effective Responses with Multi-Head Attention Mechanism",
        "abstract": "Attention mechanism has become a popular and widely used component in sequence-to-sequence models. However, previous research on neural generative dialogue systems always generates universal responses, and the attention distribution learned by the model always attends to the same semantic aspect. To solve this problem, in this paper, we propose a novel Multi-Head Attention Mechanism (MHAM) for generative dialog systems, which aims at capturing multiple semantic aspects from the user utterance. Further, a regularizer is formulated to force different attention heads to concentrate on certain aspects. The proposed mechanism leads to more informative, diverse, and relevant response generated. Experimental results show that our proposed model outperforms several strong baselines.",
        "authors": [
            "Chongyang Tao",
            "Shen Gao",
            "Mingyue Shang",
            "Wei Wu",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "citations": 139,
        "references": 30,
        "year": 2018
    },
    {
        "title": "GIQA: Generated Image Quality Assessment",
        "abstract": null,
        "authors": [
            "Shuyang Gu",
            "Jianmin Bao",
            "Dong Chen",
            "Fang Wen"
        ],
        "citations": 76,
        "references": 44,
        "year": 2020
    },
    {
        "title": "Learning Robotic Manipulation through Visual Planning and Acting",
        "abstract": "Planning for robotic manipulation requires reasoning about the changes a robot can affect on objects. When such interactions can be modelled analytically, as in domains with rigid objects, efficient planning algorithms exist. However, in both domestic and industrial domains, the objects of interest can be soft, or deformable, and hard to model analytically. For such cases, we posit that a data-driven modelling approach is more suitable. In recent years, progress in deep generative models has produced methods that learn to `imagine' plausible images from data. Building on the recent Causal InfoGAN generative model, in this work we learn to imagine goal-directed object manipulation directly from raw image data of self-supervised interaction of the robot with the object. After learning, given a goal observation of the system, our model can generate an imagined plan -- a sequence of images that transition the object into the desired goal. To execute the plan, we use it as a reference trajectory to track with a visual servoing controller, which we also learn from the data as an inverse dynamics model. In a simulated manipulation task, we show that separating the problem into visual planning and visual tracking control is more sample efficient and more interpretable than alternative data-driven approaches. We further demonstrate our approach on learning to imagine and execute in 3 environments, the final of which is deformable rope manipulation on a PR2 robot.",
        "authors": [
            "Angelina Wang",
            "Thanard Kurutach",
            "Kara Liu",
            "P. Abbeel",
            "Aviv Tamar"
        ],
        "citations": 115,
        "references": 51,
        "year": 2019
    },
    {
        "title": "A Learned Representation for Scalable Vector Graphics",
        "abstract": "Dramatic advances in generative models have resulted in near photographic quality for artificially rendered faces, animals and other objects in the natural world. In spite of such advances, a higher level understanding of vision and imagery does not arise from exhaustively modeling an object, but instead identifying higher-level attributes that best summarize the aspects of an object. In this work we attempt to model the drawing process of fonts by building sequential generative models of vector graphics. This model has the benefit of providing a scale-invariant representation for imagery whose latent representation may be systematically manipulated and exploited to perform style propagation. We demonstrate these results on a large dataset of fonts crawled from the web and highlight how such a model captures the statistical dependencies and richness of this dataset. We envision that our model can find use as a tool for graphic designers to facilitate font design.",
        "authors": [
            "Raphael Gontijo Lopes",
            "David R Ha",
            "D. Eck",
            "Jonathon Shlens"
        ],
        "citations": 104,
        "references": 63,
        "year": 2019
    },
    {
        "title": "Learning Latent Representations for Speech Generation and Transformation",
        "abstract": "An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.",
        "authors": [
            "Wei-Ning Hsu",
            "Yu Zhang",
            "James R. Glass"
        ],
        "citations": 144,
        "references": 24,
        "year": 2017
    },
    {
        "title": "Variational Inference using Implicit Distributions",
        "abstract": "Generative adversarial networks (GANs) have given us a great tool to fit implicit generative models to data. Implicit distributions are ones we can sample from easily, and take derivatives of samples with respect to model parameters. These models are highly expressive and we argue they can prove just as useful for variational inference (VI) as they are for generative modelling. Several papers have proposed GAN-like algorithms for inference, however, connections to the theory of VI are not always well understood. This paper provides a unifying review of existing algorithms establishing connections between variational autoencoders, adversarially learned inference, operator VI, GAN-based image reconstruction, and more. Secondly, the paper provides a framework for building new algorithms: depending on the way the variational bound is expressed we introduce prior-contrastive and joint-contrastive methods, and show practical inference algorithms based on either density ratio estimation or denoising.",
        "authors": [
            "Ferenc Huszár"
        ],
        "citations": 131,
        "references": 32,
        "year": 2017
    },
    {
        "title": "A Model to Search for Synthesizable Molecules",
        "abstract": "Deep generative models are able to suggest new organic molecules by generating strings, trees, and graphs representing their structure. While such models allow one to generate molecules with desirable properties, they give no guarantees that the molecules can actually be synthesized in practice. We propose a new molecule generation model, mirroring a more realistic real-world process, where (a) reactants are selected, and (b) combined to form more complex molecules. More specifically, our generative model proposes a bag of initial reactants (selected from a pool of commercially-available molecules) and uses a reaction model to predict how they react together to generate new molecules. We first show that the model can generate diverse, valid and unique molecules due to the useful inductive biases of modeling reactions. Furthermore, our model allows chemists to interrogate not only the properties of the generated molecules but also the feasibility of the synthesis routes. We conclude by using our model to solve retrosynthesis problems, predicting a set of reactants that can produce a target product.",
        "authors": [
            "John Bradshaw",
            "Brooks Paige",
            "Matt J. Kusner",
            "Marwin H. S. Segler",
            "José Miguel Hernández-Lobato"
        ],
        "citations": 104,
        "references": 61,
        "year": 2019
    },
    {
        "title": "Deep learning for molecular generation.",
        "abstract": "De novo drug design aims to generate novel chemical compounds with desirable chemical and pharmacological properties from scratch using computer-based methods. Recently, deep generative neural networks have become a very active research frontier in de novo drug discovery, both in theoretical and in experimental evidence, shedding light on a promising new direction of automatic molecular generation and optimization. In this review, we discussed recent development of deep learning models for molecular generation and summarized them as four different generative architectures with four different optimization strategies. We also discussed future directions of deep generative models for de novo drug design.",
        "authors": [
            "Youjun Xu",
            "Kangjie Lin",
            "Shiwei Wang",
            "Lei Wang",
            "Chenjing Cai",
            "Chen Song",
            "L. Lai",
            "Jianfeng Pei"
        ],
        "citations": 102,
        "references": 58,
        "year": 2019
    },
    {
        "title": "Image-Adaptive GAN based Reconstruction",
        "abstract": "In the recent years, there has been a significant improvement in the quality of samples produced by (deep) generative models such as variational auto-encoders and generative adversarial networks. However, the representation capabilities of these methods still do not capture the full distribution for complex classes of images, such as human faces. This deficiency has been clearly observed in previous works that use pre-trained generative models to solve imaging inverse problems. In this paper, we suggest to mitigate the limited representation capabilities of generators by making them image-adaptive and enforcing compliance of the restoration with the observations via back-projections. We empirically demonstrate the advantages of our proposed approach for image super-resolution and compressed sensing.",
        "authors": [
            "Shady Abu Hussein",
            "Tom Tirer",
            "R. Giryes"
        ],
        "citations": 88,
        "references": 43,
        "year": 2019
    },
    {
        "title": "Future Frame Prediction Using Convolutional VRNN for Anomaly Detection",
        "abstract": "Anomaly detection in videos aims at reporting anything that does not conform the normal behaviour or distribution. However, due to the sparsity of abnormal video clips in real life, collecting annotated data for supervised learning is exceptionally cumbersome. Inspired by the practicability of generative models for semi-supervised learning, we propose a novel sequential generative model based on variational autoencoder (VAE) for future frame prediction with convolutional LSTM (ConvLSTM). To the best of our knowledge, this is the first work that considers temporal information in future frame prediction based anomaly detection framework from the model perspective. Our experiments demonstrate that our approach is superior to the state-of-the-art methods on three benchmark datasets.",
        "authors": [
            "Yiwei Lu",
            "Mahesh Kumar Krishna Reddy",
            "Seyed shahabeddin Nabavi",
            "Yang Wang"
        ],
        "citations": 88,
        "references": 33,
        "year": 2019
    },
    {
        "title": "Random Sum-Product Networks: A Simple and Effective Approach to Probabilistic Deep Learning",
        "abstract": "Sum-product networks (SPNs) are expressive probabilistic models with a rich set of exact and efﬁcient inference routines. However, in order to guarantee exact inference, they require speciﬁc structural constraints, which complicate learning SPNs from data. Thereby, most SPN structure learners proposed so far are tedious to tune, do not scale easily, and are not easily integrated with deep learning frameworks. In this paper, we follow a simple “deep learning” approach, by generating unspecial-ized random structures, scalable to millions of parameters, and subsequently applying GPU-based optimization. Somewhat surprisingly, our models often perform on par with state-of-the-art SPN structure learners and deep neural networks on a diverse range of generative and discriminative scenarios. At the same time, our models yield well-calibrated uncertainties, and stand out among most deep generative and discriminative models in being robust to missing features and being able to detect anomalies.",
        "authors": [
            "Robert Peharz",
            "Antonio Vergari",
            "Karl Stelzner",
            "Alejandro Molina",
            "M. Trapp",
            "Xiaoting Shao",
            "K. Kersting",
            "Zoubin Ghahramani"
        ],
        "citations": 106,
        "references": 55,
        "year": 2019
    },
    {
        "title": "Modeling Facial Geometry Using Compositional VAEs",
        "abstract": "We propose a method for learning non-linear face geometry representations using deep generative models. Our model is a variational autoencoder with multiple levels of hidden variables where lower layers capture global geometry and higher ones encode more local deformations. Based on that, we propose a new parameterization of facial geometry that naturally decomposes the structure of the human face into a set of semantically meaningful levels of detail. This parameterization enables us to do model fitting while capturing varying level of detail under different types of geometrical constraints.",
        "authors": [
            "Timur M. Bagautdinov",
            "Chenglei Wu",
            "Jason M. Saragih",
            "P. Fua",
            "Yaser Sheikh"
        ],
        "citations": 108,
        "references": 39,
        "year": 2018
    },
    {
        "title": "A Data-Driven Market Simulator for Small Data Environments",
        "abstract": "Neural network based data-driven market simulation unveils a new and flexible way of modelling financial time series without imposing assumptions on the underlying stochastic dynamics. Though in this sense generative market simulation is model-free, the concrete modelling choices are nevertheless decisive for the features of the simulated paths. We give a brief overview of currently used generative modelling approaches and performance evaluation metrics for financial time series, and address some of the challenges to achieve good results in the latter. We also contrast some classical approaches of market simulation with simulation based on generative modelling and highlight some advantages and pitfalls of the new approach. While most generative models tend to rely on large amounts of training data, we present here a generative model that works reliably in environments where the amount of available training data is notoriously small. Furthermore, we show how a rough paths perspective combined with a parsimonious Variational Autoencoder framework provides a powerful way for encoding and evaluating financial time series in such environments where available training data is scarce. Finally, we also propose a suitable performance evaluation metric for financial time series and discuss some connections of our Market Generator to deep hedging.",
        "authors": [
            "Hans Bühler",
            "Blanka Horvath",
            "Terry Lyons",
            "Imanol Perez Arribas",
            "Ben Wood"
        ],
        "citations": 63,
        "references": 70,
        "year": 2020
    },
    {
        "title": "On Implicit Regularization in β-VAEs",
        "abstract": "While the impact of variational inference (VI) on posterior inference in a fixed generative model is well-characterized, its role in regularizing a learned generative model when used in variational autoencoders (VAEs) is poorly understood. We study the regularizing effects of variational distributions on learning in generative models from two perspectives. First, we analyze the role that the choice of variational family plays in imparting uniqueness to the learned model by restricting the set of optimal generative models. Second, we study the regularization effect of the variational family on the local geometry of the decoding model. This analysis uncovers the regularizer implicit in the $\\beta$-VAE objective, and leads to an approximation consisting of a deterministic autoencoding objective plus analytic regularizers that depend on the Hessian or Jacobian of the decoding model, unifying VAEs with recent heuristics proposed for training regularized autoencoders. We empirically verify these findings, observing that the proposed deterministic objective exhibits similar behavior to the $\\beta$-VAE in terms of objective value and sample quality.",
        "authors": [
            "Abhishek Kumar",
            "Ben Poole"
        ],
        "citations": 51,
        "references": 52,
        "year": 2020
    },
    {
        "title": "Deep active inference",
        "abstract": null,
        "authors": [
            "K. Ueltzhöffer"
        ],
        "citations": 96,
        "references": 75,
        "year": 2017
    },
    {
        "title": "The power of deep learning to ligand-based novel drug discovery",
        "abstract": "ABSTRACT Introduction Deep discriminative and generative neural-network models are becoming an integral part of the modern approach to ligand-based novel drug discovery. The variety of different architectures of neural networks, the methods of their training, and the procedures of generating new molecules require expert knowledge to choose the most suitable approach. Areas covered Three different approaches to deep learning use in ligand-based drug discovery are considered: virtual screening, neural generative models, and mutation-based structure generation. Several architectures of neural networks for building either discriminative or generative models are considered in this paper, including deep multilayer neural networks, different kinds of convolutional neural networks, recurrent neural networks, and several types of autoencoders. Several kinds of learning frameworks are also considered, including adversarial learning and reinforcement learning. Different types of representations for generating molecules, including SMILES, graphs, and several alternative string representations are also considered. Expert opinion Two kinds of problem should be solved in order to make the models built using deep neural networks, especially generative models, a valuable option in ligand-based drug discovery: the issue of interpretability and explainability of deep-learning models and the issue of synthetic accessibility of novel compounds designed by deep-learning algorithms.",
        "authors": [
            "I. Baskin"
        ],
        "citations": 53,
        "references": 119,
        "year": 2020
    },
    {
        "title": "Reduced-Reference Image Quality Assessment in Free-Energy Principle and Sparse Representation",
        "abstract": "The free-energy principle in recent studies of brain theory and neuroscience models the perception and understanding of the outside scene as an active inference process, in which the brain tries to account for the visual scene with an internal generative model. Specifically, with the internal generative model, the brain yields corresponding predictions for its encountered visual scenes. Then, the discrepancy between the visual input and its brain prediction should be closely related to the quality of perceptions. On the other hand, sparse representation has been evidenced to resemble the strategy of the primary visual cortex in the brain for representing natural images. With the strong neurobiological support for sparse representation, in this paper, we approximate the internal generative model with sparse representation and propose an image quality metric accordingly, which is named FSI (free-energy principle and sparse representation-based index for image quality assessment). In FSI, the reference and distorted images are, respectively, predicted by the sparse representation at first. Then, the difference between the entropies of the prediction discrepancies is defined to measure the image quality. Experimental results on four large-scale image databases confirm the effectiveness of the FSI and its superiority over representative image quality assessment methods. The FSI belongs to reduced-reference methods, and it only needs a single number from the reference image for quality estimation.",
        "authors": [
            "Yutao Liu",
            "Guangtao Zhai",
            "Ke Gu",
            "Xianming Liu",
            "Debin Zhao",
            "Wen Gao"
        ],
        "citations": 106,
        "references": 62,
        "year": 2017
    },
    {
        "title": "ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity?",
        "abstract": "Generating molecules with desired chemical properties is important for drug discovery. The use of generative neural networks is promising for this task. However, from visual inspection, it often appears that generated samples lack diversity. In this paper, we quantify this internal chemical diversity, and we raise the following challenge: can a nontrivial AI model reproduce natural chemical diversity for desired molecules? To illustrate this question, we consider two generative models: a Reinforcement Learning model and the recently introduced ORGAN. Both fail at this challenge. We hope this challenge will stimulate research in this direction.",
        "authors": [
            "Mostapha Benhenda"
        ],
        "citations": 96,
        "references": 27,
        "year": 2017
    },
    {
        "title": "BARThez: a Skilled Pretrained French Sequence-to-Sequence Model",
        "abstract": "Inductive transfer learning has taken the entire NLP field by storm, with models such as BERT and BART setting new state of the art on countless NLU tasks. However, most of the available models and research have been conducted for English. In this work, we introduce BARThez, the first large-scale pretrained seq2seq model for French. Being based on BART, BARThez is particularly well-suited for generative tasks. We evaluate BARThez on five discriminative tasks from the FLUE benchmark and two generative tasks from a novel summarization dataset, OrangeSum, that we created for this research. We show BARThez to be very competitive with state-of-the-art BERT-based French language models such as CamemBERT and FlauBERT. We also continue the pretraining of a multilingual BART on BARThez’ corpus, and show our resulting model, mBARThez, to significantly boost BARThez’ generative performance.",
        "authors": [
            "Moussa Kamal Eddine",
            "A. Tixier",
            "M. Vazirgiannis"
        ],
        "citations": 64,
        "references": 54,
        "year": 2020
    },
    {
        "title": "A Spectral Energy Distance for Parallel Speech Synthesis",
        "abstract": "Speech synthesis is an important practical generative modeling problem that has seen great progress over the last few years, with likelihood-based autoregressive neural models now outperforming traditional concatenative systems. A downside of such autoregressive models is that they require executing tens of thousands of sequential operations per second of generated audio, making them ill-suited for deployment on specialized deep learning hardware. Here, we propose a new learning method that allows us to train highly parallel models of speech, without requiring access to an analytical likelihood function. Our approach is based on a generalized energy distance between the distributions of the generated and real audio. This spectral energy distance is a proper scoring rule with respect to the distribution over magnitude-spectrograms of the generated waveform audio and offers statistical consistency guarantees. The distance can be calculated from minibatches without bias, and does not involve adversarial learning, yielding a stable and consistent method for training implicit generative models. Empirically, we achieve state-of-the-art generation quality among implicit generative models, as judged by the recently-proposed cFDSD metric. When combining our method with adversarial techniques, we also improve upon the recently-proposed GAN-TTS model in terms of Mean Opinion Score as judged by trained human evaluators.",
        "authors": [
            "A. Gritsenko",
            "Tim Salimans",
            "Rianne van den Berg",
            "Jasper Snoek",
            "Nal Kalchbrenner"
        ],
        "citations": 62,
        "references": 39,
        "year": 2020
    },
    {
        "title": "HoloGAN: Unsupervised Learning of 3D Representations From Natural Images",
        "abstract": "We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.",
        "authors": [
            "Thu Nguyen-Phuoc",
            "Chuan Li",
            "Lucas Theis",
            "Christian Richardt",
            "Yong-Liang Yang"
        ],
        "citations": 72,
        "references": 65,
        "year": 2019
    },
    {
        "title": "Data Augmentation for Spoken Language Understanding via Joint Variational Generation",
        "abstract": "Data scarcity is one of the main obstacles of domain adaptation in spoken language understanding (SLU) due to the high cost of creating manually tagged SLU datasets. Recent works in neural text generative models, particularly latent variable models such as variational autoencoder (VAE), have shown promising results in regards to generating plausible and natural sentences. In this paper, we propose a novel generative architecture which leverages the generative power of latent variable models to jointly synthesize fully annotated utterances. Our experiments show that existing SLU models trained on the additional synthetic examples achieve performance gains. Our approach not only helps alleviate the data scarcity issue in the SLU task for many datasets but also indiscriminately improves language understanding performances for various SLU models, supported by extensive experiments and rigorous statistical testing.",
        "authors": [
            "Kang Min Yoo",
            "Youhyun Shin",
            "Sang-goo Lee"
        ],
        "citations": 89,
        "references": 38,
        "year": 2018
    },
    {
        "title": "Learning latent representations across multiple data domains using Lifelong VAEGAN",
        "abstract": null,
        "authors": [
            "Fei Ye",
            "A. Bors"
        ],
        "citations": 62,
        "references": 68,
        "year": 2020
    },
    {
        "title": "Variational Deep Semantic Hashing for Text Documents",
        "abstract": "As the amount of textual data has been rapidly increasing over the past decade, efficient similarity search methods have become a crucial component of large-scale information retrieval systems. A popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness and flexibility in modeling to learn effective representations. The recent advances of deep learning in a wide range of applications has demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative models naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks, which is very suitable for text modeling. However, little work has leveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.",
        "authors": [
            "Suthee Chaidaroon",
            "Yi Fang"
        ],
        "citations": 72,
        "references": 42,
        "year": 2017
    },
    {
        "title": "MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation",
        "abstract": "We present MixNMatch, a conditional generative model that learns to disentangle and encode background, object pose, shape, and texture from real images with minimal supervision, for mix-and-match image generation. We build upon FineGAN, an unconditional generative model, to learn the desired disentanglement and image generator, and leverage adversarial joint image-code distribution matching to learn the latent factor encoders. MixNMatch requires bounding boxes during training to model background, but requires no other supervision. Through extensive experiments, we demonstrate MixNMatch's ability to accurately disentangle, encode, and combine multiple factors for mix-and-match image generation, including sketch2color, cartoon2img, and img2gif applications. Our code/models/demo can be found at https://github.com/Yuheng-Li/MixNMatch",
        "authors": [
            "Yuheng Li",
            "Krishna Kumar Singh",
            "Utkarsh Ojha",
            "Yong Jae Lee"
        ],
        "citations": 74,
        "references": 51,
        "year": 2019
    },
    {
        "title": "Neural Response Generation via GAN with an Approximate Embedding Layer",
        "abstract": "This paper presents a Generative Adversarial Network (GAN) to model single-turn short-text conversations, which trains a sequence-to-sequence (Seq2Seq) network for response generation simultaneously with a discriminative classifier that measures the differences between human-produced responses and machine-generated ones. In addition, the proposed method introduces an approximate embedding layer to solve the non-differentiable problem caused by the sampling-based output decoding procedure in the Seq2Seq generative model. The GAN setup provides an effective way to avoid noninformative responses (a.k.a “safe responses”), which are frequently observed in traditional neural response generators. The experimental results show that the proposed approach significantly outperforms existing neural response generation models in diversity metrics, with slight increases in relevance scores as well, when evaluated on both a Mandarin corpus and an English corpus.",
        "authors": [
            "Zhen Xu",
            "Bingquan Liu",
            "Baoxun Wang",
            "Chengjie Sun",
            "Xiaolong Wang",
            "Zhuoran Wang",
            "Chao Qi"
        ],
        "citations": 83,
        "references": 27,
        "year": 2017
    },
    {
        "title": "Inference in Deep Networks in High Dimensions",
        "abstract": "Deep generative networks provide a powerful tool for modeling complex data in a wide range of applications. In inverse problems that use these networks as generative priors on data, one must often perform inference of the inputs of the networks from the outputs. Inference is also required for sampling during stochastic training of these generative models. This paper considers inference in a deep stochastic neural network where the parameters (e.g., weights, biases and activation functions) are known and the problem is to estimate the values of the input and hidden units from the output. A novel and computationally tractable inference method called Multi-Layer Vector Approximate Message Passing (ML-VAMP) is presented. Our main contribution shows that the mean-squared error (MSE) of ML-VAMP can be exactly predicted in a certain large system limit. In addition, the MSE achieved by ML-VAMP matches the Bayes optimal value recently postulated by Reeves when certain fixed point equations have unique solutions.",
        "authors": [
            "A. Fletcher",
            "S. Rangan"
        ],
        "citations": 67,
        "references": 50,
        "year": 2017
    },
    {
        "title": "Deep Learning the Ising Model Near Criticality",
        "abstract": "It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality.",
        "authors": [
            "A. Morningstar",
            "R. Melko"
        ],
        "citations": 84,
        "references": 29,
        "year": 2017
    },
    {
        "title": "Audio-Visual Speech Enhancement Using Conditional Variational Auto-Encoders",
        "abstract": "Variational auto-encoders (VAEs) are deep generative latent variable models that can be used for learning the distribution of complex data. VAEs have been successfully used to learn a probabilistic prior over speech signals, which is then used to perform speech enhancement. One advantage of this generative approach is that it does not require pairs of clean and noisy speech signals at training. In this article, we propose audio-visual variants of VAEs for single-channel and speaker-independent speech enhancement. We develop a conditional VAE (CVAE) where the audio speech generative process is conditioned on visual information of the lip region. At test time, the audio-visual speech generative model is combined with a noise model based on nonnegative matrix factorization, and speech enhancement relies on a Monte Carlo expectation-maximization algorithm. Experiments are conducted with the recently published NTCD-TIMIT dataset as well as the GRID corpus. The results confirm that the proposed audio-visual CVAE effectively fuses audio and visual information, and it improves the speech enhancement performance compared with the audio-only VAE model, especially when the speech signal is highly corrupted by noise. We also show that the proposed unsupervised audio-visual speech enhancement approach outperforms a state-of-the-art supervised deep learning method.",
        "authors": [
            "M. Sadeghi",
            "Simon Leglaive",
            "Xavier Alameda-Pineda",
            "Laurent Girin",
            "R. Horaud"
        ],
        "citations": 63,
        "references": 71,
        "year": 2019
    },
    {
        "title": "Adversarial Variational Optimization of Non-Differentiable Simulators",
        "abstract": "Complex computer simulators are increasingly used across fields of science as generative models tying parameters of an underlying theory to experimental observations. Inference in this setup is often difficult, as simulators rarely admit a tractable density or likelihood function. We introduce Adversarial Variational Optimization (AVO), a likelihood-free inference algorithm for fitting a non-differentiable generative model incorporating ideas from generative adversarial networks, variational optimization and empirical Bayes. We adapt the training procedure of generative adversarial networks by replacing the differentiable generative network with a domain-specific simulator. We solve the resulting non-differentiable minimax problem by minimizing variational upper bounds of the two adversarial objectives. Effectively, the procedure results in learning a proposal distribution over simulator parameters, such that the JS divergence between the marginal distribution of the synthetic data and the empirical distribution of observed data is minimized. We evaluate and compare the method with simulators producing both discrete and continuous data.",
        "authors": [
            "Gilles Louppe",
            "Kyle Cranmer"
        ],
        "citations": 64,
        "references": 57,
        "year": 2017
    },
    {
        "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
        "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
        "authors": [
            "Yang Song",
            "Jascha Narain Sohl-Dickstein",
            "Diederik P. Kingma",
            "Abhishek Kumar",
            "Stefano Ermon",
            "Ben Poole"
        ],
        "citations": 1000,
        "references": 66,
        "year": 2020
    },
    {
        "title": "GENERATIVE ADVERSARIAL NETS",
        "abstract": "Estimating individualized treatment effects (ITE) is a challenging task due to the need for an individual’s potential outcomes to be learned from biased data and without having access to the counterfactuals. We propose a novel method for inferring ITE based on the Generative Adversarial Nets (GANs) framework. Our method, termed Generative Adversarial Nets for inference of Individualized Treatment Effects (GANITE), is motivated by the possibility that we can capture the uncertainty in the counterfactual distributions by attempting to learn them using a GAN. We generate proxies of the counterfactual outcomes using a counterfactual generator, G, and then pass these proxies to an ITE generator, I, in order to train it. By modeling both of these using the GAN framework, we are able to infer based on the factual data, while still accounting for the unseen counterfactuals. We test our method on three real-world datasets (with both binary and multiple treatments) and show that GANITE outperforms state-of-the-art methods.",
        "authors": [
            "Individualized Treat",
            "Jinsung Yoon"
        ],
        "citations": 1000,
        "references": 24,
        "year": 2018
    },
    {
        "title": "Wasserstein Generative Adversarial Networks",
        "abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.",
        "authors": [
            "Martín Arjovsky",
            "Soumith Chintala",
            "L. Bottou"
        ],
        "citations": 1000,
        "references": 26,
        "year": 2017
    },
    {
        "title": "Self-Attention Generative Adversarial Networks",
        "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",
        "authors": [
            "Han Zhang",
            "I. Goodfellow",
            "Dimitris N. Metaxas",
            "Augustus Odena"
        ],
        "citations": 1000,
        "references": 54,
        "year": 2018
    },
    {
        "title": "Generative Image Inpainting with Contextual Attention",
        "abstract": "Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feedforward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting.",
        "authors": [
            "Jiahui Yu",
            "Zhe L. Lin",
            "Jimei Yang",
            "Xiaohui Shen",
            "Xin Lu",
            "Thomas S. Huang"
        ],
        "citations": 1000,
        "references": 50,
        "year": 2018
    },
    {
        "title": "Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling",
        "abstract": "Progressively applying Gaussian noise transforms complex data distributions to approximately Gaussian. Reversing this dynamic defines a generative model. When the forward noising process is given by a Stochastic Differential Equation (SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the associated reverse-time SDE may be estimated using score-matching. A limitation of this approach is that the forward-time SDE must be run for a sufficiently long time for the final distribution to be approximately Gaussian. In contrast, solving the Schr\\\"odinger Bridge problem (SB), i.e. an entropy-regularized optimal transport problem on path spaces, yields diffusions which generate samples from the data distribution in finite time. We present Diffusion SB (DSB), an original approximation of the Iterative Proportional Fitting (IPF) procedure to solve the SB problem, and provide theoretical analysis along with generative modeling experiments. The first DSB iteration recovers the methodology proposed by Song et al. (2021), with the flexibility of using shorter time intervals, as subsequent DSB iterations reduce the discrepancy between the final-time marginal of the forward (resp. backward) SDE with respect to the prior (resp. data) distribution. Beyond generative modeling, DSB offers a widely applicable computational optimal transport tool as the continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi, 2013).",
        "authors": [
            "Valentin De Bortoli",
            "James Thornton",
            "J. Heng",
            "A. Doucet"
        ],
        "citations": 366,
        "references": 95,
        "year": 2021
    },
    {
        "title": "On Generative Spoken Language Modeling from Raw Audio",
        "abstract": "Abstract We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1",
        "authors": [
            "Kushal Lakhotia",
            "Evgeny Kharitonov",
            "Wei-Ning Hsu",
            "Yossi Adi",
            "Adam Polyak",
            "Benjamin Bolte",
            "Tu Nguyen",
            "Jade Copet",
            "Alexei Baevski",
            "A. Mohamed",
            "Emmanuel Dupoux"
        ],
        "citations": 312,
        "references": 80,
        "year": 2021
    },
    {
        "title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation",
        "abstract": "We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",
        "authors": [
            "Yizhe Zhang",
            "Siqi Sun",
            "Michel Galley",
            "Yen-Chun Chen",
            "Chris Brockett",
            "Xiang Gao",
            "Jianfeng Gao",
            "Jingjing Liu",
            "W. Dolan"
        ],
        "citations": 1000,
        "references": 32,
        "year": 2019
    },
    {
        "title": "Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks",
        "abstract": "Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.",
        "authors": [
            "Agrim Gupta",
            "Justin Johnson",
            "Li Fei-Fei",
            "S. Savarese",
            "Alexandre Alahi"
        ],
        "citations": 1000,
        "references": 50,
        "year": 2018
    },
    {
        "title": "A Unified Generative Framework for Various NER Subtasks",
        "abstract": "Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.",
        "authors": [
            "Hang Yan",
            "Tao Gui",
            "Junqi Dai",
            "Qipeng Guo",
            "Zheng Zhang",
            "Xipeng Qiu"
        ],
        "citations": 266,
        "references": 70,
        "year": 2021
    },
    {
        "title": "Deep Generative Modeling for Single-cell Transcriptomics",
        "abstract": null,
        "authors": [
            "Romain Lopez",
            "J. Regier",
            "Michael Cole",
            "Michael I. Jordan",
            "N. Yosef"
        ],
        "citations": 1000,
        "references": 59,
        "year": 2018
    },
    {
        "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks",
        "abstract": "In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different sub-regions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.",
        "authors": [
            "Tao Xu",
            "Pengchuan Zhang",
            "Qiuyuan Huang",
            "Han Zhang",
            "Zhe Gan",
            "Xiaolei Huang",
            "Xiaodong He"
        ],
        "citations": 1000,
        "references": 41,
        "year": 2017
    },
    {
        "title": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis",
        "abstract": "While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.",
        "authors": [
            "Katja Schwarz",
            "Yiyi Liao",
            "Michael Niemeyer",
            "Andreas Geiger"
        ],
        "citations": 825,
        "references": 78,
        "year": 2020
    },
    {
        "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabelled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of graph generation into two components: 1) attribute generation and 2) edge generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale open academic graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks?",
        "authors": [
            "Ziniu Hu",
            "Yuxiao Dong",
            "Kuansan Wang",
            "Kai-Wei Chang",
            "Yizhou Sun"
        ],
        "citations": 502,
        "references": 46,
        "year": 2020
    },
    {
        "title": "GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation",
        "abstract": "3D-aware image generative modeling aims to generate 3D-consistent images with explicitly controllable camera poses. Recent works have shown promising results by training neural radiance field (NeRF) generators on unstructured 2D images, but still cannot generate highly-realistic images with fine details. A critical reason is that the high memory and computation cost of volumetric representation learning greatly restricts the number of point samples for radiance integration during training. Deficient sampling not only limits the expressive power of the generator to handle fine details but also impedes effective GAN training due to the noise caused by unstable Monte Carlo sampling. We propose a novel approach that regulates point sampling and radiance field learning on 2D manifolds, embodied as a set of learned implicit surfaces in the 3D volume. For each viewing ray, we calculate ray-surface intersections and accumulate their radiance generated by the network. By training and rendering such radiance mani folds, our generator can produce high quality images with realistic fine details and strong visual 3D consistency. 11Project page: https://yudeng.github.io/GRAM/",
        "authors": [
            "Yu Deng",
            "Jiaolong Yang",
            "Jianfeng Xiang",
            "Xin Tong"
        ],
        "citations": 236,
        "references": 76,
        "year": 2021
    },
    {
        "title": "SinGAN: Learning a Generative Model From a Single Natural Image",
        "abstract": "We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.",
        "authors": [
            "Tamar Rott Shaham",
            "Tali Dekel",
            "T. Michaeli"
        ],
        "citations": 793,
        "references": 71,
        "year": 2019
    },
    {
        "title": "Parallel Wavegan: A Fast Waveform Generation Model Based on Generative Adversarial Networks with Multi-Resolution Spectrogram",
        "abstract": "We propose Parallel WaveGAN, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained. Furthermore, our model is able to generate high-fidelity speech even with its compact architecture. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system.",
        "authors": [
            "Ryuichi Yamamoto",
            "Eunwoo Song",
            "Jae-Min Kim"
        ],
        "citations": 767,
        "references": 31,
        "year": 2019
    },
    {
        "title": "Jukebox: A Generative Model for Music",
        "abstract": "We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at this https URL, along with model weights and code at this https URL",
        "authors": [
            "Prafulla Dhariwal",
            "Heewoo Jun",
            "Christine Payne",
            "Jong Wook Kim",
            "Alec Radford",
            "I. Sutskever"
        ],
        "citations": 656,
        "references": 85,
        "year": 2020
    },
    {
        "title": "DAGAN: Deep De-Aliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction",
        "abstract": "Compressed sensing magnetic resonance imaging (CS-MRI) enables fast acquisition, which is highly desirable for numerous clinical applications. This can not only reduce the scanning cost and ease patient burden, but also potentially reduce motion artefacts and the effect of contrast washout, thus yielding better image quality. Different from parallel imaging-based fast MRI, which utilizes multiple coils to simultaneously receive MR signals, CS-MRI breaks the Nyquist–Shannon sampling barrier to reconstruct MRI images with much less required raw data. This paper provides a deep learning-based strategy for reconstruction of CS-MRI, and bridges a substantial gap between conventional non-learning methods working only on data from a single image, and prior knowledge from large training data sets. In particular, a novel conditional Generative Adversarial Networks-based model (DAGAN)-based model is proposed to reconstruct CS-MRI. In our DAGAN architecture, we have designed a refinement learning method to stabilize our U-Net based generator, which provides an end-to-end network to reduce aliasing artefacts. To better preserve texture and edges in the reconstruction, we have coupled the adversarial loss with an innovative content loss. In addition, we incorporate frequency-domain information to enforce similarity in both the image and frequency domains. We have performed comprehensive comparison studies with both conventional CS-MRI reconstruction methods and newly investigated deep learning approaches. Compared with these methods, our DAGAN method provides superior reconstruction with preserved perceptual image details. Furthermore, each image is reconstructed in about 5 ms, which is suitable for real-time processing.",
        "authors": [
            "Guang Yang",
            "Simiao Yu",
            "Hao Dong",
            "G. Slabaugh",
            "P. Dragotti",
            "Xujiong Ye",
            "Fangde Liu",
            "S. Arridge",
            "J. Keegan",
            "Yike Guo",
            "D. Firmin"
        ],
        "citations": 856,
        "references": 75,
        "year": 2018
    },
    {
        "title": "Towards Generative Aspect-Based Sentiment Analysis",
        "abstract": "Aspect-based sentiment analysis (ABSA) has received increasing attention recently. Most existing work tackles ABSA in a discriminative manner, designing various task-specific classification networks for the prediction. Despite their effectiveness, these methods ignore the rich label semantics in ABSA problems and require extensive task-specific designs. In this paper, we propose to tackle various ABSA tasks in a unified generative framework. Two types of paradigms, namely annotation-style and extraction-style modeling, are designed to enable the training process by formulating each ABSA task as a text generation problem. We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state-of-the-art results in almost all cases. This also validates the strong generality of the proposed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design.",
        "authors": [
            "Wenxuan Zhang",
            "Xin Li",
            "Yang Deng",
            "Lidong Bing",
            "Wai Lam"
        ],
        "citations": 173,
        "references": 33,
        "year": 2021
    },
    {
        "title": "DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-To-Image Synthesis",
        "abstract": "In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.",
        "authors": [
            "Minfeng Zhu",
            "Pingbo Pan",
            "Wei Chen",
            "Yi Yang"
        ],
        "citations": 543,
        "references": 34,
        "year": 2019
    },
    {
        "title": "Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis",
        "abstract": "Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-to-image translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality.",
        "authors": [
            "Qi Mao",
            "Hsin-Ying Lee",
            "Hung-Yu Tseng",
            "Siwei Ma",
            "Ming-Hsuan Yang"
        ],
        "citations": 388,
        "references": 34,
        "year": 2019
    },
    {
        "title": "Generative Adversarial Transformers",
        "abstract": "We introduce the GANformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linear efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. Further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach. An implementation of the model is available at https://github.com/dorarad/gansformer.",
        "authors": [
            "Drew A. Hudson",
            "C. L. Zitnick"
        ],
        "citations": 167,
        "references": 81,
        "year": 2021
    },
    {
        "title": "Deep Generative Adversarial Neural Networks for Compressive Sensing MRI",
        "abstract": "Undersampled magnetic resonance image (MRI) reconstruction is typically an ill-posed linear inverse task. The time and resource intensive computations require tradeoffs between <italic>accuracy</italic> and <italic>speed</italic>. In addition, state-of-the-art compressed sensing (CS) analytics are not cognizant of the image <italic>diagnostic quality</italic>. To address these challenges, we propose a novel CS framework that uses generative adversarial networks (GAN) to model the (low-dimensional) manifold of high-quality MR images. Leveraging a mixture of least-squares (LS) GANs and pixel-wise <inline-formula> <tex-math notation=\"LaTeX\">$\\ell _{1}/\\ell _{2}$ </tex-math></inline-formula> cost, a deep residual network with skip connections is trained as the generator that learns to remove the <italic>aliasing</italic> artifacts by projecting onto the image manifold. The LSGAN learns the texture details, while the <inline-formula> <tex-math notation=\"LaTeX\">$\\ell _{1}/\\ell _{2}$ </tex-math></inline-formula> cost suppresses high-frequency noise. A discriminator network, which is a multilayer convolutional neural network (CNN), plays the role of a perceptual cost that is then jointly trained based on high-quality MR images to score the quality of retrieved images. In the operational phase, an initial aliased estimate (e.g., simply obtained by zero-filling) is propagated into the trained generator to output the desired reconstruction. This demands a very low computational overhead. Extensive evaluations are performed on a large contrast-enhanced MR dataset of pediatric patients. Images rated by expert radiologists corroborate that GANCS retrieves higher quality images with improved fine texture details compared with conventional Wavelet-based and dictionary-learning-based CS schemes as well as with deep-learning-based schemes using pixel-wise training. In addition, it offers reconstruction times of under a few milliseconds, which are two orders of magnitude faster than the current state-of-the-art CS-MRI schemes.",
        "authors": [
            "M. Mardani",
            "E. Gong",
            "J. Cheng",
            "S. Vasanawala",
            "G. Zaharchuk",
            "L. Xing",
            "J. Pauly"
        ],
        "citations": 488,
        "references": 30,
        "year": 2019
    },
    {
        "title": "BEGAN: Boundary Equilibrium Generative Adversarial Networks",
        "abstract": "We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.",
        "authors": [
            "David Berthelot",
            "Tom Schumm",
            "Luke Metz"
        ],
        "citations": 1000,
        "references": 21,
        "year": 2017
    },
    {
        "title": "SEGAN: Speech Enhancement Generative Adversarial Network",
        "abstract": "Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.",
        "authors": [
            "Santiago Pascual",
            "A. Bonafonte",
            "J. Serrà"
        ],
        "citations": 1000,
        "references": 37,
        "year": 2017
    },
    {
        "title": "Probabilistic Representation and Inverse Design of Metamaterials Based on a Deep Generative Model with Semi‐Supervised Learning Strategy",
        "abstract": "The research of metamaterials has achieved enormous success in the manipulation of light in a prescribed manner using delicately designed subwavelength structures, so‐called meta‐atoms. Even though modern numerical methods allow for the accurate calculation of the optical response of complex structures, the inverse design of metamaterials, which aims to retrieve the optimal structure according to given requirements, is still a challenging task owing to the nonintuitive and nonunique relationship between physical structures and optical responses. To better unveil this implicit relationship and thus facilitate metamaterial designs, it is proposed to represent metamaterials and model the inverse design problem in a probabilistically generative manner, enabling to elegantly investigate the complex structure–performance relationship in an interpretable way, and solve the one‐to‐many mapping issue that is intractable in a deterministic model. Moreover, to alleviate the burden of numerical calculations when collecting data, a semisupervised learning strategy is developed that allows the model to utilize unlabeled data in addition to labeled data in an end‐to‐end training. On a data‐driven basis, the proposed deep generative model can serve as a comprehensive and efficient tool that accelerates the design, characterization, and even new discovery in the research domain of metamaterials, and photonics in general.",
        "authors": [
            "Wei Ma",
            "Feng Cheng",
            "Yihao Xu",
            "Qinlong Wen",
            "Yongmin Liu"
        ],
        "citations": 393,
        "references": 65,
        "year": 2019
    },
    {
        "title": "Image De-Raining Using a Conditional Generative Adversarial Network",
        "abstract": "Severe weather conditions, such as rain and snow, adversely affect the visual quality of images captured under such conditions, thus rendering them useless for further usage and sharing. In addition, such degraded images drastically affect the performance of vision systems. Hence, it is important to address the problem of single image de-raining. However, the inherent ill-posed nature of the problem presents several challenges. We attempt to leverage powerful generative modeling capabilities of the recently introduced conditional generative adversarial networks (CGAN) by enforcing an additional constraint that the de-rained image must be indistinguishable from its corresponding ground truth clean image. The adversarial loss from GAN provides additional regularization and helps to achieve superior results. In addition to presenting a new approach to de-rain images, we introduce a new refined loss function and architectural novelties in the generator–discriminator pair for achieving improved results. The loss function is aimed at reducing artifacts introduced by GANs and ensure better visual quality. The generator sub-network is constructed using the recently introduced densely connected networks, whereas the discriminator is designed to leverage global and local information to decide if an image is real/fake. Based on this, we propose a novel single image de-raining method called image de-raining conditional generative adversarial network (ID-CGAN) that considers quantitative, visual, and also discriminative performance into the objective function. The experiments evaluated on synthetic and real images show that the proposed method outperforms many recent state-of-the-art single image de-raining methods in terms of quantitative and visual performances. Furthermore, the experimental results evaluated on object detection datasets using the Faster-RCNN also demonstrate the effectiveness of proposed method in improving the detection performance on images degraded by rain.",
        "authors": [
            "He Zhang",
            "Vishwanath A. Sindagi",
            "Vishal M. Patel"
        ],
        "citations": 963,
        "references": 89,
        "year": 2017
    },
    {
        "title": "Data Augmentation Generative Adversarial Networks",
        "abstract": "Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).",
        "authors": [
            "Antreas Antoniou",
            "A. Storkey",
            "Harrison Edwards"
        ],
        "citations": 1000,
        "references": 41,
        "year": 2017
    },
    {
        "title": "Generative Adversarial Networks for Extreme Learned Image Compression",
        "abstract": "We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.",
        "authors": [
            "E. Agustsson",
            "M. Tschannen",
            "Fabian Mentzer",
            "R. Timofte",
            "L. Gool"
        ],
        "citations": 523,
        "references": 53,
        "year": 2018
    },
    {
        "title": "A Simple Convolutional Generative Network for Next Item Recommendation",
        "abstract": "Convolutional Neural Networks (CNNs) have been recently introduced in the domain of session-based next item recommendation. An ordered collection of past items the user has interacted with in a session (or sequence) are embedded into a 2-dimensional latent matrix, and treated as an image. The convolution and pooling operations are then applied to the mapped item embeddings. In this paper, we first examine the typical session-based CNN recommender and show that both the generative model and network architecture are suboptimal when modeling long-range dependencies in the item sequence. To address the issues, we introduce a simple, but very effective generative model that is capable of learning high-level representation from both short- and long-range item dependencies. The network architecture of the proposed model is formed of a stack of holed convolutional layers, which can efficiently increase the receptive fields without relying on the pooling operation. Another contribution is the effective use of residual block structure in recommender systems, which can ease the optimization for much deeper networks. The proposed generative model attains state-of-the-art accuracy with less training time in the next item recommendation task. It accordingly can be used as a powerful recommendation baseline to beat in future, especially when there are long sequences of user feedback.",
        "authors": [
            "Fajie Yuan",
            "Alexandros Karatzoglou",
            "Ioannis Arapakis",
            "J. Jose",
            "Xiangnan He"
        ],
        "citations": 502,
        "references": 40,
        "year": 2018
    },
    {
        "title": "Generative Adversarial Networks for Hyperspectral Image Classification",
        "abstract": "A generative adversarial network (GAN) usually contains a generative network and a discriminative network in competition with each other. The GAN has shown its capability in a variety of applications. In this paper, the usefulness and effectiveness of GAN for classification of hyperspectral images (HSIs) are explored for the first time. In the proposed GAN, a convolutional neural network (CNN) is designed to discriminate the inputs and another CNN is used to generate so-called fake inputs. The aforementioned CNNs are trained together: the generative CNN tries to generate fake inputs that are as real as possible, and the discriminative CNN tries to classify the real and fake inputs. This kind of adversarial training improves the generalization capability of the discriminative CNN, which is really important when the training samples are limited. Specifically, we propose two schemes: 1) a well-designed 1D-GAN as a spectral classifier and 2) a robust 3D-GAN as a spectral–spatial classifier. Furthermore, the generated adversarial samples are used with real training samples to fine-tune the discriminative CNN, which improves the final classification performance. The proposed classifiers are carried out on three widely used hyperspectral data sets: Salinas, Indiana Pines, and Kennedy Space Center. The obtained results reveal that the proposed models provide competitive results compared to the state-of-the-art methods. In addition, the proposed GANs open new opportunities in the remote sensing community for the challenging task of HSI classification and also reveal the huge potential of GAN-based methods for the analysis of such complex and inherently nonlinear data.",
        "authors": [
            "Lin Zhu",
            "Yushi Chen",
            "Pedram Ghamisi",
            "J. Benediktsson"
        ],
        "citations": 512,
        "references": 54,
        "year": 2018
    },
    {
        "title": "Image Blind Denoising with Generative Adversarial Network Based Noise Modeling",
        "abstract": "In this paper, we consider a typical image blind denoising problem, which is to remove unknown noise from noisy images. As we all know, discriminative learning based methods, such as DnCNN, can achieve state-of-the-art denoising results, but they are not applicable to this problem due to the lack of paired training data. To tackle the barrier, we propose a novel two-step framework. First, a Generative Adversarial Network (GAN) is trained to estimate the noise distribution over the input noisy images and to generate noise samples. Second, the noise patches sampled from the first step are utilized to construct a paired training dataset, which is used, in turn, to train a deep Convolutional Neural Network (CNN) for denoising. Extensive experiments have been done to demonstrate the superiority of our approach in image blind denoising.",
        "authors": [
            "Jingwen Chen",
            "Jiawei Chen",
            "Hongyang Chao",
            "Ming Yang"
        ],
        "citations": 480,
        "references": 39,
        "year": 2018
    },
    {
        "title": "PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems",
        "abstract": "Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker’s well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named Data_Gen, based on the generative adversarial networks (GANs). This method mainly relies upon the iteratively updated global model parameters to regenerate samples of interested victims. Second, we further propose a novel generative poisoning attack model, named PoisonGAN, against the federated learning framework. This model utilizes the designed Data_Gen method to efficiently reduce the attack assumptions and make attacks feasible in practice. We finally evaluate our data generation and attack models by implementing two types of typical poisoning attack strategies, label flipping and backdoor, on a federated learning prototype. The experimental results demonstrate that these two attack models are effective in federated learning.",
        "authors": [
            "Jiale Zhang",
            "Bing Chen",
            "Xiang Cheng",
            "H. Binh",
            "Shui Yu"
        ],
        "citations": 148,
        "references": 40,
        "year": 2021
    },
    {
        "title": "A Generative Model for Inverse Design of Metamaterials",
        "abstract": "The advent of metasurfaces in recent years has ushered in a revolutionary means to manipulate the behavior of light on the nanoscale. The design of such structures, to date, has relied on the expertise of an optical scientist to guide a progression of electromagnetic simulations that iteratively solve Maxwell's equations until a locally optimized solution can be attained. In this work, we identify a solution to circumvent this conventional design procedure by means of a deep learning architecture. When fed an input set of customer-defined optical spectra, the constructed generative network generates candidate patterns that match the on-demand spectra with high fidelity. This approach reveals an opportunity to expedite the discovery and design of metasurfaces for tailored optical responses in a systematic, inverse-design manner.",
        "authors": [
            "Zhaocheng Liu",
            "Dayu Zhu",
            "S. Rodrigues",
            "Kyu-Tae Lee",
            "W. Cai"
        ],
        "citations": 480,
        "references": 34,
        "year": 2018
    },
    {
        "title": "Generative hypergraph clustering: From blockmodels to modularity",
        "abstract": "Novel clustering techniques enable the detection of modules in large datasets with multiway interactions. Hypergraphs are a natural modeling paradigm for networked systems with multiway interactions. A standard task in network analysis is the identification of closely related or densely interconnected nodes. We propose a probabilistic generative model of clustered hypergraphs with heterogeneous node degrees and edge sizes. Approximate maximum likelihood inference in this model leads to a clustering objective that generalizes the popular modularity objective for graphs. From this, we derive an inference algorithm that generalizes the Louvain graph community detection method, and a faster, specialized variant in which edges are expected to lie fully within clusters. Using synthetic and empirical data, we demonstrate that the specialized method is highly scalable and can detect clusters where graph-based methods fail. We also use our model to find interpretable higher-order structure in school contact networks, U.S. congressional bill cosponsorship and committees, product categories in copurchasing behavior, and hotel locations from web browsing sessions.",
        "authors": [
            "Philip S. Chodrow",
            "Nate Veldt",
            "Austin R. Benson"
        ],
        "citations": 120,
        "references": 123,
        "year": 2021
    },
    {
        "title": "Unsupervised Image Super-Resolution Using Cycle-in-Cycle Generative Adversarial Networks",
        "abstract": "We consider the single image super-resolution problem in a more general case that the low-/high-resolution pairs and the down-sampling process are unavailable. Different from traditional super-resolution formulation, the low-resolution input is further degraded by noises and blurring. This complicated setting makes supervised learning and accurate kernel estimation impossible. To solve this problem, we resort to unsupervised learning without paired data, inspired by the recent successful image-to-image translation applications. With generative adversarial networks (GAN) as the basic component, we propose a Cycle-in-Cycle network structure to tackle the problem within three steps. First, the noisy and blurry input is mapped to a noise-free low-resolution space. Then the intermediate image is up-sampled with a pre-trained deep model. Finally, we fine-tune the two modules in an end-to-end manner to get the high-resolution output. Experiments on NTIRE2018 datasets demonstrate that the proposed unsupervised method achieves comparable results as the state-of-the-art supervised models.",
        "authors": [
            "Yuan Yuan",
            "Siyuan Liu",
            "Jiawei Zhang",
            "Yongbing Zhang",
            "Chao Dong",
            "Liang Lin"
        ],
        "citations": 417,
        "references": 36,
        "year": 2018
    },
    {
        "title": "Privacy-Preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach",
        "abstract": "Scenario generation is a fundamental and crucial tool for decision-making in power systems with high-penetration renewables. Based on big historical data, in this article, a novel federated deep generative learning framework, called Fed-LSGAN, is proposed by integrating federated learning and least square generative adversarial networks (LSGANs) for renewable scenario generation. Specifically, federated learning learns a shared global model in a central server from renewable sites at network edges, which enables the Fed-LSGAN to generate scenarios in a privacy-preserving manner without sacrificing the generation quality by transferring model parameters, rather than all data. Meanwhile, the LSGANs-based deep generative model generates scenarios that conform to the distribution of historical data through fully capturing the spatial-temporal characteristics of renewable powers, which leverages the least squares loss function to improve the training stability and generation quality. The simulation results demonstrate that the proposal manages to generate high-quality renewable scenarios and outperforms the state-of-the-art centralized methods. Besides, an experiment with different federated learning settings is designed and conducted to verify the robustness of our method.",
        "authors": [
            "Yang Li",
            "Jiazheng Li",
            "Yi Wang"
        ],
        "citations": 143,
        "references": 32,
        "year": 2021
    },
    {
        "title": "Counterfactual Generative Networks",
        "abstract": "Neural networks are prone to learning shortcuts -- they often model simple correlations, ignoring more complex ones that potentially generalize better. Prior works on image classification show that instead of learning a connection to object shape, deep classifiers tend to exploit spurious correlations with low-level texture or the background for solving the classification task. In this work, we take a step towards more robust and interpretable classifiers that explicitly expose the task's causal structure. Building on current advances in deep generative modeling, we propose to decompose the image generation process into independent causal mechanisms that we train without direct supervision. By exploiting appropriate inductive biases, these mechanisms disentangle object shape, object texture, and background; hence, they allow for generating counterfactual images. We demonstrate the ability of our model to generate such images on MNIST and ImageNet. Further, we show that the counterfactual images can improve out-of-distribution robustness with a marginal drop in performance on the original classification task, despite being synthetic. Lastly, our generative model can be trained efficiently on a single GPU, exploiting common pre-trained models as inductive biases.",
        "authors": [
            "Axel Sauer",
            "Andreas Geiger"
        ],
        "citations": 114,
        "references": 53,
        "year": 2021
    },
    {
        "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
        "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) (CITATION) is the only prior work addressing the generative aspect of speech pre-training, which builds a text-free language model using discovered units. Unfortunately, because the units used in GSLM discard most prosodic information, GSLM fails to leverage prosody for better comprehension and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.",
        "authors": [
            "E. Kharitonov",
            "Ann Lee",
            "Adam Polyak",
            "Yossi Adi",
            "Jade Copet",
            "Kushal Lakhotia",
            "Tu Nguyen",
            "M. Rivière",
            "Abdel-rahman Mohamed",
            "Emmanuel Dupoux",
            "Wei-Ning Hsu"
        ],
        "citations": 106,
        "references": 48,
        "year": 2021
    },
    {
        "title": "Generative Adversarial Networks",
        "abstract": "Generative Adversarial Networks (GANs) have promoted a variety of applications in computer vision and natural language processing, among others, due to its generative model’s compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey to summarize systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this article also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.",
        "authors": [
            "Zhipeng Cai",
            "Zuobin Xiong",
            "Honghui Xu",
            "Peng Wang",
            "Wei Li",
            "Yi-Lun Pan"
        ],
        "citations": 127,
        "references": 162,
        "year": 2021
    },
    {
        "title": "Deep Generative Learning via Schrödinger Bridge",
        "abstract": "We propose to learn a generative model via entropy interpolation with a Schr\\\"{o}dinger Bridge. The generative learning task can be formulated as interpolating between a reference distribution and a target distribution based on the Kullback-Leibler divergence. At the population level, this entropy interpolation is characterized via an SDE on $[0,1]$ with a time-varying drift term. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by plugging the drift term estimated by a deep score estimator and a deep density ratio estimator into the Euler-Maruyama method. Under some mild smoothness assumptions of the target distribution, we prove the consistency of both the score estimator and the density ratio estimator, and then establish the consistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical results guarantee that the distribution learned by our approach converges to the target distribution. Experimental results on multimodal synthetic data and benchmark data support our theoretical findings and indicate that the generative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art GANs, suggesting a new formulation of generative learning. We demonstrate its usefulness in image interpolation and image inpainting.",
        "authors": [
            "Gefei Wang",
            "Yuling Jiao",
            "Qiang Xu",
            "Yang Wang",
            "Can Yang"
        ],
        "citations": 82,
        "references": 58,
        "year": 2021
    },
    {
        "title": "FD-GAN: Generative Adversarial Networks with Fusion-discriminator for Single Image Dehazing",
        "abstract": "Recently, convolutional neural networks (CNNs) have achieved great improvements in single image dehazing and attained much attention in research. Most existing learning-based dehazing methods are not fully end-to-end, which still follow the traditional dehazing procedure: first estimate the medium transmission and the atmospheric light, then recover the haze-free image based on the atmospheric scattering model. However, in practice, due to lack of priors and constraints, it is hard to precisely estimate these intermediate parameters. Inaccurate estimation further degrades the performance of dehazing, resulting in artifacts, color distortion and insufficient haze removal. To address this, we propose a fully end-to-end Generative Adversarial Networks with Fusion-discriminator (FD-GAN) for image dehazing. With the proposed Fusion-discriminator which takes frequency information as additional priors, our model can generator more natural and realistic dehazed images with less color distortion and fewer artifacts. Moreover, we synthesize a large-scale training dataset including various indoor and outdoor hazy images to boost the performance and we reveal that for learning-based dehazing methods, the performance is strictly influenced by the training data. Experiments have shown that our method reaches state-of-the-art performance on both public synthetic datasets and real-world images with more visually pleasing dehazed results.",
        "authors": [
            "Yu Dong",
            "Yihao Liu",
            "He Zhang",
            "Shifeng Chen",
            "Y. Qiao"
        ],
        "citations": 210,
        "references": 33,
        "year": 2020
    },
    {
        "title": "Generative Face Completion",
        "abstract": "In this paper, we propose an effective face completion algorithm using a deep generative model. Different from well-studied background completion, the face completion task is more challenging as it often requires to generate semantically new pixels for the missing key components (e.g., eyes and mouths) that contain large appearance variations. Unlike existing nonparametric algorithms that search for patches to synthesize, our algorithm directly generates contents for missing regions based on a neural network. The model is trained with a combination of a reconstruction loss, two adversarial losses and a semantic parsing loss, which ensures pixel faithfulness and local-global contents consistency. With extensive experimental results, we demonstrate qualitatively and quantitatively that our model is able to deal with a large area of missing pixels in arbitrary shapes and generate realistic face completion results.",
        "authors": [
            "Yijun Li",
            "Sifei Liu",
            "Jimei Yang",
            "Ming-Hsuan Yang"
        ],
        "citations": 592,
        "references": 29,
        "year": 2017
    },
    {
        "title": "StylePeople: A Generative Model of Fullbody Human Avatars",
        "abstract": "We propose a new type of full-body human avatars, which combines parametric mesh-based body model with a neural texture. We show that with the help of neural textures, such avatars can successfully model clothing and hair, which usually poses a problem for mesh-based approaches. We also show how these avatars can be created from multiple frames of a video using backpropagation. We then propose a generative model for such avatars that can be trained from datasets of images and videos of people. The generative model allows us to sample random avatars as well as to create dressed avatars of people from one or few images. The code for the project is available at saic-violet.github.io/style-people.",
        "authors": [
            "A. Grigorev",
            "K. Iskakov",
            "A. Ianina",
            "Renat Bashirov",
            "Ilya Zakharkin",
            "Alexander Vakhitov",
            "V. Lempitsky"
        ],
        "citations": 74,
        "references": 57,
        "year": 2021
    },
    {
        "title": "Perceptual Generative Adversarial Networks for Small Object Detection",
        "abstract": "Detecting small objects is notoriously challenging due to their low resolution and noisy representation. Existing object detection pipelines usually detect small objects through learning representations of all the objects at multiple scales. However, the performance gain of such ad hoc architectures is usually limited to pay off the computational cost. In this work, we address the small object detection problem by developing a single architecture that internally lifts representations of small objects to super-resolved ones, achieving similar characteristics as large objects and thus more discriminative for detection. For this purpose, we propose a new Perceptual Generative Adversarial Network (Perceptual GAN) model that improves small object detection through narrowing representation difference of small objects from the large ones. Specifically, its generator learns to transfer perceived poor representations of the small objects to super-resolved ones that are similar enough to real large objects to fool a competing discriminator. Meanwhile its discriminator competes with the generator to identify the generated representation and imposes an additional perceptual requirement - generated representations of small objects must be beneficial for detection purpose - on the generator. Extensive evaluations on the challenging Tsinghua-Tencent 100K [45] and the Caltech [9] benchmark well demonstrate the superiority of Perceptual GAN in detecting small objects, including traffic signs and pedestrians, over well-established state-of-the-arts.",
        "authors": [
            "Jianan Li",
            "Xiaodan Liang",
            "Yunchao Wei",
            "Tingfa Xu",
            "Jiashi Feng",
            "Shuicheng Yan"
        ],
        "citations": 687,
        "references": 46,
        "year": 2017
    },
    {
        "title": "WaterGAN: Unsupervised Generative Network to Enable Real-Time Color Correction of Monocular Underwater Images",
        "abstract": "This letter reports on WaterGAN, a generative adversarial network (GAN) for generating realistic underwater images from in-air image and depth pairings in an unsupervised pipeline used for color correction of monocular underwater images. Cameras onboard autonomous and remotely operated vehicles can capture high-resolution images to map the seafloor; however, underwater image formation is subject to the complex process of light propagation through the water column. The raw images retrieved are characteristically different than images taken in air due to effects, such as absorption and scattering, which cause attenuation of light at different rates for different wavelengths. While this physical process is well described theoretically, the model depends on many parameters intrinsic to the water column as well as the structure of the scene. These factors make recovery of these parameters difficult without simplifying assumptions or field calibration; hence, restoration of underwater images is a nontrivial problem. Deep learning has demonstrated great success in modeling complex nonlinear systems but requires a large amount of training data, which is difficult to compile in deep sea environments. Using WaterGAN, we generate a large training dataset of corresponding depth, in-air color images, and realistic underwater images. These data serve as input to a two-stage network for color correction of monocular underwater images. Our proposed pipeline is validated with testing on real data collected from both a pure water test tank and from underwater surveys collected in the field. Source code, sample datasets, and pretrained models are made publicly available.",
        "authors": [
            "Jie Li",
            "K. Skinner",
            "R. Eustice",
            "M. Johnson-Roberson"
        ],
        "citations": 611,
        "references": 30,
        "year": 2017
    },
    {
        "title": "Multivariate Time Series Imputation with Generative Adversarial Networks",
        "abstract": "Multivariate time series usually contain a large number of missing values, which hinders the application of advanced analysis methods on multivariate time series data. Conventional approaches to addressing the challenge of missing values, including mean/zero imputation, case deletion, and matrix factorization-based imputation, are all incapable of modeling the temporal dependencies and the nature of complex distribution in multivariate time series. In this paper, we treat the problem of missing value imputation as data generation. Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we propose to learn the overall distribution of a multivariate time series dataset with GAN, which is further used to generate the missing values for each sample. Different from the image data, the time series data are usually incomplete due to the nature of data recording process. A modified Gate Recurrent Unit is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that the proposed model outperformed the baselines in terms of accuracy of imputation. Experimental results also showed that a simple model on the imputed data can achieve state-of-the-art results on the prediction tasks, demonstrating the benefits of our model in downstream applications.",
        "authors": [
            "Yonghong Luo",
            "Xiangrui Cai",
            "Y. Zhang",
            "Jun Xu",
            "Xiaojie Yuan"
        ],
        "citations": 403,
        "references": 46,
        "year": 2018
    },
    {
        "title": "Generating Multi-label Discrete Patient Records using Generative Adversarial Networks",
        "abstract": "Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN.",
        "authors": [
            "E. Choi",
            "Siddharth Biswal",
            "B. Malin",
            "J. Duke",
            "W. Stewart",
            "Jimeng Sun"
        ],
        "citations": 520,
        "references": 54,
        "year": 2017
    },
    {
        "title": "Compressed Sensing MRI Reconstruction Using a Generative Adversarial Network With a Cyclic Loss",
        "abstract": "Compressed sensing magnetic resonance imaging (CS-MRI) has provided theoretical foundations upon which the time-consuming MRI acquisition process can be accelerated. However, it primarily relies on iterative numerical solvers, which still hinders their adaptation in time-critical applications. In addition, recent advances in deep neural networks have shown their potential in computer vision and image processing, but their adaptation to MRI reconstruction is still in an early stage. In this paper, we propose a novel deep learning-based generative adversarial model, <italic>RefineGAN</italic>, for fast and accurate CS-MRI reconstruction. The proposed model is a variant of fully-residual convolutional autoencoder and generative adversarial networks (GANs), specifically designed for CS-MRI formulation; it employs deeper generator and discriminator networks with cyclic data consistency loss for faithful interpolation in the given under-sampled <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>-space data. In addition, our solution leverages a chained network to further enhance the reconstruction quality. <italic>RefineGAN</italic> is fast and accurate—the reconstruction process is extremely rapid, as low as tens of milliseconds for reconstruction of a <inline-formula> <tex-math notation=\"LaTeX\">$256\\times 256$ </tex-math></inline-formula> image, because it is one-way deployment on a feed-forward network, and the image quality is superior even for extremely low sampling rate (as low as 10%) due to the data-driven nature of the method. We demonstrate that <italic>RefineGAN</italic> outperforms the state-of-the-art CS-MRI methods by a large margin in terms of both running time and image quality via evaluation using several open-source MRI databases.",
        "authors": [
            "Tran Minh Quan",
            "Thanh Nguyen-Duc",
            "W. Jeong"
        ],
        "citations": 528,
        "references": 54,
        "year": 2017
    },
    {
        "title": "Generative Causal Explanations for Graph Neural Networks",
        "abstract": "This paper presents Gem, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, Gem explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, Gem, once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets show that Gem achieves a relative increase of the explanation accuracy by up to $30\\%$ and speeds up the explanation process by up to $110\\times$ as compared to its state-of-the-art alternatives.",
        "authors": [
            "Wanyu Lin",
            "Hao Lan",
            "Baochun Li"
        ],
        "citations": 154,
        "references": 35,
        "year": 2021
    },
    {
        "title": "Vitruvion: A Generative Model of Parametric CAD Sketches",
        "abstract": "Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards. The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements. This relational specification can be viewed as the construction of a constraint program, allowing edits to coherently propagate to other parts of the design. Machine learning offers the intriguing possibility of accelerating the design process via generative modeling of these structures, enabling new tools such as autocompletion, constraint inference, and conditional synthesis. In this work, we present such an approach to generative modeling of parametric CAD sketches, which constitute the basic computational building blocks of modern mechanical design. Our model, trained on real-world designs from the SketchGraphs dataset, autoregressively synthesizes sketches as sequences of primitives, with initial coordinates, and constraints that reference back to the sampled primitives. As samples from the model match the constraint graph representation used in standard CAD software, they may be directly imported, solved, and edited according to downstream design tasks. In addition, we condition the model on various contexts, including partial sketches (primers) and images of hand-drawn sketches. Evaluation of the proposed approach demonstrates its ability to synthesize realistic CAD sketches and its potential to aid the mechanical design workflow.",
        "authors": [
            "Ari Seff",
            "Wenda Zhou",
            "Nick Richardson",
            "Ryan P. Adams"
        ],
        "citations": 53,
        "references": 29,
        "year": 2021
    },
    {
        "title": "Ceptre: A Language for Modeling Generative Interactive Systems",
        "abstract": "\n \n We present a rule specification language called Ceptre,intended to enable rapid prototyping for experimental game mechanics, especially in domains that depend on procedural generation and multi-agent simulation. Ceptre can be viewed as an explication of a new methodology for understanding games based on linear logic, a formal logic concerned with resource usage. We present a correspondence between gameplay and proof search in linear logic, building on prior work on generating narratives. In Ceptre, we introduce the ability to add interactivity selectively into a generative model, enabling inspection of intermediate states for debugging and exploration as well as a means of play. We claim that this methodology can support game designers and researchers in designing, anaylzing, and debugging the core systems of their work in generative, multi-agent gameplay. To support this claim, we provide two case studies implemented in Ceptre, one from interactive narrative and one from a strategy-like domain.\n \n",
        "authors": [
            "Chris Martens"
        ],
        "citations": 57,
        "references": 28,
        "year": 2021
    },
    {
        "title": "Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network",
        "abstract": "In this paper, we present a modular robotic system to tackle the problem of generating and performing antipodal robotic grasps for unknown objects from the n-channel image of the scene. We propose a novel Generative Residual Convolutional Neural Network (GR-ConvNet) model that can generate robust antipodal grasps from n-channel input at real-time speeds (∼20ms). We evaluate the proposed model architecture on standard datasets and a diverse set of household objects. We achieved state-of-the-art accuracy of 97.7% and 94.6% on Cornell and Jacquard grasping datasets, respectively. We also demonstrate a grasp success rate of 95.4% and 93% on household and adversarial objects, respectively, using a 7 DoF robotic arm.",
        "authors": [
            "Sulabh Kumra",
            "Shirin Joshi",
            "F. Sahin"
        ],
        "citations": 255,
        "references": 43,
        "year": 2019
    },
    {
        "title": "HumanGAN: A Generative Model of Human Images",
        "abstract": "Generative adversarial networks achieve great performance in photorealistic image synthesis in various domains, including human images. However, they usually employ latent vectors that encode the sampled outputs globally. This does not allow convenient control of semantically relevant individual parts of the image, and cannot draw samples that only differ in partial aspects, such as clothing style. We address these limitations and present a generative model for images of dressed humans offering control over pose, local body part appearance and garment style. This is the first method to solve various aspects of human image generation, such as global appearance sampling, pose transfer, parts and garment transfer, and part sampling jointly in a unified framework. As our model encodes part-based latent appearance vectors in a normalized pose-independent space and warps them to different poses, it preserves body and clothing appearance under varying posture. Experiments show that our flexible and general generative method outperforms task-specific baselines for pose-conditioned image generation, pose transfer and part sampling in terms of realism and output resolution.",
        "authors": [
            "Kripasindhu Sarkar",
            "Lingjie Liu",
            "Vladislav Golyanik",
            "C. Theobalt"
        ],
        "citations": 56,
        "references": 56,
        "year": 2021
    },
    {
        "title": "Generative modeling of brain maps with spatial autocorrelation",
        "abstract": null,
        "authors": [
            "J. Burt",
            "Markus Helmer",
            "Maxwell Shinn",
            "A. Anticevic",
            "J. Murray"
        ],
        "citations": 272,
        "references": 65,
        "year": 2020
    },
    {
        "title": "Electrocardiogram generation with a bidirectional LSTM-CNN generative adversarial network",
        "abstract": null,
        "authors": [
            "Fei Zhu",
            "Fei Ye",
            "Yuchen Fu",
            "QUAN LIU",
            "Bairong Shen"
        ],
        "citations": 278,
        "references": 41,
        "year": 2019
    },
    {
        "title": "A de novo molecular generation method using latent vector based generative adversarial network",
        "abstract": null,
        "authors": [
            "Oleksii Prykhodko",
            "Simon Johansson",
            "Panagiotis-Christos Kotsias",
            "Josep Arús‐Pous",
            "E. Bjerrum",
            "O. Engkvist",
            "Hongming Chen"
        ],
        "citations": 264,
        "references": 67,
        "year": 2019
    },
    {
        "title": "Generative-Discriminative Feature Representations for Open-Set Recognition",
        "abstract": "We address the problem of open-set recognition, where the goal is to determine if a given sample belongs to one of the classes used for training a model (known classes). The main challenge in open-set recognition is to disentangle open-set samples that produce high class activations from known-set samples. We propose two techniques to force class activations of open-set samples to be low. First, we train a generative model for all known classes and then augment the input with the representation obtained from the generative model to learn a classifier. This network learns to associate high classification probabilities both when image content is from the correct class as well as when the input and the reconstructed image are consistent with each other. Second, we use self-supervision to force the network to learn more informative featues when assigning class scores to improve separation of classes from each other and from open-set samples. We evaluate the performance of the proposed method with recent open-set recognition works across three datasets, where we obtain state-of-the-art results.",
        "authors": [
            "Pramuditha Perera",
            "Vlad I. Morariu",
            "R. Jain",
            "Varun Manjunatha",
            "Curtis Wigington",
            "Vicente Ordonez",
            "Vishal M. Patel"
        ],
        "citations": 167,
        "references": 30,
        "year": 2020
    },
    {
        "title": "A Variational Inequality Perspective on Generative Adversarial Networks",
        "abstract": "Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but they are notably difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "authors": [
            "Gauthier Gidel",
            "Hugo Berard",
            "Pascal Vincent",
            "Simon Lacoste-Julien"
        ],
        "citations": 343,
        "references": 69,
        "year": 2018
    },
    {
        "title": "Experimental Quantum Generative Adversarial Networks for Image Generation",
        "abstract": "Quantum machine learning is expected to be one of the first practical applications of near-term quantum devices. Pioneer theoretical works suggest that quantum generative adversarial networks (GANs) may exhibit a potential exponential advantage over classical GANs, thus attracting widespread attention. However, it remains elusive whether quantum GANs implemented on near-term quantum devices can actually solve real-world learning tasks. Here, we devise a flexible quantum GAN scheme to narrow this knowledge gap, which could accomplish image generation with arbitrarily high-dimensional features, and could also take advantage of quantum superposition to train multiple examples in parallel. For the first time, we experimentally achieve the learning and generation of real-world hand-written digit images on a superconducting quantum processor. Moreover, we utilize a gray-scale bar dataset to exhibit the competitive performance between quantum GANs and the classical GANs based on multilayer perceptron and convolutional neural network architectures, respectively, benchmarked by the Frechet Distance score. Our work provides guidance for developing advanced quantum generative models on near-term quantum devices and opens up an avenue for exploring quantum advantages in various GAN-related learning tasks.",
        "authors": [
            "Heliang Huang",
            "Yuxuan Du",
            "M. Gong",
            "You-Wei Zhao",
            "Yulin Wu",
            "Chaoyue Wang",
            "Shaowei Li",
            "Futian Liang",
            "Jin Lin",
            "Yu Xu",
            "Rui Yang",
            "Tongliang Liu",
            "Min-Hsiu Hsieh",
            "H. Deng",
            "H. Rong",
            "Cheng-Zhi Peng",
            "Chao Lu",
            "Yu-Ao Chen",
            "D. Tao",
            "Xiaobo Zhu",
            "Jian-Wei Pan"
        ],
        "citations": 158,
        "references": 87,
        "year": 2020
    },
    {
        "title": "Designing complex architectured materials with generative adversarial networks",
        "abstract": "Complex architectured materials are designed with generative adversarial networks to approach Hashin-Shtrikman upper bounds. Architectured materials on length scales from nanometers to meters are desirable for diverse applications. Recent advances in additive manufacturing have made mass production of complex architectured materials technologically and economically feasible. Existing architecture design approaches such as bioinspiration, Edisonian, and optimization, however, generally rely on experienced designers’ prior knowledge, limiting broad applications of architectured materials. Particularly challenging is designing architectured materials with extreme properties, such as the Hashin-Shtrikman upper bounds on isotropic elasticity in an experience-free manner without prior knowledge. Here, we present an experience-free and systematic approach for the design of complex architectured materials with generative adversarial networks. The networks are trained using simulation data from millions of randomly generated architectures categorized based on different crystallographic symmetries. We demonstrate modeling and experimental results of more than 400 two-dimensional architectures that approach the Hashin-Shtrikman upper bounds on isotropic elastic stiffness with porosities from 0.05 to 0.75.",
        "authors": [
            "Yunwei Mao",
            "Qi He",
            "Xuanhe Zhao"
        ],
        "citations": 189,
        "references": 53,
        "year": 2020
    },
    {
        "title": "Generative Adversarial Networks and Its Applications in Biomedical Informatics",
        "abstract": "The basic Generative Adversarial Networks (GAN) model is composed of the input vector, generator, and discriminator. Among them, the generator and discriminator are implicit function expressions, usually implemented by deep neural networks. GAN can learn the generative model of any data distribution through adversarial methods with excellent performance. It has been widely applied to different areas since it was proposed in 2014. In this review, we introduced the origin, specific working principle, and development history of GAN, various applications of GAN in digital image processing, Cycle-GAN, and its application in medical imaging analysis, as well as the latest applications of GAN in medical informatics and bioinformatics.",
        "authors": [
            "L. Lan",
            "Lei You",
            "Zeyang Zhang",
            "Zhiwei Fan",
            "Weiling Zhao",
            "Nianyin Zeng",
            "Yidong Chen",
            "Xiaobo Zhou"
        ],
        "citations": 152,
        "references": 111,
        "year": 2020
    },
    {
        "title": "Synthetic promoter design in Escherichia coli based on a deep generative network",
        "abstract": "Abstract Promoter design remains one of the most important considerations in metabolic engineering and synthetic biology applications. Theoretically, there are 450 possible sequences for a 50-nt promoter, of which naturally occurring promoters make up only a small subset. To explore the vast number of potential sequences, we report a novel AI-based framework for de novo promoter design in Escherichia coli. The model, which was guided by sequence features learned from natural promoters, could capture interactions between nucleotides at different positions and design novel synthetic promoters in silico. We combined a deep generative model that guides the search for artificial sequences with a predictive model to preselect the most promising promoters. The AI-designed promoters were optimized based on the promoter activity in E. coli and the predictive model. After two rounds of optimization, up to 70.8% of the AI-designed promoters were experimentally demonstrated to be functional, and few of them shared significant sequence similarity with the E. coli genome. Our work provided an end-to-end approach to the de novo design of novel promoter elements, indicating the potential to apply deep learning methods to de novo genetic element design.",
        "authors": [
            "Ye Wang",
            "Haochen Wang",
            "Lei Wei",
            "Shuailin Li",
            "Liyang Liu",
            "Xiaowo Wang"
        ],
        "citations": 125,
        "references": 60,
        "year": 2020
    },
    {
        "title": "Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model",
        "abstract": "This paper studies a central issue in modern reinforcement learning, the sample efficiency, and makes progress toward solving an idealistic scenario that assumes access to a generative model or a simulator. Despite a large number of prior works tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy has yet to be determined. In particular, all prior results suffer from a severe sample size barrier in the sense that their claimed statistical guarantees hold only when the sample size exceeds some enormous threshold. The current paper overcomes this barrier and fully settles this problem; more specifically, we establish the minimax optimality of the model-based approach for any given target accuracy level. To the best of our knowledge, this work delivers the first minimax-optimal guarantees that accommodate the entire range of sample sizes (beyond which finding a meaningful policy is information theoretically infeasible).",
        "authors": [
            "Gen Li",
            "Yuting Wei",
            "Yuejie Chi",
            "Yuantao Gu",
            "Yuxin Chen"
        ],
        "citations": 117,
        "references": 76,
        "year": 2020
    },
    {
        "title": "Triple Generative Adversarial Nets",
        "abstract": "Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players---a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.",
        "authors": [
            "Chongxuan Li",
            "T. Xu",
            "Jun Zhu",
            "Bo Zhang"
        ],
        "citations": 438,
        "references": 37,
        "year": 2017
    },
    {
        "title": "Contrastive Triple Extraction with Generative Transformer",
        "abstract": "Triple extraction is an essential task in information extraction for natural language processing and knowledge graph construction. In this paper, we revisit the end-to-end triple extraction task for sequence generation. Since generative triple extraction may struggle to capture long-term dependencies and generate unfaithful triples, we introduce a novel model, contrastive triple extraction with a generative transformer. Specifically, we introduce a single shared transformer module for encoder-decoder-based generation. To generate faithful results, we propose a novel triplet contrastive training object. Moreover, we introduce two mechanisms to further improve model performance (i.e., batch-wise dynamic attention-masking and triple-wise calibration). Experimental results on three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves better performance than that of baselines.",
        "authors": [
            "Hongbin Ye",
            "Ningyu Zhang",
            "Shumin Deng",
            "Mosha Chen",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen"
        ],
        "citations": 118,
        "references": 55,
        "year": 2020
    },
    {
        "title": "Assessing the impact of generative AI on medicinal chemistry",
        "abstract": null,
        "authors": [
            "W. Walters",
            "M. Murcko"
        ],
        "citations": 140,
        "references": 23,
        "year": 2020
    },
    {
        "title": "Hierarchical Generative Modeling for Controllable Speech Synthesis",
        "abstract": "This paper proposes a neural sequence-to-sequence text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model based on the variational autoencoder (VAE) framework, with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, we train a high-quality controllable TTS model on real found data, which is capable of inferring speaker and style attributes from a noisy utterance and use it to synthesize clean speech with controllable speaking style.",
        "authors": [
            "Wei-Ning Hsu",
            "Yu Zhang",
            "Ron J. Weiss",
            "H. Zen",
            "Yonghui Wu",
            "Yuxuan Wang",
            "Yuan Cao",
            "Ye Jia",
            "Z. Chen",
            "Jonathan Shen",
            "Patrick Nguyen",
            "Ruoming Pang"
        ],
        "citations": 269,
        "references": 45,
        "year": 2018
    },
    {
        "title": "Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series",
        "abstract": "Today's Cyber-Physical Systems (CPSs) are large, complex, and affixed with networked sensors and actuators that are targets for cyber-attacks. Conventional detection techniques are unable to deal with the increasingly dynamic and complex nature of the CPSs. On the other hand, the networked sensors and actuators generate large amounts of data streams that can be continuously monitored for intrusion events. Unsupervised machine learning techniques can be used to model the system behaviour and classify deviant behaviours as possible attacks. In this work, we proposed a novel Generative Adversarial Networks-based Anomaly Detection (GAN-AD) method for such complex networked CPSs. We used LSTM-RNN in our GAN to capture the distribution of the multivariate time series of the sensors and actuators under normal working conditions of a CPS. Instead of treating each sensor's and actuator's time series independently, we model the time series of multiple sensors and actuators in the CPS concurrently to take into account of potential latent interactions between them. To exploit both the generator and the discriminator of our GAN, we deployed the GAN-trained discriminator together with the residuals between generator-reconstructed data and the actual samples to detect possible anomalies in the complex CPS. We used our GAN-AD to distinguish abnormal attacked situations from normal working conditions for a complex six-stage Secure Water Treatment (SWaT) system. Experimental results showed that the proposed strategy is effective in identifying anomalies caused by various attacks with high detection rate and low false positive rate as compared to existing methods.",
        "authors": [
            "Dan Li",
            "Dacheng Chen",
            "Jonathan Goh",
            "See-Kiong Ng"
        ],
        "citations": 278,
        "references": 46,
        "year": 2018
    },
    {
        "title": "Sample Complexity of Robust Reinforcement Learning with a Generative Model",
        "abstract": "The Robust Markov Decision Process (RMDP) framework focuses on designing control policies that are robust against the parameter uncertainties due to the mismatches between the simulator model and real-world settings. An RMDP problem is typically formulated as a max-min problem, where the objective is to find the policy that maximizes the value function for the worst possible model that lies in an uncertainty set around a nominal model. The standard robust dynamic programming approach requires the knowledge of the nominal model for computing the optimal robust policy. In this work, we propose a model-based reinforcement learning (RL) algorithm for learning an $\\epsilon$-optimal robust policy when the nominal model is unknown. We consider three different forms of uncertainty sets, characterized by the total variation distance, chi-square divergence, and KL divergence. For each of these uncertainty sets, we give a precise characterization of the sample complexity of our proposed algorithm. In addition to the sample complexity results, we also present a formal analytical argument on the benefit of using robust policies. Finally, we demonstrate the performance of our algorithm on two benchmark problems.",
        "authors": [
            "Kishan Panaganti",
            "D. Kalathil"
        ],
        "citations": 64,
        "references": 39,
        "year": 2021
    },
    {
        "title": "A generative modeling approach for benchmarking and training shallow quantum circuits",
        "abstract": null,
        "authors": [
            "Marcello Benedetti",
            "Delfina Garcia-Pintos",
            "O. Perdomo",
            "Vicente Leyton-Ortega",
            "Y. Nam",
            "A. Perdomo-Ortiz"
        ],
        "citations": 305,
        "references": 72,
        "year": 2018
    },
    {
        "title": "Molecular generative model based on conditional variational autoencoder for de novo molecular design",
        "abstract": null,
        "authors": [
            "Jaechang Lim",
            "Seongok Ryu",
            "Jin Woo Kim",
            "W. Kim"
        ],
        "citations": 311,
        "references": 31,
        "year": 2018
    },
    {
        "title": "A case study of conditional deep convolutional generative adversarial networks in machine fault diagnosis",
        "abstract": null,
        "authors": [
            "Jia Luo",
            "Jinying Huang",
            "Hongmei Li"
        ],
        "citations": 132,
        "references": 53,
        "year": 2020
    },
    {
        "title": "Deep Generative Modeling for Protein Design",
        "abstract": null,
        "authors": [
            "Alexey Strokach",
            "Philip M. Kim"
        ],
        "citations": 75,
        "references": 95,
        "year": 2021
    },
    {
        "title": "Convolutional Graph Autoencoder: A Generative Deep Neural Network for Probabilistic Spatio-Temporal Solar Irradiance Forecasting",
        "abstract": "Machine learning on graphs is an important and omnipresent task for a vast variety of applications including anomaly detection and dynamic network analysis. In this paper, a deep generative model is introduced to capture continuous probability densities corresponding to the nodes of an arbitrary graph. In contrast to all learning formulations in the area of discriminative pattern recognition, we propose a scalable generative optimization/algorithm theoretically proved to capture distributions at the nodes of a graph. Our model is able to generate samples from the probability densities learned at each node. This probabilistic data generation model, i.e., convolutional graph autoencoder (CGAE), is devised based on the localized first-order approximation of spectral graph convolutions, deep learning, and the variational Bayesian inference. We apply the CGAE to a new problem, the spatio-temporal probabilistic solar irradiance prediction. Multiple solar radiation measurement sites in a wide area in northern states of the U.S. are modeled as an undirected graph. Using our proposed model, the distribution of future irradiance given historical radiation observations is estimated for every site/node. Numerical results on the national solar radiation database show state-of-the-art performance for probabilistic radiation prediction on geographically distributed irradiance data in terms of reliability, sharpness, and continuous ranked probability score.",
        "authors": [
            "Mahdi Khodayar",
            "Saeed Mohammadi",
            "M. Khodayar",
            "Jianhui Wang",
            "Guangyi Liu"
        ],
        "citations": 113,
        "references": 58,
        "year": 2020
    },
    {
        "title": "Hamiltonian Generative Networks",
        "abstract": "The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. We hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to deep learning.",
        "authors": [
            "Peter Toth",
            "Danilo Jimenez Rezende",
            "Andrew Jaegle",
            "S. Racanière",
            "Aleksandar Botev",
            "I. Higgins"
        ],
        "citations": 205,
        "references": 35,
        "year": 2019
    },
    {
        "title": "E²GAN: End-to-End Generative Adversarial Network for Multivariate Time Series Imputation",
        "abstract": "The missing values, appear in most of multivariate time series, prevent advanced analysis of multivariate time series data. Existing imputation approaches try to deal with missing values by deletion, statistical imputation, machine learning based imputation and generative imputation. However, these methods are either incapable of dealing with temporal information or multi-stage. This paper proposes an end-to-end generative model E²GAN to impute missing values in multivariate time series. With the help of the discriminative loss and the squared error loss, E²GAN can impute the incomplete time series by the nearest generated complete time series at one stage. Experiments on multiple real-world datasets show that our model outperforms the baselines on the imputation accuracy and achieves state-of-the-art classification/regression results on the downstream applications. Additionally, our method also gains better time efficiency than multi-stage method on the training of neural networks.",
        "authors": [
            "Yonghong Luo",
            "Ying Zhang",
            "Xiangrui Cai",
            "Xiaojie Yuan"
        ],
        "citations": 204,
        "references": 32,
        "year": 2019
    },
    {
        "title": "Transforming and Projecting Images into Class-conditional Generative Networks",
        "abstract": null,
        "authors": [
            "Minyoung Huh",
            "Richard Zhang",
            "Jun-Yan Zhu",
            "S. Paris",
            "Aaron Hertzmann"
        ],
        "citations": 107,
        "references": 73,
        "year": 2020
    },
    {
        "title": "Generative Modeling with Denoising Auto-Encoders and Langevin Sampling",
        "abstract": "We study convergence of a generative modeling method that first estimates the score function of the distribution using Denoising Auto-Encoders (DAE) or Denoising Score Matching (DSM) and then employs Langevin diffusion for sampling. We show that both DAE and DSM provide estimates of the score of the Gaussian smoothed population density, allowing us to apply the machinery of Empirical Processes. \nWe overcome the challenge of relying only on $L^2$ bounds on the score estimation error and provide finite-sample bounds in the Wasserstein distance between the law of the population distribution and the law of this sampling scheme. We then apply our results to the homotopy method of arXiv:1907.05600 and provide theoretical justification for its empirical success.",
        "authors": [
            "A. Block",
            "Youssef Mroueh",
            "A. Rakhlin"
        ],
        "citations": 86,
        "references": 39,
        "year": 2020
    },
    {
        "title": "Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis",
        "abstract": null,
        "authors": [
            "Ceyuan Yang",
            "Yujun Shen",
            "Bolei Zhou"
        ],
        "citations": 195,
        "references": 58,
        "year": 2019
    },
    {
        "title": "Content-aware generative modeling of graphic design layouts",
        "abstract": "Layout is fundamental to graphic designs. For visual attractiveness and efficient communication of messages and ideas, graphic design layouts often have great variation, driven by the contents to be presented. In this paper, we study the problem of content-aware graphic design layout generation. We propose a deep generative model for graphic design layouts that is able to synthesize layout designs based on the visual and textual semantics of user inputs. Unlike previous approaches that are oblivious to the input contents and rely on heuristic criteria, our model captures the effect of visual and textual contents on layouts, and implicitly learns complex layout structure variations from data without the use of any heuristic rules. To train our model, we build a large-scale magazine layout dataset with fine-grained layout annotations and keyword labeling. Experimental results show that our model can synthesize high-quality layouts based on the visual semantics of input images and keyword-based summary of input text. We also demonstrate that our model internally learns powerful features that capture the subtle interaction between contents and layouts, which are useful for layout-aware design retrieval.",
        "authors": [
            "Xinru Zheng",
            "Xiaotian Qiao",
            "Ying Cao",
            "Rynson W. H. Lau"
        ],
        "citations": 174,
        "references": 48,
        "year": 2019
    },
    {
        "title": "Defending Neural Backdoors via Generative Distribution Modeling",
        "abstract": "Neural backdoor attack is emerging as a severe security threat to deep learning, while the capability of existing defense methods is limited, especially for complex backdoor triggers. In the work, we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers, all of which can influence the backdoored model. Thus, previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution, e.g., via generative modeling, is a key of effective defense. However, existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work, we propose max-entropy staircase approximator (MESA) for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.",
        "authors": [
            "Ximing Qiao",
            "Yukun Yang",
            "H. Li"
        ],
        "citations": 170,
        "references": 26,
        "year": 2019
    },
    {
        "title": "Generative Pre-Training for Speech with Autoregressive Predictive Coding",
        "abstract": "Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.",
        "authors": [
            "Yu-An Chung",
            "James R. Glass"
        ],
        "citations": 166,
        "references": 41,
        "year": 2019
    },
    {
        "title": "Conditional Generative Neural System for Probabilistic Trajectory Prediction",
        "abstract": "Effective understanding of the environment and accurate trajectory prediction of surrounding dynamic obstacles are critical for intelligent systems such as autonomous vehicles and wheeled mobile robotics navigating in complex scenarios to achieve safe and high-quality decision making, motion planning and control. Due to the uncertain nature of the future, it is desired to make inference from a probability perspective instead of deterministic prediction. In this paper, we propose a conditional generative neural system (CGNS) for probabilistic trajectory prediction to approximate the data distribution, with which realistic, feasible and diverse future trajectory hypotheses can be sampled. The system combines the strengths of conditional latent space learning and variational divergence minimization, and leverages both static context and interaction information with soft attention mechanisms. We also propose a regularization method for incorporating soft constraints into deep neural networks with differentiable barrier functions, which can regulate and push the generated samples into the feasible regions. The proposed system is evaluated on several public benchmark datasets for pedestrian trajectory prediction and a roundabout naturalistic driving dataset collected by ourselves. The experimental results demonstrate that our model achieves better performance than various baseline approaches in terms of prediction accuracy.",
        "authors": [
            "Jiachen Li",
            "Hengbo Ma",
            "M. Tomizuka"
        ],
        "citations": 163,
        "references": 43,
        "year": 2019
    },
    {
        "title": "Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal",
        "abstract": "This work considers the sample and computational complexity of obtaining an $\\epsilon$-optimal policy in a discounted Markov Decision Process (MDP), given only access to a generative model. In this work, we study the effectiveness of the most natural plug-in approach to model-based planning: we build the maximum likelihood estimate of the transition model in the MDP from observations and then find an optimal policy in this empirical MDP. We ask arguably the most basic and unresolved question in model based planning: is the naive \"plug-in\" approach, non-asymptotically, minimax optimal in the quality of the policy it finds, given a fixed sample size? Here, the non-asymptotic regime refers to when the sample size is sublinear in the model size. \nWith access to a generative model, we resolve this question in the strongest possible sense: our main result shows that \\emph{any} high accuracy solution in the plug-in model constructed with $N$ samples, provides an $\\epsilon$-optimal policy in the true underlying MDP (where $\\epsilon$ is the minimax accuracy with $N$ samples at every state, action pair). In comparison, all prior (non-asymptotically) minimax optimal results use model free approaches, such as the Variance Reduced Q-value iteration algorithm (Sidford et al 2018), while the best known model-based results (e.g. Azar et al 2013) require larger sample sizes in their dependence on the planning horizon or the state space. Notably, we show that the model-based approach allows the use of \\emph{any} efficient planning algorithm in the empirical MDP, which simplifies algorithm design as this approach does not tie the algorithm to the sampling procedure. The core of our analysis is avnovel \"absorbing MDP\" construction to address the statistical dependency issues that arise in the analysis of model-based planning approaches, a construction which may be helpful more generally.",
        "authors": [
            "Alekh Agarwal",
            "S. Kakade",
            "Lin F. Yang"
        ],
        "citations": 160,
        "references": 26,
        "year": 2019
    },
    {
        "title": "Generative adversarial networks (GAN) based efficient sampling of chemical composition space for inverse design of inorganic materials",
        "abstract": null,
        "authors": [
            "Yabo Dan",
            "Yong Zhao",
            "Xiang Li",
            "Shaobo Li",
            "Ming Hu",
            "Jianjun Hu"
        ],
        "citations": 175,
        "references": 41,
        "year": 2019
    },
    {
        "title": "Learning with Good Feature Representations in Bandits and in RL with a Generative Model",
        "abstract": "The construction by Du et al. (2019) implies that even if a learner is given linear features in $\\mathbb R^d$ that approximate the rewards in a bandit with a uniform error of $\\epsilon$, then searching for an action that is optimal up to $O(\\epsilon)$ requires examining essentially all actions. We use the Kiefer-Wolfowitz theorem to prove a positive result that by checking only a few actions, a learner can always find an action that is suboptimal with an error of at most $O(\\epsilon \\sqrt{d})$. Thus, features are useful when the approximation error is small relative to the dimensionality of the features. The idea is applied to stochastic bandits and reinforcement learning with a generative model where the learner has access to $d$-dimensional linear features that approximate the action-value functions for all policies to an accuracy of $\\epsilon$. For linear bandits, we prove a bound on the regret of order $\\sqrt{dn \\log(k)} + \\epsilon n \\sqrt{d} \\log(n)$ with $k$ the number of actions and $n$ the horizon. For RL we show that approximate policy iteration can learn a policy that is optimal up to an additive error of order $\\epsilon \\sqrt{d}/(1 - \\gamma)^2$ and using $d/(\\epsilon^2(1 - \\gamma)^4)$ samples from a generative model. These bounds are independent of the finer details of the features. We also investigate how the structure of the feature set impacts the tradeoff between sample complexity and estimation error.",
        "authors": [
            "Tor Lattimore",
            "Csaba Szepesvari"
        ],
        "citations": 160,
        "references": 30,
        "year": 2019
    },
    {
        "title": "Deep Generative Modeling for Mechanistic-based Learning and Design of Metamaterial Systems",
        "abstract": null,
        "authors": [
            "Liwei Wang",
            "Yu-Chin Chan",
            "Faez Ahmed",
            "Zhao Liu",
            "Ping Zhu",
            "Wei Chen"
        ],
        "citations": 181,
        "references": 64,
        "year": 2020
    },
    {
        "title": "Diabetic Retinopathy Diagnosis Using Multichannel Generative Adversarial Network With Semisupervision",
        "abstract": "Diabetic retinopathy (DR) is one of the major causes of blindness. It is of great significance to apply deep-learning techniques for DR recognition. However, deep-learning algorithms often depend on large amounts of labeled data, which is expensive and time-consuming to obtain in the medical imaging area. In addition, the DR features are inconspicuous and spread out over high-resolution fundus images. Therefore, it is a big challenge to learn the distribution of such DR features. This article proposes a multichannel-based generative adversarial network (MGAN) with semisupervision to grade DR. The multichannel generative model is developed to generate a series of subfundus images corresponding to the scattering DR features. By minimizing the dependence on labeled data, the proposed semisupervised MGAN can identify the inconspicuous lesion features by using high-resolution fundus images without compression. Experimental results on the public Messidor data set show that the proposed model can grade DR effectively. Note to Practitioners—This article is motivated by the challenging problem due to the inadequacy of labeled data in medical image analysis and the dispersion of efficient features in high-resolution medical images. As for the inadequacy of labeled data in medical image analysis, the reasons mainly include the followings: 1) the high-quality annotation of medical imaging sample depends heavily on scarce medical expertise which is very expensive and 2) comparing with natural issues, it is more difficult to collect medical images because of privacy issues. It is of great significance to apply deep-learning techniques for diabetic retinopathy (DR) recognition. In this article, the multichannel generative adversarial network (GAN) with semisupervision is developed for DR-aided diagnosis. The proposed model can deal with DR classification problem with inadequacy of labeled data in the following ways: 1) the multichannel generative scheme is proposed to generate a series of subfundus images corresponding to the scattering DR features and 2) the proposed multichannel-based GAN (MGAN) model with semisupervision can make full use of both labeled data and unlabeled data. The experimental results demonstrate that the proposed model outperforms the other representative models in terms of accuracy, area under ROC curve (AUC), sensitivity, and specificity.",
        "authors": [
            "Shuqiang Wang",
            "Xiangyu Wang",
            "Yong Hu",
            "Yanyan Shen",
            "Zhile Yang",
            "Min Gan",
            "Baiying Lei"
        ],
        "citations": 100,
        "references": 68,
        "year": 2020
    },
    {
        "title": "Weakly-Supervised Action Localization by Generative Attention Modeling",
        "abstract": "Weakly-supervised temporal action localization is a problem of learning an action localization model with only video-level action labeling available. The general framework largely relies on the classification activation, which employs an attention model to identify the action-related frames and then categorizes them into different classes. Such method results in the action-context confusion issue: context frames near action clips tend to be recognized as action frames themselves, since they are closely related to the specific classes. To solve the problem, in this paper we propose to model the class-agnostic frame-wise probability conditioned on the frame attention using conditional Variational Auto-Encoder (VAE). With the observation that the context exhibits notable difference from the action at representation level, a probabilistic model, i.e., conditional VAE, is learned to model the likelihood of each frame given the attention. By maximizing the conditional probability with respect to the attention, the action and non-action frames are well separated. Experiments on THUMOS14 and ActivityNet1.2 demonstrate advantage of our method and effectiveness in handling action-context confusion problem. Code is now available on GitHub.",
        "authors": [
            "Baifeng Shi",
            "Qi Dai",
            "Yadong Mu",
            "Jingdong Wang"
        ],
        "citations": 144,
        "references": 60,
        "year": 2020
    },
    {
        "title": "Gradient Matching Generative Networks for Zero-Shot Learning",
        "abstract": "Zero-shot learning (ZSL) is one of the most promising problems where substantial progress can potentially be achieved through unsupervised learning, due to distributional differences between supervised and zero-shot classes. For this reason, several works investigate the incorporation of discriminative domain adaptation techniques into ZSL, which, however, lead to modest improvements in ZSL accuracy. In contrast, we propose a generative model that can naturally learn from unsupervised examples, and synthesize training examples for unseen classes purely based on their class embeddings, and therefore, reduce the zero-shot learning problem into a supervised classification task. The proposed approach consists of two important components: (i) a conditional Generative Adversarial Network that learns to produce samples that mimic the characteristics of unsupervised data examples, and (ii) the Gradient Matching (GM) loss that measures the quality of the gradient signal obtained from the synthesized examples. Using our GM loss formulation, we enforce the generator to produce examples from which accurate classifiers can be trained. Experimental results on several ZSL benchmark datasets show that our approach leads to significant improvements over the state of the art in generalized zero-shot classification.",
        "authors": [
            "Mert Bulent Sariyildiz",
            "R. G. Cinbis"
        ],
        "citations": 136,
        "references": 55,
        "year": 2019
    },
    {
        "title": "A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts",
        "abstract": "Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g. Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision. Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning.",
        "authors": [
            "Yizhe Zhu",
            "Mohamed Elhoseiny",
            "Bingchen Liu",
            "Xi Peng",
            "A. Elgammal"
        ],
        "citations": 374,
        "references": 53,
        "year": 2017
    },
    {
        "title": "Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification",
        "abstract": "We propose a generative model of unordered point sets, such as point clouds, in the forms of an energy-based model, where the energy function is parameterized by an input-permutation-invariant bottom-up neural network. The energy function learns a coordinate encoding of each point and then aggregates all individual point features into an energy for the whole point cloud. We call our model the Generative PointNet because it can be derived from the discriminative PointNet. Our model can be trained by MCMC-based maximum likelihood learning (as well as its variants), without the help of any assisting networks like those in GANs and VAEs. Unlike most point cloud generators that rely on hand-crafted distance metrics, our model does not require any hand-crafted distance metric for the point cloud generation, because it synthesizes point clouds by matching observed examples in terms of statistical properties defined by the energy function. Furthermore, we can learn a short-run MCMC toward the energy-based model as a flow-like generator for point cloud reconstruction and interpolation. The learned point cloud representation can be useful for point cloud classification. Experiments demonstrate the advantages of the proposed generative model of point clouds.",
        "authors": [
            "Jianwen Xie",
            "Yifei Xu",
            "Zilong Zheng",
            "Song-Chun Zhu",
            "Y. Wu"
        ],
        "citations": 74,
        "references": 60,
        "year": 2020
    },
    {
        "title": "Robust Inference via Generative Classifiers for Handling Noisy Labels",
        "abstract": "Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy training datasets. To mitigate the issue, we propose a novel inference method, termed Robust Generative classifier (RoG), applicable to any discriminative (e.g., softmax) neural classifier pre-trained on noisy datasets. In particular, we induce a generative classifier on top of hidden feature spaces of the pre-trained DNNs, for obtaining a more robust decision boundary. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy with neither re-training of the deep model nor changing its architectures. With the assumption of Gaussian distribution for features, we prove that RoG generalizes better than baselines under noisy labels. Finally, we propose the ensemble version of RoG to improve its performance by investigating the layer-wise characteristics of DNNs. Our extensive experimental results demonstrate the superiority of RoG given different learning models optimized by several training techniques to handle diverse scenarios of noisy labels.",
        "authors": [
            "Kimin Lee",
            "Sukmin Yun",
            "Kibok Lee",
            "Honglak Lee",
            "Bo Li",
            "Jinwoo Shin"
        ],
        "citations": 126,
        "references": 57,
        "year": 2019
    },
    {
        "title": "Generative Modeling Using the Sliced Wasserstein Distance",
        "abstract": "Generative Adversarial Nets (GANs) are very successful at modeling distributions from given samples, even in the high-dimensional case. However, their formulation is also known to be hard to optimize and often not stable. While this is particularly true for early GAN formulations, there has been significant empirically motivated and theoretically founded progress to improve stability, for instance, by using the Wasserstein distance rather than the Jenson-Shannon divergence. Here, we consider an alternative formulation for generative modeling based on random projections which, in its simplest form, results in a single objective rather than a saddle-point formulation. By augmenting this approach with a discriminator we improve its accuracy. We found our approach to be significantly more stable compared to even the improved Wasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated in our method is a good measure of the actual distance between the distributions and, for the first time for GAN training, we are able to show estimates for the same.",
        "authors": [
            "Ishani Deshpande",
            "Ziyu Zhang",
            "A. Schwing"
        ],
        "citations": 209,
        "references": 37,
        "year": 2018
    },
    {
        "title": "Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects",
        "abstract": "We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for image sequences. It can reliably discover and track objects through the sequence; it can also conditionally generate future frames, thereby simulating expected motion of objects. This is achieved by explicitly encoding object numbers, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al. 2016), including unsupervised learning, made possible by inductive biases present in the model structure. We use a moving multi-\\textsc{mnist} dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how \\textsc{sqair} overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.",
        "authors": [
            "Adam R. Kosiorek",
            "Hyunjik Kim",
            "I. Posner",
            "Y. Teh"
        ],
        "citations": 250,
        "references": 49,
        "year": 2018
    },
    {
        "title": "Generative Adversarial User Model for Reinforcement Learning Based Recommendation System",
        "abstract": "There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.",
        "authors": [
            "Xinshi Chen",
            "Shuang Li",
            "Hui Li",
            "Shaohua Jiang",
            "Yuan Qi",
            "Le Song"
        ],
        "citations": 189,
        "references": 30,
        "year": 2018
    },
    {
        "title": "Poisoning Attack in Federated Learning using Generative Adversarial Nets",
        "abstract": "Federated learning is a novel distributed learning framework, where the deep learning model is trained in a collaborative manner among thousands of participants. The shares between server and participants are only model parameters, which prevent the server from direct access to the private training data. However, we notice that the federated learning architecture is vulnerable to an active attack from insider participants, called poisoning attack, where the attacker can act as a benign participant in federated learning to upload the poisoned update to the server so that he can easily affect the performance of the global model. In this work, we study and evaluate a poisoning attack in federated learning system based on generative adversarial nets (GAN). That is, an attacker first acts as a benign participant and stealthily trains a GAN to mimic prototypical samples of the other participants' training set which does not belong to the attacker. Then these generated samples will be fully controlled by the attacker to generate the poisoning updates, and the global model will be compromised by the attacker with uploading the scaled poisoning updates to the server. In our evaluation, we show that the attacker in our construction can successfully generate samples of other benign participants using GAN and the global model performs more than 80% accuracy on both poisoning tasks and main tasks.",
        "authors": [
            "Jiale Zhang",
            "Junjun Chen",
            "Di Wu",
            "Bing Chen",
            "Shui Yu"
        ],
        "citations": 142,
        "references": 21,
        "year": 2019
    },
    {
        "title": "Generative replay with feedback connections as a general strategy for continual learning",
        "abstract": "A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \"soft targets\") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.",
        "authors": [
            "Gido M. van de Ven",
            "A. Tolias"
        ],
        "citations": 216,
        "references": 41,
        "year": 2018
    },
    {
        "title": "High Dimensional Channel Estimation Using Deep Generative Networks",
        "abstract": "This paper presents a novel compressed sensing (CS) approach to high dimensional wireless channel estimation by optimizing the input to a deep generative network. Channel estimation using generative networks relies on the assumption that the reconstructed channel lies in the range of a generative model. Channel reconstruction using generative priors outperforms conventional CS techniques and requires fewer pilots. It also eliminates the need of a priori knowledge of the sparsifying basis, instead using the structure captured by the deep generative model as a prior. Using this prior, we also perform channel estimation from one-bit quantized pilot measurements, and propose a novel optimization objective function that attempts to maximize the correlation between the received signal and the generator’s channel estimate while minimizing the rank of the channel estimate. Our approach significantly outperforms sparse signal recovery methods such as Orthogonal Matching Pursuit (OMP) and Approximate Message Passing (AMP) algorithms such as EM-GM-AMP for narrowband mmWave channel reconstruction, and its execution time is not noticeably affected by the increase in the number of received pilot symbols.",
        "authors": [
            "Eren Balevi",
            "Akash S. Doshi",
            "A. Jalal",
            "A. Dimakis",
            "J. Andrews"
        ],
        "citations": 63,
        "references": 56,
        "year": 2020
    },
    {
        "title": "Generative OpenMax for Multi-Class Open Set Classification",
        "abstract": "We present a conceptually new and flexible method for multi-class open set classification. Unlike previous methods where unknown classes are inferred with respect to the feature or decision distance to the known classes, our approach is able to provide explicit modelling and decision score for unknown classes. The proposed method, called Gener- ative OpenMax (G-OpenMax), extends OpenMax by employing generative adversarial networks (GANs) for novel category image synthesis. We validate the proposed method on two datasets of handwritten digits and characters, resulting in superior results over previous deep learning based method OpenMax Moreover, G-OpenMax provides a way to visualize samples representing the unknown classes from open space. Our simple and effective approach could serve as a new direction to tackle the challenging multi-class open set classification problem.",
        "authors": [
            "ZongYuan Ge",
            "S. Demyanov",
            "Zetao Chen",
            "R. Garnavi"
        ],
        "citations": 345,
        "references": 32,
        "year": 2017
    },
    {
        "title": "Generative Neural Networks for Anomaly Detection in Crowded Scenes",
        "abstract": "Security surveillance is critical to social harmony and people’s peaceful life. It has a great impact on strengthening social stability and life safeguarding. Detecting anomaly timely, effectively and efficiently in video surveillance remains challenging. This paper proposes a new approach, called <inline-formula> <tex-math notation=\"LaTeX\">$S^{2}$ </tex-math></inline-formula>-VAE, for anomaly detection from video data. The <inline-formula> <tex-math notation=\"LaTeX\">$S^{2}$ </tex-math></inline-formula>-VAE consists of two proposed neural networks: a Stacked Fully Connected Variational AutoEncoder (<inline-formula> <tex-math notation=\"LaTeX\">$S_{F}$ </tex-math></inline-formula>-VAE) and a Skip Convolutional VAE (<inline-formula> <tex-math notation=\"LaTeX\">$S_{C}$ </tex-math></inline-formula>-VAE). The <inline-formula> <tex-math notation=\"LaTeX\">$S_{F}$ </tex-math></inline-formula>-VAE is a shallow generative network to obtain a model like Gaussian mixture to fit the distribution of the actual data. The <inline-formula> <tex-math notation=\"LaTeX\">$S_{C}$ </tex-math></inline-formula>-VAE, as a key component of <inline-formula> <tex-math notation=\"LaTeX\">$S^{2}$ </tex-math></inline-formula>-VAE, is a deep generative network to take advantages of CNN, VAE and skip connections. Both <inline-formula> <tex-math notation=\"LaTeX\">$S_{F}$ </tex-math></inline-formula>-VAE and <inline-formula> <tex-math notation=\"LaTeX\">$S_{C}$ </tex-math></inline-formula>-VAE are efficient and effective generative networks and they can achieve better performance for detecting both local abnormal events and global abnormal events. The proposed <inline-formula> <tex-math notation=\"LaTeX\">$S^{2}$ </tex-math></inline-formula>-VAE is evaluated using four public datasets. The experimental results show that the <inline-formula> <tex-math notation=\"LaTeX\">$S^{2}$ </tex-math></inline-formula>-VAE outperforms the state-of-the-art algorithms. The code is available publicly at <uri>https://github.com/tianwangbuaa/</uri>.",
        "authors": [
            "Tian Wang",
            "Meina Qiao",
            "Zhiwei Lin",
            "Ce Li",
            "H. Snoussi",
            "Zhe Liu",
            "Chang Choi"
        ],
        "citations": 119,
        "references": 50,
        "year": 2019
    },
    {
        "title": "A Generative Appearance Model for End-To-End Video Object Segmentation",
        "abstract": "One of the fundamental challenges in video object segmentation is to find an effective representation of the target and background appearance. The best performing approaches resort to extensive fine-tuning of a convolutional neural network for this purpose. Besides being prohibitively expensive, this strategy cannot be truly trained end-to-end since the online fine-tuning procedure is not integrated into the offline training of the network. To address these issues, we propose a network architecture that learns a powerful representation of the target and background appearance in a single forward pass. The introduced appearance module learns a probabilistic generative model of target and background feature distributions. Given a new image, it predicts the posterior class probabilities, providing a highly discriminative cue, which is processed in later network modules. Both the learning and prediction stages of our appearance module are fully differentiable, enabling true end-to-end training of the entire segmentation pipeline. Comprehensive experiments demonstrate the effectiveness of the proposed approach on three video object segmentation benchmarks. We close the gap to approaches based on online fine-tuning on DAVIS17, while operating at 15 FPS on a single GPU. Furthermore, our method outperforms all published approaches on the large-scale YouTube-VOS dataset.",
        "authors": [
            "Joakim Johnander",
            "Martin Danelljan",
            "Emil Brissman",
            "F. Khan",
            "M. Felsberg"
        ],
        "citations": 181,
        "references": 35,
        "year": 2018
    },
    {
        "title": "Phase Retrieval Under a Generative Prior",
        "abstract": "The phase retrieval problem asks to recover a natural signal $y_0 \\in \\mathbb{R}^n$ from $m$ quadratic observations, where $m$ is to be minimized. As is common in many imaging problems, natural signals are considered sparse with respect to a known basis, and the generic sparsity prior is enforced via $\\ell_1$ regularization. While successful in the realm of linear inverse problems, such $\\ell_1$ methods have encountered possibly fundamental limitations, as no computationally efficient algorithm for phase retrieval of a $k$-sparse signal has been proven to succeed with fewer than $O(k^2\\log n)$ generic measurements, exceeding the theoretical optimum of $O(k \\log n)$. In this paper, we propose a novel framework for phase retrieval by 1) modeling natural signals as being in the range of a deep generative neural network $G : \\mathbb{R}^k \\rightarrow \\mathbb{R}^n$ and 2) enforcing this prior directly by optimizing an empirical risk objective over the domain of the generator. Our formulation has provably favorable global geometry for gradient methods, as soon as $m = O(kd^2\\log n)$, where $d$ is the depth of the network. Specifically, when suitable deterministic conditions on the generator and measurement matrix are met, we construct a descent direction for any point outside of a small neighborhood around the unique global minimizer and its negative multiple, and show that such conditions hold with high probability under Gaussian ensembles of multilayer fully-connected generator networks and measurement matrices. This formulation for structured phase retrieval thus has two advantages over sparsity based methods: 1) deep generative priors can more tightly represent natural signals and 2) information theoretically optimal sample complexity. We corroborate these results with experiments showing that exploiting generative models in phase retrieval tasks outperforms sparse phase retrieval methods.",
        "authors": [
            "Paul Hand",
            "Oscar Leong",
            "V. Voroninski"
        ],
        "citations": 183,
        "references": 41,
        "year": 2018
    },
    {
        "title": "Simultaneous deep generative modelling and clustering of single-cell genomic data",
        "abstract": null,
        "authors": [
            "Qiao Liu",
            "Shengquan Chen",
            "R. Jiang",
            "W. Wong"
        ],
        "citations": 64,
        "references": 74,
        "year": 2020
    },
    {
        "title": "Generative Layout Modeling using Constraint Graphs",
        "abstract": "We propose a new generative model for layout generation. We generate layouts in three steps. First, we generate the layout elements as nodes in a layout graph. Second, we compute constraints between layout elements as edges in the layout graph. Third, we solve for the final layout using constrained optimization. For the first two steps, we build on recent transformer architectures. The layout optimization implements the constraints efficiently. We show three practical contributions compared to the state of the art: our work requires no user input, produces higher quality layouts, and enables many novel capabilities for conditional layout generation.",
        "authors": [
            "W. Para",
            "Paul Guerrero",
            "T. Kelly",
            "L. Guibas",
            "Peter Wonka"
        ],
        "citations": 60,
        "references": 82,
        "year": 2020
    },
    {
        "title": "Tree Tensor Networks for Generative Modeling",
        "abstract": "Matrix product states (MPSs), a tensor network designed for one-dimensional quantum systems, were recently proposed for generative modeling of natural data (such as images) in terms of the ``Born machine.'' However, the exponential decay of correlation in MPSs restricts its representation power heavily for modeling complex data such as natural images. In this work, we push forward the effort of applying tensor networks to machine learning by employing the tree tensor network (TTN), which exhibits balanced performance in expressibility and efficient training and sampling. We design the tree tensor network to utilize the two-dimensional prior of the natural images and develop sweeping learning and sampling algorithms which can be efficiently implemented utilizing graphical processing units. We apply our model to random binary patterns and the binary MNIST data sets of handwritten digits. We show that the TTN is superior to MPSs for generative modeling in keeping the correlation of pixels in natural images, as well as giving better log-likelihood scores in standard data sets of handwritten digits. We also compare its performance with state-of-the-art generative models such as variational autoencoders, restricted Boltzmann machines, and PixelCNN. Finally, we discuss the future development of tensor network states in machine learning problems.",
        "authors": [
            "Song Cheng",
            "Lei Wang",
            "T. Xiang",
            "Pan Zhang"
        ],
        "citations": 113,
        "references": 50,
        "year": 2019
    },
    {
        "title": "Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model",
        "abstract": "In this paper we consider the problem of computing an $\\epsilon$-optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in $O(1)$ time. Given such a DMDP with states $\\states$, actions $\\actions$, discount factor $\\gamma\\in(0,1)$, and rewards in range $[0, 1]$ we provide an algorithm which computes an $\\epsilon$-optimal policy with probability $1 - \\delta$ where {\\it both} the run time spent and number of sample taken is upper bounded by \\[ O\\left[\\frac{|\\cS||\\cA|}{(1-\\gamma)^3 \\epsilon^2} \\log \\left(\\frac{|\\cS||\\cA|}{(1-\\gamma)\\delta \\epsilon} \\right) \\log\\left(\\frac{1}{(1-\\gamma)\\epsilon}\\right)\\right] ~. \\] For fixed values of $\\epsilon$, this improves upon the previous best known bounds by a factor of $(1 - \\gamma)^{-1}$ and matches the sample complexity lower bounds proved in \\cite{azar2013minimax} up to logarithmic factors. We also extend our method to computing $\\epsilon$-optimal policies for finite-horizon MDP with a generative model and provide a nearly matching sample complexity lower bound.",
        "authors": [
            "Aaron Sidford",
            "Mengdi Wang",
            "X. Wu",
            "Lin F. Yang",
            "Y. Ye"
        ],
        "citations": 182,
        "references": 24,
        "year": 2018
    },
    {
        "title": "A Generative Model for Molecular Distance Geometry",
        "abstract": "Great computational effort is invested in generating equilibrium states for molecular systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates statistically independent samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties.",
        "authors": [
            "G. Simm",
            "José Miguel Hernández-Lobato"
        ],
        "citations": 105,
        "references": 49,
        "year": 2019
    },
    {
        "title": "Unsupervised Adversarial Depth Estimation Using Cycled Generative Networks",
        "abstract": "While recent deep monocular depth estimation approaches based on supervised regression have achieved remarkable performance, costly ground truth annotations are required during training. To cope with this issue, in this paper we present a novel unsupervised deep learning approach for predicting depth maps and show that the depth estimation task can be effectively tackled within an adversarial learning framework. Specifically, we propose a deep generative network that learns to predict the correspondence field (i.e. the disparity map) between two image views in a calibrated stereo camera setting. The proposed architecture consists of two generative sub-networks jointly trained with adversarial learning for reconstructing the disparity map and organized in a cycle such as to provide mutual constraints and supervision to each other. Extensive experiments on the publicly available datasets KITTI and Cityscapes demonstrate the effectiveness of the proposed model and competitive results with state of the art methods. The code is available at https://github.com/andrea-pilzer/unsup-stereo-depthGAN",
        "authors": [
            "Andrea Pilzer",
            "Dan Xu",
            "M. Puscas",
            "E. Ricci",
            "N. Sebe"
        ],
        "citations": 173,
        "references": 31,
        "year": 2018
    },
    {
        "title": "Generative Adversarial Network",
        "abstract": null,
        "authors": [
            "O. Yalçin"
        ],
        "citations": 95,
        "references": 19,
        "year": 2019
    },
    {
        "title": "vGraph: A Generative Model for Joint Community Detection and Node Representation Learning",
        "abstract": "This paper focuses on two fundamental tasks of graph analysis: community detection and node representation learning, which capture the global and local structures of graphs, respectively. In the current literature, these two tasks are usually independently studied while they are actually highly correlated. We propose a probabilistic generative model called vGraph to learn community membership and node representation collaboratively. Specifically, we assume that each node can be represented as a mixture of communities, and each community is defined as a multinomial distribution over nodes. Both the mixing coefficients and the community distribution are parameterized by the low-dimensional representations of the nodes and communities. We designed an effective variational inference algorithm which regularizes the community membership of neighboring nodes to be similar in the latent space. Experimental results on multiple real-world graphs show that vGraph is very effective in both community detection and node representation learning, outperforming many competitive baselines in both tasks. We show that the framework of vGraph is quite flexible and can be easily extended to detect hierarchical communities.",
        "authors": [
            "Fan-Yun Sun",
            "Meng Qu",
            "Jordan Hoffmann",
            "Chin-Wei Huang",
            "Jian Tang"
        ],
        "citations": 85,
        "references": 43,
        "year": 2019
    },
    {
        "title": "Generative Counterfactual Introspection for Explainable Deep Learning",
        "abstract": "In this work, we propose an introspection technique for deep neural networks that relies on a generative model to instigate salient editing of the input image for model interpretation. Such modification provides the fundamental interventional operation that allows us to obtain answers to counterfactual inquiries, i.e., what meaningful change can be made to the input image in order to alter the prediction. We demonstrate how to reveal interesting properties of the given classifiers by utilizing the proposed introspection approach on both the MNIST and the CelebA dataset.",
        "authors": [
            "Shusen Liu",
            "B. Kailkhura",
            "Donald Loveland",
            "Yong Han"
        ],
        "citations": 84,
        "references": 33,
        "year": 2019
    },
    {
        "title": "The hippocampal formation as a hierarchical generative model supporting generative replay and continual learning",
        "abstract": "We advance a novel computational theory of the hippocampal formation as a hierarchical generative model that organizes sequential experiences, such as rodent trajectories during spatial navigation, into coherent spatiotemporal contexts. We propose that the hippocampal generative model is endowed with inductive biases to identify individual items of experience (first hierarchical layer), organize them into sequences (second layer) and cluster them into maps (third layer). This theory entails a novel characterization of hippocampal reactivations as generative replay: the offline resampling of fictive sequences from the generative model, which supports the continual learning of multiple sequential experiences. We show that the model learns and efficiently retains multiple spatial navigation trajectories, by organizing them into spatial maps. Furthermore, the model reproduces flexible and prospective aspects of hippocampal dynamics that are challenging to explain within existing frameworks. This theory reconciles multiple roles of the hippocampal formation in map-based navigation, episodic memory and imagination.",
        "authors": [
            "Ivilin Peev Stoianov",
            "D. Maisto",
            "G. Pezzulo"
        ],
        "citations": 46,
        "references": 143,
        "year": 2020
    },
    {
        "title": "Generative Adversarial Network for Medical Images (MI-GAN)",
        "abstract": null,
        "authors": [
            "Talha Iqbal",
            "Hazrat Ali"
        ],
        "citations": 183,
        "references": 46,
        "year": 2018
    },
    {
        "title": "Image Colorization Using Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "Kamyar Nazeri",
            "Eric Ng",
            "Mehran Ebrahimi"
        ],
        "citations": 137,
        "references": 23,
        "year": 2018
    },
    {
        "title": "Scaffold-based molecular design with a graph generative model",
        "abstract": "Searching for new molecules in areas like drug discovery often starts from the core structures of known molecules. Such a method has called for a strategy of designing derivative compounds retaining a particular scaffold as a substructure. On this account, our present work proposes a graph generative model that targets its use in scaffold-based molecular design. Our model accepts a molecular scaffold as input and extends it by sequentially adding atoms and bonds. The generated molecules are then guaranteed to contain the scaffold with certainty, and their properties can be controlled by conditioning the generation process on desired properties. The learned rule of extending molecules can well generalize to arbitrary kinds of scaffolds, including those unseen during learning. In the conditional generation of molecules, our model can simultaneously control multiple chemical properties despite the search space constrained by fixing the substructure. As a demonstration, we applied our model to designing inhibitors of the epidermal growth factor receptor and show that our model can employ a simple semi-supervised extension to broaden its applicability to situations where only a small amount of data is available.",
        "authors": [
            "Jaechang Lim",
            "Sang-Yeon Hwang",
            "Seokhyun Moon",
            "Seung-Su Kim",
            "W. Kim"
        ],
        "citations": 105,
        "references": 46,
        "year": 2019
    },
    {
        "title": "From Target to Drug: Generative Modeling for Multimodal Structure-Based Ligand Design.",
        "abstract": "Chemical space is impractically large and conventional structure-based virtual screening techniques cannot be used to simply search through the entire space to discover effective bioactive molecules. To address this shortcoming we propose a generative adversarial network to generate, rather than search, diverse three-dimensional ligand shapes complementary to the pocket. Furthermore, we show that the generated molecule shapes can be decoded using a shape-captioning network into a sequence of SMILES enabling directly structure-based de novo drug design. We evaluate the quality of the method by both structure- (Docking) and ligand-based (QSAR) virtual screening methods. For both evaluation approaches we observed enrichment compared to random sampling from initial chemical space of ZINC drug-like compounds.",
        "authors": [
            "Miha Škalič",
            "Davide Sabbadin",
            "B. Sattarov",
            "Simone Sciabola",
            "G. de Fabritiis"
        ],
        "citations": 104,
        "references": 44,
        "year": 2019
    },
    {
        "title": "PiiGAN: Generative Adversarial Networks for Pluralistic Image Inpainting",
        "abstract": "The latest methods based on deep learning have achieved amazing results regarding the complex work of inpainting large missing areas in an image. But this type of method generally attempts to generate one single “optimal” result, ignoring many other plausible results. Considering the uncertainty of the inpainting task, one sole result can hardly be regarded as a desired regeneration of the missing area. In view of this weakness, which is related to the design of the previous algorithms, we propose a novel deep generative model equipped with a brand new style extractor which can extract the style feature (latent vector) from the ground truth. Once obtained, the extracted style feature and the ground truth are both input into the generator. We also craft a consistency loss that guides the generated image to approximate the ground truth. After iterations, our generator is able to learn the mapping of styles corresponding to multiple sets of vectors. The proposed model can generate a large number of results consistent with the context semantics of the image. Moreover, we evaluated the effectiveness of our model on three datasets, i.e., CelebA, PlantVillage, and MauFlex. Compared to state-of-the-art inpainting methods, this model is able to offer desirable inpainting results with both better quality and higher diversity. The code and model will be made available on https://github.com/vivitsai/PiiGAN.",
        "authors": [
            "Weiwei Cai",
            "Zhanguo Wei"
        ],
        "citations": 110,
        "references": 52,
        "year": 2019
    },
    {
        "title": "Distributional Sliced-Wasserstein and Applications to Generative Modeling",
        "abstract": "Sliced-Wasserstein distance (SWD) and its variation, Max Sliced-Wasserstein distance (Max-SWD), have been widely used in the recent years due to their fast computation and scalability when the probability measures lie in very high dimension. However, these distances still have their weakness, SWD requires a lot of projection samples because it uses the uniform distribution to sample projecting directions, Max-SWD uses only one projection, causing it to lose a large amount of information. In this paper, we propose a novel distance that finds optimal penalized probability measure over the slices, which is named Distributional Sliced-Wasserstein distance (DSWD). We show that the DSWD is a generalization of both SWD and Max-SWD, and the proposed distance could be found by searching for the push-forward measure over a set of measures satisfying some certain constraints. Moreover, similar to SWD, we can extend Generalized Sliced-Wasserstein distance (GSWD) to Distributional Generalized Sliced-Wasserstein distance (DGSWD). Finally, we carry out extensive experiments to demonstrate the favorable generative modeling performances of our distances over the previous sliced-based distances in large-scale real datasets.",
        "authors": [
            "Khai Nguyen",
            "Nhat Ho",
            "Tung Pham",
            "H. Bui"
        ],
        "citations": 92,
        "references": 53,
        "year": 2020
    },
    {
        "title": "Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning",
        "abstract": "Deep Learning has recently become hugely popular in machine learning for its ability to solve end-to-end learning systems, in which the features and the classifiers are learned simultaneously, providing significant improvements in classification accuracy in the presence of highly-structured and large databases. Its success is due to a combination of recent algorithmic breakthroughs, increasingly powerful computers, and access to significant amounts of data. Researchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15. Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level differential privacy applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack).",
        "authors": [
            "B. Hitaj",
            "G. Ateniese",
            "F. Pérez-Cruz"
        ],
        "citations": 1000,
        "references": 102,
        "year": 2017
    },
    {
        "title": "Mol-CycleGAN: a generative model for molecular optimization",
        "abstract": null,
        "authors": [
            "Łukasz Maziarka",
            "Agnieszka Pocha",
            "Jan Kaczmarczyk",
            "Krzysztof Rataj",
            "Tomasz Danel",
            "M. Warchoł"
        ],
        "citations": 220,
        "references": 56,
        "year": 2019
    },
    {
        "title": "Learning a Generative Model for Fusing Infrared and Visible Images via Conditional Generative Adversarial Network with Dual Discriminators",
        "abstract": "In this paper, we propose a new end-to-end model, called dual-discriminator conditional generative adversarial network (DDcGAN), for fusing infrared and visible images of different resolutions. Unlike the pixel-level methods and existing deep learning-based methods, the fusion task is accomplished through the adversarial process between a generator and two discriminators, in addition to the specially designed content loss. The generator is trained to generate real-like fused images to fool discriminators. The two discriminators are trained to calculate the JS divergence between the probability distribution of downsampled fused images and infrared images, and the JS divergence between the probability distribution of gradients of fused images and gradients of visible images, respectively. Thus, the fused images can compensate for the features that are not constrained by the single content loss. Consequently, the prominence of thermal targets in the infrared image and the texture details in the visible image can be preserved or even enhanced in the fused image simultaneously. Moreover, by constraining and distinguishing between the downsampled fused image and the low-resolution infrared image, DDcGAN can be preferably applied to the fusion of different resolution images. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our method over the state-of-the-art.",
        "authors": [
            "Han Xu",
            "Pengwei Liang",
            "Wei Yu",
            "Junjun Jiang",
            "Jiayi Ma"
        ],
        "citations": 83,
        "references": 28,
        "year": 2019
    },
    {
        "title": "Multi-style Generative Network for Real-time Transfer",
        "abstract": null,
        "authors": [
            "Hang Zhang",
            "Kristin J. Dana"
        ],
        "citations": 272,
        "references": 56,
        "year": 2017
    },
    {
        "title": "Learning Travel Time Distributions with Deep Generative Model",
        "abstract": "Travel time estimation of a given route with respect to real-time traffic condition is extremely useful for many applications like route planning. We argue that it is even more useful to estimate the travel time distribution, from which we can derive the expected travel time as well as the uncertainty. In this paper, we develop a deep generative model - DeepGTT - to learn the travel time distribution for any route by conditioning on the real-time traffic. DeepGTT interprets the generation of travel time using a three-layer hierarchical probabilistic model. In the first layer, we present two techniques, amortization and spatial smoothness embeddings, to share statistical strength among different road segments; a convolutional neural net based representation learning component is also proposed to capture the dynamically changing real-time traffic condition. In the middle layer, a nonlinear factorization model is developed to generate auxiliary random variable i.e., speed. The introduction of this middle layer separates the statical spatial features from the dynamically changing real-time traffic conditions, allowing us to incorporate the heterogeneous influencing factors into a single model. In the last layer, an attention mechanism based function is proposed to collectively generate the observed travel time. DeepGTT describes the generation process in a reasonable manner, and thus it not only produces more accurate results but also is more efficient. On a real-world large-scale data set, we show that DeepGTT produces substantially better results than state-of-the-art alternatives in two tasks: travel time estimation and route recovery from sparse trajectory data.",
        "authors": [
            "Xiucheng Li",
            "G. Cong",
            "Aixin Sun",
            "Yun Cheng"
        ],
        "citations": 67,
        "references": 46,
        "year": 2019
    },
    {
        "title": "Multi-style Generative Reading Comprehension",
        "abstract": "This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success.",
        "authors": [
            "Kyosuke Nishida",
            "Itsumi Saito",
            "Kosuke Nishida",
            "Kazutoshi Shinoda",
            "Atsushi Otsuka",
            "Hisako Asano",
            "J. Tomita"
        ],
        "citations": 68,
        "references": 76,
        "year": 2019
    },
    {
        "title": "Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis",
        "abstract": null,
        "authors": [
            "Luke de Oliveira",
            "Michela Paganini",
            "B. Nachman"
        ],
        "citations": 276,
        "references": 46,
        "year": 2017
    },
    {
        "title": "Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions",
        "abstract": "By building upon the recent theory that established the connection between implicit generative modeling (IGM) and optimal transport, in this study, we propose a novel parameter-free algorithm for learning the underlying distributions of complicated datasets and sampling from them. The proposed algorithm is based on a functional optimization problem, which aims at finding a measure that is close to the data distribution as much as possible and also expressive enough for generative modeling purposes. We formulate the problem as a gradient flow in the space of probability measures. The connections between gradient flows and stochastic differential equations let us develop a computationally efficient algorithm for solving the optimization problem. We provide formal theoretical analysis where we prove finite-time error guarantees for the proposed algorithm. To the best of our knowledge, the proposed algorithm is the first nonparametric IGM algorithm with explicit theoretical guarantees. Our experimental results support our theory and show that our algorithm is able to successfully capture the structure of different types of data distributions.",
        "authors": [
            "Umut Simsekli",
            "A. Liutkus",
            "Szymon Majewski",
            "Alain Durmus"
        ],
        "citations": 111,
        "references": 54,
        "year": 2018
    },
    {
        "title": "SCH-GAN: Semi-Supervised Cross-Modal Hashing by Generative Adversarial Network",
        "abstract": "Cross-modal hashing maps heterogeneous multimedia data into a common Hamming space to realize fast and flexible cross-modal retrieval. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they heavily rely on large-scale labeled cross-modal training data which are hard to obtain, since multiple modalities are involved. They also ignore the rich information contained in the large amount of unlabeled data across different modalities, which can help to model the correlations between different modalities. To address these problems, in this paper, we propose a novel semi-supervised cross-modal hashing approach by generative adversarial network (SCH-GAN). The main contributions can be summarized as follows: 1) we propose a novel generative adversarial network for cross-modal hashing, in which the generative model tries to select margin examples of one modality from unlabeled data when given a query of another modality (e.g., giving a text query to retrieve images and vice versa). The discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of the discriminative model and 2) we propose a reinforcement learning-based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote a discriminative model. Extensive experiments verify the effectiveness of our proposed approach, compared with nine state-of-the-art methods on three widely used datasets.",
        "authors": [
            "Jian Zhang",
            "Yuxin Peng",
            "Mingkuan Yuan"
        ],
        "citations": 115,
        "references": 63,
        "year": 2018
    },
    {
        "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
        "abstract": "The recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.",
        "authors": [
            "Ken Kansky",
            "Tom Silver",
            "David A. Mély",
            "Mohamed Eldawy",
            "M. Lázaro-Gredilla",
            "Xinghua Lou",
            "N. Dorfman",
            "Szymon Sidor",
            "Scott Phoenix",
            "Dileep George"
        ],
        "citations": 229,
        "references": 28,
        "year": 2017
    },
    {
        "title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization",
        "abstract": "We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.",
        "authors": [
            "Piji Li",
            "Wai Lam",
            "Lidong Bing",
            "Zihao Wang"
        ],
        "citations": 207,
        "references": 43,
        "year": 2017
    },
    {
        "title": "Semantic Image Inpainting with Progressive Generative Networks",
        "abstract": "Recently, image inpainting task has revived with the help of deep learning techniques. Deep neural networks, especially the generative adversarial networks~(GANs) make it possible to recover the missing details in images. Due to the lack of sufficient context information, most existing methods fail to get satisfactory inpainting results. This work investigates a more challenging problem, e.g., the newly-emerging semantic image inpainting - a task to fill in large holes in natural images. In this paper, we propose an end-to-end framework named progressive generative networks~(PGN), which regards the semantic image inpainting task as a curriculum learning problem. Specifically, we divide the hole filling process into several different phases and each phase aims to finish a course of the entire curriculum. After that, an LSTM framework is used to string all the phases together. By introducing this learning strategy, our approach is able to progressively shrink the large corrupted regions in natural images and yields promising inpainting results. Moreover, the proposed approach is quite fast to evaluate as the entire hole filling is performed in a single forward pass. Extensive experiments on Paris Street View and ImageNet dataset clearly demonstrate the superiority of our approach. Code for our models is available at https://github.com/crashmoon/Progressive-Generative-Networks.",
        "authors": [
            "Haoran Zhang",
            "Zhenzhen Hu",
            "Changzhi Luo",
            "W. Zuo",
            "M. Wang"
        ],
        "citations": 128,
        "references": 35,
        "year": 2018
    },
    {
        "title": "Stabilizing Generative Adversarial Networks: A Survey",
        "abstract": "Generative Adversarial Networks (GANs) are a type of generative model which have received much attention due to their ability to model complex real-world data. Despite their recent successes, the process of training GANs remains challenging, suffering from instability problems such as non-convergence, vanishing or exploding gradients, and mode collapse. In recent years, a diverse set of approaches have been proposed which focus on stabilizing the GAN training procedure. The purpose of this survey is to provide a comprehensive overview of the GAN training stabilization methods which can be found in the literature. We discuss the advantages and disadvantages of each approach, offer a comparative summary, and conclude with a discussion of open problems.",
        "authors": [
            "Maciej Wiatrak",
            "Stefano V. Albrecht",
            "A. Nystrom"
        ],
        "citations": 76,
        "references": 110,
        "year": 2019
    },
    {
        "title": "HyperGAN: A Generative Model for Diverse, Performant Neural Networks",
        "abstract": "Standard neural networks are often overconfident when presented with data outside the training distribution. We introduce HyperGAN, a new generative model for learning a distribution of neural network parameters. HyperGAN does not require restrictive assumptions on priors, and networks sampled from it can be used to quickly create very large and diverse ensembles. HyperGAN employs a novel mixer to project prior samples to a latent space with correlated dimensions, and samples from the latent space are then used to generate weights for each layer of a deep neural network. We show that HyperGAN can learn to generate parameters which label the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty estimates than standard ensembles by evaluating on out of distribution data as well as adversarial examples.",
        "authors": [
            "Neale Ratzlaff",
            "Fuxin Li"
        ],
        "citations": 58,
        "references": 26,
        "year": 2019
    },
    {
        "title": "Stain normalization of histopathology images using generative adversarial networks",
        "abstract": "Computational histopathology involves CAD for microscopic analysis of stained histopathological slides to study presence, localization or grading of disease. An important stage in a CAD system, stain color normalization, has been broadly studied. The existing approaches are mainly defined in the context of stain deconvolution and template matching. In this paper, we propose a novel approach to this problem by introducing a parametric, fully unsupervised generative model. Our model is based on end-to-end machine learning in the framework of generative adversarial networks. It can learn a nonlinear transformation of a set of latent variables, which are forced to have a prior Dirichlet distribution and control the color of staining hematoxylin and eosin (H&E) images. By replacing the latent variables of a source image with those extracted from a template image in the trained model, it can generate a new color copy of the source image while preserving the important tissue structures resembling the chromatic information of the template image. Our proposed method can instantly be applied to new unseen images, which is different from previous methods that need to compute some statistical properties on input test data. This is potentially problematic when the test sample sizes are limited. Experiments on H&E images from different laboratories show that the proposed model outperforms most state-of-the-art methods.",
        "authors": [
            "F. G. Zanjani",
            "S. Zinger",
            "B. Bejnordi",
            "J. Laak",
            "P. D. With"
        ],
        "citations": 112,
        "references": 26,
        "year": 2018
    },
    {
        "title": "Dream Lens: Exploration and Visualization of Large-Scale Generative Design Datasets",
        "abstract": "This paper presents Dream Lens, an interactive visual analysis tool for exploring and visualizing large-scale generative design datasets. Unlike traditional computer aided design, where users create a single model, with generative design, users specify high-level goals and constraints, and the system automatically generates hundreds or thousands of candidates all meeting the design criteria. Once a large collection of design variations is created, the designer is left with the task of finding the design, or set of designs, which best meets their requirements. This is a complicated task which could require analyzing the structural characteristics and visual aesthetics of the designs. Two studies are conducted which demonstrate the usability and usefulness of the Dream Lens system, and a generatively designed dataset of 16,800 designs for a sample design problem is described and publicly released to encourage advancement in this area.",
        "authors": [
            "Justin Matejka",
            "Michael Glueck",
            "Erin Bradner",
            "Ali Hashemi",
            "Tovi Grossman",
            "G. Fitzmaurice"
        ],
        "citations": 112,
        "references": 43,
        "year": 2018
    },
    {
        "title": "Augmenting Image Classifiers Using Data Augmentation Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "Antreas Antoniou",
            "A. Storkey",
            "Harrison Edwards"
        ],
        "citations": 110,
        "references": 44,
        "year": 2018
    },
    {
        "title": "Voice Impersonation Using Generative Adversarial Networks",
        "abstract": "Voice impersonation is not the same as voice transformation, although the latter is an essential element of it. In voice impersonation, the resultant voice must convincingly convey the impression of having been naturally produced by the target speaker, mimicking not only the pitch and other perceivable signal qualities, but also the style of the target speaker. In this paper, we propose a novel neural-network based speech quality- and style-mimicry framework for the synthesis of impersonated voices. The framework is built upon a fast and accurate generative adversarial network model. Given spectrographic representations of source and target speakers' voices, the model learns to mimic the target speaker's voice quality and style, regardless of the linguistic content of either's voice, generating a synthetic spectrogram from which the time-domain signal is reconstructed using the Griffin-Lim method. In effect, this model reframes the well-known problem of style-transfer for images as the problem of style-transfer for speech signals, while intrinsically addressing the problem of durational variability of speech sounds. Experiments demonstrate that the model can generate extremely convincing samples of impersonated speech. It is even able to impersonate voices across different genders effectively. Results are qualitatively evaluated using standard procedures for evaluating synthesized voices.",
        "authors": [
            "Yang Gao",
            "Rita Singh",
            "B. Raj"
        ],
        "citations": 96,
        "references": 20,
        "year": 2018
    },
    {
        "title": "Shadow Detection with Conditional Generative Adversarial Networks",
        "abstract": "We introduce scGAN, a novel extension of conditional Generative Adversarial Networks (GAN) tailored for the challenging problem of shadow detection in images. Previous methods for shadow detection focus on learning the local appearance of shadow regions, while using limited local context reasoning in the form of pairwise potentials in a Conditional Random Field. In contrast, the proposed adversarial approach is able to model higher level relationships and global scene characteristics. We train a shadow detector that corresponds to the generator of a conditional GAN, and augment its shadow accuracy by combining the typical GAN loss with a data loss term. Due to the unbalanced distribution of the shadow labels, we use weighted cross entropy. With the standard GAN architecture, properly setting the weight for the cross entropy would require training multiple GANs, a computationally expensive grid procedure. In scGAN, we introduce an additional sensitivity parameter w to the generator. The proposed approach effectively parameterizes the loss of the trained detector. The resulting shadow detector is a single network that can generate shadow maps corresponding to different sensitivity levels, obviating the need for multiple models and a costly training procedure. We evaluate our method on the large-scale SBU and UCF shadow datasets, and observe up to 17% error reduction with respect to the previous state-of-the-art method.",
        "authors": [
            "Vu Nguyen",
            "T. F. Y. Vicente",
            "Maozheng Zhao",
            "Minh Hoai",
            "D. Samaras"
        ],
        "citations": 179,
        "references": 32,
        "year": 2017
    },
    {
        "title": "A Generative Model for Zero Shot Learning Using Conditional Variational Autoencoders",
        "abstract": "Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available. This setting is important in the real world since one may not be able to obtain images of all the possible classes at training. While previous approaches have tried to model the relationship between the class attribute space and the image space via some kind of a transfer function in order to model the image space correspondingly to an unseen class, we take a different approach and try to generate the samples from the given attributes, using a conditional variational autoencoder, and use the generated samples for classification of the unseen classes. By extensive testing on four benchmark datasets, we show that our model outperforms the state of the art, particularly in the more realistic generalized setting, where the training classes can also appear at the test time along with the novel classes.",
        "authors": [
            "Ashish Mishra",
            "M. K. Reddy",
            "Anurag Mittal",
            "H. Murthy"
        ],
        "citations": 291,
        "references": 47,
        "year": 2017
    },
    {
        "title": "Steganographic generative adversarial networks",
        "abstract": "Steganography is collection of methods to hide secret information (“payload”) within non-secret information “container”). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in steganographic applications.",
        "authors": [
            "Denis Volkhonskiy",
            "I. Nazarov",
            "B. Borisenko",
            "Evgeny Burnaev"
        ],
        "citations": 205,
        "references": 31,
        "year": 2017
    },
    {
        "title": "Deep Generative Adversarial Compression Artifact Removal",
        "abstract": "Compression artifacts arise in images whenever a lossy compression algorithm is applied. These artifacts eliminate details present in the original image, or add noise and small structures; because of these effects they make images less pleasant for the human eye, and may also lead to decreased performance of computer vision algorithms such as object detectors. To eliminate such artifacts, when decompressing an image, it is required to recover the original image from a disturbed version. To this end, we present a feed-forward fully convolutional residual network model trained using a generative adversarial framework. To provide a baseline, we show that our model can be also trained optimizing the Structural Similarity (SSIM), which is a better loss with respect to the simpler Mean Squared Error (MSE). Our GAN is able to produce images with more photorealistic details than MSE or SSIM based networks. Moreover we show that our approach can be used as a pre-processing step for object detection in case images are degraded by compression to a point that state-of-the art detectors fail. In this task, our GAN method obtains better performance than MSE or SSIM trained networks.",
        "authors": [
            "L. Galteri",
            "Lorenzo Seidenari",
            "Marco Bertini",
            "A. Bimbo"
        ],
        "citations": 203,
        "references": 40,
        "year": 2017
    },
    {
        "title": "Learning Single-Image 3D Reconstruction by Generative Modelling of Shape, Pose and Shading",
        "abstract": null,
        "authors": [
            "Paul Henderson",
            "V. Ferrari"
        ],
        "citations": 122,
        "references": 64,
        "year": 2019
    },
    {
        "title": "Modeling financial time-series with generative adversarial networks",
        "abstract": null,
        "authors": [
            "Shuntaro Takahashi",
            "Yu Chen",
            "Kumiko Tanaka-Ishii"
        ],
        "citations": 122,
        "references": 50,
        "year": 2019
    },
    {
        "title": "Unsupervised Generative Adversarial Cross-modal Hashing",
        "abstract": "\n \n Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods, since no intensive labeling work is involved. However, existing unsupervised methods learn hashing functions by preserving inter and intra correlations, while ignoring the underlying manifold structure across different modalities, which is extremely helpful to capture meaningful nearest neighbors of different modalities for cross-modal retrieval. To address the above problem, in this paper we propose an Unsupervised Generative Adversarial Cross-modal Hashing approach (UGACH), which makes full use of GAN's ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows: (1) We propose a generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed UGACH, given a data of one modality, the generative model tries to fit the distribution over the manifold structure, and select informative data of another modality to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. (2) We propose a correlation graph based approach to capture the underlying manifold structure across different modalities, so that data of different modalities but within the same manifold can have smaller Hamming distance and promote retrieval accuracy. Extensive experiments compared with 6 state-of-the-art methods on 2 widely-used datasets verify the effectiveness of our proposed approach.\n \n",
        "authors": [
            "Jian Zhang",
            "Yuxin Peng",
            "Mingkuan Yuan"
        ],
        "citations": 192,
        "references": 28,
        "year": 2017
    },
    {
        "title": "The Spiked Matrix Model With Generative Priors",
        "abstract": "We investigate the statistical and algorithmic properties of random neural-network generative priors in a simple inference problem: spiked-matrix estimation. We establish a rigorous expression for the performance of the Bayes-optimal estimator in the high-dimensional regime, and identify the statistical threshold for weak-recovery of the spike. Next, we derive a message-passing algorithm taking into account the latent structure of the spike, and show that its performance is asymptotically optimal for natural choices of the generative network architecture. The absence of an algorithmic gap in this case is in stark contrast to known results for sparse spikes, another popular prior for modelling low-dimensional signals, and for which no algorithm is known to achieve the optimal statistical threshold. Finally, we show that linearising our message passing algorithm yields a simple spectral method also achieving the optimal threshold for reconstruction. We conclude with an experiment on a real data set showing that our bespoke spectral method outperforms vanilla PCA.",
        "authors": [
            "Benjamin Aubin",
            "Bruno Loureiro",
            "Antoine Maillard",
            "Florent Krzakala",
            "L. Zdeborová"
        ],
        "citations": 51,
        "references": 61,
        "year": 2019
    },
    {
        "title": "Unsupervised Diverse Colorization via Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "Yun Cao",
            "Zhiming Zhou",
            "Weinan Zhang",
            "Yong Yu"
        ],
        "citations": 173,
        "references": 32,
        "year": 2017
    },
    {
        "title": "Generative Domain-Migration Hashing for Sketch-to-Image Retrieval",
        "abstract": null,
        "authors": [
            "Jingyi Zhang",
            "Fumin Shen",
            "Li Liu",
            "Fan Zhu",
            "Mengyang Yu",
            "Ling Shao",
            "Heng Tao Shen",
            "L. Gool"
        ],
        "citations": 84,
        "references": 59,
        "year": 2018
    },
    {
        "title": "Generative Adversarial Network for Abstractive Text Summarization",
        "abstract": "\n \n In this paper, we propose an adversarial process for abstractive text summarization, in which we simultaneously train a generative model G and a discriminative model D. In particular, we build the generator G as an agent of reinforcement learning, which takes the raw text as input and predicts the abstractive summarization. We also build a discriminator which attempts to distinguish the generated summary from the ground truth summary. Extensive experiments demonstrate that our model achieves competitive ROUGE scores with the state-of-the-art methods on CNN/Daily Mail dataset. Qualitatively, we show that our model is able to generate more abstractive, readable and diverse summaries.\n \n",
        "authors": [
            "Linqing Liu",
            "Yao Lu",
            "Min Yang",
            "Qiang Qu",
            "Jia Zhu",
            "Hongyan Li"
        ],
        "citations": 165,
        "references": 6,
        "year": 2017
    },
    {
        "title": "Context-Aware Generative Adversarial Privacy",
        "abstract": "Preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge. On the one hand, context-free privacy solutions, such as differential privacy, provide strong privacy guarantees, but often lead to a significant reduction in utility. On the other hand, context-aware privacy solutions, such as information theoretic privacy, achieve an improved privacy-utility tradeoff, but assume that the data holder has access to dataset statistics. We circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy (GAP). GAP leverages recent advancements in generative adversarial networks (GANs) to allow the data holder to learn privatization schemes from the dataset itself. Under GAP, learning the privacy mechanism is formulated as a constrained minimax game between two players: a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals' private variables, and an adversary that tries to infer the private variables from the sanitized dataset. To evaluate GAP's performance, we investigate two simple (yet canonical) statistical dataset models: (a) the binary data model, and (b) the binary Gaussian mixture model. For both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that the privacy mechanisms learned from data (in a generative adversarial fashion) match the theoretically optimal ones. This demonstrates that our framework can be easily applied in practice, even in the absence of dataset statistics.",
        "authors": [
            "Chong Huang",
            "P. Kairouz",
            "Xiao Chen",
            "L. Sankar",
            "R. Rajagopal"
        ],
        "citations": 155,
        "references": 103,
        "year": 2017
    },
    {
        "title": "Enhancing Collaborative Filtering with Generative Augmentation",
        "abstract": "Collaborative filtering (CF) has become one of the most popular and widely used methods in recommender systems, but its performance degrades sharply for users with rare interaction data. Most existing hybrid CF methods try to incorporate side information such as review texts to alleviate the data sparsity problem. However, the process of exploiting and integrating side information is computationally expensive. Existing hybrid recommendation methods treat each user equally and ignore that the pure CF methods have already achieved both effective and efficient recommendation performance for active users with sufficient interaction records and the little improvement brought by side information to these active users is ignorable. Therefore, they are not cost-effective solutions. One cost-effective idea to bypass this dilemma is to generate sufficient \"real\" interaction data for the inactive users with the help of side information, and then a pure CF method could be performed on this augmented dataset effectively. However, there are three major challenges to implement this idea. Firstly, how to ensure the correctness of the generated interaction data. Secondly, how to combine the data augmentation process and recommendation process into a unified model and train the model end-to-end. Thirdly, how to make the solution generalizable for various side information and recommendation tasks. In light of these challenges, we propose a generic and effective CF model called AugCF that supports a wide variety of recommendation tasks. AugCF is based on Conditional Generative Adversarial Nets that additionally consider the class (like or dislike) as a feature to generate new interaction data, which can be a sufficiently real augmentation to the original dataset. Also, AugCF adopts a novel discriminator loss and Gumbel-Softmax approximation to enable end-to-end training. Finally, extensive experiments are conducted on two large-scale recommendation datasets, and the experimental results show the superiority of our proposed model.",
        "authors": [
            "Qinyong Wang",
            "Hongzhi Yin",
            "Hao Wang",
            "Q. Nguyen",
            "Zi Huang",
            "Li-zhen Cui"
        ],
        "citations": 98,
        "references": 50,
        "year": 2019
    },
    {
        "title": "Global-to-local generative model for 3D shapes",
        "abstract": "We introduce a generative model for 3D man-made shapes. The presented method takes a global-to-local (G2L) approach. An adversarial network (GAN) is built first to construct the overall structure of the shape, segmented and labeled into parts. A novel conditional auto-encoder (AE) is then augmented to act as a part-level refiner. The GAN, associated with additional local discriminators and quality losses, synthesizes a voxel-based model, and assigns the voxels with part labels that are represented in separate channels. The AE is trained to amend the initial synthesis of the parts, yielding more plausible part geometries. We also introduce new means to measure and evaluate the performance of an adversarial generative model. We demonstrate that our global-to-local generative model produces significantly better results than a plain three-dimensional GAN, in terms of both their shape variety and the distribution with respect to the training data.",
        "authors": [
            "Hao Wang",
            "Nadav Schor",
            "Ruizhen Hu",
            "Haibin Huang",
            "D. Cohen-Or",
            "Hui Huang"
        ],
        "citations": 72,
        "references": 45,
        "year": 2018
    },
    {
        "title": "A Generative Model for category text generation",
        "abstract": null,
        "authors": [
            "Yang Li",
            "Q. Pan",
            "Suhang Wang",
            "Tao Yang",
            "E. Cambria"
        ],
        "citations": 144,
        "references": 52,
        "year": 2018
    },
    {
        "title": "Differentially Private Releasing via Deep Generative Model",
        "abstract": "Privacy-preserving releasing of complex data (e.g., image, text, audio) represents a long-standing challenge for the data mining research community. Due to rich semantics of the data and lack of a priori knowledge about the analysis task, excessive sanitization is often necessary to ensure privacy, leading to significant loss of the data utility. In this paper, we present dp-GAN, a general private releasing framework for semantic-rich data. Instead of sanitizing and then releasing the data, the data curator publishes a deep generative model which is trained using the original data in a differentially private manner; with the generative model, the analyst is able to produce an unlimited amount of synthetic data for arbitrary analysis tasks. In contrast of alternative solutions, dp-GAN highlights a set of key features: (i) it provides theoretical privacy guarantee via enforcing the differential privacy principle; (ii) it retains desirable utility in the released model, enabling a variety of otherwise impossible analyses; and (iii) most importantly, it achieves practical training scalability and stability by employing multi-fold optimization strategies. Through extensive empirical evaluation on benchmark datasets and analyses, we validate the efficacy of dp-GAN.",
        "authors": [
            "Xinyang Zhang",
            "S. Ji",
            "Ting Wang"
        ],
        "citations": 68,
        "references": 44,
        "year": 2018
    },
    {
        "title": "Lund jet images from generative and cycle-consistent adversarial networks",
        "abstract": null,
        "authors": [
            "S. Carrazza",
            "F. Dreyer"
        ],
        "citations": 49,
        "references": 63,
        "year": 2019
    },
    {
        "title": "Boosting Deep Learning Risk Prediction with Generative Adversarial Networks for Electronic Health Records",
        "abstract": "The rapid growth of Electronic Health Records (EHRs), as well as the accompanied opportunities in Data-Driven Healthcare (DDH), has been attracting widespread interests and attentions. Recent progress in the design and applications of deep learning methods has shown promising results and is forcing massive changes in healthcare academia and industry, but most of these methods rely on massive labeled data. In this work, we propose a general deep learning framework which is able to boost risk prediction performance with limited EHR data. Our model takes a modified generative adversarial network namely ehrGAN, which can provide plausible labeled EHR data by mimicking real patient records, to augment the training dataset in a semi-supervised learning manner. We use this generative model together with a convolutional neural network (CNN) based prediction model to improve the onset prediction performance. Experiments on two real healthcare datasets demonstrate that our proposed framework produces realistic data samples and achieves significant improvements on classification tasks with the generated data over several stat-of-the-art baselines.",
        "authors": [
            "Zhengping Che",
            "Yu Cheng",
            "Shuangfei Zhai",
            "Zhaonan Sun",
            "Yan Liu"
        ],
        "citations": 149,
        "references": 49,
        "year": 2017
    },
    {
        "title": "Multi-chart generative surface modeling",
        "abstract": "This paper introduces a 3D shape generative model based on deep neural networks. A new image-like (i.e., tensor) data representation for genus-zero 3D shapes is devised. It is based on the observation that complicated shapes can be well represented by multiple parameterizations (charts), each focusing on a different part of the shape. The new tensor data representation is used as input to Generative Adversarial Networks for the task of 3D shape generation. The 3D shape tensor representation is based on a multi-chart structure that enjoys a shape covering property and scale-translation rigidity. Scale-translation rigidity facilitates high quality 3D shape learning and guarantees unique reconstruction. The multi-chart structure uses as input a dataset of 3D shapes (with arbitrary connectivity) and a sparse correspondence between them. The output of our algorithm is a generative model that learns the shape distribution and is able to generate novel shapes, interpolate shapes, and explore the generated shape space. The effectiveness of the method is demonstrated for the task of anatomic shape generation including human body and bone (teeth) shape generation.",
        "authors": [
            "Heli Ben-Hamu",
            "Haggai Maron",
            "Itay Kezurer",
            "G. Avineri",
            "Y. Lipman"
        ],
        "citations": 74,
        "references": 48,
        "year": 2018
    },
    {
        "title": "From optimal transport to generative modeling: the VEGAN cookbook",
        "abstract": "We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution $P_X$ and the latent variable model distribution $P_G$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from $P_X$ and $P_G$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.",
        "authors": [
            "O. Bousquet",
            "S. Gelly",
            "I. Tolstikhin",
            "Carl-Johann Simon-Gabriel",
            "B. Schoelkopf"
        ],
        "citations": 143,
        "references": 17,
        "year": 2017
    },
    {
        "title": "A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs",
        "abstract": "Computer or human? Proving that we are human is now part of many tasks that we do on the internet, such as creating an email account, voting in an online poll, or even downloading a scientific paper. One of the most popular tests is text-based CAPTCHA, where would-be users are asked to decipher letters that may be distorted, partially obscured, or shown against a busy background. This test is used because computers find it tricky, but (most) humans do not. George et al. developed a hierarchical model for computer vision that was able to solve CAPTCHAs with a high accuracy rate using comparatively little training data. The results suggest that moving away from text-based CAPTCHAs, as some online services have done, may be a good idea. Science, this issue p. eaag2612 A hierarchical computer vision model solves CAPTCHAs with a high accuracy rate using relatively little training data. INTRODUCTION Compositionality, generalization, and learning from a few examples are among the hallmarks of human intelligence. CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart), images used by websites to block automated interactions, are examples of problems that are easy for people but difficult for computers. CAPTCHAs add clutter and crowd letters together to create a chicken-and-egg problem for algorithmic classifiers—the classifiers work well for characters that have been segmented out, but segmenting requires an understanding of the characters, which may be rendered in a combinatorial number of ways. CAPTCHAs also demonstrate human data efficiency: A recent deep-learning approach for parsing one specific CAPTCHA style required millions of labeled examples, whereas humans solve new styles without explicit training. By drawing inspiration from systems neuroscience, we introduce recursive cortical network (RCN), a probabilistic generative model for vision in which message-passing–based inference handles recognition, segmentation, and reasoning in a unified manner. RCN learns with very little training data and fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters. In addition, RCN outperforms deep neural networks on a variety of benchmarks while being orders of magnitude more data-efficient. RATIONALE Modern deep neural networks resemble the feed-forward hierarchy of simple and complex cells in the neocortex. Neuroscience has postulated computational roles for lateral and feedback connections, segregated contour and surface representations, and border-ownership coding observed in the visual cortex, yet these features are not commonly used by deep neural nets. We hypothesized that systematically incorporating these findings into a new model could lead to higher data efficiency and generalization. Structured probabilistic models provide a natural framework for incorporating prior knowledge, and belief propagation (BP) is an inference algorithm that can match the cortical computational speed. The representational choices in RCN were determined by investigating the computational underpinnings of neuroscience data under the constraint that accurate inference should be possible using BP. RESULTS RCN was effective in breaking a wide variety of CAPTCHAs with very little training data and without using CAPTCHA-specific heuristics. By comparison, a convolutional neural network required a 50,000-fold larger training set and was less robust to perturbations to the input. Similar results are shown on one- and few-shot MNIST (modified National Institute of Standards and Technology handwritten digit data set) classification, where RCN was significantly more robust to clutter introduced during testing. As a generative model, RCN outperformed neural network models when tested on noisy and cluttered examples and generated realistic samples from one-shot training of handwritten characters. RCN also proved to be effective at an occlusion reasoning task that required identifying the precise relationships between characters at multiple points of overlap. On a standard benchmark for parsing text in natural scenes, RCN outperformed state-of-the-art deep-learning methods while requiring 300-fold less training data. CONCLUSION Our work demonstrates that structured probabilistic models that incorporate inductive biases from neuroscience can lead to robust, generalizable machine learning models that learn with high data efficiency. In addition, our model’s effectiveness in breaking text-based CAPTCHAs with very little training data suggests that websites should seek more robust mechanisms for detecting automated interactions. Breaking CAPTCHAs using a generative vision model. Text-based CAPTCHAs exploit the data efficiency and generative aspects of human vision to create a challenging task for machines. By handling recognition and segmentation in a unified way, our model fundamentally breaks the defense of text-based CAPTCHAs. Shown are the parses by our model for a variety of CAPTCHAs . Learning from a few examples and generalizing to markedly different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing–based inference handles recognition, segmentation, and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects such as data efficiency and compositionality that may be important in the path toward general artificial intelligence.",
        "authors": [
            "Dileep George",
            "Wolfgang Lehrach",
            "Ken Kansky",
            "M. Lázaro-Gredilla",
            "C. Laan",
            "B. Marthi",
            "Xinghua Lou",
            "Zhaoshi Meng",
            "Yi Liu",
            "Hua-Yan Wang",
            "Alexander Lavin",
            "D. Phoenix"
        ],
        "citations": 252,
        "references": 112,
        "year": 2017
    },
    {
        "title": "Deep Generative Filter for Motion Deblurring",
        "abstract": "Removing blur caused by camera shake in images has always been a challenging problem in computer vision literature due to its ill-posed nature. Motion blur caused due to the relative motion between the camera and the object in 3D space induces a spatially varying blurring effect over the entire image. In this paper, we propose a novel deep filter based on Generative Adversarial Network (GAN) architecture integrated with global skip connection and dense architecture in order to tackle this problem. Our model, while bypassing the process of blur kernel estimation, significantly reduces the test time which is necessary for practical applications. The experiments on the benchmark datasets prove the effectiveness of the proposed method which outperforms the state-of-the-art blind deblurring algorithms both quantitatively and qualitatively.",
        "authors": [
            "S. Ramakrishnan",
            "Shubham Pachori",
            "Aalok Gangopadhyay",
            "S. Raman"
        ],
        "citations": 116,
        "references": 24,
        "year": 2017
    },
    {
        "title": "Regressive and generative neural networks for scalar field theory",
        "abstract": "We explore the perspectives of machine learning techniques in the context of quantum field theories. In particular, we discuss two-dimensional complex scalar field theory at nonzero temperature and chemical potential---a theory with a nontrivial phase diagram. A neural network is successfully trained to recognize the different phases of this system and to predict the values of various observables, based on the field configurations. We analyze a broad range of chemical potentials and find that the network is robust and able to recognize patterns far away from the point where it was trained. Aside from the regressive analysis, which belongs to supervised learning, an unsupervised generative network is proposed to produce new quantum field configurations that follow a specific distribution. An implicit local constraint fulfilled by the physical configurations was found to be automatically captured by our generative model. We elaborate on potential uses of such a generative approach for sampling outside the training region.",
        "authors": [
            "K. Zhou",
            "G. Endrődi",
            "L. Pang",
            "H. Stöcker"
        ],
        "citations": 72,
        "references": 41,
        "year": 2018
    },
    {
        "title": "GraphRNN: A Deep Generative Model for Graphs",
        "abstract": "Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. \nIn order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.",
        "authors": [
            "Jiaxuan You",
            "Rex Ying",
            "Xiang Ren",
            "William L. Hamilton",
            "J. Leskovec"
        ],
        "citations": 101,
        "references": 33,
        "year": 2018
    },
    {
        "title": "A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation",
        "abstract": "In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthesis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye geometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermediate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model's effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields.",
        "authors": [
            "Kang Wang",
            "Rui Zhao",
            "Q. Ji"
        ],
        "citations": 58,
        "references": 42,
        "year": 2018
    },
    {
        "title": "Stochastic Generative Hashing",
        "abstract": "Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive databases. However, due to the requirement of discrete outputs for the hash functions, learning such functions is known to be very challenging. In addition, the objective functions adopted by existing hashing techniques are mostly chosen heuristically. In this paper, we propose a novel generative approach to learn hash functions through Minimum Description Length principle such that the learned hash codes maximally compress the dataset and can also be used to regenerate the inputs. We also develop an efficient learning algorithm based on the stochastic distributional gradient, which avoids the notorious difficulty caused by binary output constraints, to jointly optimize the parameters of the hash function and the associated generative model. Extensive experiments on a variety of large-scale datasets show that the proposed method achieves better retrieval results than the existing state-of-the-art methods.",
        "authors": [
            "Bo Dai",
            "Ruiqi Guo",
            "Sanjiv Kumar",
            "Niao He",
            "Le Song"
        ],
        "citations": 104,
        "references": 51,
        "year": 2017
    },
    {
        "title": "A generative model of whole-brain effective connectivity",
        "abstract": null,
        "authors": [
            "S. Frässle",
            "Ekaterina I. Lomakina",
            "L. Kasper",
            "Zina-Mary Manjaly",
            "A. Leff",
            "K. Pruessmann",
            "J. Buhmann",
            "K. Stephan"
        ],
        "citations": 94,
        "references": 133,
        "year": 2018
    },
    {
        "title": "The shape variational autoencoder: A deep generative model of part‐segmented 3D objects",
        "abstract": "We introduce a generative model of part‐segmented 3D objects: the shape variational auto‐encoder (ShapeVAE). The ShapeVAE describes a joint distribution over the existence of object parts, the locations of a dense set of surface points, and over surface normals associated with these points. Our model makes use of a deep encoder‐decoder architecture that leverages the part‐decomposability of 3D objects to embed high‐dimensional shape representations and sample novel instances. Given an input collection of part‐segmented objects with dense point correspondences the ShapeVAE is capable of synthesizing novel, realistic shapes, and by performing conditional inference enables imputation of missing parts or surface normals. In addition, by generating both points and surface normals, our model allows for the use of powerful surface‐reconstruction methods for mesh synthesis. We provide a quantitative evaluation of the ShapeVAE on shape‐completion and test‐set log‐likelihood tasks and demonstrate that the model performs favourably against strong baselines. We demonstrate qualitatively that the ShapeVAE produces plausible shape samples, and that it captures a semantically meaningful shape‐embedding. In addition we show that the ShapeVAE facilitates mesh reconstruction by sampling consistent surface normals.",
        "authors": [
            "C. Nash",
            "Christopher K. I. Williams"
        ],
        "citations": 129,
        "references": 54,
        "year": 2017
    },
    {
        "title": "Deep Generative Dual Memory Network for Continual Learning",
        "abstract": "Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time -- this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.",
        "authors": [
            "Nitin Kamra",
            "Umang Gupta",
            "Yan Liu"
        ],
        "citations": 147,
        "references": 44,
        "year": 2017
    },
    {
        "title": "Unleashing Generative Modeling",
        "abstract": null,
        "authors": [
            "Micheal Lanham"
        ],
        "citations": 0,
        "references": 0,
        "year": 2021
    },
    {
        "title": "Unsupervised Primitive Discovery for Improved 3D Generative Modeling",
        "abstract": "3D shape generation is a challenging problem due to the high-dimensional output space and complex part configurations of real-world objects. As a result, existing algorithms experience difficulties in accurate generative modeling of 3D shapes. Here, we propose a novel factorized generative model for 3D shape generation that sequentially transitions from coarse to fine scale shape generation. To this end, we introduce an unsupervised primitive discovery algorithm based on a higher-order conditional random field model. Using the primitive parts for shapes as attributes, a parameterized 3D representation is modeled in the first stage. This representation is further refined in the next stage by adding fine scale details to shape. Our results demonstrate improved representation ability of the generative model and better quality samples of newly generated 3D shapes. Further, our primitive generation approach can accurately parse common objects into a simplified representation.",
        "authors": [
            "Salman Hameed Khan",
            "Yulan Guo",
            "Munawar Hayat",
            "Nick Barnes"
        ],
        "citations": 32,
        "references": 46,
        "year": 2019
    },
    {
        "title": "Online generative model personalization for hand tracking",
        "abstract": "We present a new algorithm for real-time hand tracking on commodity depth-sensing devices. Our method does not require a user-specific calibration session, but rather learns the geometry as the user performs live in front of the camera, thus enabling seamless virtual interaction at the consumer level. The key novelty in our approach is an online optimization algorithm that jointly estimates pose and shape in each frame, and determines the uncertainty in such estimates. This knowledge allows the algorithm to integrate per-frame estimates over time, and build a personalized geometric model of the captured user. Our approach can easily be integrated in state-of-the-art continuous generative motion tracking software. We provide a detailed evaluation that shows how our approach achieves accurate motion tracking for real-time applications, while significantly simplifying the workflow of accurate hand performance capture. We also provide quantitative evaluation datasets at http://gfx.uvic.ca/datasets/handy",
        "authors": [
            "Anastasia Tkach",
            "A. Tagliasacchi",
            "Edoardo Remelli",
            "M. Pauly",
            "A. Fitzgibbon"
        ],
        "citations": 89,
        "references": 42,
        "year": 2017
    },
    {
        "title": "Generative Attribute Controller with Conditional Filtered Generative Adversarial Networks",
        "abstract": "We present a generative attribute controller (GAC), a novel functionality for generating or editing an image while intuitively controlling large variations of an attribute. This controller is based on a novel generative model called the conditional filtered generative adversarial network (CFGAN), which is an extension of the conventional conditional GAN (CGAN) that incorporates a filtering architecture into the generator input. Unlike the conventional CGAN, which represents an attribute directly using an observable variable (e.g., the binary indicator of attribute presence) so its controllability is restricted to attribute labeling (e.g., restricted to an ON or OFF control), the CFGAN has a filtering architecture that associates an attribute with a multi-dimensional latent variable, enabling latent variations of the attribute to be represented. We also define the filtering architecture and training scheme considering controllability, enabling the variations of the attribute to be intuitively controlled using typical controllers (radio buttons and slide bars). We evaluated our CFGAN on MNIST, CUB, and CelebA datasets and show that it enables large variations of an attribute to be not only represented but also intuitively controlled while retaining identity. We also show that the learned latent space has enough expressive power to conduct attribute transfer and attribute-based image retrieval.",
        "authors": [
            "Takuhiro Kaneko",
            "Kaoru Hiramatsu",
            "K. Kashino"
        ],
        "citations": 91,
        "references": 57,
        "year": 2017
    },
    {
        "title": "Introspective Neural Networks for Generative Modeling",
        "abstract": "We study unsupervised learning by developing a generative model built from progressively learned deep convolutional neural networks. The resulting generator is additionally a discriminator, capable of \"introspection\" in a sense — being able to self-evaluate the difference between its generated samples and the given training data. Through repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. Specifically, our model learns a sequence of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and unsupervised feature learning.",
        "authors": [
            "Justin Lazarow",
            "Long Jin",
            "Z. Tu"
        ],
        "citations": 64,
        "references": 51,
        "year": 2017
    },
    {
        "title": "Determining Excitatory and Inhibitory Neuronal Activity from Multimodal fMRI Data Using a Generative Hemodynamic Model",
        "abstract": "Hemodynamic responses, in general, and the blood oxygenation level-dependent (BOLD) fMRI signal, in particular, provide an indirect measure of neuronal activity. There is strong evidence that the BOLD response correlates well with post-synaptic changes, induced by changes in the excitatory and inhibitory (E-I) balance between active neuronal populations. Typical BOLD responses exhibit transients, such as the early-overshoot and post-stimulus undershoot, that can be linked to transients in neuronal activity, but they can also result from vascular uncoupling between cerebral blood flow (CBF) and venous cerebral blood volume (venous CBV). Recently, we have proposed a novel generative hemodynamic model of the BOLD signal within the dynamic causal modeling framework, inspired by physiological observations, called P-DCM (Havlicek et al., 2015). We demonstrated the generative model's ability to more accurately model commonly observed neuronal and vascular transients in single regions but also effective connectivity between multiple brain areas (Havlicek et al., 2017b). In this paper, we additionally demonstrate the versatility of the generative model to jointly explain dynamic relationships between neuronal and hemodynamic physiological variables underlying the BOLD signal using multi-modal data. For this purpose, we utilized three distinct data-sets of experimentally induced responses in the primary visual areas measured in human, cat, and monkey brain, respectively: (1) CBF and BOLD responses; (2) CBF, total CBV, and BOLD responses (Jin and Kim, 2008); and (3) positive and negative neuronal and BOLD responses (Shmuel et al., 2006). By fitting the generative model to the three multi-modal experimental data-sets, we showed that the presence or absence of dynamic features in the BOLD signal is not an unambiguous indication of presence or absence of those features on the neuronal level. Nevertheless, the generative model that takes into account the dynamics of the physiological mechanisms underlying the BOLD response allowed dissociating neuronal from vascular transients and deducing excitatory and inhibitory neuronal activity time-courses from BOLD data alone and from multi-modal data.",
        "authors": [
            "M. Havlíček",
            "D. Ivanov",
            "A. Roebroeck",
            "K. Uludağ"
        ],
        "citations": 79,
        "references": 115,
        "year": 2017
    },
    {
        "title": "Generative model benchmarks for superconducting qubits",
        "abstract": "In this work we experimentally demonstrate how generative model training can be used as a benchmark for small ($<5$ qubits) quantum devices. Performance is quantified using three data analytic metrics: the Kullbeck-Leiber divergence, and two adaptations of the F1 score. Using the $2\\times2$ Bars and Stripes dataset, we determine optimal circuit constructions for generative model training on superconducting qubits by including hardware connectivity constraints into circuit design. We show that on noisy hardware sparsely connected, shallow circuits out-perform denser counterparts.",
        "authors": [
            "Kathleen E. Hamilton",
            "E. Dumitrescu",
            "R. Pooser"
        ],
        "citations": 38,
        "references": 23,
        "year": 2018
    },
    {
        "title": "Generative Model",
        "abstract": null,
        "authors": [],
        "citations": 35,
        "references": 0,
        "year": 2018
    },
    {
        "title": "Lifelong Generative Modeling",
        "abstract": null,
        "authors": [
            "Jason Ramapuram",
            "Magda Gregorová",
            "Alexandros Kalousis"
        ],
        "citations": 112,
        "references": 169,
        "year": 2017
    },
    {
        "title": "A Geometric View of Optimal Transportation and Generative Model",
        "abstract": null,
        "authors": [
            "Na Lei",
            "Kehua Su",
            "Li Cui",
            "S. Yau",
            "X. Gu"
        ],
        "citations": 134,
        "references": 58,
        "year": 2017
    },
    {
        "title": "CytoGAN: Generative Modeling of Cell Images",
        "abstract": "We explore the application of Generative Adversarial Networks to the domain of morphological profiling of human cultured cells imaged by fluorescence microscopy. When evaluated for their ability to group cell images responding to treatment by chemicals of known classes, we find that adversarially learned representations are superior to autoencoder-based approaches. While currently inferior to classical computer vision and transfer learning, the adversarial framework enables useful visualization of the variation of cellular images due to their generative capabilities.",
        "authors": [
            "Peter Goldsborough",
            "Nick Pawlowski",
            "Juan C. Caicedo",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "citations": 65,
        "references": 20,
        "year": 2017
    },
    {
        "title": "Coverless Steganography for Digital Images Based on a Generative Model",
        "abstract": "In this paper, we propose a novel coverless image steganographic scheme based on a generative model. In our scheme, the secret image is first fed to the generative model database, to generate a meaning-normal and independent image different from the secret images. The generated image is then transmitted to the receiver and fed to the generative model database to generate another image visually the same as the secret image. Thus, we only need to transmit the meaning-normal image which is not related to the secret image, and we can achieve the same effect as the transmission of the secret image. This is the first time to propose the coverless image information hiding method based on generative model, compared with the traditional image steganography. The transmitted image is not embedded with any information of the secret image in this method, therefore, can effectively resist steganalysis tools. Experimental results show that our scheme has high capacity, security and reliability.",
        "authors": [
            "X. Duan"
        ],
        "citations": 32,
        "references": 31,
        "year": 2018
    },
    {
        "title": "A generative model for intention recognition and manipulation assistance in teleoperation",
        "abstract": "Performing remote manipulation tasks by tele-operation with limited bandwidth, communication delays and environmental differences is a challenging problem. In this paper, we learn a task-parameterized generative model from the teleoperator demonstrations using a hidden semi-Markov model that provides assistance in performing remote manipulation tasks. We present a probabilistic formulation to capture the intention of the teleoperator, and subsequently assist the teleoperator by time-independent shared control and/or time-dependent autonomous control formulations of the model. In the shared control mode, the model corrects the remote arm movement based on the current state of the teleoperator; whereas in the autonomous control mode, the model generates the movement of the remote arm for autonomous task execution. We show the formulation of the model with virtual fixtures and provide comparisons to benchmark our approach. Teleoperation experiments with the Baxter robot for reaching a movable target and opening a valve reveal that the proposed methodology improves the performance of the teleoperator and caters for environmental differences in performing remote manipulation tasks.",
        "authors": [
            "A. Tanwani",
            "S. Calinon"
        ],
        "citations": 60,
        "references": 33,
        "year": 2017
    },
    {
        "title": "A Generative Model For Electron Paths",
        "abstract": "Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using `arrow-pushing' diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret, (b) incorporating constraints of chemistry, such as balanced atom counts before and after the reaction, and (c) naturally encoding the sparsity of chemical reactions, which usually involve changes in only a small number of atoms in the reactants.We design a method to extract approximate reaction paths from any dataset of atom-mapped reaction SMILES strings. Our model achieves excellent performance on an important subset of the USPTO reaction dataset, comparing favorably to the strongest baselines. Furthermore, we show that our model recovers a basic knowledge of chemistry without being explicitly trained to do so.",
        "authors": [
            "John Bradshaw",
            "Matt J. Kusner",
            "Brooks Paige",
            "Marwin H. S. Segler",
            "José Miguel Hernández-Lobato"
        ],
        "citations": 50,
        "references": 37,
        "year": 2018
    },
    {
        "title": "A Generative Model For Electron Paths",
        "abstract": "Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using `arrow-pushing' diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret, (b) incorporating constraints of chemistry, such as balanced atom counts before and after the reaction, and (c) naturally encoding the sparsity of chemical reactions, which usually involve changes in only a small number of atoms in the reactants.We design a method to extract approximate reaction paths from any dataset of atom-mapped reaction SMILES strings. Our model achieves excellent performance on an important subset of the USPTO reaction dataset, comparing favorably to the strongest baselines. Furthermore, we show that our model recovers a basic knowledge of chemistry without being explicitly trained to do so.",
        "authors": [
            "John Bradshaw",
            "Matt J. Kusner",
            "Brooks Paige",
            "Marwin H. S. Segler",
            "José Miguel Hernández-Lobato"
        ],
        "citations": 50,
        "references": 37,
        "year": 2018
    },
    {
        "title": "Generative Modeling with Conditional Autoencoders: Building an Integrated Cell",
        "abstract": "We present a conditional generative model to learn variation in cell and nuclear morphology and the location of subcellular structures from microscopy images. Our model generalizes to a wide range of subcellular localization and allows for a probabilistic interpretation of cell and nuclear morphology and structure localization from fluorescence images. We demonstrate the effectiveness of our approach by producing photo-realistic cell images using our generative model. The conditional nature of the model provides the ability to predict the localization of unobserved structures given cell and nuclear morphology.",
        "authors": [
            "Gregory R. Johnson",
            "Rory M. Donovan-Maiye",
            "M. Maleckar"
        ],
        "citations": 36,
        "references": 20,
        "year": 2017
    },
    {
        "title": "Generative Modeling",
        "abstract": "(1) Discriminative modeling is used to teach machines how to categorize existing data. With a large enough dataset, a machine can learn to categorize a dataset of handwritten numbers by digit, to differentiate positive and negative reviews on Yelp, to detect the probability of an x-ray containing a broken bone. The model is trained on a dataset for which we already have the correct labels for, compared to the ground truth, and modified to improve its accuracy.",
        "authors": [
            "Anna Dai"
        ],
        "citations": 7,
        "references": 4,
        "year": 2020
    },
    {
        "title": "AUTO-ENCODING VARIATIONAL BAYES",
        "abstract": "To make decisions based on a model fit by Auto-Encoding Variational Bayes (AEVB), practitioners typically use importance sampling to estimate a functional of the posterior distribution. The variational distribution found by AEVB serves as the proposal distribution for importance sampling. However, this proposal distribution may give unreliable (high variance) importance sampling estimates, thus leading to poor decisions. We explore how changing the objective function for learning the variational distribution, while continuing to learn the generative model based on the ELBO, affects the quality of downstream decisions. For a particular model, we characterize the error of importance sampling as a function of posterior variance and show that proposal distributions learned with evidence upper bounds are better. Motivated by these theoretical results, we propose a novel variant of the VAE. In addition to experimenting with MNIST, we present a full-fledged application of the proposed method to single-cell RNA sequencing. In this challenging instance of multiple hypothesis testing, the proposed method surpasses the current state of the art.",
        "authors": [
            "Romain Lopez",
            "Pierre Boyeau",
            "N. Yosef",
            "Michael I. Jordan",
            "J. Regier"
        ],
        "citations": 1000,
        "references": 53,
        "year": 2020
    },
    {
        "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
        "abstract": "The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
        "authors": [
            "Linting Xue",
            "Noah Constant",
            "Adam Roberts",
            "Mihir Kale",
            "Rami Al-Rfou",
            "Aditya Siddhant",
            "Aditya Barua",
            "Colin Raffel"
        ],
        "citations": 1000,
        "references": 55,
        "year": 2020
    },
    {
        "title": "Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions",
        "abstract": null,
        "authors": [
            "Iqbal H. Sarker"
        ],
        "citations": 1000,
        "references": 141,
        "year": 2021
    },
    {
        "title": "Neural Discrete Representation Learning",
        "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
        "authors": [
            "Aäron van den Oord",
            "O. Vinyals",
            "K. Kavukcuoglu"
        ],
        "citations": 1000,
        "references": 43,
        "year": 2017
    },
    {
        "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
        "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
        "authors": [
            "Tero Karras",
            "Timo Aila",
            "S. Laine",
            "J. Lehtinen"
        ],
        "citations": 1000,
        "references": 66,
        "year": 2017
    },
    {
        "title": "Adversarial Discriminative Domain Adaptation",
        "abstract": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.",
        "authors": [
            "Eric Tzeng",
            "Judy Hoffman",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "citations": 1000,
        "references": 32,
        "year": 2017
    },
    {
        "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
        "abstract": "With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.",
        "authors": [
            "Zirui Wang",
            "Jiahui Yu",
            "Adams Wei Yu",
            "Zihang Dai",
            "Yulia Tsvetkov",
            "Yuan Cao"
        ],
        "citations": 729,
        "references": 66,
        "year": 2021
    },
    {
        "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
        "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
        "authors": [
            "Li Dong",
            "Nan Yang",
            "Wenhui Wang",
            "Furu Wei",
            "Xiaodong Liu",
            "Yu Wang",
            "Jianfeng Gao",
            "M. Zhou",
            "H. Hon"
        ],
        "citations": 1000,
        "references": 59,
        "year": 2019
    },
    {
        "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
        "abstract": "Significance Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue–residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction. In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",
        "authors": [
            "Alexander Rives",
            "Siddharth Goyal",
            "Joshua Meier",
            "Demi Guo",
            "Myle Ott",
            "C. L. Zitnick",
            "Jerry Ma",
            "R. Fergus"
        ],
        "citations": 1000,
        "references": 150,
        "year": 2019
    },
    {
        "title": "EnlightenGAN: Deep Light Enhancement Without Paired Supervision",
        "abstract": "Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and the attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. Our codes and pre-trained models are available at: https://github.com/VITA-Group/EnlightenGAN.",
        "authors": [
            "Yifan Jiang",
            "Xinyu Gong",
            "Ding Liu",
            "Yu Cheng",
            "Chen Fang",
            "Xiaohui Shen",
            "Jianchao Yang",
            "Pan Zhou",
            "Zhangyang Wang"
        ],
        "citations": 1000,
        "references": 60,
        "year": 2019
    },
    {
        "title": "Free-Form Image Inpainting With Gated Convolution",
        "abstract": "We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: \\url{https://github.com/JiahuiYu/generative_inpainting}.",
        "authors": [
            "Jiahui Yu",
            "Zhe L. Lin",
            "Jimei Yang",
            "Xiaohui Shen",
            "Xin Lu",
            "Thomas S. Huang"
        ],
        "citations": 1000,
        "references": 54,
        "year": 2018
    },
    {
        "title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro",
        "abstract": "The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market- 1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/ Person-reID_GAN.",
        "authors": [
            "Zhedong Zheng",
            "Liang Zheng",
            "Yi Yang"
        ],
        "citations": 1000,
        "references": 63,
        "year": 2017
    },
    {
        "title": "Tacotron: Towards End-to-End Speech Synthesis",
        "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.",
        "authors": [
            "Yuxuan Wang",
            "R. Skerry-Ryan",
            "Daisy Stanton",
            "Yonghui Wu",
            "Ron J. Weiss",
            "N. Jaitly",
            "Zongheng Yang",
            "Y. Xiao",
            "Z. Chen",
            "Samy Bengio",
            "Quoc V. Le",
            "Yannis Agiomyrgiannakis",
            "R. Clark",
            "R. Saurous"
        ],
        "citations": 1000,
        "references": 26,
        "year": 2017
    },
    {
        "title": "Normalizing Flows for Probabilistic Modeling and Inference",
        "abstract": "Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.",
        "authors": [
            "G. Papamakarios",
            "Eric T. Nalisnick",
            "Danilo Jimenez Rezende",
            "S. Mohamed",
            "Balaji Lakshminarayanan"
        ],
        "citations": 1000,
        "references": 150,
        "year": 2019
    },
    {
        "title": "Toward Multimodal Image-to-Image Translation",
        "abstract": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \\emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
        "authors": [
            "Jun-Yan Zhu",
            "Richard Zhang",
            "Deepak Pathak",
            "Trevor Darrell",
            "Alexei A. Efros",
            "Oliver Wang",
            "Eli Shechtman"
        ],
        "citations": 1000,
        "references": 55,
        "year": 2017
    },
    {
        "title": "GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training",
        "abstract": null,
        "authors": [
            "S. Akçay",
            "Amir Atapour-Abarghouei",
            "T. Breckon"
        ],
        "citations": 1000,
        "references": 55,
        "year": 2018
    },
    {
        "title": "Meta-Learning with Latent Embedding Optimization",
        "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.",
        "authors": [
            "Andrei A. Rusu",
            "Dushyant Rao",
            "Jakub Sygnowski",
            "O. Vinyals",
            "Razvan Pascanu",
            "Simon Osindero",
            "R. Hadsell"
        ],
        "citations": 1000,
        "references": 63,
        "year": 2018
    },
    {
        "title": "Age Progression/Regression by Conditional Adversarial Autoencoder",
        "abstract": "If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5? The answer is probably a No. Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is first mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth.",
        "authors": [
            "Zhifei Zhang",
            "Yang Song",
            "H. Qi"
        ],
        "citations": 1000,
        "references": 33,
        "year": 2017
    },
    {
        "title": "Trajectron++: Dynamically-Feasible Trajectory Forecasting with Heterogeneous Data",
        "abstract": null,
        "authors": [
            "Tim Salzmann",
            "B. Ivanovic",
            "Punarjay Chakravarty",
            "M. Pavone"
        ],
        "citations": 791,
        "references": 59,
        "year": 2020
    },
    {
        "title": "Wasserstein Auto-Encoders",
        "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.",
        "authors": [
            "Ilya O. Tolstikhin",
            "Olivier Bousquet",
            "Sylvain Gelly",
            "B. Schölkopf"
        ],
        "citations": 998,
        "references": 38,
        "year": 2017
    },
    {
        "title": "Toward Controlled Generation of Text",
        "abstract": "Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",
        "authors": [
            "Zhiting Hu",
            "Zichao Yang",
            "Xiaodan Liang",
            "R. Salakhutdinov",
            "E. Xing"
        ],
        "citations": 961,
        "references": 41,
        "year": 2017
    },
    {
        "title": "Molecular de-novo design through deep reinforcement learning",
        "abstract": null,
        "authors": [
            "Marcus Olivecrona",
            "T. Blaschke",
            "O. Engkvist",
            "Hongming Chen"
        ],
        "citations": 938,
        "references": 49,
        "year": 2017
    },
    {
        "title": "Closed-Form Factorization of Latent Semantics in GANs",
        "abstract": "A rich set of interpretable dimensions has been shown to emerge in the latent space of the Generative Adversarial Networks (GANs) trained for synthesizing images. In order to identify such latent dimensions for image editing, previous methods typically annotate a collection of synthesized samples and train linear classifiers in the latent space. However, they require a clear definition of the target attribute as well as the corresponding manual annotations, limiting their applications in practice. In this work, we examine the internal representation learned by GANs to reveal the underlying variation factors in an unsupervised manner. In particular, we take a closer look into the generation mechanism of GANs and further propose a closedform factorization algorithm for latent semantic discovery by directly decomposing the pre-trained weights. With a lightning-fast implementation, our approach is capable of not only finding semantically meaningful dimensions comparably to the state-of-the-art supervised methods, but also resulting in far more versatile concepts across multiple GAN models trained on a wide range of datasets.1",
        "authors": [
            "Yujun Shen",
            "Bolei Zhou"
        ],
        "citations": 566,
        "references": 30,
        "year": 2020
    },
    {
        "title": "SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints",
        "abstract": "This paper addresses the problem of path prediction for multiple interacting agents in a scene, which is a crucial step for many autonomous platforms such as self-driving cars and social robots. We present SoPhie; an interpretable framework based on Generative Adversarial Network (GAN), which leverages two sources of information, the path history of all the agents in a scene, and the scene context information, using images of the scene. To predict a future path for an agent, both physical and social information must be leveraged. Previous work has not been successful to jointly model physical and social interactions. Our approach blends a social attention mechanism with physical attention that helps the model to learn where to look in a large scene and extract the most salient parts of the image relevant to the path. Whereas, the social attention component aggregates information across the different agent interactions and extracts the most important trajectory information from the surrounding neighbors. SoPhie also takes advantage of GAN to generates more realistic samples and to capture the uncertain nature of the future paths by modeling its distribution. All these mechanisms enable our approach to predict socially and physically plausible paths for the agents and to achieve state-of-the-art performance on several different trajectory forecasting benchmarks.",
        "authors": [
            "Amir Sadeghian",
            "Vineet Kosaraju",
            "A. Sadeghian",
            "N. Hirose",
            "S. Savarese"
        ],
        "citations": 845,
        "references": 37,
        "year": 2018
    },
    {
        "title": "Generating Adversarial Examples with Adversarial Networks",
        "abstract": "Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results. Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial exam- ples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses. We apply Adv- GAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly. Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.",
        "authors": [
            "Chaowei Xiao",
            "Bo Li",
            "Jun-Yan Zhu",
            "Warren He",
            "M. Liu",
            "D. Song"
        ],
        "citations": 836,
        "references": 52,
        "year": 2018
    },
    {
        "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network",
        "abstract": "Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.",
        "authors": [
            "Chun-Liang Li",
            "Wei-Cheng Chang",
            "Yu Cheng",
            "Yiming Yang",
            "B. Póczos"
        ],
        "citations": 685,
        "references": 39,
        "year": 2017
    },
    {
        "title": "Unsupervised label noise modeling and loss correction",
        "abstract": "Despite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. When there are a mixture of correct and mislabelled targets, networks \ntend to fit the former before the latter. This suggests using a suitable two-component mixture model as an unsupervised generative model of sample loss values during training to allow online estimation of the probability that a sample is mislabelled. Specifically, we propose a beta mixture to estimate this probability and correct the loss by relying on the network prediction (the so-called bootstrapping loss). We further adapt mixup augmentation to drive our approach a step further. Experiments on CIFAR-10/100 and TinyImageNet demonstrate a robustness to label noise that substantially outperforms recent state-of-the-art. Source code is available at https://git.io/fjsvE and Appendix at https://arxiv.org/abs/1904.11238.",
        "authors": [
            "Eric Arazo Sanchez",
            "Diego Ortego",
            "Paul Albert",
            "N. O’Connor",
            "Kevin McGuinness"
        ],
        "citations": 574,
        "references": 44,
        "year": 2019
    },
    {
        "title": "Active Inference: A Process Theory",
        "abstract": "This article describes a process theory based on active inference and belief propagation. Starting from the premise that all neuronal processing (and action selection) can be explained by maximizing Bayesian model evidence—or minimizing variational free energy—we ask whether neuronal responses can be described as a gradient descent on variational free energy. Using a standard (Markov decision process) generative model, we derive the neuronal dynamics implicit in this description and reproduce a remarkable range of well-characterized neuronal phenomena. These include repetition suppression, mismatch negativity, violation responses, place-cell activity, phase precession, theta sequences, theta-gamma coupling, evidence accumulation, race-to-bound dynamics, and transfer of dopamine responses. Furthermore, the (approximately Bayes’ optimal) behavior prescribed by these dynamics has a degree of face validity, providing a formal explanation for reward seeking, context learning, and epistemic foraging. Technically, the fact that a gradient descent appears to be a valid description of neuronal activity means that variational free energy is a Lyapunov function for neuronal dynamics, which therefore conform to Hamilton’s principle of least action.",
        "authors": [
            "Karl J. Friston",
            "Thomas H. B. FitzGerald",
            "Francesco Rigoli",
            "P. Schwartenbeck",
            "G. Pezzulo"
        ],
        "citations": 752,
        "references": 122,
        "year": 2017
    },
    {
        "title": "Image Super-Resolution via Iterative Refinement",
        "abstract": "We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models (Ho et al. 2020), (Sohl-Dickstein et al. 2015) to image-to-image translation, and performs super-resolution through a stochastic iterative denoising process. Output images are initialized with pure Gaussian noise and iteratively refined using a U-Net architecture that is trained on denoising at various noise levels, conditioned on a low-resolution input image. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8× face super-resolution task on CelebA-HQ for which SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GAN baselines do not exceed a fool rate of 34%. We evaluate SR3 on a 4× super-resolution task on ImageNet, where SR3 outperforms baselines in human evaluation and classification accuracy of a ResNet-50 classifier trained on high-resolution images. We further show the effectiveness of SR3 in cascaded image generation, where a generative model is chained with super-resolution models to synthesize high-resolution images with competitive FID scores on the class-conditional 256×256 ImageNet generation challenge.",
        "authors": [
            "Chitwan Saharia",
            "Jonathan Ho",
            "William Chan",
            "Tim Salimans",
            "David J. Fleet",
            "Mohammad Norouzi"
        ],
        "citations": 1000,
        "references": 74,
        "year": 2021
    },
    {
        "title": "Deep learning enables rapid identification of potent DDR1 kinase inhibitors",
        "abstract": null,
        "authors": [
            "A. Zhavoronkov",
            "Y. Ivanenkov",
            "A. Aliper",
            "M. Veselov",
            "V. Aladinskiy",
            "Anastasiya V Aladinskaya",
            "V. Terentiev",
            "Daniil Polykovskiy",
            "Maksim Kuznetsov",
            "Arip Asadulaev",
            "Yury Volkov",
            "Artem Zholus",
            "Shayakhmetov Rim",
            "Alexander Zhebrak",
            "L. Minaeva",
            "B. Zagribelnyy",
            "Lennart H Lee",
            "R. Soll",
            "D. Madge",
            "Li Xing",
            "Tao Guo",
            "Alán Aspuru-Guzik"
        ],
        "citations": 821,
        "references": 29,
        "year": 2019
    },
    {
        "title": "The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process",
        "abstract": "Many events occur in the world. Some event types are stochastically excited or inhibited—in the sense of having their probabilities elevated or decreased—by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when . We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM . This generative model allows past events to inﬂuence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.",
        "authors": [
            "Hongyuan Mei",
            "Jason Eisner"
        ],
        "citations": 578,
        "references": 32,
        "year": 2017
    },
    {
        "title": "MoFA: Model-Based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction",
        "abstract": "In this work we propose a novel model-based deep convolutional autoencoder that addresses the highly challenging problem of reconstructing a 3D human face from a single in-the-wild color image. To this end, we combine a convolutional encoder network with an expert-designed generative model that serves as decoder. The core innovation is the differentiable parametric decoder that encapsulates image formation analytically based on a generative model. Our decoder takes as input a code vector with exactly defined semantic meaning that encodes detailed face pose, shape, expression, skin reflectance and scene illumination. Due to this new way of combining CNN-based with model-based face reconstruction, the CNN-based encoder learns to extract semantically meaningful parameters from a single monocular input image. For the first time, a CNN encoder and an expert-designed generative model can be trained end-to-end in an unsupervised manner, which renders training on very large (unlabeled) real world data feasible. The obtained reconstructions compare favorably to current state-of-the-art approaches in terms of quality and richness of representation.",
        "authors": [
            "A. Tewari",
            "M. Zollhöfer",
            "Hyeongwoo Kim",
            "Pablo Garrido",
            "Florian Bernard",
            "P. Pérez",
            "C. Theobalt"
        ],
        "citations": 536,
        "references": 68,
        "year": 2017
    },
    {
        "title": "DeepVoxels: Learning Persistent 3D Feature Embeddings",
        "abstract": "In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.",
        "authors": [
            "V. Sitzmann",
            "Justus Thies",
            "Felix Heide",
            "M. Nießner",
            "Gordon Wetzstein",
            "M. Zollhöfer"
        ],
        "citations": 641,
        "references": 65,
        "year": 2018
    },
    {
        "title": "Transformers in Vision: A Survey",
        "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.",
        "authors": [
            "Salman Hameed Khan",
            "Muzammal Naseer",
            "Munawar Hayat",
            "Syed Waqas Zamir",
            "F. Khan",
            "M. Shah"
        ],
        "citations": 1000,
        "references": 285,
        "year": 2021
    },
    {
        "title": "The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection",
        "abstract": null,
        "authors": [
            "Paul Bergmann",
            "Kilian Batzner",
            "Michael Fauser",
            "David Sattlegger",
            "C. Steger"
        ],
        "citations": 294,
        "references": 46,
        "year": 2021
    },
    {
        "title": "Model Adaptation: Unsupervised Domain Adaptation Without Source Data",
        "abstract": "In this paper, we investigate a challenging unsupervised domain adaptation setting --- unsupervised model adaptation. We aim to explore how to rely only on unlabeled target data to improve performance of an existing source prediction model on the target domain, since labeled source data may not be available in some real-world scenarios due to data privacy issues. For this purpose, we propose a new framework, which is referred to as collaborative class conditional generative adversarial net to bypass the dependence on the source data. Specifically, the prediction model is to be improved through generated target-style data, which provides more accurate guidance for the generator. As a result, the generator and the prediction model can collaborate with each other without source data. Furthermore, due to the lack of supervision from source data, we propose a weight constraint that encourages similarity to the source model. A clustering-based regularization is also introduced to produce more discriminative features in the target domain. Compared to conventional domain adaptation methods, our model achieves superior performance on multiple adaptation tasks with only unlabeled target data, which verifies its effectiveness in this challenging setting.",
        "authors": [
            "Rui Li",
            "Qianfen Jiao",
            "Wenming Cao",
            "Hau-San Wong",
            "Si Wu"
        ],
        "citations": 463,
        "references": 66,
        "year": 2020
    },
    {
        "title": "Deep MR to CT Synthesis Using Unpaired Data",
        "abstract": null,
        "authors": [
            "J. Wolterink",
            "A. Dinkla",
            "M. Savenije",
            "P. Seevinck",
            "C. Berg",
            "I. Išgum"
        ],
        "citations": 579,
        "references": 13,
        "year": 2017
    },
    {
        "title": "Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data",
        "abstract": "On-device machine learning (ML) enables the training process to exploit a massive amount of user-generated private data samples. To enjoy this benefit, inter-device communication overhead should be minimized. With this end, we propose federated distillation (FD), a distributed model training algorithm whose communication payload size is much smaller than a benchmark scheme, federated learning (FL), particularly when the model size is large. Moreover, user-generated data samples are likely to become non-IID across devices, which commonly degrades the performance compared to the case with an IID dataset. To cope with this, we propose federated augmentation (FAug), where each device collectively trains a generative model, and thereby augments its local data towards yielding an IID dataset. Empirical studies demonstrate that FD with FAug yields around 26x less communication overhead while achieving 95-98% test accuracy compared to FL.",
        "authors": [
            "Eunjeong Jeong",
            "Seungeun Oh",
            "Hyesung Kim",
            "Jihong Park",
            "M. Bennis",
            "Seong-Lyun Kim"
        ],
        "citations": 539,
        "references": 13,
        "year": 2018
    },
    {
        "title": "Longformer: The Long-Document Transformer",
        "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
        "authors": [
            "Iz Beltagy",
            "Matthew E. Peters",
            "Arman Cohan"
        ],
        "citations": 1000,
        "references": 59,
        "year": 2020
    },
    {
        "title": "HuMoR: 3D Human Motion Model for Robust Pose Estimation",
        "abstract": "We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal pose and shape. Though substantial progress has been made in estimating 3D human motion and shape from dynamic observations, recovering plausible pose sequences in the presence of noise and occlusions remains a challenge. For this purpose, we propose an expressive generative model in the form of a conditional variational autoencoder, which learns a distribution of the change in pose at each step of a motion sequence. Furthermore, we introduce a flexible optimization-based approach that leverages HuMoR as a motion prior to robustly estimate plausible pose and shape from ambiguous observations. Through extensive evaluations, we demonstrate that our model generalizes to diverse motions and body shapes after training on a large motion capture dataset, and enables motion reconstruction from multiple input modalities including 3D keypoints and RGB(-D) videos. See the project page at geometry.stanford.edu/projects/humor.",
        "authors": [
            "Davis Rempe",
            "Tolga Birdal",
            "Aaron Hertzmann",
            "Jimei Yang",
            "Srinath Sridhar",
            "L. Guibas"
        ],
        "citations": 277,
        "references": 108,
        "year": 2021
    },
    {
        "title": "Online Continual Learning with Maximally Interfered Retrieval",
        "abstract": "Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or \"single-pass through the data\" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work, we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at this https URL.",
        "authors": [
            "Rahaf Aljundi",
            "Lucas Caccia",
            "Eugene Belilovsky",
            "Massimo Caccia",
            "Min Lin",
            "Laurent Charlin",
            "T. Tuytelaars"
        ],
        "citations": 477,
        "references": 41,
        "year": 2019
    },
    {
        "title": "Enhanced Pix2pix Dehazing Network",
        "abstract": "In this paper, we reduce the image dehazing problem to an image-to-image translation problem, and propose Enhanced Pix2pix Dehazing Network (EPDN), which generates a haze-free image without relying on the physical scattering model. EPDN is embedded by a generative adversarial network, which is followed by a well-designed enhancer. Inspired by visual perception global-first theory, the discriminator guides the generator to create a pseudo realistic image on a coarse scale, while the enhancer following the generator is required to produce a realistic dehazing image on the fine scale. The enhancer contains two enhancing blocks based on the receptive field model, which reinforces the dehazing effect in both color and details. The embedded GAN is jointly trained with the enhancer. Extensive experiment results on synthetic datasets and real-world datasets show that the proposed EPDN is superior to the state-of-the-art methods in terms of PSNR, SSIM, PI, and subjective visual effect.",
        "authors": [
            "Yanyun Qu",
            "Yizi Chen",
            "Jingying Huang",
            "Yuan Xie"
        ],
        "citations": 499,
        "references": 27,
        "year": 2019
    },
    {
        "title": "Long Text Generation via Adversarial Training with Leaked Information",
        "abstract": "\n \n Automatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets(GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional MANAGER module, which takes the extracted features of current generated words and outputs a latent vector to guide the WORKER module for next-word generation.Our extensive experiments on synthetic data and various real-world tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between MANAGER and WORKER.\n \n",
        "authors": [
            "Jiaxian Guo",
            "Sidi Lu",
            "Han Cai",
            "Weinan Zhang",
            "Yong Yu",
            "Jun Wang"
        ],
        "citations": 475,
        "references": 30,
        "year": 2017
    },
    {
        "title": "Mixed Precision Training",
        "abstract": "Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.",
        "authors": [
            "P. Micikevicius",
            "Sharan Narang",
            "Jonah Alben",
            "G. Diamos",
            "Erich Elsen",
            "David García",
            "Boris Ginsburg",
            "Michael Houston",
            "Oleksii Kuchaiev",
            "Ganesh Venkatesh",
            "Hao Wu"
        ],
        "citations": 1000,
        "references": 38,
        "year": 2017
    },
    {
        "title": "Brain-inspired replay for continual learning with artificial neural networks",
        "abstract": null,
        "authors": [
            "Gido M. van de Ven",
            "H. Siegelmann",
            "A. Tolias"
        ],
        "citations": 428,
        "references": 81,
        "year": 2020
    },
    {
        "title": "DAG-GNN: DAG Structure Learning with Graph Neural Networks",
        "abstract": "Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \\url{this https URL}.",
        "authors": [
            "Yue Yu",
            "Jie Chen",
            "Tian Gao",
            "Mo Yu"
        ],
        "citations": 439,
        "references": 54,
        "year": 2019
    },
    {
        "title": "Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification",
        "abstract": "Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a \"learning via translation\" framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation. Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.",
        "authors": [
            "Weijian Deng",
            "Liang Zheng",
            "Guoliang Kang",
            "Yezhou Yang",
            "Qixiang Ye",
            "Jianbin Jiao"
        ],
        "citations": 878,
        "references": 56,
        "year": 2017
    },
    {
        "title": "Structure-Preserving Super Resolution With Gradient Guidance",
        "abstract": "Structures matter in single image super resolution (SISR). Recent studies benefiting from generative adversarial network (GAN) have promoted the development of SISR by recovering photo-realistic images. However, there are always undesired structural distortions in the recovered images. In this paper, we propose a structure-preserving super resolution method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. Specifically, we exploit gradient maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure priors for the SR process. On the other hand, we propose a gradient loss which imposes a second-order restriction on the super-resolved images. Along with the previous image-space loss functions, the gradient-space objectives help generative networks concentrate more on geometric structures. Moreover, our method is model-agnostic, which can be potentially used for off-the-shelf SR networks. Experimental results show that we achieve the best PI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with state-of-the-art perceptual-driven SR methods. Visual results demonstrate our superiority in restoring structures while generating natural SR images.",
        "authors": [
            "Cheng Ma",
            "Yongming Rao",
            "Yean Cheng",
            "Ce Chen",
            "Jiwen Lu",
            "Jie Zhou"
        ],
        "citations": 280,
        "references": 57,
        "year": 2020
    },
    {
        "title": "Generalized Zero-Shot Learning via Synthesized Examples",
        "abstract": "We present a generative framework for generalized zero-shot learning where the training and test classes are not necessarily disjoint. Built upon a variational autoencoder based architecture, consisting of a probabilistic encoder and a probabilistic conditional decoder, our model can generate novel exemplars from seen/unseen classes, given their respective class attributes. These exemplars can subsequently be used to train any off-the-shelf classification model. One of the key aspects of our encoder-decoder architecture is a feedback-driven mechanism in which a discriminator (a multivariate regressor) learns to map the generated exemplars to the corresponding class attribute vectors, leading to an improved generator. Our model's ability to generate and leverage examples from unseen classes to train the classification model naturally helps to mitigate the bias towards predicting seen classes in generalized zero-shot learning settings. Through a comprehensive set of experiments, we show that our model outperforms several state-of-the-art methods, on several benchmark datasets, for both standard as well as generalized zero-shot learning.",
        "authors": [
            "Gundeep Arora",
            "V. Verma",
            "Ashish Mishra",
            "Piyush Rai"
        ],
        "citations": 433,
        "references": 37,
        "year": 2017
    },
    {
        "title": "FearNet: Brain-Inspired Model for Incremental Learning",
        "abstract": "Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.",
        "authors": [
            "Ronald Kemker",
            "Christopher Kanan"
        ],
        "citations": 453,
        "references": 42,
        "year": 2017
    },
    {
        "title": "Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints",
        "abstract": "Recent advances in Generative Adversarial Networks (GANs) have shown increasing success in generating photorealistic images. But they also raise challenges to visual forensics and model attribution. We present the first study of learning GAN fingerprints towards image attribution and using them to classify an image as real or GAN-generated. For GAN-generated images, we further identify their sources. Our experiments show that (1) GANs carry distinct model fingerprints and leave stable fingerprints in their generated images, which support image attribution; (2) even minor differences in GAN training can result in different fingerprints, which enables fine-grained model authentication; (3) fingerprints persist across different image frequencies and patches and are not biased by GAN artifacts; (4) fingerprint finetuning is effective in immunizing against five types of adversarial image perturbations; and (5) comparisons also show our learned fingerprints consistently outperform several baselines in a variety of setups.",
        "authors": [
            "Ning Yu",
            "Larry S. Davis",
            "Mario Fritz"
        ],
        "citations": 392,
        "references": 75,
        "year": 2018
    },
    {
        "title": "Pose-Normalized Image Generation for Person Re-identification",
        "abstract": null,
        "authors": [
            "Xuelin Qian",
            "Yanwei Fu",
            "T. Xiang",
            "Wenxuan Wang",
            "Jie Qiu",
            "Yang Wu",
            "Yu-Gang Jiang",
            "X. Xue"
        ],
        "citations": 429,
        "references": 90,
        "year": 2017
    },
    {
        "title": "Neural Voice Cloning with a Few Samples",
        "abstract": "Voice cloning is a highly desired feature for personalized speech interfaces. Neural network based speech synthesis has been shown to generate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio samples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios and to be used with a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios. While speaker adaptation can achieve better naturalness and similarity, the cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment.",
        "authors": [
            "Sercan Ö. Arik",
            "Jitong Chen",
            "Kainan Peng",
            "Wei Ping",
            "Yanqi Zhou"
        ],
        "citations": 365,
        "references": 50,
        "year": 2018
    },
    {
        "title": "Population Based Training of Neural Networks",
        "abstract": "Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \\emph{Population Based Training (PBT)}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.",
        "authors": [
            "Max Jaderberg",
            "Valentin Dalibard",
            "Simon Osindero",
            "Wojciech M. Czarnecki",
            "Jeff Donahue",
            "Ali Razavi",
            "O. Vinyals",
            "Tim Green",
            "Iain Dunning",
            "K. Simonyan",
            "Chrisantha Fernando",
            "K. Kavukcuoglu"
        ],
        "citations": 711,
        "references": 56,
        "year": 2017
    },
    {
        "title": "NetGAN: Generating Graphs via Random Walks",
        "abstract": "We propose NetGAN - the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective. NetGAN is able to produce graphs that exhibit the well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, NetGAN opens exciting further avenues for research.",
        "authors": [
            "Aleksandar Bojchevski",
            "Oleksandr Shchur",
            "Daniel Zügner",
            "Stephan Günnemann"
        ],
        "citations": 351,
        "references": 45,
        "year": 2018
    },
    {
        "title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents",
        "abstract": "We introduce two tactics, namely the strategically-timed attack and the enchanting attack, to attack reinforcement learning agents trained by deep reinforcement learning algorithms using adversarial examples. In the strategically-timed attack, the adversary aims at minimizing the agent's reward by only attacking the agent at a small subset of time steps in an episode. Limiting the attack activity to this subset helps prevent detection of the attack by the agent. We propose a novel method to determine when an adversarial example should be crafted and applied. In the enchanting attack, the adversary aims at luring the agent to a designated target state. This is achieved by combining a generative model and a planning algorithm: while the generative model predicts the future states, the planning algorithm generates a preferred sequence of actions for luring the agent. A sequence of adversarial examples is then crafted to lure the agent to take the preferred sequence of actions. We apply the proposed tactics to the agents trained by the state-of-the-art deep reinforcement learning algorithm including DQN and A3C. In 5 Atari games, our strategically-timed attack reduces as much reward as the uniform attack (i.e., attacking at every time step) does by attacking the agent 4 times less often. Our enchanting attack lures the agent toward designated target states with a more than 70% success rate. Example videos are available at http://yclin.me/adversarial_attack_RL/.",
        "authors": [
            "Yen-Chen Lin",
            "Zhang-Wei Hong",
            "Yuan-Hong Liao",
            "Meng-Li Shih",
            "Ming-Yu Liu",
            "Min Sun"
        ],
        "citations": 394,
        "references": 21,
        "year": 2017
    },
    {
        "title": "Facial Expression Recognition by De-expression Residue Learning",
        "abstract": "A facial expression is a combination of an expressive component and a neutral component of a person. In this paper, we propose to recognize facial expressions by extracting information of the expressive component through a de-expression learning procedure, called De-expression Residue Learning (DeRL). First, a generative model is trained by cGAN. This model generates the corresponding neutral face image for any input face image. We call this procedure de-expression because the expressive information is filtered out by the generative model; however, the expressive information is still recorded in the intermediate layers. Given the neutral face image, unlike previous works using pixel-level or feature-level difference for facial expression classification, our new method learns the deposition (or residue) that remains in the intermediate layers of the generative model. Such a residue is essential as it contains the expressive component deposited in the generative model from any input facial expression images. Seven public facial expression databases are employed in our experiments. With two databases (BU-4DFE and BP4D-spontaneous) for pre-training, the DeRL method has been evaluated on five databases, CK+, Oulu-CASIA, MMI, BU-3DFE, and BP4D+. The experimental results demonstrate the superior performance of the proposed method.",
        "authors": [
            "Huiyuan Yang",
            "U. Ciftci",
            "L. Yin"
        ],
        "citations": 344,
        "references": 37,
        "year": 2018
    },
    {
        "title": "Object-Driven Text-To-Image Synthesis via Adversarial Training",
        "abstract": "In this paper, we propose Object-driven Attentive Generative Adversarial Newtorks (Obj-GANs) that allow attention-driven, multi-stage refinement for synthesizing complex images from text descriptions. With a novel object-driven attentive generative network, the Obj-GAN can synthesize salient objects by paying attention to their most relevant words in the text descriptions and their pre-generated class label. In addition, a novel object-wise discriminator based on the Fast R-CNN model is proposed to provide rich object-wise discrimination signals on whether the synthesized object matches the text description and the pre-generated class label. The proposed Obj-GAN significantly outperforms the previous state of the art in various metrics on the large-scale MS-COCO benchmark, increasing the inception score by 27% and decreasing the FID score by 11%. A thorough comparison between the classic grid attention and the new object-driven attention is provided through analyzing their mechanisms and visualizing their attention layers, showing insights of how the proposed model generates complex scenes in high quality.",
        "authors": [
            "Wenbo Li",
            "Pengchuan Zhang",
            "Lei Zhang",
            "Qiuyuan Huang",
            "Xiaodong He",
            "Siwei Lyu",
            "Jianfeng Gao"
        ],
        "citations": 293,
        "references": 37,
        "year": 2019
    },
    {
        "title": "Unsupervised removal of systematic background noise from droplet-based single-cell experiments using CellBender",
        "abstract": null,
        "authors": [
            "Stephen J. Fleming",
            "M. Chaffin",
            "A. Arduini",
            "Amer-Denis Akkad",
            "E. Banks",
            "J. Marioni",
            "A. Philippakis",
            "P. Ellinor",
            "M. Babadi"
        ],
        "citations": 305,
        "references": 62,
        "year": 2019
    },
    {
        "title": "Stock Movement Prediction from Tweets and Historical Prices",
        "abstract": "Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected.",
        "authors": [
            "Yumo Xu",
            "Shay B. Cohen"
        ],
        "citations": 319,
        "references": 28,
        "year": 2018
    },
    {
        "title": "ProGen: Language Modeling for Protein Generation",
        "abstract": "Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",
        "authors": [
            "Ali Madani",
            "Bryan McCann",
            "N. Naik",
            "N. Keskar",
            "N. Anand",
            "Raphael R. Eguchi",
            "Po-Ssu Huang",
            "R. Socher"
        ],
        "citations": 253,
        "references": 53,
        "year": 2020
    },
    {
        "title": "Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction",
        "abstract": "\n \n Conventional methods of 3D object generative modeling learn volumetric predictions using deep networks with 3D convolutional operations, which are direct analogies to classical 2D ones. However, these methods are computationally wasteful in attempt to predict 3D shapes, where information is rich only on the surfaces. In this paper, we propose a novel 3D generative modeling framework to efficiently generate object shapes in the form of dense point clouds. We use 2D convolutional operations to predict the 3D structure from multiple viewpoints and jointly apply geometric reasoning with 2D projection optimization. We introduce the pseudo-renderer, a differentiable module to approximate the true rendering operation, to synthesize novel depth maps for optimization. Experimental results for single-image 3D object reconstruction tasks show that we outperforms state-of-the-art methods in terms of shape similarity and prediction density.\n \n",
        "authors": [
            "Chen-Hsuan Lin",
            "Chen Kong",
            "S. Lucey"
        ],
        "citations": 400,
        "references": 36,
        "year": 2017
    },
    {
        "title": "The Cramer Distance as a Solution to Biased Wasserstein Gradients",
        "abstract": "The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling, and most recently in reinforcement learning. In this paper we describe three natural properties of probability divergences that we believe reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cramer distance. We show that the Cramer distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. We give empirical results on a number of domains comparing these three divergences. To illustrate the practical relevance of the Cramer distance we design a new algorithm, the Cramer Generative Adversarial Network (GAN), and show that it has a number of desirable properties over the related Wasserstein GAN.",
        "authors": [
            "Marc G. Bellemare",
            "Ivo Danihelka",
            "Will Dabney",
            "S. Mohamed",
            "Balaji Lakshminarayanan",
            "Stephan Hoyer",
            "R. Munos"
        ],
        "citations": 330,
        "references": 37,
        "year": 2017
    },
    {
        "title": "Generalized Sliced Wasserstein Distances",
        "abstract": "The Wasserstein distance and its variations, e.g., the sliced-Wasserstein (SW) distance, have recently drawn attention from the machine learning community. The SW distance, specifically, was shown to have similar properties to the Wasserstein distance, while being much simpler to compute, and is therefore used in various applications including generative modeling and general supervised/unsupervised learning. In this paper, we first clarify the mathematical connection between the SW distance and the Radon transform. We then utilize the generalized Radon transform to define a new family of distances for probability measures, which we call generalized sliced-Wasserstein (GSW) distances. We also show that, similar to the SW distance, the GSW distance can be extended to a maximum GSW (max-GSW) distance. We then provide the conditions under which GSW and max-GSW distances are indeed distances. Finally, we compare the numerical performance of the proposed distances on several generative modeling tasks, including SW flows and SW auto-encoders.",
        "authors": [
            "Soheil Kolouri",
            "Kimia Nadjahi",
            "Umut Simsekli",
            "R. Badeau",
            "G. Rohde"
        ],
        "citations": 275,
        "references": 54,
        "year": 2019
    },
    {
        "title": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations",
        "abstract": "The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.",
        "authors": [
            "Yunzhu Li",
            "Jiaming Song",
            "Stefano Ermon"
        ],
        "citations": 348,
        "references": 45,
        "year": 2017
    },
    {
        "title": "Global optimization of dielectric metasurfaces using a physics-driven neural network",
        "abstract": "We present a global optimizer, based on a conditional generative neural network, which can output ensembles of highly efficient topology-optimized metasurfaces operating across a range of parameters. A key feature of the network is that it initially generates a distribution of devices that broadly samples the design space, and then shifts and refines this distribution towards favorable design space regions over the course of optimization. Training is performed by calculating the forward and adjoint electromagnetic simulations of outputted devices and using the subsequent efficiency gradients for backpropagation. With metagratings operating across a range of wavelengths and angles as a model system, we show that devices produced from the trained generative network have efficiencies comparable to or better than the best devices produced by adjoint-based topology optimization, while requiring less computational cost. Our reframing of adjoint-based optimization to the training of a generative neural network applies generally to physical systems that can utilize gradients to improve performance.",
        "authors": [
            "Jiaqi Jiang",
            "Jonathan A. Fan"
        ],
        "citations": 308,
        "references": 38,
        "year": 2019
    },
    {
        "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
        "abstract": "Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.",
        "authors": [
            "Oyvind Tafjord",
            "Bhavana Dalvi",
            "Peter Clark"
        ],
        "citations": 230,
        "references": 28,
        "year": 2020
    },
    {
        "title": "Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems",
        "abstract": "End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.",
        "authors": [
            "Andrea Madotto",
            "Chien-Sheng Wu",
            "Pascale Fung"
        ],
        "citations": 295,
        "references": 45,
        "year": 2018
    },
    {
        "title": "HP-GAN: Probabilistic 3D Human Motion Prediction via GAN",
        "abstract": "Predicting and understanding human motion dynamics has many applications, such as motion synthesis, augmented reality, security, and autonomous vehicles. Due to the recent success of generative adversarial networks (GAN), there has been much interest in probabilistic estimation and synthetic data generation using deep neural network architectures and learning algorithms. We propose a novel sequence-to-sequence model for probabilistic human motion prediction, trained with a modified version of improved Wasserstein generative adversarial networks (WGAN-GP), in which we use a custom loss function designed for human motion prediction. Our model, which we call HP-GAN, learns a probability density function of future human poses conditioned on previous poses. It predicts multiple sequences of possible future human poses, each from the same input sequence but a different vector z drawn from a random distribution. Furthermore, to quantify the quality of the non-deterministic predictions, we simultaneously train a motion-quality-assessment model that learns the probability that a given skeleton sequence is a real human motion. We test our algorithm on two of the largest skeleton datasets: NTURGB-D and Human3.6M. We train our model on both single and multiple action types. Its predictive power for long-term motion estimation is demonstrated by generating multiple plausible futures of more than 30 frames from just 10 frames of input. We show that most sequences generated from the same input have more than 50% probabilities of being judged as a real human sequence. We published all the code used in this paper to https://github.com/ebarsoum/hpgan.",
        "authors": [
            "Emad Barsoum",
            "J. Kender",
            "Zicheng Liu"
        ],
        "citations": 306,
        "references": 42,
        "year": 2017
    },
    {
        "title": "Cross-Modal Deep Variational Hand Pose Estimation",
        "abstract": "The human hand moves in complex and high-dimensional ways, making estimation of 3D hand pose configurations from images alone a challenging task. In this work we propose a method to learn a statistical hand model represented by a cross-modal trained latent space via a generative deep neural network. We derive an objective function from the variational lower bound of the VAE framework and jointly optimize the resulting cross-modal KL-divergence and the posterior reconstruction objective, naturally admitting a training regime that leads to a coherent latent space across multiple modalities such as RGB images, 2D keypoint detections or 3D hand configurations. Additionally, it grants a straightforward way of using semi-supervision. This latent space can be directly used to estimate 3D hand poses from RGB images, outperforming the state-of-the art in different settings. Furthermore, we show that our proposed method can be used without changes on depth images and performs comparably to specialized methods. Finally, the model is fully generative and can synthesize consistent pairs of hand configurations across modalities. We evaluate our method on both RGB and depth datasets and analyze the latent space qualitatively.",
        "authors": [
            "Adrian Spurr",
            "Jie Song",
            "Seonwook Park",
            "Otmar Hilliges"
        ],
        "citations": 278,
        "references": 39,
        "year": 2018
    },
    {
        "title": "Variational Autoencoders and Nonlinear ICA: A Unifying Framework",
        "abstract": "The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model's marginal distribution over observed variables fits the data. Often, we're interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case.",
        "authors": [
            "Ilyes Khemakhem",
            "Diederik P. Kingma",
            "Aapo Hyvärinen"
        ],
        "citations": 525,
        "references": 41,
        "year": 2019
    },
    {
        "title": "Learning to Compose Domain-Specific Transformations for Data Augmentation",
        "abstract": "Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.",
        "authors": [
            "Alexander J. Ratner",
            "Henry R. Ehrenberg",
            "Zeshan Hussain",
            "Jared A. Dunnmon",
            "C. Ré"
        ],
        "citations": 339,
        "references": 36,
        "year": 2017
    },
    {
        "title": "De Novo Design of Bioactive Small Molecules by Artificial Intelligence",
        "abstract": "Generative artificial intelligence offers a fresh view on molecular design. We present the first‐time prospective application of a deep learning model for designing new druglike compounds with desired activities. For this purpose, we trained a recurrent neural network to capture the constitution of a large set of known bioactive compounds represented as SMILES strings. By transfer learning, this general model was fine‐tuned on recognizing retinoid X and peroxisome proliferator‐activated receptor agonists. We synthesized five top‐ranking compounds designed by the generative model. Four of the compounds revealed nanomolar to low‐micromolar receptor modulatory activity in cell‐based assays. Apparently, the computational model intrinsically captured relevant chemical and biological knowledge without the need for explicit rules. The results of this study advocate generative artificial intelligence for prospective de novo molecular design, and demonstrate the potential of these methods for future medicinal chemistry.",
        "authors": [
            "D. Merk",
            "Lukas Friedrich",
            "F. Grisoni",
            "G. Schneider"
        ],
        "citations": 277,
        "references": 23,
        "year": 2018
    },
    {
        "title": "DeepFake Detection by Analyzing Convolutional Traces",
        "abstract": "The Deepfake phenomenon has become very popular nowadays thanks to the possibility to create incredibly realistic images using deep learning tools, based mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we focus on the analysis of Deepfakes of human faces with the objective of creating a new detection method able to detect a forensics trace hidden in images: a sort of fingerprint left in the image generation process. The proposed technique, by means of an Expectation Maximization (EM) algorithm, extracts a set of local features specifically addressed to model the underlying convolutional generative process. Ad-hoc validation has been employed through experimental tests with naive classifiers on five different architectures (GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as ground-truth for non-fakes. Results demonstrated the effectiveness of the technique in distinguishing the different architectures and the corresponding generation process.",
        "authors": [
            "Luca Guarnera",
            "O. Giudice",
            "S. Battiato"
        ],
        "citations": 186,
        "references": 44,
        "year": 2020
    },
    {
        "title": "A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning",
        "abstract": "This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.",
        "authors": [
            "Marco Fraccaro",
            "Simon Kamronn",
            "U. Paquet",
            "O. Winther"
        ],
        "citations": 273,
        "references": 36,
        "year": 2017
    },
    {
        "title": "Be Your Own Prada: Fashion Synthesis with Structural Coherence",
        "abstract": "We present a novel and effective approach for generating new clothing on a wearer through generative adversarial learning. Given an input image of a person and a sentence describing a different outfit, our model “redresses” the person as desired, while at the same time keeping the wearer and her/his pose unchanged. Generating new outfits with precise regions conforming to a language description while retaining wearer’s body structure is a new challenging task. Existing generative adversarial networks are not ideal in ensuring global coherence of structure given both the input photograph and language description as conditions. We address this challenge by decomposing the complex generative process into two conditional stages. In the first stage, we generate a plausible semantic segmentation map that obeys the wearer’s pose as a latent spatial arrangement. An effective spatial constraint is formulated to guide the generation of this semantic segmentation map. In the second stage, a generative model with a newly proposed compositional mapping layer is used to render the final image with precise regions and textures conditioned on this map. We extended the DeepFashion dataset [8] by collecting sentence descriptions for 79K images. We demonstrate the effectiveness of our approach through both quantitative and qualitative evaluations. A user study is also conducted.",
        "authors": [
            "Shizhan Zhu",
            "S. Fidler",
            "R. Urtasun",
            "Dahua Lin",
            "Chen Change Loy"
        ],
        "citations": 270,
        "references": 20,
        "year": 2017
    },
    {
        "title": "Diversified Texture Synthesis with Feed-Forward Networks",
        "abstract": "Recent progresses on deep discriminative and generative modeling have shown promising results on texture synthesis. However, existing feed-forward based methods trade off generality for efficiency, which suffer from many issues, such as shortage of generality (i.e., build one network per texture), lack of diversity (i.e., always produce visually identical output) and suboptimality (i.e., generate less satisfying visual effects). In this work, we focus on solving these issues for improved texture synthesis. We propose a deep generative feed-forward network which enables efficient synthesis of multiple textures within one single network and meaningful interpolation between them. Meanwhile, a suite of important techniques are introduced to achieve better convergence and diversity. With extensive experiments, we demonstrate the effectiveness of the proposed model and techniques for synthesizing a large number of textures and show its applications with the stylization.",
        "authors": [
            "Yijun Li",
            "Chen Fang",
            "Jimei Yang",
            "Zhaowen Wang",
            "Xin Lu",
            "Ming-Hsuan Yang"
        ],
        "citations": 258,
        "references": 35,
        "year": 2017
    },
    {
        "title": "Deep Variational Reinforcement Learning for POMDPs",
        "abstract": "Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.",
        "authors": [
            "Maximilian Igl",
            "L. Zintgraf",
            "T. Le",
            "Frank Wood",
            "Shimon Whiteson"
        ],
        "citations": 248,
        "references": 52,
        "year": 2018
    },
    {
        "title": "DCGAN-Based Data Augmentation for Tomato Leaf Disease Identification",
        "abstract": "Tomato leaf disease seriously affects the yield of tomato. It is extremely vital for agricultural economy to identify agricultural diseases. The traditional data augmentation methods, such as rotation, flip and translation, are severely limited, which cannot achieve good generalization results. To improve the recognition accuracy of tomato leaf diseases, a new method of data augmentation by generative adversarial networks (GANs) is proposed for leaf disease recognition in this work. Generated images augmented by deep convolutional generative adversarial networks (DCGAN) and original images as the input of GoogLeNet, this model can achieve a top-1 average identification accuracy of 94.33%. By adjusting the hyper-parameters, modifying the architecture of the convolutional neural networks, and selecting different generative adversarial networks, an improved model for training and testing 5 classes of tomato leaf images was obtained. Meanwhile, images generated by DCGAN not only enlarge the size of the data set, but also have the characteristics of diversity, which makes the model have a good generalization effect. We have also visually confirmed that the images generated by DCGAN have much better quality and are more convincing through the t-Distributed Stochastic Neighbor Embedding (t-SNE) and Visual Turing Test. Experiments with tomato leaf disease identification show that DCGAN can generate data that approximate to real images, which can be used to (1) provide a larger data set for the training of large neural networks, and improve the performance of the recognition model through highly discriminating image generation technology; (2) reduce the cost of data collection; (3) enhance the diversity of data and the generalization ability of the recognition models.",
        "authors": [
            "Qiufeng Wu",
            "Yiping Chen",
            "Jun Meng"
        ],
        "citations": 166,
        "references": 49,
        "year": 2020
    },
    {
        "title": "Zero-VAE-GAN: Generating Unseen Features for Generalized and Transductive Zero-Shot Learning",
        "abstract": "Zero-shot learning (ZSL) is a challenging task due to the lack of unseen class data during training. Existing works attempt to establish a mapping between the visual and class spaces through a common intermediate semantic space. The main limitation of existing methods is the strong bias towards seen class, known as the domain shift problem, which leads to unsatisfactory performance in both conventional and generalized ZSL tasks. To tackle this challenge, we propose to convert ZSL to the conventional supervised learning by generating features for unseen classes. To this end, a joint generative model that couples variational autoencoder (VAE) and generative adversarial network (GAN), called Zero-VAE-GAN, is proposed to generate high-quality unseen features. To enhance the class-level discriminability, an adversarial categorization network is incorporated into the joint framework. Besides, we propose two self-training strategies to augment unlabeled unseen features for the transductive extension of our model, addressing the domain shift problem to a large extent. Experimental results on five standard benchmarks and a large-scale dataset demonstrate the superiority of our generative model over the state-of-the-art methods for conventional, especially generalized ZSL tasks. Moreover, the further improvement of the transductive setting demonstrates the effectiveness of the proposed self-training strategies.",
        "authors": [
            "R. Gao",
            "Xingsong Hou",
            "Jie Qin",
            "Jiaxin Chen",
            "Li Liu",
            "Fan Zhu",
            "Zhao Zhang",
            "Ling Shao"
        ],
        "citations": 144,
        "references": 91,
        "year": 2020
    },
    {
        "title": "Plant Disease Detection Using Generated Leaves Based on DoubleGAN",
        "abstract": "Plant leaves can be used to effectively detect plant diseases. However, the number of images of unhealthy leaves collected from various plants is usually unbalanced. It is difficult to detect diseases using such an unbalanced dataset. We used DoubleGAN (a double generative adversarial network) to generate images of unhealthy plant leaves to balance such datasets. We proposed using DoubleGAN to generate high-resolution images of unhealthy leaves using fewer samples. DoubleGAN is divided into two stages. In stage 1, we used healthy leaves and unhealthy leaves as inputs. First, the healthy leaf images were used as inputs for the WGAN (Wasserstein generative adversarial network) to obtain the pretrained model. Then, unhealthy leaves were used for the pretrained model to generate 64*64 pixel images of unhealthy leaves. In stage 2, a superresolution generative adversarial network (SRGAN) was used to obtain corresponding 256*256 pixel images to expand the unbalanced dataset. Finally, compared with images generated by DCGAN (Deep convolution generative adversarial network). The dataset expanded with DoubleGAN, the generated images are clearer than DCGAN, and the accuracy of plant species and disease recognition reached 99.80 and 99.53 percent, respectively. The recognition results are better than those from the original dataset.",
        "authors": [
            "Yafeng Zhao",
            "Zhen Chen",
            "Xuan Gao",
            "Wenlong Song",
            "Qiang Xiong",
            "Junfeng Hu",
            "Zhichao Zhang"
        ],
        "citations": 91,
        "references": 19,
        "year": 2021
    },
    {
        "title": "DeepHawkes: Bridging the Gap between Prediction and Understanding of Information Cascades",
        "abstract": "Online social media remarkably facilitates the production and delivery of information, intensifying the competition among vast information for users' attention and highlighting the importance of predicting the popularity of information. Existing approaches for popularity prediction fall into two paradigms: feature-based approaches and generative approaches. Feature-based approaches extract various features (e.g., user, content, structural, and temporal features), and predict the future popularity of information by training a regression/classification model. Their predictive performance heavily depends on the quality of hand-crafted features. In contrast, generative approaches devote to characterizing and modeling the process that a piece of information accrues attentions, offering us high ease to understand the underlying mechanisms governing the popularity dynamics of information cascades. But they have less desirable predictive power since they are not optimized for popularity prediction. In this paper, we propose DeepHawkes to combat the defects of existing methods, leveraging end-to-end deep learning to make an analogy to interpretable factors of Hawkes process --- a widely-used generative process to model information cascade. DeepHawkes inherits the high interpretability of Hawkes process and possesses the high predictive power of deep learning methods, bridging the gap between prediction and understanding of information cascades. We verify the effectiveness of DeepHawkes by applying it to predict retweet cascades of Sina Weibo and citation cascades of a longitudinal citation dataset. Experimental results demonstrate that DeepHawkes outperforms both feature-based and generative approaches.",
        "authors": [
            "Qi Cao",
            "Huawei Shen",
            "Keting Cen",
            "W. Ouyang",
            "Xueqi Cheng"
        ],
        "citations": 226,
        "references": 38,
        "year": 2017
    },
    {
        "title": "Convolutional Prototype Network for Open Set Recognition",
        "abstract": "Despite the success of convolutional neural network (CNN) in conventional closed-set recognition (CSR), it still lacks robustness for dealing with unknowns (those out of known classes) in open environment. To improve the robustness of CNN in open-set recognition (OSR) and meanwhile maintain its high accuracy in CSR, we propose an alternative deep framework called convolutional prototype network (CPN), which keeps CNN for representation learning but replaces the closed-world assumed softmax with an open-world oriented and human-like prototype model. To equip CPN with discriminative ability for classifying known samples, we design several discriminative losses for training. Moreover, to increase the robustness of CPN for unknowns, we interpret CPN from the perspective of generative model and further propose a generative loss, which is essentially maximizing the log-likelihood of known samples and serves as a latent regularization for discriminative learning. The combination of discriminative and generative losses makes CPN a hybrid model with advantages for both CSR and OSR. Under the designed losses, the CPN is trained end-to-end for learning the convolutional network and prototypes jointly. For application of CPN in OSR, we propose two rejection rules for detecting different types of unknowns. Experiments on several datasets demonstrate the efficiency and effectiveness of CPN for both CSR and OSR tasks.",
        "authors": [
            "Hong-Ming Yang",
            "Xu-Yao Zhang",
            "Fei Yin",
            "Qing Yang",
            "Cheng-Lin Liu"
        ],
        "citations": 131,
        "references": 0,
        "year": 2020
    },
    {
        "title": "Deep Cross-Modal Audio-Visual Generation",
        "abstract": "Cross-modal audio-visual perception has been a long-lasting topic in psychology and neurology, and various studies have discovered strong correlations in human perception of auditory and visual stimuli. Despite work on computational multimodal modeling, the problem of cross-modal audio-visual generation has not been systematically studied in the literature. In this paper, we make the first attempt to solve this cross-modal generation problem leveraging the power of deep generative adversarial training. Specifically, we use conditional generative adversarial networks to achieve cross-modal audio-visual generation of musical performances. We explore different encoding methods for audio and visual signals, and work on two scenarios: instrument-oriented generation and pose-oriented generation. Being the first to explore this new problem, we compose two new datasets with pairs of images and sounds of musical performances of different instruments. Our experiments using both classification and human evaluation demonstrate that our model has the ability to generate one modality, i.e., audio/visual, from the other modality, i.e., visual/audio, to a good extent. Our experiments on various design choices along with the datasets will facilitate future research in this new problem space.",
        "authors": [
            "Lele Chen",
            "Sudhanshu Srivastava",
            "Z. Duan",
            "Chenliang Xu"
        ],
        "citations": 210,
        "references": 35,
        "year": 2017
    },
    {
        "title": "DIVA: Domain Invariant Variational Autoencoders",
        "abstract": "We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in fields like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.",
        "authors": [
            "Maximilian Ilse",
            "J. Tomczak",
            "Christos Louizos",
            "M. Welling"
        ],
        "citations": 182,
        "references": 49,
        "year": 2019
    },
    {
        "title": "Joint Pose and Expression Modeling for Facial Expression Recognition",
        "abstract": "Facial expression recognition (FER) is a challenging task due to different expressions under arbitrary poses. Most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifiers for each pose. Different from existing methods, in this paper, we propose an end-to-end deep learning model by exploiting different poses and expressions jointly for simultaneous facial image synthesis and pose-invariant facial expression recognition. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, the encoder-decoder structure of the generator can learn a generative and discriminative identity representation for face images. Second, the identity representation is explicitly disentangled from both expression and pose variations through the expression and pose codes. Third, our model can automatically generate face images with different expressions under arbitrary poses to enlarge and enrich the training set for FER. Quantitative and qualitative evaluations on both controlled and in-the-wild datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.",
        "authors": [
            "Feifei Zhang",
            "Tianzhu Zhang",
            "Qi-rong Mao",
            "Changsheng Xu"
        ],
        "citations": 194,
        "references": 66,
        "year": 2018
    },
    {
        "title": "Training of quantum circuits on a hybrid quantum computer",
        "abstract": "We train generative modeling circuits on a quantum hybrid computer showing an optimization strategy and a resource trade-off. Generative modeling is a flavor of machine learning with applications ranging from computer vision to chemical design. It is expected to be one of the techniques most suited to take advantage of the additional resources provided by near-term quantum computers. Here, we implement a data-driven quantum circuit training algorithm on the canonical Bars-and-Stripes dataset using a quantum-classical hybrid machine. The training proceeds by running parameterized circuits on a trapped ion quantum computer and feeding the results to a classical optimizer. We apply two separate strategies, Particle Swarm and Bayesian optimization to this task. We show that the convergence of the quantum circuit to the target distribution depends critically on both the quantum hardware and classical optimization strategy. Our study represents the first successful training of a high-dimensional universal quantum circuit and highlights the promise and challenges associated with hybrid learning schemes.",
        "authors": [
            "D. Zhu",
            "N. Linke",
            "Marcello Benedetti",
            "K. Landsman",
            "N. Nguyen",
            "C. H. Alderete",
            "A. Perdomo-Ortiz",
            "N. Korda",
            "A. Garfoot",
            "C. Brecque",
            "L. Egan",
            "O. Perdomo",
            "C. Monroe"
        ],
        "citations": 200,
        "references": 44,
        "year": 2018
    },
    {
        "title": "Modeling the Influence of Data Structure on Learning in Neural Networks: The Hidden Manifold Model",
        "abstract": "Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterised by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data, or assumes that elements of each data sample are drawn independently from some factorised probability distribution. These approaches are thus by construction blind to the correlation structure of real-world data sets and their impact on learning in neural networks. Here, we introduce a generative model for structured data sets that we call the hidden manifold model (HMM). The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a \"Gaussian Equivalence Property\" (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This permits us to analyse in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.",
        "authors": [
            "Sebastian Goldt",
            "Marc Mézard",
            "Florent Krzakala",
            "L. Zdeborová"
        ],
        "citations": 150,
        "references": 99,
        "year": 2019
    },
    {
        "title": "The Born supremacy: quantum advantage and training of an Ising Born machine",
        "abstract": null,
        "authors": [
            "Brian Coyle",
            "Daniel Mills",
            "V. Danos",
            "E. Kashefi"
        ],
        "citations": 146,
        "references": 123,
        "year": 2019
    },
    {
        "title": "A Simple Exponential Family Framework for Zero-Shot Learning",
        "abstract": null,
        "authors": [
            "V. Verma",
            "Piyush Rai"
        ],
        "citations": 194,
        "references": 39,
        "year": 2017
    },
    {
        "title": "Generalised free energy and active inference",
        "abstract": null,
        "authors": [
            "Thomas Parr",
            "Karl J. Friston"
        ],
        "citations": 175,
        "references": 109,
        "year": 2018
    },
    {
        "title": "Fake Faces Identification via Convolutional Neural Network",
        "abstract": "Generative Adversarial Network (GAN) is a prominent generative model that are widely used in various applications. Recent studies have indicated that it is possible to obtain fake face images with a high visual quality based on this novel model. If those fake faces are abused in image tampering, it would cause some potential moral, ethical and legal problems. In this paper, therefore, we first propose a Convolutional Neural Network (CNN) based method to identify fake face images generated by the current best method [20], and provide experimental evidences to show that the proposed method can achieve satisfactory results with an average accuracy over 99.4%. In addition, we provide comparative results evaluated on some variants of the proposed CNN architecture, including the high pass filter, the number of the layer groups and the activation function, to further verify the rationality of our method.",
        "authors": [
            "Huaxiao Mo",
            "Bolin Chen",
            "Weiqi Luo"
        ],
        "citations": 159,
        "references": 22,
        "year": 2018
    },
    {
        "title": "Structured event memory: a neuro-symbolic model of event cognition",
        "abstract": "Humans spontaneously organize a continuous experience into discrete events and use the learned structure of these events to generalize and organize memory. We introduce the Structured Event Memory (SEM) model of event cognition, which accounts for human abilities in event segmentation, memory, and generalization. SEM is derived from a probabilistic generative model of event dynamics defined over structured symbolic scenes. By embedding symbolic scene representations in a vector space and parametrizing the scene dynamics in this continuous space, SEM combines the advantages of structured and neural network approaches to high-level cognition. Using probabilistic reasoning over this generative model, SEM can infer event boundaries, learn event schemata, and use event knowledge to reconstruct past experience. We show that SEM can scale up to high-dimensional input spaces, producing human-like event segmentation for naturalistic video data, and accounts for a wide array of memory phenomena.",
        "authors": [
            "Nicholas T. Franklin",
            "K. Norman",
            "C. Ranganath",
            "Jeffrey M. Zacks",
            "S. Gershman"
        ],
        "citations": 133,
        "references": 203,
        "year": 2019
    },
    {
        "title": "3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks",
        "abstract": "The success of various applications including robotics, digital content creation, and visualization demand a structured and abstract representation of the 3D world from limited sensor data. Inspired by the nature of human perception of 3D shapes as a collection of simple parts, we explore such an abstract shape representation based on primitives. Given a single depth image of an object, we present 3DPRNN, a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives. Our generative model encodes symmetry characteristics of common man-made objects, preserves long-range structural coherence, and describes objects of varying complexity with a compact representation. We also propose a method based on Gaussian Fields to generate a large scale dataset of primitive-based shape representations to train our network. We evaluate our approach on a wide range of examples and show that it outperforms nearest-neighbor based shape retrieval methods and is on-par with voxelbased generative models while using a significantly reduced parameter space.",
        "authors": [
            "Chuhang Zou",
            "Ersin Yumer",
            "Jimei Yang",
            "Duygu Ceylan",
            "Derek Hoiem"
        ],
        "citations": 197,
        "references": 47,
        "year": 2017
    },
    {
        "title": "Learning Formation of Physically-Based Face Attributes",
        "abstract": "Based on a combined data set of 4000 high resolution facial scans, we introduce a non-linear morphable face model, capable of producing multifarious face geometry of pore-level resolution, coupled with material attributes for use in physically-based rendering. We aim to maximize the variety of the participant’s face identities, while increasing the robustness of correspondence between unique components, including middle-frequency geometry, albedo maps, specular intensity maps and high-frequency displacement details. Our deep learning based generative model learns to correlate albedo and geometry, which ensures the anatomical correctness of the generated assets. We demonstrate potential use of our generative model for novel identity generation, model fitting, interpolation, animation, high fidelity data visualization, and low-to-high resolution data domain transferring. We hope the release of this generative model will encourage further cooperation between all graphics, vision, and data focused professionals, while demonstrating the cumulative value of every individual’s complete biometric profile.",
        "authors": [
            "Ruilong Li",
            "Kalle Bladin",
            "Yajie Zhao",
            "Chinmay Chinara",
            "Owen Ingraham",
            "Pengda Xiang",
            "Xinglei Ren",
            "P. Prasad",
            "B. Kishore",
            "Jun Xing",
            "Hao Li"
        ],
        "citations": 100,
        "references": 52,
        "year": 2020
    },
    {
        "title": "Semi-Implicit Graph Variational Auto-Encoders",
        "abstract": "Semi-implicit graph variational auto-encoder (SIG-VAE) is proposed to expand the flexibility of variational graph auto-encoders (VGAE) to model graph data. SIG-VAE employs a hierarchical variational framework to enable neighboring node sharing for better generative modeling of graph dependency structure, together with a Bernoulli-Poisson link decoder. Not only does this hierarchical construction provide a more flexible generative graph model to better capture real-world graph properties, but also does SIG-VAE naturally lead to semi-implicit hierarchical variational inference that allows faithful modeling of implicit posteriors of given graph data, which may exhibit heavy tails, multiple modes, skewness, and rich dependency structures. Compared to VGAE, the derived graph latent representations by SIG-VAE are more interpretable, due to more expressive generative model and more faithful inference enabled by the flexible semi-implicit construction. Extensive experiments with a variety of graph data show that SIG-VAE significantly outperforms state-of-the-art methods on several different graph analytic tasks.",
        "authors": [
            "Arman Hasanzadeh",
            "Ehsan Hajiramezanali",
            "N. Duffield",
            "K. Narayanan",
            "Mingyuan Zhou",
            "Xiaoning Qian"
        ],
        "citations": 126,
        "references": 57,
        "year": 2019
    },
    {
        "title": "Conditional LSTM-GAN for Melody Generation from Lyrics",
        "abstract": "Melody generation from lyrics has been a challenging research issue in the field of artificial intelligence and music, which enables us to learn and discover latent relationships between interesting lyrics and accompanying melodies. Unfortunately, the limited availability of a paired lyrics–melody dataset with alignment information has hindered the research progress. To address this problem, we create a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment through leveraging different music sources where alignment relationship between syllables and music attributes is extracted. Most importantly, we propose a novel deep generative model, conditional Long Short-Term Memory (LSTM)–Generative Adversarial Network for melody generation from lyrics, which contains a deep LSTM generator and a deep LSTM discriminator both conditioned on lyrics. In particular, lyrics-conditioned melody and alignment relationship between syllables of given lyrics and notes of predicted melody are generated simultaneously. Extensive experimental results have proved the effectiveness of our proposed lyrics-to-melody generative model, where plausible and tuneful sequences can be inferred from lyrics.",
        "authors": [
            "Yi Yu",
            "Simon Canales"
        ],
        "citations": 124,
        "references": 33,
        "year": 2019
    }
]