[
    {
        "title": "Elucidating the Design Space of Diffusion-Based Generative Models",
        "abstract": "We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.",
        "authors": [
            "Tero Karras",
            "M. Aittala",
            "Timo Aila",
            "S. Laine"
        ],
        "citations": 1000,
        "references": 65,
        "year": 2022
    },
    {
        "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers",
        "abstract": "We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: learning in discrete or continuous time, the objective function, the interpolant that connects the distributions, and deterministic or stochastic sampling. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 and 512x512 benchmark using the exact same model structure, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06 and 2.62, respectively.",
        "authors": [
            "Nanye Ma",
            "Mark Goldstein",
            "M. S. Albergo",
            "Nicholas M. Boffi",
            "Eric Vanden-Eijnden",
            "Saining Xie"
        ],
        "citations": 86,
        "references": 73,
        "year": 2024
    },
    {
        "title": "Towards Universal Fake Image Detectors that Generalize Across Generative Models",
        "abstract": "With generative models proliferating at a rapid rate, there is a growing need for general purpose fake image detectors. In this work, we first show that the existing paradigm, which consists of training a deep network for real-vs-fake classification, fails to detect fake images from newer breeds of generative models when trained to detect GAN fake images. Upon analysis, we find that the resulting classifier is asymmetrically tuned to detect patterns that make an image fake. The real class becomes a ‘sink’ class holding anything that is not fake, including generated images from models not accessible during training. Building upon this discovery, we propose to perform real-vs-fake classification without learning; i.e., using a feature space not explicitly trained to distinguish real from fake images. We use nearest neighbor and linear probing as instantiations of this idea. When given access to the feature space of a large pretrained vision-language model, the very simple baseline of nearest neighbor classification has surprisingly good generalization ability in detecting fake images from a wide variety of generative models; e.g., it improves upon the SoTA [50] by +15.07 mAP and +25.90% acc when tested on unseen diffusion and autoregressive models. Our code, models, and data can be found at https://github.com/Yuheng-Li/UniversalFakeDetect",
        "authors": [
            "Utkarsh Ojha",
            "Yuheng Li",
            "Yong Jae Lee"
        ],
        "citations": 136,
        "references": 49,
        "year": 2023
    },
    {
        "title": "VBench: Comprehensive Benchmark Suite for Video Generative Models",
        "abstract": "Video generation has witnessed significant advance-ments, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal eval-uation system should provide insights to inform future de-velopments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects “video generation quality” into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has three appealing proper-ties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity in-consistency, motion smoothness, temporal flickering, and spatial relationship, etc.). The evaluation metrics with fine-grained levels reveal individual models' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models' ability across various evaluation dimensions, and various content types. We also investi-gate the gaps between video and image generation models. We will open-source VBench, including all prompts, evaluation methods, generated videos, and human preference an-notations, and also include more video generation models in VBench to drive forward the field of video generation.",
        "authors": [
            "Ziqi Huang",
            "Yinan He",
            "Jiashuo Yu",
            "Fan Zhang",
            "Chenyang Si",
            "Yuming Jiang",
            "Yuanhan Zhang",
            "Tianxing Wu",
            "Qingyang Jin",
            "Nattapol Chanpaisit",
            "Yaohui Wang",
            "Xinyuan Chen",
            "Limin Wang",
            "Dahua Lin",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "citations": 158,
        "references": 145,
        "year": 2023
    },
    {
        "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
        "abstract": "This paper presents a novel method for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 90% of the time.",
        "authors": [
            "Junlin Han",
            "Filippos Kokkinos",
            "Philip Torr"
        ],
        "citations": 27,
        "references": 73,
        "year": 2024
    },
    {
        "title": "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)",
        "abstract": "Traditional recommender systems (RS) typically use user-item rating histories as their main data source. However, deep generative models now have the capability to model and sample from complex data distributions, including user-item interactions, text, images, and videos, enabling novel recommendation tasks. This comprehensive, multidisciplinary survey connects key advancements in RS using Generative Models (Gen-RecSys), covering: interaction-driven generative models; the use of large language models (LLM) and textual data for natural language recommendation; and the integration of multimodal models for generating and processing images/videos in RS. Our work highlights necessary paradigms for evaluating the impact and harm of Gen-RecSys and identifies open challenges. This survey accompanies a tutorial presented at ACM KDD'24, with supporting materials provided at: https://encr.pw/vDhLq.",
        "authors": [
            "Yashar Deldjoo",
            "Zhankui He",
            "Julian McAuley",
            "A. Korikov",
            "Scott Sanner",
            "Arnau Ramisa",
            "Rene Vidal",
            "M. Sathiamoorthy",
            "Atoosa Kasirzadeh",
            "Silvia Milano"
        ],
        "citations": 26,
        "references": 198,
        "year": 2024
    },
    {
        "title": "Improving and generalizing flow-based generative models with minibatch optimal transport",
        "abstract": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schr\\\"odinger bridge inference.",
        "authors": [
            "Alexander Tong",
            "Nikolay Malkin",
            "G. Huguet",
            "Yanlei Zhang",
            "Jarrid Rector-Brooks",
            "Kilian Fatras",
            "Guy Wolf",
            "Y. Bengio"
        ],
        "citations": 151,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Machine Unlearning for Image-to-Image Generative Models",
        "abstract": "Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models. Our code is available at https://github.com/jpmorganchase/l2l-generator-unlearning.",
        "authors": [
            "Guihong Li",
            "Hsiang Hsu",
            "Chun-Fu Chen",
            "R. Marculescu"
        ],
        "citations": 15,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Orca: A Distributed Serving System for Transformer-Based Generative Models",
        "abstract": "Large-scale Transformer-based models trained for generation tasks (e.g., GPT-3) have recently attracted huge interest, emphasizing the need for system support for serving models in this family. Since these models generate a next token in an autoregressive manner, one has to run the model multiple times to process an inference request where each iteration of the model generates a single output token for the request. However, existing systems for inference serving do not perform well on this type of workload that has a multi-iteration characteristic, due to their inflexible scheduling mechanism that cannot change the current batch of requests being processed; requests that have finished earlier than other requests in a batch cannot return to the client, while newly arrived requests have to wait until the current batch completely finishes. In this paper, we propose iteration-level scheduling, a new scheduling mechanism that schedules execution at the granularity of iteration (instead of request) where the scheduler invokes the execution engine to run only a single iteration of the model on the batch. In addition, to apply batching and iteration-level scheduling to a Transformer model at the same time, we suggest selective batching, which applies batching only to a selected set of operations. Based on these two techniques, we have implemented a distributed serving system called ORCA, with additional designs for scalability to models with hundreds of billions of parameters. Our evaluation on a GPT-3 175B model shows that ORCA can significantly outperform NVIDIA FasterTransformer in terms of both latency and throughput: 36.9× throughput improvement at the same level of latency.",
        "authors": [
            "Gyeong-In Yu",
            "Joo Seong Jeong"
        ],
        "citations": 296,
        "references": 62,
        "year": 2022
    },
    {
        "title": "A framework for demonstrating practical quantum advantage: comparing quantum against classical generative models",
        "abstract": null,
        "authors": [
            "Mohamed Hibat-Allah",
            "M. Mauri",
            "Juan Carrasquilla",
            "A. Perdomo-Ortiz"
        ],
        "citations": 13,
        "references": 23,
        "year": 2024
    },
    {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "M. Gales"
        ],
        "citations": 308,
        "references": 47,
        "year": 2023
    },
    {
        "title": "AI as Agency Without Intelligence: on ChatGPT, Large Language Models, and Other Generative Models",
        "abstract": null,
        "authors": [
            "Luciano Floridi"
        ],
        "citations": 180,
        "references": 20,
        "year": 2023
    },
    {
        "title": "Self-Consuming Generative Models Go MAD",
        "abstract": "Seismic advances in generative AI algorithms have led to the temptation to use AI-synthesized data to train next-generation models. Repeating this process creates autophagous (“self-consuming”) loops whose properties are poorly understood. We conduct a thorough analysis using state-of-the-art generative image models of three autophagous loop families that differ in how they incorporate fixed or fresh real training data and whether previous generations' samples have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD) and show that appreciable MADness arises in just a few generations.",
        "authors": [
            "Sina Alemohammad",
            "Josue Casco-Rodriguez",
            "L. Luzi",
            "Ahmed Imtiaz Humayun",
            "H. Babaei",
            "Daniel LeJeune",
            "Ali Siahkoohi",
            "Richard Baraniuk"
        ],
        "citations": 111,
        "references": 92,
        "year": 2023
    },
    {
        "title": "Using artificial intelligence in craft education: crafting with text-to-image generative models",
        "abstract": "ABSTRACT Artificial intelligence (AI) and the automation of creative work have received little attention in craft education. This study aimed to address this gap by exploring Finnish pre-service craft teachers’ and teacher educators’ (N = 15) insights into the potential benefits and challenges of AI, particularly text-to-image generative AI. This study implemented a hands-on workshop on creative making with text-to-image generative AI in order to stimulate discourses and capture imaginaries concerning generative AI. The results revealed that making with AI inspired teachers to consider the unique nature of crafts as well as the tensions and tradeoffs of adopting generative AI in craft practices. The teachers identified concerns in data-driven design, including algorithmic bias, copyright violations and black-boxing creativity, as well as in power relationships, hybrid influencing and behaviour engineering. The article concludes with a discussion of the complicated relationships the results uncovered between creative making and generative AI.",
        "authors": [
            "Henriikka Vartiainen",
            "M. Tedre"
        ],
        "citations": 102,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Is synthetic data from generative models ready for image recognition?",
        "abstract": "Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. Though the results are astonishing to human eyes, how applicable these generated images are for recognition tasks remains under-explored. In this work, we extensively study whether and how synthetic images generated from state-of-the-art text-to-image generation models can be used for image recognition tasks, and focus on two perspectives: synthetic data for improving classification models in data-scarce settings (i.e. zero-shot and few-shot), and synthetic data for large-scale model pre-training for transfer learning. We showcase the powerfulness and shortcomings of synthetic data from existing generative models, and propose strategies for better applying synthetic data for recognition tasks. Code: https://github.com/CVMI-Lab/SyntheticData.",
        "authors": [
            "Ruifei He",
            "Shuyang Sun",
            "Xin Yu",
            "Chuhui Xue",
            "Wenqing Zhang",
            "Philip H. S. Torr",
            "Song Bai",
            "Xiaojuan Qi"
        ],
        "citations": 239,
        "references": 77,
        "year": 2022
    },
    {
        "title": "Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models",
        "abstract": "The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.",
        "authors": [
            "Alvin Heng",
            "Harold Soh"
        ],
        "citations": 69,
        "references": 36,
        "year": 2023
    },
    {
        "title": "On Provable Copyright Protection for Generative Models",
        "abstract": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
        "authors": [
            "Nikhil Vyas",
            "S. Kakade",
            "B. Barak"
        ],
        "citations": 74,
        "references": 40,
        "year": 2023
    },
    {
        "title": "DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models",
        "abstract": "With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models. DiffusionDB is publicly available at: https://poloclub.github.io/diffusiondb.",
        "authors": [
            "Zijie J. Wang",
            "Evan Montoya",
            "David Munechika",
            "Haoyang Yang",
            "Benjamin Hoover",
            "Duen Horng Chau"
        ],
        "citations": 227,
        "references": 56,
        "year": 2022
    },
    {
        "title": "PFGM++: Unlocking the Potential of Physics-Inspired Generative Models",
        "abstract": "We introduce a new family of physics-inspired generative models termed PFGM++ that unifies diffusion models and Poisson Flow Generative Models (PFGM). These models realize generative trajectories for $N$ dimensional data by embedding paths in $N{+}D$ dimensional space while still controlling the progression with a simple scalar norm of the $D$ additional variables. The new models reduce to PFGM when $D{=}1$ and to diffusion models when $D{\\to}\\infty$. The flexibility of choosing $D$ allows us to trade off robustness against rigidity as increasing $D$ results in more concentrated coupling between the data and the additional variable norms. We dispense with the biased large batch field targets used in PFGM and instead provide an unbiased perturbation-based objective similar to diffusion models. To explore different choices of $D$, we provide a direct alignment method for transferring well-tuned hyperparameters from diffusion models ($D{\\to} \\infty$) to any finite $D$ values. Our experiments show that models with finite $D$ can be superior to previous state-of-the-art diffusion models on CIFAR-10/FFHQ $64{\\times}64$ datasets, with FID scores of $1.91/2.43$ when $D{=}2048/128$. In class-conditional setting, $D{=}2048$ yields current state-of-the-art FID of $1.74$ on CIFAR-10. In addition, we demonstrate that models with smaller $D$ exhibit improved robustness against modeling errors. Code is available at https://github.com/Newbeeer/pfgmpp",
        "authors": [
            "Yilun Xu",
            "Ziming Liu",
            "Yonglong Tian",
            "Shangyuan Tong",
            "Max Tegmark",
            "T. Jaakkola"
        ],
        "citations": 53,
        "references": 37,
        "year": 2023
    },
    {
        "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
        "abstract": "Text-to-image generative models such as Stable Diffusion and DALL•E raise many ethical concerns due to the generation of harmful images such as Not-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety filters are often adopted to prevent the generation of NSFW images. In this work, we propose SneakyPrompt, the first automated attack framework, to jailbreak text-to-image generative models such that they generate NSFW images even if safety filters are adopted. Given a prompt that is blocked by a safety filter, SneakyPrompt repeatedly queries the text-to-image generative model and strategically perturbs tokens in the prompt based on the query results to bypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement learning to guide the perturbation of tokens. Our evaluation shows that SneakyPrompt successfully jailbreaks DALL•E 2 with closed-box safety filters to generate NSFW images. Moreover, we also deploy several state-of-the-art, open-source safety filters on a Stable Diffusion model. Our evaluation shows that SneakyPrompt not only successfully generates NSFW images, but also outperforms existing text adversarial attacks when extended to jailbreak text-to-image generative models, in terms of both the number of queries and qualities of the generated NSFW images. SneakyPrompt is open-source and available at this repository: https://github.com/Yuchen413/text2image_safety.",
        "authors": [
            "Yuchen Yang",
            "Bo Hui",
            "Haolin Yuan",
            "Neil Gong",
            "Yinzhi Cao"
        ],
        "citations": 44,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models",
        "abstract": "Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used. Our attack is based on two assumptions: (1) The attacker has access to a\"quality oracle\"that can evaluate whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a\"perturbation oracle\"which can modify an output with a nontrivial probability of maintaining quality, and which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully removes the watermarks planted by all three schemes, with only minor quality degradation.",
        "authors": [
            "Hanlin Zhang",
            "Benjamin L. Edelman",
            "Danilo Francati",
            "Daniele Venturi",
            "G. Ateniese",
            "Boaz Barak"
        ],
        "citations": 42,
        "references": 77,
        "year": 2023
    },
    {
        "title": "On the Stability of Iterative Retraining of Generative Models on their own Data",
        "abstract": "Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models will be trained on both clean and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets -- from classical training on real data to self-consuming generative models trained on purely synthetic data. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion of clean training data (w.r.t. synthetic data) is large enough. We empirically validate our theory on both synthetic and natural images by iteratively training normalizing flows and state-of-the-art diffusion models on CIFAR10 and FFHQ.",
        "authors": [
            "Quentin Bertrand",
            "A. Bose",
            "Alexandre Duplessis",
            "Marco Jiralerspong",
            "Gauthier Gidel"
        ],
        "citations": 31,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Generative models for protein structures and sequences",
        "abstract": null,
        "authors": [
            "Chloe Hsu",
            "Clara Fannjiang",
            "Jennifer Listgarten"
        ],
        "citations": 8,
        "references": 7,
        "year": 2024
    },
    {
        "title": "Generative Models as an Emerging Paradigm in the Chemical Sciences",
        "abstract": "Traditional computational approaches to design chemical species are limited by the need to compute properties for a vast number of candidates, e.g., by discriminative modeling. Therefore, inverse design methods aim to start from the desired property and optimize a corresponding chemical structure. From a machine learning viewpoint, the inverse design problem can be addressed through so-called generative modeling. Mathematically, discriminative models are defined by learning the probability distribution function of properties given the molecular or material structure. In contrast, a generative model seeks to exploit the joint probability of a chemical species with target characteristics. The overarching idea of generative modeling is to implement a system that produces novel compounds that are expected to have a desired set of chemical features, effectively sidestepping issues found in the forward design process. In this contribution, we overview and critically analyze popular generative algorithms like generative adversarial networks, variational autoencoders, flow, and diffusion models. We highlight key differences between each of the models, provide insights into recent success stories, and discuss outstanding challenges for realizing generative modeling discovered solutions in chemical applications.",
        "authors": [
            "Dylan M. Anstine",
            "O. Isayev"
        ],
        "citations": 107,
        "references": 128,
        "year": 2023
    },
    {
        "title": "Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data",
        "abstract": "We present Viewset Diffusion, a diffusion-based generator that outputs 3D objects while only using multi-view 2D data for supervision. We note that there exists a one-to-one mapping between viewsets, i.e., collections of several 2D views of an object, and 3D models. Hence, we train a diffusion model to generate viewsets, but design the neural network generator to reconstruct internally corresponding 3D models, thus generating those too. We fit a diffusion model to a large number of viewsets for a given category of objects. The resulting generator can be conditioned on zero, one or more input views. Conditioned on a single view, it performs 3D reconstruction accounting for the ambiguity of the task and allowing to sample multiple solutions compatible with the input. The model performs reconstruction efficiently, in a feed-forward manner, and is trained using only rendering losses using as few as three views per viewset. Project page: szymanowiczs.github.io/viewset-diffusion.",
        "authors": [
            "Stanislaw Szymanowicz",
            "C. Rupprecht",
            "A. Vedaldi"
        ],
        "citations": 80,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Generative models improve fairness of medical classifiers under distribution shifts",
        "abstract": null,
        "authors": [
            "Ira Ktena",
            "Olivia Wiles",
            "Isabela Albuquerque",
            "Sylvestre-Alvise Rebuffi",
            "Ryutaro Tanno",
            "Abhijit Guha Roy",
            "Shekoofeh Azizi",
            "D. Belgrave",
            "Pushmeet Kohli",
            "A. Karthikesalingam",
            "Ali Taylan Cemgil",
            "Sven Gowal"
        ],
        "citations": 53,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Minimizing Trajectory Curvature of ODE-based Generative Models",
        "abstract": "Recent ODE/SDE-based generative models, such as diffusion models, rectified flows, and flow matching, define a generative process as a time reversal of a fixed forward process. Even though these models show impressive performance on large-scale datasets, numerical simulation requires multiple evaluations of a neural network, leading to a slow sampling speed. We attribute the reason to the high curvature of the learned generative trajectories, as it is directly related to the truncation error of a numerical solver. Based on the relationship between the forward process and the curvature, here we present an efficient method of training the forward process to minimize the curvature of generative trajectories without any ODE/SDE simulation. Experiments show that our method achieves a lower curvature than previous models and, therefore, decreased sampling costs while maintaining competitive performance. Code is available at https://github.com/sangyun884/fast-ode.",
        "authors": [
            "Sangyun Lee",
            "Beomsu Kim",
            "Jong-Chul Ye"
        ],
        "citations": 43,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Mosaic-SDF for 3D Generative Models",
        "abstract": "Current diffusion or flow-based generative models for 3D shapes divide to two: distilling pre-trained 2D image diffusion models, and training directly on 3D shapes. When training a diffusion or flow models on 3D shapes a crucial design choice is the shape representation. An effective shape representation needs to adhere three design principles: it should allow an efficient conversion of large 3D datasets to the representation form; it should provide a good tradeoff of approximation power versus number of parameters; and it should have a simple tensorial form that is compatible with existing powerful neural architectures. While standard 3D shape representations such as volumetric grids and point clouds do not adhere to all these principles simultaneously, we advocate in this paper a new representation that does. We introduce Mosaic-SDF (M-SDF): a simple 3D shape representation that approximates the Signed Distance Function (SDF) of a given shape by using a set of local grids spread near the shape's boundary. The M-SDF representation is fast to compute for each shape individually making it readily parallelizable; it is parameter efficient as it only covers the space around the shape's boundary; and it has a simple matrix form, compatible with Transformer-based architectures. We demonstrate the efficacy of the M-SDF representation by using it to train a 3D generative flow model including class-conditioned generation with the ShapeNetCore-V2 (3D Warehouse) dataset, and text-to-3D generation using a dataset of about 600k caption-shape pairs.",
        "authors": [
            "Lior Yariv",
            "Omri Puny",
            "Natalia Neverova",
            "Oran Gafni",
            "Y. Lipman"
        ],
        "citations": 30,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models",
        "abstract": "Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to $\\ell_2$-accurate estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model), we derive a convergence rate proportional to $1/\\sqrt{T}$, matching the state-of-the-art theory. Imposing only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), our results characterize how $\\ell_2$ score estimation errors affect the quality of the data generation processes. In contrast to prior works, our theory is developed based on an elementary yet versatile non-asymptotic approach without resorting to toolboxes for SDEs and ODEs. Further, we design two accelerated variants, improving the convergence to $1/T^2$ for the ODE-based sampler and $1/T$ for the DDPM-type sampler, which might be of independent theoretical and empirical interest.",
        "authors": [
            "Gen Li",
            "Yuting Wei",
            "Yuxin Chen",
            "Yuejie Chi"
        ],
        "citations": 45,
        "references": 48,
        "year": 2023
    },
    {
        "title": "High-Fidelity Image Compression with Score-based Generative Models",
        "abstract": "Despite the tremendous success of diffusion generative models in text-to-image generation, replicating this success in the domain of image compression has proven difficult. In this paper, we demonstrate that diffusion can significantly improve perceptual quality at a given bit-rate, outperforming state-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is achieved using a simple but theoretically motivated two-stage approach combining an autoencoder targeting MSE followed by a further score-based decoder. However, as we will show, implementation details matter and the optimal design decisions can differ greatly from typical text-to-image models.",
        "authors": [
            "Emiel Hoogeboom",
            "E. Agustsson",
            "Fabian Mentzer",
            "Luca Versari",
            "G. Toderici",
            "Lucas Theis"
        ],
        "citations": 27,
        "references": 47,
        "year": 2023
    },
    {
        "title": "High-throughput Generative Inference of Large Language Models with a Single GPU",
        "abstract": "The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen",
        "authors": [
            "Ying Sheng",
            "Lianmin Zheng",
            "Binhang Yuan",
            "Zhuohan Li",
            "Max Ryabinin",
            "Daniel Y. Fu",
            "Zhiqiang Xie",
            "Beidi Chen",
            "Clark W. Barrett",
            "Joseph Gonzalez",
            "Percy Liang",
            "Christopher Ré",
            "Ion Stoica",
            "Ce Zhang"
        ],
        "citations": 277,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Doom or Deliciousness: Challenges and Opportunities for Visualization in the Age of Generative Models",
        "abstract": "Generative text‐to‐image models (as exemplified by DALL‐E, MidJourney, and Stable Diffusion) have recently made enormous technological leaps, demonstrating impressive results in many graphical domains—from logo design to digital painting to photographic composition. However, the quality of these results has led to existential crises in some fields of art, leading to questions about the role of human agency in the production of meaning in a graphical context. Such issues are central to visualization, and while these generative models have yet to be widely applied in visualization, it seems only a matter of time until their integration is manifest. Seeking to circumvent similar ponderous dilemmas, we attempt to understand the roles that generative models might play across visualization. We do so by constructing a framework that characterizes what these technologies offer at various stages of the visualization workflow, augmented and analyzed through semi‐structured interviews with 21 experts from related domains. Through this work, we map the space of opportunities and risks that might arise in this intersection, identifying doomsday prophecies and delicious low‐hanging fruits that are ripe for research.",
        "authors": [
            "V. Schetinger",
            "Sara Di Bartolomeo",
            "Mennatallah El-Assady",
            "A. Mcnutt",
            "M. Miller",
            "J. P. A. Passos",
            "J. L. Adams"
        ],
        "citations": 35,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
        "abstract": "Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",
        "authors": [
            "Luke Bailey",
            "Euan Ong",
            "Stuart Russell",
            "Scott Emmons"
        ],
        "citations": 54,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Integrating spatial and single-cell transcriptomics data using deep generative models with SpatialScope",
        "abstract": null,
        "authors": [
            "Xiaomeng Wan",
            "Jiashun Xiao",
            "S. Tam",
            "Mingxuan Cai",
            "Ryohichi Sugimura",
            "Yang Wang",
            "Xiang Wan",
            "Zhixiang Lin",
            "A. Wu",
            "Can Yang"
        ],
        "citations": 32,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Speech Enhancement and Dereverberation With Diffusion-Based Generative Models",
        "abstract": "In this work, we build upon our previous publication and use diffusion-based generative models for speech enhancement. We present a detailed overview of the diffusion process that is based on a stochastic differential equation and delve into an extensive theoretical examination of its implications. Opposed to usual conditional generation tasks, we do not start the reverse process from pure Gaussian noise but from a mixture of noisy speech and Gaussian noise. This matches our forward process which moves from clean speech to noisy speech by including a drift term. We show that this procedure enables using only 30 diffusion steps to generate high-quality clean speech estimates. By adapting the network architecture, we are able to significantly improve the speech enhancement performance, indicating that the network, rather than the formalism, was the main limitation of our original approach. In an extensive cross-dataset evaluation, we show that the improved method can compete with recent discriminative models and achieves better generalization when evaluating on a different corpus than used for training. We complement the results with an instrumental evaluation using real-world noisy recordings and a listening experiment, in which our proposed method is rated best. Examining different sampler configurations for solving the reverse process allows us to balance the performance and computational speed of the proposed method. Moreover, we show that the proposed method is also suitable for dereverberation and thus not limited to additive background noise removal.",
        "authors": [
            "Julius Richter",
            "Simon Welker",
            "Jean-Marie Lemercier",
            "Bunlong Lay",
            "Timo Gerkmann"
        ],
        "citations": 150,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Generative Multimodal Models are In-Context Learners",
        "abstract": "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.",
        "authors": [
            "Quan Sun",
            "Yufeng Cui",
            "Xiaosong Zhang",
            "Fan Zhang",
            "Qiying Yu",
            "Zhengxiong Luo",
            "Yueze Wang",
            "Yongming Rao",
            "Jingjing Liu",
            "Tiejun Huang",
            "Xinlong Wang"
        ],
        "citations": 170,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Circumventing Concept Erasure Methods For Text-to-Image Generative Models",
        "abstract": "Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to\"erase\"sensitive concepts from text-to-image models. In this work, we examine five recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve\"erased\"concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.",
        "authors": [
            "Minh Pham",
            "Kelly O. Marshall",
            "C. Hegde"
        ],
        "citations": 22,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models",
        "abstract": "Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem – given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.",
        "authors": [
            "Nan Liu",
            "Yilun Du",
            "Shuang Li",
            "J. Tenenbaum",
            "A. Torralba"
        ],
        "citations": 21,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Generative models for sound field reconstruction.",
        "abstract": "This work examines the use of generative adversarial networks for reconstructing sound fields from experimental data. It is investigated whether generative models, which learn the underlying statistics of a given signal or process, can improve the spatio-temporal reconstruction of a sound field by extending its bandwidth. The problem is significant as acoustic array processing is naturally band limited by the spatial sampling of the sound field (due to the difficulty to satisfy the Nyquist criterion in space domain at high frequencies). In this study, the reconstruction of spatial room impulse responses in a conventional room is tested based on three different generative adversarial models. The results indicate that the models can improve the reconstruction, mostly by recovering some of the sound field energy that would otherwise be lost at high frequencies. There is an encouraging outlook in the use of statistical learning models to overcome the bandwidth limitations of acoustic sensor arrays. The approach can be of interest in other areas, such as computational acoustics, to alleviate the classical computational burden at high frequencies.",
        "authors": [
            "Efren Fernandez-Grande",
            "Xenofon Karakonstantis",
            "Diego Caviedes-Nozal",
            "P. Gerstoft"
        ],
        "citations": 28,
        "references": 21,
        "year": 2023
    },
    {
        "title": "Antigen-Specific Antibody Design and Optimization with Diffusion-Based Generative Models for Protein Structures",
        "abstract": "Antibodies are immune system proteins that protect the host by binding to specific antigens such as viruses and bacteria. The binding between antibodies and antigens is mainly determined by the complementarity-determining regions (CDR) of the antibodies. In this work, we develop a deep generative model that jointly models sequences and structures of CDRs based on diffusion probabilistic models and equivariant neural networks. Our method is the first deep learning-based method that generates antibodies explicitly targeting specific antigen structures and is one of the earliest diffusion probabilistic models for protein structures. The model is a “Swiss Army Knife” capable of sequence-structure co-design, sequence design for given backbone structures, and antibody optimization. We conduct extensive experiments to evaluate the quality of both sequences and structures of designed antibodies. We find that our model could yield competitive results in binding affinity measured by biophysical energy functions and other protein design metrics.",
        "authors": [
            "Shitong Luo",
            "Yufeng Su",
            "Xingang Peng",
            "Sheng Wang",
            "Jian Peng",
            "Jianzhu Ma"
        ],
        "citations": 165,
        "references": 64,
        "year": 2022
    },
    {
        "title": "Benchmarking Generated Poses: How Rational is Structure-based Drug Design with Generative Models?",
        "abstract": "Deep generative models for structure-based drug design (SBDD), where molecule generation is conditioned on a 3D protein pocket, have received considerable interest in recent years. These methods offer the promise of higher-quality molecule generation by explicitly modelling the 3D interaction between a potential drug and a protein receptor. However, previous work has primarily focused on the quality of the generated molecules themselves, with limited evaluation of the 3D molecule \\emph{poses} that these methods produce, with most work simply discarding the generated pose and only reporting a\"corrected\"pose after redocking with traditional methods. Little is known about whether generated molecules satisfy known physical constraints for binding and the extent to which redocking alters the generated interactions. We introduce PoseCheck, an extensive analysis of multiple state-of-the-art methods and find that generated molecules have significantly more physical violations and fewer key interactions compared to baselines, calling into question the implicit assumption that providing rich 3D structure information improves molecule complementarity. We make recommendations for future research tackling identified failure modes and hope our benchmark can serve as a springboard for future SBDD generative modelling work to have a real-world impact.",
        "authors": [
            "Charles Harris",
            "Kieran Didi",
            "Arian R. Jamasb",
            "Chaitanya K. Joshi",
            "Simon V. Mathis",
            "P. Liò",
            "T. Blundell"
        ],
        "citations": 26,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Concept Algebra for (Score-Based) Text-Controlled Generative Models",
        "abstract": "This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. A key property of such models is that they can compose disparate concepts in a `disentangled' manner. This suggests these models have internal representations that encode concepts in a `disentangled' manner. Here, we focus on the idea that concepts are encoded as subspaces of some representation space. We formalize what this means, show there's a natural choice for the representation, and develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples using Stable Diffusion. Code in https://github.com/zihao12/concept-algebra-code",
        "authors": [
            "Zihao Wang",
            "Lin Gui",
            "Jeffrey Negrea",
            "Victor Veitch"
        ],
        "citations": 24,
        "references": 42,
        "year": 2023
    },
    {
        "title": "SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters",
        "abstract": "Text-to-image generative models such as Stable Diffusion and DALL · E 2 have attracted much attention since their publication due to their wide application in the real world. One challenging problem of text-to-image generative models is the generation of Not-Safe-for-Work (NSFW) content, e.g., those related to violence and adult. Therefore, a common practice is to deploy a so-called safety filter, which blocks NSFW content based on either text or image features. Prior works have studied the possible bypass of such safety filters. However, existing works are largely manual and specific to Stable Diffusion’s official safety filter. Moreover, the bypass ratio of Stable Diffusion’s safety filter is as low as 23.51% based on our evaluation. In this paper, we propose the first automated attack framework, called SneakyPrompt, to evaluate the robustness of real-world safety filters in state-of-the-art text-to-image generative models. Our key insight is to search for alternative tokens in a prompt that generates NSFW images so that the generated prompt (called an adversarial prompt) bypasses existing safety filters. Specifically, SneakyPrompt utilizes reinforcement learning (RL) to guide an agent with positive rewards on semantic similarity and bypass success. Ourevaluation shows that SneakyPrompt successfully generated NSFW content using an online model DALL · E 2 with its default, closed-box safety filter enabled. At the same time, we also deploy several open-source state-of-the-art safety filters on a Stable Diffusion model and show that SneakyPrompt not only successfully generates NSFW content, but also outperforms existing adversarial attacks in terms of the number of queries and image qualities.",
        "authors": [
            "Yuchen Yang",
            "Bo Hui",
            "Haolin Yuan",
            "N. Gong",
            "Yinzhi Cao"
        ],
        "citations": 19,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples",
        "abstract": "Deep generative models have demonstrated the ability to generate complex, high-dimensional, and photo-realistic data. However, a uniﬁed framework for evaluating different generative modeling families remains a challenge. Indeed, likelihood-based metrics do not apply in many cases while pure sample-based metrics such as FID fail to capture known failure modes such as overﬁtting on training data. In this work, we introduce the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to quantitatively measure the quality/diversity of generated samples while taking into account overﬁtting. We empirically demonstrate the ability of FLS to identify speciﬁc overﬁtting problem cases, even when previously proposed metrics fail. We further perform an extensive experimental evaluation on various image datasets and model classes. Our results indicate that FLS matches intuitions of previous metrics, such as FID, while providing a more holistic evaluation of generative models that highlights models whose generalization abilities are under or overappreciated. Code for computing FLS is provided at https://github.com/marcojira/ﬂs.",
        "authors": [
            "Marco Jiralerspong",
            "A. Bose",
            "Gauthier Gidel"
        ],
        "citations": 19,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Predictability and Surprise in Large Generative Models",
        "abstract": "Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.",
        "authors": [
            "Deep Ganguli",
            "Danny Hernandez",
            "Liane Lovitt",
            "Nova Dassarma",
            "T. Henighan",
            "Andy Jones",
            "Nicholas Joseph",
            "John Kernion",
            "Benjamin Mann",
            "Amanda Askell",
            "Yuntao Bai",
            "Anna Chen",
            "Tom Conerly",
            "Dawn Drain",
            "Nelson Elhage",
            "S. E. Showk",
            "Stanislav Fort",
            "Zac Hatfield-Dodds",
            "Scott Johnston",
            "Shauna Kravec",
            "Neel Nanda",
            "Kamal Ndousse",
            "Catherine Olsson",
            "D. Amodei",
            "Dario Amodei",
            "Tom B. Brown",
            "Jared Kaplan",
            "Sam McCandlish",
            "C. Olah",
            "Jack Clark"
        ],
        "citations": 231,
        "references": 101,
        "year": 2022
    },
    {
        "title": "Optical Generative Models",
        "abstract": "Generative models cover various application areas, including image, video and music synthesis, natural language processing, and molecular design, among many others. As digital generative models become larger, scalable inference in a fast and energy-efficient manner becomes a challenge. Here, we present optical generative models inspired by diffusion models, where a shallow and fast digital encoder first maps random noise into phase patterns that serve as optical generative seeds for a desired data distribution; a jointly-trained free-space-based reconfigurable decoder all-optically processes these generative seeds to create novel images (never seen before) following the target data distribution. Except for the illumination power and the random seed generation through a shallow encoder, these optical generative models do not consume computing power during the synthesis of novel images. We report the optical generation of monochrome and multi-color novel images of handwritten digits, fashion products, butterflies, and human faces, following the data distributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets, respectively, achieving an overall performance comparable to digital neural network-based generative models. To experimentally demonstrate optical generative models, we used visible light to generate, in a snapshot, novel images of handwritten digits and fashion products. These optical generative models might pave the way for energy-efficient, scalable and rapid inference tasks, further exploiting the potentials of optics and photonics for artificial intelligence-generated content.",
        "authors": [
            "Shiqi Chen",
            "Yuhang Li",
            "Hanlong Chen",
            "Aydogan Ozcan"
        ],
        "citations": 0,
        "references": 51,
        "year": 2024
    },
    {
        "title": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
        "abstract": "During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.",
        "authors": [
            "Roberto Gozalo-Brizuela",
            "E.C. Garrido-Merchán"
        ],
        "citations": 222,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Regulating ChatGPT and other Large Generative AI Models",
        "abstract": "Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",
        "authors": [
            "P. Hacker",
            "A. Engel",
            "M. Mauer"
        ],
        "citations": 269,
        "references": 176,
        "year": 2023
    },
    {
        "title": "Accelerated Motion Correction for MRI Using Score-Based Generative Models",
        "abstract": "Magnetic Resonance Imaging (MRI) is a powerful medical imaging modality, but unfortunately suffers from long scan times which, aside from increasing operational costs, can lead to image artifacts due to patient motion. Motion during the acquisition leads to inconsistencies in measured data that manifest as blurring and ghosting if unaccounted for in the image reconstruction process. Various deep learning based reconstruction techniques have been proposed which decrease scan time by reducing the number of measurements needed for a high fidelity reconstructed image. Additionally, deep learning has been used to correct motion using end-to-end techniques. This, however, increases susceptibility to distribution shifts at test time (sampling pattern, motion level). In this work we propose a framework for jointly reconstructing highly sub-sampled MRI data while estimating patient motion using score-based generative models. Our method does not make specific assumptions on the sampling trajectory or motion pattern at training time and thus can be flexibly applied to various types of measurement models and patient motion. We demonstrate our framework on retrospectively accelerated 2D brain MRI corrupted by rigid motion.",
        "authors": [
            "Brett Levac",
            "A. Jalal",
            "Jonathan I. Tamir"
        ],
        "citations": 22,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design",
        "abstract": "Deep generative models such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, and Transformers, have shown great promise in a variety of applications, including image and speech synthesis, natural language processing, and drug discovery. However, when applied to engineering design problems, evaluating the performance of these models can be challenging, as traditional statistical metrics based on likelihood may not fully capture the requirements of engineering applications. This paper doubles as a review and practical guide to evaluation metrics for deep generative models (DGMs) in engineering design. We first summarize the well-accepted `classic' evaluation metrics for deep generative models grounded in machine learning theory. Using case studies, we then highlight why these metrics seldom translate well to design problems but see frequent use due to the lack of established alternatives. Next, we curate a set of design-specific metrics which have been proposed across different research communities and can be used for evaluating deep generative models. These metrics focus on unique requirements in design and engineering, such as constraint satisfaction, functional performance, novelty, and conditioning. Throughout our discussion, we apply the metrics to models trained on simple-to-visualize 2-dimensional example problems. Finally, we evaluate four deep generative models on a bicycle frame design problem and structural topology generation problem. In particular, we showcase the use of proposed metrics to quantify performance target achievement, design novelty, and geometric constraints. We publicly release the code for the datasets, models, and metrics used throughout the paper at https://decode.mit.edu/projects/metrics/.",
        "authors": [
            "Lyle Regenwetter",
            "Akash Srivastava",
            "Dan Gutfreund",
            "Faez Ahmed"
        ],
        "citations": 22,
        "references": 133,
        "year": 2023
    },
    {
        "title": "The Role of ChatGPT, Generative Language Models, and Artificial Intelligence in Medical Education: A Conversation With ChatGPT and a Call for Papers",
        "abstract": "ChatGPT is a generative language model tool launched by OpenAI on November 30, 2022, enabling the public to converse with a machine on a broad range of topics. In January 2023, ChatGPT reached over 100 million users, making it the fastest-growing consumer application to date. This interview with ChatGPT is part 2 of a larger interview with ChatGPT. It provides a snapshot of the current capabilities of ChatGPT and illustrates the vast potential for medical education, research, and practice but also hints at current problems and limitations. In this conversation with Gunther Eysenbach, the founder and publisher of JMIR Publications, ChatGPT generated some ideas on how to use chatbots in medical education. It also illustrated its capabilities to generate a virtual patient simulation and quizzes for medical students; critiqued a simulated doctor-patient communication and attempts to summarize a research article (which turned out to be fabricated); commented on methods to detect machine-generated text to ensure academic integrity; generated a curriculum for health professionals to learn about artificial intelligence (AI); and helped to draft a call for papers for a new theme issue to be launched in JMIR Medical Education on ChatGPT. The conversation also highlighted the importance of proper “prompting.” Although the language generator does make occasional mistakes, it admits these when challenged. The well-known disturbing tendency of large language models to hallucinate became evident when ChatGPT fabricated references. The interview provides a glimpse into the capabilities and limitations of ChatGPT and the future of AI-supported medical education. Due to the impact of this new technology on medical education, JMIR Medical Education is launching a call for papers for a new e-collection and theme issue. The initial draft of the call for papers was entirely machine generated by ChatGPT, but will be edited by the human guest editors of the theme issue.",
        "authors": [
            "G. Eysenbach"
        ],
        "citations": 434,
        "references": 25,
        "year": 2023
    },
    {
        "title": "CatAlyst: Domain-Extensible Intervention for Preventing Task Procrastination Using Large Generative Models",
        "abstract": "CatAlyst uses generative models to help workers’ progress by influencing their task engagement instead of directly contributing to their task outputs. It prompts distracted workers to resume their tasks by generating a continuation of their work and presenting it as an intervention that is more context-aware than conventional (predetermined) feedback. The prompt can function by drawing their interest and lowering the hurdle for resumption even when the generated continuation is insufficient to substitute their work, while recent human-AI collaboration research aiming at work substitution depends on a stable high accuracy. This frees CatAlyst from domain-specific model-tuning and makes it applicable to various tasks. Our studies involving writing and slide-editing tasks demonstrated CatAlyst’s effectiveness in helping workers swiftly resume tasks with a lowered cognitive load. The results suggest a new form of human-AI collaboration where large generative models publicly available but imperfect for each individual domain can contribute to workers’ digital well-being.",
        "authors": [
            "Riku Arakawa",
            "Hiromu Yakura",
            "Masataka Goto"
        ],
        "citations": 16,
        "references": 90,
        "year": 2023
    },
    {
        "title": "On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey",
        "abstract": "Diffusion models and large language models have emerged as leading-edge generative models, revolutionizing various aspects of human life. However, the practical implementations of these models have also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trust-worthiness. Despite the abundance of literature on this subject, a comprehensive survey speciﬁcally delving into the intersection of large-scale generative models and their trustwor-thiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: 1) privacy, 2) security, as well as 3) fairness and responsibility. After constructing an extensive map outlining the trustworthiness of the models, we provide practical recommendations and identify future directions which are crucial for promoting the trustworthiness of the models and will beneﬁt society as a whole.",
        "authors": [
            "Mingyuan Fan",
            "Cen Chen",
            "Chengyu Wang",
            "Jun Huang"
        ],
        "citations": 20,
        "references": 140,
        "year": 2023
    },
    {
        "title": "The Design Space of Generative Models",
        "abstract": "Card et al.'s classic paper\"The Design Space of Input Devices\"established the value of design spaces as a tool for HCI analysis and invention. We posit that developing design spaces for emerging pre-trained, generative AI models is necessary for supporting their integration into human-centered systems and practices. We explore what it means to develop an AI model design space by proposing two design spaces relating to generative AI models: the first considers how HCI can impact generative models (i.e., interfaces for models) and the second considers how generative models can impact HCI (i.e., models as an HCI prototyping material).",
        "authors": [
            "M. Morris",
            "Carrie J. Cai",
            "J. Holbrook",
            "Chinmay Kulkarni",
            "Michael Terry"
        ],
        "citations": 22,
        "references": 14,
        "year": 2023
    },
    {
        "title": "Generative Novel View Synthesis with 3D-Aware Diffusion Models",
        "abstract": "We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method’s ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.",
        "authors": [
            "Eric Chan",
            "Koki Nagano",
            "Matthew Chan",
            "Alexander W. Bergman",
            "Jeong Joon Park",
            "A. Levy",
            "M. Aittala",
            "Shalini De Mello",
            "Tero Karras",
            "Gordon Wetzstein"
        ],
        "citations": 198,
        "references": 119,
        "year": 2023
    },
    {
        "title": "FinGPT: Large Generative Models for a Small Language",
        "abstract": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.",
        "authors": [
            "Risto Luukkonen",
            "Ville Komulainen",
            "Jouni Luoma",
            "Anni Eskelinen",
            "Jenna Kanerva",
            "Hanna-Mari Kupari",
            "Filip Ginter",
            "Veronika Laippala",
            "Niklas Muennighoff",
            "Aleksandra Piktus",
            "Thomas Wang",
            "Nouamane Tazi",
            "Teven Le Scao",
            "Thomas Wolf",
            "Osma Suominen",
            "Samuli Sairanen",
            "Mikko Merioksa",
            "Jyrki Heinonen",
            "Aija Vahtola",
            "Samuel Antao",
            "S. Pyysalo"
        ],
        "citations": 32,
        "references": 45,
        "year": 2023
    },
    {
        "title": "On Kinetic Optimal Probability Paths for Generative Models",
        "abstract": "Recent successful generative models are trained by fitting a neural network to an a-priori defined tractable probability density path taking noise to training examples. In this paper we investigate the space of Gaussian probability paths, which includes diffusion paths as an instance, and look for an optimal member in some useful sense. In particular, minimizing the Kinetic Energy (KE) of a path is known to make particles' trajectories simple, hence easier to sample, and empirically improve performance in terms of likelihood of unseen data and sample generation quality. We investigate Kinetic Optimal (KO) Gaussian paths and offer the following observations: (i) We show the KE takes a simplified form on the space of Gaussian paths, where the data is incorporated only through a single, one dimensional scalar function, called the \\emph{data separation function}. (ii) We characterize the KO solutions with a one dimensional ODE. (iii) We approximate data-dependent KO paths by approximating the data separation function and minimizing the KE. (iv) We prove that the data separation function converges to $1$ in the general case of arbitrary normalized dataset consisting of $n$ samples in $d$ dimension as $n/\\sqrt{d}\\rightarrow 0$. A consequence of this result is that the Conditional Optimal Transport (Cond-OT) path becomes \\emph{kinetic optimal} as $n/\\sqrt{d}\\rightarrow 0$. We further support this theory with empirical experiments on ImageNet.",
        "authors": [
            "Neta Shaul",
            "Ricky T. Q. Chen",
            "Maximilian Nickel",
            "Matt Le",
            "Y. Lipman"
        ],
        "citations": 19,
        "references": 24,
        "year": 2023
    },
    {
        "title": "siVAE: interpretable deep generative models for single-cell transcriptomes",
        "abstract": null,
        "authors": [
            "Yongin Choi",
            "Ruoxin Li",
            "G. Quon"
        ],
        "citations": 19,
        "references": 134,
        "year": 2023
    },
    {
        "title": "Wasserstein Convergence Guarantees for a General Class of Score-Based Generative Models",
        "abstract": "Score-based generative models (SGMs) is a recent class of deep generative models with state-of-the-art performance in many applications. In this paper, we establish convergence guarantees for a general class of SGMs in 2-Wasserstein distance, assuming accurate score estimates and smooth log-concave data distribution. We specialize our result to several concrete SGMs with specific choices of forward processes modelled by stochastic differential equations, and obtain an upper bound on the iteration complexity for each model, which demonstrates the impacts of different choices of the forward processes. We also provide a lower bound when the data distribution is Gaussian. Numerically, we experiment SGMs with different forward processes, some of which are newly proposed in this paper, for unconditional image generation on CIFAR-10. We find that the experimental results are in good agreement with our theoretical predictions on the iteration complexity, and the models with our newly proposed forward processes can outperform existing models.",
        "authors": [
            "Xuefeng Gao",
            "Hoang M. Nguyen",
            "Lingjiong Zhu"
        ],
        "citations": 12,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Generative AI and large language models in health care: pathways to implementation",
        "abstract": null,
        "authors": [
            "Marium M. Raza",
            "Kaushik P. Venkatesh",
            "J. Kvedar"
        ],
        "citations": 25,
        "references": 16,
        "year": 2024
    },
    {
        "title": "Generative emulation of weather forecast ensembles with diffusion models",
        "abstract": "Uncertainty quantification is crucial to decision-making. A prominent example is probabilistic forecasting in numerical weather prediction. The dominant approach to representing uncertainty in weather forecasting is to generate an ensemble of forecasts by running physics-based simulations under different conditions, which is a computationally costly process. We propose to amortize the computational cost by emulating these forecasts with deep generative diffusion models learned from historical data. The learned models are highly scalable with respect to high-performance computing accelerators and can sample thousands of realistic weather forecasts at low cost. When designed to emulate operational ensemble forecasts, the generated ones are similar to physics-based ensembles in statistical properties and predictive skill. When designed to correct biases present in the operational forecasting system, the generated ensembles show improved probabilistic forecast metrics. They are more reliable and forecast probabilities of extreme weather events more accurately. While we focus on weather forecasting, this methodology may enable creating large climate projection ensembles for climate risk assessment.",
        "authors": [
            "Lizao Li",
            "Rob Carver",
            "I. Lopez‐Gomez",
            "Fei Sha",
            "John Anderson"
        ],
        "citations": 25,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations",
        "abstract": "Generative language models have improved drastically, and can now produce realistic text outputs that are difficult to distinguish from human-written content. For malicious actors, these language models bring the promise of automating the creation of convincing and misleading text for use in influence operations. This report assesses how language models might change influence operations in the future, and what steps can be taken to mitigate this threat. We lay out possible changes to the actors, behaviors, and content of online influence operations, and provide a framework for stages of the language model-to-influence operations pipeline that mitigations could target (model construction, model access, content dissemination, and belief formation). While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.",
        "authors": [
            "Josh A. Goldstein",
            "Girish Sastry",
            "Micah Musser",
            "Renee DiResta",
            "M. Gentzel",
            "Katerina Sedova"
        ],
        "citations": 199,
        "references": 200,
        "year": 2023
    },
    {
        "title": "Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models",
        "abstract": "Zero-shot referring image segmentation is a challenging task because it aims to find an instance segmentation mask based on the given referring descriptions, without training on this type of paired data. Current zero-shot methods mainly focus on using pre-trained discriminative models (e.g., CLIP). However, we have observed that generative models (e.g., Stable Diffusion) have potentially understood the relationships between various visual elements and text descriptions, which are rarely investigated in this task. In this work, we introduce a novel Referring Diffusional segmentor (Ref-Diff) for this task, which leverages the fine-grained multi-modal information from generative models. We demonstrate that without a proposal generator, a generative model alone can achieve comparable performance to existing SOTA weakly-supervised models. When we combine both generative and discriminative models, our Ref-Diff outperforms these competing methods by a significant margin. This indicates that generative models are also beneficial for this task and can complement discriminative models for better referring segmentation. Our code is publicly available at https://github.com/kodenii/Ref-Diff.",
        "authors": [
            "Minheng Ni",
            "Yabo Zhang",
            "Kailai Feng",
            "Xiaoming Li",
            "Yiwen Guo",
            "W. Zuo"
        ],
        "citations": 17,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Safety and Fairness for Content Moderation in Generative Models",
        "abstract": "With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.",
        "authors": [
            "Susan Hao",
            "Piyush Kumar",
            "Sarah Laszlo",
            "Shivani Poddar",
            "Bhaktipriya Radharapu",
            "R. Shelby"
        ],
        "citations": 16,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Evaluating Generative Models for Graph-to-Text Generation",
        "abstract": "Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores. We have made the text generated by generative models publicly available.",
        "authors": [
            "Shuzhou Yuan",
            "Michael Färber"
        ],
        "citations": 15,
        "references": 37,
        "year": 2023
    },
    {
        "title": "InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
        "abstract": "The foundation models have recently shown excellent performance on a variety of downstream tasks in computer vision. However, most existing vision foundation models simply focus on image-level pretraining and adpation, which are limited for dynamic and complex video-level understanding tasks. To fill the gap, we present general video foundation models, InternVideo, by taking advantage of both generative and discriminative self-supervised video learning. Specifically, InternVideo efficiently explores masked video modeling and video-language contrastive learning as the pretraining objectives, and selectively coordinates video representations of these two complementary frameworks in a learnable manner to boost various video applications. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets from extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications. Especially, our methods can obtain 91.1% and 77.2% top-1 accuracy on the challenging Kinetics-400 and Something-Something V2 benchmarks, respectively. All of these results effectively show the generality of our InternVideo for video understanding. The code will be released at https://github.com/OpenGVLab/InternVideo .",
        "authors": [
            "Yi Wang",
            "Kunchang Li",
            "Yizhuo Li",
            "Yinan He",
            "Bingkun Huang",
            "Zhiyu Zhao",
            "Hongjie Zhang",
            "Jilan Xu",
            "Yi Liu",
            "Zun Wang",
            "Sen Xing",
            "Guo Chen",
            "Junting Pan",
            "Jiashuo Yu",
            "Yali Wang",
            "Limin Wang",
            "Yu Qiao"
        ],
        "citations": 258,
        "references": 110,
        "year": 2022
    },
    {
        "title": "Accelerated Discovery of Macrocyclic CDK2 Inhibitor QR-6401 by Generative Models and Structure-Based Drug Design.",
        "abstract": "Selective CDK2 inhibitors have the potential to provide effective therapeutics for CDK2-dependent cancers and for combating drug resistance due to high cyclin E1 (CCNE1) expression intrinsically or CCNE1 amplification induced by treatment of CDK4/6 inhibitors. Generative models that take advantage of deep learning are being increasingly integrated into early drug discovery for hit identification and lead optimization. Here we report the discovery of a highly potent and selective macrocyclic CDK2 inhibitor QR-6401 (23) accelerated by the application of generative models and structure-based drug design (SBDD). QR-6401 (23) demonstrated robust antitumor efficacy in an OVCAR3 ovarian cancer xenograft model via oral administration.",
        "authors": [
            "Yang Yu",
            "Junhong Huang",
            "H. He",
            "Jing Han",
            "Geyan Ye",
            "Tingyang Xu",
            "Xianqiang Sun",
            "Xiumei Chen",
            "Xiao Ren",
            "Chunlai Li",
            "Huijuan Li",
            "Wei Huang",
            "Yangyang Liu",
            "Xinjuan Wang",
            "Yongzhi Gao",
            "Nianhe Cheng",
            "Na Guo",
            "Xibo Chen",
            "Jianxia Feng",
            "Yu-Ran Hua",
            "Chong Liu",
            "Guoyun Zhu",
            "Zhi Xie",
            "L. Yao",
            "W. Zhong",
            "Xin-de Chen",
            "Wei Liu",
            "Hailong Li"
        ],
        "citations": 18,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Deep Generative Models in De Novo Drug Molecule Generation",
        "abstract": "The discovery of new drugs has important implications for human health. Traditional methods for drug discovery rely on experiments to optimize the structure of lead molecules, which are time-consuming and high-cost. Recently, artificial intelligence has exhibited promising and efficient performance for drug-like molecule generation. In particular, deep generative models achieve great success in de novo generation of drug-like molecules with desired properties, showing massive potential for novel drug discovery. In this study, we review the recent progress of molecule generation using deep generative models, mainly focusing on molecule representations, public databases, data processing tools, and advanced artificial intelligence based molecule generation frameworks. In particular, we present a comprehensive comparison of state-of-the-art deep generative models for molecule generation and a summary of commonly used molecular design strategies. We identify research gaps and challenges of molecule generation such as the need for better databases, missing 3D information in molecular representation, and the lack of high-precision evaluation metrics. We suggest future directions for molecular generation and drug discovery.",
        "authors": [
            "Chao Pang",
            "Jianbo Qiao",
            "Xiangxiang Zeng",
            "Quan Zou",
            "Leyi Wei"
        ],
        "citations": 16,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Generative Models for Inverse Imaging Problems: From mathematical foundations to physics-driven applications",
        "abstract": "Physics-informed generative modeling for inverse problems in computational imaging is a fast-growing field encompassing a variety of methods and applications. Here, we review a few generative modeling techniques, such as variational autoencoders (VAEs) and generative adversarial networks (GANs), as well as more recent developments in score-based generative models. Through different imaging applications, we review how the generative modeling techniques are effectively combined with the physics of the imaging problem, e.g., the measurement forward model and physical properties of the target objects, to solve the inverse problems.",
        "authors": [
            "Zhizhen Zhao",
            "J. C. Ye",
            "Y. Bresler"
        ],
        "citations": 15,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Generative Models: What do they know? Do they know things? Let's find out!",
        "abstract": "Generative models excel at mimicking real scenes, suggesting they might inherently encode important intrinsic scene properties. In this paper, we aim to explore the following key questions: (1) What intrinsic knowledge do generative models like GANs, Autoregressive models, and Diffusion models encode? (2) Can we establish a general framework to recover intrinsic representations from these models, regardless of their architecture or model type? (3) How minimal can the required learnable parameters and labeled data be to successfully recover this knowledge? (4) Is there a direct link between the quality of a generative model and the accuracy of the recovered scene intrinsics? Our findings indicate that a small Low-Rank Adaptators (LoRA) can recover intrinsic images-depth, normals, albedo and shading-across different generators (Autoregressive, GANs and Diffusion) while using the same decoder head that generates the image. As LoRA is lightweight, we introduce very few learnable parameters (as few as 0.04% of Stable Diffusion model weights for a rank of 2), and we find that as few as 250 labeled images are enough to generate intrinsic images with these LoRA modules. Finally, we also show a positive correlation between the generative model's quality and the accuracy of the recovered intrinsics through control experiments.",
        "authors": [
            "Xiaodan Du",
            "Nicholas I. Kolkin",
            "Greg Shakhnarovich",
            "Anand Bhattad"
        ],
        "citations": 17,
        "references": 71,
        "year": 2023
    },
    {
        "title": "An Overview of Deep Generative Models in Functional and Evolutionary Genomics.",
        "abstract": "Following the widespread use of deep learning for genomics, deep generative modeling is also becoming a viable methodology for the broad field. Deep generative models (DGMs) can learn the complex structure of genomic data and allow researchers to generate novel genomic instances that retain the real characteristics of the original dataset. Aside from data generation, DGMs can also be used for dimensionality reduction by mapping the data space to a latent space, as well as for prediction tasks via exploitation of this learned mapping or supervised/semisupervised DGM designs. In this review, we briefly introduce generative modeling and two currently prevailing architectures, we present conceptual applications along with notable examples in functional and evolutionary genomics, and we provide our perspective on potential challenges and future directions.Expected final online publication date for the Annual Review of Biomedical Data Science, Volume 6 is August 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",
        "authors": [
            "Burak Yelmen",
            "F. Jay"
        ],
        "citations": 16,
        "references": 119,
        "year": 2023
    },
    {
        "title": "The imperative for regulatory oversight of large language models (or generative AI) in healthcare",
        "abstract": null,
        "authors": [
            "B. Meskó",
            "E. Topol"
        ],
        "citations": 407,
        "references": 7,
        "year": 2023
    },
    {
        "title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models",
        "abstract": "In this work, we introduce a self-supervised feature representation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for specific perception tasks. We investigate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an alternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple generative models, dense prediction benchmarks, and several pre-training regimes. We empirically find that our DreamTeacher significantly outperforms existing self-supervised representation learning approaches across the board. Unsupervised ImageNet pre-training with DreamTeacher leads to significant improvements over ImageNet classification pre-training on downstream datasets, showcasing generative models, and diffusion generative models specifically, as a promising approach to representation learning on large, diverse datasets without requiring manual annotation.",
        "authors": [
            "Daiqing Li",
            "Huan Ling",
            "Amlan Kar",
            "David Acuna",
            "Seung Wook Kim",
            "Karsten Kreis",
            "A. Torralba",
            "S. Fidler"
        ],
        "citations": 14,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion Generative Models",
        "abstract": "Diffusion-based generative models have been used as powerful priors for magnetic resonance imaging (MRI) reconstruction. We present a learning method to optimize sub-sampling patterns for compressed sensing multi-coil MRI that leverages pre-trained diffusion generative models. Crucially, during training we use a single-step reconstruction based on the posterior mean estimate given by the diffusion model and the MRI measurement process. Experiments across varying anatomies, acceleration factors, and pattern types show that sampling operators learned with our method lead to competitive, and in the case of 2D patterns, improved reconstructions compared to baseline patterns. Our method requires as few as five training images to learn effective sampling patterns.",
        "authors": [
            "Sriram Ravula",
            "Brett Levac",
            "A. Jalal",
            "Jonathan I. Tamir",
            "A. Dimakis"
        ],
        "citations": 13,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models",
        "abstract": "Generative models have demonstrated revolutionary success in various visual creation tasks, but in the meantime, they have been exposed to the threat of leaking private information of their training data. Several membership inference attacks (MIAs) have been proposed to exhibit the privacy vulnerability of generative models by classifying a query image as a training dataset member or nonmember. However, these attacks suffer from major limitations, such as requiring shadow models and white-box access, and either ignoring or only focusing on the unique property of diffusion models, which block their generalization to multiple generative models. In contrast, we propose the first generalized membership inference attack against a variety of generative models such as generative adversarial networks, [variational] autoencoders, implicit functions, and the emerging diffusion models. We leverage only generated distributions from target generators and auxiliary nonmember datasets, therefore regarding target generators as black boxes and agnostic to their architectures or application scenarios. Experiments validate that all the generative models are vulnerable to our attack. For instance, our work achieves attack AUC > 0.99 against DDPM, DDIM, and FastDPM trained on CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the text-conditional generation), and LIIF achieves AUC > 0.90. As a result, we appeal to our community to be aware of such privacy leakage risks when designing and publishing generative models.1",
        "authors": [
            "Minxing Zhang",
            "Ning Yu",
            "Rui Wen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "citations": 13,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Deep Generative Models for Synthetic Data: A Survey",
        "abstract": "A growing interest in synthetic data has stimulated the development and advancement of a large variety of deep generative models for a wide range of applications. However, as this research has progressed, its streams have become more specialized and disconnected from one another. This is why models for synthesizing text data for natural language processing cannot readily be compared to models for synthesizing health records anymore. To mitigate this isolation, we propose a data-driven evaluation framework for generative models for synthetic sequential data, an important and challenging sub-category of synthetic data, based on five high-level criteria: representativeness, novelty, realism, diversity and coherence of a synthetic data-set relative to the original data-set regardless of the models’ internal structures. The criteria reflect requirements different domains impose on synthetic data and allow model users to assess the quality of synthetic data across models. In a critical review of generative models for sequential data, we examine and compare the importance of each performance criterion in numerous domains. We find that realism and coherence are more important for synthetic data natural language, speech and audio processing tasks. At the same time, novelty and representativeness are more important for healthcare and mobility data. We also find that measurement of representativeness is often accomplished using statistical metrics, realism by using human judgement, and novelty using privacy tests.",
        "authors": [
            "Peter Eigenschink",
            "Thomas Reutterer",
            "S. Vamosi",
            "Ralf Vamosi",
            "Chang Sun",
            "K. Kalcher"
        ],
        "citations": 13,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
        "abstract": "This survey reviews the progress of diffusion models in generating images from text, ~\\textit{i.e.} text-to-image diffusion models. As a self-contained work, this survey starts with a brief introduction of how diffusion models work for image synthesis, followed by the background for text-conditioned image synthesis. Based on that, we present an organized review of pioneering methods and their improvements on text-to-image generation. We further summarize applications beyond image generation, such as text-guided generation for various modalities like videos, and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.",
        "authors": [
            "Chenshuang Zhang",
            "Chaoning Zhang",
            "Mengchun Zhang",
            "In-So Kweon"
        ],
        "citations": 214,
        "references": 151,
        "year": 2023
    },
    {
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
        "abstract": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
        "authors": [
            "Dongfu Jiang",
            "Xiang Ren",
            "Bill Yuchen Lin"
        ],
        "citations": 177,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Attributing Image Generative Models using Latent Fingerprints",
        "abstract": "Generative models have enabled the creation of contents that are indistinguishable from those taken from nature. Open-source development of such models raised concerns about the risks of their misuse for malicious purposes. One potential risk mitigation strategy is to attribute generative models via fingerprinting. Current fingerprinting methods exhibit a significant tradeoff between robust attribution accuracy and generation quality while lacking design principles to improve this tradeoff. This paper investigates the use of latent semantic dimensions as fingerprints, from where we can analyze the effects of design variables, including the choice of fingerprinting dimensions, strength, and capacity, on the accuracy-quality tradeoff. Compared with previous SOTA, our method requires minimum computation and is more applicable to large-scale models. We use StyleGAN2 and the latent diffusion model to demonstrate the efficacy of our method.",
        "authors": [
            "Guangyu Nie",
            "C. Kim",
            "Yezhou Yang",
            "Yi Ren"
        ],
        "citations": 13,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Updated Primer on Generative Artificial Intelligence and Large Language Models in Medical Imaging for Medical Professionals",
        "abstract": "The emergence of Chat Generative Pre-trained Transformer (ChatGPT), a chatbot developed by OpenAI, has garnered interest in the application of generative artificial intelligence (AI) models in the medical field. This review summarizes different generative AI models and their potential applications in the field of medicine and explores the evolving landscape of Generative Adversarial Networks and diffusion models since the introduction of generative AI models. These models have made valuable contributions to the field of radiology. Furthermore, this review also explores the significance of synthetic data in addressing privacy concerns and augmenting data diversity and quality within the medical domain, in addition to emphasizing the role of inversion in the investigation of generative models and outlining an approach to replicate this process. We provide an overview of Large Language Models, such as GPTs and bidirectional encoder representations (BERTs), that focus on prominent representatives and discuss recent initiatives involving language-vision models in radiology, including innovative large language and vision assistant for biomedicine (LLaVa-Med), to illustrate their practical application. This comprehensive review offers insights into the wide-ranging applications of generative AI models in clinical research and emphasizes their transformative potential.",
        "authors": [
            "Kiduk Kim",
            "Kyungjin Cho",
            "Ryoungwoo Jang",
            "Sunggu Kyung",
            "Soyoung Lee",
            "S. Ham",
            "Edward Choi",
            "G. Hong",
            "N. Kim"
        ],
        "citations": 14,
        "references": 137,
        "year": 2024
    },
    {
        "title": "BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models",
        "abstract": "The rise in popularity of text-to-image generative artificial intelligence (AI) has attracted widespread public interest. We demonstrate that this technology can be attacked to generate content that subtly manipulates its users. We propose a Backdoor Attack on text-to-image Generative Models (BAGM), which upon triggering, infuses the generated images with manipulative details that are naturally blended in the content. Our attack is the first to target three popular text-to-image generative models across three stages of the generative process by modifying the behaviour of the embedded tokenizer, the language model or the image generative model. Based on the penetration level, BAGM takes the form of a suite of attacks that are referred to as surface, shallow and deep attacks in this article. Given the existing gap within this domain, we also contribute a comprehensive set of quantitative metrics designed specifically for assessing the effectiveness of backdoor attacks on text-to-image models. The efficacy of BAGM is established by attacking state-of-the-art generative models, using a marketing scenario as the target domain. To that end, we contribute a dataset of branded product images. Our embedded backdoors increase the bias towards the target outputs by more than five times the usual, without compromising the model robustness or the generated content utility. By exposing generative AI’s vulnerabilities, we encourage researchers to tackle these challenges and practitioners to exercise caution when using pre-trained models. Relevant code and input prompts can be found at https://github.com/JJ-Vice/BAGM, and the dataset is available at: https://ieee-dataport.org/documents/marketable-foods-mf-dataset",
        "authors": [
            "J. Vice",
            "Naveed Akhtar",
            "Richard I. Hartley",
            "A. Mian"
        ],
        "citations": 10,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Generative Model Predictive Control: Approximating MPC Law With Generative Models",
        "abstract": null,
        "authors": [
            "Xun Shen"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Generative Modeling of Seismic Data Using Score-Based Generative Models",
        "abstract": null,
        "authors": [
            "C. Meng",
            "J. Gao",
            "Y. Tian",
            "H. Chen",
            "R. Luo"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "GenPhys: From Physical Processes to Generative Models",
        "abstract": "Since diffusion models (DM) and the more recent Poisson flow generative models (PFGM) are inspired by physical processes, it is reasonable to ask: Can physical processes offer additional new generative models? We show that the answer is yes. We introduce a general family, Generative Models from Physical Processes (GenPhys), where we translate partial differential equations (PDEs) describing physical processes to generative models. We show that generative models can be constructed from s-generative PDEs (s for smooth). GenPhys subsume the two existing generative models (DM and PFGM) and even give rise to new families of generative models, e.g.,\"Yukawa Generative Models\"inspired from weak interactions. On the other hand, some physical processes by default do not belong to the GenPhys family, e.g., the wave equation and the Schr\\\"{o}dinger equation, but could be made into the GenPhys family with some modifications. Our goal with GenPhys is to explore and expand the design space of generative models.",
        "authors": [
            "Ziming Liu",
            "Di Luo",
            "Yilun Xu",
            "T. Jaakkola",
            "M. Tegmark"
        ],
        "citations": 11,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Feature Unlearning for Generative Models via Implicit Feedback",
        "abstract": "We tackle the problem of feature unlearning from a pre-trained image generative model. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a speciﬁc feature, such as hairstyle from facial images, from the pretrained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pretrained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we develop an implicit feedback mechanism where a user can select images containing the target feature. From the implicit feedback, we identify a latent representation corresponding to the target feature and then use the representation to un-learn the generative model. Our framework is generalizable for the two well-known families of generative models: GANs and VAEs. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the ﬁdelity of the original models.",
        "authors": [
            "Saemi Moon",
            "Seunghyuk Cho",
            "Dongwoo Kim"
        ],
        "citations": 11,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Securing Deep Generative Models with Universal Adversarial Signature",
        "abstract": "Recent advances in deep generative models have led to the development of methods capable of synthesizing high-quality, realistic images. These models pose threats to society due to their potential misuse. Prior research attempted to mitigate these threats by detecting generated images, but the varying traces left by different generative models make it challenging to create a universal detector capable of generalizing to new, unseen generative models. In this paper, we propose to inject a universal adversarial signature into an arbitrary pre-trained generative model, in order to make its generated contents more detectable and traceable. First, the imperceptible optimal signature for each image can be found by a signature injector through adversarial training. Subsequently, the signature can be incorporated into an arbitrary generator by fine-tuning it with the images processed by the signature injector. In this way, the detector corresponding to the signature can be reused for any fine-tuned generator for tracking the generator identity. The proposed method is validated on the FFHQ and ImageNet datasets with various state-of-the-art generative models, consistently showing a promising detection rate. Code will be made publicly available at \\url{https://github.com/zengxianyu/genwm}.",
        "authors": [
            "Yu Zeng",
            "Mo Zhou",
            "Yuan Xue",
            "Vishal M. Patel"
        ],
        "citations": 10,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems",
        "abstract": "Conditional generative models became a very powerful tool to sample from Bayesian inverse problem posteriors. It is well-known in classical Bayesian literature that posterior measures are quite robust with respect to perturbations of both the prior measure and the negative log-likelihood, which includes perturbations of the observations. However, to the best of our knowledge, the robustness of conditional generative models with respect to perturbations of the observations has not been investigated yet. In this paper, we prove for the first time that appropriately learned conditional generative models provide robust results for single observations.",
        "authors": [
            "Fabian Altekrüger",
            "Paul Hagemann",
            "G. Steidl"
        ],
        "citations": 9,
        "references": 60,
        "year": 2023
    },
    {
        "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
        "abstract": "Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.",
        "authors": [
            "Zhenyu (Allen) Zhang",
            "Ying Sheng",
            "Tianyi Zhou",
            "Tianlong Chen",
            "Lianmin Zheng",
            "Ruisi Cai",
            "Zhao Song",
            "Yuandong Tian",
            "Christopher Ré",
            "Clark W. Barrett",
            "Zhangyang Wang",
            "Beidi Chen"
        ],
        "citations": 140,
        "references": 154,
        "year": 2023
    },
    {
        "title": "Resolving Ambiguities in Text-to-Image Generative Models",
        "abstract": "Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate the Text-to-image Ambiguity Benchmark (TAB) dataset to study different types of ambiguities in text-to-image generative models. We then propose the Text-to-ImagE Disambiguation (TIED) framework to disambiguate the prompts given to the text-to-image generative models by soliciting clarifications from the end user. Through automatic and human evaluations, we show the effectiveness of our framework in generating more faithful images aligned with end user intention in the presence of ambiguities.",
        "authors": [
            "Ninareh Mehrabi",
            "Palash Goyal",
            "Apurv Verma",
            "J. Dhamala",
            "Varun Kumar",
            "Qian Hu",
            "Kai-Wei Chang",
            "R. Zemel",
            "A. Galstyan",
            "Rahul Gupta"
        ],
        "citations": 8,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models",
        "abstract": "There has been considerable recent progress in designing new proteins using deep learning methods1–9. Despite this progress, a general deep learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modeling but limited success when applied to protein modeling, likely due to the complexity of protein backbone geometry and sequence-structure relationships. Here we show that by fine tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold Diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of new designs. In a manner analogous to networks which produce images from user-specified inputs, RFdiffusion enables the design of diverse, complex, functional proteins from simple molecular specifications.",
        "authors": [
            "Joseph L. Watson",
            "David Juergens",
            "N. Bennett",
            "Brian L. Trippe",
            "Jason Yim",
            "Helen E. Eisenach",
            "Woody Ahern",
            "Andrew J. Borst",
            "R. Ragotte",
            "L. Milles",
            "B. Wicky",
            "Nikita Hanikel",
            "S. Pellock",
            "A. Courbet",
            "W. Sheffler",
            "Jue Wang",
            "Preetham Venkatesh",
            "Isaac Sappington",
            "Susana Vázquez Torres",
            "A. Lauko",
            "Valentin De Bortoli",
            "Emile Mathieu",
            "R. Barzilay",
            "T. Jaakkola",
            "F. DiMaio",
            "M. Baek",
            "D. Baker"
        ],
        "citations": 161,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Generative models for molecular discovery: Recent advances and challenges",
        "abstract": "Development of new products often relies on the discovery of novel molecules. While conventional molecular design involves using human expertise to propose, synthesize, and test new molecules, this process can be cost and time intensive, limiting the number of molecules that can be reasonably tested. Generative modeling provides an alternative approach to molecular discovery by reformulating molecular design as an inverse design problem. Here, we review the recent advances in the state‐of‐the‐art of generative molecular design and discusses the considerations for integrating these models into real molecular discovery campaigns. We first review the model design choices required to develop and train a generative model including common 1D, 2D, and 3D representations of molecules and typical generative modeling neural network architectures. We then describe different problem statements for molecular discovery applications and explore the benchmarks used to evaluate models based on those problem statements. Finally, we discuss the important factors that play a role in integrating generative models into experimental workflows. Our aim is that this review will equip the reader with the information and context necessary to utilize generative modeling within their domain.",
        "authors": [
            "Camille L. Bilodeau",
            "Wengong Jin",
            "T. Jaakkola",
            "R. Barzilay",
            "K. Jensen"
        ],
        "citations": 164,
        "references": 120,
        "year": 2022
    },
    {
        "title": "Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain",
        "abstract": "Score-based generative models (SGMs) have recently shown impressive results for difficult generative tasks such as the unconditional and conditional generation of natural images and audio signals. In this work, we extend these models to the complex short-time Fourier transform (STFT) domain, proposing a novel training task for speech enhancement using a complex-valued deep neural network. We derive this training task within the formalism of stochastic differential equations (SDEs), thereby enabling the use of predictor-corrector samplers. We provide alternative formulations inspired by previous publications on using generative diffusion models for speech enhancement, avoiding the need for any prior assumptions on the noise distribution and making the training task purely generative which, as we show, results in improved enhancement performance.",
        "authors": [
            "Simon Welker",
            "Julius Richter",
            "Timo Gerkmann"
        ],
        "citations": 87,
        "references": 47,
        "year": 2022
    },
    {
        "title": "How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?",
        "abstract": "Text-to-image generative models have achieved unprecedented success in generating high-quality images based on natural language descriptions. However, it is shown that these models tend to favor specific social groups when prompted with neutral text descriptions (e.g., ‘a photo of a lawyer’). Following Zhao et al. (2021), we study the effect on the diversity of the generated images when adding ethical intervention that supports equitable judgment (e.g., ‘if all individuals can be a lawyer irrespective of their gender’) in the input prompts. To this end, we introduce an Ethical NaTural Language Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset to evaluate the change in image generations conditional on ethical interventions across three social axes – gender, skin color, and culture. Through CLIP-based and human evaluation on minDALL.E, DALL.E-mini and Stable Diffusion, we find that the model generations cover diverse social groups while preserving the image quality. In some cases, the generations would be anti-stereotypical (e.g., models tend to create images with individuals that are perceived as man when fed with prompts about makeup) in the presence of ethical intervention. Preliminary studies indicate that a large change in the model predictions is triggered by certain phrases such as ‘irrespective of gender’ in the context of gender bias in the ethical interventions. We release code and annotated data at https://github.com/Hritikbansal/entigen_emnlp.",
        "authors": [
            "Hritik Bansal",
            "Da Yin",
            "Masoud Monajatipoor",
            "Kai-Wei Chang"
        ],
        "citations": 84,
        "references": 25,
        "year": 2022
    },
    {
        "title": "How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models",
        "abstract": "Deep generative models have the potential to fundamentally change the way we create high-fidelity digital content but are often hard to control. Prompting a generative model is a promising recent development that in principle enables end-users to creatively leverage zero-shot and few-shot learning to assign new tasks to an AI ad-hoc, simply by writing them down. However, for the majority of end-users writing effective prompts is currently largely a trial and error process. To address this, we discuss the key opportunities and challenges for interactive creative applications that use prompting as a new paradigm for Human-AI interaction. Based on our analysis, we propose four design goals for user interfaces that support prompting. We illustrate these with concrete UI design sketches, focusing on the use case of creative writing. The research community in HCI and AI can take these as starting points to develop adequate user interfaces for models capable of zero- and few-shot learning.",
        "authors": [
            "Hai Dang",
            "Lukas Mecke",
            "Florian Lehmann",
            "Sven Goller",
            "Daniel Buschek"
        ],
        "citations": 79,
        "references": 24,
        "year": 2022
    },
    {
        "title": "A survey on deep learning applied to medical images: from simple artificial neural networks to generative models",
        "abstract": null,
        "authors": [
            "P. Celard",
            "E. L. Iglesias",
            "J. M. Sorribes-Fdez",
            "R. Romero",
            "A. S. Vieira",
            "María Lourdes Borrajo Diz"
        ],
        "citations": 85,
        "references": 194,
        "year": 2022
    },
    {
        "title": "Let us Build Bridges: Understanding and Extending Diffusion Generative Models",
        "abstract": "Diffusion-based generative models have achieved promising results recently, but raise an array of open questions in terms of conceptual understanding, theoretical analysis, algorithm improvement and extensions to discrete, structured, non-Euclidean domains. This work tries to re-exam the overall framework, in order to gain better theoretical understandings and develop algorithmic extensions for data from arbitrary domains. By viewing diffusion models as latent variable models with unobserved diffusion trajectories and applying maximum likelihood estimation (MLE) with latent trajectories imputed from an auxiliary distribution, we show that both the model construction and the imputation of latent trajectories amount to constructing diffusion bridge processes that achieve deterministic values and constraints at end point, for which we provide a systematic study and a suit of tools. Leveraging our framework, we present 1) a first theoretical error analysis for learning diffusion generation models, and 2) a simple and unified approach to learning on data from different discrete and constrained domains. Experiments show that our methods perform superbly on generating images, semantic segments and 3D point clouds.",
        "authors": [
            "Xingchao Liu",
            "Lemeng Wu",
            "Mao Ye",
            "Qiang Liu"
        ],
        "citations": 69,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Score-based Generative Models for Calorimeter Shower Simulation",
        "abstract": "Score-based generative models are a new class of generative algorithms that have been shown to produce realistic images even in high dimensional spaces, currently surpassing other state-of-the-art models for different benchmark categories and applications. In this work we introduce CaloScore, a score-based generative model for collider physics applied to calorimeter shower generation. Three different diffusion models are investigated using the Fast Calorimeter Simulation Challenge 2022 dataset. CaloScore is the first application of a score-based generative model in collider physics and is able to produce high-fidelity calorimeter images for all datasets, providing an alternative paradigm for calorimeter shower simulation.",
        "authors": [
            "V. Mikuni",
            "B. Nachman"
        ],
        "citations": 71,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Score-Based Generative Models Detect Manifolds",
        "abstract": "Score-based generative models (SGMs) need to approximate the scores $\\nabla \\log p_t$ of the intermediate distributions as well as the final distribution $p_T$ of the forward process. The theoretical underpinnings of the effects of these approximations are still lacking. We find precise conditions under which SGMs are able to produce samples from an underlying (low-dimensional) data manifold $\\mathcal{M}$. This assures us that SGMs are able to generate the\"right kind of samples\". For example, taking $\\mathcal{M}$ to be the subset of images of faces, we find conditions under which the SGM robustly produces an image of a face, even though the relative frequencies of these images might not accurately represent the true data generating distribution. Moreover, this analysis is a first step towards understanding the generalization properties of SGMs: Taking $\\mathcal{M}$ to be the set of all training samples, our results provide a precise description of when the SGM memorizes its training data.",
        "authors": [
            "Jakiw Pidstrigach"
        ],
        "citations": 57,
        "references": 65,
        "year": 2022
    },
    {
        "title": "A survey of multimodal deep generative models",
        "abstract": "Multimodal learning is a framework for building models that make predictions based on different types of modalities. Important challenges in multimodal learning are the inference of shared representations from arbitrary modalities and cross-modal generation via these representations; however, achieving this requires taking the heterogeneous nature of multimodal data into account. In recent years, deep generative models, i.e. generative models in which distributions are parameterized by deep neural networks, have attracted much attention, especially variational autoencoders, which are suitable for accomplishing the above challenges because they can consider heterogeneity and infer good representations of data. Therefore, various multimodal generative models based on variational autoencoders, called multimodal deep generative models, have been proposed in recent years. In this paper, we provide a categorized survey of studies on multimodal deep generative models. GRAPHICAL ABSTRACT",
        "authors": [
            "Masahiro Suzuki",
            "Y. Matsuo"
        ],
        "citations": 62,
        "references": 138,
        "year": 2022
    },
    {
        "title": "The Power of Generative AI: A Review of Requirements, Models, Input-Output Formats, Evaluation Metrics, and Challenges",
        "abstract": "Generative artificial intelligence (AI) has emerged as a powerful technology with numerous applications in various domains. There is a need to identify the requirements and evaluation metrics for generative AI models designed for specific tasks. The purpose of the research aims to investigate the fundamental aspects of generative AI systems, including their requirements, models, input–output formats, and evaluation metrics. The study addresses key research questions and presents comprehensive insights to guide researchers, developers, and practitioners in the field. Firstly, the requirements necessary for implementing generative AI systems are examined and categorized into three distinct categories: hardware, software, and user experience. Furthermore, the study explores the different types of generative AI models described in the literature by presenting a taxonomy based on architectural characteristics, such as variational autoencoders (VAEs), generative adversarial networks (GANs), diffusion models, transformers, language models, normalizing flow models, and hybrid models. A comprehensive classification of input and output formats used in generative AI systems is also provided. Moreover, the research proposes a classification system based on output types and discusses commonly used evaluation metrics in generative AI. The findings contribute to advancements in the field, enabling researchers, developers, and practitioners to effectively implement and evaluate generative AI models for various applications. The significance of the research lies in understanding that generative AI system requirements are crucial for effective planning, design, and optimal performance. A taxonomy of models aids in selecting suitable options and driving advancements. Classifying input–output formats enables leveraging diverse formats for customized systems, while evaluation metrics establish standardized methods to assess model quality and performance.",
        "authors": [
            "A. Bandi",
            "Pydi Venkata Satya Ramesh Adapa",
            "Yudu Eswar Vinay Pratap Kumar Kuchi"
        ],
        "citations": 142,
        "references": 53,
        "year": 2023
    },
    {
        "title": "The Power of Generative AI: A Review of Requirements, Models, Input-Output Formats, Evaluation Metrics, and Challenges",
        "abstract": "Generative artificial intelligence (AI) has emerged as a powerful technology with numerous applications in various domains. There is a need to identify the requirements and evaluation metrics for generative AI models designed for specific tasks. The purpose of the research aims to investigate the fundamental aspects of generative AI systems, including their requirements, models, input–output formats, and evaluation metrics. The study addresses key research questions and presents comprehensive insights to guide researchers, developers, and practitioners in the field. Firstly, the requirements necessary for implementing generative AI systems are examined and categorized into three distinct categories: hardware, software, and user experience. Furthermore, the study explores the different types of generative AI models described in the literature by presenting a taxonomy based on architectural characteristics, such as variational autoencoders (VAEs), generative adversarial networks (GANs), diffusion models, transformers, language models, normalizing flow models, and hybrid models. A comprehensive classification of input and output formats used in generative AI systems is also provided. Moreover, the research proposes a classification system based on output types and discusses commonly used evaluation metrics in generative AI. The findings contribute to advancements in the field, enabling researchers, developers, and practitioners to effectively implement and evaluate generative AI models for various applications. The significance of the research lies in understanding that generative AI system requirements are crucial for effective planning, design, and optimal performance. A taxonomy of models aids in selecting suitable options and driving advancements. Classifying input–output formats enables leveraging diverse formats for customized systems, while evaluation metrics establish standardized methods to assess model quality and performance.",
        "authors": [
            "A. Bandi",
            "Pydi Venkata Satya Ramesh Adapa",
            "Yudu Eswar Vinay Pratap Kumar Kuchi"
        ],
        "citations": 142,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Poisson Flow Generative Models",
        "abstract": "We propose a new\"Poisson flow\"generative model (PFGM) that maps a uniform distribution on a high-dimensional hemisphere into any data distribution. We interpret the data points as electrical charges on the $z=0$ hyperplane in a space augmented with an additional dimension $z$, generating a high-dimensional electric field (the gradient of the solution to Poisson equation). We prove that if these charges flow upward along electric field lines, their initial distribution in the $z=0$ plane transforms into a distribution on the hemisphere of radius $r$ that becomes uniform in the $r \\to\\infty$ limit. To learn the bijective transformation, we estimate the normalized field in the augmented space. For sampling, we devise a backward ODE that is anchored by the physically meaningful additional dimension: the samples hit the unaugmented data manifold when the $z$ reaches zero. Experimentally, PFGM achieves current state-of-the-art performance among the normalizing flow models on CIFAR-10, with an Inception score of $9.68$ and a FID score of $2.35$. It also performs on par with the state-of-the-art SDE approaches while offering $10\\times $ to $20 \\times$ acceleration on image generation tasks. Additionally, PFGM appears more tolerant of estimation errors on a weaker network architecture and robust to the step size in the Euler method. The code is available at https://github.com/Newbeeer/poisson_flow .",
        "authors": [
            "Yilun Xu",
            "Ziming Liu",
            "M. Tegmark",
            "T. Jaakkola"
        ],
        "citations": 69,
        "references": 43,
        "year": 2022
    },
    {
        "title": "MIMO Channel Estimation Using Score-Based Generative Models",
        "abstract": "Channel estimation is a critical task in multiple-input multiple-output (MIMO) digital communications that substantially affects end-to-end system performance. In this work, we introduce a novel approach for channel estimation using deep score-based generative models. A model is trained to estimate the gradient of the logarithm of a distribution and is used to iteratively refine estimates given measurements of a signal. We introduce a framework for training score-based generative models for wireless MIMO channels and performing channel estimation based on posterior sampling at test time. We derive theoretical robustness guarantees for channel estimation with posterior sampling in single-input single-output scenarios, and experimentally verify performance in the MIMO setting. Our results in simulated channels show competitive in-distribution performance, and robust out-of-distribution performance, with gains of up to 5 dB in end-to-end coded communication performance compared to supervised deep learning methods. Simulations on the number of pilots show that high fidelity channel estimation with 25% pilot density is possible for MIMO channel sizes of up to $64 \\times 256$ . Complexity analysis reveals that model size can efficiently trade performance for estimation latency, and that the proposed approach is competitive with compressed sensing in terms of floating-point operation (FLOP) count.",
        "authors": [
            "Marius Arvinte",
            "Jonathan I. Tamir"
        ],
        "citations": 41,
        "references": 70,
        "year": 2022
    },
    {
        "title": "3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models",
        "abstract": "We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation, and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation. Code: https://1zb.github.io/3DShape2VecSet/.",
        "authors": [
            "Biao Zhang",
            "Jiapeng Tang",
            "M. Nießner",
            "Peter Wonka"
        ],
        "citations": 130,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Deep Generative Models for Materials Discovery and Machine Learning-Accelerated Innovation",
        "abstract": "Machine learning and artificial intelligence (AI/ML) methods are beginning to have significant impact in chemistry and condensed matter physics. For example, deep learning methods have demonstrated new capabilities for high-throughput virtual screening, and global optimization approaches for inverse design of materials. Recently, a relatively new branch of AI/ML, deep generative models (GMs), provide additional promise as they encode material structure and/or properties into a latent space, and through exploration and manipulation of the latent space can generate new materials. These approaches learn representations of a material structure and its corresponding chemistry or physics to accelerate materials discovery, which differs from traditional AI/ML methods that use statistical and combinatorial screening of existing materials via distinct structure-property relationships. However, application of GMs to inorganic materials has been notably harder than organic molecules because inorganic structure is often more complex to encode. In this work we review recent innovations that have enabled GMs to accelerate inorganic materials discovery. We focus on different representations of material structure, their impact on inverse design strategies using variational autoencoders or generative adversarial networks, and highlight the potential of these approaches for discovering materials with targeted properties needed for technological innovation.",
        "authors": [
            "Addis S. Fuhr",
            "B. Sumpter"
        ],
        "citations": 48,
        "references": 84,
        "year": 2022
    },
    {
        "title": "On Evaluation Metrics for Graph Generative Models",
        "abstract": "In image generation, generative models can be evaluated naturally by visually inspecting model outputs. However, this is not always the case for graph generative models (GGMs), making their evaluation challenging. Currently, the standard process for evaluating GGMs suffers from three critical limitations: i) it does not produce a single score which makes model selection challenging, ii) in many cases it fails to consider underlying edge and node features, and iii) it is prohibitively slow to perform. In this work, we mitigate these issues by searching for scalar, domain-agnostic, and scalable metrics for evaluating and ranking GGMs. To this end, we study existing GGM metrics and neural-network-based metrics emerging from generative models of images that use embeddings extracted from a task-specific network. Motivated by the power of certain Graph Neural Networks (GNNs) to extract meaningful graph representations without any training, we introduce several metrics based on the features extracted by an untrained random GNN. We design experiments to thoroughly test metrics on their ability to measure the diversity and fidelity of generated graphs, as well as their sample and computational efficiency. Depending on the quantity of samples, we recommend one of two random-GNN-based metrics that we show to be more expressive than pre-existing metrics. While we focus on applying these metrics to GGM evaluation, in practice this enables the ability to easily compute the dissimilarity between any two sets of graphs regardless of domain. Our code is released at: https://github.com/uoguelph-mlrg/GGM-metrics.",
        "authors": [
            "Rylee Thompson",
            "Boris Knyazev",
            "Elahe Ghalebi",
            "Jungtaek Kim",
            "Graham W. Taylor"
        ],
        "citations": 39,
        "references": 55,
        "year": 2022
    },
    {
        "title": "A Study on the Evaluation of Generative Models",
        "abstract": "Implicit generative models, which do not return likelihood values, such as generative adversarial networks and diffusion models, have become prevalent in recent years. While it is true that these models have shown remarkable results, evaluating their performance is challenging. This issue is of vital importance to push research forward and identify meaningful gains from random noise. Currently, heuristic metrics such as the Inception score (IS) and Frechet Inception Distance (FID) are the most common evaluation metrics, but what they measure is not entirely clear. Additionally, there are questions regarding how meaningful their score actually is. In this work, we study the evaluation metrics of generative models by generating a high-quality synthetic dataset on which we can estimate classical metrics for comparison. Our study shows that while FID and IS do correlate to several f-divergences, their ranking of close models can vary considerably making them problematic when used for fain-grained comparison. We further used this experimental setting to study which evaluation metric best correlates with our probabilistic metrics. Lastly, we look into the base features used for metrics such as FID.",
        "authors": [
            "Eyal Betzalel",
            "Coby Penso",
            "Aviv Navon",
            "Ethan Fetaya"
        ],
        "citations": 42,
        "references": 32,
        "year": 2022
    },
    {
        "title": "Increasing the accuracy and resolution of precipitation forecasts using deep generative models",
        "abstract": "Accurately forecasting extreme rainfall is notoriously difficult, but is also ever more crucial for society as climate change increases the frequency of such extremes. Global numerical weather prediction models often fail to capture extremes, and are produced at too low a resolution to be actionable, while regional, high-resolution models are hugely expensive both in computation and labour. In this paper we explore the use of deep generative models to simultaneously correct and downscale (super-resolve) global ensemble forecasts over the Continental US. Specifically, using fine-grained radar observations as our ground truth, we train a conditional Generative Adversarial Network -- coined CorrectorGAN -- via a custom training procedure and augmented loss function, to produce ensembles of high-resolution, bias-corrected forecasts based on coarse, global precipitation forecasts in addition to other relevant meteorological fields. Our model outperforms an interpolation baseline, as well as super-resolution-only and CNN-based univariate methods, and approaches the performance of an operational regional high-resolution model across an array of established probabilistic metrics. Crucially, CorrectorGAN, once trained, produces predictions in seconds on a single machine. These results raise exciting questions about the necessity of regional models, and whether data-driven downscaling and correction methods can be transferred to data-poor regions that so far have had no access to high-resolution forecasts.",
        "authors": [
            "Ilan Price",
            "S. Rasp"
        ],
        "citations": 44,
        "references": 28,
        "year": 2022
    },
    {
        "title": "The Advent of Generative Language Models in Medical Education",
        "abstract": "Artificial intelligence (AI) and generative language models (GLMs) present significant opportunities for enhancing medical education, including the provision of realistic simulations, digital patients, personalized feedback, evaluation methods, and the elimination of language barriers. These advanced technologies can facilitate immersive learning environments and enhance medical students' educational outcomes. However, ensuring content quality, addressing biases, and managing ethical and legal concerns present obstacles. To mitigate these challenges, it is necessary to evaluate the accuracy and relevance of AI-generated content, address potential biases, and develop guidelines and policies governing the use of AI-generated content in medical education. Collaboration among educators, researchers, and practitioners is essential for developing best practices, guidelines, and transparent AI models that encourage the ethical and responsible use of GLMs and AI in medical education. By sharing information about the data used for training, obstacles encountered, and evaluation methods, developers can increase their credibility and trustworthiness within the medical community. In order to realize the full potential of AI and GLMs in medical education while mitigating potential risks and obstacles, ongoing research and interdisciplinary collaboration are necessary. By collaborating, medical professionals can ensure that these technologies are effectively and responsibly integrated, contributing to enhanced learning experiences and patient care.",
        "authors": [
            "Mert Karabacak",
            "B. Ozkara",
            "Konstantinos Margetis",
            "M. Wintermark",
            "S. Bisdas"
        ],
        "citations": 92,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration",
        "abstract": "Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.",
        "authors": [
            "Ping Yu",
            "Hua Xu",
            "Xia Hu",
            "Chao Deng"
        ],
        "citations": 97,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Large Language Models for Generative Information Extraction: A Survey",
        "abstract": "Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).",
        "authors": [
            "Derong Xu",
            "Wei Chen",
            "Wenjun Peng",
            "Chao Zhang",
            "Tong Xu",
            "Xiangyu Zhao",
            "Xian Wu",
            "Yefeng Zheng",
            "Enhong Chen"
        ],
        "citations": 92,
        "references": 298,
        "year": 2023
    },
    {
        "title": "Deep generative models for peptide design",
        "abstract": "Computers can already be programmed for superhuman pattern recognition of images and text. For machines to discover novel molecules, they must first be trained to sort through the many characteristics of molecules and determine which properties should be retained, suppressed, or enhanced to optimize functions of interest. Machines need to be able to understand, read, write, and eventually create new molecules. Today, this creative process relies on deep generative models, which have gained popularity since powerful deep neural networks were introduced to generative model frameworks. In recent years, they have demonstrated excellent ability to model complex distribution of real-word data (e.g., images, audio, text, molecules, and biological sequences). Deep generative models can generate data beyond those provided in training samples, thus yielding an efficient and rapid tool for exploring the massive search space of high-dimensional data such as DNA/protein sequences and facilitating the design of biomolecules with desired functions. Here, we review the emerging field of deep generative models applied to peptide science. In particular, we discuss several popular deep generative model frameworks as well as their applications to generate peptides with various kinds of properties (e.g., antimicrobial, anticancer, cell penetration, etc). We conclude our review with a discussion of current limitations and future perspectives in this emerging field.",
        "authors": [
            "Fangping Wan",
            "Daphne Kontogiorgos-Heintz",
            "César de la Fuente-Nunez"
        ],
        "citations": 44,
        "references": 62,
        "year": 2022
    },
    {
        "title": "Will Large-scale Generative Models Corrupt Future Datasets?",
        "abstract": "Recently proposed large-scale text-to-image generative models such as DALL•E 2 [47], Midjourney [42], and StableDiffusion [51] can generate high-quality and realistic images from users’ prompts. Not limited to the research community, ordinary Internet users enjoy these generative models, and consequently, a tremendous amount of generated images have been shared on the Internet. Meanwhile, today’s success of deep learning in the computer vision field owes a lot to images collected from the Internet. These trends lead us to a research question: \"will such generated images impact the quality of future datasets and the performance of computer vision models positively or negatively?\" This paper empirically answers this question by simulating contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using a state-of-the-art generative model and evaluate models trained with \"contaminated\" datasets on various tasks, including image classification and image generation. Throughout experiments, we conclude that generated images negatively affect downstream performance, while the significance depends on tasks and the amount of generated images. The generated datasets and the codes for experiments will be publicly released for future research. Generated datasets and source codes are available from https://github.com/moskomule/dataset-contamination.",
        "authors": [
            "Ryuichiro Hataya",
            "Han Bao",
            "Hiromi Arai"
        ],
        "citations": 40,
        "references": 70,
        "year": 2022
    },
    {
        "title": "How Much Is Enough? A Study on Diffusion Times in Score-Based Generative Models",
        "abstract": "Score-based diffusion models are a class of generative models whose dynamics is described by stochastic differential equations that map noise into data. While recent works have started to lay down a theoretical foundation for these models, a detailed understanding of the role of the diffusion time T is still lacking. Current best practice advocates for a large T to ensure that the forward dynamics brings the diffusion sufficiently close to a known and simple noise distribution; however, a smaller value of T should be preferred for a better approximation of the score-matching objective and higher computational efficiency. Starting from a variational interpretation of diffusion models, in this work we quantify this trade-off and suggest a new method to improve quality and efficiency of both training and sampling, by adopting smaller diffusion times. Indeed, we show how an auxiliary model can be used to bridge the gap between the ideal and the simulated forward dynamics, followed by a standard reverse diffusion process. Empirical results support our analysis; for image data, our method is competitive with regard to the state of the art, according to standard sample quality metrics and log-likelihood.",
        "authors": [
            "Giulio Franzese",
            "Simone Rossi",
            "Lixuan Yang",
            "A. Finamore",
            "Dario Rossi",
            "M. Filippone",
            "Pietro Michiardi"
        ],
        "citations": 41,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Learning to Learn with Generative Models of Neural Network Checkpoints",
        "abstract": "We explore a data-driven approach for learning to optimize neural networks. We construct a dataset of neural network checkpoints and train a generative model on the parameters. In particular, our model is a conditional diffusion transformer that, given an initial input parameter vector and a prompted loss, error, or return, predicts the distribution over parameter updates that achieve the desired metric. At test time, it can optimize neural networks with unseen parameters for downstream tasks in just one update. We find that our approach successfully generates parameters for a wide range of loss prompts. Moreover, it can sample multimodal parameter solutions and has favorable scaling properties. We apply our method to different neural network architectures and tasks in supervised and reinforcement learning.",
        "authors": [
            "William S. Peebles",
            "Ilija Radosavovic",
            "Tim Brooks",
            "Alexei A. Efros",
            "J. Malik"
        ],
        "citations": 51,
        "references": 79,
        "year": 2022
    },
    {
        "title": "Advances and Challenges in De Novo Drug Design Using Three-Dimensional Deep Generative Models",
        "abstract": "A persistent goal for de novo drug design is to generate novel chemical compounds with desirable properties in a labor-, time-, and cost-efficient manner. Deep generative models provide alternative routes to this goal. Numerous model architectures and optimization strategies have been explored in recent years, most of which have been developed to generate two-dimensional molecular structures. Some generative models aiming at three-dimensional (3D) molecule generation have also been proposed, gaining attention for their unique advantages and potential to directly design drug-like molecules in a target-conditioning manner. This review highlights current developments in 3D molecular generative models combined with deep learning and discusses future directions for de novo drug design.",
        "authors": [
            "Weixin Xie",
            "Fanhao Wang",
            "Yibo Li",
            "L. Lai",
            "Jianfeng Pei"
        ],
        "citations": 39,
        "references": 38,
        "year": 2022
    },
    {
        "title": "Deep Generative Models in the Industrial Internet of Things: A Survey",
        "abstract": "Advances in communication technologies and artificial intelligence are accelerating the paradigm of industrial Internet of Things (IIoT). With IIoT enabling continuous integration of sensors and controllers with the network, intelligent analysis of the generated Big Data is a critical requirement. Although IIoT is considered a subset of IoT, it has its own peculiarities in terms of higher levels of safety, security, and low-latency communication in an environment of critical real-time operations. Under these circumstances, discriminative deep learning (DL) algorithms are unsuitable due to their need for large amounts of labeled and balanced training data, uncertainty of inputs, etc. To overcome these issues, researchers have started using deep generative models (DGMs), which combine the flexibility of DL with the inference power of probabilistic modeling. In this article, we review the state of the art of DGMs and their applicability to IIoT, classifying the reviewed works into the IIoT application areas of anomaly detection, trust-boundary protection, network traffic prediction, and platform monitoring. Following an analysis of existing IIoT DGM implementations, we identify challenges (i.e., weak discriminative capability, insufficient interpretability, lack of generalization ability, generated data vulnerability, privacy concern, and data complexity) that need to be investigated in order to accelerate the adoption of DGMs in IIoT and also propose some potential research directions.",
        "authors": [
            "Suparna De",
            "M. Bermúdez-Edo",
            "Honghui Xu",
            "Zhipeng Cai"
        ],
        "citations": 40,
        "references": 89,
        "year": 2022
    },
    {
        "title": "Deep Generative Models on 3D Representations: A Survey",
        "abstract": "—Generative models, as an important family of statistical modeling, target learning the observed data distribution via generating new instances. Along with the rise of neural networks, deep generative models, such as variational autoencoders (VAEs) and generative adversarial network (GANs), have made tremendous progress in 2D image synthesis. Recently, researchers switch their attentions from the 2D space to the 3D space considering that 3D data better aligns with our physical world and hence enjoys great potential in practice. However, unlike a 2D image, which owns an efﬁcient representation ( i.e. , pixel grid) by nature, representing 3D data could face far more challenges. Concretely, we would expect an ideal 3D representation to be capable enough to model shapes and appearances in details, and to be highly efﬁcient so as to model high-resolution data with fast speed and low memory cost. However, existing 3D representations, such as point clouds, meshes, and recent neural ﬁelds, usually fail to meet the above requirements simultaneously. In this survey, we make a thorough review of the development of 3D generation, including 3D shape generation and 3D-aware image synthesis, from the perspectives of both algorithms and more importantly representations. We hope that our discussion could help the community track the evolution of this ﬁeld and further spark some innovative ideas to advance this challenging task.",
        "authors": [
            "Zifan Shi",
            "Sida Peng",
            "Yinghao Xu",
            "Yiyi Liao",
            "Yujun Shen"
        ],
        "citations": 40,
        "references": 254,
        "year": 2022
    },
    {
        "title": "Restoration based Generative Models",
        "abstract": "Denoising diffusion models (DDMs) have recently attracted increasing attention by showing impressive synthesis quality. DDMs are built on a diffusion process that pushes data to the noise distribution and the models learn to denoise. In this paper, we establish the interpretation of DDMs in terms of image restoration (IR). Integrating IR literature allows us to use an alternative objective and diverse forward processes, not confining to the diffusion process. By imposing prior knowledge on the loss function grounded on MAP-based estimation, we eliminate the need for the expensive sampling of DDMs. Also, we propose a multi-scale training, which improves the performance compared to the diffusion process, by taking advantage of the flexibility of the forward process. Experimental results demonstrate that our model improves the quality and efficiency of both training and inference. Furthermore, we show the applicability of our model to inverse problems. We believe that our framework paves the way for designing a new type of flexible general generative model.",
        "authors": [
            "Jaemoo Choi",
            "Yesom Park",
            "Myung-joo Kang"
        ],
        "citations": 5,
        "references": 101,
        "year": 2023
    },
    {
        "title": "Data-driven discovery of 2D materials by deep generative models",
        "abstract": null,
        "authors": [
            "Peder Lyngby",
            "K. Thygesen"
        ],
        "citations": 54,
        "references": 52,
        "year": 2022
    },
    {
        "title": "Typology of Risks of Generative Text-to-Image Models",
        "abstract": "This paper investigates the direct risks and harms associated with modern text-to-image generative models, such as DALL-E and Midjourney, through a comprehensive literature review. While these models offer unprecedented capabilities for generating images, their development and use introduce new types of risk that require careful consideration. Our review reveals significant knowledge gaps concerning the understanding and treatment of these risks despite some already being addressed. We offer a taxonomy of risks across six key stakeholder groups, inclusive of unexplored issues, and suggest future research directions. We identify 22 distinct risk types, spanning issues from data bias to malicious use. The investigation presented here is intended to enhance the ongoing discourse on responsible model development and deployment. By highlighting previously overlooked risks and gaps, it aims to shape subsequent research and governance initiatives, guiding them toward the responsible, secure, and ethically conscious evolution of text-to-image models.",
        "authors": [
            "Charlotte M. Bird",
            "Eddie L. Ungless",
            "Atoosa Kasirzadeh"
        ],
        "citations": 65,
        "references": 193,
        "year": 2023
    },
    {
        "title": "Evaluating generative models in high energy physics",
        "abstract": "There has been a recent explosion in research into machine-learning-based generative modeling to tackle computational challenges for simulations in high energy physics (HEP). In order to use such alternative simulators in practice, we need well-defined metrics to compare different generative models and evaluate their discrepancy from the true distributions. We present the first systematic review and investigation into evaluation metrics and their sensitivity to failure modes of generative models, using the framework of two-sample goodness-of-fit testing, and their relevance and viability for HEP. Inspired by previous work in both physics and computer vision, we propose two new metrics, the Fr\\'echet and kernel physics distances (FPD and KPD, respectively), and perform a variety of experiments measuring their performance on simple Gaussian-distributed, and simulated high energy jet datasets. We find FPD, in particular, to be the most sensitive metric to all alternative jet distributions tested and recommend its adoption, along with the KPD and Wasserstein distances between individual feature distributions, for evaluating generative models in HEP. We finally demonstrate the efficacy of these proposed metrics in evaluating and comparing a novel attention-based generative adversarial particle transformer to the state-of-the-art message-passing generative adversarial network jet simulation model. The code for our proposed metrics is provided in the open source JetNet Python library.",
        "authors": [
            "R. Kansal",
            "Anni Li",
            "Javier Mauricio Duarte",
            "N. Chernyavskaya",
            "M. Pierini",
            "B. Orzari",
            "T. Tomei"
        ],
        "citations": 32,
        "references": 75,
        "year": 2022
    },
    {
        "title": "Can Push-forward Generative Models Fit Multimodal Distributions?",
        "abstract": "Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them\"push-forward\"models and study their expressivity. We show that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that generative models consisting of stacked networks with stochastic input at each step, such as diffusion models do not suffer of such limitations.",
        "authors": [
            "Antoine Salmona",
            "Valentin De Bortoli",
            "J. Delon",
            "A. Desolneux"
        ],
        "citations": 33,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Subspace Diffusion Generative Models",
        "abstract": null,
        "authors": [
            "Bowen Jing",
            "Gabriele Corso",
            "Renato Berlinghieri",
            "Tommi Jaakkola"
        ],
        "citations": 70,
        "references": 22,
        "year": 2022
    },
    {
        "title": "Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models",
        "abstract": "Generative models (e.g., GANs, diffusion models) learn the underlying data distribution in an unsupervised manner. However, many applications of interest require sampling from a particular region of the output space or sampling evenly over a range of characteristics. For efficient sampling in these scenarios, we propose Generative Visual Prompt (PromptGen), a framework for distributional control over pre-trained generative models by incorporating knowledge of other off-the-shelf models. PromptGen defines control as energy-based models (EBMs) and samples images in a feed-forward manner by approximating the EBM with invertible neural networks, avoiding optimization at inference. Our experiments demonstrate how PromptGen can efficiently sample from several unconditional generative models (e.g., StyleGAN2, StyleNeRF, diffusion autoencoder, NVAE) in a controlled or/and de-biased manner using various off-the-shelf models: (1) with the CLIP model as control, PromptGen can sample images guided by text, (2) with image classifiers as control, PromptGen can de-bias generative models across a set of attributes or attribute combinations, and (3) with inverse graphics models as control, PromptGen can sample images of the same identity in different poses. (4) Finally, PromptGen reveals that the CLIP model shows a\"reporting bias\"when used as control, and PromptGen can further de-bias this controlled distribution in an iterative manner. The code is available at https://github.com/ChenWu98/Generative-Visual-Prompt.",
        "authors": [
            "Chen Henry Wu",
            "Saman Motamed",
            "Shaunak Srivastava",
            "F. D. L. Torre"
        ],
        "citations": 32,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Identifiability of deep generative models without auxiliary information",
        "abstract": "We prove identifiability of a broad class of deep latent variable models that (a) have universal approximation capabilities and (b) are the decoders of variational autoencoders that are commonly used in practice. Unlike existing work, our analysis does not require weak supervision, auxiliary information, or conditioning in the latent space. Specifically, we show that for a broad class of generative (i.e. unsupervised) models with universal approximation capabilities, the side information $u$ is not necessary: We prove identifiability of the entire generative model where we do not observe $u$ and only observe the data $x$. The models we consider match autoencoder architectures used in practice that leverage mixture priors in the latent space and ReLU/leaky-ReLU activations in the encoder, such as VaDE and MFC-VAE. Our main result is an identifiability hierarchy that significantly generalizes previous work and exposes how different assumptions lead to different\"strengths\"of identifiability, and includes certain\"vanilla\"VAEs with isotropic Gaussian priors as a special case. For example, our weakest result establishes (unsupervised) identifiability up to an affine transformation, and thus partially resolves an open problem regarding model identifiability raised in prior work. These theoretical results are augmented with experiments on both simulated and real data.",
        "authors": [
            "Bohdan Kivva",
            "Goutham Rajendran",
            "Pradeep Ravikumar",
            "Bryon Aragam"
        ],
        "citations": 43,
        "references": 101,
        "year": 2022
    },
    {
        "title": "De novo molecular design with deep molecular generative models for PPI inhibitors",
        "abstract": "We construct a protein-protein interaction (PPI) targeted drug-likeness dataset and propose a deep molecular generative framework to generate novel drug-likeness molecules from the features of the seed compounds. This framework gains inspiration from published molecular generative models, uses the key features associated with PPI inhibitors as input and develops deep molecular generative models for de novo molecular design of PPI inhibitors. For the first time, quantitative estimation index for compounds targeting PPI was applied to the evaluation of the molecular generation model for de novo design of PPI-targeted compounds. Our results estimated that the generated molecules had better PPI-targeted drug-likeness and drug-likeness. Additionally, our model also exhibits comparable performance to other several state-of-the-art molecule generation models. The generated molecules share chemical space with iPPI-DB inhibitors as demonstrated by chemical space analysis. The peptide characterization-oriented design of PPI inhibitors and the ligand-based design of PPI inhibitors are explored. Finally, we recommend that this framework will be an important step forward for the de novo design of PPI-targeted therapeutics.",
        "authors": [
            "Jianmin Wang",
            "Yanyi Chu",
            "Jiashun Mao",
            "Hyeon-Nae Jeon",
            "Haiyan Jin",
            "Amir Zeb",
            "Yuil Jang",
            "Kwang-Hwi Cho",
            "Tao Song",
            "Kyoung Tai No"
        ],
        "citations": 31,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Diagnosing and Fixing Manifold Overfitting in Deep Generative Models",
        "abstract": "Likelihood-based, or explicit, deep generative models use neural networks to construct flexible high-dimensional densities. This formulation directly contradicts the manifold hypothesis, which states that observed data lies on a low-dimensional manifold embedded in high-dimensional ambient space. In this paper we investigate the pathologies of maximum-likelihood training in the presence of this dimensionality mismatch. We formally prove that degenerate optima are achieved wherein the manifold itself is learned but not the distribution on it, a phenomenon we call manifold overfitting. We propose a class of two-step procedures consisting of a dimensionality reduction step followed by maximum-likelihood density estimation, and prove that they recover the data-generating distribution in the nonparametric regime, thus avoiding manifold overfitting. We also show that these procedures enable density estimation on the manifolds learned by implicit models, such as generative adversarial networks, hence addressing a major shortcoming of these models. Several recently proposed methods are instances of our two-step procedures; we thus unify, extend, and theoretically justify a large class of models.",
        "authors": [
            "G. Loaiza-Ganem",
            "Brendan Leigh Ross",
            "Jesse C. Cresswell",
            "Anthony L. Caterini"
        ],
        "citations": 26,
        "references": 157,
        "year": 2022
    },
    {
        "title": "Diffusion Generative Models in Infinite Dimensions",
        "abstract": "Diffusion generative models have recently been applied to domains where the available data can be seen as a discretization of an underlying function, such as audio signals or time series. However, these models operate directly on the discretized data, and there are no semantics in the modeling process that relate the observed data to the underlying functional forms. We generalize diffusion models to operate directly in function space by developing the foundational theory for such models in terms of Gaussian measures on Hilbert spaces. A significant benefit of our function space point of view is that it allows us to explicitly specify the space of functions we are working in, leading us to develop methods for diffusion generative modeling in Sobolev spaces. Our approach allows us to perform both unconditional and conditional generation of function-valued data. We demonstrate our methods on several synthetic and real-world benchmarks.",
        "authors": [
            "Gavin Kerrigan",
            "Justin Ley",
            "Padhraic Smyth"
        ],
        "citations": 25,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models",
        "abstract": "We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at https://github.com/layer6ai-labs/dgm-eval.",
        "authors": [
            "G. Stein",
            "Jesse C. Cresswell",
            "Rasa Hosseinzadeh",
            "Yi Sui",
            "Brendan Leigh Ross",
            "Valentin Villecroze",
            "Zhaoyan Liu",
            "Anthony L. Caterini",
            "J. E. T. Taylor",
            "G. Loaiza-Ganem"
        ],
        "citations": 58,
        "references": 112,
        "year": 2023
    },
    {
        "title": "GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information",
        "abstract": "We investigate how multiple sliders with and without feedforward visualizations influence users’ control of generative models. In an online study (N=138), we collected a dataset of people interacting with a generative adversarial network (StyleGAN2) in an image reconstruction task. We found that more control dimensions (sliders) significantly increase task difficulty and user actions. Visual feedforward partly mitigates this by enabling more goal-directed interaction. However, we found no evidence of faster or more accurate task performance. This indicates a tradeoff between feedforward detail and implied cognitive costs, such as attention. Moreover, we found that visualizations alone are not always sufficient for users to understand individual control dimensions. Our study quantifies fundamental UI design factors and resulting interaction behavior in this context, revealing opportunities for improvement in the UI design for interactive applications of generative models. We close by discussing design directions and further aspects.",
        "authors": [
            "Hai Dang",
            "Lukas Mecke",
            "Daniel Buschek"
        ],
        "citations": 25,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Generative Models of Brain Dynamics",
        "abstract": "This review article gives a high-level overview of the approaches across different scales of organization and levels of abstraction. The studies covered in this paper include fundamental models in computational neuroscience, nonlinear dynamics, data-driven methods, as well as emergent practices. While not all of these models span the intersection of neuroscience, AI, and system dynamics, all of them do or can work in tandem as generative models, which, as we argue, provide superior properties for the analysis of neuroscientific data. We discuss the limitations and unique dynamical traits of brain data and the complementary need for hypothesis- and data-driven modeling. By way of conclusion, we present several hybrid generative models from recent literature in scientific machine learning, which can be efficiently deployed to yield interpretable models of neural dynamics.",
        "authors": [
            "Mahta Ramezanian-Panahi",
            "German Abrevaya",
            "Jean-Christophe Gagnon-Audet",
            "Vikram S. Voleti",
            "I. Rish",
            "G. Dumas"
        ],
        "citations": 27,
        "references": 245,
        "year": 2022
    },
    {
        "title": "Out-of-distribution Detection via Frequency-regularized Generative Models",
        "abstract": "Modern deep generative models can assign high likelihood to inputs drawn from outside the training distribution, posing threats to models in open-world deployments. While much research attention has been placed on defining new test-time measures of OOD uncertainty, these methods do not fundamentally change how deep generative models are regularized and optimized in training. In particular, generative models are shown to overly rely on the background information to estimate the likelihood. To address the issue, we propose a novel frequency-regularized learning (FRL) framework for OOD detection, which incorporates high-frequency information into training and guides the model to focus on semantically relevant features. FRL effectively improves performance on a wide range of generative architectures, including variational auto-encoder, GLOW, and PixelCNN++. On a new large-scale evaluation task, FRL achieves the state-of-the-art performance, outperforming a strong baseline Likelihood Regret by 10.7% (AUROC) while achieving 147× faster inference speed. Extensive ablations show that FRL improves the OOD detection performance while preserving the image generation quality. Code is available at https://github.com/mu-cai/FRL.",
        "authors": [
            "Mu Cai",
            "Yixuan Li"
        ],
        "citations": 28,
        "references": 58,
        "year": 2022
    },
    {
        "title": "De Novo Molecule Design Using Molecular Generative Models Constrained by Ligand-Protein Interactions",
        "abstract": "In recent years, molecular deep generative models have attracted much attention for its application in de novo drug design. The data-driven molecular deep generative model approximates the high dimensional distribution of the chemical space through learning from a large number of molecular structural data. So far, most of the molecular generative models rely on purely 2D ligand information in structure generation. Here, we propose a novel molecular deep generative model which adopts a recurrent neural network architecture coupled with a ligand-protein interaction fingerprint as constraints. The fingerprint was constructed on ligand docking poses and represents the 3D binding mode of ligands in the protein pocket. In the current work, generative models constrained with interaction fingerprints were trained and compared with normal RNN models. It has been shown that models trained with constraints of ligand-protein interaction fingerprint have a clear tendency to generating compounds maintaining similar binding modes. Our results demonstrate the potential application of the interaction fingerprint-constrained generative model for the targeted molecule generation and guided exploration on the drug-like chemical space.",
        "authors": [
            "Jie Zhang",
            "Hongming Chen"
        ],
        "citations": 27,
        "references": 0,
        "year": 2022
    },
    {
        "title": "End-to-end protein–ligand complex structure generation with diffusion-based generative models",
        "abstract": null,
        "authors": [
            "Shuya Nakata",
            "Yoshiharu Mori",
            "S. Tanaka"
        ],
        "citations": 32,
        "references": 49,
        "year": 2022
    },
    {
        "title": "How generative AI models such as ChatGPT can be (mis)used in SPC practice, education, and research? An exploratory study",
        "abstract": "Abstract Generative Artificial Intelligence (AI) models such as OpenAI’s ChatGPT have the potential to revolutionize Statistical Process Control (SPC) practice, learning, and research. However, these tools are in the early stages of development and can be easily misused or misunderstood. In this paper, we give an overview of the development of Generative AI. Specifically, we explore ChatGPT’s ability to provide code, explain basic concepts, and create knowledge related to SPC practice, learning, and research. By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well-known concepts but struggles with more nuanced tasks, such as explaining less widely known terms and creating code from scratch. We find that using new AI tools may help practitioners, educators, and researchers to be more efficient and productive. However, in their current stages of development, some results are misleading and wrong. Overall, the use of generative AI models in SPC must be properly validated and used in conjunction with other methods to ensure accurate results.",
        "authors": [
            "F. Megahed",
            "Ying-Ju Chen",
            "Joshua A. Ferris",
            "S. Knoth",
            "L. A. Jones‐Farmer"
        ],
        "citations": 100,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Distribution-Preserving Steganography Based on Text-to-Speech Generative Models",
        "abstract": "Steganography is the art and science of hiding secret messages in public communication so that the presence of secret messages cannot be detected. There are two distribution-preserving steganographic frameworks, one is sampler-based and the other is compression-based. The former requires a perfect sampler which yields data following the same distribution, and the latter needs the explicit distribution of generative objects. However, these two conditions are too strict even unrealistic in the traditional data environment, e.g., the distribution of natural images is hard to seize. Fortunately, generative models bring new vitality to distribution-preserving steganography, which can serve as the perfect sampler or provide the explicit distribution of generative media. Taking text-to-speech generation task as an example, we propose distribution-preserving steganography based on WaveGlow and WaveRNN, which corresponds to the former two categories. Steganalysis experiments and theoretical analysis are conducted to demonstrate that the proposed methods can preserve the distribution.",
        "authors": [
            "Kejiang Chen",
            "Hang Zhou",
            "Hanqing Zhao",
            "Dongdong Chen",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "citations": 19,
        "references": 64,
        "year": 2022
    },
    {
        "title": "On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models",
        "abstract": "Diffusion-based Deep Generative Models (DDGMs) offer state-of-the-art performance in generative modeling. Their main strength comes from their unique setup in which a model (the backward diffusion process) is trained to reverse the forward diffusion process, which gradually adds noise to the input signal. Although DDGMs are well studied, it is still unclear how the small amount of noise is transformed during the backward diffusion process. Here, we focus on analyzing this problem to gain more insight into the behavior of DDGMs and their denoising and generative capabilities. We observe a fluid transition point that changes the functionality of the backward diffusion process from generating a (corrupted) image from noise to denoising the corrupted image to the final sample. Based on this observation, we postulate to divide a DDGM into two parts: a denoiser and a generator. The denoiser could be parameterized by a denoising auto-encoder, while the generator is a diffusion-based model with its own set of parameters. We experimentally validate our proposition, showing its pros and cons.",
        "authors": [
            "K. Deja",
            "Anna Kuzina",
            "Tomasz Trzci'nski",
            "J. Tomczak"
        ],
        "citations": 23,
        "references": 35,
        "year": 2022
    },
    {
        "title": "Fair Generative Models via Transfer Learning",
        "abstract": "This work addresses fair generative models. Dataset biases have been a major cause of unfairness in deep generative models. Previous work had proposed to augment large, biased datasets with small, unbiased reference datasets. Under this setup, a weakly-supervised approach has been proposed, which achieves state-of-the-art quality and fairness in generated samples. In our work, based on this setup, we propose a simple yet effective approach. Specifically, first, we propose fairTL, a transfer learning approach to learn fair generative models. Under fairTL, we pre-train the generative model with the available large, biased datasets and subsequently adapt the model using the small, unbiased reference dataset. We find that our fairTL can learn expressive sample generation during pre-training, thanks to the large (biased) dataset. This knowledge is then transferred to the target model during adaptation, which also learns to capture the underlying fair distribution of the small reference dataset. Second, we propose fairTL++, where we introduce two additional innovations to improve upon fairTL: (i) multiple feedback and (ii) Linear-Probing followed by Fine-Tuning (LP-FT). Taking one step further, we consider an alternative, challenging setup when only a pre-trained (potentially biased) model is available but the dataset that was used to pre-train the model is inaccessible. We demonstrate that our proposed fairTL and fairTL++ remain very effective under this setup. We note that previous work requires access to the large, biased datasets and is incapable of handling this more challenging setup. Extensive experiments show that fairTL and fairTL++ achieve state-of-the-art in both quality and fairness of generated samples. The code and additional resources can be found at bearwithchris.github.io/fairTL/.",
        "authors": [
            "Christopher T. H. Teo",
            "Milad Abdollahzadeh",
            "Ngai-Man Cheung"
        ],
        "citations": 22,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Deep Generative Models for Fast Photon Shower Simulation in ATLAS",
        "abstract": null,
        "authors": [
            "G. Aad",
            "B. Abbott",
            "D. Abbott",
            "A. A. Abud",
            "K. Abeling",
            "D. Abhayasinghe",
            "S. Abidi",
            "A. Aboulhorma",
            "H. Abramowicz",
            "H. Abreu",
            "Y. Abulaiti",
            "A. Hoffman",
            "B. Acharya",
            "B. Achkar",
            "L. Adam",
            "C. Bourdarios",
            "L. Adamczyk",
            "L. Adamek",
            "S. Addepalli",
            "J. Adelman",
            "A. Adiguzel",
            "S. Adorni",
            "T. Adye",
            "A. Affolder",
            "Y. Afik",
            "M. N. Agaras",
            "J. Agarwala",
            "A. Aggarwal",
            "C. Agheorghiesei",
            "J. A. Aguilar-Saavedra",
            "A. Ahmad",
            "F. Ahmadov",
            "W. Ahmed",
            "X. Ai",
            "G. Aielli",
            "I. Aizenberg",
            "M. Akbiyik",
            "T. Åkesson",
            "A. Akimov",
            "K. Khoury",
            "G. Alberghi",
            "J. Albert",
            "P. Albicocco",
            "M. J. A. Verzini",
            "S. Alderweireldt",
            "M. Aleksa",
            "I. Aleksandrov",
            "C. Alexa",
            "T. Alexopoulos",
            "A. Alfonsi",
            "F. Alfonsi",
            "M. Alhroob",
            "B. Ali",
            "S. Ali",
            "M. Aliev",
            "G. Alimonti",
            "C. Allaire",
            "B. Allbrooke",
            "P. Allport",
            "A. Aloisio",
            "F. Alonso",
            "C. Alpigiani",
            "E. A. Camelia",
            "M. A. Estevez",
            "M. Alviggi",
            "Y. A. Coutinho",
            "A. Ambler",
            "L. Ambroz",
            "C. Amelung",
            "D. Amidei",
            "S. P. A. D. Santos",
            "S. Amoroso",
            "K. R. Amos",
            "C. Amrouche",
            "V. Ananiev",
            "C. Anastopoulos",
            "N. Andari",
            "T. Andeen",
            "J. Anders",
            "S. Y. Andrean",
            "A. Andreazza",
            "S. Angelidakis",
            "A. Angerami",
            "A. Anisenkov",
            "A. Annovi",
            "C. Antel",
            "M. T. Anthony",
            "E. Antipov",
            "M. Antonelli",
            "D. Antrim",
            "F. Anulli",
            "M. Aoki",
            "J. Pozo",
            "M. Aparo",
            "L. A. Bella",
            "C. Appelt",
            "N. Aranzabal",
            "V. A. Ferraz",
            "C. Arcangeletti",
            "A. Arce",
            "E. Arena",
            "J. Arguin",
            "S. Argyropoulos",
            "J. Arling",
            "A. J. Armbruster",
            "O. Arnaez",
            "H. Arnold",
            "Z. P. A. Tame",
            "G. Artoni",
            "H. Asada",
            "K. Asai",
            "S. Asai",
            "N. Asbah",
            "E. Asimakopoulou",
            "J. Assahsah",
            "K. Assamagan",
            "R. Astalos",
            "R. Atkin",
            "M. Atkinson",
            "N. B. Atlay",
            "H. Atmani",
            "P. Atmasiddha",
            "K. Augsten",
            "S. Auricchio",
            "V. A. Austrup",
            "G. Avner",
            "G. Avolio",
            "M. Ayoub",
            "G. Azuelos",
            "D. Babal",
            "H. Bachacou",
            "K. Bachas",
            "A. Bachiu",
            "F. Backman",
            "A. Badea",
            "P. Bagnaia",
            "M. Bahmani",
            "A. Bailey",
            "V. Bailey",
            "J. Baines",
            "C. Bakalis",
            "O. Baker",
            "P. Bakker",
            "E. Bakos",
            "D. B. Gupta",
            "S. Balaji",
            "R. Balasubramanian",
            "E. Baldin",
            "P. Balek",
            "E. Ballabene",
            "F. Balli",
            "L. M. Baltes",
            "W. Balunas",
            "J. Balz",
            "E. Banas",
            "M. Bandieramonte",
            "A. Bandyopadhyay",
            "S. Bansal",
            "L. Barak",
            "E. Barberio",
            "D. Barberis",
            "M. Barbero",
            "G. Barbour",
            "K. N. Barends",
            "T. Barillari",
            "M. Barisits",
            "J. Barkeloo",
            "T. Barklow",
            "R. Barnett",
            "P. Baron",
            "A. Baroncelli",
            "G. Barone",
            "A. Barr",
            "L. B. Navarro",
            "F. Barreiro",
            "J. G. D. da Costa",
            "U. Barron",
            "S. Barsov",
            "F. Bartels",
            "R. Bartoldus",
            "G. Bartolini",
            "A. E. Barton",
            "P. Bartos",
            "A. Basalaev",
            "A. Basan",
            "M. Baselga",
            "I. Bashta",
            "A. Bassalat",
            "M. J. Basso",
            "C. Basson",
            "R. Bates",
            "S. Batlamous",
            "J. R. Batley",
            "Batool Babaei Jahromi",
            "M. Battaglia",
            "M. Bauce",
            "F. Bauer",
            "P. Bauer",
            "A. Bayirli",
            "J. Beacham",
            "T. Beau",
            "P. Beauchemin",
            "F. Becherer",
            "P. Bechtle",
            "H. Beck",
            "K. Becker",
            "C. Becot",
            "A. Beddall",
            "V. Bednyakov",
            "C. Bee",
            "L. Beemster",
            "T. Beermann",
            "M. Begalli",
            "M. Begel",
            "A. Behera",
            "J. Behr",
            "C. B. da Cruz E Silva",
            "J. F. Beirer",
            "F. Beisiegel",
            "M. Belfkir",
            "G. Bella",
            "L. Bellagamba",
            "A. Bellerive",
            "P. Bellos",
            "K. Beloborodov",
            "K. Belotskiy",
            "N. Belyaev",
            "D. Benchekroun",
            "Y. Benhammou",
            "D. Benjamin",
            "M. Benoit",
            "J. Bensinger",
            "S. Bentvelsen",
            "L. Beresford",
            "M. Beretta",
            "D. Berge",
            "E. Kuutmann",
            "N. Berger",
            "B. Bergmann",
            "J. Beringer",
            "S. Berlendis",
            "G. Bernardi",
            "C. Bernius",
            "F. Bernlochner",
            "T. Berry",
            "P. Berta",
            "A. Berthold",
            "I. Bertram",
            "O. Bylund",
            "S. Bethke",
            "A. Betti",
            "A. Bevan",
            "S. Bhatta",
            "D. S. Bhattacharya",
            "P. Bhattarai",
            "V. Bhopatkar",
            "R. Bi",
            "R. Bianchi",
            "O. Biebel",
            "R. Bielski",
            "N. Biesuz",
            "M. Biglietti",
            "T. Billoud",
            "M. Bindi",
            "A. Bingul",
            "C. Bini",
            "S. Biondi",
            "A. Biondini",
            "C. Birch-sykes",
            "G. Bird",
            "M. Birman",
            "T. Bisanz",
            "D. Biswas",
            "A. Bitadze",
            "K. Bjørke",
            "I. Bloch",
            "C. Blocker",
            "A. Blue",
            "U. Blumenschein",
            "J. Blumenthal",
            "G. Bobbink",
            "V. Bobrovnikov",
            "M. Boehler",
            "D. Bogavac",
            "A. Bogdanchikov",
            "C. Bohm",
            "V. Boisvert",
            "P. Bokan",
            "T. Bold",
            "M. Bomben",
            "M. Bona",
            "M. Boonekamp",
            "C. D. Booth",
            "A. Borbély",
            "H. Borecka-Bielska",
            "L. S. Borgna",
            "G. Borissov",
            "D. Bortoletto",
            "D. Boscherini",
            "M. Bosman",
            "J. D. B. Sola",
            "K. Bouaouda",
            "J. Boudreau",
            "E. Bouhova-Thacker",
            "D. Boumediene",
            "R. Bouquet",
            "A. Boveia",
            "J. Boyd",
            "D. Boye",
            "I. Boyko",
            "J. Bracinik",
            "N. Brahimi",
            "G. Brandt",
            "O. Brandt",
            "F. Braren",
            "B. Brau",
            "J. Brau",
            "W. D. B. Madden",
            "K. Brendlinger",
            "R. Brener",
            "L. Brenner",
            "R. Brenner",
            "S. Bressler",
            "B. Brickwedde",
            "D. Britton",
            "D. Britzger",
            "I. Brock",
            "G. Brooijmans",
            "W. Brooks",
            "E. Brost",
            "P. B. D. de Renstrom",
            "B. Brüers",
            "D. Bruncko",
            "A. Bruni",
            "G. Bruni",
            "M. Bruschi",
            "N. Bruscino",
            "L. Bryngemark",
            "T. Buanes",
            "Q. Buat",
            "P. Buchholz",
            "A. Buckley",
            "I. Budagov",
            "M. K. Bugge",
            "O. Bulekov",
            "B. Bullard",
            "S. Burdin",
            "C. Burgard",
            "A. Burger",
            "B. Burghgrave",
            "J. Burr",
            "C. D. Burton",
            "J. C. Burzynski",
            "E. L. Busch",
            "V. Büscher",
            "P. Bussey",
            "J. Butler",
            "C. Buttar",
            "J. Butterworth",
            "W. Buttinger",
            "C. Vazquez",
            "A. Buzykaev",
            "G. Cabras",
            "S. Urbán",
            "D. Caforio",
            "H. Cai",
            "Y. Cai",
            "V. Cairo",
            "O. Cakir",
            "N. Calace",
            "P. Calafiura",
            "G. Calderini",
            "P. Calfayan",
            "G. Callea",
            "L. Caloba",
            "D. Calvet",
            "S. Calvet",
            "T. Calvet",
            "M. Calvetti",
            "R. C. Toro",
            "S. Camarda",
            "D. C. Munoz",
            "P. Camarri",
            "M. T. Camerlingo",
            "D. Cameron",
            "C. Camincher",
            "M. Campanelli",
            "A. Camplani",
            "V. Canale",
            "A. Canesse",
            "M. C. Bret",
            "J. Cantero",
            "Y. Cao",
            "F. Capocasa",
            "M. Capua",
            "A. Carbone",
            "R. Cardarelli",
            "J. Cardenas",
            "F. Cardillo",
            "T. Carli",
            "G. Carlino",
            "B. Carlson",
            "E. M. Carlson",
            "L. Carminati",
            "M. Carnesale",
            "S. Caron",
            "E. Carquin",
            "S. Carrá",
            "G. Carratta",
            "J. Carter",
            "T. M. Carter",
            "D. Casadei",
            "M. Casado",
            "A. Casha",
            "E. G. Castiglia",
            "F. L. Castillo",
            "L. C. Garcia",
            "V. C. Gimenez",
            "N. Castro",
            "A. Catinaccio",
            "J. Catmore",
            "V. Cavaliere",
            "N. Cavalli",
            "V. Cavasinni",
            "E. Celebi",
            "F. Celli",
            "M. S. Centonze",
            "K. Cerny",
            "A. S. Cerqueira",
            "A. Cerri",
            "L. Cerrito",
            "F. Cerutti",
            "A. Cervelli",
            "S. Çetin",
            "Z. Chadi",
            "D. Chakraborty",
            "M. Chala",
            "J. Chan",
            "W. S. Chan",
            "W. Chan",
            "J. D. Chapman",
            "B. Chargeishvili",
            "D. Charlton",
            "T. P. Charman",
            "M. Chatterjee",
            "S. Chekanov",
            "S. Chekulaev",
            "G. Chelkov",
            "A. Chen",
            "B. Chen",
            "B. Chen",
            "C. Chen",
            "H. Chen",
            "H. Chen",
            "J. Chen",
            "J. Chen",
            "S. Chen",
            "S. Chen",
            "X. Chen",
            "X. Chen",
            "Y. Chen",
            "C. Cheng",
            "H. Cheng",
            "A. Cheplakov",
            "E. Cheremushkina",
            "E. Cherepanova",
            "R. C. Moursli",
            "E. Cheu",
            "K. Cheung",
            "L. Chevalier",
            "V. Chiarella",
            "G. Chiarelli",
            "G. Chiodini",
            "A. Chisholm",
            "A. Chitan",
            "Y. Chiu",
            "M. Chizhov",
            "K. Choi",
            "A. Chomont",
            "Y. Chou",
            "E. Chow",
            "T. Chowdhury",
            "L. D. Christopher",
            "M. Chu",
            "X. Chu",
            "J. Chudoba",
            "J. Chwastowski",
            "D. Cieri",
            "K. M. Ciesla",
            "V. Cindro",
            "A. Ciocio",
            "F. Cirotto",
            "Z. Citron",
            "M. Citterio",
            "D. Ciubotaru",
            "B. M. Ciungu",
            "A. Clark",
            "P. J. Clark",
            "J. M. C. Columbie",
            "S. E. Clawson"
        ],
        "citations": 33,
        "references": 39,
        "year": 2022
    },
    {
        "title": "medigan: a Python library of pretrained generative models for medical image synthesis",
        "abstract": "Abstract. Purpose Deep learning has shown great promise as the backbone of clinical decision support systems. Synthetic data generated by generative models can enhance the performance and capabilities of data-hungry deep learning models. However, there is (1) limited availability of (synthetic) datasets and (2) generative models are complex to train, which hinders their adoption in research and clinical applications. To reduce this entry barrier, we explore generative model sharing to allow more researchers to access, generate, and benefit from synthetic data. Approach We propose medigan, a one-stop shop for pretrained generative models implemented as an open-source framework-agnostic Python library. After gathering end-user requirements, design decisions based on usability, technical feasibility, and scalability are formulated. Subsequently, we implement medigan based on modular components for generative model (i) execution, (ii) visualization, (iii) search & ranking, and (iv) contribution. We integrate pretrained models with applications across modalities such as mammography, endoscopy, x-ray, and MRI. Results The scalability and design of the library are demonstrated by its growing number of integrated and readily-usable pretrained generative models, which include 21 models utilizing nine different generative adversarial network architectures trained on 11 different datasets. We further analyze three medigan applications, which include (a) enabling community-wide sharing of restricted data, (b) investigating generative model evaluation metrics, and (c) improving clinical downstream tasks. In (b), we extract Fréchet inception distances (FID) demonstrating FID variability based on image normalization and radiology-specific feature extractors. Conclusion medigan allows researchers and developers to create, increase, and domain-adapt their training data in just a few lines of code. Capable of enriching and accelerating the development of clinical machine learning models, we show medigan’s viability as platform for generative model sharing. Our multimodel synthetic data experiments uncover standards for assessing and reporting metrics, such as FID, in image synthesis studies.",
        "authors": [
            "Richard Osuala",
            "Grzegorz Skorupko",
            "Noussair Lazrak",
            "Lidia Garrucho",
            "E. García",
            "Smriti Joshi",
            "Socayna Jouide",
            "Michael Rutherford",
            "F. Prior",
            "Kaisar Kushibar",
            "Oliver Díaz",
            "Karim Lekadir"
        ],
        "citations": 19,
        "references": 118,
        "year": 2022
    },
    {
        "title": "Partial Identification of Treatment Effects with Implicit Generative Models",
        "abstract": "We consider the problem of partial identification, the estimation of bounds on the treatment effects from observational data. Although studied using discrete treatment variables or in specific causal graphs (e.g., instrumental variables), partial identification has been recently explored using tools from deep generative modeling. We propose a new method for partial identification of average treatment effects(ATEs) in general causal graphs using implicit generative models comprising continuous and discrete random variables. Since ATE with continuous treatment is generally non-regular, we leverage the partial derivatives of response functions to define a regular approximation of ATE, a quantity we call uniform average treatment derivative (UATD). We prove that our algorithm converges to tight bounds on ATE in linear structural causal models (SCMs). For nonlinear SCMs, we empirically show that using UATD leads to tighter and more stable bounds than methods that directly optimize the ATE.",
        "authors": [
            "Vahid Balazadeh Meresht",
            "Vasilis Syrgkanis",
            "R. G. Krishnan"
        ],
        "citations": 18,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Indeterminacy in Generative Models: Characterization and Strong Identifiability",
        "abstract": "Most modern probabilistic generative models, such as the variational autoencoder (VAE), have certain indeterminacies that are unresolvable even with an infinite amount of data. Different tasks tolerate different indeterminacies, however recent applications have indicated the need for strongly identifiable models, in which an observation corresponds to a unique latent code. Progress has been made towards reducing model indeterminacies while maintaining flexibility, and recent work excludes many--but not all--indeterminacies. In this work, we motivate model-identifiability in terms of task-identifiability, then construct a theoretical framework for analyzing the indeterminacies of latent variable models, which enables their precise characterization in terms of the generator function and prior distribution spaces. We reveal that strong identifiability is possible even with highly flexible nonlinear generators, and give two such examples. One is a straightforward modification of iVAE (arXiv:1907.04809 [stat.ML]); the other uses triangular monotonic maps, leading to novel connections between optimal transport and identifiability.",
        "authors": [
            "Quanhan Xi",
            "Benjamin Bloem-Reddy"
        ],
        "citations": 19,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Amortized Projection Optimization for Sliced Wasserstein Generative Models",
        "abstract": "Seeking informative projecting directions has been an important task in utilizing sliced Wasserstein distance in applications. However, finding these directions usually requires an iterative optimization procedure over the space of projecting directions, which is computationally expensive. Moreover, the computational issue is even more severe in deep learning applications, where computing the distance between two mini-batch probability measures is repeated several times. This nested loop has been one of the main challenges that prevent the usage of sliced Wasserstein distances based on good projections in practice. To address this challenge, we propose to utilize the learning-to-optimize technique or amortized optimization to predict the informative direction of any given two mini-batch probability measures. To the best of our knowledge, this is the first work that bridges amortized optimization and sliced Wasserstein generative models. In particular, we derive linear amortized models, generalized linear amortized models, and non-linear amortized models which are corresponding to three types of novel mini-batch losses, named amortized sliced Wasserstein. We demonstrate the favorable performance of the proposed sliced losses in deep generative modeling on standard benchmark datasets.",
        "authors": [
            "Khai Nguyen",
            "Nhat Ho"
        ],
        "citations": 17,
        "references": 64,
        "year": 2022
    },
    {
        "title": "Why Are Conditional Generative Models Better Than Unconditional Ones?",
        "abstract": "Extensive empirical evidence demonstrates that conditional generative models are easier to train and perform better than unconditional ones by exploiting the labels of data. So do score-based diffusion models. In this paper, we analyze the phenomenon formally and identify that the key of conditional learning is to partition the data properly. Inspired by the analyses, we propose self-conditioned diffusion models (SCDM), which is trained conditioned on indices clustered by the k -means algorithm on the features extracted by a model pre-trained in a self-supervised manner. SCDM signiﬁcantly improves the unconditional model across various datasets and achieves a record-breaking FID of 3.94 on ImageNet 64x64 without labels. Besides, SCDM achieves a slightly better FID than the corresponding conditional model on CIFAR10.",
        "authors": [
            "Fan Bao",
            "Chongxuan Li",
            "Jiacheng Sun",
            "Jun Zhu"
        ],
        "citations": 16,
        "references": 22,
        "year": 2022
    },
    {
        "title": "An empirical Bayes method for differential expression analysis of single cells with deep generative models",
        "abstract": "Detecting differentially expressed genes is important for characterizing subpopulations of cells. In scRNA-seq data, however, nuisance variation due to technical factors like sequencing depth and RNA capture efficiency obscures the underlying biological signal. Deep generative models have been extensively applied to scRNA-seq data, with a special focus on embedding cells into a low-dimensional latent space and correcting for batch effects. However, little attention has been given to the problem of utilizing the uncertainty from the deep generative model for differential expression. Furthermore, the existing approaches do not allow controlling for the effect size or the false discovery rate. Here, we present lvm-DE, a generic Bayesian approach for performing differential expression from using a fitted deep generative model, while controlling the false discovery rate. We apply the lvm-DE framework to scVI and scSphere, two deep generative models. The resulting approaches outperform the state-of-the-art methods at estimating the log fold change in gene expression levels, as well as detecting differentially expressed genes between subpopulations of cells.",
        "authors": [
            "Pierre Boyeau",
            "J. Regier",
            "Adam Gayoso",
            "Michael I. Jordan",
            "Romain Lopez",
            "N. Yosef"
        ],
        "citations": 24,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Scaling Laws for Generative Mixed-Modal Language Models",
        "abstract": "Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.",
        "authors": [
            "Armen Aghajanyan",
            "L. Yu",
            "Alexis Conneau",
            "Wei-Ning Hsu",
            "Karen Hambardzumyan",
            "Susan Zhang",
            "Stephen Roller",
            "Naman Goyal",
            "Omer Levy",
            "Luke Zettlemoyer"
        ],
        "citations": 81,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling",
        "abstract": "Score-based generative models (SGMs) have recently emerged as a promising class of generative models. However, a fundamental limitation is that their inference is very slow due to a need for many (e.g., 2000) iterations of sequential computations. An intuitive acceleration method is to reduce the sampling iterations which however causes severe performance degradation. We investigate this problem by viewing the diffusion sampling process as a Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause to be ill-conditioned curvature. Under this insight, we propose a model-agnostic preconditioned diffusion sampling (PDS) method that leverages matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS is proven theoretically to converge to the original target distribution of a SGM, no need for retraining. Extensive experiments on three image datasets with a variety of resolutions and diversity validate that PDS consistently accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In particular, PDS can accelerate by up to 29x on more challenging high resolution (1024x1024) image generation.",
        "authors": [
            "He Ma",
            "Li Zhang",
            "Xiatian Zhu",
            "Jianfeng Feng"
        ],
        "citations": 22,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Unifying Generative Models with GFlowNets",
        "abstract": "There are many frameworks for deep generative modeling, each often presented with their own specific training algorithms and inference methods. Here, we demonstrate the connections between existing deep generative models and the recently introduced GFlowNet framework, a probabilistic inference machine which treats sampling as a decision-making process. This analysis sheds light on their overlapping traits and provides a unifying viewpoint through the lens of learning with Markovian trajectories. Our framework provides a means for unifying training and inference algorithms, and provides a route to shine a unifying light over many generative models. Beyond this, we provide a practical and experimentally verified recipe for improving generative modeling with insights from the GFlowNet perspective.",
        "authors": [
            "Dinghuai Zhang",
            "Ricky T. Q. Chen",
            "Nikolay Malkin",
            "Y. Bengio"
        ],
        "citations": 22,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Accelerating Score-based Generative Models for High-Resolution Image Synthesis",
        "abstract": "Score-based generative models (SGMs) have recently emerged as a promising class of generative models. The key idea is to produce high-quality images by recurrently adding Gaussian noises and gradients to a Gaussian sample until converging to the target distribution, a.k.a. the diffusion sampling. To ensure stability of convergence in sampling and generation quality, however, this sequential sampling process has to take a small step size and many sampling iterations (e.g., 2000). Several acceleration methods have been proposed with focus on low-resolution generation. In this work, we consider the acceleration of high-resolution generation with SGMs, a more challenging yet more important problem. We prove theoretically that this slow convergence drawback is primarily due to the ignorance of the target distribution. Further, we introduce a novel Target Distribution Aware Sampling (TDAS) method by leveraging the structural priors in space and frequency domains. Extensive experiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can consistently accelerate state-of-the-art SGMs, particularly on more challenging high resolution (1024x1024) image generation tasks by up to 18.4x, whilst largely maintaining the synthesis quality. With fewer sampling iterations, TDAS can still generate good quality images. In contrast, the existing methods degrade drastically or even fails completely",
        "authors": [
            "He Ma",
            "Li Zhang",
            "Xiatian Zhu",
            "Jingfeng Zhang",
            "Jianfeng Feng"
        ],
        "citations": 13,
        "references": 34,
        "year": 2022
    },
    {
        "title": "GenLabel: Mixup Relabeling using Generative Models",
        "abstract": "Mixup is a data augmentation method that generates new data points by mixing a pair of input data. While mixup generally improves the prediction performance, it sometimes degrades the performance. In this paper, we first identify the main causes of this phenomenon by theoretically and empirically analyzing the mixup algorithm. To resolve this, we propose GenLabel, a simple yet effective relabeling algorithm designed for mixup. In particular, GenLabel helps the mixup algorithm correctly label mixup samples by learning the class-conditional data distribution using generative models. Via extensive theoretical and empirical analysis, we show that mixup, when used together with GenLabel, can effectively resolve the aforementioned phenomenon, improving the generalization performance and the adversarial robustness.",
        "authors": [
            "Jy-yong Sohn",
            "Liang Shang",
            "Hongxu Chen",
            "J. Moon",
            "Dimitris Papailiopoulos",
            "Kangwook Lee"
        ],
        "citations": 13,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Transframer: Arbitrary Frame Prediction with Generative Models",
        "abstract": "We present a general-purpose framework for image modelling and vision tasks based on probabilistic frame prediction. Our approach unifies a broad range of tasks, from image segmentation, to novel view synthesis and video interpolation. We pair this framework with an architecture we term Transframer, which uses U-Net and Transformer components to condition on annotated context frames, and outputs sequences of sparse, compressed image features. Transframer is the state-of-the-art on a variety of video generation benchmarks, is competitive with the strongest models on few-shot view synthesis, and can generate coherent 30 second videos from a single image without any explicit geometric information. A single generalist Transframer simultaneously produces promising results on 8 tasks, including semantic segmentation, image classification and optical flow prediction with no task-specific architectural components, demonstrating that multi-task computer vision can be tackled using probabilistic image models. Our approach can in principle be applied to a wide range of applications that require learning the conditional structure of annotated image-formatted data.",
        "authors": [
            "C. Nash",
            "João Carreira",
            "Jacob Walker",
            "Iain Barr",
            "Andrew Jaegle",
            "Mateusz Malinowski",
            "P. Battaglia"
        ],
        "citations": 36,
        "references": 71,
        "year": 2022
    },
    {
        "title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI",
        "abstract": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
        "authors": [
            "Chenshuang Zhang",
            "Chaoning Zhang",
            "Sheng Zheng",
            "Mengchun Zhang",
            "Maryam Qamar",
            "S. Bae",
            "In-So Kweon"
        ],
        "citations": 59,
        "references": 141,
        "year": 2023
    },
    {
        "title": "Design in the DARK: Learning Deep Generative Models for De Novo Protein Design",
        "abstract": "The design of novel protein sequences is providing paths towards the development of novel therapeutics and materials. At the forefront is the challenging field of de novo protein design, which looks to design protein sequences unlike those found in nature using general design methodologies. In this work, we develop a tool for de novo design, based on a deep generative sequence model, that rapidly samples novel protein sequences with diverse and ordered structures. To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences. The resulting model generalizes where models trained on natural sequences struggle and greatly improves on the efficiency of comparable sampling-based approaches. We further show how it can generate high quality candidates for de novo design problems and aid in the development of further novel design methods, in all, providing another step, amongst others, towards truly automated and intelligent protein design.",
        "authors": [
            "Lewis Moffat",
            "S. Kandathil",
            "David T. Jones"
        ],
        "citations": 25,
        "references": 82,
        "year": 2022
    },
    {
        "title": "The ArtBench Dataset: Benchmarking Generative Models with Artworks",
        "abstract": "We introduce ArtBench-10, the first class-balanced, high-quality, cleanly annotated, and standardized dataset for benchmarking artwork generation. It comprises 60,000 images of artwork from 10 distinctive artistic styles, with 5,000 training images and 1,000 testing images per style. ArtBench-10 has several advantages over previous artwork datasets. Firstly, it is class-balanced while most previous artwork datasets suffer from the long tail class distributions. Secondly, the images are of high quality with clean annotations. Thirdly, ArtBench-10 is created with standardized data collection, annotation, filtering, and preprocessing procedures. We provide three versions of the dataset with different resolutions ($32\\times32$, $256\\times256$, and original image size), formatted in a way that is easy to be incorporated by popular machine learning frameworks. We also conduct extensive benchmarking experiments using representative image synthesis models with ArtBench-10 and present in-depth analysis. The dataset is available at https://github.com/liaopeiyuan/artbench under a Fair Use license.",
        "authors": [
            "Peiyuan Liao",
            "Xiuyu Li",
            "Xihui Liu",
            "K. Keutzer"
        ],
        "citations": 34,
        "references": 59,
        "year": 2022
    },
    {
        "title": "MAUVE Scores for Generative Models: Theory and Practice",
        "abstract": "Generative artificial intelligence has made significant strides, producing text indistinguishable from human prose and remarkably photorealistic images. Automatically measuring how close the generated data distribution is to the target distribution is central to diagnosing existing models and developing better ones. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore three approaches to statistically estimate these scores: vector quantization, non-parametric estimation, and classifier-based estimation. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of $f$-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We demonstrate in the vision domain that MAUVE can identify known properties of generated images on par with or better than existing metrics. In conclusion, we present practical recommendations for using MAUVE effectively with language and image modalities.",
        "authors": [
            "Krishna Pillutla",
            "Lang Liu",
            "John Thickstun",
            "S. Welleck",
            "Swabha Swayamdipta",
            "Rowan Zellers",
            "Sewoong Oh",
            "Yejin Choi",
            "Zaïd Harchaoui"
        ],
        "citations": 17,
        "references": 116,
        "year": 2022
    },
    {
        "title": "A generalized framework for lung Cancer classification based on deep generative models",
        "abstract": null,
        "authors": [
            "W. Salama",
            "A. Shokry",
            "M. Aly"
        ],
        "citations": 18,
        "references": 35,
        "year": 2022
    },
    {
        "title": "Microstructure reconstruction using diffusion-based generative models",
        "abstract": "Microstructure reconstruction has been an essential part of computational material engineering to reveal the relationship between microstructures and material properties. However, finding a general solution for microstructure characterization and reconstruction (MCR) tasks is still challenging, although there have been many attempts such as the descriptor-based MCR methods. To address this generality problem, the denoising diffusion models are first employed for the microstructure reconstruction task in this study. The applicability of the diffusion-based models is validated with several types of microstructures (e.g., polycrystalline alloy, carbonate, ceramics, copolymer, fiber composite, etc.) that have different morphological characteristics. The quality of the generated images is assessed with the quantitative evaluation metrics (FID score, precision, and recall) and the conventional statistical microstructure descriptors. Furthermore, the formulation of implicit probabilistic models (which yields non-Markovian diffusion processes) is adopted to accelerate the sampling process, thereby controlling the computational cost considering the practicability and reliability. The results show that the denoising diffusion models are well applicable to the reconstruction of various types of microstructures with different spatial distributions and morphological features. The diffusion-based approach provides a stable training process with simple implementation for generating visually similar and statistically equivalent microstructures. In these regards, the diffusion model has great potential to be used as a universal microstructure reconstruction method for handling complex microstructures for materials science.",
        "authors": [
            "Kang-Hyun Lee",
            "G. Yun"
        ],
        "citations": 35,
        "references": 85,
        "year": 2022
    },
    {
        "title": "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models",
        "abstract": "Detecting fake images is becoming a major goal of computer vision. This need is becoming more and more pressing with the continuous improvement of synthesis methods based on Generative Adversarial Networks (GAN), and even more with the appearance of powerful methods based on Diffusion Models (DM). Towards this end, it is important to gain insight into which image features better discriminate fake images from real ones. In this paper we report on our systematic study of a large number of image generators of different families, aimed at discovering the most forensically relevant characteristics of real and generated images. Our experiments provide a number of interesting observations and shed light on some intriguing properties of synthetic images: (1) not only the GAN models but also the DM and VQ-GAN (Vector Quantized Generative Adversarial Networks) models give rise to visible artifacts in the Fourier domain and exhibit anomalous regular patterns in the autocorrelation; (2) when the dataset used to train the model lacks sufficient variety, its biases can be transferred to the generated images; (3) synthetic and real images exhibit significant differences in the mid-high frequency signal content, observable in their radial and angular spectral power distributions.",
        "authors": [
            "Riccardo Corvi",
            "D. Cozzolino",
            "G. Poggi",
            "Koki Nagano",
            "L. Verdoliva"
        ],
        "citations": 62,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification",
        "abstract": "Diffusion models have been widely adopted in image generation, producing higher-quality and more diverse samples than generative adversarial networks (GANs). We introduce a latent diffusion model (LDM) for precipitation nowcasting - short-term forecasting based on the latest observational data. The LDM is more stable and requires less computation to train than GANs, albeit with more computationally expensive generation. We benchmark it against the GAN-based Deep Generative Models of Rainfall (DGMR) and a statistical model, PySTEPS. The LDM produces more accurate precipitation predictions, while the comparisons are more mixed when predicting whether the precipitation exceeds predefined thresholds. The clearest advantage of the LDM is that it generates more diverse predictions than DGMR or PySTEPS. Rank distribution tests indicate that the distribution of samples from the LDM accurately reflects the uncertainty of the predictions. Thus, LDMs are promising for any applications where uncertainty quantification is important, such as weather and climate.",
        "authors": [
            "J. Leinonen",
            "U. Hamann",
            "D. Nerini",
            "U. Germann",
            "Gabriele Franch"
        ],
        "citations": 42,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Model Inversion Attack by Integration of Deep Generative Models: Privacy-Sensitive Face Generation From a Face Recognition System",
        "abstract": "Cybersecurity in front of attacks to a face recognition system is an emerging issue in the cloud era, especially due to its strong bonds with the privacy of the users registered to the system. A possible attack is the model inversion attack (MIA) which aims to reveal the identity of a targeted user by generating the most proper datapoint input to the system with maximum corresponding confidence score at the output. The generated data of a registered user can be maliciously used as a serious invasion of the user privacy. In literature, MIA processes are categorized into white-box and black-box scenarios which are respectively with and without information about the system structure, parameters, and partially about the users. This research work assumes the MIA under semi-white box scenario of availability of system model structure and parameters but not any user data information, and verifies it as a severe threat even for a deep-learning-based face recognition system despite its complex structure and the diversity of registered user data. The alert state is promoted by Deep MIA which is the integration of deep generative models in MIA, and <inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula>-GAN integrated MIA-initilized by a face based seed (<inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula>-GAN-MIA-FS) is proposed. As a novel MIA search strategy, a pre-trained deep generative model with capability of generating a face image from a random feature vector is used for narrowing down the image search space to the feature vectors space, which has much lower dimensions. This allows the MIA process to efficiently search for a low-dimensional feature vector whose corresponding face image maximizes the confidence score. We have experimentally evaluated the proposed method by two objective criteria and three subjective criteria in comparison to <inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula>-GAN-integrated MIA initialized with a random seed (<inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula>-GAN-MIA-RS), DCGAN-integrated MIA (DCGAN-MIA), and the conventional MIA. The evaluation results approve the efficiency and superiority of the proposed technique in generating natural looking face clones with high recognizability as the targeted users.",
        "authors": [
            "M. Khosravy",
            "Kazuaki Nakamura",
            "Yuki Hirose",
            "Naoko Nitta",
            "N. Babaguchi"
        ],
        "citations": 46,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Transferable Generative Models",
        "abstract": null,
        "authors": [
            "Ajay Jain"
        ],
        "citations": 0,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Assessing Deep Generative Models in Chemical Composition Space",
        "abstract": "The computational discovery of novel materials has been one of the main motivations behind research in theoretical chemistry for several decades. Despite much eﬀort, this is far from a solved problem, however. Among other reasons, this is due to the enormous space of possible structures and compositions that could potentially be of interest. In the case of inorganic materials, this is exacerbated by the combinatorics of the periodic table, since even a single crystal structure can in principle display millions of compositions. Consequently, there is a need for tools that enable a more guided exploration of the materials design space. Here, generative machine learning (ML) models have recently emerged as a promising technology. In this work, we assess the performance of a range of deep generative models based on Reinforcement Learning (RL), Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN) for the prototypical case of designing Elpasolite compositions with low formation energies. By relying on the fully enumerated space of 2 million main group Elpasolites, the precision, coverage and diversity of the generated materials is rigorously assessed. Additionally, a hyperparameter selection scheme for generative models in chemical composition space is developed.",
        "authors": [
            "Hanna Türk",
            "Elisabetta Landini",
            "C. Kunkel",
            "Johannes T. Margraf",
            "K. Reuter"
        ],
        "citations": 12,
        "references": 90,
        "year": 2022
    },
    {
        "title": "Diffeomorphic Counterfactuals With Generative Models",
        "abstract": "Counterfactuals can explain classification decisions of neural networks in a human interpretable way. We propose a simple but effective method to generate such counterfactuals. More specifically, we perform a suitable diffeomorphic coordinate transformation and then perform gradient ascent in these coordinates to find counterfactuals which are classified with great confidence as a specified target class. We propose two methods to leverage generative models to construct such suitable coordinate systems that are either exactly or approximately diffeomorphic. We analyze the generation process theoretically using Riemannian differential geometry and validate the quality of the generated counterfactuals using various qualitative and quantitative measures.",
        "authors": [
            "Ann-Kathrin Dombrowski",
            "Jan E. Gerken",
            "Klaus-Robert Müller",
            "P. Kessel"
        ],
        "citations": 12,
        "references": 119,
        "year": 2022
    },
    {
        "title": "Enhancing Deep Reinforcement Learning: A Tutorial on Generative Diffusion Models in Network Optimization",
        "abstract": "Generative Diffusion Models (GDMs) have emerged as a transformative force in the realm of Generative Artificial Intelligence (GenAI), demonstrating their versatility and efficacy across various applications. The ability to model complex data distributions and generate high-quality samples has made GDMs particularly effective in tasks such as image generation and reinforcement learning. Furthermore, their iterative nature, which involves a series of noise addition and denoising steps, is a powerful and unique approach to learning and generating data. This paper serves as a comprehensive tutorial on applying GDMs in network optimization tasks. We delve into the strengths of GDMs, emphasizing their wide applicability across various domains, such as vision, text, and audio generation. We detail how GDMs can be effectively harnessed to solve complex optimization problems inherent in networks. The paper first provides a basic background of GDMs and their applications in network optimization. This is followed by a series of case studies, showcasing the integration of GDMs with Deep Reinforcement Learning (DRL), incentive mechanism design, Semantic Communications (SemCom), Internet of Vehicles (IoV) networks, etc. These case studies underscore the practicality and efficacy of GDMs in real-world scenarios, offering insights into network design. We conclude with a discussion on potential future directions for GDM research and applications, providing major insights into how they can continue to shape the future of network optimization.",
        "authors": [
            "Hongyang Du",
            "Ruichen Zhang",
            "Yinqiu Liu",
            "Jiacheng Wang",
            "Yi-Lan Lin",
            "Zonghang Li",
            "Dusist Niyato",
            "Jiawen Kang",
            "Zehui Xiong",
            "Shuguang Cui",
            "Bo Ai",
            "Haibo Zhou",
            "Dong In Kim"
        ],
        "citations": 52,
        "references": 238,
        "year": 2023
    },
    {
        "title": "EigenFold: Generative Protein Structure Prediction with Diffusion Models",
        "abstract": "Protein structure prediction has reached revolutionary levels of accuracy on single structures, yet distributional modeling paradigms are needed to capture the conformational ensembles and flexibility that underlie biological function. Towards this goal, we develop EigenFold, a diffusion generative modeling framework for sampling a distribution of structures from a given protein sequence. We define a diffusion process that models the structure as a system of harmonic oscillators and which naturally induces a cascading-resolution generative process along the eigenmodes of the system. On recent CAMEO targets, EigenFold achieves a median TMScore of 0.84, while providing a more comprehensive picture of model uncertainty via the ensemble of sampled structures relative to existing methods. We then assess EigenFold's ability to model and predict conformational heterogeneity for fold-switching proteins and ligand-induced conformational change. Code is available at https://github.com/bjing2016/EigenFold.",
        "authors": [
            "Bowen Jing",
            "Ezra Erives",
            "Peter Pao-Huang",
            "Gabriele Corso",
            "B. Berger",
            "T. Jaakkola"
        ],
        "citations": 51,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Revisiting flow generative models for Out-of-distribution detection",
        "abstract": null,
        "authors": [
            "Dihong Jiang",
            "Sun Sun",
            "Yaoliang Yu"
        ],
        "citations": 30,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Performance of Generative Large Language Models on Ophthalmology Board Style Questions.",
        "abstract": null,
        "authors": [
            "Louis Z Cai",
            "Abdulla R. Shaheen",
            "Andrew C. Jin",
            "Riya Fukui",
            "Jonathan S. Yi",
            "Nicolas A. Yannuzzi",
            "C. Alabiad"
        ],
        "citations": 73,
        "references": 8,
        "year": 2023
    },
    {
        "title": "Large Generative AI Models for Telecom: The Next Big Thing?",
        "abstract": "The evolution of generative artificial intelligence (GenAI) constitutes a turning point in reshaping the future of technology in different aspects. Wireless networks, in particular, with the blooming of self-evolving networks, represent a rich field for exploiting GenAI and reaping several benefits that can fundamentally change the way wireless networks are designed and operated nowadays. To be specific, large GenAI models are envisioned to open up a new era of autonomous wireless networks, in which multi-modal GenAI models trained over various Telecom data, can be fine-tuned to perform several downstream tasks, eliminating the need for building and training dedicated AI models for each specific task, and paving the way for the realization of artificial general intelligence (AGI)-empowered wireless networks. In this article, we aim to unfold the opportunities that can be reaped from integrating large GenAI models into the Telecom domain. In particular, we first highlight the applications of large GenAI models in future wireless networks, defining potential use-cases and revealing insights on the associated theoretical and practical challenges. Furthermore, we unveil how 6G can open up new opportunities through connecting multiple on-device large GenAI models, and hence, pave the way to the collective intelligence paradigm. Finally, we put a forward-looking vision of how large GenAI models will be the key to realize self-evolving networks.",
        "authors": [
            "Lina Bariah",
            "Qiyang Zhao",
            "Han Zou",
            "Yu Tian",
            "F. Bader",
            "M. Debbah"
        ],
        "citations": 45,
        "references": 15,
        "year": 2023
    },
    {
        "title": "Introducing nonlinear activations into quantum generative models",
        "abstract": "Due to the linearity of quantum mechanics, it remains a challenge to design quantum generative machine learning models that embed non-linear activations into the evolution of the statevector. However, some of the most successful classical generative models, such as those based on neural networks, involve highly non-linear dynamics for quality training. In this paper, we explore the effect of these dynamics in quantum generative modeling by introducing a model that adds non-linear activations via a neural network structure onto the standard Born Machine framework - the Quantum Neuron Born Machine (QNBM). To achieve this, we utilize a previously introduced Quantum Neuron subroutine, which is a repeat-until-success circuit with mid-circuit measurements and classical control. After introducing the QNBM, we investigate how its performance depends on network size, by training a 3-layer QNBM with 4 output neurons and various input and hidden layer sizes. We then compare our non-linear QNBM to the linear Quantum Circuit Born Machine (QCBM). We allocate similar time and memory resources to each model, such that the only major difference is the qubit overhead required by the QNBM. With gradient-based training, we show that while both models can easily learn a trivial uniform probability distribution, on a more challenging class of distributions, the QNBM achieves an almost 3x smaller error rate than a QCBM with a similar number of tunable parameters. We therefore provide evidence that suggests that non-linearity is a useful resource in quantum generative models, and we put forth the QNBM as a new model with good generative performance and potential for quantum advantage.",
        "authors": [
            "Kaitlin Gili",
            "Mykolas Sveistrys",
            "C. Ballance"
        ],
        "citations": 10,
        "references": 37,
        "year": 2022
    },
    {
        "title": "Score-Based Generative Models for Molecule Generation",
        "abstract": "Recent advances in generative models have made exploring design spaces easier for de novo molecule generation. However, popular generative models like GANs and normalizing flows face challenges such as training instabilities due to adversarial training and architectural constraints, respectively. Score-based generative models sidestep these challenges by modelling the gradient of the log probability density using a score function approximation, as opposed to modelling the density function directly, and sampling from it using annealed Langevin Dynamics. We believe that score-based generative models could open up new opportunities in molecule generation due to their architectural flexibility, such as replacing the score function with an SE(3) equivariant model. In this work, we lay the foundations by testing the efficacy of score-based models for molecule generation. We train a Transformer-based score function on Self-Referencing Embedded Strings (SELFIES) representations of 1.5 million samples from the ZINC dataset and use the Moses benchmarking framework to evaluate the generated samples on a suite of metrics.",
        "authors": [
            "Dwaraknath Gnaneshwar",
            "Bharath Ramsundar",
            "Dhairya Gandhi",
            "Rachel C. Kurchin",
            "V. Viswanathan"
        ],
        "citations": 11,
        "references": 21,
        "year": 2022
    },
    {
        "title": "Comparing the latent space of generative models",
        "abstract": null,
        "authors": [
            "A. Asperti",
            "Valerio Tonelli"
        ],
        "citations": 11,
        "references": 105,
        "year": 2022
    },
    {
        "title": "Evaluating Generalization in Classical and Quantum Generative Models",
        "abstract": ",",
        "authors": [
            "Kaitlin Gili",
            "M. Mauri",
            "A. Perdomo-Ortiz"
        ],
        "citations": 22,
        "references": 73,
        "year": 2022
    },
    {
        "title": "On the Evaluation of Generative Models in High Energy Physics",
        "abstract": "There has been a recent explosion in research into machine-learning-based generative modeling to tackle computational challenges for simulations in high energy physics (HEP). In order to use such alternative simulators in practice, we need well deﬁned metrics to compare diﬀerent generative models and evaluate their discrepancy from the true distributions. We present the ﬁrst systematic review and investigation into evaluation metrics and their sensitivity to failure modes of generative models, using the framework of two-sample goodness-of-ﬁt testing, and their relevance and viability for HEP. Inspired by previous work in both physics and computer vision, we propose two new metrics, the Fr´echet and kernel physics distances (FPD and KPD), and perform a variety of experiments measuring their performance on simple Gaussian-distributed, and simulated high energy jet datasets. We ﬁnd FPD, in particular, to be the most sensitive metric to all alternative jet distributions tested and recommend its adoption, along with the KPD and Wasserstein distances between individual feature distributions, for evaluating generative models in HEP. We ﬁnally demonstrate the eﬃcacy of these proposed metrics in evaluating and comparing a novel attention-based generative adversarial particle transfomer to the state-of-the-art message-passing generative adversarial network jet simulation model.",
        "authors": [
            "Raghav Kansal",
            "Anni Li",
            "Javier Mauricio Duarte",
            "N. Chernyavskaya",
            "M. Pierini",
            "B. Orzari",
            "T. Tomei"
        ],
        "citations": 8,
        "references": 85,
        "year": 2022
    },
    {
        "title": "Disentangled Spatiotemporal Graph Generative Models",
        "abstract": "Spatiotemporal graph represents a crucial data structure where the nodes and edges are embedded in a geometric space and their attribute values can evolve dynamically over time. Nowadays, spatiotemporal graph data is becoming increasingly popular and important, ranging from microscale (e.g. protein folding), to middle-scale (e.g. dynamic functional connectivity), to macro-scale (e.g. human mobility network). Although disentangling and understanding the correlations among spatial, temporal, and graph aspects have been a long-standing key topic in network science, they typically rely on network processes hypothesized by human knowledge. They usually fit well towards the properties that the predefined principles are tailored for, but usually cannot do well for the others, especially for many key domains where the human has yet very limited knowledge such as protein folding and biological neuronal networks. In this paper, we aim at pushing forward the modeling and understanding of spatiotemporal graphs via new disentangled deep generative models. Specifically, a new Bayesian model is proposed that factorizes spatiotemporal graphs into spatial, temporal, and graph factors as well as the factors that explain the interplay among them. A variational objective function and new mutual information thresholding algorithms driven by information bottleneck theory have been proposed to maximize the disentanglement among the factors with theoretical guarantees. Qualitative and quantitative experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed model over the state-of-the-arts by up to 69.2% for graph generation and 41.5% for interpretability.",
        "authors": [
            "Yuanqi Du",
            "Xiaojie Guo",
            "Hengning Cao",
            "Yanfang Ye",
            "Liang Zhao"
        ],
        "citations": 19,
        "references": 63,
        "year": 2022
    },
    {
        "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances",
        "abstract": "The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them cheaply. This paper aims to reduce the monetary cost for serving LLMs by leveraging preemptible GPU instances on modern clouds, which offer accesses to spare GPU resources at a much cheaper price than regular instances but may be preempted by the cloud provider at any time. Serving LLMs on preemptible instances requires addressing challenges induced by frequent instance preemptions and the necessity of migrating instances to handle the preemptions. This paper presents SpotServe, the first distributed LLM serving system on preemptible instances. Several key techniques of SpotServe realize fast and reliable serving of generative LLMs on cheap preemptible instances. First, SpotServe dynamically adapts the LLM parallelization configuration for dynamic instance availability and fluctuating workload, while balancing the trade-off among the overall throughput, inference latency and monetary costs. Second, to minimize the cost of migrating instances for dynamic reparallelization, the task of migrating instances is formulated as a bipartite graph matching problem in SpotServe, which uses the Kuhn-Munkres algorithm to identify an optimal migration plan that minimizes communication cost. Finally, to take advantage of the grace period offered by modern cloud platforms, we introduce stateful inference recovery, a new inference mechanism that commits inference progress at a much finer granularity and allows SpotServe to cheaply resume inference upon preemption. We evaluate SpotServe on real spot instance preemption traces and various popular LLMs and show that SpotServe can reduce the P99 tail latency by 2.4 - 9.1× compared with the best existing LLM serving systems. We also show that SpotServe can leverage the price advantage of preemptive instances, saving 54% monetary cost compared with only using on-demand instances. The code is publicly available at: https://github.com/Hsword/SpotServe.",
        "authors": [
            "Xupeng Miao",
            "Chunan Shi",
            "Jiangfei Duan",
            "Xiaoli Xi",
            "Dahua Lin",
            "Bin Cui",
            "Zhihao Jia"
        ],
        "citations": 42,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Generative Semantic Communication: Diffusion Models Beyond Bit Recovery",
        "abstract": "Semantic communication is expected to be one of the cores of next-generation AI-based communications. One of the possibilities offered by semantic communication is the capability to regenerate, at the destination side, images or videos semantically equivalent to the transmitted ones, without necessarily recovering the transmitted sequence of bits. The current solutions still lack the ability to build complex scenes from the received partial information. Clearly, there is an unmet need to balance the effectiveness of generation methods and the complexity of the transmitted information, possibly taking into account the goal of communication. In this paper, we aim to bridge this gap by proposing a novel generative diffusion-guided framework for semantic communication that leverages the strong abilities of diffusion models in synthesizing multimedia content while preserving semantic features. We reduce bandwidth usage by sending highly-compressed semantic information only. Then, the diffusion model learns to synthesize semantic-consistent scenes through spatially-adaptive normalizations from such denoised semantic information. We prove, through an in-depth assessment of multiple scenarios, that our method outperforms existing solutions in generating high-quality images with preserved semantic information even in cases where the received content is significantly degraded. More specifically, our results show that objects, locations, and depths are still recognizable even in the presence of extremely noisy conditions of the communication channel. The code is available at https://github.com/ispamm/GESCO.",
        "authors": [
            "Eleonora Grassucci",
            "S. Barbarossa",
            "D. Comminiello"
        ],
        "citations": 42,
        "references": 49,
        "year": 2023
    },
    {
        "title": "DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models",
        "abstract": "Magnetic resonance imaging (MRI) is a common and life-saving medical imaging technique. However, acquiring high signal-to-noise ratio MRI scans requires long scan times, resulting in increased costs and patient discomfort, and decreased throughput. Thus, there is great interest in denoising MRI scans, especially for the subtype of diffusion MRI scans that are severely SNR-limited. While most prior MRI denoising methods are supervised in nature, acquiring supervised training datasets for the multitude of anatomies, MRI scanners, and scan parameters proves impractical. Here, we propose Denoising Diffusion Models for Denoising Diffusion MRI (DDM$^2$), a self-supervised denoising method for MRI denoising using diffusion denoising generative models. Our three-stage framework integrates statistic-based denoising theory into diffusion models and performs denoising through conditional generation. During inference, we represent input noisy measurements as a sample from an intermediate posterior distribution within the diffusion Markov chain. We conduct experiments on 4 real-world in-vivo diffusion MRI datasets and show that our DDM$^2$ demonstrates superior denoising performances ascertained with clinically-relevant visual qualitative and quantitative metrics.",
        "authors": [
            "Tiange Xiang",
            "Mahmut Yurt",
            "Ali B. Syed",
            "K. Setsompop",
            "A. Chaudhari"
        ],
        "citations": 36,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Can Generative Large Language Models Perform ASR Error Correction?",
        "abstract": "ASR error correction is an interesting option for post processing speech recognition system outputs. These error correction models are usually trained in a supervised fashion using the decoding results of a target ASR system. This approach can be computationally intensive and the model is tuned to a specific ASR system. Recently generative large language models (LLMs) have been applied to a wide range of natural language processing tasks, as they can operate in a zero-shot or few shot fashion. In this paper we investigate using ChatGPT, a generative LLM, for ASR error correction. Based on the ASR N-best output, we propose both unconstrained and constrained, where a member of the N-best list is selected, approaches. Additionally, zero and 1-shot settings are evaluated. Experiments show that this generative LLM approach can yield performance gains for two different state-of-the-art ASR architectures, transducer and attention-encoder-decoder based, and multiple test sets.",
        "authors": [
            "Rao Ma",
            "Mengjie Qian",
            "Potsawee Manakul",
            "M. Gales",
            "K. Knill"
        ],
        "citations": 38,
        "references": 37,
        "year": 2023
    },
    {
        "title": "SpectralDiff: A Generative Framework for Hyperspectral Image Classification With Diffusion Models",
        "abstract": "Hyperspectral image (HSI) classification is an important issue in remote sensing field with extensive applications in Earth science. In recent years, a large number of deep learning-based HSI classification methods have been proposed. However, the existing methods have limited ability to handle high-dimensional, highly redundant, and complex data, making it challenging to capture the spectral–spatial distributions of data and relationships between samples. To address this issue, we propose a generative framework for HSI classification with diffusion models (SpectralDiff) that effectively mines the distribution information of high-dimensional and highly redundant data by iteratively denoising and explicitly constructing the data generation process, thus better reflecting the relationships between samples. The framework consists of a spectral–spatial diffusion module and an attention-based classification module. The spectral–spatial diffusion module adopts forward and reverse spectral–spatial diffusion processes to achieve adaptive construction of sample relationships without requiring prior knowledge of graphical structure or neighborhood information. It captures spectral–spatial distribution and contextual information of objects in HSI and mines unsupervised spectral–spatial diffusion features within the reverse diffusion process. Finally, these features are fed into the attention-based classification module for per-pixel classification. The diffusion features can facilitate cross-sample perception via reconstruction distribution, leading to improved classification performance. Experiments on three public HSI datasets demonstrate that the proposed method can achieve better performance than state-of-the-art methods. For the sake of reproducibility, the source code of SpectralDiff will be publicly available at https://github.com/chenning0115/SpectralDiff.",
        "authors": [
            "Ning Chen",
            "Jun Yue",
            "Leyuan Fang",
            "Shaobo Xia"
        ],
        "citations": 41,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Large Language Models for Generative Recommendation: A Survey and Visionary Discussions",
        "abstract": "Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields, e.g., recommender systems (RS). However, most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor), which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages, such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods, and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS tasks. We hope that this survey can provide the context and guidance needed to explore this interesting and emerging topic.",
        "authors": [
            "Lei Li",
            "Yongfeng Zhang",
            "Dugang Liu",
            "L. Chen"
        ],
        "citations": 53,
        "references": 112,
        "year": 2023
    },
    {
        "title": "SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models",
        "abstract": "Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization. Leveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery. In addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing. However, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities. In this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation. We introduce SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for both image and video data. Thanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation across six datasets. Furthermore, our learned object features can be utilized by existing object-centric dynamics models, improving video prediction quality and downstream temporal reasoning tasks. Finally, we demonstrate the scalability of SlotDiffusion to unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated with self-supervised pre-trained image encoders.",
        "authors": [
            "Ziyi Wu",
            "Jingyu Hu",
            "Wuyue Lu",
            "Igor Gilitschenski",
            "Animesh Garg"
        ],
        "citations": 35,
        "references": 118,
        "year": 2023
    },
    {
        "title": "Design Ideation with AI - Sketching, Thinking and Talking with Generative Machine Learning Models",
        "abstract": "Generative machine learning models provide opportunities to support design work in various parts of the design process. This study investigates how generative machine learning and large language models may play a part in creative design processes of ideation, early prototyping and sketching. A workshop was conducted in which design practitioners and design researchers developed design concepts for a provided design case, with the help of GPT-3. The findings point to three main themes, including i) the practical usefulness and limitations of the system in design ideation processes, ii) how the form of user interaction shapes users’ expectations of the system’s capabilities and potentials, and iii), how the broader discourse around AI both limits and enables how co-creative processes involving human and AI unfolds. The discussion outlines design implications and alternative framings of this kind of co-creative design practices based on post-human perspectives on design and technology use.",
        "authors": [
            "Jakob Tholander",
            "Martin Jonsson"
        ],
        "citations": 45,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models",
        "abstract": "Long-horizon tasks, usually characterized by complex subtask dependencies, present a significant challenge in manipulation planning. Skill chaining is a practical approach to solving unseen tasks by combining learned skill priors. However, such methods are myopic if sequenced greedily and face scalability issues with search-based planning strategy. To address these challenges, we introduce Generative Skill Chaining~(GSC), a probabilistic framework that learns skill-centric diffusion models and composes their learned distributions to generate long-horizon plans during inference. GSC samples from all skill models in parallel to efficiently solve unseen tasks while enforcing geometric constraints. We evaluate the method on various long-horizon tasks and demonstrate its capability in reasoning about action dependencies, constraint handling, and generalization, along with its ability to replan in the face of perturbations. We show results in simulation and on real robot to validate the efficiency and scalability of GSC, highlighting its potential for advancing long-horizon task planning. More details are available at: https://generative-skill-chaining.github.io/",
        "authors": [
            "Utkarsh Aashu Mishra",
            "Shangjie Xue",
            "Yongxin Chen",
            "Danfei Xu"
        ],
        "citations": 48,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Why open-source generative AI models are an ethical way forward for science",
        "abstract": null,
        "authors": [
            "A. Spirling"
        ],
        "citations": 58,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Generative Diffusion Models on Graphs: Methods and Applications",
        "abstract": "Diffusion models, as a novel generative paradigm, have achieved remarkable success in various image generation tasks such as image inpainting, image-to-text translation, and video generation. Graph generation is a crucial computational task on graphs with numerous real-world applications. It aims to learn the distribution of given graphs and then generate new graphs. Given the great success of diffusion models in image generation, increasing efforts have been made to leverage these techniques to advance graph generation in recent years. In this paper, we first provide a comprehensive overview of generative diffusion models on graphs, In particular, we review representative algorithms for three variants of graph diffusion models, i.e., Score Matching with Langevin Dynamics (SMLD), Denoising Diffusion Probabilistic Model (DDPM), and Score-based Generative Model (SGM). Then, we summarize the major applications of generative diffusion models on graphs with a specific focus on molecule and protein modeling. Finally, we discuss promising directions in generative diffusion models on graph-structured data.",
        "authors": [
            "Wenqi Fan",
            "C. Liu",
            "Yunqing Liu",
            "Jiatong Li",
            "Hang Li",
            "Hui Liu",
            "Jiliang Tang",
            "Qing Li"
        ],
        "citations": 49,
        "references": 122,
        "year": 2023
    },
    {
        "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
        "abstract": "As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.",
        "authors": [
            "David Esiobu",
            "X. Tan",
            "Saghar Hosseini",
            "Megan Ung",
            "Yuchen Zhang",
            "Jude Fernandes",
            "Jane Dwivedi-Yu",
            "Eleonora Presani",
            "Adina Williams",
            "Eric Michael Smith"
        ],
        "citations": 38,
        "references": 0,
        "year": 2023
    },
    {
        "title": "DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models",
        "abstract": "Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, DiffusionShield ensures low distortion of the original image, high watermark detection performance, and the ability to embed lengthy messages. We conduct rigorous and comprehensive experiments to show the effectiveness of DiffusionShield in defending against infringement by GDMs and its superiority over traditional watermarking methods. The code for DiffusionShield is accessible in https://github.com/Yingqiancui/DiffusionShield.",
        "authors": [
            "Yingqian Cui",
            "J. Ren",
            "Han Xu",
            "Pengfei He",
            "Hui Liu",
            "Lichao Sun",
            "Jiliang Tang"
        ],
        "citations": 41,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Combating Misinformation in the Era of Generative AI Models",
        "abstract": "Misinformation has been a persistent and harmful phenomenon affecting our society in various ways, including individuals' physical health and economic stability. With the rise of short video platforms and related applications, the spread of multi-modal misinformation, encompassing images, texts, audios, and videos have exacerbated these concerns. The introduction of generative AI models like ChatGPT and Stable Diffusion has further complicated matters, giving rise to Artificial Intelligence Generated Content (AIGC) and presenting new challenges in detecting and mitigating misinformation. Consequently, traditional approaches to misinformation detection and intervention have become inadequate in this evolving landscape. This paper explores the challenges posed by AIGC in the context of misinformation. It examines the issue from psychological and societal perspectives, and explores the subtle manipulation traces found in AIGC at signal, perceptual, semantic, and human levels. By scrutinizing manipulation traces such as signal manipulation, semantic inconsistencies, logical incoherence, and psychological strategies, our objective is to tackle AI-generated misinformation and provide a conceptual design of systematic explainable solution. Ultimately, we aim for this paper to contribute valuable insights into combating misinformation, particularly in the era of AIGC.",
        "authors": [
            "Danni Xu",
            "Shaojing Fan",
            "Mohan S. Kankanhalli"
        ],
        "citations": 39,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy",
        "abstract": "The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA [1] and GDPR [2]. Synthetic data generation, using generative AI models like GANs [3] and VAEs [4], offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training [5], explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.",
        "authors": [
            "Aryan Jadon",
            "Shashank Kumar"
        ],
        "citations": 33,
        "references": 29,
        "year": 2023
    },
    {
        "title": "A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material",
        "abstract": "Diffusion models have become a new SOTA generative modeling method in various fields, for which there are multiple survey works that provide an overall survey. With the number of articles on diffusion models increasing exponentially in the past few years, there is an increasing need for surveys of diffusion models on specific fields. In this work, we are committed to conducting a survey on the graph diffusion models. Even though our focus is to cover the progress of diffusion models in graphs, we first briefly summarize how other generative modeling methods are used for graphs. After that, we introduce the mechanism of diffusion models in various forms, which facilitates the discussion on the graph diffusion models. The applications of graph diffusion models mainly fall into the category of AI-generated content (AIGC) in science, for which we mainly focus on how graph diffusion models are utilized for generating molecules and proteins but also cover other cases, including materials design. Moreover, we discuss the issue of evaluating diffusion models in the graph domain and the existing challenges.",
        "authors": [
            "Mengchun Zhang",
            "Maryam Qamar",
            "Taegoo Kang",
            "Yuna Jung",
            "Chenshuang Zhang",
            "S. Bae",
            "Chaoning Zhang"
        ],
        "citations": 37,
        "references": 151,
        "year": 2023
    },
    {
        "title": "Multiscale Generative Models: Improving Performance of a Generative Model Using Feedback from Other Dependent Generative Models",
        "abstract": "Realistic fine-grained multi-agent simulation of real-world complex systems is crucial for many downstream tasks such as reinforcement learning. Recent work has used generative models (GANs in particular) for providing high-fidelity simulation of real-world systems. However, such generative models are often monolithic and miss out on modeling the interaction in multi-agent systems. In this work, we take a first step towards building multiple interacting generative models (GANs) that reflects the interaction in real world. We build and analyze a hierarchical set-up where a higher-level GAN is conditioned on the output of multiple lower-level GANs. We present a technique of using feedback from the higher-level GAN to improve performance of lower-level GANs. We mathematically characterize the conditions under which our technique is impactful, including understanding the transfer learning nature of our set-up. We present three distinct experiments on synthetic data, time series data, and image domain, revealing the wide applicability of our technique.",
        "authors": [
            "Changyu Chen",
            "Avinandan Bose",
            "Shih-Fen Cheng",
            "Arunesh Sinha"
        ],
        "citations": 0,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Capsule Networks as Generative Models",
        "abstract": null,
        "authors": [
            "Alex B. Kiefer",
            "Beren Millidge",
            "Alexander Tschantz",
            "C. Buckley"
        ],
        "citations": 0,
        "references": 52,
        "year": 2022
    },
    {
        "title": "Spontaneous symmetry breaking in generative diffusion models",
        "abstract": "\n Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking phenomenon that divides the generative dynamics into two distinct phases: (1) a linear steady-state dynamics around a central fixed-point and, (2) an attractor dynamics directed towards the data manifold. These two ‘phases’ are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3× Fréchet inception distance improvements on fast samplers, while also increasing sample diversity (e.g. racial composition of generated CelebA images). Our work offers a new way to understand the generative dynamics of diffusion models that has the potential to bring about higher performance and less biased fast-samplers.",
        "authors": [
            "G. Raya",
            "L. Ambrogioni"
        ],
        "citations": 22,
        "references": 48,
        "year": 2023
    },
    {
        "title": "HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models",
        "abstract": "Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the output transcription. The proposed benchmark contains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains a significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based methods. More surprisingly, LLM with reasonable prompt and its generative capability can even correct those tokens that are missing in N-best list. We make our results publicly accessible for reproducible pipelines with released pre-trained models, thus providing a new evaluation paradigm for ASR error correction with LLMs.",
        "authors": [
            "Cheng Chen",
            "Yuchen Hu",
            "Chao-Han Huck Yang",
            "Sabato Marco Siniscalchi",
            "Pin-Yu Chen",
            "E. Chng"
        ],
        "citations": 36,
        "references": 113,
        "year": 2023
    },
    {
        "title": "Using Captum to Explain Generative Language Models",
        "abstract": "Captum is a comprehensive library for model explainability in PyTorch, offering a range of methods from the interpretability literature to enhance users’ understanding of PyTorch models. In this paper, we introduce new features in Captum that are specifically designed to analyze the behavior of generative language models. We provide an overview of the available functionalities and example applications of their potential for understanding learned associations within generative language models.",
        "authors": [
            "Vivek Miglani",
            "Aobo Yang",
            "Aram H. Markosyan",
            "Diego Garcia-Olano",
            "Narine Kokhlikyan"
        ],
        "citations": 20,
        "references": 20,
        "year": 2023
    },
    {
        "title": "What Matters In The Structured Pruning of Generative Language Models?",
        "abstract": "Auto-regressive large language models such as GPT-3 require enormous computational resources to use. Traditionally, structured pruning methods are employed to reduce resource usage. However, their application to and efficacy for generative language models is heavily under-explored. In this paper we conduct an comprehensive evaluation of common structured pruning methods, including magnitude, random, and movement pruning on the feed-forward layers in GPT-type models. Unexpectedly, random pruning results in performance that is comparable to the best established methods, across multiple natural language generation tasks. To understand these results, we provide a framework for measuring neuron-level redundancy of models pruned by different methods, and discover that established structured pruning methods do not take into account the distinctiveness of neurons, leaving behind excess redundancies. In view of this, we introduce Globally Unique Movement (GUM) to improve the uniqueness of neurons in pruned models. We then discuss the effects of our techniques on different redundancy metrics to explain the improved performance.",
        "authors": [
            "Michael Santacroce",
            "Zixin Wen",
            "Yelong Shen",
            "Yuan-Fang Li"
        ],
        "citations": 27,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Probabilistic Generative Models",
        "abstract": "A generative model is a model that learns to imitate some patterngenerating process. That is, we have (high-dimensional) patterns x ∈ X that are generated according to a probability distribution p on X . We want to learn a model pθ that is indistinguishable from p. In particular, our model should (1) avoid missing patterns that are generated by p (this is called mode collapse) and (2) avoid generating patterns that are not supported by p. A famous example is the example of generating human faces, where we want to generate facial features proportional to how frequently they occur in real human faces.",
        "authors": [],
        "citations": 0,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Use of Generative Artificial Intelligence, Including Large Language Models Such as ChatGPT, in Scientific Publications: Policies of KJR and Prominent Authorities",
        "abstract": "Generative artificial intelligence (AI) refers to algorithms that can be used to create new content, such as text, code, images, videos, and audio. Particularly, with the introduction of generative adversarial networks (GAN) in medical imaging [1,2], generative AI has gained significant attention in the scientific community, leading to numerous publications in the past few years. The Korean Journal of Radiology (KJR) has published several articles on this topic [3-5]. However, the landscape of generative AI in scientific research and publication has dramatically shifted with the emergence of generative large language models (LLMs), such as ChatGPT, which are capable of generating text that closely resembles human writing and easily accessible to the public. The use of LLMs is rapidly expanding in scientific publications [6], creating ethical and legal concerns and challenges related to research integrity, plagiarism, copyright infringement, and authorship, not only for authors, but also for peer reviewers and editors [7-9]. Moreover, these concerns and challenges Use of Generative Artificial Intelligence, Including Large Language Models Such as ChatGPT, in Scientific Publications: Policies of KJR and Prominent Authorities",
        "authors": [
            "Seong Ho Park"
        ],
        "citations": 26,
        "references": 22,
        "year": 2023
    },
    {
        "title": "Probabilistic Generative Models",
        "abstract": "A generative model is a model that learns to imitate some patterngenerating process. That is, we have (high-dimensional) patterns x ∈ X that are generated according to a probability distribution p on X . We want to learn a model pθ that is indistinguishable from p. In particular, our model should (1) avoid missing patterns that are generated by p (this is called mode collapse) and (2) avoid generating patterns that are not supported by p. A famous example is the example of generating human faces, where we want to generate facial features proportional to how frequently they occur in real human faces.",
        "authors": [
            "Jonas Hübotter"
        ],
        "citations": 0,
        "references": 3,
        "year": 2022
    },
    {
        "title": "Unifying GANs and Score-Based Diffusion as Generative Particle Models",
        "abstract": "Particle-based deep generative models, such as gradient flows and score-based diffusion models, have recently gained traction thanks to their striking performance. Their principle of displacing particle distributions using differential equations is conventionally seen as opposed to the previously widespread generative adversarial networks (GANs), which involve training a pushforward generator network. In this paper we challenge this interpretation, and propose a novel framework that unifies particle and adversarial generative models by framing generator training as a generalization of particle models. This suggests that a generator is an optional addition to any such generative model. Consequently, integrating a generator into a score-based diffusion model and training a GAN without a generator naturally emerge from our framework. We empirically test the viability of these original models as proofs of concepts of potential applications of our framework.",
        "authors": [
            "Jean-Yves Franceschi",
            "Mike Gartrell",
            "Ludovic Dos Santos",
            "Thibaut Issenhuth",
            "Emmanuel de B'ezenac",
            "Mickaël Chen",
            "A. Rakotomamonjy"
        ],
        "citations": 16,
        "references": 83,
        "year": 2023
    },
    {
        "title": "Towards Realistic Generative 3D Face Models",
        "abstract": "In recent years, there has been significant progress in 2D generative face models fueled by applications such as animation, synthetic data generation, and digital avatars. However, due to the absence of 3D information, these 2D models often struggle to accurately disentangle facial attributes like pose, expression, and illumination, limiting their editing capabilities. To address this limitation, this paper proposes a 3D controllable generative face model to produce high-quality albedo and precise 3D shapes by leveraging existing 2D generative models. By combining 2D face generative models with semantic face manipulation, this method enables editing of detailed 3D rendered faces. The proposed framework utilizes an alternating descent optimization approach over shape and albedo. Differentiable rendering is used to train high-quality shapes and albedo without 3D supervision. Moreover, this approach outperforms most state-of-the-art (SOTA) methods in the well-known NoW and REALY benchmarks for 3D face re construction. It also outperforms the SOTA reconstruction models in recovering rendered faces’ identities across novel poses. Additionally, the paper demonstrates direct control of expressions in 3D faces by exploiting latent space leading to text-based editing of 3D faces.",
        "authors": [
            "Aashish Rai",
            "Hiresh Gupta",
            "Ayush Pandey",
            "Francisco Vicente Carrasco",
            "Shingo Takagi",
            "Amaury Aubel",
            "Daeil Kim",
            "Aayush Prakash",
            "F. D. L. Torre"
        ],
        "citations": 16,
        "references": 62,
        "year": 2023
    },
    {
        "title": "The Ethical Implications of Generative Audio Models: A Systematic Literature Review",
        "abstract": "Generative audio models typically focus their applications in music and speech generation, with recent models having human-like quality in their audio output. This paper conducts a systematic literature review of 884 papers in the area of generative audio models in order to both quantify the degree to which researchers in the field are considering potential negative impacts and identify the types of ethical implications researchers in this area need to consider. Though 65% of generative audio research papers note positive potential impacts of their work, less than 10% discuss any negative impacts. This jarringly small percentage of papers considering negative impact is particularly worrying because the issues brought to light by the few papers doing so are raising serious ethical implications and concerns relevant to the broader field such as the potential for fraud, deep-fakes, and copyright infringement. By quantifying this lack of ethical consideration in generative audio research and identifying key areas of potential harm, this paper lays the groundwork for future work in the field at a critical point in time in order to guide more conscientious research as this field progresses.",
        "authors": [
            "J. Barnett"
        ],
        "citations": 18,
        "references": 216,
        "year": 2023
    },
    {
        "title": "A Survey on Generative Diffusion Models",
        "abstract": "Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artificial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.",
        "authors": [
            "Hanqun Cao",
            "Cheng Tan",
            "Zhangyang Gao",
            "Yilun Xu",
            "Guangyong Chen",
            "P. Heng",
            "Stan Z. Li"
        ],
        "citations": 122,
        "references": 333,
        "year": 2022
    },
    {
        "title": "TaleBrush: Sketching Stories with Generative Pretrained Language Models",
        "abstract": "While advanced text generation algorithms (e.g., GPT-3) have enabled writers to co-create stories with an AI, guiding the narrative remains a challenge. Existing systems often leverage simple turn-taking between the writer and the AI in story development. However, writers remain unsupported in intuitively understanding the AI’s actions or steering the iterative generation. We introduce TaleBrush, a generative story ideation tool that uses line sketching interactions with a GPT-based language model for control and sensemaking of a protagonist’s fortune in co-created stories. Our empirical evaluation found our pipeline reliably controls story generation while maintaining the novelty of generated sentences. In a user study with 14 participants with diverse writing experiences, we found participants successfully leveraged sketching to iteratively explore and write stories according to their intentions about the character’s fortune while taking inspiration from generated stories. We conclude with a reflection on how sketching interactions can facilitate the iterative human-AI co-creation process.",
        "authors": [
            "John Joon Young Chung",
            "Wooseok Kim",
            "Kang Min Yoo",
            "Hwaran Lee",
            "Eytan Adar",
            "Minsuk Chang"
        ],
        "citations": 140,
        "references": 162,
        "year": 2022
    },
    {
        "title": "The statistical thermodynamics of generative diffusion models",
        "abstract": "Generative diffusion models have achieved spectacular performance in many areas of machine learning and generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, variational inference and stochastic calculus, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We show that these phase-transitions are always in a mean-field universality class, as they are the result of a self-consistency condition in the generative dynamics. We argue that the critical instability that arises from the phase transitions lies at the heart of their generative capabilities, which are characterized by a set of mean-field critical exponents. Finally, we show that the dynamic equation of the generative process can be interpreted as a stochastic adiabatic transformation that minimizes the free energy while keeping the system in thermal equilibrium.",
        "authors": [
            "Luca Ambrogioni"
        ],
        "citations": 11,
        "references": 36,
        "year": 2023
    },
    {
        "title": "RITA: a Study on Scaling Up Generative Protein Sequence Models",
        "abstract": "In this work we introduce RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database. Such generative models hold the promise of greatly accelerating protein design. We conduct the first systematic study of how capabilities evolve with model size for autoregressive transformers in the protein domain: we evaluate RITA models in next amino acid prediction, zero-shot fitness, and enzyme function prediction, showing benefits from increased scale. We release the RITA models openly, to the benefit of the research community.",
        "authors": [
            "Daniel Hesslow",
            "Niccoló Zanichelli",
            "Pascal Notin",
            "Iacopo Poli",
            "D. Marks"
        ],
        "citations": 75,
        "references": 45,
        "year": 2022
    },
    {
        "title": "GMMSeg: Gaussian Mixture based Generative Semantic Segmentation Models",
        "abstract": "Prevalent semantic segmentation solutions are, in essence, a dense discriminative classifier of p(class|pixel feature). Though straightforward, this de facto paradigm neglects the underlying data distribution p(pixel feature|class), and struggles to identify out-of-distribution data. Going beyond this, we propose GMMSeg, a new family of segmentation models that rely on a dense generative classifier for the joint distribution p(pixel feature,class). For each class, GMMSeg builds Gaussian Mixture Models (GMMs) via Expectation-Maximization (EM), so as to capture class-conditional densities. Meanwhile, the deep dense representation is end-to-end trained in a discriminative manner, i.e., maximizing p(class|pixel feature). This endows GMMSeg with the strengths of both generative and discriminative models. With a variety of segmentation architectures and backbones, GMMSeg outperforms the discriminative counterparts on three closed-set datasets. More impressively, without any modification, GMMSeg even performs well on open-world datasets. We believe this work brings fundamental insights into the related fields.",
        "authors": [
            "Chen Liang",
            "Wenguan Wang",
            "Jiaxu Miao",
            "Yi Yang"
        ],
        "citations": 94,
        "references": 143,
        "year": 2022
    },
    {
        "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
        "abstract": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines.",
        "authors": [
            "Zhengfu He",
            "Tianxiang Sun",
            "Kuan Wang",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "citations": 93,
        "references": 42,
        "year": 2022
    },
    {
        "title": "M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems",
        "abstract": "Industrial recommender systems have been growing increasingly complex, may involve \\emph{diverse domains} such as e-commerce products and user-generated contents, and can comprise \\emph{a myriad of tasks} such as retrieval, ranking, explanation generation, and even AI-assisted content production. The mainstream approach so far is to develop individual algorithms for each domain and each task. In this paper, we explore the possibility of developing a unified foundation model to support \\emph{open-ended domains and tasks} in an industrial recommender system, which may reduce the demand on downstream settings' data and can minimize the carbon footprint by avoiding training a separate model from scratch for every task. Deriving a unified foundation is challenging due to (i) the potentially unlimited set of downstream domains and tasks, and (ii) the real-world systems' emphasis on computational efficiency. We thus build our foundation upon M6, an existing large-scale industrial pretrained language model similar to GPT-3 and T5, and leverage M6's pretrained ability for sample-efficient downstream adaptation, by representing user behavior data as plain texts and converting the tasks to either language understanding or generation. To deal with a tight hardware budget, we propose an improved version of prompt tuning that outperforms fine-tuning with negligible 1\\% task-specific parameters, and employ techniques such as late interaction, early exiting, parameter sharing, and pruning to further reduce the inference time and the model size. We demonstrate the foundation model's versatility on a wide range of tasks such as retrieval, ranking, zero-shot recommendation, explanation generation, personalized content creation, and conversational recommendation, and manage to deploy it on both cloud servers and mobile devices.",
        "authors": [
            "Zeyu Cui",
            "Jianxin Ma",
            "Chang Zhou",
            "Jingren Zhou",
            "Hongxia Yang"
        ],
        "citations": 162,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Compression of Generative Pre-trained Language Models via Quantization",
        "abstract": "The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.",
        "authors": [
            "Chaofan Tao",
            "Lu Hou",
            "Wei Zhang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Ping Luo",
            "Ngai Wong"
        ],
        "citations": 95,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation",
        "abstract": "Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety. This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs). The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care. Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings. The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care.",
        "authors": [
            "Anjanava Biswas",
            "Wrick Talukdar"
        ],
        "citations": 934,
        "references": 43,
        "year": 2024
    },
    {
        "title": "Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models",
        "abstract": "In this paper, we present a natural language code synthesis tool, GenLine, backed by 1) a large generative language model and 2) a set of task-specific prompts that create or change code. To understand the user experience of natural language code synthesis with these new types of models, we conducted a user study in which participants applied GenLine to two programming tasks. Our results indicate that while natural language code synthesis can sometimes provide a magical experience, participants still faced challenges. In particular, participants felt that they needed to learn the model’s “syntax,” despite their input being natural language. Participants also struggled to form an accurate mental model of the types of requests the model can reliably translate and developed a set of strategies to debug model input. From these findings, we discuss design implications for future natural language code synthesis tools built using large generative language models.",
        "authors": [
            "Ellen Jiang",
            "Edwin Toh",
            "A. Molina",
            "Kristen Olson",
            "Claire Kayacik",
            "Aaron Donsbach",
            "Carrie J. Cai",
            "Michael Terry"
        ],
        "citations": 73,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
        "abstract": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.",
        "authors": [
            "Dongjun Kim",
            "Yeongmin Kim",
            "Wanmo Kang",
            "Il-Chul Moon"
        ],
        "citations": 70,
        "references": 77,
        "year": 2022
    },
    {
        "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models",
        "abstract": "—The recent advance of self-supervised learning associated with the Transformer architecture enables natural language processing (NLP) to exhibit extremely low perplexity. Such powerful models demand ever-increasing model size, and thus, large amounts of computations and memory footprints. In this paper, we propose an efﬁcient inference framework for large-scale generative language models. As the key to reducing model size, we quantize weights by a non-uniform quantization method. Then, quantized matrix multiplications are accelerated by our proposed kernel, called nuQmm, which allows a wide trade-off between compression ratio and accuracy. Our proposed nuQmm reduces the latency of not only each GPU but also the entire inference of large LMs because a high compression ratio (by low-bit quantization) mitigates the minimum required number of GPUs. We demonstrate that nuQmm can accelerate the inference speed of the GPT-3 (175B) model by about 14.4 times and save energy consumption by 93%.",
        "authors": [
            "Gunho Park",
            "Baeseong Park",
            "S. Kwon",
            "Byeongwook Kim",
            "Youngjoo Lee",
            "Dongsoo Lee"
        ],
        "citations": 67,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Generative Language Models for Paragraph-Level Question Generation",
        "abstract": "Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting. It includes general-purpose datasets such as SQuAD for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust QG baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English.QG-Bench is released along with the fine-tuned models presented in the paper (https://github.com/asahi417/lm-question-generation), which are also available as a demo (https://autoqg.net/).",
        "authors": [
            "Asahi Ushio",
            "Fernando Alva-Manchego",
            "José Camacho-Collados"
        ],
        "citations": 39,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Multilingual Generative Language Models for Zero-Shot Cross-Lingual Event Argument Extraction",
        "abstract": "We present a study on leveraging multilingual pre-trained generative language models for zero-shot cross-lingual event argument extraction (EAE). By formulating EAE as a language generation task, our method effectively encodes event structures and captures the dependencies between arguments. We design language-agnostic templates to represent the event argument structures, which are compatible with any language, hence facilitating the cross-lingual transfer. Our proposed model finetunes multilingual pre-trained generative language models to generate sentences that fill in the language-agnostic template with arguments extracted from the input passage. The model is trained on source languages and is then directly applied to target languages for event argument extraction. Experiments demonstrate that the proposed model outperforms the current state-of-the-art models on zero-shot cross-lingual EAE. Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE.",
        "authors": [
            "Kuan-Hao Huang",
            "I-Hung Hsu",
            "P. Natarajan",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "citations": 61,
        "references": 52,
        "year": 2022
    },
    {
        "title": "Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues",
        "abstract": "Conversational bots have become non-traditional methods for therapy among individuals suffering from psychological illnesses. Leveraging deep neural generative language models, we propose a deep trainable neural conversational model for therapy-oriented response generation. We leverage transfer learning methods during training on therapy and counseling based data from Reddit and AlexanderStreet. This was done to adapt existing generative models – GPT2 and DialoGPT – to the task of automated dialog generation. Through quantitative evaluation of the linguistic quality, we observe that the dialog generation model - DialoGPT (345M) with transfer learning on video data attains scores similar to a human response baseline. However, human evaluation of responses by conversational bots show mostly signs of generic advice or information sharing instead of therapeutic interaction.",
        "authors": [
            "Avisha Das",
            "S. Selek",
            "Alia R. Warner",
            "X. Zuo",
            "Yan Hu",
            "V. Keloth",
            "Jianfu Li",
            "Wen Zheng",
            "Hua Xu"
        ],
        "citations": 30,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Calomplification — the power of generative calorimeter models",
        "abstract": "Motivated by the high computational costs of classical simulations, machine-learned generative models can be extremely useful in particle physics and elsewhere. They become especially attractive when surrogate models can efficiently learn the underlying distribution, such that a generated sample outperforms a training sample of limited size. This kind of GANplification has been observed for simple Gaussian models. We show the same effect for a physics simulation, specifically photon showers in an electromagnetic calorimeter.",
        "authors": [
            "S. Bieringer",
            "A. Butter",
            "S. Diefenbacher",
            "E. Eren",
            "F. Gaede",
            "D. Hundhausen",
            "G. Kasieczka",
            "B. Nachman",
            "T. Plehn",
            "Mathias Trabs"
        ],
        "citations": 30,
        "references": 65,
        "year": 2022
    },
    {
        "title": "Physically constrained generative adversarial networks for improving precipitation fields from Earth system models",
        "abstract": null,
        "authors": [
            "P. Hess",
            "Markus Drüke",
            "S. Petri",
            "Felix M. Strnad",
            "N. Boers"
        ],
        "citations": 53,
        "references": 76,
        "year": 2022
    },
    {
        "title": "scGPT: toward building a foundation model for single-cell multi-omics using generative AI.",
        "abstract": null,
        "authors": [
            "Haotian Cui",
            "Chloe X. Wang",
            "Hassaan Maan",
            "Kuan Pang",
            "Fengning Luo",
            "Nan Duan",
            "Bo Wang"
        ],
        "citations": 196,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Prompt Tuning for Generative Multimodal Pretrained Models",
        "abstract": "Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at \\url{https://github.com/OFA-Sys/OFA}",
        "authors": [
            "Han Yang",
            "Junyang Lin",
            "An Yang",
            "Peng Wang",
            "Chang Zhou",
            "Hongxia Yang"
        ],
        "citations": 29,
        "references": 73,
        "year": 2022
    },
    {
        "title": "Aligning Generative Language Models with Human Values",
        "abstract": ",",
        "authors": [
            "Ruibo Liu",
            "Ge Zhang",
            "Xinyu Feng",
            "Soroush Vosoughi"
        ],
        "citations": 49,
        "references": 55,
        "year": 2022
    },
    {
        "title": "The Infinite Index: Information Retrieval on Generative Text-To-Image Models",
        "abstract": "Conditional generative models such as DALL-E and Stable Diffusion generate images based on a user-defined text, the prompt. Finding and refining prompts that produce a desired image has become the art of prompt engineering. Generative models do not provide a built-in retrieval model for a user’s information need expressed through prompts. In light of an extensive literature review, we reframe prompt engineering for generative models as interactive text-based retrieval on a novel kind of “infinite index”. We apply these insights for the first time in a case study on image generation for game design with an expert. Finally, we envision how active learning may help to guide the retrieval of generated images.",
        "authors": [
            "Niklas Deckers",
            "Maik Frobe",
            "Johannes Kiesel",
            "G. Pandolfo",
            "Christopher Schröder",
            "Benno Stein",
            "Martin Potthast"
        ],
        "citations": 16,
        "references": 111,
        "year": 2022
    },
    {
        "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
        "abstract": "Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.",
        "authors": [
            "Bohao Li",
            "Rui Wang",
            "Guangzhi Wang",
            "Yuying Ge",
            "Yixiao Ge",
            "Ying Shan"
        ],
        "citations": 373,
        "references": 39,
        "year": 2023
    },
    {
        "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
        "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
        "authors": [
            "Junnan Li",
            "Dongxu Li",
            "S. Savarese",
            "Steven C. H. Hoi"
        ],
        "citations": 1000,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Generative Representational Instruction Tuning",
        "abstract": "All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by>60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.",
        "authors": [
            "Niklas Muennighoff",
            "Hongjin Su",
            "Liang Wang",
            "Nan Yang",
            "Furu Wei",
            "Tao Yu",
            "Amanpreet Singh",
            "Douwe Kiela"
        ],
        "citations": 66,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Can Generative AI improve social science?",
        "abstract": "Generative AI that can produce realistic text, images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might influence social science research. I argue Generative AI has the potential to improve survey research, online experiments, automated content analyses, agent-based models, and other techniques commonly used to study human behavior. In the second section of this article, I discuss the many limitations of Generative AI. I examine how bias in the data used to train these tools can negatively impact social science research—as well as a range of other challenges related to ethics, replication, environmental impact, and the proliferation of low-quality research. I conclude by arguing that social scientists can address many of these limitations by creating open-source infrastructure for research on human behavior. Such infrastructure is not only necessary to ensure broad access to high-quality research tools, I argue, but also because the progress of AI will require deeper understanding of the social forces that guide human behavior.",
        "authors": [
            "Christopher A Bail"
        ],
        "citations": 70,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
        "abstract": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
        "authors": [
            "Huiwen Chang",
            "Han Zhang",
            "Jarred Barber",
            "AJ Maschinot",
            "José Lezama",
            "Lu Jiang",
            "Ming Yang",
            "K. Murphy",
            "W. Freeman",
            "Michael Rubinstein",
            "Yuanzhen Li",
            "Dilip Krishnan"
        ],
        "citations": 456,
        "references": 87,
        "year": 2023
    },
    {
        "title": "The potential of generative AI for personalized persuasion at scale",
        "abstract": null,
        "authors": [
            "S. C. Matz",
            "J. D. Teeny",
            "S. Vaid",
            "H. Peters",
            "G. M. Harari",
            "M. Cerf"
        ],
        "citations": 78,
        "references": 79,
        "year": 2024
    },
    {
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
        "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.",
        "authors": [
            "Hanze Dong",
            "Wei Xiong",
            "Deepanshu Goyal",
            "Rui Pan",
            "Shizhe Diao",
            "Jipeng Zhang",
            "Kashun Shum",
            "T. Zhang"
        ],
        "citations": 323,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design",
        "abstract": "Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or structure.",
        "authors": [
            "Andrew Campbell",
            "Jason Yim",
            "R. Barzilay",
            "Tom Rainforth",
            "T. Jaakkola"
        ],
        "citations": 47,
        "references": 0,
        "year": 2024
    },
    {
        "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
        "abstract": "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models",
        "authors": [
            "Dustin Podell",
            "Zion English",
            "Kyle Lacey",
            "A. Blattmann",
            "Tim Dockhorn",
            "Jonas Muller",
            "Joe Penna",
            "Robin Rombach"
        ],
        "citations": 1000,
        "references": 56,
        "year": 2023
    },
    {
        "title": "MEGA: Multilingual Evaluation of Generative AI",
        "abstract": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",
        "authors": [
            "Kabir Ahuja",
            "Rishav Hada",
            "Millicent Ochieng",
            "Prachi Jain",
            "Harshita Diddee",
            "Krithika Ramesh",
            "Samuel C. Maina",
            "T. Ganu",
            "Sameer Segal",
            "Maxamed Axmed",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "citations": 217,
        "references": 100,
        "year": 2023
    },
    {
        "title": "A multimodal generative AI copilot for human pathology",
        "abstract": null,
        "authors": [
            "Ming Y. Lu",
            "Bowen Chen",
            "Drew F. K. Williamson",
            "Richard J. Chen",
            "Melissa Zhao",
            "Aaron K Chow",
            "Kenji Ikemura",
            "Ahrong Kim",
            "Dimitra Pouli",
            "Ankush Patel",
            "Amr Soliman",
            "Chengkuan Chen",
            "Tong Ding",
            "Judy J. Wang",
            "Georg K. Gerber",
            "Ivy Liang",
            "L. Le",
            "Anil V. Parwani",
            "Luca L Weishaupt",
            "Faisal Mahmood"
        ],
        "citations": 56,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Generative AI",
        "abstract": null,
        "authors": [
            "Stefan Feuerriegel",
            "Jochen Hartmann",
            "Christian Janiesch",
            "Patrick Zschech"
        ],
        "citations": 331,
        "references": 129,
        "year": 2023
    },
    {
        "title": "Generative AI for designing and validating easily synthesizable and structurally novel antibiotics",
        "abstract": null,
        "authors": [
            "Kyle Swanson",
            "Gary Liu",
            "Denise Catacutan",
            "Autumn Arnold",
            "James Y. Zou",
            "Jonathan M. Stokes"
        ],
        "citations": 34,
        "references": 72,
        "year": 2024
    },
    {
        "title": "AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs",
        "abstract": "As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs. In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs. In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend.",
        "authors": [
            "Zeyi Liao",
            "Huan Sun"
        ],
        "citations": 47,
        "references": 50,
        "year": 2024
    },
    {
        "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
        "abstract": "We present DiffBIR, a general restoration pipeline that could handle different blind image restoration tasks in a unified framework. DiffBIR decouples blind image restoration problem into two stages: 1) degradation removal: removing image-independent content; 2) information regeneration: generating the lost image content. Each stage is developed independently but they work seamlessly in a cascaded manner. In the first stage, we use restoration modules to remove degradations and obtain high-fidelity restored results. For the second stage, we propose IRControlNet that leverages the generative ability of latent diffusion models to generate realistic details. Specifically, IRControlNet is trained based on specially produced condition images without distracting noisy content for stable generation performance. Moreover, we design a region-adaptive restoration guidance that can modify the denoising process during inference without model re-training, allowing users to balance realness and fidelity through a tunable guidance scale. Extensive experiments have demonstrated DiffBIR's superiority over state-of-the-art approaches for blind image super-resolution, blind face restoration and blind image denoising tasks on both synthetic and real-world datasets. The code is available at https://github.com/XPixelGroup/DiffBIR.",
        "authors": [
            "X. Lin",
            "Jingwen He",
            "Zi-Yuan Chen",
            "Zhaoyang Lyu",
            "Ben Fei",
            "Bo Dai",
            "Wanli Ouyang",
            "Y. Qiao",
            "Chao Dong"
        ],
        "citations": 136,
        "references": 97,
        "year": 2023
    },
    {
        "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
        "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.",
        "authors": [
            "Yihan Cao",
            "Siyu Li",
            "Yixin Liu",
            "Zhiling Yan",
            "Yutong Dai",
            "Philip S. Yu",
            "Lichao Sun"
        ],
        "citations": 395,
        "references": 280,
        "year": 2023
    },
    {
        "title": "Unlocking de novo antibody design with generative artificial intelligence",
        "abstract": "Generative AI has the potential to redefine the process of therapeutic antibody discovery. In this report, we describe and validate deep generative models for the de novo design of antibodies against human epidermal growth factor receptor (HER2) without additional optimization. The models enabled an efficient workflow that combined in silico design methods with high-throughput experimental techniques to rapidly identify binders from a library of ∼106 heavy chain complementarity-determining region (HCDR) variants. We demonstrated that the workflow achieves binding rates of 10.6% for HCDR3 and 1.8% for HCDR123 designs and is statistically superior to baselines. We further characterized 421 diverse binders using surface plasmon resonance (SPR), finding 71 with low nanomolar affinity similar to the therapeutic anti-HER2 antibody trastuzumab. A selected subset of 11 diverse high-affinity binders were functionally equivalent or superior to trastuzumab, with most demonstrating suitable developability features. We designed one binder with ∼3x higher cell-based potency compared to trastuzumab and another with improved cross-species reactivity1. Our generative AI approach unlocks an accelerated path to designing therapeutic antibodies against diverse targets.",
        "authors": [
            "Amir Shanehsazzadeh",
            "S. Bachas",
            "George Kasun",
            "J. Sutton",
            "A. Steiger",
            "Richard W. Shuai",
            "Christa Kohnert",
            "Alex Morehead",
            "Amber Brown",
            "Chelsea Chung",
            "Breanna K. Luton",
            "Nicolas Diaz",
            "Matt McPartlon",
            "Bailey Knight",
            "Macey Radach",
            "K. Bateman",
            "David A. Spencer",
            "Jovan Cejovic",
            "Gaelin Kopec-Belliveau",
            "Robel Haile",
            "Edriss Yassine",
            "Cailen M. McCloskey",
            "Monica Natividad",
            "Dalton Chapman",
            "Luka Stojanovic",
            "G. Rakocevic",
            "G. Hannum",
            "Engin Yapici",
            "Katherine M. Moran",
            "Rodante Caguiat",
            "S. Abdulhaqq",
            "Zheyuan Guo",
            "Lillian R. Klug",
            "Miles Gander",
            "Joshua Meier"
        ],
        "citations": 46,
        "references": 100,
        "year": 2024
    },
    {
        "title": "From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy",
        "abstract": "Undoubtedly, the evolution of Generative AI (GenAI) models has been the highlight of digital transformation in the year 2022. As the different GenAI models like ChatGPT and Google Bard continue to foster their complexity and capability, it’s critical to understand its consequences from a cybersecurity perspective. Several instances recently have demonstrated the use of GenAI tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. This research paper highlights the limitations, challenges, potential risks, and opportunities of GenAI in the domain of cybersecurity and privacy. The work presents the vulnerabilities of ChatGPT, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. This paper demonstrates successful example attacks like Jailbreaks, reverse psychology, and prompt injection attacks on the ChatGPT. The paper also investigates how cyber offenders can use the GenAI tools in developing cyber attacks, and explore the scenarios where ChatGPT can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. This paper then examines defense techniques and uses GenAI tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. We will also discuss the social, legal, and ethical implications of ChatGPT. In conclusion, the paper highlights open challenges and future directions to make this GenAI secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts.",
        "authors": [
            "Maanak Gupta",
            "Charankumar Akiri",
            "Kshitiz Aryal",
            "Elisabeth Parker",
            "Lopamudra Praharaj"
        ],
        "citations": 267,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Art and the science of generative AI",
        "abstract": "Understanding shifts in creative work will help guide AI’s impact on the media ecosystem The capabilities of a new class of tools, colloquially known as generative artificial intelligence (AI), is a topic of much debate. One prominent application thus far is the production of high-quality artistic media for visual arts, concept art, music, and literature, as well as video and animation. For example, diffusion models can synthesize high-quality images (1), and large language models (LLMs) can produce sensible-sounding and impressive prose and verse in a wide range of contexts (2). The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI—and making policy decisions around it—requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity.",
        "authors": [
            "Ziv Epstein",
            "Aaron Hertzmann",
            "L. Herman",
            "Robert Mahari",
            "M. Frank",
            "Matthew Groh",
            "Hope Schroeder",
            "Amy Smith",
            "Memo Akten",
            "Jessica Fjeld",
            "H. Farid",
            "Neil Leach",
            "A. Pentland",
            "Olga Russakovsky"
        ],
        "citations": 217,
        "references": 140,
        "year": 2023
    },
    {
        "title": "Transforming Education: A Comprehensive Review of Generative Artificial Intelligence in Educational Settings through Bibliometric and Content Analysis",
        "abstract": "In the ever-evolving era of technological advancements, generative artificial intelligence (GAI) emerges as a transformative force, revolutionizing education. This review paper, guided by the PRISMA framework, presents a comprehensive analysis of GAI in education, synthesizing key insights from a selection of 207 research papers to identify research gaps and future directions in the field. This study begins with a content analysis that explores GAI’s transformative impact in specific educational domains, including medical education and engineering education. The versatile applications of GAI encompass assessment, personalized learning support, and intelligent tutoring systems. Ethical considerations, interdisciplinary collaboration, and responsible technology use are highlighted, emphasizing the need for transparent GAI models and addressing biases. Subsequently, a bibliometric analysis of GAI in education is conducted, examining prominent AI tools, research focus, geographic distribution, and interdisciplinary collaboration. ChatGPT emerges as a dominant GAI tool, and the analysis reveals significant and exponential growth in GAI research in 2023. Moreover, this paper identifies promising future research directions, such as GAI-enhanced curriculum design and longitudinal studies tracking its long-term impact on learning outcomes. These findings provide a comprehensive understanding of GAI’s potential in reshaping education and offer valuable insights to researchers, educators, and policymakers interested in the intersection of GAI and education.",
        "authors": [
            "Zied Bahroun",
            "Chiraz Anane",
            "Vian Ahmed",
            "Andrew Zacca"
        ],
        "citations": 190,
        "references": 151,
        "year": 2023
    },
    {
        "title": "A study of generative large language model for medical research and healthcare",
        "abstract": null,
        "authors": [
            "C.A.I. Peng",
            "Xi Yang",
            "Aokun Chen",
            "Kaleb E. Smith",
            "Nima M. Pournejatian",
            "Anthony B Costa",
            "Cheryl Martin",
            "Mona G. Flores",
            "Ying Zhang",
            "Tanja Magoc",
            "Gloria P. Lipori",
            "Duane A. Mitchell",
            "N. Ospina",
            "M. M. Ahmed",
            "W. Hogan",
            "E. Shenkman",
            "Yi Guo",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "citations": 172,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) on Melanoma Skin Cancer Detection",
        "abstract": "Neural networks and image enhancement have become significant in oncology by aiding in the early diagnosis of various cancer types, including Melanoma, the most lethal type of skin cancer. The conventional approach to diagnosing melanoma includes using dermoscopic tools to capture skin lesions. However, these captured skin lesions have limitations, such as low resolution, artifacts on the skin, and variations in lighting conditions. One promising method for improving the resolution of these dermoscopic images is the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN). To evaluate the effectiveness of ESRGAN in melanoma skin cancer detection, researchers trained CNN models with ESRGAN-enhanced images and not enhanced images and compared their performance. They used the ISIC 2020 dataset and balanced it with random undersampling and data augmentation. The study utilized two deep learning models, VGG16 and ResNet50, to compare their performance with and without ESRGAN enhancement. The results showed that the enhanced dataset outperformed the unprocessed dataset, with ResNet50 achieving an impressive accuracy of 98.2% and VGG16 achieving 94.74%. Additionally, training with the enhanced dataset took 5 minutes longer in VGG16 and 18 minutes longer in ResNet50 which led to significantly better results. In conclusion, the study shows that ESRGAN can improve the performance of deep learning models in melanoma skin cancer detection.",
        "authors": [
            "R. Hernandez",
            "Adrian Angel Ragasa",
            "Lloyd H. Macatangay",
            "Lanie P. Palad",
            "Celinne Mendez",
            "Celinne Mendez"
        ],
        "citations": 165,
        "references": 20,
        "year": 2023
    },
    {
        "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
        "abstract": "The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",
        "authors": [
            "Ethan Chern",
            "Steffi Chern",
            "Shiqi Chen",
            "Weizhe Yuan",
            "Kehua Feng",
            "Chunting Zhou",
            "Junxian He",
            "Graham Neubig",
            "Pengfei Liu"
        ],
        "citations": 151,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges",
        "abstract": "As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations.",
        "authors": [
            "Yan Chen",
            "Pouyan Esmaeilzadeh"
        ],
        "citations": 53,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Challenges and Opportunities of Generative AI for Higher Education as Explained by ChatGPT",
        "abstract": "ChatGPT is revolutionizing the field of higher education by leveraging deep learning models to generate human-like content. However, its integration into academic settings raises concerns regarding academic integrity, plagiarism detection, and the potential impact on critical thinking skills. This article presents a study that adopts a thing ethnography approach to understand ChatGPT’s perspective on the challenges and opportunities it represents for higher education. The research explores the potential benefits and limitations of ChatGPT, as well as mitigation strategies for addressing the identified challenges. Findings emphasize the urgent need for clear policies, guidelines, and frameworks to responsibly integrate ChatGPT in higher education. It also highlights the need for empirical research to understand user experiences and perceptions. The findings provide insights that can guide future research efforts in understanding the implications of ChatGPT and similar Artificial Intelligence (AI) systems in higher education. The study concludes by highlighting the importance of thing ethnography as an innovative approach for engaging with intelligent AI systems and calls for further research to explore best practices and strategies in utilizing Generative AI for educational purposes.",
        "authors": [
            "Rosario Michel-Villarreal",
            "E. Vilalta-Perdomo",
            "D. Salinas-Navarro",
            "Ricardo Thierry-Aguilera",
            "F. S. Gerardou"
        ],
        "citations": 200,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services",
        "abstract": "Artificial Intelligence-Generated Content (AIGC) is an automated method for generating, manipulating, and modifying valuable and diverse data using AI algorithms creatively. This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, namely mobile AIGC networks, that provide personalized and customized AIGC services in real time while maintaining user privacy. We begin by introducing the background and fundamentals of generative models and the lifecycle of AIGC services at mobile AIGC networks, which includes data collection, training, fine-tuning, inference, and product management. We then discuss the collaborative cloud-edge-mobile infrastructure and technologies required to support AIGC services and enable users to access AIGC at mobile edge networks. Furthermore, we explore AIGC-driven creative applications and use cases for mobile AIGC networks. Additionally, we discuss the implementation, security, and privacy challenges of deploying mobile AIGC networks. Finally, we highlight some future research directions and open issues for the full realization of mobile AIGC networks.",
        "authors": [
            "Minrui Xu",
            "Hongyang Du",
            "Dusist Niyato",
            "Jiawen Kang",
            "Zehui Xiong",
            "Shiwen Mao",
            "Zhu Han",
            "A. Jamalipour",
            "Dong In Kim",
            "X. Shen",
            "Victor C. M. Leung",
            "H. Poor"
        ],
        "citations": 148,
        "references": 314,
        "year": 2023
    },
    {
        "title": "MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer",
        "abstract": "Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \\url{https://github.com/OpenGVLab/MM-Interleaved}.",
        "authors": [
            "Changyao Tian",
            "Xizhou Zhu",
            "Yuwen Xiong",
            "Weiyun Wang",
            "Zhe Chen",
            "Wenhai Wang",
            "Yuntao Chen",
            "Lewei Lu",
            "Tong Lu",
            "Jie Zhou",
            "Hongsheng Li",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "citations": 34,
        "references": 112,
        "year": 2024
    },
    {
        "title": "Human Motion Diffusion as a Generative Prior",
        "abstract": "Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Lastly, using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing. We evaluate the composition methods using an off-the-shelf motion diffusion model, and further compare the results to dedicated models trained for these specific tasks.",
        "authors": [
            "Yonatan Shafir",
            "Guy Tevet",
            "Roy Kapon",
            "Amit H. Bermano"
        ],
        "citations": 153,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Flow Matching for Generative Modeling",
        "abstract": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",
        "authors": [
            "Y. Lipman",
            "Ricky T. Q. Chen",
            "Heli Ben-Hamu",
            "Maximilian Nickel",
            "Matt Le"
        ],
        "citations": 613,
        "references": 67,
        "year": 2022
    },
    {
        "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
        "abstract": "In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. Codes are released at \\url{https://github.com/microsoft/GenerativeImage2Text}.",
        "authors": [
            "Jianfeng Wang",
            "Zhengyuan Yang",
            "Xiaowei Hu",
            "Linjie Li",
            "Kevin Lin",
            "Zhe Gan",
            "Zicheng Liu",
            "Ce Liu",
            "Lijuan Wang"
        ],
        "citations": 458,
        "references": 149,
        "year": 2022
    },
    {
        "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
        "abstract": "Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.",
        "authors": [
            "Elias Frantar",
            "Saleh Ashkboos",
            "T. Hoefler",
            "Dan Alistarh"
        ],
        "citations": 656,
        "references": 37,
        "year": 2022
    },
    {
        "title": "MaskGIT: Masked Generative Image Transformer",
        "abstract": "Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 48x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation. Project page: masked-generative-image-transformer.github.io.",
        "authors": [
            "Huiwen Chang",
            "Han Zhang",
            "Lu Jiang",
            "Ce Liu",
            "W. Freeman"
        ],
        "citations": 486,
        "references": 61,
        "year": 2022
    },
    {
        "title": "GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images",
        "abstract": "As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high-fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.",
        "authors": [
            "Jun Gao",
            "Tianchang Shen",
            "Zian Wang",
            "Wenzheng Chen",
            "K. Yin",
            "Daiqing Li",
            "O. Litany",
            "Zan Gojcic",
            "S. Fidler"
        ],
        "citations": 384,
        "references": 79,
        "year": 2022
    },
    {
        "title": "InCoder: A Generative Model for Code Infilling and Synthesis",
        "abstract": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models",
        "authors": [
            "Daniel Fried",
            "Armen Aghajanyan",
            "Jessy Lin",
            "Sida I. Wang",
            "Eric Wallace",
            "Freda Shi",
            "Ruiqi Zhong",
            "Wen-tau Yih",
            "Luke Zettlemoyer",
            "M. Lewis"
        ],
        "citations": 539,
        "references": 81,
        "year": 2022
    },
    {
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
        "abstract": "Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional test-time compute via majority voting for better verification. We demonstrate that GenRM outperforms discriminative, DPO verifiers, and LLM-as-a-Judge, resulting in a 16-40% improvement in the number of problems solved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, we find that training GenRM with synthetic verification rationales is sufficient to pick out subtle errors on math problems. Finally, we demonstrate that generative verifiers scale favorably with model size and inference-time compute.",
        "authors": [
            "Lunjun Zhang",
            "Arian Hosseini",
            "Hritik Bansal",
            "Mehran Kazemi",
            "Aviral Kumar",
            "Rishabh Agarwal"
        ],
        "citations": 37,
        "references": 60,
        "year": 2024
    },
    {
        "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
        "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.",
        "authors": [
            "Renqian Luo",
            "Liai Sun",
            "Yingce Xia",
            "Tao Qin",
            "Sheng Zhang",
            "Hoifung Poon",
            "Tie-Yan Liu"
        ],
        "citations": 656,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining",
        "abstract": "Mainstream 3D representation learning approaches are built upon contrastive or generative modeling pretext tasks, where great improvements in performance on various downstream tasks have been achieved. However, we find these two paradigms have different characteristics: (i) contrastive models are data-hungry that suffer from a representation over-fitting issue; (ii) generative models have a data filling issue that shows inferior data scaling capacity compared to contrastive models. This motivates us to learn 3D representations by sharing the merits of both paradigms, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose Contrast with Reconstruct (ReCon) that unifies these two paradigms. ReCon is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student. An encoder-decoder style ReCon-block is proposed that transfers knowledge through cross attention with stop-gradient, which avoids pretraining over-fitting and pattern difference issues. ReCon achieves a new state-of-the-art in 3D representation learning, e.g., 91.26% accuracy on ScanObjectNN. Codes have been released at https://github.com/qizekun/ReCon.",
        "authors": [
            "Zekun Qi",
            "Runpei Dong",
            "Guo Fan",
            "Zheng Ge",
            "Xiangyu Zhang",
            "Kaisheng Ma",
            "Li Yi"
        ],
        "citations": 100,
        "references": 107,
        "year": 2023
    },
    {
        "title": "Is Conditional Generative Modeling all you need for Decision-Making?",
        "abstract": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.",
        "authors": [
            "Anurag Ajay",
            "Yilun Du",
            "Abhi Gupta",
            "J. Tenenbaum",
            "T. Jaakkola",
            "Pulkit Agrawal"
        ],
        "citations": 281,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
        "abstract": "Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",
        "authors": [
            "Shaden Smith",
            "M. Patwary",
            "Brandon Norick",
            "P. LeGresley",
            "Samyam Rajbhandari",
            "J. Casper",
            "Zhun Liu",
            "Shrimai Prabhumoye",
            "George Zerveas",
            "V. Korthikanti",
            "Elton Zhang",
            "R. Child",
            "Reza Yazdani Aminabadi",
            "J. Bernauer",
            "Xia Song",
            "Mohammad Shoeybi",
            "Yuxiong He",
            "Michael Houston",
            "Saurabh Tiwary",
            "Bryan Catanzaro"
        ],
        "citations": 673,
        "references": 78,
        "year": 2022
    },
    {
        "title": "Generative Artificial Intelligence: A Systematic Review and Applications",
        "abstract": "In recent years, the study of artificial intelligence (AI) has undergone a paradigm shift. This has been propelled by the groundbreaking capabilities of generative models both in supervised and unsupervised learning scenarios. Generative AI has shown state-of-the-art performance in solving perplexing real-world conundrums in fields such as image translation, medical diagnostics, textual imagery fusion, natural language processing, and beyond. This paper documents the systematic review and analysis of recent advancements and techniques in Generative AI with a detailed discussion of their applications including application-specific models. Indeed, the major impact that generative AI has made to date, has been in language generation with the development of large language models, in the field of image translation and several other interdisciplinary applications of generative AI. Moreover, the primary contribution of this paper lies in its coherent synthesis of the latest advancements in these areas, seamlessly weaving together contemporary breakthroughs in the field. Particularly, how it shares an exploration of the future trajectory for generative AI. In conclusion, the paper ends with a discussion of Responsible AI principles, and the necessary ethical considerations for the sustainability and growth of these generative models.",
        "authors": [
            "S. S. Sengar",
            "Affan Bin Hasan",
            "Sanjay Kumar",
            "Fiona Carroll"
        ],
        "citations": 15,
        "references": 168,
        "year": 2024
    },
    {
        "title": "Copyright Protection in Generative AI: A Technical Perspective",
        "abstract": "Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.",
        "authors": [
            "Jie Ren",
            "Han Xu",
            "Pengfei He",
            "Yingqian Cui",
            "Shenglai Zeng",
            "Jiankun Zhang",
            "Hongzhi Wen",
            "Jiayuan Ding",
            "Hui Liu",
            "Yi Chang",
            "Jiliang Tang"
        ],
        "citations": 19,
        "references": 177,
        "year": 2024
    },
    {
        "title": "Revolutionizing personalized medicine with generative AI: a systematic review",
        "abstract": null,
        "authors": [
            "Isaias Ghebrehiwet",
            "Nazar Zaki",
            "Rafat Damseh",
            "Mohd Saberi Mohamad"
        ],
        "citations": 20,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity",
        "abstract": "The advent of Generative AI, particularly through Large Language Models (LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI landscape. Advanced LLMs exhibit multimodality, handling diverse data formats, thereby broadening their application scope. However, the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance. This paper delves into the legal and regulatory implications of Generative AI and LLMs in the European Union context, analyzing aspects of liability, privacy, intellectual property, and cybersecurity. It critically examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA) draft, in addressing the unique challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the legislative framework and proposes recommendations to ensure the safe and compliant deployment of generative models, ensuring they align with the EU's evolving digital landscape and legal standards.",
        "authors": [
            "Claudio Novelli",
            "F. Casolari",
            "Philipp Hacker",
            "Giorgio Spedicato",
            "Luciano Floridi"
        ],
        "citations": 22,
        "references": 103,
        "year": 2024
    },
    {
        "title": "Sentiment Analysis in the Age of Generative AI",
        "abstract": null,
        "authors": [
            "Jan Ole Krugmann",
            "Jochen Hartmann"
        ],
        "citations": 32,
        "references": 37,
        "year": 2024
    },
    {
        "title": "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",
        "abstract": "Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this\"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.",
        "authors": [
            "Yixin Liu",
            "Kai Zhang",
            "Yuan Li",
            "Zhiling Yan",
            "Chujie Gao",
            "Ruoxi Chen",
            "Zhengqing Yuan",
            "Yue Huang",
            "Hanchi Sun",
            "Jianfeng Gao",
            "Lifang He",
            "Lichao Sun"
        ],
        "citations": 150,
        "references": 192,
        "year": 2024
    },
    {
        "title": "Score-based generative modeling for de novo protein design",
        "abstract": null,
        "authors": [
            "Jin Sub Lee",
            "Jisun Kim",
            "Philip M. Kim"
        ],
        "citations": 88,
        "references": 40,
        "year": 2023
    },
    {
        "title": "The Robots Are Here: Navigating the Generative AI Revolution in Computing Education",
        "abstract": "Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.",
        "authors": [
            "J. Prather",
            "Paul Denny",
            "Juho Leinonen",
            "Brett A. Becker",
            "Ibrahim Albluwi",
            "Michelle Craig",
            "H. Keuning",
            "Natalie Kiesler",
            "Tobias Kohn",
            "Andrew Luxton-Reilly",
            "Stephen Macneil",
            "Andrew Petersen",
            "Raymond Pettit",
            "Brent N. Reeves",
            "Jaromír Šavelka"
        ],
        "citations": 138,
        "references": 178,
        "year": 2023
    },
    {
        "title": "The Short-Term Effects of Generative Artificial Intelligence on Employment: Evidence from an Online Labor Market",
        "abstract": "Generative artificial intelligence (AI) holds the potential to either complement workers by enhancing their productivity or substitute them. We examine the short-term effects of the recently released generative AI models (ChatGPT, DALL-E 2, and Midjourney) on the employment outcomes of freelancers on a large online platform. We find that freelancers in highly affected occupations suffer from the introduction of generative AI, experiencing reductions in both employment and earnings. We find similar effects studying the release of other image-based generative AI models. Exploring the heterogeneity by freelancers’ employment history, we do not find evidence that high-quality service, measured by their past performance and employment, moderates the adverse effects on employment. In fact, we find suggestive evidence that top freelancers are disproportionately affected by AI. These results suggest that generative AI may transform the role of human capital in the organization and reduce overall demand for workers. Supplemental Material: The online appendices are available at https://doi.org/10.1287/orsc.2023.18441 .",
        "authors": [
            "Xiang Hui",
            "O. Reshef",
            "Luofeng Zhou"
        ],
        "citations": 28,
        "references": 69,
        "year": 2024
    },
    {
        "title": "AI models collapse when trained on recursively generated data",
        "abstract": null,
        "authors": [
            "Ilia Shumailov",
            "Zakhar Shumaylov",
            "Yiren Zhao",
            "Nicolas Papernot",
            "Ross Anderson",
            "Yarin Gal"
        ],
        "citations": 101,
        "references": 10,
        "year": 2024
    },
    {
        "title": "On the assessment of generative AI in modeling tasks: an experience report with ChatGPT and UML",
        "abstract": null,
        "authors": [
            "J. Cámara",
            "J. Troya",
            "Lola Burgueño",
            "Antonio Vallecillo"
        ],
        "citations": 99,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations",
        "abstract": "Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (\"Generative Recommenders\"), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.",
        "authors": [
            "Jiaqi Zhai",
            "Lucy Liao",
            "Xing Liu",
            "Yueming Wang",
            "Rui Li",
            "Xuan Cao",
            "Leon Gao",
            "Zhaojie Gong",
            "Fangda Gu",
            "Michael He",
            "Yin-Hua Lu",
            "Yu Shi"
        ],
        "citations": 20,
        "references": 68,
        "year": 2024
    },
    {
        "title": "Consistency Models",
        "abstract": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.",
        "authors": [
            "Yang Song",
            "Prafulla Dhariwal",
            "Mark Chen",
            "I. Sutskever"
        ],
        "citations": 648,
        "references": 86,
        "year": 2023
    },
    {
        "title": "A Primer on Generative Artificial Intelligence",
        "abstract": "Many educators and professionals in different industries may need to become more familiar with the basic concepts of artificial intelligence (AI) and generative artificial intelligence (Gen-AI). Therefore, this paper aims to introduce some of the basic concepts of AI and Gen-AI. The approach of this explanatory paper is first to introduce some of the underlying concepts, such as artificial intelligence, machine learning, deep learning, artificial neural networks, and large language models (LLMs), that would allow the reader to better understand generative AI. The paper also discusses some of the applications and implications of generative AI on businesses and education, followed by the current challenges associated with generative AI.",
        "authors": [
            "Faisal Kalota"
        ],
        "citations": 19,
        "references": 83,
        "year": 2024
    },
    {
        "title": "Rethinking open source generative AI: open-washing and the EU AI Act",
        "abstract": "The past year has seen a steep rise in generative AI systems that claim to be open. But how open are they really? The question of what counts as open source in generative AI is poised to take on particular importance in light of the upcoming EU AI Act that regulates open source systems differently, creating an urgent need for practical openness assessment. Here we use an evidence-based framework that distinguishes 14 dimensions of openness, from training datasets to scientific and technical documentation and from licensing to access methods. Surveying over 45 generative AI systems (both text and text-to-image), we find that while the term open source is widely used, many models are ‘open weight’ at best and many providers seek to evade scientific, legal and regulatory scrutiny by withholding information on training and fine-tuning data. We argue that openness in generative AI is necessarily composite (consisting of multiple elements) and gradient (coming in degrees), and point out the risk of relying on single features like access or licensing to declare models open or not. Evidence-based openness assessment can help foster a generative AI landscape in which models can be effectively regulated, model providers can be held accountable, scientists can scrutinise generative AI, and end users can make informed decisions.",
        "authors": [
            "Andreas Liesenfeld",
            "Mark Dingemanse"
        ],
        "citations": 18,
        "references": 60,
        "year": 2024
    },
    {
        "title": "Future of software development with generative AI",
        "abstract": null,
        "authors": [
            "Jaakko Sauvola",
            "Sasu Tarkoma",
            "Mika Klemettinen",
            "J. Riekki",
            "David Doermann"
        ],
        "citations": 20,
        "references": 0,
        "year": 2024
    },
    {
        "title": "InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models",
        "abstract": "We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators.",
        "authors": [
            "Jiale Xu",
            "Weihao Cheng",
            "Yiming Gao",
            "Xintao Wang",
            "Shenghua Gao",
            "Ying Shan"
        ],
        "citations": 94,
        "references": 67,
        "year": 2024
    },
    {
        "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
        "abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.",
        "authors": [
            "Bill Yuchen Lin",
            "Yicheng Fu",
            "Karina Yang",
            "Prithviraj Ammanabrolu",
            "Faeze Brahman",
            "Shiyu Huang",
            "Chandra Bhagavatula",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "citations": 105,
        "references": 41,
        "year": 2023
    },
    {
        "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification",
        "abstract": "The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree veriﬁcation. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM’s outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is veriﬁed by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree veriﬁer instead of an incremental decoder, which signiﬁcantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.",
        "authors": [
            "Xupeng Miao",
            "G. Oliaro",
            "Zhihao Zhang",
            "Xinhao Cheng",
            "Zeyu Wang",
            "Rae Ying Yee Wong",
            "Zhuoming Chen",
            "Daiyaan Arfeen",
            "Reyna Abhyankar",
            "Zhihao Jia"
        ],
        "citations": 96,
        "references": 44,
        "year": 2023
    },
    {
        "title": "MatterGen: a generative model for inorganic materials design",
        "abstract": "The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. We believe that the quality of generated materials and the breadth of MatterGen's capabilities represent a major advancement towards creating a universal generative model for materials design.",
        "authors": [
            "Claudio Zeni",
            "Robert Pinsler",
            "Daniel Zügner",
            "Andrew Fowler",
            "Matthew Horton",
            "Xiang Fu",
            "Sasha Shysheya",
            "Jonathan Crabbe",
            "Lixin Sun",
            "Jake Smith",
            "Ryota Tomioka",
            "Tian Xie"
        ],
        "citations": 69,
        "references": 0,
        "year": 2023
    },
    {
        "title": "JourneyDB: A Benchmark for Generative Image Understanding",
        "abstract": "While recent advancements in vision-language models have had a transformative impact on multi-modal comprehension, the extent to which these models possess the ability to comprehend generated images remains uncertain. Synthetic images, in comparison to real data, encompass a higher level of diversity in terms of both content and style, thereby presenting significant challenges for the models to fully grasp. In light of this challenge, we introduce a comprehensive dataset, referred to as JourneyDB, that caters to the domain of generative images within the context of multi-modal visual understanding. Our meticulously curated dataset comprises 4 million distinct and high-quality generated images, each paired with the corresponding text prompts that were employed in their creation. Furthermore, we additionally introduce an external subset with results of another 22 text-to-image generative models, which makes JourneyDB a comprehensive benchmark for evaluating the comprehension of generated images. On our dataset, we have devised four benchmarks to assess the performance of generated image comprehension in relation to both content and style interpretation. These benchmarks encompass prompt inversion, style retrieval, image captioning, and visual question answering. Lastly, we evaluate the performance of state-of-the-art multi-modal models when applied to the JourneyDB dataset, providing a comprehensive analysis of their strengths and limitations in comprehending generated content. We anticipate that the proposed dataset and benchmarks will facilitate further research in the field of generative content understanding. The dataset is publicly available at https://journeydb.github.io.",
        "authors": [
            "Junting Pan",
            "Keqiang Sun",
            "Yuying Ge",
            "Hao Li",
            "Haodong Duan",
            "Xiaoshi Wu",
            "Renrui Zhang",
            "Aojun Zhou",
            "Zipeng Qin",
            "Yi Wang",
            "Jifeng Dai",
            "Y. Qiao",
            "Hongsheng Li"
        ],
        "citations": 67,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Generative AI in Medicine and Healthcare: Promises, Opportunities and Challenges",
        "abstract": "Generative AI (artificial intelligence) refers to algorithms and models, such as OpenAI’s ChatGPT, that can be prompted to generate various types of content. In this narrative review, we present a selection of representative examples of generative AI applications in medicine and healthcare. We then briefly discuss some associated issues, such as trust, veracity, clinical safety and reliability, privacy, copyrights, ownership, and opportunities, e.g., AI-driven conversational user interfaces for friendlier human-computer interaction. We conclude that generative AI will play an increasingly important role in medicine and healthcare as it further evolves and gets better tailored to the unique settings and requirements of the medical domain and as the laws, policies and regulatory frameworks surrounding its use start taking shape.",
        "authors": [
            "Peng Zhang",
            "M. Boulos"
        ],
        "citations": 101,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Generalizing Dataset Distillation via Deep Generative Prior",
        "abstract": "Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthe-size a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite a recent upsurge of progress in the field, existing dataset dis-tillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pretrained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.",
        "authors": [
            "George Cazenavette",
            "Tongzhou Wang",
            "A. Torralba",
            "Alexei A. Efros",
            "Jun-Yan Zhu"
        ],
        "citations": 63,
        "references": 81,
        "year": 2023
    },
    {
        "title": "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation",
        "abstract": "Robot learning methods have the potential for widespread generalization across tasks, environments, and objects. However, these methods require large diverse datasets that are expensive to collect in real-world robotics settings. For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot's own experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a data source. We show that despite these generative models being trained on largely non-robotics data, they can serve as effective ways to impart priors into the process of robot learning in a way that enables widespread generalization. In particular, we show how pre-trained generative models can serve as effective tools for semantically meaningful data augmentation. By leveraging these pre-trained models for generating appropriate\"semantic\"data augmentations, we propose a system GenAug that is able to significantly improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to re-target behavior to novel scenarios, while only requiring marginal amounts of real-world data. We demonstrate the efficacy of this system on a number of object manipulation problems in the real world, showing a 40% improvement in generalization to novel scenes and objects.",
        "authors": [
            "Zoey Chen",
            "Sho Kiami",
            "Abhishek Gupta",
            "Vikash Kumar"
        ],
        "citations": 62,
        "references": 39,
        "year": 2023
    },
    {
        "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
        "abstract": "We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.",
        "authors": [
            "Yufei Wang",
            "Zhou Xian",
            "Feng Chen",
            "Tsun-Hsuan Wang",
            "Yian Wang",
            "Zackory Erickson",
            "David Held",
            "Chuang Gan"
        ],
        "citations": 62,
        "references": 147,
        "year": 2023
    },
    {
        "title": "Large language models in medicine",
        "abstract": null,
        "authors": [
            "A. J. Thirunavukarasu",
            "D. Ting",
            "Kabilan Elangovan",
            "Laura Gutierrez",
            "Ting Fang Tan",
            "D. Ting"
        ],
        "citations": 1000,
        "references": 136,
        "year": 2023
    },
    {
        "title": "GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis",
        "abstract": "Synthesizing high-fidelity complex images from text is challenging. Based on large pretraining, the autoregressive and diffusion models can synthesize photo-realistic images. Although these large models have shown notable progress, there remain three flaws. 1) These models require tremendous training data and parameters to achieve good performance. 2) The multi-step generation design slows the image synthesis process heavily. 3) The synthesized visual features are challenging to control and require delicately designed prompts. To enable high-quality, efficient, fast, and controllable text-to-image synthesis, we propose Generative Adversarial CLIPs, namely GALIP. GALIP leverages the powerful pretrained CLIP model both in the discriminator and generator. Specifically, we propose a CLIP-based discriminator. The complex scene understanding ability of CLIP enables the discriminator to accurately assess the image quality. Furthermore, we propose a CLIP-empowered generator that induces the visual concepts from CLIP through bridge features and prompts. The CLIP-integrated generator and discriminator boost training efficiency, and as a result, our model only requires about 3% training data and 6% learnable parameters, achieving comparable results to large pretrained autoregressive and diffusion models. Moreover, our model achieves ~120×faster synthesis speed and inherits the smooth latent space from GAN. The extensive experimental results demonstrate the excellent performance of our GALIP. Code is available at https://github.com/tobran/GALIP.",
        "authors": [
            "Ming Tao",
            "Bingkun Bao",
            "Hao Tang",
            "Changsheng Xu"
        ],
        "citations": 84,
        "references": 61,
        "year": 2023
    },
    {
        "title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
        "abstract": "Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces a methodology, inspired by unCLIP, for instruction-tuning generative models of behavior without relying on a large dataset of instruction-labeled trajectories. Using this methodology, we create an instruction-tuned Video Pretraining (VPT) model called STEVE-1, which can follow short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, reducing the need for costly human text annotations, and all for only $60 of compute. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier-free guidance, and data scaling. All resources, including our model weights, training scripts, and evaluation tools are made available for further research.",
        "authors": [
            "Shalev Lifshitz",
            "Keiran Paster",
            "Harris Chan",
            "Jimmy Ba",
            "Sheila A. McIlraith"
        ],
        "citations": 58,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models",
        "abstract": "Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen --- or excite --- their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts. Code is available at our project page: https://attendandexcite.github.io/Attend-and-Excite/.",
        "authors": [
            "Hila Chefer",
            "Yuval Alaluf",
            "Yael Vinker",
            "Lior Wolf",
            "D. Cohen-Or"
        ],
        "citations": 410,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Extracting Training Data from Diffusion Models",
        "abstract": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
        "authors": [
            "Nicholas Carlini",
            "Jamie Hayes",
            "Milad Nasr",
            "Matthew Jagielski",
            "Vikash Sehwag",
            "Florian Tramèr",
            "Borja Balle",
            "Daphne Ippolito",
            "Eric Wallace"
        ],
        "citations": 481,
        "references": 78,
        "year": 2023
    },
    {
        "title": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation",
        "abstract": "Recent advancements in Natural Language Processing (NLP) have led to the development of NLP-based recommender systems that have shown superior performance. However, current models commonly treat items as mere IDs and adopt discriminative modeling, resulting in limitations of (1) fully leveraging the content information of items and the language modeling capabilities of NLP models; (2) interpreting user interests to improve relevance and diversity; and (3) adapting practical circumstances such as growing item inventories. To address these limitations, we present GPT4Rec, a novel and flexible generative framework inspired by search engines. It first generates hypothetical\"search queries\"given item titles in a user's history, and then retrieves items for recommendation by searching these queries. The framework overcomes previous limitations by learning both user and item embeddings in the language space. To well-capture user interests with different aspects and granularity for improving relevance and diversity, we propose a multi-query generation technique with beam search. The generated queries naturally serve as interpretable representations of user interests and can be searched to recommend cold-start items. With GPT-2 language model and BM25 search engine, our framework outperforms state-of-the-art methods by $75.7\\%$ and $22.2\\%$ in Recall@K on two public datasets. Experiments further revealed that multi-query generation with beam search improves both the diversity of retrieved items and the coverage of a user's multi-interests. The adaptiveness and interpretability of generated queries are discussed with qualitative case studies.",
        "authors": [
            "Jinming Li",
            "Wentao Zhang",
            "Tiantian Wang",
            "Guanglei Xiong",
            "Alan Lu",
            "G. Medioni"
        ],
        "citations": 79,
        "references": 30,
        "year": 2023
    },
    {
        "title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
        "abstract": "Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.",
        "authors": [
            "Yuan Li",
            "Yixuan Zhang",
            "Lichao Sun"
        ],
        "citations": 74,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Identifying and Mitigating the Security Risks of Generative AI",
        "abstract": "Every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks. This paper reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. We discuss short-term and long-term goals for the community on this topic. We hope this paper provides both a launching point for a discussion on this important topic as well as interesting problems that the research community can work to address.",
        "authors": [
            "Clark W. Barrett",
            "Bradley L Boyd",
            "Ellie Burzstein",
            "Nicholas Carlini",
            "Brad Chen",
            "Jihye Choi",
            "A. Chowdhury",
            "Mihai Christodorescu",
            "Anupam Datta",
            "S. Feizi",
            "Kathleen Fisher",
            "Tatsunori Hashimoto",
            "Dan Hendrycks",
            "S. Jha",
            "Daniel Kang",
            "F. Kerschbaum",
            "E. Mitchell",
            "John C. Mitchell",
            "Zulfikar Ramzan",
            "K. Shams",
            "D. Song",
            "Ankur Taly",
            "Diyi Yang"
        ],
        "citations": 73,
        "references": 114,
        "year": 2023
    },
    {
        "title": "scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI",
        "abstract": "Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between linguistic constructs and cellular biology — where texts comprise words, similarly, cells are defined by genes — our study probes the applicability of foundation models to advance cellular biology and genetics research. Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell-type annotation, multi-batch integration, multi-omic integration, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at https://github.com/bowang-lab/scGPT.",
        "authors": [
            "Haotian Cui",
            "Chloe X. Wang",
            "Hassaan Maan",
            "Kuan Pang",
            "Fengning Luo",
            "Bo Wang"
        ],
        "citations": 71,
        "references": 105,
        "year": 2023
    },
    {
        "title": "An Enhanced AI-Based Network Intrusion Detection System Using Generative Adversarial Networks",
        "abstract": "As communication technology advances, various and heterogeneous data are communicated in distributed environments through network systems. Meanwhile, along with the development of communication technology, the attack surface has expanded, and concerns regarding network security have increased. Accordingly, to deal with potential threats, research on network intrusion detection systems (NIDSs) has been actively conducted. Among the various NIDS technologies, recent interest is focused on artificial intelligence (AI)-based anomaly detection systems, and various models have been proposed to improve the performance of NIDS. However, there still exists the problem of data imbalance, in which AI models cannot sufficiently learn malicious behavior and thus fail to detect network threats accurately. In this study, we propose a novel AI-based NIDS that can efficiently resolve the data imbalance problem and improve the performance of the previous systems. To address the aforementioned problem, we leveraged a state-of-the-art generative model that could generate plausible synthetic data for minor attack traffic. In particular, we focused on the reconstruction error and Wasserstein distance-based generative adversarial networks, and autoencoder-driven deep learning models. To demonstrate the effectiveness of our system, we performed comprehensive evaluations over various data sets and demonstrated that the proposed systems significantly outperformed the previous AI-based NIDS.",
        "authors": [
            "Cheolhee Park",
            "Jonghoon Lee",
            "Youngsoo Kim",
            "Jong-Geun Park",
            "Hyunjin Kim",
            "Dowon Hong"
        ],
        "citations": 76,
        "references": 45,
        "year": 2023
    },
    {
        "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
        "abstract": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.",
        "authors": [
            "Kaizhi Zheng",
            "Xuehai He",
            "Xin Eric Wang"
        ],
        "citations": 75,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Generative Artificial Intelligence: Implications and Considerations for Higher Education Practice",
        "abstract": "Generative Artificial Intelligence (GAI) has emerged as a transformative force in higher education, offering both challenges and opportunities. This paper explores the multifaceted impact of GAI on academic work, with a focus on student life and, in particular, the implications for international students. While GAI, exemplified by models like ChatGPT, has the potential to revolutionize education, concerns about academic integrity have arisen, leading to debates on the use of AI detection tools. This essay highlights the difficulties in reliably detecting AI-generated content, raising concerns about potential false accusations against students. It also discusses biases within AI models, emphasizing the need for fairness and equity in AI-based assessments with a particular emphasis on the disproportionate impact of GAI on international students, who already face biases and discrimination. It also highlights the potential for AI to mitigate some of these challenges by providing language support and accessibility features. Finally, this essay acknowledges the disruptive potential of GAI in higher education and calls for a balanced approach that addresses both the challenges and opportunities it presents by emphasizing the importance of AI literacy and ethical considerations in adopting AI technologies to ensure equitable access and positive outcomes for all students. We offer a coda to Ng et al.’s AI competency framework, mapped to the Revised Bloom’s Taxonomy, through a lens of cultural competence with AI as a means of supporting educators to use these tools equitably in their teaching.",
        "authors": [
            "T. Farrelly",
            "Nick Baker"
        ],
        "citations": 73,
        "references": 55,
        "year": 2023
    },
    {
        "title": "T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models",
        "abstract": "The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., structure and color) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn low-cost T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications. Our code is available at https://github.com/TencentARC/T2I-Adapter.",
        "authors": [
            "Chong Mou",
            "Xintao Wang",
            "Liangbin Xie",
            "Jing Zhang",
            "Zhongang Qi",
            "Ying Shan",
            "Xiaohu Qie"
        ],
        "citations": 769,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets",
        "abstract": "We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .",
        "authors": [
            "A. Blattmann",
            "Tim Dockhorn",
            "Sumith Kulal",
            "Daniel Mendelevitch",
            "Maciej Kilian",
            "Dominik Lorenz"
        ],
        "citations": 616,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations",
        "abstract": "Generating graph-structured data requires learning the underlying distribution of graphs. Yet, this is a challenging problem, and the previous graph generative methods either fail to capture the permutation-invariance property of graphs or cannot sufficiently model the complex dependency between nodes and edges, which is crucial for generating real-world graphs such as molecules. To overcome such limitations, we propose a novel score-based generative model for graphs with a continuous-time framework. Specifically, we propose a new graph diffusion process that models the joint distribution of the nodes and edges through a system of stochastic differential equations (SDEs). Then, we derive novel score matching objectives tailored for the proposed diffusion process to estimate the gradient of the joint log-density with respect to each component, and introduce a new solver for the system of SDEs to efficiently sample from the reverse diffusion process. We validate our graph generation method on diverse datasets, on which it either achieves significantly superior or competitive performance to the baselines. Further analysis shows that our method is able to generate molecules that lie close to the training distribution yet do not violate the chemical valency rule, demonstrating the effectiveness of the system of SDEs in modeling the node-edge relationships. Our code is available at https://github.com/harryjo97/GDSS.",
        "authors": [
            "Jaehyeong Jo",
            "Seul Lee",
            "Sung Ju Hwang"
        ],
        "citations": 180,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Training Diffusion Models with Reinforcement Learning",
        "abstract": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io .",
        "authors": [
            "Kevin Black",
            "Michael Janner",
            "Yilun Du",
            "Ilya Kostrikov",
            "S. Levine"
        ],
        "citations": 207,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Mapping out a research agenda for generative artificial intelligence in tertiary education",
        "abstract": "Generative artificial intelligence (AI) has taken the world by storm. In this editorial, we outline some of the key areas of tertiary education impacted by large language models and associated applications that will require re-thinking and research to address in the short to medium term. Given how rapidly generative AI developments are currently occurring, this editorial is speculative. Although there is a long history of research on AI in education, the current situation is both unprecedented and seemingly not something that the AI in education community fully predicted. We also outline the editorial position of AJET in regards to generative AI to assist authors using tools such as ChatGPT as any part of the research or writing process. This is a rapidly evolving space. We have attempted to provide some clarity in this editorial while acknowledging that we may need to revisit some or all of what we offer here in the weeks and months ahead.",
        "authors": [
            "J. Lodge",
            "K. Thompson",
            "L. Corrin"
        ],
        "citations": 67,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Generative Recommendation: Towards Next-generation Recommender Paradigm",
        "abstract": "Recommender systems typically retrieve items from an item corpus for personalized recommendations. However, such a retrieval-based recommender paradigm faces two limitations: 1) the human-generated items in the corpus might fail to satisfy the users' diverse information needs, and 2) users usually adjust the recommendations via inefficient passive feedback, e.g., clicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success, offering the potential to overcome these limitations: 1) generative AI can produce personalized items to satisfy users' information needs, and 2) the newly emerged large language models significantly reduce the efforts of users to precisely express information needs via natural language instructions. In this light, the boom of AIGC points the way towards the next-generation recommender paradigm with two new objectives: 1) generating personalized content through generative AI, and 2) integrating user instructions to guide content generation. To this end, we propose a novel Generative Recommender paradigm named GeneRec, which adopts an AI generator to personalize content generation and leverages user instructions. Specifically, we pre-process users' instructions and traditional feedback via an instructor to output the generation guidance. Given the guidance, we instantiate the AI generator through an AI editor and an AI creator to repurpose existing items and create new items. Eventually, GeneRec can perform content retrieval, repurposing, and creation to satisfy users' information needs. Besides, to ensure the trustworthiness of the generated items, we emphasize various fidelity checks. Moreover, we provide a roadmap to envision future developments of GeneRec and several domain-specific applications of GeneRec with potential research tasks. Lastly, we study the feasibility of implementing AI editor and AI creator on micro-video generation.",
        "authors": [
            "Wenjie Wang",
            "Xinyu Lin",
            "Fuli Feng",
            "Xiangnan He",
            "Tat-seng Chua"
        ],
        "citations": 64,
        "references": 96,
        "year": 2023
    },
    {
        "title": "Illuminating protein space with a programmable generative model",
        "abstract": null,
        "authors": [
            "John Ingraham",
            "Max Baranov",
            "Zak Costello",
            "Vincent Frappier",
            "Ahmed Ismail",
            "Shan Tie",
            "Wujie Wang",
            "Vincent Xue",
            "F. Obermeyer",
            "Andrew L. Beam",
            "G. Grigoryan"
        ],
        "citations": 273,
        "references": 126,
        "year": 2022
    },
    {
        "title": "Natural scene reconstruction from fMRI signals using generative latent diffusion",
        "abstract": null,
        "authors": [
            "Furkan Ozcelik",
            "Rufin VanRullen"
        ],
        "citations": 62,
        "references": 51,
        "year": 2023
    },
    {
        "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
        "abstract": "Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes:\"an image is worth a thousand words\". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at \\url{https://ip-adapter.github.io}.",
        "authors": [
            "Hu Ye",
            "Jun Zhang",
            "Siyi Liu",
            "Xiao Han",
            "Wei Yang"
        ],
        "citations": 463,
        "references": 51,
        "year": 2023
    },
    {
        "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
        "abstract": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
        "authors": [
            "Elias Frantar",
            "Dan Alistarh"
        ],
        "citations": 473,
        "references": 56,
        "year": 2023
    },
    {
        "title": "ChatGPT and Generative AI: Possibilities for Its Contribution to Lesson Planning, Critical Thinking and Openness in Teacher Education",
        "abstract": "Although artificial intelligence (AI) has been part of our lives for some time, the launch of the Generative Pretrained Transformer (ChatGPT) has given it renewed attention. While most of these debates are about higher education in general, this article focuses on schoolteacher education and teacher training. This research aimed to determine the contribution of generative AI tools such as ChatGPT in lesson planning, critical thinking and openness in education. The research used a qualitative approach and document analysis following an interpretative paradigm. The findings reveal that generative language models such as ChatGPT can provide specific materials and support mechanisms, such as lesson plans, to schoolteachers and student teachers. It also showed that ChatGPT has levelled the playing field by opening access to lesson plans to all teachers. However, to unleash their full potential for education, it is crucial to approach these models with caution and critically evaluate their limitations and potential biases, understanding that they are tools to support teaching and learning and do not replace teachers. The study’s contribution lies in ChatGPT-generated lesson plans’ implications and the enhancement of critical thinking for teacher education, and it also underscores the need for further research to explore best practices for integrating ChatGPT in lesson planning.",
        "authors": [
            "G. van den Berg",
            "Elize du Plessis"
        ],
        "citations": 81,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Generative AI for Medical Imaging: extending the MONAI Framework",
        "abstract": "Recent advances in generative AI have brought incredible breakthroughs in several areas, including medical imaging. These generative models have tremendous potential not only to help safely share medical data via synthetic datasets but also to perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due to the complexity of these models, their implementation and reproducibility can be difficult. This complexity can hinder progress, act as a use barrier, and dissuade the comparison of new methods with existing works. In this study, we present MONAI Generative Models, a freely available open-source platform that allows researchers and developers to easily train, evaluate, and deploy generative models and related applications. Our platform reproduces state-of-art studies in a standardised way involving different architectures (such as diffusion models, autoregressive transformers, and GANs), and provides pre-trained models for the community. We have implemented these models in a generalisable fashion, illustrating that their results can be extended to 2D or 3D scenarios, including medical images with different modalities (like CT, MRI, and X-Ray data) and from different anatomical areas. Finally, we adopt a modular and extensible approach, ensuring long-term maintainability and the extension of current applications for future features.",
        "authors": [
            "W. H. Pinaya",
            "M. Graham",
            "E. Kerfoot",
            "Petru-Daniel Tudosiu",
            "J. Dafflon",
            "Virginia Fernandez",
            "Pedro Sanchez",
            "Julia Wolleb",
            "P. F. D. Costa",
            "Ashay Patel",
            "Hyungjin Chung",
            "Can Zhao",
            "Wei Peng",
            "Zelong Liu",
            "X. Mei",
            "Oeslle Lucena",
            "Jong-Chul Ye",
            "S. Tsaftaris",
            "Prerna Dogra",
            "Andrew Feng",
            "M. Modat",
            "P. Nachev",
            "S. Ourselin",
            "M. Cardoso"
        ],
        "citations": 50,
        "references": 59,
        "year": 2023
    },
    {
        "title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks",
        "abstract": "As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.",
        "authors": [
            "Rui Zhu",
            "Qihang Zhao",
            "J. Eshraghian"
        ],
        "citations": 73,
        "references": 88,
        "year": 2023
    },
    {
        "title": "BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine",
        "abstract": "Foundation models (FMs) have exhibited remarkable performance across a wide range of downstream tasks in many domains. Nevertheless, general-purpose FMs often face challenges when confronted with domain-specific problems, due to their limited access to the proprietary training data in a particular domain. In biomedicine, there are various biological modalities, such as molecules, proteins, and cells, which are encoded by the language of life and exhibit significant modality gaps with human natural language. In this paper, we introduce BioMedGPT, an open multimodal generative pre-trained transformer (GPT) for biomedicine, to bridge the gap between the language of life and human natural language. BioMedGPT allows users to easily ``communicate'' with diverse biological modalities through free text, which is the first of its kind. BioMedGPT aligns different biological modalities with natural language via a large generative language model, namely, BioMedGPT-LM. We publish BioMedGPT-10B, which unifies the feature spaces of molecules, proteins, and natural language via encoding and alignment. Through fine-tuning, BioMedGPT-10B outperforms or is on par with human and significantly larger general-purpose foundation models on the biomedical QA task. It also demonstrates promising performance in the molecule QA and protein QA tasks, which could greatly accelerate the discovery of new drugs and therapeutic targets. In addition, BioMedGPT-LM-7B is the first large generative language model based on Llama2 in the biomedical domain, therefore is commercial friendly. Both BioMedGPT-10B and BioMedGPT-LM-7B are open-sourced to the research community. In addition, we publish the datasets that are meticulously curated for the alignment of multi-modalities, i.e., PubChemQA and UniProtQA. All the models, codes, and datasets are available at \\url{https://github.com/PharMolix/OpenBioMed}.",
        "authors": [
            "Yi Luo",
            "Jiahuan Zhang",
            "Siqi Fan",
            "Kai Yang",
            "Yushuai Wu",
            "Mu Qiao",
            "Zaiqing Nie"
        ],
        "citations": 60,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Adapting Frechet Audio Distance for Generative Music Evaluation",
        "abstract": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.",
        "authors": [
            "Azalea Gui",
            "H. Gamper",
            "Sebastian Braun",
            "Dimitra Emmanouilidou"
        ],
        "citations": 44,
        "references": 30,
        "year": 2023
    },
    {
        "title": "The Generative AI Paradox: \"What It Can Create, It May Not Understand\"",
        "abstract": "The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.",
        "authors": [
            "Peter West",
            "Ximing Lu",
            "Nouha Dziri",
            "Faeze Brahman",
            "Linjie Li",
            "Jena D. Hwang",
            "Liwei Jiang",
            "Jillian R. Fisher",
            "Abhilasha Ravichander",
            "Khyathi Raghavi Chandu",
            "Benjamin Newman",
            "Pang Wei Koh",
            "Allyson Ettinger",
            "Yejin Choi"
        ],
        "citations": 55,
        "references": 56,
        "year": 2023
    },
    {
        "title": "GOGGLE: Generative Modelling for Tabular Data by Learning Relational Structure",
        "abstract": "Deep generative models learn highly complex and non-linear representations to generate realistic synthetic data. While they have achieved notable success in computer vision and natural language processing, similar advances have been less demonstrable in the tabular domain. This is partially because generative modelling of tabular data entails a particular set of challenges, including heterogeneous relationships, limited number of samples, and difficulties in incorporating prior knowledge. Additionally, unlike their counterparts in image and sequence domain, deep generative models for tabular data almost exclusively employ fully-connected layers, which encode weak inductive biases about relationships between inputs. Real-world data generating processes can often be represented using relational structures, which encode sparse, heterogeneous relationships between variables. In this work, we learn and exploit relational structure underlying tabular data (where typical dimensionality d < 100) to better model variable dependence, and as a natural means to introduce regularization on relationships and include prior knowledge. Specifically, we introduce GOGGLE, an end-to-end message passing scheme that jointly learns the relational structure and corresponding functional relationships as the basis of generating synthetic samples. Using real-world datasets, we provide empirical evidence that the proposed method is effective in generating realistic synthetic data and exploiting domain knowledge for downstream tasks.",
        "authors": [
            "Tennison Liu",
            "Zhaozhi Qian",
            "Jeroen Berrevoets",
            "M. Schaar"
        ],
        "citations": 38,
        "references": 87,
        "year": 2023
    },
    {
        "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
        "abstract": "Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.",
        "authors": [
            "Aidan Gilson",
            "Conrad W Safranek",
            "Thomas Huang",
            "V. Socrates",
            "Ling Chi",
            "R. Taylor",
            "David Chartash"
        ],
        "citations": 1000,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Accelerating Innovation With Generative AI: AI-Augmented Digital Prototyping and Innovation Methods",
        "abstract": "Easy-to-use generative artificial intelligence (AI) is democratizing the use of AI in innovation management and may significantly change the way how we work and innovate. In this article, we show how large language models (LLMs), such as generative pretrained transformer (GPT), can augment the early phases of innovation, in particular, exploration, ideation, and digital prototyping. Drawing on six months of experimenting with LLMs in internal and client innovation projects, we share first-hand experiences and concrete examples of AI-assisted approaches. The article highlights a large variety of use cases for generative AI ranging from user journey mapping to idea generation and prototyping and foreshadows the promising role LLMs may play in future knowledge management systems. Moreover, we argue that generative AI may become a game changer in early prototyping as the delegation of tasks to an artificial agent can result in faster iterations and reduced costs. Our experiences also provide insights into how human innovation teams purposively and effectively interact with AIs and integrate them into their workflows.",
        "authors": [
            "Volker Bilgram",
            "F. Laarmann"
        ],
        "citations": 68,
        "references": 15,
        "year": 2023
    },
    {
        "title": "Compositional Generative Modeling: A Single Model is Not All You Need",
        "abstract": "Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.",
        "authors": [
            "Yilun Du",
            "L. Kaelbling"
        ],
        "citations": 10,
        "references": 51,
        "year": 2024
    },
    {
        "title": "PointGPT: Auto-regressively Generative Pre-training from Point Clouds",
        "abstract": "Large language models (LLMs) based on the generative pre-training transformer (GPT) have demonstrated remarkable effectiveness across a diverse range of downstream tasks. Inspired by the advancements of the GPT, we present PointGPT, a novel approach that extends the concept of GPT to point clouds, addressing the challenges associated with disorder properties, low information density, and task gaps. Specifically, a point cloud auto-regressive generation task is proposed to pre-train transformer models. Our method partitions the input point cloud into multiple point patches and arranges them in an ordered sequence based on their spatial proximity. Then, an extractor-generator based transformer decoder, with a dual masking strategy, learns latent representations conditioned on the preceding point patches, aiming to predict the next one in an auto-regressive manner. Our scalable approach allows for learning high-capacity models that generalize well, achieving state-of-the-art performance on various downstream tasks. In particular, our approach achieves classification accuracies of 94.9% on the ModelNet40 dataset and 93.4% on the ScanObjectNN dataset, outperforming all other transformer models. Furthermore, our method also attains new state-of-the-art accuracies on all four few-shot learning benchmarks.",
        "authors": [
            "Guang-Sheng Chen",
            "Meiling Wang",
            "Yi Yang",
            "Kai Yu",
            "Li Yuan",
            "Yufeng Yue"
        ],
        "citations": 55,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Designing Participatory AI: Creative Professionals’ Worries and Expectations about Generative AI",
        "abstract": "Generative AI, i.e., the group of technologies that automatically generate visual or written content based on text prompts, has undergone a leap in complexity and become widely available within just a few years. Such technologies potentially introduce a massive disruption to creative fields. This paper presents the results of a qualitative survey (N = 23) investigating how creative professionals think about generative AI. The results show that the advancement of these AI models prompts important reflections on what defines creativity and how creatives imagine using AI to support their workflows. Based on these reflections, we discuss how we might design participatory AI in the domain of creative expertise with the goal of empowering creative professionals in their present and future coexistence with AI.",
        "authors": [
            "Nanna Inie",
            "Jeanette Falk",
            "Steve Tanimoto"
        ],
        "citations": 53,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Generative Judge for Evaluating Alignment",
        "abstract": "The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, Auto-J, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, Auto-J outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.",
        "authors": [
            "Junlong Li",
            "Shichao Sun",
            "Weizhe Yuan",
            "Run-Ze Fan",
            "Hai Zhao",
            "Pengfei Liu"
        ],
        "citations": 53,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",
        "abstract": "Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies. State-of-the-art models like OpenAI’s ChatGPT [8] and GPT-4 [9] could enhance programming education in various roles, e.g., by acting as a personalized digital tutor for a student, a digital assistant for an educator, and a digital peer for collaborative learning [1, 2, 7]. In our work, we seek to comprehensively evaluate and benchmark state-of-the-art large language models for various scenarios in programming education. Recent works have evaluated several large language models in the context of programming education [4, 6, 10, 11, 12]. However, these works are limited for several reasons: they have typically focused on evaluating a specific model for a specific education scenario (e.g., generating explanations), or have considered models that are already outdated (e.g., OpenAI’s Codex [3] is no longer publicly available since March 2023). Consequently, there is a lack of systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios in programming education. These scenarios are designed to capture distinct roles these models could play, namely digital tutors, assistants, and peers, as discussed above. More concretely, we consider the following six scenarios: (1) program repair, i.e., fixing a student’s buggy program; (2) hint generation, i.e., providing a natural language hint to the student to help resolve current issues; (3) grading feedback, i.e., grading a student’s program w.r.t. a given rubric; (4) peer programming, i.e., completing a partially written program or generating a sketch for the solution program; (5) task creation, i.e., generating new tasks that exercise specific types of concepts or bugs; (6) contextualized explanation, i.e., explaining specific concepts or functions in the context of a given program. Our study uses a mix of quantitative and qualitative evaluation to compare the performance of these models with the performance of human tutors. We conduct our evaluation based on 5 introductory Python programming problems with a diverse set of input/output specifications. For each of these problems, we consider 5 buggy programs based on publicly accessible submissions from geeksforgeeks.org [5] (see Figure 1); these buggy programs are picked to capture different types of bugs for each problem. We will provide a detailed analysis of the data and results in a longer version of this poster. Our preliminary results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors’ performance for several scenarios.",
        "authors": [
            "Tung Phung",
            "Victor-Alexandru Pădurean",
            "J. Cambronero",
            "Sumit Gulwani",
            "Tobias Kohn",
            "R. Majumdar",
            "A. Singla",
            "Gustavo Soares"
        ],
        "citations": 54,
        "references": 28,
        "year": 2023
    },
    {
        "title": "RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions",
        "abstract": "Generative AI models have shown impressive ability to produce images with text prompts, which could benefit creativity in visual art creation and self-expression. However, it is unclear how precisely the generated images express contexts and emotions from the input texts. We explored the emotional expressiveness of AI-generated images and developed RePrompt, an automatic method to refine text prompts toward precise expression of the generated images. Inspired by crowdsourced editing strategies, we curated intuitive text features, such as the number and concreteness of nouns, and trained a proxy model to analyze the feature effects on the AI-generated image. With model explanations of the proxy model, we curated a rubric to adjust text prompts to optimize image generation for precise emotion expression. We conducted simulation and user studies, which showed that RePrompt significantly improves the emotional expressiveness of AI-generated images, especially for negative emotions.",
        "authors": [
            "Yunlong Wang",
            "Shuyuan Shen",
            "Brian Y. Lim"
        ],
        "citations": 66,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Structure and Content-Guided Video Synthesis with Diffusion Models",
        "abstract": "Text-guided generative diffusion models unlock powerful image creation and editing tools. Recent approaches that edit the content of footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames.In this work, we present a structure and content-guided video diffusion model that edits videos based on descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. A novel guidance method, enabled by joint video and image training, exposes explicit control over temporal consistency. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.",
        "authors": [
            "Patrick Esser",
            "Johnathan Chiu",
            "Parmida Atighehchian",
            "Jonathan Granskog",
            "Anastasis Germanidis"
        ],
        "citations": 414,
        "references": 69,
        "year": 2023
    },
    {
        "title": "A 3D Generative Model for Structure-Based Drug Design",
        "abstract": "We study a fundamental problem in structure-based drug design -- generating molecules that bind to specific protein binding sites. While we have witnessed the great success of deep generative models in drug design, the existing methods are mostly string-based or graph-based. They are limited by the lack of spatial information and thus unable to be applied to structure-based design tasks. Particularly, such models have no or little knowledge of how molecules interact with their target proteins exactly in 3D space. In this paper, we propose a 3D generative model that generates molecules given a designated 3D protein binding site. Specifically, given a binding site as the 3D context, our model estimates the probability density of atom's occurrences in 3D space -- positions that are more likely to have atoms will be assigned higher probability. To generate 3D molecules, we propose an auto-regressive sampling scheme -- atoms are sampled sequentially from the learned distribution until there is no room for new atoms. Combined with this sampling scheme, our model can generate valid and diverse molecules, which could be applicable to various structure-based molecular design tasks such as molecule sampling and linker design. Experimental results demonstrate that molecules sampled from our model exhibit high binding affinity to specific targets and good drug properties such as drug-likeness even if the model is not explicitly optimized for them.",
        "authors": [
            "Shitong Luo"
        ],
        "citations": 150,
        "references": 38,
        "year": 2022
    },
    {
        "title": "Video Diffusion Models",
        "abstract": "Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/",
        "authors": [
            "Jonathan Ho",
            "Tim Salimans",
            "Alexey Gritsenko",
            "William Chan",
            "Mohammad Norouzi",
            "David J. Fleet"
        ],
        "citations": 1000,
        "references": 81,
        "year": 2022
    },
    {
        "title": "The Social Impact of Generative AI: An Analysis on ChatGPT",
        "abstract": "In recent months, the impact of Artificial Intelligence (AI) on citizens’ lives has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a citizen-centric AI.",
        "authors": [
            "M. T. Baldassarre",
            "D. Caivano",
            "Berenice Fernandez Nieto",
            "Domenico Gigante",
            "Azzurra Ragone"
        ],
        "citations": 45,
        "references": 101,
        "year": 2023
    },
    {
        "title": "Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network",
        "abstract": "Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized by neural networks. We show that our method, called JSP-GFN, offers an accurate approximation of the joint posterior, while comparing favorably against existing methods on both simulated and real data.",
        "authors": [
            "T. Deleu",
            "Mizu Nishikawa-Toomey",
            "Jithendaraa Subramanian",
            "Nikolay Malkin",
            "Laurent Charlin",
            "Y. Bengio"
        ],
        "citations": 39,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Generative artificial intelligence",
        "abstract": null,
        "authors": [
            "Leonardo Banh",
            "G. Strobel"
        ],
        "citations": 42,
        "references": 184,
        "year": 2023
    },
    {
        "title": "Recommender Systems with Generative Retrieval",
        "abstract": "Modern recommender systems perform large-scale retrieval by first embedding queries and item candidates in the same unified space, followed by approximate nearest neighbor search to select top candidates given a query embedding. In this paper, we propose a novel generative retrieval approach, where the retrieval model autoregressively decodes the identifiers of the target candidates. To that end, we create semantically meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic IDs for items in a user session, a Transformer-based sequence-to-sequence model is trained to predict the Semantic ID of the next item that the user will interact with. To the best of our knowledge, this is the first Semantic ID-based generative model for recommendation tasks. We show that recommender systems trained with the proposed paradigm significantly outperform the current SOTA models on various datasets. In addition, we show that incorporating Semantic IDs into the sequence-to-sequence model enhances its ability to generalize, as evidenced by the improved retrieval performance observed for items with no prior interaction history.",
        "authors": [
            "Shashank Rajput",
            "Nikhil Mehta",
            "Anima Singh",
            "Raghunandan H. Keshavan",
            "T. Vu",
            "L. Heldt",
            "Lichan Hong",
            "Yi Tay",
            "Vinh Q. Tran",
            "Jonah Samost",
            "Maciej Kula",
            "Ed H. Chi",
            "M. Sathiamoorthy"
        ],
        "citations": 49,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation",
        "abstract": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
        "authors": [
            "Hongtao Wu",
            "Ya Jing",
            "Chi-Hou Cheang",
            "Guangzeng Chen",
            "Jiafeng Xu",
            "Xinghang Li",
            "Minghuan Liu",
            "Hang Li",
            "Tao Kong"
        ],
        "citations": 53,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Toward General Design Principles for Generative AI Applications 130-144",
        "abstract": "Generative AI technologies are growing in power, utility, and use. As generative technologies are being incorporated into mainstream applications, there is a need for guidance on how to design those applications to foster productive and safe use. Based on recent research on human-AI co-creation within the HCI and AI communities, we present a set of seven principles for the design of generative AI applications. These principles are grounded in an environment of generative variability. Six principles are focused on designing for characteristics of generative AI: multiple outcomes&imperfection; exploration&control; and mental models&explanations. In addition, we urge designers to design against potential harms that may be caused by a generative model's hazardous output, misuse, or potential for human displacement. We anticipate these principles to usefully inform design decisions made in the creation of novel human-AI applications, and we invite the community to apply, revise, and extend these principles to their own work.",
        "authors": [
            "Justin D. Weisz",
            "Michael J. Muller",
            "Jessica He",
            "Stephanie Houde"
        ],
        "citations": 43,
        "references": 125,
        "year": 2023
    },
    {
        "title": "Generative Image Dynamics",
        "abstract": "We present an approach to modeling an image-space prior on scene motion. Our prior is learned from a collection of motion trajectories extracted from real video sequences de-picting natural, oscillatory dynamics of objects such as trees, flowers, candles, and clothes swaying in the wind. We model dense, long-term motion in the Fourier domain as spectral volumes, which we find are well-suited to prediction with diffusion models. Given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a spectral volume, which can be converted into a motion texture that spans an entire video. Along with an image-based rendering module, the predicted motion representation can be used for a number of downstream applications, such as turning still images into seamlessly looping videos, or allowing users to interact with objects in real images, producing realistic simulated dynamics (by interpreting the spectral volumes as image-space modal bases). See our project page for more results: generative-dynamics.github.io.",
        "authors": [
            "Zhengqi Li",
            "Richard Tucker",
            "Noah Snavely",
            "Aleksander Holynski"
        ],
        "citations": 44,
        "references": 122,
        "year": 2023
    },
    {
        "title": "DDGR: Continual Learning with Deep Diffusion-based Generative Replay",
        "abstract": "Popular deep-learning models in the field of image classification suffer from catastrophic forgetting—models will forget previously acquired skills when learning new ones. Generative replay (GR), which typically consists of a generator and a classifier, is an efficient way to mitigate catastrophic forgetting. However, conventional GR methods only focus on a single instruction relationship (generator-to-classifier), where the generator synthesizes samples for previous tasks to instruct the training of the classifier, while ignoring the ways in which the classifier can benefit the generator. In addition, most generative replay methods typically reuse the generated samples to update the generator, which causes the samples regenerated by the generator deviating from the distribution of previous tasks. To overcome these two issues, we propose a novel approach, called deep diffusion-based generative replay (DDGR), which adopts a diffusion model as the generator and calculates an instruction-operator through the classifier to instruct the generation of samples. Extensive experiments in class incremental (CI) and class incremental with repetition (CIR) settings demonstrate the advantages of DDGR. Our code is available at https://github.com/ xiaocangshengGR/DDGR .",
        "authors": [
            "R. Gao",
            "Weiwei Liu"
        ],
        "citations": 43,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Generative AI entails a credit–blame asymmetry",
        "abstract": null,
        "authors": [
            "Sebastian Porsdam Mann",
            "B. Earp",
            "Sven Nyholm",
            "J. Danaher",
            "Nikolaj Møller",
            "Hilary Bowman-Smart",
            "Joshua Hatherley",
            "Julian Koplin",
            "Monika Plozza",
            "Daniel Rodger",
            "Peter V. Treit",
            "Gregory Renard",
            "J. McMillan",
            "Julian Savulescu"
        ],
        "citations": 54,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Generative AI for Economic Research: Use Cases and Implications for Economists",
        "abstract": "Generative artificial intelligence (AI) has the potential to revolutionize research. I analyze how large language models (LLMs) such as ChatGPT can assist economists by describing dozens of use cases in six areas: ideation and feedback, writing, background research, data analysis, coding, and mathematical derivations. I provide general instructions and demonstrate specific examples of how to take advantage of each of these, classifying the LLM capabilities from experimental to highly useful. I argue that economists can reap significant productivity gains by taking advantage of generative AI to automate micro-tasks. Moreover, these gains will grow as the performance of AI systems continues to improve. I also speculate on the longer-term implications of AI-powered cognitive automation for economic research. The online resources associated with this paper explain how to get started and will provide regular updates on the latest capabilities of generative AI in economics. (JEL A11, C45, D83, I23, O33)",
        "authors": [
            "Anton Korinek"
        ],
        "citations": 51,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Challenges for higher education in the era of widespread access to Generative AI",
        "abstract": "Abstract The aim of this paper is to discuss the role and impact of Generative Artificial Intelligence (AI) systems in higher education. The proliferation of AI models such as GPT-4, Open Assistant and DALL-E presents a paradigm shift in information acquisition and learning. This transformation poses substantial challenges for traditional teaching approaches and the role of educators. The paper explores the advantages and potential threats of using Generative AI in education and necessary changes in curricula. It further discusses the need to foster digital literacy and the ethical use of AI. The paper’s findings are based on a survey conducted among university students exploring their usage and perception of these AI systems. Finally, recommendations for the use of AI in higher education are offered, which emphasize the need to harness AI’s potential while mitigating its risks. This discourse aims at stimulating policy and strategy development to ensure relevant and effective education in the rapidly evolving digital landscape.",
        "authors": [
            "K. Walczak",
            "W. Cellary"
        ],
        "citations": 42,
        "references": 45,
        "year": 2023
    },
    {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution",
        "abstract": "The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources. Building generative information-seeking models demands openly accessible datasets, which currently remain lacking. In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations. Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset. HAGRID is constructed based on human and LLM collaboration. We first automatically collect attributed explanations that follow an in-context citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability. HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.",
        "authors": [
            "Ehsan Kamalloo",
            "A. Jafari",
            "Xinyu Crystina Zhang",
            "Nandan Thakur",
            "Jimmy J. Lin"
        ],
        "citations": 38,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Persistent Nature: A Generative Model of Unbounded 3D Worlds",
        "abstract": "Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of fixed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. Our scene representation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic sky-dome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long flights through 3D landscapes, while maintaining global scene consistency-for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrapolation beyond the fixed bounds of current 3D generative models, while also supporting a persistent, camera-independent world representation that stands in contrast to auto-regressive 3D prediction models. Our project page: https://chail.github.io/persistent-nature/.",
        "authors": [
            "Lucy Chai",
            "Richard Tucker",
            "Zhengqi Li",
            "Phillip Isola",
            "Noah Snavely"
        ],
        "citations": 25,
        "references": 105,
        "year": 2023
    },
    {
        "title": "Generative AI in Computing Education: Perspectives of Students and Instructors",
        "abstract": "Generative models are now capable of producing natural language text that is, in some cases, comparable in quality to the text produced by people. In the computing education context, these models are being used to generate code, code explanations, and programming exercises. The rapid adoption of these models has prompted multiple position papers and workshops which discuss the implications of these models for computing education, both positive and negative. This paper presents results from a series of semi-structured interviews with 12 students and 6 instructors about their awareness, experiences, and preferences regarding the use of tools powered by generative AI in computing classrooms. The results suggest that Generative AI (GAI) tools will play an increasingly significant role in computing education. However, students and instructors also raised numerous concerns about how these models should be integrated to best support the needs and learning goals of students. We also identified interesting tensions and alignments that emerged between how instructors and students prefer to engage with these models. We discuss these results and provide recommendations related to curriculum development, assessment methods, and pedagogical practice. As GAI tools become increasingly prevalent, it's important to understand educational stakeholders' preferences and values to ensure that these tools can be used for good and that potential harms can be mitigated.",
        "authors": [
            "Cynthia Zastudil",
            "M. Rogalska",
            "C. Kapp",
            "Jennifer L. Vaughn",
            "S. Macneil"
        ],
        "citations": 26,
        "references": 68,
        "year": 2023
    },
    {
        "title": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger",
        "abstract": "Textual backdoor attacks, characterized by subtle manipulations of input triggers and training dataset labels, pose significant threats to security-sensitive applications. The rise of advanced generative models, such as GPT-4, with their capacity for human-like rewriting, makes these attacks increasingly challenging to detect. In this study, we conduct an in-depth examination of black-box generative models as tools for backdoor attacks, thereby emphasizing the need for effective defense strategies. We propose BGMAttack, a novel framework that harnesses advanced generative models to execute stealthier backdoor attacks on text classifiers. Unlike prior approaches constrained by subpar generation quality, BGMAttack renders backdoor triggers more elusive to human cognition and advanced machine detection. A rigorous evaluation of attack effectiveness over four sentiment classification tasks, complemented by four human cognition stealthiness tests, reveals BGMAttack’s superior performance, achieving a state-of-the-art attack success rate of 97.35% on average while maintaining superior stealth compared to conventional methods. The dataset and code are available: https://github.com/JiazhaoLi/BGMAttack.",
        "authors": [
            "Jiazhao Li",
            "Yijin Yang",
            "Zhuofeng Wu",
            "V. Vydiswaran",
            "Chaowei Xiao"
        ],
        "citations": 35,
        "references": 62,
        "year": 2023
    },
    {
        "title": "GenRec: Large Language Model for Generative Recommendation",
        "abstract": "In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recommendation tasks. Subsequently, we use these prompts to fine-tune the LLaMA backbone LLM on a dataset of user-item interactions, represented by textual data, to capture user preferences and item characteristics. Our research underscores the potential of LLM-based generative recommendation in revolutionizing the domain of recommendation systems and offers a foundational framework for future explorations in this field. We conduct extensive experiments on benchmark datasets, and the experiments shows that our GenRec has significant better results on large dataset.",
        "authors": [
            "Jianchao Ji",
            "Zelong Li",
            "Shuyuan Xu",
            "Wenyue Hua",
            "Yingqiang Ge",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "citations": 36,
        "references": 19,
        "year": 2023
    },
    {
        "title": "Unleashing Text-to-Image Diffusion Models for Visual Perception",
        "abstract": "Diffusion models (DMs) have become the new trend of generative models and have demonstrated a powerful ability of conditional synthesis. Among those, text-to-image diffusion models pre-trained on large-scale image-text pairs are highly controllable by customizable prompts. Unlike the unconditional generative models that focus on low-level attributes and details, text-to-image diffusion models contain more high-level knowledge thanks to the vision-language pre-training. In this paper, we propose VPD (Visual Perception with pre-trained Diffusion models), a new framework that exploits the semantic information of a pre-trained text-to-image diffusion model in visual perception tasks. Instead of using the pre-trained denoising autoencoder in a diffusion-based pipeline, we simply use it as a backbone and aim to study how to take full advantage of the learned knowledge. Specifically, we prompt the denoising decoder with proper textual inputs and refine the text features with an adapter, leading to a better alignment to the pre-trained stage and making the visual contents interact with the text prompts. We also propose to utilize the cross-attention maps between the visual features and the text features to provide explicit guidance. Compared with other pre-training methods, we show that vision-language pre-trained diffusion models can be faster adapted to downstream visual perception tasks using the proposed VPD. Extensive experiments on semantic segmentation, referring image segmentation, and depth estimation demonstrate the effectiveness of our method. Notably, VPD attains 0.254 RMSE on NYUv2 depth estimation and 73.3% oIoU on RefCOCO-val referring image segmentation, establishing new records on these two benchmarks. Code is available at https://github.com/wl-zhao/VPD.",
        "authors": [
            "Wenliang Zhao",
            "Yongming Rao",
            "Zuyan Liu",
            "Benlin Liu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "citations": 165,
        "references": 60,
        "year": 2023
    },
    {
        "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
        "abstract": "In conversational search, the user’s real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting.However, manually rewritten queries are not always the best search queries.Thus, training a rewriting model on them would lead to sub-optimal queries. Another useful information to enhance the search query is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers.By combining both, ConvGQR can produce better search queries.In addition, to relate query reformulation to the retrieval task, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR.",
        "authors": [
            "Fengran Mo",
            "Kelong Mao",
            "Yutao Zhu",
            "Yihong Wu",
            "Kaiyu Huang",
            "J. Nie"
        ],
        "citations": 35,
        "references": 54,
        "year": 2023
    },
    {
        "title": "cMolGPT: A Conditional Generative Pre-Trained Transformer for Target-Specific De Novo Molecular Generation",
        "abstract": "Deep generative models applied to the generation of novel compounds in small-molecule drug design have attracted a lot of attention in recent years. To design compounds that interact with specific target proteins, we propose a Generative Pre-Trained Transformer (GPT)-inspired model for de novo target-specific molecular design. By implementing different keys and values for the multi-head attention conditional on a specified target, the proposed method can generate drug-like compounds both with and without a specific target. The results show that our approach (cMolGPT) is capable of generating SMILES strings that correspond to both drug-like and active compounds. Moreover, the compounds generated from the conditional model closely match the chemical space of real target-specific molecules and cover a significant portion of novel compounds. Thus, the proposed Conditional Generative Pre-Trained Transformer (cMolGPT) is a valuable tool for de novo molecule design and has the potential to accelerate the molecular optimization cycle time.",
        "authors": [
            "Ye Wang",
            "Honggang Zhao",
            "Simone Sciabola",
            "Wenlu Wang"
        ],
        "citations": 34,
        "references": 43,
        "year": 2023
    },
    {
        "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
        "abstract": "Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/ ; our dataset can be previewed at https://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version of this work is at https://openreview.net/pdf?id=kaHpo8OZw2 .",
        "authors": [
            "Boxin Wang",
            "Weixin Chen",
            "Hengzhi Pei",
            "Chulin Xie",
            "Mintong Kang",
            "Chenhui Zhang",
            "Chejian Xu",
            "Zidi Xiong",
            "Ritik Dutta",
            "Rylan Schaeffer",
            "Sang Truong",
            "Simran Arora",
            "Mantas Mazeika",
            "Dan Hendrycks",
            "Zi-Han Lin",
            "Yuk-Kit Cheng",
            "Sanmi Koyejo",
            "D. Song",
            "Bo Li"
        ],
        "citations": 327,
        "references": 0,
        "year": 2023
    },
    {
        "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
        "abstract": "Generative Pre-trained Transformer (GPT) models have shown remarkable capabilities for natural language generation, but their performance for machine translation has not been thoroughly investigated. In this paper, we present a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different GPT models in comparison with state-of-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation. We experiment with eighteen different translation directions involving high and low resource languages, as well as non English-centric translations, and evaluate the performance of three GPT models: ChatGPT, GPT3.5 (text-davinci-003), and text-davinci-002. Our results show that GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages. We also show that hybrid approaches, which combine GPT models with other translation systems, can further enhance the translation quality. We perform comprehensive analysis and human evaluation to further understand the characteristics of GPT translations. We hope that our paper provides valuable insights for researchers and practitioners in the field and helps to better understand the potential and limitations of GPT models for translation.",
        "authors": [
            "Amr Hendy",
            "M. Abdelrehim",
            "Amr Sharaf",
            "Vikas Raunak",
            "Mohamed Gabr",
            "Hitokazu Matsushita",
            "Young Jin Kim",
            "M. Afify",
            "H. Awadalla"
        ],
        "citations": 356,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
        "authors": [
            "Weiwei Sun",
            "Lingyong Yan",
            "Xinyu Ma",
            "Pengjie Ren",
            "Dawei Yin",
            "Z. Ren"
        ],
        "citations": 226,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Masked Generative Distillation",
        "abstract": "Knowledge distillation has been applied to various tasks successfully. The current distillation algorithm usually improves students' performance by imitating the output of the teacher. This paper shows that teachers can also improve students' representation power by guiding students' feature recovery. From this point of view, we propose Masked Generative Distillation (MGD), which is simple: we mask random pixels of the student's feature and force it to generate the teacher's full feature through a simple block. MGD is a truly general feature-based distillation method, which can be utilized on various tasks, including image classification, object detection, semantic segmentation and instance segmentation. We experiment on different models with extensive datasets and the results show that all the students achieve excellent improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1 accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP, SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at https://github.com/yzd-v/MGD.",
        "authors": [
            "Zhendong Yang",
            "Zhe Li",
            "Mingqi Shao",
            "Dachuan Shi",
            "Zehuan Yuan",
            "Chun Yuan"
        ],
        "citations": 139,
        "references": 39,
        "year": 2022
    },
    {
        "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget",
        "abstract": "Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.",
        "authors": [
            "Ilia Shumailov",
            "Zakhar Shumaylov",
            "Yiren Zhao",
            "Y. Gal",
            "Nicolas Papernot",
            "Ross Anderson"
        ],
        "citations": 236,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Augmenting Greybox Fuzzing with Generative AI",
        "abstract": "Real-world programs expecting structured inputs often has a format-parsing stage gating the deeper program space. Neither a mutation-based approach nor a generative approach can provide a solution that is effective and scalable. Large language models (LLM) pre-trained with an enormous amount of natural language corpus have proved to be effective for understanding the implicit format syntax and generating format-conforming inputs. In this paper, propose ChatFuzz, a greybox fuzzer augmented by generative AI. More specifically, we pick a seed in the fuzzer's seed pool and prompt ChatGPT generative models to variations, which are more likely to be format-conforming and thus of high quality. We conduct extensive experiments to explore the best practice for harvesting the power of generative LLM models. The experiment results show that our approach improves the edge coverage by 12.77\\% over the SOTA greybox fuzzer (AFL++) on 12 target programs from three well-tested benchmarks. As for vulnerability detection, \\sys is able to perform similar to or better than AFL++ for programs with explicit syntax rules but not for programs with non-trivial syntax.",
        "authors": [
            "Jie Hu",
            "Qian Zhang",
            "Heng Yin"
        ],
        "citations": 29,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Synthetic Data from Diffusion Models Improves ImageNet Classification",
        "abstract": "Deep generative models are becoming increasingly powerful, now generating diverse high fidelity photo-realistic samples given text prompts. Have they reached the point where models of natural images can be used for generative data augmentation, helping to improve challenging discriminative tasks? We show that large-scale text-to image diffusion models can be fine-tuned to produce class conditional models with SOTA FID (1.76 at 256x256 resolution) and Inception Score (239 at 256x256). The model also yields a new SOTA in Classification Accuracy Scores (64.96 for 256x256 generative samples, improving to 69.24 for 1024x1024 samples). Augmenting the ImageNet training set with samples from the resulting models yields significant improvements in ImageNet classification accuracy over strong ResNet and Vision Transformer baselines.",
        "authors": [
            "Shekoofeh Azizi",
            "Simon Kornblith",
            "Chitwan Saharia",
            "Mohammad Norouzi",
            "David J. Fleet"
        ],
        "citations": 240,
        "references": 73,
        "year": 2023
    },
    {
        "title": "High-resolution image reconstruction with latent diffusion models from human brain activity",
        "abstract": "Reconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straight-forward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs. Please check out our webpage at https://sites.google.com/view/stablediffusion-with-brain/.",
        "authors": [
            "Yu Takagi",
            "Shinji Nishimoto"
        ],
        "citations": 195,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet",
        "abstract": "The rapid adoption of generative Artificial Intelligence (AI) tools that can generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have put the societal impacts of these technologies at the center of public debate. These tools are possible due to the massive amount of data (text and images) that is publicly available through the Internet. At the same time, these generative AI tools become content creators that are already contributing to the data that is available to train future models. Therefore, future versions of generative AI tools will be trained with a mix of human-created and AI-generated content, causing a potential feedback loop between generative AI and public data repositories. This interaction raises many questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI generated data? Will they evolve and improve with the new data sets or on the contrary will they degrade? Will evolution introduce biases or reduce diversity in subsequent generations of generative AI tools? What are the societal implications of the possible degradation of these models? Can we mitigate the effects of this feedback loop? In this document, we explore the effect of this interaction and report some initial results using simple diffusion models trained with various image datasets. Our results show that the quality and diversity of the generated images can degrade over time suggesting that incorporating AI-created data can have undesired effects on future versions of generative models.",
        "authors": [
            "Gonzalo Martínez",
            "Lauren Watson",
            "P. Reviriego",
            "José Alberto Hernández",
            "Marc Juárez",
            "Rik Sarkar"
        ],
        "citations": 43,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Wavelet-Improved Score-Based Generative Model for Medical Imaging",
        "abstract": "The score-based generative model (SGM) has demonstrated remarkable performance in addressing challenging under-determined inverse problems in medical imaging. However, acquiring high-quality training datasets for these models remains a formidable task, especially in medical image reconstructions. Prevalent noise perturbations or artifacts in low-dose Computed Tomography (CT) or under-sampled Magnetic Resonance Imaging (MRI) hinder the accurate estimation of data distribution gradients, thereby compromising the overall performance of SGMs when trained with these data. To alleviate this issue, we propose a wavelet-improved denoising technique to cooperate with the SGMs, ensuring effective and stable training. Specifically, the proposed method integrates a wavelet sub-network and the standard SGM sub-network into a unified framework, effectively alleviating inaccurate distribution of the data distribution gradient and enhancing the overall stability. The mutual feedback mechanism between the wavelet sub-network and the SGM sub-network empowers the neural network to learn accurate scores even when handling noisy samples. This combination results in a framework that exhibits superior stability during the learning process, leading to the generation of more precise and reliable reconstructed images. During the reconstruction process, we further enhance the robustness and quality of the reconstructed images by incorporating regularization constraint. Our experiments, which encompass various scenarios of low-dose and sparse-view CT, as well as MRI with varying under-sampling rates and masks, demonstrate the effectiveness of the proposed method by significantly enhanced the quality of the reconstructed images. Especially, our method with noisy training samples achieves comparable results to those obtained using clean data. Our code at https://zenodo.org/record/8266123.",
        "authors": [
            "Weiwen Wu",
            "Yanyang Wang",
            "Qie-gen Liu",
            "Ge Wang",
            "Jianjia Zhang"
        ],
        "citations": 46,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Generative AI for brain image computing and brain network computing: a review",
        "abstract": "Recent years have witnessed a significant advancement in brain imaging techniques that offer a non-invasive approach to mapping the structure and function of the brain. Concurrently, generative artificial intelligence (AI) has experienced substantial growth, involving using existing data to create new content with a similar underlying pattern to real-world data. The integration of these two domains, generative AI in neuroimaging, presents a promising avenue for exploring various fields of brain imaging and brain network computing, particularly in the areas of extracting spatiotemporal brain features and reconstructing the topological connectivity of brain networks. Therefore, this study reviewed the advanced models, tasks, challenges, and prospects of brain imaging and brain network computing techniques and intends to provide a comprehensive picture of current generative AI techniques in brain imaging. This review is focused on novel methodological approaches and applications of related new methods. It discussed fundamental theories and algorithms of four classic generative models and provided a systematic survey and categorization of tasks, including co-registration, super-resolution, enhancement, classification, segmentation, cross-modality, brain network analysis, and brain decoding. This paper also highlighted the challenges and future directions of the latest work with the expectation that future research can be beneficial.",
        "authors": [
            "Changwei Gong",
            "Changhong Jing",
            "Xuhang Chen",
            "Chi-Man Pun",
            "Guoli Huang",
            "Ashirbani Saha",
            "M. Nieuwoudt",
            "Han-Xiong Li",
            "Yong Hu",
            "Shuqiang Wang"
        ],
        "citations": 48,
        "references": 107,
        "year": 2023
    },
    {
        "title": "Towards social generative AI for education: theory, practices and ethics",
        "abstract": "ABSTRACT This opinion paper explores educational interactions involving humans and artificial intelligences not as sequences of prompts and responses, but as a social process of conversation and exploration. In this conception, learners continually converse with AI language models and other human learners within a dynamic computational medium of internet tools and resources. Learning happens when this distributed human-AI system sets goals, builds meaning from data, consolidates understanding, reconciles differences, and transfers knowledge to new domains. Building social generative AI for education will require development of powerful AI systems that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors. This raises fundamental problems of ethics. Such systems should be aware of their limitations, their responsibility to learners and the integrity of the internet, and their respect for human teachers and experts. We need to consider how to design and constrain social generative AI for education.",
        "authors": [
            "M. Sharples"
        ],
        "citations": 44,
        "references": 21,
        "year": 2023
    },
    {
        "title": "ChatGPT: A Case Study on Copyright Challenges for Generative Artificial Intelligence Systems",
        "abstract": "\n This article focuses on copyright issues pertaining to generative artificial intelligence (AI) systems, with particular emphasis on the ChatGPT case study as a primary exemplar. In order to generate high-quality outcomes, generative AI systems require substantial quantities of training data, which may frequently comprise copyright-protected information. This prompts inquiries into the legal principles of fair use, the creation of derivative works and the lawfulness of data gathering and utilisation. The utilisation of input data for the purpose of training and enhancing AI models presents significant concerns regarding potential violations of copyright. This paper offers suggestions for safeguarding the interests of copyright holders and competitors, while simultaneously addressing legal challenges and expediting the advancement of AI technologies. This study analyses the ChatGPT platform as a case example to explore the necessary modifications that copyright regulations must undergo to adequately tackle the intricacies of authorship and ownership in the realm of AI-generated creative content.",
        "authors": [
            "N. Lucchi"
        ],
        "citations": 43,
        "references": 8,
        "year": 2023
    },
    {
        "title": "DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
        "abstract": "Existing text-video retrieval solutions are, in essence, discriminant models focused on maximizing the conditional likelihood, i.e., p(candidates|query). While straightforward, this de facto paradigm overlooks the underlying data distribution p(query), which makes it challenging to identify out-of-distribution data. To address this limitation, we creatively tackle this task from a generative viewpoint and model the correlation between the text and the video as their joint probability p(candidates,query). This is accomplished through a diffusion-based text-video retrieval framework (Diffusion-Ret), which models the retrieval task as a process of gradually generating joint distribution from noise. During training, DiffusionRet is optimized from both the generation and discrimination perspectives, with the generator being optimized by generation loss and the feature extractor trained with contrastive loss. In this way, DiffusionRet cleverly leverages the strengths of both generative and discriminative methods. Extensive experiments on five commonly used text-video retrieval benchmarks, including MSRVTT, LSMDC, MSVD, ActivityNet Captions, and DiDeMo, with superior performances, justify the efficacy of our method. More encouragingly, without any modification, DiffusionRet even performs well in out-domain retrieval settings. We believe this work brings fundamental insights into the related fields. Code is available at https://github.com/jpthu17/DiffusionRet.",
        "authors": [
            "Peng Jin",
            "Hao Li",
            "Zesen Cheng",
            "Kehan Li",
            "Xiang Ji",
            "Chang Liu",
            "Li-ming Yuan",
            "Jie Chen"
        ],
        "citations": 39,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Aligning Text-to-Image Models using Human Feedback",
        "abstract": "Deep generative models have shown impressive results in text-to-image synthesis. However, current text-to-image models often generate images that are inadequately aligned with text prompts. We propose a fine-tuning method for aligning such models using human feedback, comprising three stages. First, we collect human feedback assessing model output alignment from a set of diverse text prompts. We then use the human-labeled image-text dataset to train a reward function that predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing reward-weighted likelihood to improve image-text alignment. Our method generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model. We also analyze several design choices and find that careful investigations on such design choices are important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for learning from human feedback to significantly improve text-to-image models.",
        "authors": [
            "Kimin Lee",
            "Hao Liu",
            "M. Ryu",
            "Olivia Watkins",
            "Yuqing Du",
            "Craig Boutilier",
            "P. Abbeel",
            "M. Ghavamzadeh",
            "S. Gu"
        ],
        "citations": 207,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models",
        "abstract": "Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with\"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io",
        "authors": [
            "Rongjie Huang",
            "Jia-Bin Huang",
            "Dongchao Yang",
            "Yi Ren",
            "Luping Liu",
            "Mingze Li",
            "Zhenhui Ye",
            "Jinglin Liu",
            "Xiaoyue Yin",
            "Zhou Zhao"
        ],
        "citations": 255,
        "references": 70,
        "year": 2023
    },
    {
        "title": "A First Look at LLM-Powered Generative News Recommendation",
        "abstract": "Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. We will publish our code and data 1 for other researchers to reproduce our work.",
        "authors": [
            "Qijiong Liu",
            "Nuo Chen",
            "Tetsuya Sakai",
            "Xiao-Ming Wu"
        ],
        "citations": 40,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Riemannian Score-Based Generative Modeling",
        "abstract": "Score-based generative models (SGMs) are a powerful class of generative models that exhibit remarkable empirical performance. Score-based generative modelling (SGM) consists of a ``noising'' stage, whereby a diffusion is used to gradually add Gaussian noise to data, and a generative model, which entails a ``denoising'' process defined by approximating the time-reversal of the diffusion. Existing SGMs assume that data is supported on a Euclidean space, i.e. a manifold with flat geometry. In many domains such as robotics, geoscience or protein modelling, data is often naturally described by distributions living on Riemannian manifolds and current SGM techniques are not appropriate. We introduce here Riemannian Score-based Generative Models (RSGMs), a class of generative models extending SGMs to Riemannian manifolds. We demonstrate our approach on a variety of manifolds, and in particular with earth and climate science spherical data.",
        "authors": [
            "Valentin De Bortoli",
            "Emile Mathieu",
            "M. Hutchinson",
            "James Thornton",
            "Y. Teh",
            "A. Doucet"
        ],
        "citations": 137,
        "references": 168,
        "year": 2022
    },
    {
        "title": "VQ3D: Learning a 3D-Aware Generative Model on ImageNet",
        "abstract": "Recent work has shown the possibility of training generative models of 3D content from 2D image collections on small datasets corresponding to a single object class, such as human faces, animal faces, or cars. However, these models struggle on larger, more complex datasets. To model diverse and unconstrained image collections such as ImageNet, we present VQ3D, which introduces a NeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1 allows for the reconstruction of an input image and the ability to change the camera position around the image, and our Stage 2 allows for the generation of new 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images from the 1000-class ImageNet dataset of 1.2 million training images, and achieves a competitive ImageNet generation FID score of 16.8. Our project webpage is at this url.",
        "authors": [
            "Kyle Sargent",
            "Jing Yu Koh",
            "Han Zhang",
            "Huiwen Chang",
            "Charles Herrmann",
            "Pratul P. Srinivasan",
            "Jiajun Wu",
            "Deqing Sun"
        ],
        "citations": 26,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Intriguing properties of generative classifiers",
        "abstract": "What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.",
        "authors": [
            "P. Jaini",
            "Kevin Clark",
            "Robert Geirhos"
        ],
        "citations": 25,
        "references": 47,
        "year": 2023
    },
    {
        "title": "A Survey on Generative Diffusion Model",
        "abstract": "—Deep learning shows excellent potential in generation tasks thanks to deep latent representation. Generative models are classes of models that can generate observations randomly with respect to certain implied parameters. Recently, the diffusion Model has become a raising class of generative models by virtue of its power-generating ability. Nowadays, great achievements have been reached. More applications except for computer vision, speech generation, bioinformatics, and natural language processing are to be explored in this ﬁeld. However, the diffusion model has its genuine drawback of a slow generation process, leading to many enhanced works. This survey makes a summary of the ﬁeld of the diffusion model. We ﬁrst state the main problem with two landmark works – DDPM and DSM. Then, we present a diverse range of advanced techniques to speed up the diffusion models – training schedule, training-free sampling, mixed-modeling, and score & diffusion uniﬁcation. Regarding existing models, we also provide a benchmark of FID score, IS, and NLL according to speciﬁc NFE. Moreover, applications with diffusion models are introduced including computer vision, sequence modeling, audio, and AI for science. Finally, there is a summarization of this ﬁeld together with limitations & further directions.",
        "authors": [
            "Hanqun Cao",
            "Cheng Tan",
            "Zhangyang Gao",
            "Yilun Xu",
            "Guangyong Chen",
            "P. Heng",
            "Stan Z. Li"
        ],
        "citations": 146,
        "references": 360,
        "year": 2022
    },
    {
        "title": "Effective Data Augmentation With Diffusion Models",
        "abstract": "Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains.",
        "authors": [
            "Brandon Trabucco",
            "Kyle Doherty",
            "Max Gurinas",
            "R. Salakhutdinov"
        ],
        "citations": 171,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Generative diffusion in very large dimensions",
        "abstract": "Generative models based on diffusion have become the state of the art in the last few years, notably for image generation. Here, we analyze them in the high-dimensional limit, where data are formed by a very large number of variables. We use methods from statistical physics and focus on two well-controlled high-dimensional cases: a Gaussian model and the Curie–Weiss model of ferromagnetism. In the latter case, we highlight the mechanism of symmetry breaking in the inverse diffusion, and point out that, in order to reconstruct the relative asymmetry of the two low-temperature states, and thus to obtain the correct probability weights, one needs a database with a number of points much larger than the dimension of each data point. We characterize the scaling laws in the number of data and in the number of dimensions for an efficient generation.",
        "authors": [
            "G. Biroli",
            "Marc M'ezard"
        ],
        "citations": 24,
        "references": 18,
        "year": 2023
    },
    {
        "title": "Investigating Explainability of Generative AI for Code through Scenario-based Design",
        "abstract": "What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users’ explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users’ explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.",
        "authors": [
            "Jiao Sun",
            "Q. Liao",
            "Michael J. Muller",
            "Mayank Agarwal",
            "Stephanie Houde",
            "Kartik Talamadupula",
            "Justin D. Weisz"
        ],
        "citations": 133,
        "references": 108,
        "year": 2022
    },
    {
        "title": "Learning to Rank in Generative Retrieval",
        "abstract": "Generative retrieval stands out as a promising new paradigm in text retrieval that aims to generate identifier strings of relevant passages as the retrieval target. This generative paradigm taps into powerful generative language models, distinct from traditional sparse or dense retrieval methods. However, only learning to generate is insufficient for generative retrieval. Generative retrieval learns to generate identifiers of relevant passages as an intermediate goal and then converts predicted identifiers into the final passage rank list. The disconnect between the learning objective of autoregressive models and the desired passage ranking target leads to a learning gap. To bridge this gap, we propose a learning-to-rank framework for generative retrieval, dubbed LTRGR. LTRGR enables generative retrieval to learn to rank passages directly, optimizing the autoregressive model toward the final passage ranking target via a rank loss. This framework only requires an additional learning-to-rank training phase to enhance current generative retrieval systems and does not add any burden to the inference stage. We conducted experiments on three public benchmarks, and the results demonstrate that LTRGR achieves state-of-the-art performance among generative retrieval methods. The code and checkpoints are released at https://github.com/liyongqi67/LTRGR.",
        "authors": [
            "Yongqing Li",
            "Nan Yang",
            "Liang Wang",
            "Furu Wei",
            "Wenjie Li"
        ],
        "citations": 30,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Large language models in education: A focus on the complementary relationship between human teachers and ChatGPT",
        "abstract": null,
        "authors": [
            "Jae-Bong Jeon",
            "Seongyong Lee"
        ],
        "citations": 228,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Trainability barriers and opportunities in quantum generative modeling",
        "abstract": "Quantum generative models, in providing inherently efficient sampling strategies, show promise for achieving a near-term advantage on quantum hardware. Nonetheless, important questions remain regarding their scalability. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using implicit generative models (such as quantum circuit-based models) with explicit losses (such as the KL divergence) leads to a new flavour of barren plateau. In contrast, the Maximum Mean Discrepancy (MMD), which is a popular example of an implicit loss, can be viewed as the expectation value of an observable that is either low-bodied and trainable, or global and untrainable depending on the choice of kernel. However, in parallel, we highlight that the low-bodied losses required for trainability cannot in general distinguish high-order correlations, leading to a fundamental tension between exponential concentration and the emergence of spurious minima. We further propose a new local quantum fidelity-type loss which, by leveraging quantum circuits to estimate the quality of the encoded distribution, is both faithful and enjoys trainability guarantees. Finally, we compare the performance of different loss functions for modelling real-world data from the High-Energy-Physics domain and confirm the trends predicted by our theoretical results.",
        "authors": [
            "Manuel S. Rudolph",
            "S. Lerch",
            "Supanut Thanasilp",
            "Oriel Kiss",
            "S. Vallecorsa",
            "M. Grossi",
            "Zoe Holmes"
        ],
        "citations": 20,
        "references": 107,
        "year": 2023
    },
    {
        "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
        "abstract": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTHl4/mage.",
        "authors": [
            "Tianhong Li",
            "Huiwen Chang",
            "Shlok Kumar Mishra",
            "Han Zhang",
            "D. Katabi",
            "Dilip Krishnan"
        ],
        "citations": 112,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
        "abstract": "We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate seven versions of GPT models, including ChatGPT. We show that our method for translation quality assessment only works with GPT 3.5 and larger models. Comparing to results from WMT22’s Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.",
        "authors": [
            "Tom Kocmi",
            "C. Federmann"
        ],
        "citations": 271,
        "references": 28,
        "year": 2023
    },
    {
        "title": "PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology",
        "abstract": "As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We open-source our dataset, as well as a comprehensive toolkit for extensive pathology data collection and preprocessing at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology.",
        "authors": [
            "Yuxuan Sun",
            "Chenglu Zhu",
            "S. Zheng",
            "Kai Zhang",
            "Zhongyi Shui",
            "Xiaoxuan Yu",
            "Yizhi Zhao",
            "Honglin Li",
            "Yunlong Zhang",
            "Ruojia Zhao",
            "Xinheng Lyu",
            "Lin Yang"
        ],
        "citations": 28,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Generative artificial intelligence empowers digital twins in drug discovery and clinical trials",
        "abstract": "ABSTRACT Introduction The concept of Digital Twins (DTs) translated to drug development and clinical trials describes virtual representations of systems of various complexities, ranging from individual cells to entire humans, and enables in silico simulations and experiments. DTs increase the efficiency of drug discovery and development by digitalizing processes associated with high economic, ethical, or social burden. The impact is multifaceted: DT models sharpen disease understanding, support biomarker discovery and accelerate drug development, thus advancing precision medicine. One way to realize DTs is by generative artificial intelligence (AI), a cutting-edge technology that enables the creation of novel, realistic and complex data with desired properties. Areas covered The authors provide a brief introduction to generative AI and describe how it facilitates the modeling of DTs. In addition, they compare existing implementations of generative AI for DTs in drug discovery and clinical trials. Finally, they discuss technical and regulatory challenges that should be addressed before DTs can transform drug discovery and clinical trials. Expert opinion The current state of DTs in drug discovery and clinical trials does not exploit the entire power of generative AI yet and is limited to simulation of a small number of characteristics. Nonetheless, generative AI has the potential to transform the field by leveraging recent developments in deep learning and customizing models for the needs of scientists, physicians and patients.",
        "authors": [
            "M. Bordukova",
            "N. Makarov",
            "Raul Rodriguez-Esteban",
            "F. Schmich",
            "M. P. Menden"
        ],
        "citations": 35,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Scalable and Effective Generative Information Retrieval",
        "abstract": "Recent research has shown that transformer networks can be used as differentiable search indexes by representing each document as a sequence of document ID tokens. These generative retrieval models cast the retrieval problem to a document ID generation problem for each query. Despite their elegant design, existing generative retrieval models only perform well on artificially-constructed and small-scale collections. This paper represents an important milestone in generative retrieval research by showing that generative retrieval models can be trained to perform effectively on large-scale standard retrieval benchmarks. In more detail, we propose RIPOR- an optimization framework for generative retrieval that is designed based on two often-overlooked fundamental design considerations. First, RIPOR introduces a novel prefix-oriented ranking optimization algorithm for accurate estimation of relevance score during sequential document ID generation. Second, RIPOR constructs document IDs based on the relevance associations between queries and documents. Evaluation on MSMARCO and TREC Deep Learning Track reveals that RIPOR surpasses state-of-the-art generative retrieval models by a large margin (e.g., 30.5% MRR improvements on MS MARCO Dev Set).",
        "authors": [
            "Hansi Zeng",
            "Chen Luo",
            "Bowen Jin",
            "Sheikh Muhammad Sarwar",
            "Tianxin Wei",
            "Hamed Zamani"
        ],
        "citations": 23,
        "references": 75,
        "year": 2023
    },
    {
        "title": "PoET: A generative model of protein families as sequences-of-sequences",
        "abstract": "Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\\textbf{P}$r$\\textbf{o}$tein $\\textbf{E}$volutionary $\\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens sequentially within sequences while attending between sequences order invariantly, allowing PoET to scale to context lengths beyond those used during training. In extensive experiments on deep mutational scanning datasets, we show that PoET outperforms existing protein language models and evolutionary sequence models for variant function prediction across proteins of all MSA depths. We also demonstrate PoET's ability to controllably generate new protein sequences.",
        "authors": [
            "Timothy F. Truong",
            "Tristan Bepler"
        ],
        "citations": 24,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Diffusion Model for Generative Image Denoising",
        "abstract": "In supervised learning for image denoising, usually the paired clean images and noisy images are collected or synthesised to train a denoising model. L2 norm loss or other distance functions are used as the objective function for training. It often leads to an over-smooth result with less image details. In this paper, we regard the denoising task as a problem of estimating the posterior distribution of clean images conditioned on noisy images. We apply the idea of diffusion model to realize generative image denoising. According to the noise model in denoising tasks, we redefine the diffusion process such that it is different from the original one. Hence, the sampling of the posterior distribution is a reverse process of dozens of steps from the noisy image. We consider three types of noise model, Gaussian, Gamma and Poisson noise. With the guarantee of theory, we derive a unified strategy for model training. Our method is verified through experiments on three types of noise models and achieves excellent performance.",
        "authors": [
            "Yutong Xie",
            "Minne Yuan",
            "Bin Dong",
            "Quanzheng Li"
        ],
        "citations": 27,
        "references": 33,
        "year": 2023
    },
    {
        "title": "MatFusion: A Generative Diffusion Model for SVBRDF Capture",
        "abstract": "We formulate SVBRDF estimation from photographs as a diffusion task. To model the distribution of spatially varying materials, we first train a novel unconditional SVBRDF diffusion backbone model on a large set of 312, 165 synthetic spatially varying material exemplars. This SVBRDF diffusion backbone model, named MatFusion, can then serve as a basis for refining a conditional diffusion model to estimate the material properties from a photograph under controlled or uncontrolled lighting. Our backbone MatFusion model is trained using only a loss on the reflectance properties, and therefore refinement can be paired with more expensive rendering methods without the need for backpropagation during training. Because the conditional SVBRDF diffusion models are generative, we can synthesize multiple SVBRDF estimates from the same input photograph from which the user can select the one that best matches the users’ expectation. We demonstrate the flexibility of our method by refining different SVBRDF diffusion models conditioned on different types of incident lighting, and show that for a single photograph under colocated flash lighting our method achieves equal or better accuracy than existing SVBRDF estimation methods.",
        "authors": [
            "Sam Sartor",
            "Pieter Peers"
        ],
        "citations": 23,
        "references": 47,
        "year": 2023
    },
    {
        "title": "RePaint: Inpainting using Denoising Diffusion Probabilistic Models",
        "abstract": "Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint",
        "authors": [
            "Andreas Lugmayr",
            "Martin Danelljan",
            "Andrés Romero",
            "F. Yu",
            "R. Timofte",
            "L. Gool"
        ],
        "citations": 1000,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications",
        "abstract": "Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy",
        "authors": [
            "Ling Yang",
            "Zhilong Zhang",
            "Shenda Hong",
            "Runsheng Xu",
            "Yue Zhao",
            "Yingxia Shao",
            "Wentao Zhang",
            "Ming-Hsuan Yang",
            "Bin Cui"
        ],
        "citations": 1000,
        "references": 394,
        "year": 2022
    },
    {
        "title": "The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification",
        "abstract": "This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. We have associated the identified vulnerabilities with Common Weakness Enumeration (CWE) numbers. We make the source code available for the 112,000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms. Our study unveiled that according to ESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.",
        "authors": [
            "Norbert Tihanyi",
            "Tamás Bisztray",
            "Ridhi Jain",
            "M. Ferrag",
            "L. Cordeiro",
            "Vasileios Mavroeidis"
        ],
        "citations": 33,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Imitating Human Behaviour with Diffusion Models",
        "abstract": "Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment.",
        "authors": [
            "Tim Pearce",
            "Tabish Rashid",
            "A. Kanervisto",
            "David Bignell",
            "Mingfei Sun",
            "Raluca Georgescu",
            "Sergio Valcarcel Macua",
            "Shan Zheng Tan",
            "I. Momennejad",
            "Katja Hofmann",
            "Sam Devlin"
        ],
        "citations": 155,
        "references": 44,
        "year": 2023
    },
    {
        "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
        "abstract": "Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA.",
        "authors": [
            "Jakub L'ala",
            "Odhran O'Donoghue",
            "Aleksandar Shtedritski",
            "Sam Cox",
            "Samuel G. Rodriques",
            "Andrew D. White"
        ],
        "citations": 48,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Large language models generate functional protein sequences across diverse families",
        "abstract": null,
        "authors": [
            "Ali Madani",
            "Ben Krause",
            "E. Greene",
            "Subu Subramanian",
            "Benjamin P. Mohr",
            "J. Holton",
            "J. L. Olmos",
            "Caiming Xiong",
            "Zachary Z Sun",
            "R. Socher",
            "J. Fraser",
            "N. Naik"
        ],
        "citations": 521,
        "references": 98,
        "year": 2023
    },
    {
        "title": "A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture",
        "abstract": "This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation. The research elucidates the key phases of the information storage and retrieval methodology underpinned by the RAG model. A comprehensive analysis of these steps is undertaken, emphasizing their significance in addressing the scarcity of data. The study highlights the efficacy of the proposed method, showcasing its applicability through illustrative instances. By implementing the RAG model for information storage and retrieval, the research not only contributes to a deeper comprehension of generative AI technology but also facilitates its practical usability within enterprises utilizing LLMs. This work holds substantial value in advancing the field of generative AI, offering insights into enhancing data-driven content generation and fostering active utilization of LLM-based services within corporate settings.",
        "authors": [
            "CheonSu Jeong"
        ],
        "citations": 31,
        "references": 38,
        "year": 2023
    },
    {
        "title": "DiffusionSat: A Generative Foundation Model for Satellite Imagery",
        "abstract": "Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets. As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, superresolution given multi-spectral inputs and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale generative foundation model for satellite imagery. The project website can be found here: https://samar-khanna.github.io/DiffusionSat/",
        "authors": [
            "Samar Khanna",
            "Patrick Liu",
            "Linqi Zhou",
            "Chenlin Meng",
            "Robin Rombach",
            "Marshall Burke",
            "David B. Lobell",
            "Stefano Ermon"
        ],
        "citations": 34,
        "references": 90,
        "year": 2023
    },
    {
        "title": "DiM: Distilling Dataset into Generative Model",
        "abstract": "Dataset distillation reduces the network training cost by synthesizing small and informative datasets from large-scale ones. Despite the success of the recent dataset distillation algorithms, three drawbacks still limit their wider application: i). the synthetic images perform poorly on large architectures; ii). they need to be re-optimized when the distillation ratio changes; iii). the limited diversity restricts the performance when the distillation ratio is large. In this paper, we propose a novel distillation scheme to \\textbf{D}istill information of large train sets \\textbf{i}nto generative \\textbf{M}odels, named DiM. Specifically, DiM learns to use a generative model to store the information of the target dataset. During the distillation phase, we minimize the differences in logits predicted by a models pool between real and generated images. At the deployment stage, the generative model synthesizes various training samples from random noises on the fly. Due to the simple yet effective designs, the trained DiM can be directly applied to different distillation ratios and large architectures without extra cost. We validate the proposed DiM across 4 datasets and achieve state-of-the-art results on all of them. To the best of our knowledge, we are the first to achieve higher accuracy on complex architectures than simple ones, such as 75.1\\% with ResNet-18 and 72.6\\% with ConvNet-3 on ten images per class of CIFAR-10. Besides, DiM outperforms previous methods with 10\\% $\\sim$ 22\\% when images per class are 1 and 10 on the SVHN dataset.",
        "authors": [
            "Kai Wang",
            "Jianyang Gu",
            "Daquan Zhou",
            "Zheng Hua Zhu",
            "Wei Jiang",
            "Yang You"
        ],
        "citations": 32,
        "references": 49,
        "year": 2023
    },
    {
        "title": "The Emerging Role of Generative Artificial Intelligence in Medical Education, Research, and Practice",
        "abstract": "Recent breakthroughs in generative artificial intelligence (GAI) and the emergence of transformer-based large language models such as Chat Generative Pre-trained Transformer (ChatGPT) have the potential to transform healthcare education, research, and clinical practice. This article examines the current trends in using GAI models in medicine, outlining their strengths and limitations. It is imperative to develop further consensus-based guidelines to govern the appropriate use of GAI, not only in medical education but also in research, scholarship, and clinical practice.",
        "authors": [
            "M. Shoja",
            "J. V. D. van de Ridder",
            "Vijay Rajput"
        ],
        "citations": 31,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Generative-Contrastive Graph Learning for Recommendation",
        "abstract": "By treating users' interactions as a user-item graph, graph learning models have been widely deployed in Collaborative Filtering~(CF) based recommendation. Recently, researchers have introduced Graph Contrastive Learning~(GCL) techniques into CF to alleviate the sparse supervision issue, which first constructs contrastive views by data augmentations and then provides self-supervised signals by maximizing the mutual information between contrastive views. Despite the effectiveness, we argue that current GCL-based recommendation models are still limited as current data augmentation techniques, either structure augmentation or feature augmentation. First, structure augmentation randomly dropout nodes or edges, which is easy to destroy the intrinsic nature of the user-item graph. Second, feature augmentation imposes the same scale noise augmentation on each node, which neglects the unique characteristics of nodes on the graph. To tackle the above limitations, we propose a novel Variational Graph Generative-Contrastive Learning (VGCL) framework for recommendation. Specifically, we leverage variational graph reconstruction to estimate a Gaussian distribution of each node, then generate multiple contrastive views through multiple samplings from the estimated distributions, which builds a bridge between generative and contrastive learning. The generated contrastive views can well reconstruct the input graph without information distortion. Besides, the estimated variances are tailored to each node, which regulates the scale of contrastive loss for each node on optimization. Considering the similarity of the estimated distributions, we propose a cluster-aware twofold contrastive learning, a node-level to encourage consistency of a node's contrastive views and a cluster-level to encourage consistency of nodes in a cluster. Finally, extensive experimental results on three public datasets clearly demonstrate the effectiveness of the proposed model.",
        "authors": [
            "Yonghui Yang",
            "Zhengwei Wu",
            "Le Wu",
            "Kun Zhang",
            "Richang Hong",
            "Zhiqiang Zhang",
            "Jun Zhou",
            "Meng Wang"
        ],
        "citations": 31,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Copyright Protection and Accountability of Generative AI: Attack, Watermarking and Attribution",
        "abstract": "Generative AI (e.g., Generative Adversarial Networks – GANs) has become increasingly popular in recent years. However, Generative AI introduces significant concerns regarding the protection of Intellectual Property Rights (IPR) (resp. model accountability) pertaining to images (resp. toxic images) and models (resp. poisoned models) generated. In this paper, we propose an evaluation framework to provide a comprehensive overview of the current state of the copyright protection measures for GANs, evaluate their performance across a diverse range of GAN architectures, and identify the factors that affect their performance and future research directions. Our findings indicate that the current IPR protection methods for input images, model watermarking, and attribution networks are largely satisfactory for a wide range of GANs. We highlight that further attention must be directed towards protecting training sets, as the current approaches fail to provide robust IPR protection and provenance tracing on training sets.",
        "authors": [
            "Haonan Zhong",
            "Jiamin Chang",
            "Ziyue Yang",
            "Tingmin Wu",
            "Pathum Chamikara Mahawaga Arachchige",
            "Chehara Pathmabandu",
            "Minhui Xue"
        ],
        "citations": 33,
        "references": 20,
        "year": 2023
    },
    {
        "title": "Generative AI and the Automating of Academia",
        "abstract": null,
        "authors": [
            "R. Watermeyer",
            "L. Phipps",
            "Donna Lanclos",
            "Cathryn Knight"
        ],
        "citations": 34,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Generative AI meets Responsible AI: Practical Challenges and Opportunities",
        "abstract": "Generative AI models and applications are being rapidly developed and deployed across a wide spectrum of industries and applications ranging from writing and email assistants to graphic design and art generation to educational assistants to coding to drug discovery. However, there are several ethical and social considerations associated with generative AI models and applications. These concerns include lack of interpretability, bias and discrimination, privacy, lack of model robustness, fake and misleading content, copyright implications, plagiarism, and environmental impact associated with training and inference of generative AI models. In this tutorial, we first motivate the need for adopting responsible AI principles when developing and deploying large language models (LLMs) and other generative AI models, as part of a broader AI model governance and responsible AI framework, from societal, legal, user, and model developer perspectives, and provide a roadmap for thinking about responsible AI for generative AI in practice. We provide a brief technical overview of text and image generation models, and highlight the key responsible AI desiderata associated with these models. We then describe the technical considerations and challenges associated with realizing the above desiderata in practice. We focus on real-world generative AI use cases spanning domains such as media generation, writing assistants, copywriting, code generation, and conversational assistants, present practical solution approaches / guidelines for applying responsible AI techniques effectively, discuss lessons learned from deploying responsible AI approaches for generative AI applications in practice, and highlight the key open research problems. We hope that our tutorial will inform both researchers and practitioners, stimulate further research on responsible AI in the context of generative AI, and pave the way for building more reliable and trustworthy generative AI applications in the future.",
        "authors": [
            "K. Kenthapadi",
            "Himabindu Lakkaraju",
            "Nazneen Rajani"
        ],
        "citations": 32,
        "references": 33,
        "year": 2023
    },
    {
        "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
        "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.",
        "authors": [
            "Tyna Eloundou",
            "Sam Manning",
            "Pamela Mishkin",
            "Daniel Rock"
        ],
        "citations": 340,
        "references": 90,
        "year": 2023
    },
    {
        "title": "MAGVIT: Masked Generative Video Transformer",
        "abstract": "We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle various video synthesis tasks with a single model. We introduce a 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning. We conduct extensive experiments to demonstrate the quality, efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT performs favorably against state-of-the-art approaches and establishes the best-published FVD on three video generation benchmarks, including the challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference time by two orders of magnitude against diffusion models and by 60x against autoregressive models. (iii) A single MAGVIT model supports ten diverse generation tasks and generalizes across videos from different visual domains. The source code and trained models will be released to the public at https://magvit.cs.cmu.edu.",
        "authors": [
            "Lijun Yu",
            "Yong Cheng",
            "Kihyuk Sohn",
            "José Lezama",
            "Han Zhang",
            "Huiwen Chang",
            "A. Hauptmann",
            "Ming-Hsuan Yang",
            "Yuan Hao",
            "Irfan Essa",
            "Lu Jiang"
        ],
        "citations": 164,
        "references": 86,
        "year": 2022
    },
    {
        "title": "ChatGPT and a new academic reality: Artificial Intelligence‐written research papers and the ethics of the large language models in scholarly publishing",
        "abstract": "This article discusses OpenAI's ChatGPT, a generative pre‐trained transformer, which uses natural language processing to fulfill text‐based user requests (i.e., a “chatbot”). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT‐3, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.",
        "authors": [
            "Brady D. Lund",
            "Ting Wang",
            "Nishith Reddy Mannuru",
            "Bing Nie",
            "S. Shimray",
            "Ziang Wang"
        ],
        "citations": 416,
        "references": 107,
        "year": 2023
    },
    {
        "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
        "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
        "authors": [
            "Ce Zhou",
            "Qian Li",
            "Chen Li",
            "Jun Yu",
            "Yixin Liu",
            "Guangjing Wang",
            "Kaichao Zhang",
            "Cheng Ji",
            "Qi Yan",
            "Lifang He",
            "Hao Peng",
            "Jianxin Li",
            "Jia Wu",
            "Ziwei Liu",
            "P. Xie",
            "Caiming Xiong",
            "Jian Pei",
            "Philip S. Yu",
            "Lichao Sun Michigan State University",
            "B. University",
            "Lehigh University",
            "M. University",
            "Nanyang Technological University",
            "University of California at San Diego",
            "Duke University",
            "U. Chicago",
            "Salesforce Research"
        ],
        "citations": 418,
        "references": 0,
        "year": 2023
    },
    {
        "title": "GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models",
        "abstract": "Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after\"fine-tuning\"on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply\"style cloaks\"to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (>92%) and against adaptive countermeasures (>85%).",
        "authors": [
            "Shawn Shan",
            "Jenna Cryan",
            "Emily Wenger",
            "Haitao Zheng",
            "Rana Hanocka",
            "Ben Y. Zhao"
        ],
        "citations": 147,
        "references": 131,
        "year": 2023
    },
    {
        "title": "Generative Pre-training for Speech with Flow Matching",
        "abstract": "Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested a foundational model for generation tasks in speech can be built with generative pre-training.",
        "authors": [
            "Alexander H. Liu",
            "Matt Le",
            "Apoorv Vyas",
            "Bowen Shi",
            "Andros Tjandra",
            "Wei-Ning Hsu"
        ],
        "citations": 22,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence",
        "abstract": "Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage. In this work, we further investigate the information leakage issue and propose a generative embedding inversion attack (GEIA) that aims to reconstruct input sequences based only on their sentence embeddings. Given the black-box access to a language model, we treat sentence embeddings as initial tokens' representations and train or fine-tune a powerful decoder model to decode the whole sequences directly. We conduct extensive experiments to demonstrate that our generative inversion attack outperforms previous embedding inversion attacks in classification metrics and generates coherent and contextually similar sentences as the original inputs.",
        "authors": [
            "Haoran Li",
            "Mingshi Xu",
            "Yangqiu Song"
        ],
        "citations": 30,
        "references": 46,
        "year": 2023
    },
    {
        "title": "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models",
        "abstract": "This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously a) accomplish the synthesis of visually realistic and temporally coherent videos while b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: 1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. 2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications.",
        "authors": [
            "Yaohui Wang",
            "Xinyuan Chen",
            "Xin Ma",
            "Shangchen Zhou",
            "Ziqi Huang",
            "Yi Wang",
            "Ceyuan Yang",
            "Yinan He",
            "Jiashuo Yu",
            "Pe-der Yang",
            "Yuwei Guo",
            "Tianxing Wu",
            "Chenyang Si",
            "Yuming Jiang",
            "Cunjian Chen",
            "Chen Change Loy",
            "Bo Dai",
            "Dahua Lin",
            "Y. Qiao",
            "Ziwei Liu"
        ],
        "citations": 186,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Survey on leveraging pre-trained generative adversarial networks for image editing and restoration",
        "abstract": null,
        "authors": [
            "Ming Liu",
            "Yuxiang Wei",
            "Xiaohe Wu",
            "W. Zuo",
            "L. Zhang"
        ],
        "citations": 29,
        "references": 211,
        "year": 2023
    },
    {
        "title": "On Generative Agents in Recommendation",
        "abstract": "Recommender systems are the cornerstone of today's information dissemination, yet a disconnect between offline metrics and online performance greatly hinders their development. Addressing this challenge, we envision a recommendation simulator, capitalizing on recent breakthroughs in human-level intelligence exhibited by Large Language Models (LLMs). We propose Agent4Rec, a user simulator in recommendation, leveraging LLM-empowered generative agents equipped with user profile, memory, and actions modules specifically tailored for the recommender system. In particular, these agents' profile modules are initialized using real-world datasets (e.g. MovieLens, Steam, Amazon-Book), capturing users' unique tastes and social traits; memory modules log both factual and emotional memories and are integrated with an emotion-driven reflection mechanism; action modules support a wide variety of behaviors, spanning both taste-driven and emotion-driven actions. Each agent interacts with personalized recommender models in a page-by-page manner, relying on a pre-implemented collaborative filtering-based recommendation algorithm. We delve into both the capabilities and limitations of Agent4Rec, aiming to explore an essential research question: ``To what extent can LLM-empowered generative agents faithfully simulate the behavior of real, autonomous humans in recommender systems?'' Extensive and multi-faceted evaluations of Agent4Rec highlight both the alignment and deviation between agents and user-personalized preferences. Beyond mere performance comparison, we explore insightful experiments, such as emulating the filter bubble effect and discovering the underlying causal relationships in recommendation tasks. Our codes are available at https://github.com/LehengTHU/Agent4Rec.",
        "authors": [
            "An Zhang",
            "Leheng Sheng",
            "Yuxin Chen",
            "Hao Li",
            "Yang Deng",
            "Xiang Wang",
            "Tat-Seng Chua"
        ],
        "citations": 29,
        "references": 81,
        "year": 2023
    },
    {
        "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
        "abstract": "Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.",
        "authors": [
            "Jiahui Yu",
            "Zirui Wang",
            "Vijay Vasudevan",
            "Legg Yeung",
            "Mojtaba Seyedhosseini",
            "Yonghui Wu"
        ],
        "citations": 1000,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Generative Adversarial Networks for Face Generation: A Survey",
        "abstract": "Recently, generative adversarial networks (GANs) have progressed enormously, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression, and style. These GAN-based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, to the best of our knowledge, the GAN models applied to the face, which we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems, and performance evaluation with respect to each application and used datasets. More precisely, we review the progress of architectures and discuss the contributions and limits of each. Then, we expose the encountered problems of facial GANs and propose solutions to handle them. Additionally, as GAN evaluation has become a notable current defiance, we investigate the state-of-the-art quantitative and qualitative evaluation metrics and their applications. We conclude this work with a discussion on the face generation challenges and propose open research issues.",
        "authors": [
            "Amina Kammoun",
            "Rim Slama",
            "Hedi Tabia",
            "T. Ouni",
            "Mohmed Abid"
        ],
        "citations": 148,
        "references": 170,
        "year": 2022
    },
    {
        "title": "Generative Data‐Driven Approaches for Stochastic Subgrid Parameterizations in an Idealized Ocean Model",
        "abstract": "Subgrid parameterizations of mesoscale eddies continue to be in demand for climate simulations. These subgrid parameterizations can be powerfully designed using physics and/or data‐driven methods, with uncertainty quantification. For example, Guillaumin and Zanna (2021, https://doi.org/10.1029/2021ms002534) proposed a Machine Learning (ML) model that predicts subgrid forcing and its local uncertainty. The major assumption and potential drawback of this model is the statistical independence of stochastic residuals between grid points. Here, we aim to improve the simulation of stochastic forcing with generative models of ML, such as Generative adversarial network (GAN) and Variational autoencoder (VAE). Generative models learn the distribution of subgrid forcing conditioned on the resolved flow directly from data and they can produce new samples from this distribution. Generative models can potentially capture not only the spatial correlation but any statistically significant property of subgrid forcing. We test the proposed stochastic parameterizations offline and online in an idealized ocean model. We show that generative models are able to predict subgrid forcing and its uncertainty with spatially correlated stochastic forcing. Online simulations for a range of resolutions demonstrated that generative models are superior to the baseline ML model at the coarsest resolution.",
        "authors": [
            "Pavel Perezhogin",
            "L. Zanna",
            "C. Fernandez‐Granda"
        ],
        "citations": 21,
        "references": 130,
        "year": 2023
    },
    {
        "title": "Epidemic Modeling with Generative Agents",
        "abstract": "This study offers a new paradigm of individual-level modeling to address the grand challenge of incorporating human behavior in epidemic models. Using generative artificial intelligence in an agent-based epidemic model, each agent is empowered to make its own reasonings and decisions via connecting to a large language model such as ChatGPT. Through various simulation experiments, we present compelling evidence that generative agents mimic real-world behaviors such as quarantining when sick and self-isolation when cases rise. Collectively, the agents demonstrate patterns akin to multiple waves observed in recent pandemics followed by an endemic period. Moreover, the agents successfully flatten the epidemic curve. This study creates potential to improve dynamic system modeling by offering a way to represent human brain, reasoning, and decision making.",
        "authors": [
            "Ross Williams",
            "Niyousha Hosseinichimeh",
            "A. Majumdar",
            "Navid Ghaffarzadegan"
        ],
        "citations": 26,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Scientists' Perspectives on the Potential for Generative AI in their Fields",
        "abstract": "Generative AI models, including large language models and multimodal models that include text and other media, are on the cusp of transforming many aspects of modern life, including entertainment, education, civic life, the arts, and a range of professions. There is potential for Generative AI to have a substantive impact on the methods and pace of discovery for a range of scientific disciplines. We interviewed twenty scientists from a range of fields (including the physical, life, and social sciences) to gain insight into whether or how Generative AI technologies might add value to the practice of their respective disciplines, including not only ways in which AI might accelerate scientific discovery (i.e., research), but also other aspects of their profession, including the education of future scholars and the communication of scientific findings. In addition to identifying opportunities for Generative AI to augment scientists' current practices, we also asked participants to reflect on concerns about AI. These findings can help guide the responsible development of models and interfaces for scientific education, inquiry, and communication.",
        "authors": [
            "M. Morris"
        ],
        "citations": 25,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Generative AI enhances individual creativity but reduces the collective diversity of novel content",
        "abstract": "Creativity is core to being human. Generative artificial intelligence (AI)—including powerful large language models (LLMs)—holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on generative AI ideas. We study the causal impact of generative AI ideas on the production of short stories in an online experiment where some writers obtained story ideas from an LLM. We find that access to generative AI ideas causes stories to be evaluated as more creative, better written, and more enjoyable, especially among less creative writers. However, generative AI–enabled stories are more similar to each other than stories by humans alone. These results point to an increase in individual creativity at the risk of losing collective novelty. This dynamic resembles a social dilemma: With generative AI, writers are individually better off, but collectively a narrower scope of novel content is produced. Our results have implications for researchers, policy-makers, and practitioners interested in bolstering creativity.",
        "authors": [
            "Anil R. Doshi",
            "Oliver P. Hauser"
        ],
        "citations": 28,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Generative AI: Implications and Applications for Education",
        "abstract": "The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.",
        "authors": [
            "A. Tzirides",
            "A. Saini",
            "Gabriela C. Zapata",
            "Duane Searsmith",
            "B. Cope",
            "M. Kalantzis",
            "Vania Castro",
            "Theodora Kourkoulou",
            "John Jones",
            "Rodrigo Abrantes da Silva",
            "Jennifer K. Whiting",
            "Nikoleta Polyxeni Kastania"
        ],
        "citations": 24,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
        "abstract": "As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.",
        "authors": [
            "Emilio Ferrara"
        ],
        "citations": 205,
        "references": 163,
        "year": 2023
    },
    {
        "title": "Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions",
        "abstract": "The integration of large language models (LLMs), such as those in the Generative Pre-trained Transformers (GPT) series, into medical education has the potential to transform learning experiences for students and elevate their knowledge, skills, and competence. Drawing on a wealth of professional and academic experience, we propose that LLMs hold promise for revolutionizing medical curriculum development, teaching methodologies, personalized study plans and learning materials, student assessments, and more. However, we also critically examine the challenges that such integration might pose by addressing issues of algorithmic bias, overreliance, plagiarism, misinformation, inequity, privacy, and copyright concerns in medical education. As we navigate the shift from an information-driven educational paradigm to an artificial intelligence (AI)–driven educational paradigm, we argue that it is paramount to understand both the potential and the pitfalls of LLMs in medical education. This paper thus offers our perspective on the opportunities and challenges of using LLMs in this context. We believe that the insights gleaned from this analysis will serve as a foundation for future recommendations and best practices in the field, fostering the responsible and effective use of AI technologies in medical education.",
        "authors": [
            "Alaa A. Abd-alrazaq",
            "Rawan AlSaad",
            "Dari Alhuwail",
            "Arfan Ahmed",
            "P. Healy",
            "Syed Latifi",
            "S. Aziz",
            "R. Damseh",
            "Sadam Alabed Alrazak",
            "Javaid Sheikh"
        ],
        "citations": 196,
        "references": 68,
        "year": 2023
    },
    {
        "title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
        "abstract": "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's\"paint-with-words\"capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/",
        "authors": [
            "Y. Balaji",
            "Seungjun Nah",
            "Xun Huang",
            "Arash Vahdat",
            "Jiaming Song",
            "Qinsheng Zhang",
            "Karsten Kreis",
            "M. Aittala",
            "Timo Aila",
            "S. Laine",
            "Bryan Catanzaro",
            "Tero Karras",
            "Ming-Yu Liu"
        ],
        "citations": 708,
        "references": 90,
        "year": 2022
    },
    {
        "title": "A generative model of memory construction and consolidation",
        "abstract": "Episodic memories are (re)constructed, combining unique features with familiar schemas, share neural substrates with imagination, and show schema-based distortions that increase with consolidation. Here we present a computational model in which hippocampal replay (from an autoassociative network) trains generative models (variational autoencoders) in neo-cortex to (re)create sensory experiences via latent variable representations in entorhinal, medial prefrontal, and anterolateral temporal cortices. Simulations show effects of memory age and hippocampal lesions in agreement with previous models, but also provide mechanisms for se-mantic memory, imagination, episodic future thinking, relational inference, and schema-based distortions including boundary extension. The model explains how unique sensory and predict-able conceptual or schematic elements of memories are stored and reconstructed by efficiently combining both hippocampal and neocortical systems, optimising the use of limited hippocam-pal storage for new and unusual information. Overall, we believe hippocampal replay training neocortical generative models provides a comprehensive account of memory construction, ima-gination and consolidation.",
        "authors": [
            "Eleanor Spens",
            "N. Burgess"
        ],
        "citations": 19,
        "references": 172,
        "year": 2023
    },
    {
        "title": "Bayesian Structure Learning with Generative Flow Networks",
        "abstract": "In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference.",
        "authors": [
            "T. Deleu",
            "Ant'onio G'ois",
            "Chris C. Emezue",
            "M. Rankawat",
            "Simon Lacoste-Julien",
            "Stefan Bauer",
            "Y. Bengio"
        ],
        "citations": 127,
        "references": 60,
        "year": 2022
    },
    {
        "title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers",
        "abstract": "Generating images from textual descriptions has gained a lot of attention. Recently, DALL-E [44], a multimodal transformer language model, and its variants have shown high-quality text-to-image generation capabilities with a simple architecture and training objective, powered by large-scale training data and computation. However, despite the interesting image generation results, there has not been a detailed analysis on how to evaluate such models. In this work, we investigate the reasoning capabilities and social biases of such text-to-image generative transformers in detail. First, we measure four visual reasoning skills: object recognition, object counting, color recognition, and spatial relation understanding. For this, we propose P AINT S KILLS , a diagnostic dataset and evaluation toolkit that measures these four visual reasoning skills. Second, we measure the text alignment and quality of the generated images based on pretrained image cap-tioning, image-text retrieval, and image classiﬁcation models. Third, we assess social biases in the models. For this, we suggest evaluation of gender and racial biases of text-to-image generation models based on a pretrained image-text retrieval model and human evaluation. In our experiments, we show that recent text-to-image generative trans-former models perform better in recognizing and counting objects than recognizing colors and understanding spatial relations, while there exists a large gap between the model performances and upper bound accuracy on all skills. Next, we demonstrate that recent text-to-image models learn spe-ciﬁc gender/racial biases from web image-text pairs. We also show that our automatic evaluations of visual reasoning skills and gender bias are highly correlated with human judgments. We hope our work will help guide and care-fully measure future research progress in improving text-to-image generation models on challenging",
        "authors": [
            "Jaemin Cho",
            "Abhaysinh Zala",
            "Mohit Bansal"
        ],
        "citations": 122,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Improved Techniques for Training Consistency Models",
        "abstract": "Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\\times 64$ respectively in a single sampling step. These scores mark a 3.5$\\times$ and 4$\\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.",
        "authors": [
            "Yang Song",
            "Prafulla Dhariwal"
        ],
        "citations": 106,
        "references": 48,
        "year": 2023
    },
    {
        "title": "GIRAFFE HD: A High-Resolution 3D-aware Generative Model",
        "abstract": "3D-aware generative models have shown that the introduction of 3D information can lead to more controllable image generation. In particular, the current state-of-the-art model GIRAFFE [38] can control each object's rotation, translation, scale, and scene camera pose without corresponding supervision. However, GIRAFFE only operates well when the image resolution is low. We propose GIRAFFE HD, a high-resolution 3D-aware generative model that inherits all of GIRAFFE's controllable features while generating high-quality, high-resolution images (5122resolution and above). The key idea is to leverage a style- based neural renderer, and to independently generate the foreground and background to force their disentanglement while imposing consistency constraints to stitch them together to composite a coherent final image. We demonstrate state-of-the-art 3D controllable high-resolution image generation on multiple natural image datasets.",
        "authors": [
            "Yang Xue",
            "Yuheng Li",
            "Krishna Kumar Singh",
            "Yong Jae Lee"
        ],
        "citations": 94,
        "references": 52,
        "year": 2022
    },
    {
        "title": "Video Probabilistic Diffusion Models in Projected Latent Space",
        "abstract": "Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their high-dimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computation and memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion model (PVDM), a probabilistic diffusion model which learns a video distribution in a low-dimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized latent space and the training/sampling procedure to synthesize videos of arbitrary length with a single model. Experiments on popular video generation datasets demonstrate the superiority of PVDM compared with previous video synthesis methods; e.g., PVDM obtains the FVD score of 639.7 on the UCF-101 long video (128 frames) generation benchmark, which improves 1773.4 of the prior state-of-the-art.",
        "authors": [
            "Sihyun Yu",
            "Kihyuk Sohn",
            "Subin Kim",
            "Jinwoo Shin"
        ],
        "citations": 146,
        "references": 85,
        "year": 2023
    },
    {
        "title": "Rapamycin in the context of Pascal’s Wager: generative pre-trained transformer perspective",
        "abstract": "Large language models utilizing transformer neural networks and other deep learning architectures demonstrated unprecedented results in many tasks previously accessible only to human intelligence. In this article, we collaborate with ChatGPT, an AI model developed by OpenAI to speculate on the applications of Rapamycin, in the context of Pascal’s Wager philosophical argument commonly utilized to justify the belief in god. In response to the query “Write an exhaustive research perspective on why taking Rapamycin may be more beneficial than not taking Rapamycin from the perspective of Pascal’s wager” ChatGPT provided the pros and cons for the use of Rapamycin considering the preclinical evidence of potential life extension in animals. This article demonstrates the potential of ChatGPT to produce complex philosophical arguments and should not be used for any off-label use of Rapamycin.",
        "authors": [
            "A. Zhavoronkov"
        ],
        "citations": 139,
        "references": 3,
        "year": 2022
    },
    {
        "title": "MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises",
        "abstract": "Multimodal VAEs have recently gained attention as efficient models for weaklysupervised generative learning with a large number of modalities. However, all existing variants of multimodal VAEs are affected by a non-trivial trade-off between generative quality and generative coherence. We focus on the mixture-ofexperts multimodal VAE (MMVAE), which achieves good coherence only at the expense of sample diversity and a resulting lack of generative quality. We present a novel variant of the MMVAE that improves its generative quality, while maintaining high semantic coherence. For this, shared and modality-specific information is modelled in separate latent subspaces. In contrast to previous approaches with separate subspaces, our model is robust to changes in latent dimensionality and regularization hyperparameters. We show that our model achieves both good generative coherence and high generative quality in challenging experiments, including more complex multimodal datasets than those used in previous works.",
        "authors": [
            "Emanuele Palumbo",
            "Imant Daunhawer",
            "Julia E. Vogt"
        ],
        "citations": 19,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Generative Sequential Recommendation with GPTRec",
        "abstract": "Sequential recommendation is an important recommendation task that aims to predict the next item in a sequence. Recently, adaptations of language models, particularly Transformer-based models such as SASRec and BERT4Rec, have achieved state-of-the-art results in sequential recommendation. In these models, item ids replace tokens in the original language models. However, this approach has limitations. First, the vocabulary of item ids may be many times larger than in language models. Second, the classical Top-K recommendation approach used by these models may not be optimal for complex recommendation objectives, including auxiliary objectives such as diversity, coverage or coherence. Recent progress in generative language models inspires us to revisit generative approaches to address these challenges. This paper presents the GPTRec sequential recommendation model, which is based on the GPT-2 architecture. GPTRec can address large vocabulary issues by splitting item ids into sub-id tokens using a novel SVD Tokenisation algorithm based on quantised item embeddings from an SVD decomposition of the user-item interaction matrix. The paper also presents a novel Next-K recommendation strategy, which generates recommendations item-by-item, considering already recommended items. The Next-K strategy can be used for producing complex interdependent recommendation lists. We experiment with GPTRec on the MovieLens-1M dataset and show that using sub-item tokenisation GPTRec can match the quality of SASRec while reducing the embedding table by 40%. We also show that the recommendations generated by GPTRec on MovieLens-1M using the Next-K recommendation strategy match the quality of SASRec in terms of NDCG@10, meaning that the model can serve as a strong starting point for future research.",
        "authors": [
            "A. Petrov",
            "Craig Macdonald"
        ],
        "citations": 19,
        "references": 39,
        "year": 2023
    },
    {
        "title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model",
        "abstract": "Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for BioBART and find that sentence permutation has negative effects on downstream tasks.",
        "authors": [
            "Hongyi Yuan",
            "Zheng Yuan",
            "Ruyi Gan",
            "Jiaxing Zhang",
            "Yutao Xie",
            "Sheng Yu"
        ],
        "citations": 115,
        "references": 80,
        "year": 2022
    },
    {
        "title": "Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport",
        "abstract": "Optimal Transport (OT) problem investigates a transport map that bridges two distributions while minimizing a given cost function. In this regard, OT between tractable prior distribution and data has been utilized for generative modeling tasks. However, OT-based methods are susceptible to outliers and face optimization challenges during training. In this paper, we propose a novel generative model based on the semi-dual formulation of Unbalanced Optimal Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution matching. This approach provides better robustness against outliers, stability during training, and faster convergence. We validate these properties empirically through experiments. Moreover, we study the theoretical upper-bound of divergence between distributions in UOT. Our model outperforms existing OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 6.36 on CelebA-HQ-256. The code is available at \\url{https://github.com/Jae-Moo/UOTM}.",
        "authors": [
            "Jaemoo Choi",
            "Jaewoong Choi",
            "Myung-joo Kang"
        ],
        "citations": 18,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Improving Generative Model-based Unfolding with Schrödinger Bridges",
        "abstract": "Machine learning-based unfolding has enabled unbinned and high-dimensional differential cross section measurements. Two main approaches have emerged in this research area: one based on discriminative models and one based on generative models. The main advantage of discriminative models is that they learn a small correction to a starting simulation while generative models scale better to regions of phase space with little data. We propose to use Schroedinger Bridges and diffusion models to create SBUnfold, an unfolding approach that combines the strengths of both discriminative and generative models. The key feature of SBUnfold is that its generative model maps one set of events into another without having to go through a known probability density as is the case for normalizing flows and standard diffusion models. We show that SBUnfold achieves excellent performance compared to state of the art methods on a synthetic Z+jets dataset.",
        "authors": [
            "S. Diefenbacher",
            "Guan-Horng Liu",
            "V. Mikuni",
            "B. Nachman",
            "Weili Nie"
        ],
        "citations": 18,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Federated Generative Model on Multi-Source Heterogeneous Data in IoT",
        "abstract": "The study of generative models is a promising branch of deep learning techniques, which has been successfully applied to different scenarios, such as Artificial Intelligence and the Internet of Things. While in most of the existing works, the generative models are realized as a centralized structure, raising the threats of security and privacy and the overburden of communication costs. Rare efforts have been committed to investigating distributed generative models, especially when the training data comes from multiple heterogeneous sources under realistic IoT settings. In this paper, to handle this challenging problem, we design a federated generative model framework that can learn a powerful generator for the hierarchical IoT systems. Particularly, our generative model framework can solve the problem of distributed data generation on multi-source heterogeneous data in two scenarios, i.e., feature related scenario and label related scenario. In addition, in our federated generative models, we develop a synchronous and an asynchronous updating methods to satisfy different application requirements. Extensive experiments on a simulated dataset and multiple real datasets are conducted to evaluate the data generation performance of our proposed generative models through comparison with the state-of-the-arts.",
        "authors": [
            "Zuobin Xiong",
            "Wei Li",
            "Zhipeng Cai"
        ],
        "citations": 18,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Dynamical regimes of diffusion models",
        "abstract": null,
        "authors": [
            "Giulio Biroli",
            "Tony Bonnaire",
            "Valentin De Bortoli",
            "Marc M'ezard"
        ],
        "citations": 23,
        "references": 50,
        "year": 2024
    },
    {
        "title": "A mean-field games laboratory for generative modeling",
        "abstract": "We demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. In generative flows, a Lagrangian formulation is used where each particle (generated sample) aims to minimize a loss function over its simulated path. The loss, however, is dependent on the paths of other particles, which leads to a competition among the population of particles. The asymptotic behavior of this competition yields a mean-field game. We establish connections between MFGs and major classes of generative flows and diffusions including continuous-time normalizing flows, score-based generative models (SGM), and Wasserstein gradient flows. Furthermore, we study the mathematical properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled forward-backward nonlinear partial differential equations. The mathematical structure described by the MFG optimality conditions identifies the inductive biases of generative flows. We investigate the well-posedness and structure of normalizing flows, unravel the mathematical structure of SGMs, and derive a MFG formulation of Wasserstein gradient flows. From an algorithmic perspective, the optimality conditions yields Hamilton-Jacobi-Bellman (HJB) regularizers for enhanced training of generative models. In particular, we propose and demonstrate an HJB-regularized SGM with improved performance over standard SGMs. We present this framework as an MFG laboratory which serves as a platform for revealing new avenues of experimentation and invention of generative models.",
        "authors": [
            "Benjamin J. Zhang",
            "M. Katsoulakis"
        ],
        "citations": 15,
        "references": 84,
        "year": 2023
    },
    {
        "title": "A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot",
        "abstract": "In machine learning, generative modeling aims to learn to generate new data statistically similar to the training data distribution. In this paper, we survey learning generative models under limited data, few shots and zero shot, referred to as Generative Modeling under Data Constraint (GM-DC). This is an important topic when data acquisition is challenging, e.g. healthcare applications. We discuss background, challenges, and propose two taxonomies: one on GM-DC tasks and another on GM-DC approaches. Importantly, we study interactions between different GM-DC tasks and approaches. Furthermore, we highlight research gaps, research trends, and potential avenues for future exploration. Project website: https://gmdc-survey.github.io.",
        "authors": [
            "Milad Abdollahzadeh",
            "Touba Malekzadeh",
            "Christopher T. H. Teo",
            "Keshigeyan Chandrasegaran",
            "Guimeng Liu",
            "Ngai-Man Cheung"
        ],
        "citations": 17,
        "references": 227,
        "year": 2023
    },
    {
        "title": "MMM: Generative Masked Motion Model",
        "abstract": "Recent advances in text-to-motion generation using dif-fusion and autoregressive models have shown promising re-sults. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text to-kens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this al-lows parallel and iterative decoding of multiple motion to-kens that are highly consistent with fine-grained text de-scriptions, therefore simultaneously achieving high-fidelity and high-speed motion generation. In addition, MMM has innate motion editability. By simply placing mask tokens in the place that needs editing, MMM automatically fills the gaps while guaranteeing smooth transitions between editing and non-editing parts. Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM surpasses current leading methods in generating high-quality motion (evidenced by superior FID scores of 0.08 and 0.429), while offering advanced editing features such as body-part modification, motion in-betweening, and the synthesis of long motion sequences. In addition, MMM is two orders of magnitude faster on a single mid-range GPU than editable motion diffusion models. Our project page is available at https://exitudio.github.io/MMM-page/.",
        "authors": [
            "Ekkasit Pinyoanuntapong",
            "Pu Wang",
            "Minwoo Lee",
            "Chen Chen"
        ],
        "citations": 20,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Endora: Video Generation Models as Endoscopy Simulators",
        "abstract": "Generative models hold promise for revolutionizing medical education, robot-assisted surgery, and data augmentation for machine learning. Despite progress in generating 2D medical images, the complex domain of clinical video generation has largely remained untapped.This paper introduces \\model, an innovative approach to generate medical videos that simulate clinical endoscopy scenes. We present a novel generative model design that integrates a meticulously crafted spatial-temporal video transformer with advanced 2D vision foundation model priors, explicitly modeling spatial-temporal dynamics during video generation. We also pioneer the first public benchmark for endoscopy simulation with video generation models, adapting existing state-of-the-art methods for this endeavor.Endora demonstrates exceptional visual quality in generating endoscopy videos, surpassing state-of-the-art methods in extensive testing. Moreover, we explore how this endoscopy simulator can empower downstream video analysis tasks and even generate 3D medical scenes with multi-view consistency. In a nutshell, Endora marks a notable breakthrough in the deployment of generative AI for clinical endoscopy research, setting a substantial stage for further advances in medical content generation. For more details, please visit our project page: https://endora-medvidgen.github.io/.",
        "authors": [
            "Chenxin Li",
            "Hengyu Liu",
            "Yifan Liu",
            "Brandon Y. Feng",
            "Wuyang Li",
            "Xinyu Liu",
            "Zhen Chen",
            "Jing Shao",
            "Yixuan Yuan"
        ],
        "citations": 23,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Denoising Diffusion Restoration Models",
        "abstract": "Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM's versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set.",
        "authors": [
            "Bahjat Kawar",
            "Michael Elad",
            "Stefano Ermon",
            "Jiaming Song"
        ],
        "citations": 667,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Generative Prompt Model for Weakly Supervised Object Localization",
        "abstract": "Weakly supervised object localization (WSOL) remains challenging when learning object localization models from image category labels. Conventional methods that discriminatively train activation models ignore representative yet less discriminative object parts. In this study, we propose a generative prompt model (GenPromp), defining the first generative pipeline to localize less discriminative object parts by formulating WSOL as a conditional image denoising procedure. During training, GenPromp converts image category labels to learnable prompt embeddings which are fed to a generative model to conditionally recover the input image with noise and learn representative embeddings. During inference, GenPromp combines the representative embeddings with discriminative embeddings (queried from an off-the-shelf vision-language model) for both representative and discriminative capacity. The combined embeddings are finally used to generate multi-scale high-quality attention maps, which facilitate localizing full object extent. Experiments on CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline for WSOL with the generative model. Code is available at https://github.com/callsys/GenPromp.",
        "authors": [
            "Yuzhong Zhao",
            "Qixiang Ye",
            "Weijia Wu",
            "Chunhua Shen",
            "Fang Wan"
        ],
        "citations": 21,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Convergence of score-based generative modeling for general data distributions",
        "abstract": "Score-based generative modeling (SGM) has grown to be a hugely successful method for learning to generate samples from complex data distributions such as that of images and audio. It is based on evolving an SDE that transforms white noise into a sample from the learned distribution, using estimates of the score function, or gradient log-pdf. Previous convergence analyses for these methods have suffered either from strong assumptions on the data distribution or exponential dependencies, and hence fail to give efficient guarantees for the multimodal and non-smooth distributions that arise in practice and for which good empirical performance is observed. We consider a popular kind of SGM -- denoising diffusion models -- and give polynomial convergence guarantees for general data distributions, with no assumptions related to functional inequalities or smoothness. Assuming $L^2$-accurate score estimates, we obtain Wasserstein distance guarantees for any distribution of bounded support or sufficiently decaying tails, as well as TV guarantees for distributions with further smoothness assumptions.",
        "authors": [
            "Holden Lee",
            "Jianfeng Lu",
            "Yixin Tan"
        ],
        "citations": 110,
        "references": 25,
        "year": 2022
    },
    {
        "title": "Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation",
        "abstract": "A diffusion model learns to predict a vector field of gradients. We propose to apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instantiate to be a voxel radiance field. This setup aggregates 2D scores at multiple camera viewpoints into a 3D score, and re-purposes a pretrained 2D model for 3D data generation. We identify a technical challenge of distribution mismatch that arises in this application, and propose a novel estimation mechanism to resolve it. We run our algorithm on several off-the-shelf diffusion image generative models, including the recently released Stable Diffusion trained on the large-scale LAION 5B dataset.",
        "authors": [
            "Haochen Wang",
            "Xiaodan Du",
            "Jiahao Li",
            "Raymond A. Yeh",
            "Gregory Shakhnarovich"
        ],
        "citations": 455,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
        "abstract": "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences: we show that, regardless of the premise, models falsely label NLI test samples as entailing when the hypothesis is attested in training data, and that entities are used as ``indices'' to access the memorized data. Second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation.",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Liang Cheng",
            "Mohammad Javad Hosseini",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "citations": 158,
        "references": 46,
        "year": 2023
    },
    {
        "title": "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities",
        "abstract": "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models. SUPERB was a step towards introducing a common benchmark to evaluate pre-trained models across various speech tasks. In this paper, we introduce SUPERB-SG, a new benchmark focusing on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB. We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks. It entails freezing pre-trained model parameters, only using simple task-specific trainable heads. The goal is to be inclusive of all researchers, and encourage efficient use of computational resources. We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation.",
        "authors": [
            "Hsiang-Sheng Tsai",
            "Heng-Jui Chang",
            "Wen-Chin Huang",
            "Zili Huang",
            "Kushal Lakhotia",
            "Shu-Wen Yang",
            "Shuyan Dong",
            "Andy T. Liu",
            "Cheng-I Lai",
            "Jiatong Shi",
            "Xuankai Chang",
            "Phil Hall",
            "Hsuan-Jui Chen",
            "Shang-Wen Li",
            "Shinji Watanabe",
            "Abdel-rahman Mohamed",
            "Hung-yi Lee"
        ],
        "citations": 98,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Phoenix: A Federated Generative Diffusion Model",
        "abstract": "Generative AI has made impressive strides in enabling users to create diverse and realistic visual content such as images, videos, and audio. However, training generative models on large centralized datasets can pose challenges in terms of data privacy, security, and accessibility. Federated learning (FL) is an approach that uses decentralized techniques to collaboratively train a shared deep learning model while retaining the training data on individual edge devices to preserve data privacy. This paper proposes a novel method for training a Denoising Diffusion Probabilistic Model (DDPM) across multiple data sources using FL techniques. Diffusion models, a newly emerging generative model, show promising results in achieving superior quality images than Generative Adversarial Networks (GANs). Our proposed method Phoenix is an unconditional diffusion model that leverages strategies to improve the data diversity of generated samples even when trained on data with statistical heterogeneity or Non-IID (Non-Independent and Identically Distributed) data. We demonstrate how our approach outperforms the default diffusion model in a FL setting. These results indicate that high-quality samples can be generated by maintaining data diversity, preserving privacy, and reducing communication between data sources, offering exciting new possibilities in the field of generative AI.",
        "authors": [
            "Fiona Victoria Stanley Jothiraj",
            "A. Mashhadi"
        ],
        "citations": 14,
        "references": 53,
        "year": 2023
    },
    {
        "title": "The Stable Signature: Rooting Watermarks in Latent Diffusion Models",
        "abstract": "Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. We introduce an active content tracing method combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that the Stable Signature is robust to image modifications. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep 10% of the content, with 90+% accuracy at a false positive rate below 10−6.",
        "authors": [
            "Pierre Fernandez",
            "Guillaume Couairon",
            "Herv'e J'egou",
            "Matthijs Douze",
            "T. Furon"
        ],
        "citations": 126,
        "references": 106,
        "year": 2023
    },
    {
        "title": "NatGen: generative pre-training by “naturalizing” source code",
        "abstract": "Pre-trained Generative Language models (e.g., PLBART, CodeT5, SPT-Code) for source code yielded strong results on several tasks in the past few years, including code generation and translation. These models have adopted varying pre-training objectives to learn statistics of code construction from very large-scale corpora in a self-supervised fashion; the success of pre-trained models largely hinges on these pre-training objectives. This paper proposes a new pre-training objective, “Naturalizing” of source code, exploiting code’s bimodal, dual-channel (formal & natural channels) nature. Unlike natural language, code’s bimodal, dual-channel nature allows us to generate semantically equivalent code at scale. We introduce six classes of semantic preserving transformations to introduce unnatural forms of code, and then force our model to produce more natural original programs written by developers. Learning to generate equivalent, but more natural code, at scale, over large corpora of open-source code, without explicit manual supervision, helps the model learn to both ingest & generate code. We fine-tune our model in three generative Software Engineering tasks: code generation, code translation, and code refinement with limited human-curated labeled data and achieve state-of-the-art performance rivaling CodeT5. We show that our pre-trained model is especially competitive at zero-shot and few-shot learning, and better at learning code properties (e.g., syntax, data flow)",
        "authors": [
            "Saikat Chakraborty",
            "Toufique Ahmed",
            "Yangruibo Ding",
            "Prem Devanbu",
            "Baishakhi Ray"
        ],
        "citations": 102,
        "references": 67,
        "year": 2022
    },
    {
        "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
        "abstract": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \\url{https://github.com/Shark-NLP/DiffuSeq}",
        "authors": [
            "Shansan Gong",
            "Mukai Li",
            "Jiangtao Feng",
            "Zhiyong Wu",
            "Lingpeng Kong"
        ],
        "citations": 264,
        "references": 53,
        "year": 2022
    },
    {
        "title": "On the Importance of Noise Scheduling for Diffusion Models",
        "abstract": "We empirically study the effect of noise scheduling strategies for denoising diffusion generative models. There are three findings: (1) the noise scheduling is crucial for the performance, and the optimal one depends on the task (e.g., image sizes), (2) when increasing the image size, the optimal noise scheduling shifts towards a noisier one (due to increased redundancy in pixels), and (3) simply scaling the input data by a factor of $b$ while keeping the noise schedule function fixed (equivalent to shifting the logSNR by $\\log b$) is a good strategy across image sizes. This simple recipe, when combined with recently proposed Recurrent Interface Network (RIN), yields state-of-the-art pixel-based diffusion models for high-resolution images on ImageNet, enabling single-stage, end-to-end generation of diverse and high-fidelity images at 1024$\\times$1024 resolution (without upsampling/cascades).",
        "authors": [
            "Ting Chen"
        ],
        "citations": 118,
        "references": 21,
        "year": 2023
    },
    {
        "title": "Geometric Latent Diffusion Models for 3D Molecule Generation",
        "abstract": "Generative models, especially diffusion models (DMs), have achieved promising results for generating feature-rich geometries and advancing foundational science problems such as molecule design. Inspired by the recent huge success of Stable (latent) Diffusion models, we propose a novel and principled method for 3D molecule generation named Geometric Latent Diffusion Models (GeoLDM). GeoLDM is the first latent DM model for the molecular geometry domain, composed of autoencoders encoding structures into continuous latent codes and DMs operating in the latent space. Our key innovation is that for modeling the 3D molecular geometries, we capture its critical roto-translational equivariance constraints by building a point-structured latent space with both invariant scalars and equivariant tensors. Extensive experiments demonstrate that GeoLDM can consistently achieve better performance on multiple molecule generation benchmarks, with up to 7\\% improvement for the valid percentage of large biomolecules. Results also demonstrate GeoLDM's higher capacity for controllable generation thanks to the latent modeling. Code is provided at \\url{https://github.com/MinkaiXu/GeoLDM}.",
        "authors": [
            "Minkai Xu",
            "Alexander Powers",
            "R. Dror",
            "Stefano Ermon",
            "J. Leskovec"
        ],
        "citations": 99,
        "references": 62,
        "year": 2023
    },
    {
        "title": "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models",
        "abstract": "Attaining a high degree of user controllability in visual generation often requires intricate, fine-grained inputs like layouts. However, such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models. We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs. LayoutGPT can generate plausible layouts in multiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness. Lastly, LayoutGPT achieves comparable performance to supervised methods in 3D indoor scene synthesis, demonstrating its effectiveness and potential in multiple visual domains.",
        "authors": [
            "Weixi Feng",
            "Wanrong Zhu",
            "Tsu-Jui Fu",
            "Varun Jampani",
            "Arjun Reddy Akula",
            "Xuehai He",
            "Sugato Basu",
            "X. Wang",
            "William Yang Wang"
        ],
        "citations": 108,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation",
        "abstract": "Talking face generation has historically struggled to produce head movements and natural facial expressions without guidance from additional reference videos. Recent developments in diffusion-based generative models allow for more realistic and stable data synthesis and their performance on image and video generation has surpassed that of other generative models. In this work, we present an autoregressive diffusion model that requires only one identity image and audio sequence to generate a video of a realistic talking head. Our solution is capable of hallucinating head movements, facial expressions, such as blinks, and preserving a given background. We evaluate our model on two different datasets, achieving state-of-the-art results in expressiveness and smoothness on both of them.1",
        "authors": [
            "Michal Stypulkowski",
            "Konstantinos Vougioukas",
            "Sen He",
            "Maciej Ziȩba",
            "Stavros Petridis",
            "M. Pantic"
        ],
        "citations": 101,
        "references": 54,
        "year": 2023
    },
    {
        "title": "A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts",
        "abstract": "Despite continuous improvements, precipitation forecasts are still not as accurate and reliable as those of other meteorological variables. A major contributing factor to this is that several key processes affecting precipitation distribution and intensity occur below the resolved scale of global weather models. Generative adversarial networks (GANs) have been demonstrated by the computer vision community to be successful at super‐resolution problems, that is, learning to add fine‐scale structure to coarse images. Leinonen et al. (2020, https://doi.org/10.1109/TGRS.2020.3032790) previously applied a GAN to produce ensembles of reconstructed high‐resolution atmospheric fields, given coarsened input data. In this paper, we demonstrate this approach can be extended to the more challenging problem of increasing the accuracy and resolution of comparatively low‐resolution input from a weather forecasting model, using high‐resolution radar measurements as a “ground truth.” The neural network must learn to add resolution and structure whilst accounting for non‐negligible forecast error. We show that GANs and VAE‐GANs can match the statistical properties of state‐of‐the‐art pointwise post‐processing methods whilst creating high‐resolution, spatially coherent precipitation maps. Our model compares favorably to the best existing downscaling methods in both pixel‐wise and pooled CRPS scores, power spectrum information and rank histograms (used to assess calibration). We test our models and show that they perform in a range of scenarios, including heavy rainfall.",
        "authors": [
            "L. Harris",
            "Andrew T. T. McRae",
            "M. Chantry",
            "P. Dueben",
            "T. Palmer"
        ],
        "citations": 92,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Generative Modelling With Inverse Heat Dissipation",
        "abstract": "While diffusion models have shown great success in image generation, their noise-inverting generative process does not explicitly consider the structure of images, such as their inherent multi-scale nature. Inspired by diffusion models and the empirical success of coarse-to-fine modelling, we propose a new diffusion-like model that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image. We interpret the solution of the forward heat equation with constant additive noise as a variational approximation in the diffusion latent variable model. Our new model shows emergent qualitative properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images. Spectral analysis on natural images highlights connections to diffusion models and reveals an implicit coarse-to-fine inductive bias in them.",
        "authors": [
            "Severi Rissanen",
            "Markus Heinonen",
            "A. Solin"
        ],
        "citations": 87,
        "references": 93,
        "year": 2022
    },
    {
        "title": "Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling",
        "abstract": "In offline reinforcement learning, weighted regression is a common method to ensure the learned policy stays close to the behavior policy and to prevent selecting out-of-sample actions. In this work, we show that due to the limited distributional expressivity of policy models, previous methods might still select unseen actions during training, which deviates from their initial motivation. To address this problem, we adopt a generative approach by decoupling the learned policy into two parts: an expressive generative behavior model and an action evaluation model. The key insight is that such decoupling avoids learning an explicitly parameterized policy model with a closed-form expression. Directly learning the behavior policy allows us to leverage existing advances in generative modeling, such as diffusion-based methods, to model diverse behaviors. As for action evaluation, we combine our method with an in-sample planning technique to further avoid selecting out-of-sample actions and increase computational efficiency. Experimental results on D4RL datasets show that our proposed method achieves competitive or superior performance compared with state-of-the-art offline RL methods, especially in complex tasks such as AntMaze. We also empirically demonstrate that our method can successfully learn from a heterogeneous dataset containing multiple distinctive but similarly successful strategies, whereas previous unimodal policies fail.",
        "authors": [
            "Huayu Chen",
            "Cheng Lu",
            "Chengyang Ying",
            "Hang Su",
            "Jun Zhu"
        ],
        "citations": 84,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Diffusion Models for Adversarial Purification",
        "abstract": "Adversarial purification refers to a class of defense methods that remove adversarial perturbations using a generative model. These methods do not make assumptions on the form of attack and the classification model, and thus can defend pre-existing classifiers against unseen threats. However, their performance currently falls behind adversarial training methods. In this work, we propose DiffPure that uses diffusion models for adversarial purification: Given an adversarial example, we first diffuse it with a small amount of noise following a forward diffusion process, and then recover the clean image through a reverse generative process. To evaluate our method against strong adaptive attacks in an efficient and scalable way, we propose to use the adjoint method to compute full gradients of the reverse generative process. Extensive experiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQ with three classifier architectures including ResNet, WideResNet and ViT demonstrate that our method achieves the state-of-the-art results, outperforming current adversarial training and adversarial purification methods, often by a large margin. Project page: https://diffpure.github.io.",
        "authors": [
            "Weili Nie",
            "Brandon Guo",
            "Yujia Huang",
            "Chaowei Xiao",
            "Arash Vahdat",
            "Anima Anandkumar"
        ],
        "citations": 342,
        "references": 70,
        "year": 2022
    },
    {
        "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models",
        "abstract": "Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth—a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10,000x smaller than a normal DreamBooth model.",
        "authors": [
            "Nataniel Ruiz",
            "Yuanzhen Li",
            "Varun Jampani",
            "Wei Wei",
            "Tingbo Hou",
            "Y. Pritch",
            "N. Wadhwa",
            "Michael Rubinstein",
            "Kfir Aberman"
        ],
        "citations": 135,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions",
        "abstract": "Probabilistic diffusion models have achieved state-of-the-art results for image synthesis, inpainting, and text-to-image tasks. However, they are still in the early stages of generating complex 3D shapes. This work proposes Diffusion-SDF, a generative model for shape completion, single-view reconstruction, and reconstruction of real-scanned point clouds. We use neural signed distance functions (SDFs) as our 3D representation to parameterize the geometry of various signals (e.g., point clouds, 2D images) through neural networks. Neural SDFs are implicit functions and diffusing them amounts to learning the reversal of their neural network weights, which we solve using a custom modulation module. Extensive experiments show that our method is capable of both realistic unconditional generation and conditional generation from partial inputs. This work expands the domain of diffusion models from learning 2D, explicit representations, to 3D, implicit representations. Code is released at https://github.com/princeton-computational-imaging/Diffusion-SDF.",
        "authors": [
            "Gene Chou",
            "Yuval Bahat",
            "Felix Heide"
        ],
        "citations": 88,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Systematic Review of Generative Adversarial Networks (GANs) for Medical Image Classification and Segmentation",
        "abstract": null,
        "authors": [
            "J. Jeong",
            "Amara Tariq",
            "Tobiloba Adejumo",
            "H. Trivedi",
            "J. Gichoya",
            "I. Banerjee"
        ],
        "citations": 86,
        "references": 91,
        "year": 2022
    },
    {
        "title": "Federated Learning of Generative Image Priors for MRI Reconstruction",
        "abstract": "Multi-institutional efforts can facilitate training of deep MRI reconstruction models, albeit privacy risks arise during cross-site sharing of imaging data. Federated learning (FL) has recently been introduced to address privacy concerns by enabling distributed training without transfer of imaging data. Existing FL methods employ conditional reconstruction models to map from undersampled to fully-sampled acquisitions via explicit knowledge of the accelerated imaging operator. Since conditional models generalize poorly across different acceleration rates or sampling densities, imaging operators must be fixed between training and testing, and they are typically matched across sites. To improve patient privacy, performance and flexibility in multi-site collaborations, here we introduce Federated learning of Generative IMage Priors (FedGIMP) for MRI reconstruction. FedGIMP leverages a two-stage approach: cross-site learning of a generative MRI prior, and prior adaptation following injection of the imaging operator. The global MRI prior is learned via an unconditional adversarial model that synthesizes high-quality MR images based on latent variables. A novel mapper subnetwork produces site-specific latents to maintain specificity in the prior. During inference, the prior is first combined with subject-specific imaging operators to enable reconstruction, and it is then adapted to individual cross-sections by minimizing a data-consistency loss. Comprehensive experiments on multi-institutional datasets clearly demonstrate enhanced performance of FedGIMP against both centralized and FL methods based on conditional models.",
        "authors": [
            "Gokberk Elmas",
            "S. Dar",
            "Yilmaz Korkmaz",
            "Emir Ceyani",
            "Burak Susam",
            "Muzaffer Ozbey",
            "S. Avestimehr",
            "Tolga cCukur"
        ],
        "citations": 74,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment",
        "abstract": "Training a generative adversarial network (GAN) with limited data has been a challenging task. A feasible solution is to start with a GAN well-trained on a large scale source domain and adapt it to the target domain with a few samples, termed as few shot generative model adaption. However, existing methods are prone to model overfitting and collapse in extremely few shot setting (less than 10). To solve this problem, we propose a relaxed spatial structural alignment (RSSA) method to calibrate the target generative models during the adaption. We design a cross-domain spatial structural consistency loss comprising the self-correlation and disturbance correlation consistency loss. It helps align the spatial structural information between the synthesis image pairs of the source and target domains. To relax the cross-domain alignment, we compress the original latent space of generative models to a subspace. Image pairs generated from the subspace are pulled closer. Qualitative and quantitative experiments show that our method consistently surpasses the state-of-the-art methods in few shot setting. Our source code: https://github.com/StevenShaw1999/RSSA.",
        "authors": [
            "Jiayu Xiao",
            "Liang Li",
            "Chaofei Wang",
            "Zhengjun Zha",
            "Qingming Huang"
        ],
        "citations": 65,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Improving Diffusion Models for Inverse Problems using Manifold Constraints",
        "abstract": "Recently, diffusion models have been used to solve various inverse problems in an unsupervised manner with appropriate modifications to the sampling process. However, the current solvers, which recursively apply a reverse diffusion step followed by a projection-based measurement consistency step, often produce suboptimal results. By studying the generative sampling path, here we show that current solvers throw the sample path off the data manifold, and hence the error accumulates. To address this, we propose an additional correction term inspired by the manifold constraint, which can be used synergistically with the previous solvers to make the iterations close to the manifold. The proposed manifold constraint is straightforward to implement within a few lines of code, yet boosts the performance by a surprisingly large margin. With extensive experiments, we show that our method is superior to the previous methods both theoretically and empirically, producing promising results in many applications such as image inpainting, colorization, and sparse-view computed tomography. Code available https://github.com/HJ-harry/MCG_diffusion",
        "authors": [
            "Hyungjin Chung",
            "Byeongsu Sim",
            "Dohoon Ryu",
            "J. C. Ye"
        ],
        "citations": 342,
        "references": 62,
        "year": 2022
    },
    {
        "title": "Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey",
        "abstract": null,
        "authors": [
            "Xiao Wang",
            "Guangyao Chen",
            "Guangwu Qian",
            "Pengcheng Gao",
            "Xiaoyong Wei",
            "Yaowei Wang",
            "Yonghong Tian",
            "Wen Gao"
        ],
        "citations": 153,
        "references": 287,
        "year": 2023
    },
    {
        "title": "Are Diffusion Models Vulnerable to Membership Inference Attacks?",
        "abstract": "Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e.g., DDPM, and the text-to-image diffusion models, e.g., Latent Diffusion Models and Stable Diffusion. Experimental results demonstrate that our methods precisely infer the membership with high confidence on both of the two scenarios across multiple different datasets. Code is available at https://github.com/jinhaoduan/SecMI.",
        "authors": [
            "Jinhao Duan",
            "Fei Kong",
            "Shiqi Wang",
            "Xiaoshuang Shi",
            "Kaidi Xu"
        ],
        "citations": 90,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Fast Sampling of Diffusion Models with Exponential Integrator",
        "abstract": "The past few years have witnessed the great success of Diffusion models~(DMs) in generating high-fidelity samples in generative modeling tasks. A major limitation of the DM is its notoriously slow sampling procedure which normally requires hundreds to thousands of time discretization steps of the learned diffusion process to reach the desired accuracy. Our goal is to develop a fast sampling method for DMs with a much less number of steps while retaining high sample quality. To this end, we systematically analyze the sampling procedure in DMs and identify key factors that affect the sample quality, among which the method of discretization is most crucial. By carefully examining the learned diffusion process, we propose Diffusion Exponential Integrator Sampler~(DEIS). It is based on the Exponential Integrator designed for discretizing ordinary differential equations (ODEs) and leverages a semilinear structure of the learned diffusion process to reduce the discretization error. The proposed method can be applied to any DMs and can generate high-fidelity samples in as few as 10 steps. In our experiments, it takes about 3 minutes on one A6000 GPU to generate $50k$ images from CIFAR10. Moreover, by directly using pre-trained DMs, we achieve the state-of-art sampling performance when the number of score function evaluation~(NFE) is limited, e.g., 4.17 FID with 10 NFEs, 3.37 FID, and 9.74 IS with only 15 NFEs on CIFAR10. Code is available at https://github.com/qsh-zh/deis",
        "authors": [
            "Qinsheng Zhang",
            "Yongxin Chen"
        ],
        "citations": 345,
        "references": 61,
        "year": 2022
    },
    {
        "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
        "abstract": "Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.",
        "authors": [
            "Zechun Liu",
            "Barlas Oğuz",
            "Changsheng Zhao",
            "Ernie Chang",
            "Pierre Stock",
            "Yashar Mehdad",
            "Yangyang Shi",
            "Raghuraman Krishnamoorthi",
            "Vikas Chandra"
        ],
        "citations": 144,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Human Preference Score: Better Aligning Text-to-image Models with Human Preference",
        "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human preferences. Our experiments show that HPS outperforms CLIP in predicting human choices and has good generalization capability toward images generated from other models. By tuning Stable Diffusion with the guidance of HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/alignsd-web/.",
        "authors": [
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Feng Zhu",
            "Rui Zhao",
            "Hongsheng Li"
        ],
        "citations": 94,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Better Aligning Text-to-Image Models with Human Preference",
        "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classiﬁer with the collected dataset and derive a Human Preference Score (HPS) based on the classiﬁer. Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. By tuning Stable Diffusion with the guidance of the HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/align sd web/.",
        "authors": [
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Feng Zhu",
            "Rui Zhao",
            "Hongsheng Li"
        ],
        "citations": 87,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners",
        "abstract": "Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pretraining paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by ‘Prompt, Generate, then Cache’. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of-the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.",
        "authors": [
            "Renrui Zhang",
            "Xiangfei Hu",
            "Bohao Li",
            "Siyuan Huang",
            "Hanqiu Deng",
            "Hongsheng Li",
            "Y. Qiao",
            "Peng Gao"
        ],
        "citations": 139,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Brain Tumor Classification Using a Combination of Variational Autoencoders and Generative Adversarial Networks",
        "abstract": "Brain tumors are a pernicious cancer with one of the lowest five-year survival rates. Neurologists often use magnetic resonance imaging (MRI) to diagnose the type of brain tumor. Automated computer-assisted tools can help them speed up the diagnosis process and reduce the burden on the health care systems. Recent advances in deep learning for medical imaging have shown remarkable results, especially in the automatic and instant diagnosis of various cancers. However, we need a large amount of data (images) to train the deep learning models in order to obtain good results. Large public datasets are rare in medicine. This paper proposes a framework based on unsupervised deep generative neural networks to solve this limitation. We combine two generative models in the proposed framework: variational autoencoders (VAEs) and generative adversarial networks (GANs). We swap the encoder–decoder network after initially training it on the training set of available MR images. The output of this swapped network is a noise vector that has information of the image manifold, and the cascaded generative adversarial network samples the input from this informative noise vector instead of random Gaussian noise. The proposed method helps the GAN to avoid mode collapse and generate realistic-looking brain tumor magnetic resonance images. These artificially generated images could solve the limitation of small medical datasets up to a reasonable extent and help the deep learning models perform acceptably. We used the ResNet50 as a classifier, and the artificially generated brain tumor images are used to augment the real and available images during the classifier training. We compared the classification results with several existing studies and state-of-the-art machine learning models. Our proposed methodology noticeably achieved better results. By using brain tumor images generated artificially by our proposed method, the classification average accuracy improved from 72.63% to 96.25%. For the most severe class of brain tumor, glioma, we achieved 0.769, 0.837, 0.833, and 0.80 values for recall, specificity, precision, and F1-score, respectively. The proposed generative model framework could be used to generate medical images in any domain, including PET (positron emission tomography) and MRI scans of various parts of the body, and the results show that it could be a useful clinical tool for medical experts.",
        "authors": [
            "Bilal Ahmad",
            "Jun Sun",
            "Qi You",
            "V. Palade",
            "Zhongjie Mao"
        ],
        "citations": 63,
        "references": 51,
        "year": 2022
    },
    {
        "title": "An optimal control perspective on diffusion-based generative modeling",
        "abstract": "We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton-Jacobi-Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback-Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approaches on multiple numerical examples.",
        "authors": [
            "Julius Berner",
            "Lorenz Richter",
            "Karen Ullrich"
        ],
        "citations": 60,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Visual Prompt Tuning for Generative Transfer Learning",
        "abstract": "Learning generative image models from various domains efficiently needs transferring knowledge from an image synthesis model trained on a large dataset. We present a recipe for learning vision transformers by generative knowledge transfer. We base our framework on generative vision transformers representing an image as a sequence of visual tokens with the autoregressive or non-autoregressive transformers. To adapt to a new domain, we employ prompt tuning, which prepends learnable tokens called prompts to the image token sequence and introduces a new prompt design for our task. We study on a variety of visual domains with varying amounts of training images. We show the effectiveness of knowledge transfer and a significantly better image generation quality.11https://github.com/google-research/generative_transfer",
        "authors": [
            "Kihyuk Sohn",
            "Yuan Hao",
            "José Lezama",
            "Luisa F. Polanía",
            "Huiwen Chang",
            "Han Zhang",
            "Irfan Essa",
            "Lu Jiang"
        ],
        "citations": 68,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration",
        "abstract": "Plug-and-play Image Restoration (IR) has been widely recognized as a flexible and interpretable method for solving various inverse problems by utilizing any off-the-shelf denoiser as the implicit image prior. However, most existing methods focus on discriminative Gaussian denoisers. Although diffusion models have shown impressive performance for high-quality image synthesis, their potential to serve as a generative denoiser prior to the plug-and-play IR methods remains to be further explored. While several other attempts have been made to adopt diffusion models for image restoration, they either fail to achieve satisfactory results or typically require an unacceptable number of Neural Function Evaluations (NFEs) during inference. This paper proposes DiffPIR, which integrates the traditional plug-and-play method into the diffusion sampling framework. Compared to plug-and-play IR methods that rely on discriminative Gaussian denoisers, DiffPIR is expected to inherit the generative ability of diffusion models. Experimental results on three representative IR tasks, including super-resolution, image deblurring, and inpainting, demonstrate that DiffPIR achieves state-of-the-art performance on both the FFHQ and ImageNet datasets in terms of reconstruction faithfulness and perceptual quality with no more than 100 NFEs. The source code is available at https://github.com/yuanzhi-zhu/DiffPIR",
        "authors": [
            "Yuanzhi Zhu",
            "K. Zhang",
            "Jingyun Liang",
            "Jiezhang Cao",
            "B. Wen",
            "R. Timofte",
            "L. Gool"
        ],
        "citations": 134,
        "references": 62,
        "year": 2023
    },
    {
        "title": "From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer",
        "abstract": "Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset OpenBG500 for research purpose1.",
        "authors": [
            "Xin Xie",
            "Ningyu Zhang",
            "Zhoubo Li",
            "Shumin Deng",
            "Hui Chen",
            "Feiyu Xiong",
            "Mosha Chen",
            "Huajun Chen"
        ],
        "citations": 79,
        "references": 21,
        "year": 2022
    },
    {
        "title": "CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks",
        "abstract": "Knowledge-intensive language tasks (KILT) usually require a large body of information to provide correct answers. A popular paradigm to solve this problem is to combine a search system with a machine reader, where the former retrieves supporting evidences and the latter examines them to produce answers. Recently, the reader component has witnessed significant advances with the help of large-scale pre-trained generative models. Meanwhile most existing solutions in the search component rely on the traditional \"index-retrieve-then-rank'' pipeline, which suffers from large memory footprint and difficulty in end-to-end optimization. Inspired by recent efforts in constructing model-based IR models, we propose to replace the traditional multi-step search pipeline with a novel single-step generative model, which can dramatically simplify the search process and be optimized in an end-to-end manner. We show that a strong generative retrieval model can be learned with a set of adequately designed pre-training tasks, and be adopted to improve a variety of downstream KILT tasks with further fine-tuning. We name the pre-trained generative retrieval model as CorpusBrain as all information about the corpus is encoded in its parameters without the need of constructing additional index. Empirical results show that CorpusBrain can significantly outperform strong baselines for the retrieval task on the KILT benchmark and establish new state-of-the-art downstream performances. We also show that CorpusBrain works well under zero- and low-resource settings.",
        "authors": [
            "Jiangui Chen",
            "Ruqing Zhang",
            "J. Guo",
            "Y. Liu",
            "Yixing Fan",
            "Xueqi Cheng"
        ],
        "citations": 54,
        "references": 52,
        "year": 2022
    },
    {
        "title": "RELATION: A Deep Generative Model for Structure-Based De Novo Drug Design.",
        "abstract": "Deep learning (DL)-based de novo molecular design has recently gained considerable traction. Many DL-based generative models have been successfully developed to design novel molecules, but most of them are ligand-centric and the role of the 3D geometries of target binding pockets in molecular generation has not been well-exploited. Here, we proposed a new 3D-based generative model called RELATION. In the RELATION model, the BiTL algorithm was specifically designed to extract and transfer the desired geometric features of the protein-ligand complexes to a latent space for generation. The pharmacophore conditioning and docking-based Bayesian sampling were applied to efficiently navigate the vast chemical space for the design of molecules with desired geometric properties and pharmacophore features. As a proof of concept, the RELATION model was used to design inhibitors for two targets, AKT1 and CDK2. The calculation results demonstrated that the RELATION model could efficiently generate novel molecules with favorable binding affinity and pharmacophore features.",
        "authors": [
            "Mingyang Wang",
            "Chang-Yu Hsieh",
            "Jike Wang",
            "Dong Wang",
            "Gaoqi Weng",
            "Chao Shen",
            "Xiaojun Yao",
            "Zhitong Bing",
            "Honglin Li",
            "Dongsheng Cao",
            "Tingjun Hou"
        ],
        "citations": 54,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models",
        "abstract": "Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20x to 80x speed up.",
        "authors": [
            "Fan Bao",
            "Chongxuan Li",
            "Jun Zhu",
            "Bo Zhang"
        ],
        "citations": 293,
        "references": 51,
        "year": 2022
    },
    {
        "title": "3DILG: Irregular Latent Grids for 3D Generative Modeling",
        "abstract": "We propose a new representation for encoding 3D shapes as neural fields. The representation is designed to be compatible with the transformer architecture and to benefit both shape reconstruction and shape generation. Existing works on neural fields are grid-based representations with latents defined on a regular grid. In contrast, we define latents on irregular grids, enabling our representation to be sparse and adaptive. In the context of shape reconstruction from point clouds, our shape representation built on irregular grids improves upon grid-based methods in terms of reconstruction accuracy. For shape generation, our representation promotes high-quality shape generation using auto-regressive probabilistic models. We show different applications that improve over the current state of the art. First, we show results for probabilistic shape reconstruction from a single higher resolution image. Second, we train a probabilistic model conditioned on very low resolution images. Third, we apply our model to category-conditioned generation. All probabilistic experiments confirm that we are able to generate detailed and high quality shapes to yield the new state of the art in generative 3D shape modeling.",
        "authors": [
            "Biao Zhang",
            "M. Nießner",
            "Peter Wonka"
        ],
        "citations": 74,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Wavelet Score-Based Generative Modeling",
        "abstract": "Score-based generative models (SGMs) synthesize new data samples from Gaussian white noise by running a time-reversed Stochastic Differential Equation (SDE) whose drift coefficient depends on some probabilistic score. The discretization of such SDEs typically requires a large number of time steps and hence a high computational cost. This is because of ill-conditioning properties of the score that we analyze mathematically. We show that SGMs can be considerably accelerated, by factorizing the data distribution into a product of conditional probabilities of wavelet coefficients across scales. The resulting Wavelet Score-based Generative Model (WSGM) synthesizes wavelet coefficients with the same number of time steps at all scales, and its time complexity therefore grows linearly with the image size. This is proved mathematically over Gaussian distributions, and shown numerically over physical processes at phase transition and natural image datasets.",
        "authors": [
            "Florentin Guth",
            "Simon Coste",
            "Valentin De Bortoli",
            "S. Mallat"
        ],
        "citations": 47,
        "references": 64,
        "year": 2022
    },
    {
        "title": "Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images",
        "abstract": "Learning 3D generative models from a dataset of monocular images enables self-supervised 3D reasoning and controllable synthesis. State-of-the-art 3D generative models are GANs that use neural 3D volumetric representations for synthesis. Images are synthesized by rendering the volumes from a given camera. These models can disentangle the 3D scene from the camera viewpoint in any generated image. However, most models do not disentangle other factors of image formation, such as geometry and appearance. In this paper, we design a 3D GAN which can learn a disentangled model of objects, just from monocular observations. Our model can disentangle the geometry and appearance variations in the scene, i.e., we can independently sample from the geometry and appearance spaces of the generative model. This is achieved using a novel non-rigid deformable scene formulation. A 3D volume that represents an object instance is computed as a non-rigidly deformed canonical 3D volume. Our method learns the canonical volume, as well as its deformations, jointly during training. This formulation also helps us improve the disentanglement between the 3D scene and the camera viewpoints using a novel pose regularization loss defined on the 3D deformation field. In addition, we model the inverse deformations, enabling the computation of dense correspondences between images generated by our model. Finally, we design an approach to embed real images into the latent space of our model, enabling editing of real images.",
        "authors": [
            "A. Tewari",
            "R. MallikarjunB.",
            "Xingang Pan",
            "Ohad Fried",
            "Maneesh Agrawala",
            "C. Theobalt"
        ],
        "citations": 49,
        "references": 50,
        "year": 2022
    },
    {
        "title": "Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",
        "abstract": "Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers. Intriguingly we find these samplers lead to notable improvements in compositional generation across a wide set of problems such as classifier-guided ImageNet modeling and compositional text-to-image generation.",
        "authors": [
            "Yilun Du",
            "Conor Durkan",
            "Robin Strudel",
            "J. Tenenbaum",
            "S. Dieleman",
            "R. Fergus",
            "Jascha Narain Sohl-Dickstein",
            "A. Doucet",
            "Will Grathwohl"
        ],
        "citations": 105,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Prompting Large Language Models with Speech Recognition Abilities",
        "abstract": "Large language models (LLMs) have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLM by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audio embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% relatively in WER and perform multilingual speech recognition, despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to generate fewer embeddings. The results from these studies show that multilingual ASR is possible even when the LLM is frozen, or when strides of almost 1 second are used in the audio encoder opening up the possibility for LLMs to operate on long-form audio.",
        "authors": [
            "Yassir Fathullah",
            "Chunyang Wu",
            "Egor Lakomkin",
            "J. Jia",
            "Yuan Shangguan",
            "Ke Li",
            "Jinxi Guo",
            "Wenhan Xiong",
            "Jay Mahadeokar",
            "Ozlem Kalinli",
            "Christian Fuegen",
            "M. Seltzer"
        ],
        "citations": 107,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Revisiting Relation Extraction in the era of Large Language Models",
        "abstract": "Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks.",
        "authors": [
            "Somin Wadhwa",
            "Silvio Amir",
            "Byron C. Wallace"
        ],
        "citations": 110,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Diffusion Models in Vision: A Survey",
        "abstract": "Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.",
        "authors": [
            "Florinel-Alin Croitoru",
            "Vlad Hondru",
            "Radu Tudor Ionescu",
            "M. Shah"
        ],
        "citations": 894,
        "references": 171,
        "year": 2022
    },
    {
        "title": "A Bibliometric Review of Large Language Models Research from 2017 to 2023",
        "abstract": "Large language models (LLMs), such as OpenAI’s Generative Pre-trained Transformer (GPT), are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks. LLMs have become a highly sought-after research area because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains, including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of LLMs research. Overall, this paper offers valuable insights into the current state, impact, and potential of LLMs research and its applications.",
        "authors": [
            "Lizhou Fan",
            "Lingyao Li",
            "Zihui Ma",
            "Sanggyu Lee",
            "Huizi Yu",
            "Libby Hemphill"
        ],
        "citations": 111,
        "references": 86,
        "year": 2023
    },
    {
        "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds",
        "abstract": "The unprecedented advancements in Large Language Models (LLMs) have shown a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM understands colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate the perceptual and generalization capabilities of PointLLM, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different methods, including human evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples. Codes, datasets, and benchmarks are available at https://github.com/OpenRobotLab/PointLLM .",
        "authors": [
            "Runsen Xu",
            "Xiaolong Wang",
            "Tai Wang",
            "Yilun Chen",
            "Jiangmiao Pang",
            "Dahua Lin"
        ],
        "citations": 104,
        "references": 74,
        "year": 2023
    },
    {
        "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
        "abstract": "Large language models (LLMs) with instruction fine-tuning demonstrate superior generative capabilities. However, these models are resource-intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs into much smaller ones. While other similar works have been done, they are often conducted on a limited set of (usually still large) models and are not accompanied by proper evaluations. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizable, we design our instructions to cover a broad set of topics to ensure diversity. Extensive analysis of our instruction dataset confirms its diversity, and we generate responses for these instructions using gpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of models, collectively referred to as LaMini-LM, which includes models from both the encoder-decoder and decoder-only families, with varying sizes. We evaluate the performance of our models using automatic metrics on 15 different natural language processing (NLP) benchmarks, as well as through human assessment. We also assess the model for hallucination and toxicity, and for the former, we introduce a new benchmark dataset for hallucination-inducing QA. The results demonstrate that our proposed LaMini-LM models are comparable to strong baselines while being much smaller in size.",
        "authors": [
            "Minghao Wu",
            "Abdul Waheed",
            "Chiyu Zhang",
            "Muhammad Abdul-Mageed",
            "Alham Fikri Aji"
        ],
        "citations": 105,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Photorealistic Video Generation with Diffusion Models",
        "abstract": "We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of $512 \\times 896$ resolution at $8$ frames per second.",
        "authors": [
            "Agrim Gupta",
            "Lijun Yu",
            "Kihyuk Sohn",
            "Xiuye Gu",
            "Meera Hahn",
            "Fei-Fei Li",
            "Irfan Essa",
            "Lu Jiang",
            "José Lezama"
        ],
        "citations": 108,
        "references": 88,
        "year": 2023
    },
    {
        "title": "StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners",
        "abstract": "We investigate the potential of learning visual representations using synthetic images generated by text-to-image models. This is a natural question in the light of the excellent performance of such models in generating high-quality images. We consider specifically the Stable Diffusion, one of the leading open source text-to-image models. We show that (1) when the generative model is configured with proper classifier-free guidance scale, training self-supervised methods on synthetic images can match or beat the real image counterpart; (2) by treating the multiple images generated from the same text prompt as positives for each other, we develop a multi-positive contrastive learning method, which we call StableRep. With solely synthetic images, the representations learned by StableRep surpass the performance of representations learned by SimCLR and CLIP using the same set of text prompts and corresponding real images, on large scale datasets. When we further add language supervision, StableRep trained with 20M synthetic images achieves better accuracy than CLIP trained with 50M real images.",
        "authors": [
            "Yonglong Tian",
            "Lijie Fan",
            "Phillip Isola",
            "Huiwen Chang",
            "Dilip Krishnan"
        ],
        "citations": 105,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models",
        "abstract": "Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffusion process, which we show to be more robust in comparing distributions with misaligned supports. We also reveal non-trivial connections of our method to existing works such as DreamFusion, and generative adversarial training. To demonstrate the effectiveness and universality of Diff-Instruct, we consider two scenarios: distilling pre-trained diffusion models and refining existing GAN models. The experiments on distilling pre-trained diffusion models show that Diff-Instruct results in state-of-the-art single-step diffusion-based models. The experiments on refining GAN models show that the Diff-Instruct can consistently improve the pre-trained generators of GAN models across various settings.",
        "authors": [
            "Weijian Luo",
            "Tianyang Hu",
            "Shifeng Zhang",
            "Jiacheng Sun",
            "Zhenguo Li",
            "Zhihua Zhang"
        ],
        "citations": 79,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Debiasing Vision-Language Models via Biased Prompts",
        "abstract": "Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.",
        "authors": [
            "Ching-Yao Chuang",
            "Varun Jampani",
            "Yuanzhen Li",
            "A. Torralba",
            "S. Jegelka"
        ],
        "citations": 76,
        "references": 57,
        "year": 2023
    },
    {
        "title": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models",
        "abstract": "The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services have been developed to generate high-quality videos. However, these methods often use a few metrics, e.g., FVD [56] or IS [45], to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a novel framework and pipeline for exhaustively evaluating the performance of the generated videos. Our approach involves generating a diverse and comprehensive list of 700 prompts for text-to-video generation, which is based on an analysis of real-world user data and generated with the assistance of a large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmark, in terms of visual qualities, content qualities, motion qualities, and text-video alignment with 17 well-selected objective metrics. To obtain the finalleaderboard of the models, we further fit a series of coefficients to align the objective metrics to the users' opinions. Based on the proposed human alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.",
        "authors": [
            "Yaofang Liu",
            "Xiaodong Cun",
            "Xuebo Liu",
            "Xintao Wang",
            "Yong Zhang",
            "Haoxin Chen",
            "Yang Liu",
            "Tieyong Zeng",
            "Raymond H. Chan",
            "Ying Shan"
        ],
        "citations": 78,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision",
        "abstract": "Denoising diffusion models are a powerful type of generative models used to capture complex distributions of real-world signals. However, their applicability is limited to scenarios where training samples are readily available, which is not always the case in real-world applications. For example, in inverse graphics, the goal is to generate samples from a distribution of 3D scenes that align with a given image, but ground-truth 3D scenes are unavailable and only 2D images are accessible. To address this limitation, we propose a novel class of denoising diffusion probabilistic models that learn to sample from distributions of signals that are never directly observed. Instead, these signals are measured indirectly through a known differentiable forward model, which produces partial observations of the unknown signal. Our approach involves integrating the forward model directly into the denoising process. This integration effectively connects the generative modeling of observations with the generative modeling of the underlying signals, allowing for end-to-end training of a conditional generative model over signals. During inference, our approach enables sampling from the distribution of underlying signals that are consistent with a given partial observation. We demonstrate the effectiveness of our method on three challenging computer vision tasks. For instance, in the context of inverse graphics, our model enables direct sampling from the distribution of 3D scenes that align with a single 2D input image.",
        "authors": [
            "A. Tewari",
            "Tianwei Yin",
            "George Cazenavette",
            "Semon Rezchikov",
            "J. Tenenbaum",
            "F. Durand",
            "W. Freeman",
            "Vincent Sitzmann"
        ],
        "citations": 70,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Generative deep learning enables the discovery of a potent and selective RIPK1 inhibitor",
        "abstract": null,
        "authors": [
            "Yueshan Li",
            "Liting Zhang",
            "Yifei Wang",
            "Jun Zou",
            "Ruicheng Yang",
            "Xin Luo",
            "Chengyong Wu",
            "Wei Yang",
            "Chenyu Tian",
            "Hai-xia Xu",
            "Falu Wang",
            "Xin Yang",
            "Linli Li",
            "Shengyong Yang"
        ],
        "citations": 46,
        "references": 73,
        "year": 2022
    },
    {
        "title": "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions",
        "abstract": "We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does not reduce the complexity of SGMs.",
        "authors": [
            "Sitan Chen",
            "Sinho Chewi",
            "Jungshian Li",
            "Yuanzhi Li",
            "A. Salim",
            "Anru R. Zhang"
        ],
        "citations": 200,
        "references": 54,
        "year": 2022
    },
    {
        "title": "All are Worth Words: A ViT Backbone for Diffusion Models",
        "abstract": "Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and classconditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256×256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-based image modeling, the long skip connection is crucial while the down-sampling and upsampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can provide insights for future research on backbones in diffusion models and benefit generative modeling on large scale cross-modality datasets.",
        "authors": [
            "Fan Bao",
            "Shen Nie",
            "Kaiwen Xue",
            "Yue Cao",
            "Chongxuan Li",
            "Hang Su",
            "Jun Zhu"
        ],
        "citations": 218,
        "references": 89,
        "year": 2022
    },
    {
        "title": "AnoDDPM: Anomaly Detection with Denoising Diffusion Probabilistic Models using Simplex Noise",
        "abstract": "Generative models have been shown to provide a powerful mechanism for anomaly detection by learning to model healthy or normal reference data which can subsequently be used as a baseline for scoring anomalies. In this work we consider denoising diffusion probabilistic models (DDPMs) for unsupervised anomaly detection. DDPMs have superior mode coverage over generative adversarial networks (GANs) and higher sample quality than variational autoencoders (VAEs). However, this comes at the expense of poor scalability and increased sampling times due to the long Markov chain sequences required. We observe that within reconstruction-based anomaly detection a full-length Markov chain diffusion is not required. This leads us to develop a novel partial diffusion anomaly detection strategy that scales to high-resolution imagery, named AnoDDPM. A secondary problem is that Gaussian diffusion fails to capture larger anomalies; therefore we develop a multi-scale simplex noise diffusion process that gives control over the target anomaly size. AnoDDPM with simplex noise is shown to significantly outperform both f-AnoGAN and Gaussian diffusion for the tumorous dataset of 22 T1-weighted MRI scans (CCBS Edinburgh) qualitatively and quantitatively (improvement of +25.5% Sørensen–Dice coefficient, +17.6% IoU and +7.4% AUC).",
        "authors": [
            "Julian Wyatt",
            "Adam Leach",
            "Sebastian M. Schmon",
            "Chris G. Willcocks"
        ],
        "citations": 217,
        "references": 30,
        "year": 2022
    },
    {
        "title": "What is Healthy? Generative Counterfactual Diffusion for Lesion Localization",
        "abstract": "Reducing the requirement for densely annotated masks in medical image segmentation is important due to cost constraints. In this paper, we consider the problem of inferring pixel-level predictions of brain lesions by only using image-level labels for training. By leveraging recent advances in generative diffusion probabilistic models (DPM), we synthesize counterfactuals of\"How would a patient appear if X pathology was not present?\". The difference image between the observed patient state and the healthy counterfactual can be used for inferring the location of pathology. We generate counterfactuals that correspond to the minimal change of the input such that it is transformed to healthy domain. This requires training with healthy and unhealthy data in DPMs. We improve on previous counterfactual DPMs by manipulating the generation process with implicit guidance along with attention conditioning instead of using classifiers. Code is available at https://github.com/vios-s/Diff-SCM.",
        "authors": [
            "Pedro Sanchez",
            "Antanas Kascenas",
            "Xiao Liu",
            "Alison Q. O'Neil",
            "S. Tsaftaris"
        ],
        "citations": 57,
        "references": 23,
        "year": 2022
    },
    {
        "title": "Recent Advances for Quantum Neural Networks in Generative Learning",
        "abstract": "Quantum computers are next-generation devices that hold promise to perform calculations beyond the reach of classical computers. A leading method towards achieving this goal is through quantum machine learning, especially quantum generative learning. Due to the intrinsic probabilistic nature of quantum mechanics, it is reasonable to postulate that quantum generative learning models (QGLMs) may surpass their classical counterparts. As such, QGLMs are receiving growing attention from the quantum physics and computer science communities, where various QGLMs that can be efficiently implemented on near-term quantum machines with potential computational advantages are proposed. In this paper, we review the current progress of QGLMs from the perspective of machine learning. Particularly, we interpret these QGLMs, covering quantum circuit Born machines, quantum generative adversarial networks, quantum Boltzmann machines, and quantum variational autoencoders, as the quantum extension of classical generative learning models. In this context, we explore their intrinsic relations and their fundamental differences. We further summarize the potential applications of QGLMs in both conventional machine learning tasks and quantum physics. Last, we discuss the challenges and further research directions for QGLMs.",
        "authors": [
            "Jinkai Tian",
            "Xiaoyun Sun",
            "Yuxuan Du",
            "Shanshan Zhao",
            "Qing Liu",
            "Kaining Zhang",
            "Wei Yi",
            "Wanrong Huang",
            "Chaoyue Wang",
            "Xingyao Wu",
            "Min-Hsiu Hsieh",
            "Tongliang Liu",
            "Wen-Bin Yang",
            "Dacheng Tao"
        ],
        "citations": 64,
        "references": 323,
        "year": 2022
    },
    {
        "title": "Your ViT is Secretly a Hybrid Discriminative-Generative Diffusion Model",
        "abstract": "Diffusion Denoising Probability Models (DDPM) and Vision Transformer (ViT) have demonstrated significant progress in generative tasks and discriminative tasks, respectively, and thus far these models have largely been developed in their own domains. In this paper, we establish a direct connection between DDPM and ViT by integrating the ViT architecture into DDPM, and introduce a new generative model called Generative ViT (GenViT). The modeling flexibility of ViT enables us to further extend GenViT to hybrid discriminative-generative modeling, and introduce a Hybrid ViT (HybViT). Our work is among the first to explore a single ViT for image generation and classification jointly. We conduct a series of experiments to analyze the performance of proposed models and demonstrate their superiority over prior state-of-the-arts in both generative and discriminative tasks. Our code and pre-trained models can be found in https://github.com/sndnyang/Diffusion_ViT .",
        "authors": [
            "Xiulong Yang",
            "Sheng-Min Shih",
            "Yinlin Fu",
            "Xiaoting Zhao",
            "Shihao Ji"
        ],
        "citations": 50,
        "references": 68,
        "year": 2022
    },
    {
        "title": "CGOF++: Controllable 3D Face Synthesis With Conditional Generative Occupancy Fields",
        "abstract": "Capitalizing on the recent advances in image generation models, existing controllable face image synthesis methods are able to generate high-fidelity images with some levels of controllability, e.g., controlling the shapes, expressions, textures, and poses of the generated face images. However, previous methods focus on controllable 2D image generative models, which are prone to producing inconsistent face images under large expression and pose changes. In this paper, we propose a new NeRF-based conditional 3D face synthesis framework, which enables 3D controllability over the generated face images by imposing explicit 3D conditions from 3D face priors. At its core is a conditional Generative Occupancy Field (cGOF++) that effectively enforces the shape of the generated face to conform to a given 3D Morphable Model (3DMM) mesh, built on top of EG3D (Chan et al. 2022), a recent tri-plane-based generative model. To achieve accurate control over fine-grained 3D face shapes of the synthesized images, we additionally incorporate a 3D landmark loss as well as a volume warping loss into our synthesis framework. Experiments validate the effectiveness of the proposed method, which is able to generate high-fidelity face images and shows more precise 3D controllability than state-of-the-art 2D-based controllable face synthesis methods.",
        "authors": [
            "Keqiang Sun",
            "Shangzhe Wu",
            "Zhaoyang Huang",
            "Ning Zhang",
            "Quan Wang",
            "Hongsheng Li"
        ],
        "citations": 45,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance",
        "abstract": "Score-based generative models are shown to achieve remarkable empirical performances in various applications such as image generation and audio synthesis. However, a theoretical understanding of score-based diffusion models is still incomplete. Recently, Song et al. showed that the training objective of score-based generative models is equivalent to minimizing the Kullback-Leibler divergence of the generated distribution from the data distribution. In this work, we show that score-based models also minimize the Wasserstein distance between them under suitable assumptions on the model. Specifically, we prove that the Wasserstein distance is upper bounded by the square root of the objective function up to multiplicative constants and a fixed constant offset. Our proof is based on a novel application of the theory of optimal transport, which can be of independent interest to the society. Our numerical experiments support our findings. By analyzing our upper bounds, we provide a few techniques to obtain tighter upper bounds.",
        "authors": [
            "Dohyun Kwon",
            "Ying Fan",
            "Kangwook Lee"
        ],
        "citations": 40,
        "references": 50,
        "year": 2022
    },
    {
        "title": "VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting",
        "abstract": "We propose VoLux-GAN, a generative framework to synthesize 3D-aware faces with convincing relighting. Our main contribution is a volumetric HDRI relighting method that can efficiently accumulate albedo, diffuse and specular lighting contributions along each 3D ray for any desired HDR environmental map. Additionally, we show the importance of supervising the image decomposition process using multiple discriminators. In particular, we propose a data augmentation technique that leverages recent advances in single image portrait relighting to enforce consistent geometry, albedo, diffuse and specular components. Multiple experiments and comparisons with other generative frameworks show how our model is a step forward towards photorealistic relightable 3D generative models. Code and pre-trained models are available at: https://github.com/google/volux-gan.",
        "authors": [
            "Feitong Tan",
            "S. Fanello",
            "Abhimitra Meka",
            "S. Orts-Escolano",
            "Danhang Tang",
            "Rohit Pandey",
            "Jonathan Taylor",
            "P. Tan",
            "Yinda Zhang"
        ],
        "citations": 40,
        "references": 80,
        "year": 2022
    },
    {
        "title": "Missing Data Repairs for Traffic Flow With Self-Attention Generative Adversarial Imputation Net",
        "abstract": "With the rapid development of sensor technologies, time series data collected by multiple and spatially distributed sensors have been widely used in different research fields. Examples of such data include geo-tagged temperature data collected by temperature sensors, air pollutant monitoring data, and traffic data collected by road traffic sensors. Due to sensor failure, communication errors and storage loss, etc., data collected by sensors inevitably includes missing data. However, models commonly used in the analysis of such large-scale data often rely on complete data sets. This paper proposes a model for the imputation of missing data of traffic flow, which combines a self-attention mechanism, an auto-encoder, and a generative adversarial network, into a self-attention generative adversarial imputation net (SA-GAIN). The introduction of the self-attention mechanism can help the proposed model to effectively capture correlations between spatially-distributed sensors at different time points. Adversarial training through two neural networks, called generators and discriminators, allows the proposed model to generate imputed data close to the real data. In comparison with different imputation models, the proposed model shows the best performance in imputing missing data.",
        "authors": [
            "Weibin Zhang",
            "Pulin Zhang",
            "Yinghao Yu",
            "Xiying Li",
            "S. A. Biancardo",
            "Junyi Zhang"
        ],
        "citations": 58,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Generative adversarial networks and its applications in the biomedical image segmentation: a comprehensive survey",
        "abstract": null,
        "authors": [
            "Ahmed Iqbal",
            "Muhammad Sharif",
            "Mussarat Yasmin",
            "M. Raza",
            "Shabib Aftab"
        ],
        "citations": 44,
        "references": 156,
        "year": 2022
    },
    {
        "title": "Improved Generative Adversarial Network for Rotating Component Fault Diagnosis in Scenarios With Extremely Limited Data",
        "abstract": "Traditional data-driven intelligent fault diagnosis methods for rotating component commonly assume that sufficient labeled data is available. However, the rotary machine works in a normal state most of the time in practical engineering, resulting in fault diagnosis scenarios with extremely limited and imbalanced data. This phenomenon deteriorates the performance of intelligent fault diagnosis methods. In this article, a data augmentation method based on an improved variational autoencoding generative adversarial network is proposed to address this problem. First, the self-attention mechanism and Wasserstein distance with gradient penalty are introduced to improve data generation capability and training stability of the generative model. Then, the generative models are used to generate synthetic samples with high similarity and diversity based on extremely limited frequency spectrum samples. Subsequently, data filters are established to automatically refine the generated samples. Finally, the refined samples are used to enrich imbalanced dataset and improve the fault diagnosis accuracy. The effectiveness of the proposed method is validated by vibration datasets of two rotating components. Experimental results show that the proposed method can generate high-quality samples and has the potential to enhance the fault diagnosis performance in scenarios with limited data.",
        "authors": [
            "Jiang Miao",
            "Jianyu Wang",
            "Dingcheng Zhang",
            "Q. Miao"
        ],
        "citations": 41,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness",
        "abstract": "Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.",
        "authors": [
            "Felix Friedrich",
            "P. Schramowski",
            "Manuel Brack",
            "Lukas Struppek",
            "Dominik Hintersdorf",
            "Sasha Luccioni",
            "K. Kersting"
        ],
        "citations": 95,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Generative Deep Learning to Detect Cyberattacks for the IoT-23 Dataset",
        "abstract": "The rapid growth of Internet of Things (IoT) is expected to add billions of IoT devices connected to the Internet. These devices represent a vast attack surface for cyberattacks. For example, these IoT devices can be infected with botnets to enable Distributed Denial of Service (DDoS) attacks. Signature-based intrusion detection systems are traditional countermeasures for such attacks. However, these methods rely on human experts and are time-consuming in terms of updates and may not exhaust all attack types especially zero-day attacks. Deep learning has shown some promise in intrusion detection. This paper shows that it is possible to use generative deep learning methods like Adversarial Autoencoders (AAE) and Bidirectional Generative Adversarial Networks (BiGAN) to detect intruders based on an analysis of the network data. The recently posted full IoT-23 dataset based on Somfy door lock, Philips Hue and Amazon Echo devices was used to train generative deep learning models to detect a variety of attacks like DDoS, and various botnets like Mirai, Okiruk and Torii. Over 1.8 million network flows were used to train the various models. The resulting generative models outperform traditional machine learning techniques like Random Forests. Both AAE and BiGAN-based models were able to achieve an F1-Score of 0.99. A BiGAN to detect unknown attacks was also trained to detect novel zero-day attacks with an F1-Score from 0.85 to 1.",
        "authors": [
            "Nada Abdalgawad",
            "Ali Reza Sajun",
            "Yara Kaddoura",
            "I. Zualkernan",
            "Fadi A. Aloul"
        ],
        "citations": 49,
        "references": 61,
        "year": 2022
    },
    {
        "title": "GraphGDP: Generative Diffusion Processes for Permutation Invariant Graph Generation",
        "abstract": "Graph generative models have broad applications in biology, chemistry and social science. However, modelling and understanding the generative process of graphs is challenging due to the discrete and high-dimensional nature of graphs, as well as permutation invariance to node orderings in underlying graph distributions. Current leading autoregressive models fail to capture the permutation invariance nature of graphs for the reliance on generation ordering and have high time complexity. Here, we propose a continuous-time generative diffusion process for permutation invariant graph generation to mitigate these issues. Specifically, we first construct a forward diffusion process defined by a stochastic differential equation (SDE), which smoothly converts graphs within the complex distribution to random graphs that follow a known edge probability. Solving the corresponding reverse-time SDE, graphs can be generated from newly sampled random graphs. To facilitate the reverse-time SDE, we newly design a position-enhanced graph score network, capturing the evolving structure and position information from perturbed graphs for permutation equivariant score estimation. Under the evaluation of comprehensive metrics, our proposed generative diffusion process achieves competitive performance in graph distribution learning. Experimental results also show that GraphGDP can generate high-quality graphs in only 24 function evaluations, much faster than previous autoregressive models.",
        "authors": [
            "Han Huang",
            "Leilei Sun",
            "Bowen Du",
            "Yanjie Fu",
            "Weifeng Lv"
        ],
        "citations": 38,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Generative Coarse-Graining of Molecular Conformations",
        "abstract": "Coarse-graining (CG) of molecular simulations simplifies the particle representation by grouping selected atoms into pseudo-beads and drastically accelerates simulation. However, such CG procedure induces information losses, which makes accurate backmapping, i.e., restoring fine-grained (FG) coordinates from CG coordinates, a long-standing challenge. Inspired by the recent progress in generative models and equivariant networks, we propose a novel model that rigorously embeds the vital probabilistic nature and geometric consistency requirements of the backmapping transformation. Our model encodes the FG uncertainties into an invariant latent space and decodes them back to FG geometries via equivariant convolutions. To standardize the evaluation of this domain, we provide three comprehensive benchmarks based on molecular dynamics trajectories. Experiments show that our approach always recovers more realistic structures and outperforms existing data-driven methods with a significant margin.",
        "authors": [
            "Wujie Wang",
            "Minkai Xu",
            "Chen Cai",
            "Benjamin Kurt Miller",
            "T. Smidt",
            "Yusu Wang",
            "Jian Tang",
            "R. G'omez-Bombarelli"
        ],
        "citations": 32,
        "references": 73,
        "year": 2022
    },
    {
        "title": "GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks",
        "abstract": "Time series synthesis is an important research topic in the field of deep learning, which can be used for data augmentation. Time series data types can be broadly classified into regular or irregular. However, there are no existing generative models that show good performance for both types without any model changes. Therefore, we present a general purpose model capable of synthesizing regular and irregular time series data. To our knowledge, we are the first designing a general purpose time series synthesis model, which is one of the most challenging settings for time series synthesis. To this end, we design a generative adversarial network-based method, where many related techniques are carefully integrated into a single framework, ranging from neural ordinary/controlled differential equations to continuous time-flow processes. Our method outperforms all existing methods.",
        "authors": [
            "Jinsung Jeon",
            "Jeonghak Kim",
            "Haryong Song",
            "Seunghyeon Cho",
            "Noseong Park"
        ],
        "citations": 31,
        "references": 43,
        "year": 2022
    },
    {
        "title": "GraphDE: A Generative Framework for Debiased Learning and Out-of-Distribution Detection on Graphs",
        "abstract": "Despite the remarkable success of graph neural networks (GNNs) for graph representation learning, they are generally built on the (unreliable) i.i.d. assumption across training and testing data. However, real-world graph data are universally comprised of outliers in training set and out-of-distribution (OOD) testing samples from unseen domains, which solicits effective models for i) debiased learning and ii) OOD detection, towards general trustworthy purpose. In this paper, we first mathematically formulate the two challenging problems for graph data and take an initiative on tackling them under a unified probabilistic model. Specifically, we model the graph generative process to characterize the distribution shifts of graph data together with an additionally introduced latent environment variable as an indicator. We then define a variational distribution, i.e., a recognition model, to infer the environment during training of GNN. By instantiating the generative models as two-component mixtures, we derive a tractable learning objective and theoretically justify that the model can i) automatically identify and down-weight outliers in the training procedure, and ii) induce an effective OOD detector simultaneously. Experiments on diverse datasets with different types of OOD data prove that our model consistently outperforms strong baselines for both debiasing and OOD detection tasks. The source code has been made publicly available at https://github.com/Emiyalzn/GraphDE .",
        "authors": [
            "Zenan Li",
            "Qitian Wu",
            "Fan Nie",
            "Junchi Yan"
        ],
        "citations": 33,
        "references": 62,
        "year": 2022
    },
    {
        "title": "DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model",
        "abstract": "Recent 3D generative models have achieved remarkable performance in synthesizing high resolution photorealistic images with view consistency and detailed 3D shapes, but training them for diverse domains is challenging since it requires massive training images and their camera distribution information. Text-guided domain adaptation methods have shown impressive performance on converting the 2D generative model on one domain into the models on other domains with different styles by leveraging the CLIP (Contrastive Language-Image Pre-training), rather than collecting massive datasets for those domains. However, one drawback of them is that the sample diversity in the original generative model is not well-preserved in the domain-adapted generative models due to the deterministic nature of the CLIP text encoder. Text-guided domain adaptation will be even more challenging for 3D generative models not only because of catastrophic diversity loss, but also because of inferior text-image correspondence and poor image quality. Here we propose DATID-3D, a domain adaptation method tailored for 3D generative models using text-to-image diffusion models that can synthesize diverse images per text prompt without collecting additional images and camera information for the target domain. Unlike 3D extensions of prior text-guided domain adaptation methods, our novel pipeline was able to fine-tune the state-of-the-art 3D generator of the source domain to synthesize high resolution, multi-view consistent images in text-guided targeted domains without additional data, outperforming the existing text-guided domain adaptation methods in diversity and text-image correspondence. Furthermore, we propose and demonstrate diverse 3D image manipulations such as one-shot instance-selected adaptation and single-view manipulated 3D reconstruction to fully enjoy diversity in text.",
        "authors": [
            "Gwanghyun Kim",
            "S. Chun"
        ],
        "citations": 33,
        "references": 87,
        "year": 2022
    },
    {
        "title": "Generative Adversarial Framework for Cold-Start Item Recommendation",
        "abstract": "The cold-start problem has been a long-standing issue in recommendation. Embedding-based recommendation models provide recommendations by learning embeddings for each user and item from historical interactions. Therefore, such embedding-based models perform badly for cold items which haven't emerged in the training set. The most common solutions are to generate the cold embedding for the cold item from its content features. However, the cold embeddings generated from contents have different distribution as the warm embeddings are learned from historical interactions. In this case, current cold-start methods are facing an interesting seesaw phenomenon, which improves the recommendation of either the cold items or the warm items but hurts the opposite ones. To this end, we propose a general framework named Generative Adversarial Recommendation (GAR). By training the generator and the recommender adversarially, the generated cold item embeddings can have similar distribution as the warm embeddings that can even fool the recommender. Simultaneously, the recommender is fine-tuned to correctly rank the \"fake'' warm embeddings and the real warm embeddings. Consequently, the recommendation of the warms and the colds will not influence each other, thus avoiding the seesaw phenomenon. Additionally, GAR could be applied to any off-the-shelf recommendation model. Experiments on two datasets present that GAR has strong overall recommendation performance in cold-starting both the CF-based model (improved by over 30.18%) and the GNN-based model (improved by over 17.78%).",
        "authors": [
            "Hao Chen",
            "Zefan Wang",
            "Feiran Huang",
            "Xiao Huang",
            "Yue Xu",
            "Yishi Lin",
            "Peng He",
            "Zhoujun Li"
        ],
        "citations": 49,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models",
        "abstract": "Zero-shot novel view synthesis (NVS) from a single image is an essential problem in 3D object understanding. While recent approaches that leverage pre-trained generative models can synthesize high-quality novel views from in-the-wild inputs, they still struggle to maintain 3D consistency across different views. In this paper, we present Consistent-1-to-3, which is a generative framework that significantly mitigates this issue. Specifically, we decompose the NVS task into two stages: (i) transforming observed regions to a novel view, and (ii) hallucinating unseen regions. We design a scene representation transformer and view-conditioned diffusion model for performing these two stages respectively. Inside the models, to enforce 3D consistency, we propose to employ epipolar-guided attention to incorporate geometry constraints, and multi-view attention to better aggregate multi-view information. Finally, we design a hierarchy generation paradigm to generate long sequences of consistent views, allowing a full 360° observation of the provided object image. Qualitative and quantitative evaluation over multiple datasets demonstrates the effectiveness of the proposed mechanisms against state-of-the-art approaches. Our project page is at https://jianglongye.com/consistent123/.",
        "authors": [
            "Jianglong Ye",
            "Peng Wang",
            "Kejie Li",
            "Yichun Shi",
            "Heng Wang"
        ],
        "citations": 63,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
        "abstract": "Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call\"EmotionPrompt\"that combines the original prompt with emotional stimuli), e.g., 8.00% relative performance improvement in Instruction Induction and 115% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.",
        "authors": [
            "Cheng Li",
            "Jindong Wang",
            "Kaijie Zhu",
            "Yixuan Zhang",
            "Wenxin Hou",
            "Jianxun Lian",
            "Xingxu Xie"
        ],
        "citations": 87,
        "references": 40,
        "year": 2023
    },
    {
        "title": "A Generative Approach to Materials Discovery, Design, and Optimization",
        "abstract": "Despite its potential to transform society, materials research suffers from a major drawback: its long research timeline. Recently, machine-learning techniques have emerged as a viable solution to this drawback and have shown accuracies comparable to other computational techniques like density functional theory (DFT) at a fraction of the computational time. One particular class of machine-learning models, known as “generative models”, is of particular interest owing to its ability to approximate high-dimensional probability distribution functions, which in turn can be used to generate novel data such as molecular structures by sampling these approximated probability distribution functions. This review article aims to provide an in-depth understanding of the underlying mathematical principles of popular generative models such as recurrent neural networks, variational autoencoders, and generative adversarial networks and discuss their state-of-the-art applications in the domains of biomaterials and organic drug-like materials, energy materials, and structural materials. Here, we discuss a broad range of applications of these models spanning from the discovery of drugs that treat cancer to finding the first room-temperature superconductor and from the discovery and optimization of battery and photovoltaic materials to the optimization of high-entropy alloys. We conclude by presenting a brief outlook of the major challenges that lie ahead for the mainstream usage of these models for materials research.",
        "authors": [
            "Dhruv Menon",
            "R. Ranganathan"
        ],
        "citations": 33,
        "references": 137,
        "year": 2022
    },
    {
        "title": "Structural Pruning for Diffusion Models",
        "abstract": "Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across several datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50\\% reduction in FLOPs at a mere 10\\% to 20\\% of the original training expenditure; 2) Consistency: the pruned diffusion models inherently preserve generative behavior congruent with their pre-trained models. Code is available at \\url{https://github.com/VainF/Diff-Pruning}.",
        "authors": [
            "Gongfan Fang",
            "Xinyin Ma",
            "Xinchao Wang"
        ],
        "citations": 85,
        "references": 60,
        "year": 2023
    },
    {
        "title": "A Recipe for Watermarking Diffusion Models",
        "abstract": "Diffusion models (DMs) have demonstrated advantageous potential on generative tasks. Widespread interest exists in incorporating DMs into downstream applications, such as producing or editing photorealistic images. However, practical deployment and unprecedented power of DMs raise legal issues, including copyright protection and monitoring of generated content. In this regard, watermarking has been a proven solution for copyright protection and content monitoring, but it is underexplored in the DMs literature. Specifically, DMs generate samples from longer tracks and may have newly designed multimodal structures, necessitating the modification of conventional watermarking pipelines. To this end, we conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe is straightforward but involves empirically ablated implementation details, providing a foundation for future research on watermarking DMs. The code is available at https://github.com/yunqing-me/WatermarkDM.",
        "authors": [
            "Yunqing Zhao",
            "Tianyu Pang",
            "Chao Du",
            "Xiao Yang",
            "Ngai-Man Cheung",
            "Min Lin"
        ],
        "citations": 86,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Understanding Diffusion Models: A Unified Perspective",
        "abstract": "Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.",
        "authors": [
            "Calvin Luo"
        ],
        "citations": 259,
        "references": 27,
        "year": 2022
    },
    {
        "title": "Analysing Diffusion-based Generative Approaches Versus Discriminative Approaches for Speech Restoration",
        "abstract": "Diffusion-based generative models have had a high impact on the computer vision and speech processing communities these past years. Besides data generation tasks, they have also been employed for data restoration tasks like speech enhancement and dereverberation. While discriminative models have traditionally been argued to be more powerful e.g. for speech enhancement, generative diffusion approaches have recently been shown to narrow this performance gap considerably. In this paper, we systematically compare the performance of generative diffusion models and discriminative approaches on different speech restoration tasks. For this, we extend our prior contributions on diffusion-based speech enhancement in the complex time-frequency domain to the task of bandwith extension. We then compare it to a discriminatively trained neural network with the same network architecture on three restoration tasks, namely speech denoising, dereverberation and bandwidth extension. We observe that the generative approach performs globally better than its discriminative counterpart on all tasks, with the strongest benefit for non-additive distortion models, like in dereverberation and bandwidth extension. Code and audio examples can be found online1.",
        "authors": [
            "Jean-Marie Lemercier",
            "Julius Richter",
            "Simon Welker",
            "Timo Gerkmann"
        ],
        "citations": 29,
        "references": 40,
        "year": 2022
    },
    {
        "title": "GERE: Generative Evidence Retrieval for Fact Verification",
        "abstract": "Fact verification (FV) is a challenging task which aims to verify a claim using multiple evidential sentences from trustworthy corpora, e.g., Wikipedia. Most existing approaches follow a three-step pipeline framework, including document retrieval, sentence retrieval and claim verification. High-quality evidences provided by the first two steps are the foundation of the effective reasoning in the last step. Despite being important, high-quality evidences are rarely studied by existing works for FV, which often adopt the off-the-shelf models to retrieve relevant documents and sentences in an \"index-retrieve-then-rank'' fashion. This classical approach has clear drawbacks as follows: i) a large document index as well as a complicated search process is required, leading to considerable memory and computational overhead; ii) independent scoring paradigms fail to capture the interactions among documents and sentences in ranking; iii) a fixed number of sentences are selected to form the final evidence set. In this work, we proposeGERE, the first system that retrieves evidences in a generative fashion, i.e., generating the document titles as well as evidence sentence identifiers. This enables us to mitigate the aforementioned technical issues since: i) the memory and computational cost is greatly reduced because the document index is eliminated and the heavy ranking process is replaced by a light generative process; ii) the dependency between documents and that between sentences could be captured via sequential generation process; iii) the generative formulation allows us to dynamically select a precise set of relevant evidences for each claim. The experimental results on the FEVER dataset show that GERE achieves significant improvements over the state-of-the-art baselines, with both time-efficiency and memory-efficiency.",
        "authors": [
            "Jiangui Chen",
            "Ruqing Zhang",
            "J. Guo",
            "Yixing Fan",
            "Xueqi Cheng"
        ],
        "citations": 50,
        "references": 38,
        "year": 2022
    },
    {
        "title": "CLEAR: Generative Counterfactual Explanations on Graphs",
        "abstract": "Counterfactual explanations promote explainability in machine learning models by answering the question\"how should an input instance be perturbed to obtain a desired predicted label?\". The comparison of this instance before and after perturbation can enhance human interpretation. Most existing studies on counterfactual explanations are limited in tabular data or image data. In this work, we study the problem of counterfactual explanation generation on graphs. A few studies have explored counterfactual explanations on graphs, but many challenges of this problem are still not well-addressed: 1) optimizing in the discrete and disorganized space of graphs; 2) generalizing on unseen graphs; and 3) maintaining the causality in the generated counterfactuals without prior knowledge of the causal model. To tackle these challenges, we propose a novel framework CLEAR which aims to generate counterfactual explanations on graphs for graph-level prediction models. Specifically, CLEAR leverages a graph variational autoencoder based mechanism to facilitate its optimization and generalization, and promotes causality by leveraging an auxiliary variable to better identify the underlying causal model. Extensive experiments on both synthetic and real-world graphs validate the superiority of CLEAR over the state-of-the-art methods in different aspects.",
        "authors": [
            "Jing Ma",
            "Ruocheng Guo",
            "Saumitra Mishra",
            "Aidong Zhang",
            "Jundong Li"
        ],
        "citations": 47,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Reading and Writing: Discriminative and Generative Modeling for Self-Supervised Text Recognition",
        "abstract": "Existing text recognition methods usually need large-scale training data. Most of them rely on synthetic training data due to the lack of annotated real images. However, there is a domain gap between the synthetic data and real data, which limits the performance of the text recognition models. Recent self-supervised text recognition methods attempted to utilize unlabeled real images by introducing contrastive learning, which mainly learns the discrimination of the text images. Inspired by the observation that humans learn to recognize the texts through both reading and writing, we propose to learn discrimination and generation by integrating contrastive learning and masked image modeling in our self-supervised method. The contrastive learning branch is adopted to learn the discrimination of text images, which imitates the reading behavior of humans. Meanwhile, masked image modeling is firstly introduced for text recognition to learn the context generation of the text images, which is similar to the writing behavior. The experimental results show that our method outperforms previous self-supervised text recognition methods by 10.2%-20.2% on irregular scene text recognition datasets. Moreover, our proposed text recognizer exceeds previous state-of-the-art text recognition methods by averagely 5.3% on 11benchmarks, with similar model size. We also demonstrate that our pre-trained model can be easily applied to other text-related tasks with obvious performance gain.",
        "authors": [
            "Mingkun Yang",
            "Minghui Liao",
            "Pu Lu",
            "Jing Wang",
            "Shenggao Zhu",
            "Hualin Luo",
            "Qingzhen Tian",
            "X. Bai"
        ],
        "citations": 48,
        "references": 71,
        "year": 2022
    },
    {
        "title": "ObjectStitch: Generative Object Compositing",
        "abstract": "Object compositing based on 2D images is a challenging problem since it typically involves multiple processing stages such as color harmonization, geometry correction and shadow generation to generate realistic results. Furthermore, annotating training data pairs for compositing requires substantial manual effort from professionals, and is hardly scalable. Thus, with the recent advances in generative models, in this work, we propose a self-supervised framework for object compositing by leveraging the power of conditional diffusion models. Our framework can hollistically address the object compositing task in a unified model, transforming the viewpoint, geometry, color and shadow of the generated object while requiring no manual labeling. To preserve the input object's characteristics, we introduce a content adaptor that helps to maintain categorical semantics and object appearance. A data augmentation method is further adopted to improve the fidelity of the generator. Our method outperforms relevant baselines in both realism and faithfulness of the synthesized result images in a user study on various real-world images.",
        "authors": [
            "Yi-Zhe Song",
            "Zhifei Zhang",
            "Zhe Lin",
            "Scott D. Cohen",
            "Brian L. Price",
            "Jianming Zhang",
            "S. Kim",
            "Daniel G. Aliaga"
        ],
        "citations": 28,
        "references": 52,
        "year": 2022
    },
    {
        "title": "CLIP-Mesh: Generating textured meshes from text using pretrained image-text models",
        "abstract": "We present a technique for zero-shot generation of a 3D model using only a target text prompt. Without any 3D supervision our method deforms the control shape of a limit subdivided surface along with its texture map and normal map to obtain a 3D asset that corresponds to the input text prompt and can be easily deployed into games or modeling applications. We rely only on a pre-trained CLIP model that compares the input text prompt with differentiably rendered images of our 3D model. While previous works have focused on stylization or required training of generative models we perform optimization on mesh parameters directly to generate shape, texture or both. To constrain the optimization to produce plausible meshes and textures we introduce a number of techniques using image augmentations and the use of a pretrained prior that generates CLIP image embeddings given a text embedding.",
        "authors": [
            "N. Khalid",
            "Tianhao Xie",
            "Eugene Belilovsky",
            "T. Popa"
        ],
        "citations": 270,
        "references": 26,
        "year": 2022
    },
    {
        "title": "Generative Consistency for Semi-Supervised Cerebrovascular Segmentation From TOF-MRA",
        "abstract": "Cerebrovascular segmentation from Time-of-flight magnetic resonance angiography (TOF-MRA) is a critical step in computer-aided diagnosis. In recent years, deep learning models have proved its powerful feature extraction for cerebrovascular segmentation. However, they require many labeled datasets to implement effective driving, which are expensive and professional. In this paper, we propose a generative consistency for semi-supervised (GCS) model. Considering the rich information contained in the feature map, the GCS model utilizes the generation results to constrain the segmentation model. The generated data comes from labeled data, unlabeled data, and unlabeled data after perturbation, respectively. The GCS model also calculates the consistency of the perturbed data to improve the feature mining ability. Subsequently, we propose a new model as the backbone of the GSC model. It transfers TOF-MRA into graph space and establishes correlation using Transformer. We demonstrated the effectiveness of the proposed model on TOF-MRA representations, and tested the GCS model with state-of-the-art semi-supervised methods using the proposed model as backbone. The experiments prove the important role of the GCS model in cerebrovascular segmentation. Code is available at https://github.com/MontaEllis/SSL-For-Medical-Segmentation.",
        "authors": [
            "Cheng Chen",
            "Kangneng Zhou",
            "Zhiliang Wang",
            "Ruoxiu Xiao"
        ],
        "citations": 44,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Neural scaling of deep chemical models",
        "abstract": null,
        "authors": [
            "Nathan C Frey",
            "Ryan Soklaski",
            "Simon Axelrod",
            "S. Samsi",
            "Rafael G´omez-Bombarelli",
            "Connor W. Coley",
            "V. Gadepally"
        ],
        "citations": 79,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Text-to-Image Diffusion Models are Zero-Shot Classifiers",
        "abstract": "The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data. However, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks. We investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers. The key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood. We apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image classification datasets. Additionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot. Although generative pre-training is prevalent in NLP, visual foundation models often use other methods such as contrastive learning. Based on our findings, we argue that generative pre-training should be explored as a compelling alternative for vision-language tasks.",
        "authors": [
            "Kevin Clark",
            "P. Jaini"
        ],
        "citations": 81,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Deep Generative Model for Periodic Graphs",
        "abstract": "Periodic graphs are graphs consisting of repetitive local structures, such as crystal nets and polygon mesh. Their generative modeling has great potential in real-world applications such as material design and graphics synthesis. Classical models either rely on domain-specific predefined generation principles (e.g., in crystal net design), or follow geometry-based prescribed rules. Recently, deep generative models has shown great promise in automatically generating general graphs. However, their advancement into periodic graphs have not been well explored due to several key challenges in 1) maintaining graph periodicity; 2) disentangling local and global patterns; and 3) efficiency in learning repetitive patterns. To address them, this paper proposes Periodical-Graph Disentangled Variational Auto-encoder (PGD-VAE), a new deep generative models for periodic graphs that can automatically learn, disentangle, and generate local and global graph patterns. Specifically, we develop a new periodic graph encoder consisting of global-pattern encoder and local-pattern encoder that ensures to disentangle the representation into global and local semantics. We then propose a new periodic graph decoder consisting of local structure decoder, neighborhood decoder, and global structure decoder, as well as the assembler of their outputs that guarantees periodicity. Moreover, we design a new model learning objective that helps ensure the invariance of local-semantic representations for the graphs with the same local structure. Comprehensive experimental evaluations have been conducted to demonstrate the effectiveness of the proposed method. The code of proposed PGD-VAE is availabe at https://github.com/shi-yu-wang/PGD-VAE.",
        "authors": [
            "Shiyu Wang",
            "Xiaojie Guo",
            "Liang Zhao"
        ],
        "citations": 27,
        "references": 81,
        "year": 2022
    },
    {
        "title": "Deep generative model with time series-image encoding for manufacturing fault detection in die casting process",
        "abstract": null,
        "authors": [
            "Jiyoung Song",
            "Young Chul Lee",
            "Jeongsu Lee"
        ],
        "citations": 25,
        "references": 53,
        "year": 2022
    },
    {
        "title": "Designing optimized drug candidates with Generative Adversarial Network",
        "abstract": null,
        "authors": [
            "Maryam Abbasi",
            "Beatriz P. Santos",
            "Tiago O. Pereira",
            "Raul Sofia",
            "Nelson R. C. Monteiro",
            "Carlos J. V. Simões",
            "Rui M. M. Brito",
            "B. Ribeiro",
            "J. L. Oliveira",
            "Joel P. Arrais"
        ],
        "citations": 41,
        "references": 49,
        "year": 2022
    },
    {
        "title": "Generative Domain Adaptation for Face Anti-Spoofing",
        "abstract": null,
        "authors": [
            "Qianyu Zhou",
            "Ke-Yue Zhang",
            "Taiping Yao",
            "Ran Yi",
            "Kekai Sheng",
            "Shouhong Ding",
            "Lizhuang Ma"
        ],
        "citations": 43,
        "references": 114,
        "year": 2022
    },
    {
        "title": "Discriminative and Generative Learning for Linear Estimation of Random Signals [Lecture Notes]",
        "abstract": "Inference tasks in signal processing are often characterized by the availability of reliable statistical modeling with some missing instance-specific parameters. One conventional approach uses data to estimate these missing parameters and then infers based on the estimated model. Alternatively, data can also be leveraged to directly learn the inference mapping end-to-end. These approaches for combining partially-known statistical models and data in inference are related to the notions of generative and discriminative models used in the machine learning literature, typically considered in the context of classifiers. The goal of this lecture note is to introduce the concepts of generative and discriminative learning for inference with a partially-known statistical model. While machine learning systems often lack the interpretability of traditional signal processing methods, we focus on a simple setting where one can interpret and compare the approaches in a tractable manner that is accessible and relevant to signal processing readers. In particular, we exemplify the approaches for the task of Bayesian signal estimation in a jointly Gaussian setting with the mean-squared error (MSE) objective, i.e., a linear estimation setting.",
        "authors": [
            "Nir Shlezinger",
            "T. Routtenberg"
        ],
        "citations": 36,
        "references": 12,
        "year": 2022
    },
    {
        "title": "AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners",
        "abstract": "Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For example, AdaptDiffuser not only outperforms the previous art Diffuser by 20.8% on Maze2D and 7.5% on MuJoCo locomotion, but also adapts better to new tasks, e.g., KUKA pick-and-place, by 27.9% without requiring additional expert data. More visualization results and demo videos could be found on our project page.",
        "authors": [
            "Zhixuan Liang",
            "Yao Mu",
            "Mingyu Ding",
            "Fei Ni",
            "M. Tomizuka",
            "Ping Luo"
        ],
        "citations": 73,
        "references": 69,
        "year": 2023
    },
    {
        "title": "A Mathematical Investigation of Hallucination and Creativity in GPT Models",
        "abstract": "In this paper, we present a comprehensive mathematical analysis of the hallucination phenomenon in generative pretrained transformer (GPT) models. We rigorously define and measure hallucination and creativity using concepts from probability theory and information theory. By introducing a parametric family of GPT models, we characterize the trade-off between hallucination and creativity and identify an optimal balance that maximizes model performance across various tasks. Our work offers a novel mathematical framework for understanding the origins and implications of hallucination in GPT models and paves the way for future research and development in the field of large language models (LLMs).",
        "authors": [
            "Minhyeok Lee"
        ],
        "citations": 70,
        "references": 18,
        "year": 2023
    },
    {
        "title": "A Survey on Video Diffusion Models",
        "abstract": "The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.",
        "authors": [
            "Zhen Xing",
            "Qijun Feng",
            "Haoran Chen",
            "Qi Dai",
            "Hang-Rui Hu",
            "Hang Xu",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "citations": 75,
        "references": 320,
        "year": 2023
    },
    {
        "title": "LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation",
        "abstract": "Layout-to-image generation refers to the task of synthesizing photo-realistic images based on semantic layouts. In this paper, we propose LayoutDiffuse that adapts a foundational diffusion model pretrained on large-scale image or text-image datasets for layout-to-image generation. By adopting a novel neural adaptor based on layout attention and task-aware prompts, our method trains efficiently, generates images with both high perceptual quality and layout alignment, and needs less data. Experiments on three datasets show that our method significantly outperforms other 10 generative models based on GANs, VQ-VAE, and diffusion models.",
        "authors": [
            "Jiaxin Cheng",
            "Xiao Liang",
            "Xingjian Shi",
            "Tong He",
            "Tianjun Xiao",
            "Mu Li"
        ],
        "citations": 54,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics",
        "abstract": "Coarse-grained (CG) molecular dynamics enables the study of biological processes at temporal and spatial scales that would be intractable at an atomistic resolution. However, accurately learning a CG force field remains a challenge. In this work, we leverage connections between score-based generative models, force fields, and molecular dynamics to learn a CG force field without requiring any force inputs during training. Specifically, we train a diffusion generative model on protein structures from molecular dynamics simulations, and we show that its score function approximates a force field that can directly be used to simulate CG molecular dynamics. While having a vastly simplified training setup compared to previous work, we demonstrate that our approach leads to improved performance across several protein simulations for systems up to 56 amino acids, reproducing the CG equilibrium distribution and preserving the dynamics of all-atom simulations such as protein folding events.",
        "authors": [
            "Marloes Arts",
            "Victor Garcia Satorras",
            "Chin-Wei Huang",
            "Daniel Zuegner",
            "M. Federici",
            "C. Clementi",
            "Frank No'e",
            "Robert Pinsler",
            "Rianne van den Berg"
        ],
        "citations": 66,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Medical domain knowledge in domain-agnostic generative AI",
        "abstract": null,
        "authors": [
            "Jakob Nikolas Kather",
            "Narmin Ghaffari Laleh",
            "S. Foersch",
            "D. Truhn"
        ],
        "citations": 35,
        "references": 13,
        "year": 2022
    },
    {
        "title": "A high-level programming language for generative protein design",
        "abstract": "Combining a basic set of building blocks into more complex forms is a universal design principle. Most protein designs have proceeded from a manual bottom-up approach using parts created by nature, but top-down design of proteins is fundamentally hard due to biological complexity. We demonstrate how the modularity and programmability long sought for protein design can be realized through generative artificial intelligence. Advanced protein language models demonstrate emergent learning of atomic resolution structure and protein design principles. We leverage these developments to enable the programmable design of de novo protein sequences and structures of high complexity. First, we describe a high-level programming language based on modular building blocks that allows a designer to easily compose a set of desired properties. We then develop an energy-based generative model, built on atomic resolution structure prediction with a language model, that realizes all-atom structure designs that have the programmed properties. Designing a diverse set of specifications, including constraints on atomic coordinates, secondary structure, symmetry, and multimerization, demonstrates the generality and controllability of the approach. Enumerating constraints at increasing levels of hierarchical complexity shows that the approach can access a combinatorially large design space.",
        "authors": [
            "B. Hie",
            "Salvatore Candido",
            "Zeming Lin",
            "Ori Kabeli",
            "Roshan Rao",
            "Nikita Smetanin",
            "Tom Sercu",
            "Alexander Rives"
        ],
        "citations": 35,
        "references": 46,
        "year": 2022
    },
    {
        "title": "An Energy-Based Generative Adversarial Forecaster for Radar Echo Map Extrapolation",
        "abstract": "Precipitation nowcasting is an important task in weather forecast. The key challenge of the task lies at radar echo map extrapolation. Recent studies show that a convolutional recurrent neural network (ConvRNN) is a promising direction to solve the problem. However, the extrapolation results of the existing ConvRNN methods tend to be blurring and unrealistic. Recent studies show that generative adversarial network (GAN) is a promising tool to address the drawback, while it suffers from the instability for training. In this letter, we build a novel ConvRNN model based on the energy-based GAN for radar echo map extrapolation. The method can alleviate the blurring and unrealistic issues and is more stable. We have conducted experiments on a real-world data set, and the results show that the proposed method outperforms several existing models, including optical flow, convolution gated recurrent unit (ConvGRU), and generative adversarial ConvGRU (GA-ConvGRU).",
        "authors": [
            "Pengfei Xie",
            "Xutao Li",
            "Xiyang Ji",
            "Xunlai Chen",
            "Yuanzhao Chen",
            "Jia Liu",
            "Yunming Ye"
        ],
        "citations": 31,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Exploring Generative Neural Temporal Point Process",
        "abstract": "Temporal point process (TPP) is commonly used to model the asynchronous event sequence featuring occurrence timestamps and revealed by probabilistic models conditioned on historical impacts. While lots of previous works have focused on `goodness-of-fit' of TPP models by maximizing the likelihood, their predictive performance is unsatisfactory, which means the timestamps generated by models are far apart from true observations. Recently, deep generative models such as denoising diffusion and score matching models have achieved great progress in image generating tasks by demonstrating their capability of generating samples of high quality. However, there are no complete and unified works exploring and studying the potential of generative models in the context of event occurence modeling for TPP. In this work, we try to fill the gap by designing a unified \\textbf{g}enerative framework for \\textbf{n}eural \\textbf{t}emporal \\textbf{p}oint \\textbf{p}rocess (\\textsc{GNTPP}) model to explore their feasibility and effectiveness, and further improve models' predictive performance. Besides, in terms of measuring the historical impacts, we revise the attentive models which summarize influence from historical events with an adaptive reweighting term considering events' type relation and time intervals. Extensive experiments have been conducted to illustrate the improved predictive capability of \\textsc{GNTPP} with a line of generative probabilistic decoders, and performance gain from the revised attention. To the best of our knowledge, this is the first work that adapts generative models in a complete unified framework and studies their effectiveness in the context of TPP. Our codebase including all the methods given in Section.5.1.1 is open in \\url{https://github.com/BIRD-TAO/GNTPP}. We hope the code framework can facilitate future research in Neural TPPs.",
        "authors": [
            "Haitao Lin",
            "Lirong Wu",
            "Guojiang Zhao",
            "Pai Liu",
            "Stan Z. Li"
        ],
        "citations": 23,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Pythae: Unifying Generative Autoencoders in Python - A Benchmarking Use Case",
        "abstract": "In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions. Among those models, variational autoencoders have gained popularity as they have proven both to be computationally efficient and yield impressive results in multiple fields. Following this breakthrough, extensive research has been done in order to improve the original publication, resulting in a variety of different VAE models in response to different tasks. In this paper we present Pythae, a versatile open-source Python library providing both a unified implementation and a dedicated framework allowing straightforward, reproducible and reliable use of generative autoencoder models. We then propose to use this library to perform a case study benchmark where we present and compare 19 generative autoencoder models representative of some of the main improvements on downstream tasks such as image reconstruction, generation, classification, clustering and interpolation. The open-source library can be found at https://github.com/clementchadebec/benchmark_VAE.",
        "authors": [
            "Clément Chadebec",
            "Louis J. Vincent",
            "S. Allassonnière"
        ],
        "citations": 23,
        "references": 78,
        "year": 2022
    },
    {
        "title": "Accelerating material design with the generative toolkit for scientific discovery",
        "abstract": null,
        "authors": [
            "M. Manica",
            "Jannis Born",
            "Joris Cadow",
            "Dimitrios Christofidellis",
            "A. Dave",
            "D. Clarke",
            "Yves Gaëtan Nana Teukam",
            "Giorgio Giannone",
            "Samuel C. Hoffman",
            "Matthew Buchan",
            "V. Chenthamarakshan",
            "Timothy Donovan",
            "Hsiang-Han Hsu",
            "F. Zipoli",
            "Oliver Schilter",
            "Akihiro Kishimoto",
            "Lisa Hamada",
            "Inkit Padhi",
            "Karl Wehden",
            "Lauren N. McHugh",
            "Alexy Khrabrov",
            "Payel Das",
            "Seiji Takeda",
            "John Smith"
        ],
        "citations": 19,
        "references": 62,
        "year": 2022
    },
    {
        "title": "Medical image captioning via generative pretrained transformers",
        "abstract": null,
        "authors": [
            "Alexander Selivanov",
            "O. Rogov",
            "Daniil Chesakov",
            "Artem Shelmanov",
            "Irina Fedulova",
            "D. Dylov"
        ],
        "citations": 44,
        "references": 71,
        "year": 2022
    },
    {
        "title": "A Variational Perspective on Generative Flow Networks",
        "abstract": "Generative flow networks (GFNs) are a class of models for sequential sampling of composite objects, which approximate a target distribution that is defined in terms of an energy function or a reward. GFNs are typically trained using a flow matching or trajectory balance objective, which matches forward and backward transition models over trajectories. In this work, we define variational objectives for GFNs in terms of the Kullback-Leibler (KL) divergences between the forward and backward distribution. We show that variational inference in GFNs is equivalent to minimizing the trajectory balance objective when sampling trajectories from the forward model. We generalize this approach by optimizing a convex combination of the reverse- and forward KL divergence. This insight suggests variational inference methods can serve as a means to define a more general family of objectives for training generative flow networks, for example by incorporating control variates, which are commonly used in variational inference, to reduce the variance of the gradients of the trajectory balance objective. We evaluate our findings and the performance of the proposed variational objective numerically by comparing it to the trajectory balance objective on two synthetic tasks.",
        "authors": [
            "Heiko Zimmermann",
            "F. Lindsten",
            "Jan-Willem van de Meent",
            "C. A. Naesseth"
        ],
        "citations": 28,
        "references": 37,
        "year": 2022
    },
    {
        "title": "Streaming Graph Neural Networks with Generative Replay",
        "abstract": "Training Graph Neural Networks (GNNs) incrementally is a particularly urgent problem, because real-world graph data usually arrives in a streaming fashion, and inefficiently updating of the models results in out-of-date embeddings, thus degrade its performance in downstream tasks. Traditional incremental learning methods will gradually forget old knowledge when learning new patterns, which is the catastrophic forgetting problem. Although saving and revisiting historical graph data alleviates the problem, the storage limitation in real-world applications reduces the amount of saved data, causing GNN to forget other knowledge. In this paper, we propose a streaming GNN based on generative replay, which can incrementally learn new patterns while maintaining existing knowledge without accessing historical data. Specifically, our model consists of the main model (GNN) and an auxiliary generative model. The generative model based on random walks with restart can learn and generate fake historical samples (i.e., nodes and their neighborhoods), which can be trained with real data to avoid the forgetting problem. Besides, we also design an incremental update algorithm for the generative model to maintain the graph distribution and for GNN to capture the current patterns. Our model is evaluated on different streaming data sets. The node classification results prove that our model can update the model efficiently and achieve comparable performance to model retraining. Code is available at https://github.com/Junshan-Wang/SGNN-GR.",
        "authors": [
            "Junshan Wang",
            "Wenhao Zhu",
            "Guojie Song",
            "Liang Wang"
        ],
        "citations": 28,
        "references": 45,
        "year": 2022
    },
    {
        "title": "TIGGER: Scalable Generative Modelling for Temporal Interaction Graphs",
        "abstract": "There has been a recent surge in learning generative models for graphs. While impressive progress has been made on static graphs, work on generative modeling of temporal graphs is at a nascent stage with significant scope for improvement. First, existing generative models do not scale with either the time horizon or the number of nodes. Second, existing techniques are transductive in nature and thus do not facilitate knowledge transfer. Finally, due to relying on one-to-one node mapping from source to the generated graph, existing models leak node identity information and do not allow up-scaling/down-scaling the source graph size. In this paper, we bridge these gaps with a novel generative model called TIGGER. TIGGER derives its power through a combination of temporal point processes with auto-regressive modeling enabling both transductive and inductive variants. Through extensive experiments on real datasets, we establish TIGGER generates graphs of superior fidelity, while also being up to 3 orders of magnitude faster than the state-of-the-art.",
        "authors": [
            "Shubham Gupta",
            "S. Manchanda",
            "Srikanta J. Bedathur",
            "Sayan Ranu"
        ],
        "citations": 13,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Emergent analogical reasoning in large language models",
        "abstract": null,
        "authors": [
            "Taylor W. Webb",
            "K. Holyoak",
            "Hongjing Lu"
        ],
        "citations": 248,
        "references": 93,
        "year": 2022
    },
    {
        "title": "Leveraging molecular structure and bioactivity with chemical language models for de novo drug design",
        "abstract": null,
        "authors": [
            "Michael Moret",
            "Irene Pachon Angona",
            "Leandro Cotos",
            "Shen Yan",
            "Kenneth Atz",
            "Cyrill Brunner",
            "M. Baumgartner",
            "F. Grisoni",
            "G. Schneider"
        ],
        "citations": 80,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models",
        "abstract": "This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.",
        "authors": [
            "Sami Sarsa",
            "Paul Denny",
            "Arto Hellas",
            "Juho Leinonen"
        ],
        "citations": 276,
        "references": 103,
        "year": 2022
    },
    {
        "title": "Wordcraft: Story Writing With Large Language Models",
        "abstract": "The latest generation of large neural language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work, we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a generative language model to write a story. We evaluated Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences. For example, the language model is able to engage in open-ended conversation about the story, respond to writers’ custom requests expressed in natural language (such as ”rewrite this text to be more Dickensian”), and generate suggestions that serve to unblock writers in the creative process. Based on these results, we discuss design implications for future human-AI co-writing systems.",
        "authors": [
            "Ann Yuan",
            "Andy Coenen",
            "Emily Reif",
            "Daphne Ippolito"
        ],
        "citations": 240,
        "references": 47,
        "year": 2022
    },
    {
        "title": "State of the Art on Diffusion Models for Visual Computing",
        "abstract": "The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion‐based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state‐of‐the‐art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion‐based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.",
        "authors": [
            "Ryan Po",
            "Wang Yifan",
            "Vladislav Golyanik",
            "Kfir Aberman",
            "J. Barron",
            "Amit H. Bermano",
            "Eric R. Chan",
            "Tali Dekel",
            "Aleksander Holynski",
            "Angjoo Kanazawa",
            "C. K. Liu",
            "Lingjie Liu",
            "B. Mildenhall",
            "M. Nießner",
            "Bjorn Ommer",
            "C. Theobalt",
            "Peter Wonka",
            "Gordon Wetzstein"
        ],
        "citations": 78,
        "references": 340,
        "year": 2023
    },
    {
        "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
        "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.",
        "authors": [
            "Peng Li",
            "Tianxiang Sun",
            "Qiong Tang",
            "Hang Yan",
            "Yuanbin Wu",
            "Xuanjing Huang",
            "Xipeng Qiu Academy for EngineeringTechnology",
            "Fudan University",
            "School of Materials Science",
            "Technology",
            "East China Normal University"
        ],
        "citations": 61,
        "references": 44,
        "year": 2023
    },
    {
        "title": "The Impact of Multimodal Large Language Models on Health Care’s Future",
        "abstract": "When large language models (LLMs) were introduced to the public at large in late 2022 with ChatGPT (OpenAI), the interest was unprecedented, with more than 1 billion unique users within 90 days. Until the introduction of Generative Pre-trained Transformer 4 (GPT-4) in March 2023, these LLMs only contained a single mode—text. As medicine is a multimodal discipline, the potential future versions of LLMs that can handle multimodality—meaning that they could interpret and generate not only text but also images, videos, sound, and even comprehensive documents—can be conceptualized as a significant evolution in the field of artificial intelligence (AI). This paper zooms in on the new potential of generative AI, a new form of AI that also includes tools such as LLMs, through the achievement of multimodal inputs of text, images, and speech on health care’s future. We present several futuristic scenarios to illustrate the potential path forward as multimodal LLMs (M-LLMs) could represent the gateway between health care professionals and using AI for medical purposes. It is important to point out, though, that despite the unprecedented potential of generative AI in the form of M-LLMs, the human touch in medicine remains irreplaceable. AI should be seen as a tool that can augment health care professionals rather than replace them. It is also important to consider the human aspects of health care—empathy, understanding, and the doctor-patient relationship—when deploying AI.",
        "authors": [
            "B. Meskó"
        ],
        "citations": 70,
        "references": 8,
        "year": 2023
    },
    {
        "title": "Adversarial Prompting for Black Box Foundation Models",
        "abstract": "Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to signiﬁcant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce speciﬁc behaviors into the generative process, such as generating images of a particular object or biasing the frequency of speciﬁc letters in the generated text.",
        "authors": [
            "N. Maus",
            "Patrick Chao",
            "Eric Wong",
            "Jacob R. Gardner"
        ],
        "citations": 45,
        "references": 64,
        "year": 2023
    },
    {
        "title": "TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation",
        "abstract": "Denoising Diffusion models have demonstrated their proficiency for generative sampling. However, generating good samples often requires many iterations. Consequently, techniques such as binary time-distillation (BTD) have been proposed to reduce the number of network calls for a fixed architecture. In this paper, we introduce TRAnsitive Closure Time-distillation (TRACT), a new method that extends BTD. For single step diffusion,TRACT improves FID by up to 2.4x on the same architecture, and achieves new single-step Denoising Diffusion Implicit Models (DDIM) state-of-the-art FID (7.4 for ImageNet64, 3.8 for CIFAR10). Finally we tease apart the method through extended ablations. The PyTorch implementation will be released soon.",
        "authors": [
            "David Berthelot",
            "Arnaud Autef",
            "Jierui Lin",
            "Dian Ang Yap",
            "Shuangfei Zhai",
            "Siyuan Hu",
            "Daniel Zheng",
            "Walter Talbot",
            "Eric Gu"
        ],
        "citations": 60,
        "references": 51,
        "year": 2023
    },
    {
        "title": "TabDDPM: Modelling Tabular Data with Diffusion Models",
        "abstract": "Denoising diffusion probabilistic models are currently becoming the leading paradigm of generative modeling for many important data modalities. Being the most prevalent in the computer vision community, diffusion models have also recently gained some attention in other domains, including speech, NLP, and graph-like data. In this work, we investigate if the framework of diffusion models can be advantageous for general tabular problems, where datapoints are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data makes it quite challenging for accurate modeling, since the individual features can be of completely different nature, i.e., some of them can be continuous and some of them can be discrete. To address such data types, we introduce TabDDPM -- a diffusion model that can be universally applied to any tabular dataset and handles any type of feature. We extensively evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority over existing GAN/VAE alternatives, which is consistent with the advantage of diffusion models in other fields. Additionally, we show that TabDDPM is eligible for privacy-oriented setups, where the original datapoints cannot be publicly shared.",
        "authors": [
            "Akim Kotelnikov",
            "Dmitry Baranchuk",
            "Ivan Rubachev",
            "Artem Babenko"
        ],
        "citations": 175,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Fast point cloud generation with diffusion models in high energy physics",
        "abstract": "Many particle physics datasets like those generated at colliders are described by continuous coordinates (in contrast to grid points like in an image), respect a number of symmetries (like permutation invariance), and have a stochastic dimensionality. For this reason, standard deep generative models that produce images or at least a fixed set of features are limiting. We introduce a new neural network simulation based on a diffusion model that addresses these limitations named Fast Point Cloud Diffusion (FPCD). We show that our approach can reproduce the complex properties of hadronic jets from proton-proton collisions with competitive precision to other recently proposed models. Additionally, we use a procedure called progressive distillation to accelerate the generation time of our method, which is typically a significant challenge for diffusion models despite their state-of-the-art precision.",
        "authors": [
            "V. Mikuni",
            "B. Nachman",
            "M. Pettee"
        ],
        "citations": 53,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Membership Inference Attacks against Diffusion Models",
        "abstract": "Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., timesteps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then show that the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of timesteps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify that DDIM is vulnerable to the attack for small sample sizes instead of achieving a lower FID. Second, sampling steps in hyperparameters are important for resistance to the attack, whereas the impact of sampling variances is quite limited.",
        "authors": [
            "Tomoya Matsumoto",
            "Takayuki Miura",
            "Naoto Yanai"
        ],
        "citations": 42,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Medical Anomaly Detection",
        "abstract": "In medical applications, weakly supervised anomaly detection methods are of great interest, as only image-level annotations are required for training. Current anomaly detection methods mainly rely on generative adversarial networks or autoencoder models. Those models are often complicated to train or have difficulties to preserve fine details in the image. We present a novel weakly supervised anomaly detection method based on denoising diffusion implicit models. We combine the deterministic iterative noising and denoising scheme with classifier guidance for image-to-image translation between diseased and healthy subjects. Our method generates very detailed anomaly maps without the need for a complex training procedure. We evaluate our method on the BRATS2020 dataset for brain tumor detection and the CheXpert dataset for detecting pleural effusions.",
        "authors": [
            "Julia Wolleb",
            "Florentin Bieder",
            "Robin Sandkühler",
            "P. Cattin"
        ],
        "citations": 221,
        "references": 31,
        "year": 2022
    },
    {
        "title": "Survey on Generative Adversarial Networks",
        "abstract": "Generative Adversarial Networks (GANs) are a deep learning based generative model. GANs are a model for training a generative model and it is common to use deep learning models. Generative Adversarial Network(GANs) are a powerful class of neural networks that are used for unsupervised learning. GANs are basically made up of two competing neural network models which compete with each other and are able to analyze, capture and copy the variations within dataset. GANs achieve high level realism by pairing a generator which learns to produce a target output with a discriminator which learns to distinguish true data from the output of the generator. GANs used for Image Synthesis generates high resolution images. Text to face generation is a sub domain of text to image synthesis, and it has a huge impact along with the wide range of applications on public safety domain",
        "authors": [
            "Kshitija T. Nalawade",
            "Mahesh R. Kate",
            "Mangesh B. Narwade",
            "Akhil A. Shinde",
            "Shrikant A. Shinde"
        ],
        "citations": 20,
        "references": 11,
        "year": 2022
    },
    {
        "title": "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets",
        "abstract": "Diffusion models have achieved great success in a range of tasks, such as image synthesis and molecule design. As such successes hinge on large-scale training data collected from diverse sources, the trustworthiness of these collected data is hard to control or audit. In this work, we aim to explore the vulnerabilities of diffusion models under potential training data manipulations and try to answer: How hard is it to perform Trojan attacks on well-trained diffusion models? What are the adversarial targets that such Trojan attacks can achieve? To answer these questions, we propose an effective Trojan attack against diffusion models, TrojDiff, which optimizes the Trojan diffusion and generative processes during training. In particular, we design novel transitions during the Trojan diffusion process to diffuse adversarial targets into a biased Gaussian distribution and propose a new parameterization of the Trojan generative process that leads to an effective training objective for the attack. In addition, we consider three types of adversarial targets: the Trojaned diffusion models will always output instances belonging to a certain class from the in-domain distribution (In-D2D attack), out-of-domain distribution (Out-D2D-attack), and one specific instance (D2I attack). We evaluate TrojDiff on CIFAR-10 and CelebA datasets against both DDPM and DDIM diffusion models. We show that TrojDiff always achieves high attack performance under different adversarial targets using different types of triggers, while the performance in benign environments is preserved. The code is available at https://github.com/chenweixin107/TrojDiff.",
        "authors": [
            "Weixin Chen",
            "D. Song",
            "Bo Li"
        ],
        "citations": 54,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Unsupervised Medical Image Translation With Adversarial Diffusion Models",
        "abstract": "Imputation of missing images via source-to-target modality translation can improve diversity in medical imaging protocols. A pervasive approach for synthesizing target images involves one-shot mapping through generative adversarial networks (GAN). Yet, GAN models that implicitly characterize the image distribution can suffer from limited sample fidelity. Here, we propose a novel method based on adversarial diffusion modeling, SynDiff, for improved performance in medical image translation. To capture a direct correlate of the image distribution, SynDiff leverages a conditional diffusion process that progressively maps noise and source images onto the target image. For fast and accurate image sampling during inference, large diffusion steps are taken with adversarial projections in the reverse diffusion direction. To enable training on unpaired datasets, a cycle-consistent architecture is devised with coupled diffusive and non-diffusive modules that bilaterally translate between two modalities. Extensive assessments are reported on the utility of SynDiff against competing GAN and diffusion models in multi-contrast MRI and MRI-CT translation. Our demonstrations indicate that SynDiff offers quantitatively and qualitatively superior performance against competing baselines.",
        "authors": [
            "Muzaffer Ozbey",
            "S. Dar",
            "H. Bedel",
            "Onat Dalmaz",
            "cSaban Ozturk",
            "Alper Gungor",
            "Tolga cCukur"
        ],
        "citations": 211,
        "references": 86,
        "year": 2022
    },
    {
        "title": "Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths",
        "abstract": "AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models (DMs) are another class of deep generative models and have recently achieved remarkable performance on various image synthesis tasks. However, training image diffusion models usually requires substantial computational resources to achieve a high performance, which makes expanding diffusion models to high-dimensional video synthesis tasks more computationally expensive. To ease this problem while leveraging its advantages, we introduce lightweight video diffusion models that synthesize high-ﬁdelity and arbitrary-long videos from pure noise. Speciﬁcally, we propose to perform diffusion and de-noising in a low-dimensional 3D latent space, which sig-niﬁcantly outperforms previous methods on 3D pixel space when under a limited computational budget. In addition, though trained on tens of frames, our models is generate videos with arbitrary lengths, i.e., thousands of frames, in an autoregressive way. Finally, we further introduce conditional latent perturbation to reduce performance degradation during generating long-duration videos. Extensive experiments on various datasets and generated lengths suggest that our framework is able to sample much more realistic and longer videos than previous approaches, including GAN-based, autoregressive-based, and diffusion-based methods.",
        "authors": [
            "Yin-Yin He",
            "Tianyu Yang",
            "Yong Zhang",
            "Ying Shan",
            "Qifeng Chen"
        ],
        "citations": 180,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Restoring Vision in Adverse Weather Conditions With Patch-Based Denoising Diffusion Models",
        "abstract": "Image restoration under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image restoration algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image restoration by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image restoration, and experimentally show strong generalization to real-world test images.",
        "authors": [
            "Ozan Özdenizci",
            "R. Legenstein"
        ],
        "citations": 172,
        "references": 77,
        "year": 2022
    },
    {
        "title": "MIMO-GAN: Generative MIMO Channel Modeling",
        "abstract": "We propose generative channel modeling to learn statistical channel models from channel input-output measurements. Generative channel models can learn more complicated distributions and represent the field data more faithfully. They are tractable and easy to sample from, which can potentially speed up the simulation rounds. To achieve this, we leverage advances in generative adversarial network (GAN), which helps us learn an implicit distribution over stochastic MIMO channels from observed measurements. In particular, our approach MIMO-GAN implicitly models the wireless channel as a distribution of time-domain band-limited impulse responses. We evaluate MIMO-GAN on 3GPP TDL MIMO channels and observe high-consistency in capturing power, delay and spatial correlation statistics of the underlying channel. In particular, we observe MIMO-GAN achieve errors of under 3.57 ns average delay and -18.7 dB power.",
        "authors": [
            "Tribhuvanesh Orekondy",
            "A. Behboodi",
            "J. Soriaga"
        ],
        "citations": 16,
        "references": 21,
        "year": 2022
    },
    {
        "title": "3D-aware Image Generation using 2D Diffusion Models",
        "abstract": "In this paper, we introduce a novel 3D-aware image generation method that leverages 2D diffusion models. We formulate the 3D-aware image generation task as multiview 2D image set generation, and further to a sequential unconditional–conditional multiview image generation process. This allows us to utilize 2D diffusion models to boost the generative modeling power of the method. Additionally, we incorporate depth information from monocular depth estimators to construct the training data for the conditional diffusion model using only still images.We train our method on a large-scale unstructured 2D image dataset, i.e., ImageNet, which is not addressed by previous methods. It produces high-quality images that significantly outperform prior methods. Furthermore, our approach showcases its capability to generate instances with large view angles, even though the training images are diverse and unaligned, gathered from \"in-the-wild\" realworld environments.1",
        "authors": [
            "Jianfeng Xiang",
            "Jiaolong Yang",
            "Binbin Huang",
            "Xin Tong"
        ],
        "citations": 53,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Automatic Evaluation of Attribution by Large Language Models",
        "abstract": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.",
        "authors": [
            "Xiang Yue",
            "Boshi Wang",
            "Kai Zhang",
            "Ziru Chen",
            "Yu Su",
            "Huan Sun"
        ],
        "citations": 47,
        "references": 92,
        "year": 2023
    },
    {
        "title": "DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models",
        "abstract": "Understanding how proteins structurally interact is crucial to modern biology, with applications in drug discovery and protein design. Recent machine learning methods have formulated protein-small molecule docking as a generative problem with significant performance boosts over both traditional and deep learning baselines. In this work, we propose a similar approach for rigid protein-protein docking: DiffDock-PP is a diffusion generative model that learns to translate and rotate unbound protein structures into their bound conformations. We achieve state-of-the-art performance on DIPS with a median C-RMSD of 4.85, outperforming all considered baselines. Additionally, DiffDock-PP is faster than all search-based methods and generates reliable confidence estimates for its predictions. Our code is publicly available at $\\texttt{https://github.com/ketatam/DiffDock-PP}$",
        "authors": [
            "Mohamed Amine Ketata",
            "Cedrik Laue",
            "R. Mammadov",
            "Hannes Stärk",
            "Menghua Wu",
            "Gabriele Corso",
            "Ćeline Marquet",
            "R. Barzilay",
            "T. Jaakkola"
        ],
        "citations": 39,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Denoising Diffusion Bridge Models",
        "abstract": "Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score. Our method naturally unifies several classes of generative models, such as score-based diffusion models and OT-Flow-Matching, allowing us to adapt existing design and architectural choices to our more general problem. Empirically, we apply DDBMs to challenging image datasets in both pixel and latent space. On standard image translation problems, DDBMs achieve significant improvement over baseline methods, and, when we reduce the problem to image generation by setting the source distribution to random noise, DDBMs achieve comparable FID scores to state-of-the-art methods despite being built for a more general task.",
        "authors": [
            "Linqi Zhou",
            "Aaron Lou",
            "Samar Khanna",
            "Stefano Ermon"
        ],
        "citations": 34,
        "references": 59,
        "year": 2023
    },
    {
        "title": "VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models",
        "abstract": "Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.",
        "authors": [
            "Sheng-Yen Chou",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "citations": 34,
        "references": 57,
        "year": 2023
    },
    {
        "title": "On the Design Fundamentals of Diffusion Models: A Survey",
        "abstract": "Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.",
        "authors": [
            "Ziyi Chang",
            "G. Koulieris",
            "Hubert P. H. Shum"
        ],
        "citations": 44,
        "references": 306,
        "year": 2023
    },
    {
        "title": "Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models",
        "abstract": "Text-to-image generative models have demonstrated remarkable capabilities in generating high-quality images based on textual prompts. However, crafting prompts that accurately capture the user’s creative intent remains challenging. It often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user’s intention. To address these challenges, we present Promptify, an interactive system that supports prompt exploration and refinement for text-to-image generative models. Promptify utilizes a suggestion engine powered by large language models to help users quickly explore and craft diverse prompts. Our interface allows users to organize the generated images flexibly, and based on their preferences, Promptify suggests potential changes to the original prompt. This feedback loop enables users to iteratively refine their prompts and enhance desired features while avoiding unwanted ones. Our user study shows that Promptify effectively facilitates the text-to-image workflow, allowing users to create visually appealing images on their first attempt while requiring significantly less cognitive load than a widely-used baseline tool.",
        "authors": [
            "Stephen Brade",
            "Bryan Wang",
            "Maurício Sousa",
            "Sageev Oore",
            "Tovi Grossman"
        ],
        "citations": 43,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Reinforcement Learning: A Survey",
        "abstract": "Diffusion models surpass previous generative models in sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions. This survey aims to provide an overview of this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by RL algorithms. Then, we present a taxonomy of existing methods based on the roles of diffusion models in RL and explore how the preceding challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks. Finally, we conclude the survey and offer insights into future research directions. We are actively maintaining a GitHub repository for papers and other related resources in utilizing diffusion models in RL: https://github.com/apexrl/Diff4RLSurvey.",
        "authors": [
            "Zhengbang Zhu",
            "Hanye Zhao",
            "Haoran He",
            "Yichao Zhong",
            "Shenyu Zhang",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "citations": 39,
        "references": 138,
        "year": 2023
    },
    {
        "title": "Black Box Adversarial Prompting for Foundation Models",
        "abstract": "Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.",
        "authors": [
            "N. Maus",
            "Patrick Chao",
            "Eric Wong",
            "Jacob R. Gardner"
        ],
        "citations": 41,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Diffusion Models already have a Semantic Latent Space",
        "abstract": "Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/",
        "authors": [
            "Mingi Kwon",
            "Jaeseok Jeong",
            "Youngjung Uh"
        ],
        "citations": 203,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Fast Training of Diffusion Models with Masked Transformers",
        "abstract": "We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance.",
        "authors": [
            "Hongkai Zheng",
            "Weili Nie",
            "Arash Vahdat",
            "Anima Anandkumar"
        ],
        "citations": 48,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Diffusion models in bioinformatics and computational biology.",
        "abstract": null,
        "authors": [
            "Zhiye Guo",
            "Jian Liu",
            "Yanli Wang",
            "Mengrui Chen",
            "Duolin Wang",
            "Dong Xu",
            "Jianlin Cheng"
        ],
        "citations": 53,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion",
        "abstract": "Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose Copilot4D, a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer as discrete diffusion and enhance it with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, Copilot4D reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argoverse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of GPT-like unsupervised learning for robotics.",
        "authors": [
            "Lunjun Zhang",
            "Yuwen Xiong",
            "Ze Yang",
            "Sergio Casas",
            "Rui Hu",
            "R. Urtasun"
        ],
        "citations": 39,
        "references": 65,
        "year": 2023
    },
    {
        "title": "A Survey of Large Language Models Attribution",
        "abstract": "Open-domain generative systems have gained significant attention in the field of conversational AI (e.g., generative search engines). This paper presents a comprehensive review of the attribution mechanisms employed by these systems, particularly large language models. Though attribution or citation improve the factuality and verifiability, issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems. The aim of this survey is to provide valuable insights for researchers, aiding in the refinement of attribution methodologies to enhance the reliability and veracity of responses generated by open-domain generative systems. We believe that this field is still in its early stages; hence, we maintain a repository to keep track of ongoing studies at https://github.com/HITsz-TMG/awesome-llm-attributions.",
        "authors": [
            "Dongfang Li",
            "Zetian Sun",
            "Xinshuo Hu",
            "Zhenyu Liu",
            "Ziyang Chen",
            "Baotian Hu",
            "Aiguo Wu",
            "Min Zhang"
        ],
        "citations": 39,
        "references": 77,
        "year": 2023
    },
    {
        "title": "On The Detection of Synthetic Images Generated by Diffusion Models",
        "abstract": "Over the past decade, there has been tremendous progress in creating synthetic media, mainly thanks to the development of powerful methods based on generative adversarial networks (GAN). Very recently, methods based on diffusion models (DM) have been gaining the spotlight. In addition to providing an impressive level of photorealism, they enable the creation of text-based visual content, opening up new and exciting opportunities in many different application fields, from arts to video games. On the other hand, this property is an additional asset in the hands of malicious users, who can generate and distribute fake media perfectly adapted to their attacks, posing new challenges to the media forensic community. With this work, we seek to understand how difficult it is to distinguish synthetic images generated by diffusion models from pristine ones and whether current state-of-the-art detectors are suitable for the task. To this end, first we expose the forensics traces left by diffusion models, then study how current detectors, developed for GAN-generated images, perform on these new synthetic images, especially in challenging social-network scenarios involving image compression and resizing. Datasets and code are available at https:github.com/grip-unina/DMimageDetection.",
        "authors": [
            "Riccardo Corvi",
            "D. Cozzolino",
            "Giada Zingarini",
            "G. Poggi",
            "Koki Nagano",
            "L. Verdoliva"
        ],
        "citations": 176,
        "references": 34,
        "year": 2022
    },
    {
        "title": "Language Models are Realistic Tabular Data Generators",
        "abstract": "Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data's characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes.",
        "authors": [
            "V. Borisov",
            "Kathrin Seßler",
            "Tobias Leemann",
            "Martin Pawelczyk",
            "Gjergji Kasneci"
        ],
        "citations": 175,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Score-based Diffusion Models in Function Space",
        "abstract": "Diffusion models have recently emerged as a powerful framework for generative modeling. They consist of a forward process that perturbs input data with Gaussian white noise and a reverse process that learns a score function to generate samples by denoising. Despite their tremendous success, they are mostly formulated on finite-dimensional spaces, e.g., Euclidean, limiting their applications to many domains where the data has a functional form, such as in scientific computing and 3D geometric data analysis. This work introduces a mathematically rigorous framework called Denoising Diffusion Operators (DDOs) for training diffusion models in function space. In DDOs, the forward process perturbs input functions gradually using a Gaussian process. The generative process is formulated by a function-valued annealed Langevin dynamic. Our approach requires an appropriate notion of the score for the perturbed data distribution, which we obtain by generalizing denoising score matching to function spaces that can be infinite-dimensional. We show that the corresponding discretized algorithm generates accurate samples at a fixed cost independent of the data resolution. We theoretically and numerically verify the applicability of our approach on a set of function-valued problems, including generating solutions to the Navier-Stokes equation viewed as the push-forward distribution of forcings from a Gaussian Random Field (GRF), as well as volcano InSAR and MNIST-SDF.",
        "authors": [
            "Jae Hyun Lim",
            "Nikola B. Kovachki",
            "R. Baptista",
            "Christopher Beckham",
            "K. Azizzadenesheli",
            "Jean Kossaifi",
            "Vikram S. Voleti",
            "Jiaming Song",
            "Karsten Kreis",
            "J. Kautz",
            "C. Pal",
            "Arash Vahdat",
            "Anima Anandkumar"
        ],
        "citations": 35,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Shifting machine learning for healthcare from development to deployment and from models to data",
        "abstract": null,
        "authors": [
            "Angela Zhang",
            "L. Xing",
            "James Zou",
            "Joseph C. Wu"
        ],
        "citations": 187,
        "references": 191,
        "year": 2022
    },
    {
        "title": "The Utility of Language Models in Cardiology: A Narrative Review of the Benefits and Concerns of ChatGPT-4",
        "abstract": "Artificial intelligence (AI) and language models such as ChatGPT-4 (Generative Pretrained Transformer) have made tremendous advances recently and are rapidly transforming the landscape of medicine. Cardiology is among many of the specialties that utilize AI with the intention of improving patient care. Generative AI, with the use of its advanced machine learning algorithms, has the potential to diagnose heart disease and recommend management options suitable for the patient. This may lead to improved patient outcomes not only by recommending the best treatment plan but also by increasing physician efficiency. Language models could assist physicians with administrative tasks, allowing them to spend more time on patient care. However, there are several concerns with the use of AI and language models in the field of medicine. These technologies may not be the most up-to-date with the latest research and could provide outdated information, which may lead to an adverse event. Secondly, AI tools can be expensive, leading to increased healthcare costs and reduced accessibility to the general population. There is also concern about the loss of the human touch and empathy as AI becomes more mainstream. Healthcare professionals would need to be adequately trained to utilize these tools. While AI and language models have many beneficial traits, all healthcare providers need to be involved and aware of generative AI so as to assure its optimal use and mitigate any potential risks and challenges associated with its implementation. In this review, we discuss the various uses of language models in the field of cardiology.",
        "authors": [
            "Dhir Gala",
            "A. Makaryus"
        ],
        "citations": 50,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality",
        "abstract": "Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We also present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required.",
        "authors": [
            "Daniel Watson",
            "William Chan",
            "Jonathan Ho",
            "Mohammad Norouzi"
        ],
        "citations": 160,
        "references": 41,
        "year": 2022
    },
    {
        "title": "Applications of large language models in cancer care: current evidence and future perspectives",
        "abstract": "The development of large language models (LLMs) is a recent success in the field of generative artificial intelligence (AI). They are computer models able to perform a wide range of natural language processing tasks, including content generation, question answering, or language translation. In recent months, a growing number of studies aimed to assess their potential applications in the field of medicine, including cancer care. In this mini review, we described the present published evidence for using LLMs in oncology. All the available studies assessed ChatGPT, an advanced language model developed by OpenAI, alone or compared to other LLMs, such as Google Bard, Chatsonic, and Perplexity. Although ChatGPT could provide adequate information on the screening or the management of specific solid tumors, it also demonstrated a significant error rate and a tendency toward providing obsolete data. Therefore, an accurate, expert-driven verification process remains mandatory to avoid the potential for misinformation and incorrect evidence. Overall, although this new generative AI-based technology has the potential to revolutionize the field of medicine, including that of cancer care, it will be necessary to develop rules to guide the application of these tools to maximize benefits and minimize risks.",
        "authors": [
            "G. M. Iannantuono",
            "Dara Bracken-Clarke",
            "C. Floudas",
            "M. Roselli",
            "J. Gulley",
            "F. Karzai"
        ],
        "citations": 47,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Efficient Training of Language Models to Fill in the Middle",
        "abstract": "We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.",
        "authors": [
            "Mohammad Bavarian",
            "Heewoo Jun",
            "N. Tezak",
            "John Schulman",
            "C. McLeavey",
            "Jerry Tworek",
            "Mark Chen"
        ],
        "citations": 148,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models",
        "abstract": "The imputation of missing values represents a significant obstacle for many real-world data analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation model that relies on two emerging technologies, (conditional) diffusion models as state-of-the-art generative models and structured state space models as internal model architecture, which are particularly suited to capture long-term dependencies in time series data. We demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation and forecasting performance on a broad range of data sets and different missingness scenarios, including the challenging blackout-missing scenarios, where prior approaches failed to provide meaningful results.",
        "authors": [
            "Juan Miguel Lopez Alcaraz",
            "Nils Strodthoff"
        ],
        "citations": 133,
        "references": 68,
        "year": 2022
    },
    {
        "title": "Convergence of denoising diffusion models under the manifold hypothesis",
        "abstract": "Denoising diffusion models are a recent class of generative models exhibiting state-of-the-art performance in image and audio synthesis. Such models approximate the time-reversal of a forward noising process from a target distribution to a reference density, which is usually Gaussian. Despite their strong empirical results, the theoretical analysis of such models remains limited. In particular, all current approaches crucially assume that the target density admits a density w.r.t. the Lebesgue measure. This does not cover settings where the target distribution is supported on a lower-dimensional manifold or is given by some empirical distribution. In this paper, we bridge this gap by providing the first convergence results for diffusion models in this more general setting. In particular, we provide quantitative bounds on the Wasserstein distance of order one between the target data distribution and the generative distribution of the diffusion model.",
        "authors": [
            "Valentin De Bortoli"
        ],
        "citations": 135,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task",
        "abstract": "Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhibits a sudden\"emergence\"due to multiplicative reliance on the performance of constituent tasks, partially explaining emergent phenomena seen in generative models; and (iii) composing concepts with lower frequency in the training data to generate out-of-distribution samples requires considerably more optimization steps compared to generating in-distribution samples. Overall, our study lays a foundation for understanding capabilities and compositionality in generative models from a data-centric perspective.",
        "authors": [
            "Maya Okawa",
            "Ekdeep Singh Lubana",
            "Robert P. Dick",
            "Hidenori Tanaka"
        ],
        "citations": 26,
        "references": 123,
        "year": 2023
    },
    {
        "title": "Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints",
        "abstract": "The limits of open-ended generative models are unclear, yet increasingly important. What causes them to succeed and what causes them to fail? In this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. We present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. These constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model’s generative failures. We also show the generalizability of our proposed method on other large models like BLOOM and OPT. Our results and our in-context mitigation strategies reveal open challenges for future research.",
        "authors": [
            "Albert Lu",
            "Hongxin Zhang",
            "Yanzhe Zhang",
            "Xuezhi Wang",
            "Diyi Yang"
        ],
        "citations": 24,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Infinite-Dimensional Diffusion Models for Function Spaces",
        "abstract": "deﬁne diffusion-based generative models in inﬁnite dimensions",
        "authors": [
            "Jakiw Pidstrigach",
            "Youssef M. Marzouk",
            "Sebastian Reich",
            "Sven Wang"
        ],
        "citations": 20,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Query Expansion by Prompting Large Language Models",
        "abstract": "Query expansion is a widely used technique to improve the recall of search systems. In this paper, we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo-Relevance Feedback (PRF) that relies on retrieving a good set of pseudo-relevant documents to expand queries, we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts, including zero-shot, few-shot and Chain-of-Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query. Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.",
        "authors": [
            "R. Jagerman",
            "Honglei Zhuang",
            "Zhen Qin",
            "Xuanhui Wang",
            "Michael Bendersky"
        ],
        "citations": 30,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models",
        "abstract": "Proteins are macromolecules that mediate a significant fraction of the cellular processes that underlie life. An important task in bioengineering is designing proteins with specific 3D structures and chemical properties which enable targeted functions. To this end, we introduce a generative model of both protein structure and sequence that can operate at significantly larger scales than previous molecular generative modeling approaches. The model is learned entirely from experimental data and conditions its generation on a compact specification of protein topology to produce a full-atom backbone configuration as well as sequence and side-chain predictions. We demonstrate the quality of the model via qualitative and quantitative analysis of its samples. Videos of sampling trajectories are available at https://nanand2.github.io/proteins .",
        "authors": [
            "N. Anand",
            "Tudor Achim"
        ],
        "citations": 160,
        "references": 37,
        "year": 2022
    },
    {
        "title": "High Fidelity Image Counterfactuals with Probabilistic Causal Models",
        "abstract": "We present a general causal generative modelling framework for accurate estimation of high fidelity image counterfactuals with deep structural causal models. Estimation of interventional and counterfactual queries for high-dimensional structured variables, such as images, remains a challenging task. We leverage ideas from causal mediation analysis and advances in generative modelling to design new deep causal mechanisms for structured variables in causal models. Our experiments demonstrate that our proposed mechanisms are capable of accurate abduction and estimation of direct, indirect and total effects as measured by axiomatic soundness of counterfactuals.",
        "authors": [
            "Fabio De Sousa Ribeiro",
            "Tian Xia",
            "M. Monteiro",
            "Nick Pawlowski",
            "B. Glocker"
        ],
        "citations": 27,
        "references": 96,
        "year": 2023
    },
    {
        "title": "Diffusion models as plug-and-play priors",
        "abstract": "We consider the problem of inferring high-dimensional data $\\mathbf{x}$ in a model that consists of a prior $p(\\mathbf{x})$ and an auxiliary differentiable constraint $c(\\mathbf{x},\\mathbf{y})$ on $x$ given some additional information $\\mathbf{y}$. In this paper, the prior is an independently trained denoising diffusion generative model. The auxiliary constraint is expected to have a differentiable form, but can come from diverse sources. The possibility of such inference turns diffusion models into plug-and-play modules, thereby allowing a range of potential applications in adapting models to new domains and tasks, such as conditional generation or image segmentation. The structure of diffusion models allows us to perform approximate inference by iterating differentiation through the fixed denoising network enriched with different amounts of noise at each step. Considering many noised versions of $\\mathbf{x}$ in evaluation of its fitness is a novel search mechanism that may lead to new algorithms for solving combinatorial optimization problems.",
        "authors": [
            "Alexandros Graikos",
            "Nikolay Malkin",
            "N. Jojic",
            "D. Samaras"
        ],
        "citations": 183,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting",
        "abstract": "Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally-trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (refine). Notably, the generative performance of the model remains intact -- downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (synthesize).",
        "authors": [
            "Marcel Kollovieh",
            "Abdul Fatir Ansari",
            "Michael Bohlke-Schneider",
            "Jasper Zschiegner",
            "Hao Wang",
            "Yuyang Wang"
        ],
        "citations": 26,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Semantic Image Synthesis via Diffusion Models",
        "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in various image generation tasks compared with Generative Adversarial Nets (GANs). Recent work on semantic image synthesis mainly follows the \\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality or diversity of generated images. In this paper, we propose a novel framework based on DDPM for semantic image synthesis. Unlike previous conditional diffusion model directly feeds the semantic layout and noisy image as input to a U-Net structure, which may not fully leverage the information in the input semantic mask, our framework processes semantic layout and noisy image differently. It feeds noisy image to the encoder of the U-Net structure while the semantic layout to the decoder by multi-layer spatially-adaptive normalization operators. To further improve the generation quality and semantic interpretability in semantic image synthesis, we introduce the classifier-free guidance sampling strategy, which acknowledge the scores of an unconditional model for sampling process. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed method, achieving state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS).",
        "authors": [
            "Weilun Wang",
            "Jianmin Bao",
            "Wen-gang Zhou",
            "Dongdong Chen",
            "Dong Chen",
            "Lu Yuan",
            "Houqiang Li"
        ],
        "citations": 157,
        "references": 62,
        "year": 2022
    },
    {
        "title": "“I’m sorry to hear that”: Finding New Biases in Language Models with a Holistic Descriptor Dataset",
        "abstract": "As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes and are commonly used with preset bias tests that presuppose which types of biases models can exhibit. In this work, we present a new, more inclusive bias measurement dataset, HolisticBias, which includes nearly 600 descriptor terms across 13 different demographic axes. HolisticBias was assembled in a participatory process including experts and community members with lived experience of these terms. These descriptors combine with a set of bias measurement templates to produce over 450,000 unique sentence prompts, which we use to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that HolisticBias is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, which we hope will serve as a basis for more easy-to-use and standardized methods for evaluating bias in NLP models.",
        "authors": [
            "Eric Michael Smith",
            "Melissa Hall",
            "M. Kambadur",
            "Eleonora Presani",
            "Adina Williams"
        ],
        "citations": 119,
        "references": 89,
        "year": 2022
    },
    {
        "title": "Structure-based drug design with equivariant diffusion models",
        "abstract": null,
        "authors": [
            "Arne Schneuing",
            "Yuanqi Du",
            "Charles Harris",
            "Arian R. Jamasb",
            "Ilia Igashov",
            "Weitao Du",
            "T. Blundell",
            "Pietro Li'o",
            "Carla P. Gomes",
            "Max Welling",
            "Michael M. Bronstein",
            "B. Correia"
        ],
        "citations": 151,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Diffusion Models Beat GANs on Image Classification",
        "abstract": "While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which uses a single pre-training stage to address both families of tasks simultaneously. We identify diffusion models as a prime candidate. Diffusion models have risen to prominence as a state-of-the-art method for image generation, denoising, inpainting, super-resolution, manipulation, etc. Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high fidelity, diverse, novel images. The U-Net architecture, as a convolution-based architecture, generates a diverse set of feature representations in the form of intermediate feature maps. We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification. We explore optimal methods for extracting and using these embeddings for classification tasks, demonstrating promising results on the ImageNet classification task. We find that with careful feature selection and pooling, diffusion models outperform comparable generative-discriminative methods such as BigBiGAN for classification tasks. We investigate diffusion models in the transfer learning regime, examining their performance on several fine-grained visual classification datasets. We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.",
        "authors": [
            "Soumik Mukhopadhyay",
            "M. Gwilliam",
            "Vatsal Agarwal",
            "Namitha Padmanabhan",
            "A. Swaminathan",
            "Srinidhi Hegde",
            "Tianyi Zhou",
            "Abhinav Shrivastava"
        ],
        "citations": 30,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Diffusion Models Beat GANs on Image Classification",
        "abstract": "While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which uses a single pre-training stage to address both families of tasks simultaneously. We identify diffusion models as a prime candidate. Diffusion models have risen to prominence as a state-of-the-art method for image generation, denoising, inpainting, super-resolution, manipulation, etc. Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high fidelity, diverse, novel images. The U-Net architecture, as a convolution-based architecture, generates a diverse set of feature representations in the form of intermediate feature maps. We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification. We explore optimal methods for extracting and using these embeddings for classification tasks, demonstrating promising results on the ImageNet classification task. We find that with careful feature selection and pooling, diffusion models outperform comparable generative-discriminative methods such as BigBiGAN for classification tasks. We investigate diffusion models in the transfer learning regime, examining their performance on several fine-grained visual classification datasets. We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.",
        "authors": [
            "Soumik Mukhopadhyay",
            "M. Gwilliam",
            "Vatsal Agarwal",
            "Namitha Padmanabhan",
            "A. Swaminathan",
            "Srinidhi Hegde",
            "Tianyi Zhou",
            "Abhinav Shrivastava"
        ],
        "citations": 30,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Comparison of point cloud and image-based models for calorimeter fast simulation",
        "abstract": "Score based generative models are a new class of generative models that have been shown to accurately generate high dimensional calorimeter datasets. Recent advances in generative models have used images with 3D voxels to represent and model complex calorimeter showers. Point clouds, however, are likely a more natural representation of calorimeter showers, particularly in calorimeters with high granularity. Point clouds preserve all of the information of the original simulation, more naturally deal with sparse datasets, and can be implemented with more compact models and data files. In this work, two state-of-the-art score based models are trained on the same set of calorimeter simulation and directly compared.",
        "authors": [
            "Fernando Torales Acosta",
            "V. Mikuni",
            "B. Nachman",
            "M. Arratia",
            "B. Karki",
            "R. Milton",
            "P. Karande",
            "Aaron Angerami"
        ],
        "citations": 21,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Counterfactual Identifiability of Bijective Causal Models",
        "abstract": "We study counterfactual identifiability in causal models with bijective generation mechanisms (BGM), a class that generalizes several widely-used causal models in the literature. We establish their counterfactual identifiability for three common causal structures with unobserved confounding, and propose a practical learning method that casts learning a BGM as structured generative modeling. Learned BGMs enable efficient counterfactual estimation and can be obtained using a variety of deep conditional generative models. We evaluate our techniques in a visual task and demonstrate its application in a real-world video streaming simulation task.",
        "authors": [
            "Arash Nasr-Esfahany",
            "MohammadIman Alizadeh",
            "Devavrat Shah"
        ],
        "citations": 19,
        "references": 127,
        "year": 2023
    },
    {
        "title": "Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation",
        "abstract": "Generative models have had a profound impact on vision and language, paving the way for a new era of multimodal generative applications. While these successes have inspired researchers to explore using generative models in science and engineering to accelerate the design process and reduce the reliance on iterative optimization, challenges remain. Specifically, engineering optimization methods based on physics still outperform generative models when dealing with constrained environments where data is scarce and precision is paramount. To address these challenges, we introduce Diffusion Optimization Models (DOM) and Trajectory Alignment (TA), a learning framework that demonstrates the efficacy of aligning the sampling trajectory of diffusion models with the optimization trajectory derived from traditional physics-based methods. This alignment ensures that the sampling process remains grounded in the underlying physical principles. Our method allows for generating feasible and high-performance designs in as few as two steps without the need for expensive preprocessing, external surrogate models, or additional labeled data. We apply our framework to structural topology optimization, a fundamental problem in mechanical design, evaluating its performance on in- and out-of-distribution configurations. Our results demonstrate that TA outperforms state-of-the-art deep generative models on in-distribution configurations and halves the inference computational cost. When coupled with a few steps of optimization, it also improves manufacturability for out-of-distribution conditions. By significantly improving performance and inference efficiency, DOM enables us to generate high-quality designs in just a few steps and guide them toward regions of high performance and manufacturability, paving the way for the widespread application of generative models in large-scale data-driven design.",
        "authors": [
            "Giorgio Giannone",
            "Akash Srivastava",
            "O. Winther",
            "Faez Ahmed"
        ],
        "citations": 19,
        "references": 114,
        "year": 2023
    },
    {
        "title": "Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels",
        "abstract": "In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0) with one, two, or five labels per class, respectively. Notably, our results demonstrate that diffusion can generate realistic images with only a few labels (e.g.,<0.1%) and generative augmentation remains viable for semi-supervised classification. Our code is available at https://github.com/ML-GSAI/DPT.",
        "authors": [
            "Zebin You",
            "Yong Zhong",
            "Fan Bao",
            "Jiacheng Sun",
            "Chongxuan Li",
            "Jun Zhu"
        ],
        "citations": 25,
        "references": 119,
        "year": 2023
    },
    {
        "title": "Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains",
        "abstract": "Multi-modal foundation models are typically trained on millions of pairs of natural images and text captions, frequently obtained through web-crawling approaches. Although such models depict excellent generative capabilities, they do not typically generalize well to specific domains such as medical images that have fundamentally shifted distributions compared to natural images. Building generative models for medical images that faithfully depict clinical context may help alleviate the paucity of healthcare datasets. Thus, in this study, we seek to research and expand the representational capabilities of large pretrained foundation models to medical concepts, specifically for leveraging the Stable Diffusion model to generate domain specific images found in medical imaging. We explore the sub-components of the Stable Diffusion pipeline (the variational autoencoder, the U-Net and the text-encoder) to fine-tune the model to generate medical images. We benchmark the efficacy of these efforts using quantitative image quality metrics and qualitative radiologist-driven evaluations that accurately represent the clinical content of conditional text prompts. Our best-performing model improves upon the stable diffusion baseline and can be conditioned to insert a realistic-looking abnormality on a synthetic radiology image, while maintaining a 95% accuracy on a classifier trained to detect the abnormality.",
        "authors": [
            "P. Chambon",
            "Christian Blüthgen",
            "C. Langlotz",
            "Akshay Chaudhari"
        ],
        "citations": 94,
        "references": 21,
        "year": 2022
    },
    {
        "title": "Diffusion Models for Video Prediction and Infilling",
        "abstract": "Predicting and anticipating future outcomes or reasoning about missing information in a sequence are critical skills for agents to be able to make intelligent decisions. This requires strong, temporally coherent generative capabilities. Diffusion models have shown remarkable success in several generative tasks, but have not been extensively explored in the video domain. We present Random-Mask Video Diffusion (RaMViD), which extends image diffusion models to videos using 3D convolutions, and introduces a new conditioning technique during training. By varying the mask we condition on, the model is able to perform video prediction, infilling, and upsampling. Due to our simple conditioning scheme, we can utilize the same architecture as used for unconditional training, which allows us to train the model in a conditional and unconditional fashion at the same time. We evaluate RaMViD on two benchmark datasets for video prediction, on which we achieve state-of-the-art results, and one for video generation. High-resolution videos are provided at https://sites.google.com/view/video-diffusion-prediction.",
        "authors": [
            "Tobias Hoppe",
            "Arash Mehrjou",
            "Stefan Bauer",
            "Didrik Nielsen",
            "Andrea Dittadi"
        ],
        "citations": 119,
        "references": 66,
        "year": 2022
    },
    {
        "title": "Retrieval-Augmented Diffusion Models",
        "abstract": "Generative image synthesis with diffusion models has recently achieved excellent visual quality in several tasks such as text-based or class-conditional image synthesis. Much of this success is due to a dramatic increase in the computational capacity invested in training these models. This work presents an alternative approach: inspired by its successful application in natural language processing, we propose to complement the diffusion model with a retrieval-based approach and to introduce an explicit memory in the form of an external database. During training, our diffusion model is trained with similar visual features retrieved via CLIP and from the neighborhood of each training instance. By leveraging CLIP’s joint image-text embedding space, our model achieves highly competitive performance on tasks for which it has not been explicitly trained, such as class-conditional or text-image synthesis, and can be conditioned on both text and image embeddings. Moreover, we can apply our approach to unconditional generation, where it achieves state-of-the-art performance. Our approach incurs low computational and memory overheads and is easy to implement. We discuss its relationship to concurrent work and will publish code and pretrained models soon.",
        "authors": [
            "A. Blattmann",
            "Robin Rombach",
            "K. Oktay",
            "B. Ommer"
        ],
        "citations": 119,
        "references": 105,
        "year": 2022
    },
    {
        "title": "Lossy Image Compression with Conditional Diffusion Models",
        "abstract": "This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional ``content'' latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining ``texture'' variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based model, while also yielding competitive performance with VAE-based models in several distortion metrics. Furthermore, training the diffusion with $\\mathcal{X}$-parameterization enables high-quality reconstructions in only a handful of decoding steps, greatly affecting the model's practicality. Our code is available at: \\url{https://github.com/buggyyang/CDC_compression}",
        "authors": [
            "Ruihan Yang",
            "S. Mandt"
        ],
        "citations": 88,
        "references": 85,
        "year": 2022
    },
    {
        "title": "Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models",
        "abstract": "Deep generative models have emerged as promising tools for detecting arbitrary anomalies in data, dispensing with the necessity for manual labelling. Recently, autoregressive transformers have achieved state-of-the-art performance for anomaly detection in medical imaging. Nonetheless, these models still have some intrinsic weaknesses, such as requiring images to be modelled as 1D sequences, the accumulation of errors during the sampling process, and the significant inference times associated with transformers. Denoising diffusion probabilistic models are a class of non-autoregressive generative models recently shown to produce excellent samples in computer vision (surpassing Generative Adversarial Networks), and to achieve log-likelihoods that are competitive with transformers while having fast inference times. Diffusion models can be applied to the latent representations learnt by autoencoders, making them easily scalable and great candidates for application to high dimensional data, such as medical images. Here, we propose a method based on diffusion models to detect and segment anomalies in brain imaging. By training the models on healthy data and then exploring its diffusion and reverse steps across its Markov chain, we can identify anomalous areas in the latent space and hence identify anomalies in the pixel space. Our diffusion models achieve competitive performance compared with autoregressive approaches across a series of experiments with 2D CT and MRI data involving synthetic and real pathological lesions with much reduced inference times, making their usage clinically viable.",
        "authors": [
            "W. H. Pinaya",
            "M. Graham",
            "Robert J. Gray",
            "P. F. D. Costa",
            "Petru-Daniel Tudosiu",
            "P. Wright",
            "Y. Mah",
            "A. MacKinnon",
            "J. Teo",
            "H. Jäger",
            "D. Werring",
            "Geraint Rees",
            "P. Nachev",
            "S. Ourselin",
            "M. Cardoso"
        ],
        "citations": 92,
        "references": 32,
        "year": 2022
    },
    {
        "title": "Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",
        "abstract": "Machine-generated text is increasingly difficult to distinguish from text authored by humans. Powerful open-source models are freely available, and user-friendly tools that democratize access to generative models are proliferating. ChatGPT, which was released shortly after the first edition of this survey, epitomizes these trends. The great potential of state-of-the-art natural language generation (NLG) systems is tempered by the multitude of avenues for abuse. Detection of machine-generated text is a key countermeasure for reducing the abuse of NLG models, and presents significant technical challenges and numerous open problems. We provide a survey that includes 1) an extensive analysis of threat models posed by contemporary NLG systems and 2) the most complete review of machine-generated text detection methods to date. This survey places machine-generated text within its cybersecurity and social context, and provides strong guidance for future work addressing the most critical threat models. While doing so, we highlight the importance that detection systems themselves demonstrate trustworthiness through fairness, robustness, and accountability.",
        "authors": [
            "Evan Crothers",
            "N. Japkowicz",
            "H. Viktor"
        ],
        "citations": 89,
        "references": 217,
        "year": 2022
    },
    {
        "title": "Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models",
        "abstract": "Generative models have been widely studied in computer vision. Recently, diffusion models have drawn substantial attention due to the high quality of their generated images. A key desired property of image generative models is the ability to disentangle different attributes, which should enable modification towards a style without changing the semantic content, and the modification parameters should generalize to different images. Previous studies have found that generative adversarial networks (GANs) are inherently endowed with such disentanglement capability, so they can perform disentangled image editing without re-training or fine-tuning the network. In this work, we explore whether diffusion models are also inherently equipped with such a capability. Our finding is that for stable diffusion models, by partially changing the input text embedding from a neutral description (e.g., “a photo of person”) to one with style (e.g., “a photo of person with smile”) while fixing all the Gaussian random noises introduced during the denoising process, the generated images can be modified towards the target style without changing the semantic content. Based on this finding, we further propose a simple, light-weight image editing algorithm where the mixing weights of the two text embeddings are optimized for style matching and content preservation. This entire process only involves optimizing over around 50 parameters and does not fine-tune the diffusion model itself. Experiments show that the proposed method can modify a wide range of attributes, with the performance outperforming diffusion-model-based image-editing algorithms that require fine-tuning. The optimized weights generalize well to different images. Our code is publicly available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.",
        "authors": [
            "Qiucheng Wu",
            "Yujian Liu",
            "Handong Zhao",
            "Ajinkya Kale",
            "T. Bui",
            "Tong Yu",
            "Zhe Lin",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "citations": 80,
        "references": 76,
        "year": 2022
    },
    {
        "title": "BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis",
        "abstract": "Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm.",
        "authors": [
            "Max W. Y. Lam",
            "J. Wang",
            "Dan Su",
            "Dong Yu"
        ],
        "citations": 82,
        "references": 47,
        "year": 2022
    },
    {
        "title": "Differentially Private Diffusion Models",
        "abstract": "While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been demonstrated before for DP generative models. Project page and code: https://nv-tlabs.github.io/DPDM.",
        "authors": [
            "Tim Dockhorn",
            "Tianshi Cao",
            "Arash Vahdat",
            "Karsten Kreis"
        ],
        "citations": 76,
        "references": 121,
        "year": 2022
    },
    {
        "title": "A Continuous Time Framework for Discrete Denoising Models",
        "abstract": "We provide the first complete continuous time framework for denoising diffusion models of discrete data. This is achieved by formulating the forward noising process and corresponding reverse time generative process as Continuous Time Markov Chains (CTMCs). The model can be efficiently trained using a continuous time version of the ELBO. We simulate the high dimensional CTMC using techniques developed in chemical physics and exploit our continuous time framework to derive high performance samplers that we show can outperform discrete time methods for discrete data. The continuous time treatment also enables us to derive a novel theoretical result bounding the error between the generated sample distribution and the true data distribution.",
        "authors": [
            "Andrew Campbell",
            "Joe Benton",
            "Valentin De Bortoli",
            "Tom Rainforth",
            "George Deligiannidis",
            "A. Doucet"
        ],
        "citations": 93,
        "references": 47,
        "year": 2022
    },
    {
        "title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
        "abstract": null,
        "authors": [
            "Thilo Hagendorff",
            "Sarah Fabi",
            "Michal Kosinski"
        ],
        "citations": 105,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Null models in network neuroscience",
        "abstract": null,
        "authors": [
            "F. Váša",
            "B. Mišić"
        ],
        "citations": 99,
        "references": 142,
        "year": 2022
    },
    {
        "title": "How to Backdoor Diffusion Models?",
        "abstract": "Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untam-pered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consistently lead to compromised diffusion models with high utility and target specificity. Even worse, BadDiffusion can be made cost-effective by simply finetuning a clean pre-trained diffusion model to implant backdoors. We also explore some possible countermeasures for risk mitigation. Our results call attention to potential risks and possible misuse of diffusion models. Our code is available on https://github.com/IBM/BadDiffusion.",
        "authors": [
            "Sheng-Yen Chou",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "citations": 72,
        "references": 52,
        "year": 2022
    },
    {
        "title": "CARD: Classification and Regression Diffusion Models",
        "abstract": "Learning the distribution of a continuous or categorical response variable $\\boldsymbol y$ given its covariates $\\boldsymbol x$ is a fundamental problem in statistics and machine learning. Deep neural network-based supervised learning algorithms have made great progress in predicting the mean of $\\boldsymbol y$ given $\\boldsymbol x$, but they are often criticized for their ability to accurately capture the uncertainty of their predictions. In this paper, we introduce classification and regression diffusion (CARD) models, which combine a denoising diffusion-based conditional generative model and a pre-trained conditional mean estimator, to accurately predict the distribution of $\\boldsymbol y$ given $\\boldsymbol x$. We demonstrate the outstanding ability of CARD in conditional distribution prediction with both toy examples and real-world datasets, the experimental results on which show that CARD in general outperforms state-of-the-art methods, including Bayesian neural network-based ones that are designed for uncertainty estimation, especially when the conditional distribution of $\\boldsymbol y$ given $\\boldsymbol x$ is multi-modal. In addition, we utilize the stochastic nature of the generative model outputs to obtain a finer granularity in model confidence assessment at the instance level for classification tasks.",
        "authors": [
            "Xizewen Han",
            "Huangjie Zheng",
            "Mingyuan Zhou"
        ],
        "citations": 88,
        "references": 85,
        "year": 2022
    },
    {
        "title": "Accelerating Diffusion Models via Early Stop of the Diffusion Process",
        "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved impressive performance on various generation tasks. By modeling the reverse process of gradually diffusing the data distribution into a Gaussian distribution, generating a sample in DDPMs can be regarded as iteratively denoising a randomly sampled Gaussian noise. However, in practice DDPMs often need hundreds even thousands of denoising steps to obtain a high-quality sample from the Gaussian noise, leading to extremely low inference efficiency. In this work, we propose a principled acceleration strategy, referred to as Early-Stopped DDPM (ES-DDPM), for DDPMs. The key idea is to stop the diffusion process early where only the few initial diffusing steps are considered and the reverse denoising process starts from a non-Gaussian distribution. By further adopting a powerful pre-trained generative model, such as GAN and VAE, in ES-DDPM, sampling from the target non-Gaussian distribution can be efficiently achieved by diffusing samples obtained from the pre-trained generative model. In this way, the number of required denoising steps is significantly reduced. In the meantime, the sample quality of ES-DDPM also improves substantially, outperforming both the vanilla DDPM and the adopted pre-trained generative model. On extensive experiments across CIFAR-10, CelebA, ImageNet, LSUN-Bedroom and LSUN-Cat, ES-DDPM obtains promising acceleration effect and performance improvement over representative baseline methods. Moreover, ES-DDPM also demonstrates several attractive properties, including being orthogonal to existing acceleration methods, as well as simultaneously enabling both global semantic and local pixel-level control in image generation.",
        "authors": [
            "Zhaoyang Lyu",
            "Xu Xudong",
            "Ceyuan Yang",
            "Dahua Lin",
            "Bo Dai"
        ],
        "citations": 84,
        "references": 43,
        "year": 2022
    },
    {
        "title": "Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models",
        "abstract": "Diffusion models have emerged as the new state-of-the-art generative model with high quality samples, with intriguing properties such as mode coverage and high flexibility. They have also been shown to be effective inverse problem solvers, acting as the prior of the distribution, while the information of the forward model can be granted at the sampling stage. Nonetheless, as the generative process remains in the same high dimensional (i.e. identical to data dimension) space, the models have not been extended to 3D inverse problems due to the extremely high memory and computational cost. In this paper, we combine the ideas from the conventional model-based iterative reconstruction with the modern diffusion models, which leads to a highly effective method for solving 3D medical image reconstruction tasks such as sparse-view tomography, limited angle tomography, compressed sensing MRI from pre-trained 2D diffusion models. In essence, we propose to augment the 2D diffusion prior with a model-based prior in the remaining direction at test time, such that one can achieve coherent reconstructions across all dimensions. Our method can be run in a single commodity GPU, and establishes the new state-of-the-art, showing that the proposed method can perform reconstructions of high fidelity and accuracy even in the most extreme cases (e.g. 2-view 3D tomography). We further reveal that the generalization capacity of the proposed method is surprisingly high, and can be used to reconstruct volumes that are entirely different from the training dataset. Code available: https://github.com/HJ-harry/DiffusionMBIR",
        "authors": [
            "Hyungjin Chung",
            "Dohoon Ryu",
            "Michael T. McCann",
            "M. Klasky",
            "J. C. Ye"
        ],
        "citations": 83,
        "references": 40,
        "year": 2022
    },
    {
        "title": "MolGenSurvey: A Systematic Survey in Machine Learning Models for Molecule Design",
        "abstract": "Molecule design is a fundamental problem in molecular science and has critical applications in a variety of areas, such as drug discovery, material science, etc. However, due to the large searching space, it is impossible for human experts to enumerate and test all molecules in wet-lab experiments. Recently, with the rapid development of machine learning methods, especially generative methods, molecule design has achieved great progress by leveraging machine learning models to generate candidate molecules. In this paper, we systematically review the most relevant work in machine learning models for molecule design. We start with a brief review of the mainstream molecule featurization and representation methods (including 1D string, 2D graph, and 3D geometry) and general generative methods (deep generative and combinatorial optimization methods). Then we summarize all the existing molecule design problems into several venues according to the problem setup, including input, output types and goals. Finally, we conclude with the open challenges and point out future opportunities of machine learning models for molecule design in real-world applications.",
        "authors": [
            "Yuanqi Du",
            "Tianfan Fu",
            "Jimeng Sun",
            "Shengchao Liu"
        ],
        "citations": 77,
        "references": 218,
        "year": 2022
    },
    {
        "title": "Blurring Diffusion Models",
        "abstract": "Recently, Rissanen et al., (2022) have presented a new type of diffusion process for generative modeling based on heat dissipation, or blurring, as an alternative to isotropic Gaussian diffusion. Here, we show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise. In making this connection, we bridge the gap between inverse heat dissipation and denoising diffusion, and we shed light on the inductive bias that results from this modeling choice. Finally, we propose a generalized class of diffusion models that offers the best of both standard Gaussian denoising diffusion and inverse heat dissipation, which we call Blurring Diffusion Models.",
        "authors": [
            "Emiel Hoogeboom",
            "Tim Salimans"
        ],
        "citations": 68,
        "references": 21,
        "year": 2022
    },
    {
        "title": "VIDM: Video Implicit Diffusion Models",
        "abstract": "Diffusion models have emerged as a powerful generative method for synthesizing high-quality and diverse set of images. In this paper, we propose a video generation method based on diffusion models, where the effects of motion are modeled in an implicit condition manner, i.e. one can sample plausible video motions according to the latent feature of frames. We improve the quality of the generated videos by proposing multiple strategies such as sampling space truncation, robustness penalty, and positional group normalization. Various experiments are conducted on datasets consisting of videos with different resolutions and different number of frames. Results show that the proposed method outperforms the state-of-the-art generative adversarial network-based methods by a significant margin in terms of FVD scores as well as perceptible visual quality.",
        "authors": [
            "Kangfu Mei",
            "Vishal M. Patel"
        ],
        "citations": 69,
        "references": 68,
        "year": 2022
    },
    {
        "title": "Diffusion Causal Models for Counterfactual Estimation",
        "abstract": "We consider the task of counterfactual estimation from observational imaging data given a known causal structure. In particular, quantifying the causal effect of interventions for high-dimensional data with neural networks remains an open challenge. Herein we propose Diff-SCM, a deep structural causal model that builds on recent advances of generative energy-based models. In our setting, inference is performed by iteratively sampling gradients of the marginal and conditional distributions entailed by the causal model. Counterfactual estimation is achieved by firstly inferring latent variables with deterministic forward diffusion, then intervening on a reverse diffusion process using the gradients of an anti-causal predictor w.r.t the input. Furthermore, we propose a metric for evaluating the generated counterfactuals. We find that Diff-SCM produces more realistic and minimal counterfactuals than baselines on MNIST data and can also be applied to ImageNet data. Code is available https://github.com/vios-s/Diff-SCM.",
        "authors": [
            "Pedro Sanchez",
            "S. Tsaftaris"
        ],
        "citations": 63,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Denoising diffusion models for out-of-distribution detection",
        "abstract": "Out-of-distribution detection is crucial to the safe deployment of machine learning systems. Currently, unsupervised out-of-distribution detection is dominated by generative-based approaches that make use of estimates of the likelihood or other measurements from a generative model. Reconstruction-based methods offer an alternative approach, in which a measure of reconstruction error is used to determine if a sample is out-of-distribution. However, reconstruction-based approaches are less favoured, as they require careful tuning of the model’s information bottleneck-such as the size of the latent dimension - to produce good results. In this work, we exploit the view of denoising diffusion probabilistic models (DDPM) as denoising autoencoders where the bottleneck is controlled externally, by means of the amount of noise applied. We propose to use DDPMs to reconstruct an input that has been noised to a range of noise levels, and use the resulting multi-dimensional reconstruction error to classify out-of-distribution inputs. We validate our approach both on standard computer-vision datasets and on higher dimension medical datasets. Our approach outperforms not only reconstruction-based methods, but also state-of-the-art generative-based approaches. Code is available at https://github.com/marksgraham/ddpm-ood.",
        "authors": [
            "M. Graham",
            "W. H. Pinaya",
            "Petru-Daniel Tudosiu",
            "P. Nachev",
            "S. Ourselin",
            "M. Cardoso"
        ],
        "citations": 58,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Diffusion models for missing value imputation in tabular data",
        "abstract": "Missing value imputation in machine learning is the task of estimating the missing values in the dataset accurately using available information. In this task, several deep generative modeling methods have been proposed and demonstrated their usefulness, e.g., generative adversarial imputation networks. Recently, diffusion models have gained popularity because of their effectiveness in the generative modeling task in images, texts, audio, etc. To our knowledge, less attention has been paid to the investigation of the effectiveness of diffusion models for missing value imputation in tabular data. Based on recent development of diffusion models for time-series data imputation, we propose a diffusion model approach called\"Conditional Score-based Diffusion Models for Tabular data\"(TabCSDI). To effectively handle categorical variables and numerical variables simultaneously, we investigate three techniques: one-hot encoding, analog bits encoding, and feature tokenization. Experimental results on benchmark datasets demonstrated the effectiveness of TabCSDI compared with well-known existing methods, and also emphasized the importance of the categorical embedding techniques.",
        "authors": [
            "Shuhan Zheng",
            "Nontawat Charoenphakdee"
        ],
        "citations": 53,
        "references": 27,
        "year": 2022
    },
    {
        "title": "Few-Shot Diffusion Models",
        "abstract": "Denoising diffusion probabilistic models (DDPM) are powerful hierarchical latent variable models with remarkable sample generation quality and training stability. These properties can be attributed to parameter sharing in the generative hierarchy, as well as a parameter-free diffusion-based inference procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a framework for few-shot generation leveraging conditional DDPMs. FSDMs are trained to adapt the generative process conditioned on a small set of images from a given class by aggregating image patch information using a set-based Vision Transformer (ViT). At test time, the model is able to generate samples from previously unseen classes conditioned on as few as 5 samples from that class. We empirically show that FSDM can perform few-shot generation and transfer to new datasets. We benchmark variants of our method on complex vision datasets for few-shot learning and compare to unconditional and conditional DDPM baselines. Additionally, we show how conditioning the model on patch-based input set information improves training convergence.",
        "authors": [
            "Giorgio Giannone",
            "Didrik Nielsen",
            "O. Winther"
        ],
        "citations": 49,
        "references": 112,
        "year": 2022
    },
    {
        "title": "From Points to Functions: Infinite-dimensional Representations in Diffusion Models",
        "abstract": "Diffusion-based generative models learn to iteratively transfer unstructured noise to a complex target distribution as opposed to Generative Adversarial Networks (GANs) or the decoder of Variational Autoencoders (VAEs) which produce samples from the target distribution in a single step. Thus, in diffusion models every sample is naturally connected to a random trajectory which is a solution to a learned stochastic differential equation (SDE). Generative models are only concerned with the final state of this trajectory that delivers samples from the desired distribution. Abstreiter et. al showed that these stochastic trajectories can be seen as continuous filters that wash out information along the way. Consequently, it is reasonable to ask if there is an intermediate time step at which the preserved information is optimal for a given downstream task. In this work, we show that a combination of information content from different time steps gives a strictly better representation for the downstream task. We introduce an attention and recurrence based modules that ``learn to mix'' information content of various time-steps such that the resultant representation leads to superior performance in downstream tasks.",
        "authors": [
            "Sarthak Mittal",
            "Guillaume Lajoie",
            "S. Bauer",
            "Arash Mehrjou"
        ],
        "citations": 31,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models",
        "abstract": "During image editing, existing deep generative models tend to re-synthesize the entire output from scratch, including the unedited regions. This leads to a significant waste of computation, especially for minor editing operations. In this work, we present Spatially Sparse Inference (SSI), a general-purpose technique that selectively performs computation for edited regions and accelerates various generative models, including both conditional GANs and diffusion models. Our key observation is that users prone to gradually edit the input image. This motivates us to cache and reuse the feature maps of the original image. Given an edited image, we sparsely apply the convolutional filters to the edited regions while reusing the cached features for the unedited areas. Based on our algorithm, we further propose Sparse Incremental Generative Engine (SIGE) to convert the computation reduction to latency reduction on off-the-shelf hardware. With about 1%-area edits, SIGE accelerates DDPM by <inline-formula><tex-math notation=\"LaTeX\">$3.0\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mn>0</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq1-3316020.gif\"/></alternatives></inline-formula> on NVIDIA RTX 3090 and <inline-formula><tex-math notation=\"LaTeX\">$4.6\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>4</mml:mn><mml:mo>.</mml:mo><mml:mn>6</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq2-3316020.gif\"/></alternatives></inline-formula> on Apple M1 Pro GPU, Stable Diffusion by <inline-formula><tex-math notation=\"LaTeX\">$7.2\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>7</mml:mn><mml:mo>.</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq3-3316020.gif\"/></alternatives></inline-formula> on 3090, and GauGAN by <inline-formula><tex-math notation=\"LaTeX\">$5.6\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>5</mml:mn><mml:mo>.</mml:mo><mml:mn>6</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq4-3316020.gif\"/></alternatives></inline-formula> on 3090 and <inline-formula><tex-math notation=\"LaTeX\">$5.2\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>5</mml:mn><mml:mo>.</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq5-3316020.gif\"/></alternatives></inline-formula> on M1 Pro GPU. Compared to our conference paper, we enhance SIGE to accommodate attention layers and apply it to Stable Diffusion. Additionally, we offer support for Apple M1 Pro GPU and include more results to substantiate the efficacy of our method.",
        "authors": [
            "Muyang Li",
            "Ji Lin",
            "Chenlin Meng",
            "Stefano Ermon",
            "Song Han",
            "Jun-Yan Zhu"
        ],
        "citations": 33,
        "references": 112,
        "year": 2022
    },
    {
        "title": "Deep Latent State Space Models for Time-Series Generation",
        "abstract": "Methods based on ordinary differential equations (ODEs) are widely used to build generative models of time-series. In addition to high computational overhead due to explicitly computing hidden states recurrence, existing ODE-based models fall short in learning sequence data with sharp transitions - common in many real-world systems - due to numerical challenges during optimization. In this work, we propose LS4, a generative model for sequences with latent variables evolving according to a state space ODE to increase modeling capacity. Inspired by recent deep state space models (S4), we achieve speedups by leveraging a convolutional representation of LS4 which bypasses the explicit evaluation of hidden states. We show that LS4 significantly outperforms previous continuous-time generative models in terms of marginal distribution, classification, and prediction scores on real-world datasets in the Monash Forecasting Repository, and is capable of modeling highly stochastic data with sharp temporal transitions. LS4 sets state-of-the-art for continuous-time latent generative models, with significant improvement of mean squared error and tighter variational lower bounds on irregularly-sampled datasets, while also being x100 faster than other baselines on long sequences.",
        "authors": [
            "Linqi Zhou",
            "Michael Poli",
            "Winnie Xu",
            "Stefano Massaroli",
            "Stefano Ermon"
        ],
        "citations": 32,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Benchmarking deep learning-based models on nanophotonic inverse design problems",
        "abstract": "Photonic inverse design concerns the problem of finding photonic structures with target optical properties. However, tra-ditional methods based on optimization algorithms are time-consuming and computationally expensive. Recently, deep learning-based approaches have been developed to tackle the problem of inverse design efficiently. Although most of these neural network models have demonstrated high accuracy in different inverse design problems, no previous study has examined the potential effects under given constraints in nanomanufacturing. Additionally, the relative strength of different deep learning-based inverse design approaches has not been fully investigated. Here, we benchmark three commonly used deep learning models in inverse design: Tandem networks, Variational Auto-Encoders, and Generative Adversarial Networks. We provide detailed comparisons in terms of their accuracy, diversity, and robustness. We find that tandem networks and Variational Auto-Encoders give the best accuracy, while Generative Adversarial Networks lead to the most diverse predictions. Our findings could serve as a guideline for researchers to select the model that can best suit their design criteria and fabrication considerations. In addition, our code and data are publicly available, which could be used for future inverse design model development and benchmarking. nanophotonic inverse design problems. Opto-Electron Sci 210012",
        "authors": [
            "Taigao Ma",
            "Mustafa Tobah",
            "Haozhu Wang",
            "L. J. Guo"
        ],
        "citations": 51,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Grad-StyleSpeech: Any-Speaker Adaptive Text-to-Speech Synthesis with Diffusion Models",
        "abstract": "There has been a significant progress in Text-To-Speech (TTS) synthesis technology in recent years, thanks to the advancement in neural generative modeling. However, existing methods on any-speaker adaptive TTS have achieved unsatisfactory performance, due to their suboptimal accuracy in mimicking the target speakers’ styles. In this work, we present Grad-StyleSpeech, which is an any-speaker adaptive TTS framework that is based on a diffusion model that can generate highly natural speech with extremely high similarity to target speakers’ voice, given a few seconds of reference speech. Grad-StyleSpeech significantly outperforms recent speaker-adaptive TTS baselines on English benchmarks. Audio samples are available at https://nardien.github.io/grad-stylespeech-demo.",
        "authors": [
            "Minki Kang",
            "Dong Min",
            "Sung Ju Hwang"
        ],
        "citations": 45,
        "references": 26,
        "year": 2022
    },
    {
        "title": "Diffusion Models for Counterfactual Explanations",
        "abstract": "Counterfactual explanations have shown promising results as a post-hoc framework to make image classifiers more explainable. In this paper, we propose DiME, a method allowing the generation of counterfactual images using the recent diffusion models. By leveraging the guided generative diffusion process, our proposed methodology shows how to use the gradients of the target classifier to generate counterfactual explanations of input instances. Further, we analyze current approaches to evaluate spurious correlations and extend the evaluation measurements by proposing a new metric: Correlation Difference. Our experimental validations show that the proposed algorithm surpasses previous State-of-the-Art results on 5 out of 6 metrics on CelebA.",
        "authors": [
            "Guillaume Jeanneret",
            "Loïc Simon",
            "F. Jurie"
        ],
        "citations": 45,
        "references": 78,
        "year": 2022
    },
    {
        "title": "Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning",
        "abstract": "Automatically discovering failures in vision models under real-world settings remains an open challenge. This work demonstrates how off-the-shelf, large-scale, image-to-text and text-to-image models, trained on vast amounts of data, can be leveraged to automatically find such failures. In essence, a conditional text-to-image generative model is used to generate large amounts of synthetic, yet realistic, inputs given a ground-truth label. Misclassified inputs are clustered and a captioning model is used to describe each cluster. Each cluster's description is used in turn to generate more inputs and assess whether specific clusters induce more failures than expected. We use this pipeline to demonstrate that we can effectively interrogate classifiers trained on ImageNet to find specific failure cases and discover spurious correlations. We also show that we can scale the approach to generate adversarial datasets targeting specific classifier architectures. This work serves as a proof-of-concept demonstrating the utility of large-scale generative models to automatically discover bugs in vision models in an open-ended manner. We also describe a number of limitations and pitfalls related to this approach.",
        "authors": [
            "Olivia Wiles",
            "Isabela Albuquerque",
            "Sven Gowal"
        ],
        "citations": 39,
        "references": 73,
        "year": 2022
    },
    {
        "title": "Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis",
        "abstract": "Recently, diffusion models have shown remarkable results in image synthesis by gradually removing noise and amplifying signals. Although the simple generative process surprisingly works well, is this the best way to generate image data? For instance, despite the fact that human perception is more sensitive to the low frequencies of an image, diffusion models themselves do not consider any relative importance of each frequency component. Therefore, to incorporate the inductive bias for image data, we propose a novel generative process that synthesizes images in a coarse-to-fine manner. First, we generalize the standard diffusion models by enabling diffusion in a rotated coordinate system with different velocities for each component of the vector. We further propose a blur diffusion as a special case, where each frequency component of an image is diffused at different speeds. Specifically, the proposed blur diffusion consists of a forward process that blurs an image and adds noise gradually, after which a corresponding reverse process deblurs an image and removes noise progressively. Experiments show that the proposed model outperforms the previous method in FID on LSUN bedroom and church datasets. Code is available at https://github.com/sangyun884/blur-diffusion.",
        "authors": [
            "Sangyun Lee",
            "Hyungjin Chung",
            "Jaehyeon Kim",
            "Jong-Chul Ye"
        ],
        "citations": 42,
        "references": 23,
        "year": 2022
    },
    {
        "title": "Classifier-Free Diffusion Guidance",
        "abstract": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
        "authors": [
            "Jonathan Ho"
        ],
        "citations": 1000,
        "references": 25,
        "year": 2022
    },
    {
        "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
        "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.",
        "authors": [
            "Patrick Esser",
            "Sumith Kulal",
            "A. Blattmann",
            "Rahim Entezari",
            "Jonas Muller",
            "Harry Saini",
            "Yam Levi",
            "Dominik Lorenz",
            "Axel Sauer",
            "Frederic Boesel",
            "Dustin Podell",
            "Tim Dockhorn",
            "Zion English",
            "Kyle Lacey",
            "Alex Goodwin",
            "Yannik Marek",
            "Robin Rombach"
        ],
        "citations": 475,
        "references": 75,
        "year": 2024
    },
    {
        "title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
        "abstract": "Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.",
        "authors": [
            "Amir Hertz",
            "Ron Mokady",
            "J. Tenenbaum",
            "Kfir Aberman",
            "Y. Pritch",
            "D. Cohen-Or"
        ],
        "citations": 1000,
        "references": 68,
        "year": 2022
    },
    {
        "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps",
        "abstract": "Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a $4\\sim 16\\times$ speedup compared with previous state-of-the-art training-free samplers on various datasets.",
        "authors": [
            "Cheng Lu",
            "Yuhao Zhou",
            "Fan Bao",
            "Jianfei Chen",
            "Chongxuan Li",
            "Jun Zhu"
        ],
        "citations": 1000,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Multi-Concept Customization of Text-to-Image Diffusion",
        "abstract": "While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~ 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient.",
        "authors": [
            "Nupur Kumari",
            "Bin Zhang",
            "Richard Zhang",
            "Eli Shechtman",
            "Jun-Yan Zhu"
        ],
        "citations": 651,
        "references": 96,
        "year": 2022
    },
    {
        "title": "SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion",
        "abstract": "We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works.",
        "authors": [
            "Vikram S. Voleti",
            "Chun-Han Yao",
            "Mark Boss",
            "Adam Letts",
            "David Pankratz",
            "Dmitry Tochilkin",
            "Christian Laforte",
            "Robin Rombach",
            "Varun Jampani"
        ],
        "citations": 100,
        "references": 60,
        "year": 2024
    },
    {
        "title": "CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model",
        "abstract": "Feed-forward 3D generative models like the Large Reconstruction Model (LRM) have demonstrated exceptional generation speed. However, the transformer-based methods do not leverage the geometric priors of the triplane component in their architecture, often leading to sub-optimal quality given the limited size of 3D data and slow training. In this work, we present the Convolutional Reconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D generative model. Recognizing the limitations posed by sparse 3D data, we highlight the necessity of integrating geometric priors into network design. CRM builds on the key observation that the visualization of triplane exhibits spatial correspondence of six orthographic images. First, it generates six orthographic view images from a single input image, then feeds these images into a convolutional U-Net, leveraging its strong pixel-level alignment capabilities and significant bandwidth to create a high-resolution triplane. CRM further employs Flexicubes as geometric representation, facilitating direct end-to-end optimization on textured meshes. Overall, our model delivers a high-fidelity textured mesh from an image in just 10 seconds, without any test-time optimization.",
        "authors": [
            "Zhengyi Wang",
            "Yikai Wang",
            "Yifei Chen",
            "Chendong Xiang",
            "Shuo Chen",
            "Dajiang Yu",
            "Chongxuan Li",
            "Hang Su",
            "Jun Zhu"
        ],
        "citations": 84,
        "references": 70,
        "year": 2024
    },
    {
        "title": "Zero-shot Image-to-Image Translation",
        "abstract": "Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse, high-quality images. However, directly applying these models for real image editing remains challenging for two reasons. First, it is hard for users to craft a perfect text prompt depicting every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. In this work, we introduce pix2pix-zero, an image-to-image translation method that can preserve the original image’s content without manual prompting. We first automatically discover editing directions that reflect desired edits in the text embedding space. To preserve the content structure, we propose cross-attention guidance, which aims to retain the cross-attention maps of the input image throughout the diffusion process. Finally, to enable interactive editing, we distill the diffusion model into a fast conditional GAN. We conduct extensive experiments and show that our method outperforms existing and concurrent works for both real and synthetic image editing. In addition, our method does not need additional training for these edits and can directly use the existing pre-trained text-to-image diffusion model.",
        "authors": [
            "Gaurav Parmar",
            "Krishna Kumar Singh",
            "Richard Zhang",
            "Yijun Li",
            "Jingwan Lu",
            "Jun-Yan Zhu"
        ],
        "citations": 353,
        "references": 77,
        "year": 2023
    },
    {
        "title": "ChatGPT in medicine: an overview of its applications, advantages, limitations, future prospects, and ethical considerations",
        "abstract": "This paper presents an analysis of the advantages, limitations, ethical considerations, future prospects, and practical applications of ChatGPT and artificial intelligence (AI) in the healthcare and medical domains. ChatGPT is an advanced language model that uses deep learning techniques to produce human-like responses to natural language inputs. It is part of the family of generative pre-training transformer (GPT) models developed by OpenAI and is currently one of the largest publicly available language models. ChatGPT is capable of capturing the nuances and intricacies of human language, allowing it to generate appropriate and contextually relevant responses across a broad spectrum of prompts. The potential applications of ChatGPT in the medical field range from identifying potential research topics to assisting professionals in clinical and laboratory diagnosis. Additionally, it can be used to help medical students, doctors, nurses, and all members of the healthcare fraternity to know about updates and new developments in their respective fields. The development of virtual assistants to aid patients in managing their health is another important application of ChatGPT in medicine. Despite its potential applications, the use of ChatGPT and other AI tools in medical writing also poses ethical and legal concerns. These include possible infringement of copyright laws, medico-legal complications, and the need for transparency in AI-generated content. In conclusion, ChatGPT has several potential applications in the medical and healthcare fields. However, these applications come with several limitations and ethical considerations which are presented in detail along with future prospects in medicine and healthcare.",
        "authors": [
            "Tirth Dave",
            "Sai Anirudh Athaluri",
            "Satyam Singh"
        ],
        "citations": 579,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Objaverse: A Universe of Annotated 3D Objects",
        "abstract": "Massive data corpora like WebText, Wikipedia, Conceptual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today's benchmarks. A notable omisslion within this family of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K + (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new directions for research and enable new applications across the field of AI.",
        "authors": [
            "Matt Deitke",
            "Dustin Schwenk",
            "Jordi Salvador",
            "Luca Weihs",
            "Oscar Michel",
            "Eli VanderBilt",
            "Ludwig Schmidt",
            "Kiana Ehsani",
            "Aniruddha Kembhavi",
            "Ali Farhadi"
        ],
        "citations": 660,
        "references": 88,
        "year": 2022
    },
    {
        "title": "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing",
        "abstract": "The diffusion-based generative models have achieved remarkable success in text-based image generation. However, since it contains enormous randomness in generation progress, it is still challenging to apply such models for real-world visual content editing, especially in videos. In this paper, we propose FateZero, a zero-shot text-based editing method on real-world videos without per-prompt training or use-specific mask. To edit videos consistently, we propose several techniques based on the pre-trained models. Firstly, in contrast to the straightforward DDIM inversion technique, our approach captures intermediate attention maps during inversion, which effectively retain both structural and motion information. These maps are directly fused in the editing process rather than generated during denoising. To further minimize semantic leakage of the source video, we then fuse self-attentions with a blending mask obtained by cross-attention features from the source prompt. Furthermore, we have implemented a reform of the self-attention mechanism in denoising UNet by introducing spatial-temporal attention to ensure frame consistency. Yet succinct, our method is the first one to show the ability of zero-shot text-driven video style and local attribute editing from the trained text-to-image model. We also have a better zero-shot shape-aware editing ability based on the text-to-video model [52]. Extensive experiments demonstrate our superior temporal consistency and editing capability than previous works.",
        "authors": [
            "Chenyang Qi",
            "Xiaodong Cun",
            "Yong Zhang",
            "Chenyang Lei",
            "Xintao Wang",
            "Ying Shan",
            "Qifeng Chen"
        ],
        "citations": 268,
        "references": 60,
        "year": 2023
    },
    {
        "title": "LRM: Large Reconstruction Model for Single Image to 3D",
        "abstract": "We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.",
        "authors": [
            "Yicong Hong",
            "Kai Zhang",
            "Jiuxiang Gu",
            "Sai Bi",
            "Yang Zhou",
            "Difan Liu",
            "Feng Liu",
            "Kalyan Sunkavalli",
            "Trung Bui",
            "Hao Tan"
        ],
        "citations": 258,
        "references": 101,
        "year": 2023
    },
    {
        "title": "Scientific discovery in the age of artificial intelligence",
        "abstract": null,
        "authors": [
            "Hanchen Wang",
            "Tianfan Fu",
            "Yuanqi Du",
            "Wenhao Gao",
            "Kexin Huang",
            "Ziming Liu",
            "P. Chandak",
            "Shengchao Liu",
            "Peter Van Katwyk",
            "Andreea Deac",
            "Anima Anandkumar",
            "K. Bergen",
            "Carla P. Gomes",
            "Shirley Ho",
            "Pushmeet Kohli",
            "Joan Lasenby",
            "J. Leskovec",
            "Tie-Yan Liu",
            "A. Manrai",
            "Debora S. Marks",
            "Bharath Ramsundar",
            "Le Song",
            "Jimeng Sun",
            "Jian Tang",
            "Petar Velickovic",
            "Max Welling",
            "Linfeng Zhang",
            "Connor W. Coley",
            "Y. Bengio",
            "M. Zitnik"
        ],
        "citations": 587,
        "references": 269,
        "year": 2023
    },
    {
        "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
        "abstract": "Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.",
        "authors": [
            "Chunyuan Li",
            "Cliff Wong",
            "Sheng Zhang",
            "Naoto Usuyama",
            "Haotian Liu",
            "Jianwei Yang",
            "Tristan Naumann",
            "Hoifung Poon",
            "Jianfeng Gao"
        ],
        "citations": 462,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Scaling up GANs for Text-to-Image Synthesis",
        "abstract": "The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL.E 2, autoregressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that naïvely increasing the capacity of the StyleGan architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.",
        "authors": [
            "Minguk Kang",
            "Jun-Yan Zhu",
            "Richard Zhang",
            "Jaesik Park",
            "Eli Shechtman",
            "Sylvain Paris",
            "Taesung Park"
        ],
        "citations": 364,
        "references": 115,
        "year": 2023
    },
    {
        "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
        "abstract": "Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling",
        "authors": [
            "Hyungjin Chung",
            "Jeongsol Kim",
            "Michael T. McCann",
            "M. Klasky",
            "J. C. Ye"
        ],
        "citations": 560,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Human Motion Diffusion Model",
        "abstract": "Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. https://guytevet.github.io/mdm-page/ .",
        "authors": [
            "Guy Tevet",
            "Sigal Raab",
            "Brian Gordon",
            "Yonatan Shafir",
            "Daniel Cohen-Or",
            "Amit H. Bermano"
        ],
        "citations": 575,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
        "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101",
        "authors": [
            "A. Ustun",
            "Viraat Aryabumi",
            "Zheng-Xin Yong",
            "Wei-Yin Ko",
            "Daniel D'souza",
            "Gbemileke Onilude",
            "Neel Bhandari",
            "Shivalika Singh",
            "Hui-Lee Ooi",
            "Amr Kayid",
            "Freddie Vargus",
            "Phil Blunsom",
            "Shayne Longpre",
            "Niklas Muennighoff",
            "Marzieh Fadaee",
            "Julia Kreutzer",
            "Sara Hooker"
        ],
        "citations": 139,
        "references": 159,
        "year": 2024
    },
    {
        "title": "Shap-E: Generating Conditional 3D Implicit Functions",
        "abstract": "We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.",
        "authors": [
            "Heewoo Jun",
            "Alex Nichol"
        ],
        "citations": 252,
        "references": 74,
        "year": 2023
    },
    {
        "title": "GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation",
        "abstract": "Despite recent advances in text-to-3D generative methods, there is a notable absence of reliable evaluation metrics. Existing metrics usually focus on a single criterion each, such as how well the asset aligned with the input text. These metrics lack the flexibility to generalize to different evaluation criteria and might not align well with human preferences. Conducting user preference studies is an alternative that offers both adaptability and human-aligned results. User studies, however, can be very ex-pensive to scale. This paper presents an automatic, ver-satile, and human-aligned evaluation metric for text-to-3D generative models. To this end, we first develop a prompt generator using GPT-4V to generate evaluating prompts, which serve as input to compare text-to-3D models. We further design a method instructing GPT-4V to compare two 3D assets according to user-defined crite-ria. Finally, we use these pairwise comparison results to assign these models Elo ratings. Experimental results suggest our metric strongly aligns with human preference across different evaluation criteria. Our code is available at https://github.com/3DTopia/GPTEval3D.",
        "authors": [
            "Tong Wu",
            "Guandao Yang",
            "Zhibing Li",
            "Kai Zhang",
            "Ziwei Liu",
            "Leonidas J. Guibas",
            "Dahua Lin",
            "Gordon Wetzstein"
        ],
        "citations": 64,
        "references": 73,
        "year": 2024
    },
    {
        "title": "Protein generation with evolutionary diffusion: sequence is all you need",
        "abstract": "Deep generative models are increasingly powerful tools for the in silico design of novel proteins. Recently, a family of generative models called diffusion models has demonstrated the ability to generate biologically plausible proteins that are dissimilar to any actual proteins seen in nature, enabling unprecedented capability and control in de novo protein design. However, current state-of-the-art diffusion models generate protein structures, which limits the scope of their training data and restricts generations to a small and biased subset of protein design space. Here, we introduce a general-purpose diffusion framework, EvoDiff, that combines evolutionary-scale data with the distinct conditioning capabilities of diffusion models for controllable protein generation in sequence space. EvoDiff generates high-fidelity, diverse, and structurally-plausible proteins that cover natural sequence and functional space. We show experimentally that EvoDiff generations express, fold, and exhibit expected secondary structure elements. Critically, EvoDiff can generate proteins inaccessible to structure-based models, such as those with disordered regions, while maintaining the ability to design scaffolds for functional structural motifs. We validate the universality of our sequence-based formulation by experimentally characterizing intrinsically-disordered mitochondrial targeting signals, metal-binding proteins, and protein binders designed using EvoDiff. We envision that EvoDiff will expand capabilities in protein engineering beyond the structure-function paradigm toward programmable, sequence-first design.",
        "authors": [
            "Sarah Alamdari",
            "Nitya Thakkar",
            "Rianne van den Berg",
            "Alex X. Lu",
            "Nicolo Fusi",
            "Ava P. Amini",
            "Kevin Kaichuang Yang"
        ],
        "citations": 64,
        "references": 60,
        "year": 2024
    },
    {
        "title": "Adversarial Diffusion Distillation",
        "abstract": "We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality. We use score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps. ADD is the first method to unlock single-step, real-time image synthesis with foundation models. Code and weights available under https://github.com/Stability-AI/generative-models and https://huggingface.co/stabilityai/ .",
        "authors": [
            "Axel Sauer",
            "Dominik Lorenz",
            "A. Blattmann",
            "Robin Rombach"
        ],
        "citations": 219,
        "references": 71,
        "year": 2023
    },
    {
        "title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation",
        "abstract": "We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: https://justimyhxu.github.io/projects/grm/.",
        "authors": [
            "Yinghao Xu",
            "Zifan Shi",
            "Wang Yifan",
            "Hansheng Chen",
            "Ceyuan Yang",
            "Sida Peng",
            "Yujun Shen",
            "Gordon Wetzstein"
        ],
        "citations": 86,
        "references": 111,
        "year": 2024
    },
    {
        "title": "Simulating 500 million years of evolution with a language model",
        "abstract": "More than three billion years of evolution have produced an image of biology encoded into the space of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from known proteins. We present ESM3, a frontier multimodal generative language model that reasons over the sequence, structure, and function of proteins. ESM3 can follow complex prompts combining its modalities and is highly responsive to biological alignment. We have prompted ESM3 to generate fluorescent proteins with a chain of thought. Among the generations that we synthesized, we found a bright fluorescent protein at far distance (58% identity) from known fluorescent proteins. Similarly distant natural fluorescent proteins are separated by over five hundred million years of evolution.",
        "authors": [
            "Thomas Hayes",
            "Roshan Rao",
            "Halil Akin",
            "Nicholas J Sofroniew",
            "Deniz Oktay",
            "Zeming Lin",
            "Robert Verkuil",
            "Vincent Q. Tran",
            "Jonathan Deaton",
            "Marius Wiggert",
            "Rohil Badkundri",
            "Irhum Shafkat",
            "Jun Gong",
            "Alexander Derry",
            "Raul S Molina",
            "Neil Thomas",
            "Yousuf A. Khan",
            "Chetan Mishra",
            "Carolyn Kim",
            "Liam J. Bartie",
            "Matthew Nemeth",
            "Patrick D. Hsu",
            "Tom Sercu",
            "Salvatore Candido",
            "Alexander Rives"
        ],
        "citations": 82,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation",
        "abstract": "Large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, synthesizing diverse images with highly complex visual concepts. However, a pivotal challenge in leveraging such models for real-world content creation is providing users with control over the generated content. In this paper, we present a new framework that takes text-to- image synthesis to the realm of image-to-image translation - given a guidance image and a target text prompt as input, our method harnesses the power of a pre-trained text-to-image diffusion model to generate a new image that complies with the target text, while preserving the semantic layout of the guidance image. Specifically, we observe and empirically demonstrate that fine-grained control over the generated structure can be achieved by manipulating spatial features and their self-attention inside the model. This results in a simple and effective approach, where features extracted from the guidance image are directly injected into the generation process of the translated image, requiring no training or fine-tuning. We demonstrate high-quality results on versatile text-guided image translation tasks, including translating sketches, rough drawings and animations into realistic images, changing the class and appearance of objects in a given image, and modifying global qualities such as lighting and color.",
        "authors": [
            "Narek Tumanyan",
            "Michal Geyer",
            "Shai Bagon",
            "Tali Dekel"
        ],
        "citations": 490,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis",
        "abstract": "Recent text-to-image generative models can generate high-fidelity images from text inputs, but the quality of these generated images cannot be accurately evaluated by existing evaluation metrics. To address this issue, we introduce Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human preferences on images from a wide range of sources. HPD v2 comprises 798,090 human preference choices on 433,760 pairs of images, making it the largest dataset of its kind. The text prompts and images are deliberately collected to eliminate potential bias, which is a common issue in previous datasets. By fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a scoring model that can more accurately predict human preferences on generated images. Our experiments demonstrate that HPS v2 generalizes better than previous metrics across various image distributions and is responsive to algorithmic improvements of text-to-image generative models, making it a preferable evaluation metric for these models. We also investigate the design of the evaluation prompts for text-to-image generative models, to make the evaluation stable, fair and easy-to-use. Finally, we establish a benchmark for text-to-image generative models using HPS v2, which includes a set of recent text-to-image models from the academic, community and industry. The code and dataset is available at https://github.com/tgxs002/HPSv2 .",
        "authors": [
            "Xiaoshi Wu",
            "Yiming Hao",
            "Keqiang Sun",
            "Yixiong Chen",
            "Feng Zhu",
            "Rui Zhao",
            "Hongsheng Li"
        ],
        "citations": 143,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
        "abstract": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.",
        "authors": [
            "Matt Le",
            "Apoorv Vyas",
            "Bowen Shi",
            "B. Karrer",
            "Leda Sari",
            "Rashel Moritz",
            "Mary Williamson",
            "Vimal Manohar",
            "Yossi Adi",
            "Jay Mahadeokar",
            "Wei-Ning Hsu"
        ],
        "citations": 208,
        "references": 82,
        "year": 2023
    },
    {
        "title": "AlphaFold Meets Flow Matching for Generating Protein Ensembles",
        "abstract": "The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at https://github.com/bjing2016/alphaflow.",
        "authors": [
            "Bowen Jing",
            "Bonnie Berger",
            "T. Jaakkola"
        ],
        "citations": 47,
        "references": 66,
        "year": 2024
    },
    {
        "title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery",
        "abstract": "The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical\"hard\"prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also\"soft\"prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.",
        "authors": [
            "Yuxin Wen",
            "Neel Jain",
            "John Kirchenbauer",
            "Micah Goldblum",
            "Jonas Geiping",
            "T. Goldstein"
        ],
        "citations": 191,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Stochastic Interpolants: A Unifying Framework for Flows and Diffusions",
        "abstract": "A class of generative models that unifies flow-based and diffusion-based methods is introduced. These models extend the framework proposed in Albergo&Vanden-Eijnden (2023), enabling the use of a broad class of continuous-time stochastic processes called `stochastic interpolants' to bridge any two arbitrary probability density functions exactly in finite time. These interpolants are built by combining data from the two prescribed densities with an additional latent variable that shapes the bridge in a flexible way. The time-dependent probability density function of the stochastic interpolant is shown to satisfy a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion. Upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on probability flow equations or stochastic differential equations with an adjustable level of noise. The drift coefficients entering these models are time-dependent velocity fields characterized as the unique minimizers of simple quadratic objective functions, one of which is a new objective for the score of the interpolant density. Remarkably, we show that minimization of these quadratic objectives leads to control of the likelihood for any of our generative models built upon stochastic dynamics. By contrast, we establish that generative models based upon a deterministic dynamics must, in addition, control the Fisher divergence between the target and the model. We also construct estimators for the likelihood and the cross-entropy of interpolant-based generative models, discuss connections with other stochastic bridges, and demonstrate that such models recover the Schr\\\"odinger bridge between the two target densities when explicitly optimizing over the interpolant.",
        "authors": [
            "M. S. Albergo",
            "Nicholas M. Boffi",
            "E. Vanden-Eijnden"
        ],
        "citations": 181,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Composer: Creative and Controllable Image Synthesis with Composable Conditions",
        "abstract": "Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.",
        "authors": [
            "Lianghua Huang",
            "Di Chen",
            "Yu Liu",
            "Yujun Shen",
            "Deli Zhao",
            "Jingren Zhou"
        ],
        "citations": 241,
        "references": 51,
        "year": 2023
    },
    {
        "title": "GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image",
        "abstract": "We introduce GeoWizard, a new generative foundation model designed for estimating geometric attributes, e.g., depth and normals, from single images. While significant research has already been conducted in this area, the progress has been substantially limited by the low diversity and poor quality of publicly available datasets. As a result, the prior works either are constrained to limited scenarios or suffer from the inability to capture geometric details. In this paper, we demonstrate that generative models, as opposed to traditional discriminative models (e.g., CNNs and Transformers), can effectively address the inherently ill-posed problem. We further show that leveraging diffusion priors can markedly improve generalization, detail preservation, and efficiency in resource usage. Specifically, we extend the original stable diffusion model to jointly predict depth and normal, allowing mutual information exchange and high consistency between the two representations. More importantly, we propose a simple yet effective strategy to segregate the complex data distribution of various scenes into distinct sub-distributions. This strategy enables our model to recognize different scene layouts, capturing 3D geometry with remarkable fidelity. GeoWizard sets new benchmarks for zero-shot depth and normal prediction, significantly enhancing many downstream applications such as 3D reconstruction, 2D content creation, and novel viewpoint synthesis.",
        "authors": [
            "Xiao Fu",
            "Wei Yin",
            "Mu Hu",
            "Kaixuan Wang",
            "Yuexin Ma",
            "Ping Tan",
            "Shaojie Shen",
            "Dahua Lin",
            "Xiaoxiao Long"
        ],
        "citations": 44,
        "references": 80,
        "year": 2024
    },
    {
        "title": "Diffusion Self-Guidance for Controllable Image Generation",
        "abstract": "Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images. For results and an interactive demo, see our project page at https://dave.ml/selfguidance/",
        "authors": [
            "Dave Epstein",
            "A. Jabri",
            "Ben Poole",
            "Alexei A. Efros",
            "Aleksander Holynski"
        ],
        "citations": 182,
        "references": 40,
        "year": 2023
    },
    {
        "title": "DreamBooth3D: Subject-Driven Text-to-3D Generation",
        "abstract": "We present DreamBooth3D, an approach to personalize text-to-3D generative models from as few as 3-6 casually captured images of a subject. Our approach combines recent advances in personalizing text-to-image models (DreamBooth) with text-to-3D generation (DreamFusion). We find that naïvely combining these methods fails to yield satisfactory subject-specific 3D assets due to personalized text-to-image models overfitting to the input viewpoints of the subject. We overcome this through a 3-stage optimization strategy where we jointly leverage the 3D consistency of neural radiance fields together with the personalization capability of text-to-image models. Our method can produce high-quality, subject-specific 3D assets with text-driven modifications such as novel poses, colors and attributes that are not seen in any of the input images of the subject. More results are available at our project page: https://dreambooth3d.github.io",
        "authors": [
            "Amit Raj",
            "S. Kaza",
            "Ben Poole",
            "Michael Niemeyer",
            "Nataniel Ruiz",
            "B. Mildenhall",
            "Shiran Zada",
            "Kfir Aberman",
            "Michael Rubinstein",
            "J. Barron",
            "Yuanzhen Li",
            "Varun Jampani"
        ],
        "citations": 186,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow",
        "abstract": "We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions \\pi_0 and \\pi_1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from \\pi_0 and \\pi_1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of \\pi_0 and \\pi_1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.",
        "authors": [
            "Xingchao Liu",
            "Chengyue Gong",
            "Qiang Liu"
        ],
        "citations": 494,
        "references": 100,
        "year": 2022
    },
    {
        "title": "Diffusion-based Generation, Optimization, and Planning in 3D Scenes",
        "abstract": "We introduce the SceneDiffuser, a conditional generative model for 3D scene understanding. SceneDiffuser provides a unified model for solving scene-conditioned generation, optimization, and planning. In contrast to prior work, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy, SceneDiffuser jointly formulates the scene-aware generation, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differentiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previous scene-conditioned generative models. We evaluate the SceneDiffuser on various 3D scene understanding tasks, including human pose and motion generation, dexterous grasp generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant improvements compared with previous models, demonstrating the tremendous potential of the SceneDiffuser for the broad community of 3D scene understanding.",
        "authors": [
            "Siyuan Huang",
            "Zan Wang",
            "Puhao Li",
            "Baoxiong Jia",
            "Tengyu Liu",
            "Yixin Zhu",
            "Wei Liang",
            "Song-Chun Zhu"
        ],
        "citations": 158,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
        "abstract": "While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.",
        "authors": [
            "Alex Nichol",
            "Heewoo Jun",
            "Prafulla Dhariwal",
            "Pamela Mishkin",
            "Mark Chen"
        ],
        "citations": 488,
        "references": 79,
        "year": 2022
    },
    {
        "title": "TokenFlow: Consistent Diffusion Features for Consistent Video Editing",
        "abstract": "The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and motion of the input video. Our method is based on a key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos. Webpage: https://diffusion-tokenflow.github.io/",
        "authors": [
            "Michal Geyer",
            "Omer Bar-Tal",
            "Shai Bagon",
            "Tali Dekel"
        ],
        "citations": 189,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Shaping the Future of Education: Exploring the Potential and Consequences of AI and ChatGPT in Educational Settings",
        "abstract": "Over the last decade, technological advancements, especially artificial intelligence (AI), have significantly transformed educational practices. Recently, the development and adoption of Generative Pre-trained Transformers (GPT), particularly OpenAI’s ChatGPT, has sparked considerable interest. The unprecedented capabilities of these models, such as generating humanlike text and facilitating automated conversations, have broad implications in various sectors, including education and health. Despite their immense potential, concerns regarding their widespread use and opacity have been raised within the scientific community. ChatGPT, the latest version of the GPT series, has displayed remarkable proficiency, passed the US bar law exam, and amassed over a million subscribers shortly after its launch. However, its impact on the education sector has elicited mixed reactions, with some educators heralding it as a progressive step and others raising alarms over its potential to reduce analytical skills and promote misconduct. This paper aims to delve into these discussions, exploring the potential and problems associated with applying advanced AI models in education. It builds on extant literature and contributes to understanding how these technologies reshape educational norms in the “new AI gold rush” era.",
        "authors": [
            "Simone Grassini"
        ],
        "citations": 397,
        "references": 111,
        "year": 2023
    },
    {
        "title": "High-Fidelity Audio Compression with Improved RVQGAN",
        "abstract": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.",
        "authors": [
            "Rithesh Kumar",
            "Prem Seetharaman",
            "Alejandro Luebs",
            "I. Kumar",
            "Kundan Kumar"
        ],
        "citations": 210,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Exploiting Diffusion Prior for Real-World Image Super-Resolution",
        "abstract": "We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution (SR). Specifically, by employing our time-aware encoder, we can achieve promising restoration results without altering the pre-trained synthesis model, thereby preserving the generative prior and minimizing training cost. To remedy the loss of fidelity caused by the inherent stochasticity of diffusion models, we introduce a controllable feature wrapping module that allows users to balance quality and fidelity by simply adjusting a scalar value during the inference process. Moreover, we develop a progressive aggregation sampling strategy to overcome the fixed-size constraints of pre-trained diffusion models, enabling adaptation to resolutions of any size. A comprehensive evaluation of our method using both synthetic and real-world benchmarks demonstrates its superiority over current state-of-the-art approaches.",
        "authors": [
            "Jianyi Wang",
            "Zongsheng Yue",
            "Shangchen Zhou",
            "Kelvin C. K. Chan",
            "Chen Change Loy"
        ],
        "citations": 180,
        "references": 106,
        "year": 2023
    },
    {
        "title": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation",
        "abstract": "Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on image animation benchmarks, achieving state-of-the-art results.",
        "authors": [
            "Liucheng Hu",
            "Xin Gao",
            "Peng Zhang",
            "Ke Sun",
            "Bang Zhang",
            "Liefeng Bo"
        ],
        "citations": 204,
        "references": 66,
        "year": 2023
    },
    {
        "title": "LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching",
        "abstract": "The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across var-ious real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency. Our code is available at: EnVision-Research/LucidDreamer",
        "authors": [
            "Yixun Liang",
            "Xin Yang",
            "Jiantao Lin",
            "Haodong Li",
            "Xiaogang Xu",
            "Yingcong Chen"
        ],
        "citations": 122,
        "references": 50,
        "year": 2023
    },
    {
        "title": "De novo design of protein structure and function with RFdiffusion",
        "abstract": null,
        "authors": [
            "Joseph L. Watson",
            "David Juergens",
            "N. Bennett",
            "Brian L. Trippe",
            "Jason Yim",
            "Helen E. Eisenach",
            "Woody Ahern",
            "Andrew J. Borst",
            "Robert J. Ragotte",
            "L. Milles",
            "B. Wicky",
            "Nikita Hanikel",
            "S. Pellock",
            "A. Courbet",
            "W. Sheffler",
            "Jue Wang",
            "Preetham Venkatesh",
            "Isaac Sappington",
            "Susana Vázquez Torres",
            "Anna Lauko",
            "Valentin De Bortoli",
            "Emile Mathieu",
            "Sergey Ovchinnikov",
            "R. Barzilay",
            "T. Jaakkola",
            "F. DiMaio",
            "M. Baek",
            "D. Baker"
        ],
        "citations": 321,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model",
        "abstract": "We report Zero123++, an image-conditioned diffusion model for generating 3D-consistent multi-view images from a single input view. To take full advantage of pretrained 2D generative priors, we develop various conditioning and training schemes to minimize the effort of finetuning from off-the-shelf image diffusion models such as Stable Diffusion. Zero123++ excels in producing high-quality, consistent multi-view images from a single image, overcoming common issues like texture degradation and geometric misalignment. Furthermore, we showcase the feasibility of training a ControlNet on Zero123++ for enhanced control over the generation process. The code is available at https://github.com/SUDO-AI-3D/zero123plus.",
        "authors": [
            "Ruoxi Shi",
            "Hansheng Chen",
            "Zhuoyang Zhang",
            "Minghua Liu",
            "Chao Xu",
            "Xinyue Wei",
            "Linghao Chen",
            "Chong Zeng",
            "Hao Su"
        ],
        "citations": 233,
        "references": 26,
        "year": 2023
    },
    {
        "title": "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation",
        "abstract": "Despite the stunning ability to generate high-quality images by recent text-to-image models, current approaches often struggle to effectively compose objects with different attributes and relationships into a complex and coherent scene. We propose T2I-CompBench, a comprehensive benchmark for open-world compositional text-to-image generation, consisting of 6,000 compositional text prompts from 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). We further propose several evaluation metrics specifically designed to evaluate compositional text-to-image generation and explore the potential and limitations of multimodal LLMs for evaluation. We introduce a new approach, Generative mOdel fine-tuning with Reward-driven Sample selection (GORS), to boost the compositional text-to-image generation abilities of pretrained text-to-image models. Extensive experiments and evaluations are conducted to benchmark previous methods on T2I-CompBench, and to validate the effectiveness of our proposed evaluation metrics and GORS approach. Project page is available at https://karine-h.github.io/T2I-CompBench/.",
        "authors": [
            "Kaiyi Huang",
            "Kaiyue Sun",
            "Enze Xie",
            "Zhenguo Li",
            "Xihui Liu"
        ],
        "citations": 144,
        "references": 53,
        "year": 2023
    },
    {
        "title": "GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation",
        "abstract": "Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules.",
        "authors": [
            "Minkai Xu",
            "Lantao Yu",
            "Yang Song",
            "Chence Shi",
            "Stefano Ermon",
            "Jian Tang"
        ],
        "citations": 427,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation",
        "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
        "authors": [
            "Lijun Yu",
            "José Lezama",
            "N. B. Gundavarapu",
            "Luca Versari",
            "Kihyuk Sohn",
            "David C. Minnen",
            "Yong Cheng",
            "Agrim Gupta",
            "Xiuye Gu",
            "Alexander G. Hauptmann",
            "Boqing Gong",
            "Ming-Hsuan Yang",
            "Irfan Essa",
            "David A. Ross",
            "Lu Jiang"
        ],
        "citations": 158,
        "references": 82,
        "year": 2023
    },
    {
        "title": "SpectralGPT: Spectral Remote Sensing Foundation Model",
        "abstract": "The foundation model has recently garnered significant attention due to its potential to revolutionize the field of visual representation learning in a self-supervised manner. While most foundation models are tailored to effectively process RGB images for various visual tasks, there is a noticeable gap in research focused on spectral data, which offers valuable information for scene understanding, especially in remote sensing (RS) applications. To fill this gap, we created for the first time a universal RS foundation model, named SpectralGPT, which is purpose-built to handle spectral RS images using a novel 3D generative pretrained transformer (GPT). Compared to existing foundation models, SpectralGPT 1) accommodates input images with varying sizes, resolutions, time series, and regions in a progressive training fashion, enabling full utilization of extensive RS Big Data; 2) leverages 3D token generation for spatial-spectral coupling; 3) captures spectrally sequential patterns via multi-target reconstruction; and 4) trains on one million spectral RS images, yielding models with over 600 million parameters. Our evaluation highlights significant performance improvements with pretrained SpectralGPT models, signifying substantial potential in advancing spectral RS Big Data applications within the field of geoscience across four downstream tasks: single/multi-label scene classification, semantic segmentation, and change detection.",
        "authors": [
            "D. Hong",
            "Bing Zhang",
            "Xuyang Li",
            "Yuxuan Li",
            "Chenyu Li",
            "Jing Yao",
            "N. Yokoya",
            "Hao Li",
            "Pedram Ghamisi",
            "Xiuping Jia",
            "Antonio J. Plaza",
            "Paolo Gamba",
            "J. Benediktsson",
            "J. Chanussot"
        ],
        "citations": 275,
        "references": 53,
        "year": 2023
    },
    {
        "title": "In-context Learning and Induction Heads",
        "abstract": "“Induction heads” are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] → [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all “incontext learning” in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in incontext learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence. We recommend reading this paper as an HTML article. As Transformer generative models continue to scale and gain increasing real world use, addressing their associated safety problems becomes increasingly important. Mechanistic interpretability – attempting to reverse engineer the detailed computations performed by the model – offers one possible avenue for addressing these safety issues. If we can understand the internal structures that cause Transformer models to produce the outputs they do, then we may be able to address current safety problems more systematically, as well as anticipating safety problems in future more powerful models. [1, 2, 3, 4, 5]",
        "authors": [
            "Catherine Olsson",
            "Nelson Elhage",
            "Neel Nanda",
            "Nicholas Joseph",
            "Nova Dassarma",
            "T. Henighan",
            "Benjamin Mann",
            "Amanda Askell",
            "Yuntao Bai",
            "Anna Chen",
            "Tom Conerly",
            "Dawn Drain",
            "Deep Ganguli",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Scott Johnston",
            "Andy Jones",
            "John Kernion",
            "Liane Lovitt",
            "Kamal Ndousse",
            "Dario Amodei",
            "Tom B. Brown",
            "Jack Clark",
            "Jared Kaplan",
            "Sam McCandlish",
            "C. Olah"
        ],
        "citations": 394,
        "references": 13,
        "year": 2022
    },
    {
        "title": "GPTScore: Evaluate as You Desire",
        "abstract": "Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models.Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently.This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small) to 175B (e.g., GPT3).Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions.This nature helps us overcome several long-standing challenges in text evaluation–how to achieve customized, multi-faceted evaluation without model training. We make our code publicly available.",
        "authors": [
            "Jinlan Fu",
            "See-Kiong Ng",
            "Zhengbao Jiang",
            "Pengfei Liu"
        ],
        "citations": 223,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
        "abstract": "The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has stronger multimodal compositional reasoning abilities than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. These models approach the performance of SOTA discriminative classifiers and exhibit strong \"effective robustness\" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations on our website: diffusion-classifier.github.io/",
        "authors": [
            "Alexander C. Li",
            "Mihir Prabhudesai",
            "Shivam Duggal",
            "Ellis L Brown",
            "Deepak Pathak"
        ],
        "citations": 171,
        "references": 90,
        "year": 2023
    },
    {
        "title": "ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model",
        "abstract": "3D human motion generation is crucial for creative industry. Recent advances rely on generative models with domain knowledge for text-driven motion generation, leading to substantial progress in capturing common motions. However, the performance on more diverse motions remains unsatisfactory. In this work, we propose ReMoDiffuse, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process. ReMoDiffuse enhances the generalizability and diversity of text-driven motion generation with three key designs: 1) Hybrid Retrieval finds appropriate references from the database in terms of both semantic and kinematic similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval knowledge, adapting to the difference between retrieved samples and the target motion sequence. 3) Condition Mixture better utilizes the retrieval database during inference, overcoming the scale sensitivity in classifier-free guidance. Extensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality, especially for more diverse motion generation. Project page: https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html",
        "authors": [
            "Mingyuan Zhang",
            "Xinying Guo",
            "Liang Pan",
            "Zhongang Cai",
            "Fangzhou Hong",
            "Huirong Li",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "citations": 111,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Learning Interactive Real-World Simulators",
        "abstract": "Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different dimensions (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, we can simulate the visual outcome of both high-level instructions such as\"open the drawer\"and low-level controls from otherwise static scenes and objects. We use the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications. Video demos can be found at https://universal-simulator.github.io.",
        "authors": [
            "Mengjiao Yang",
            "Yilun Du",
            "Kamyar Ghasemipour",
            "Jonathan Tompson",
            "D. Schuurmans",
            "Pieter Abbeel"
        ],
        "citations": 118,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning",
        "abstract": "Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy, and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate the superiority of our method compared to prior works in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks.",
        "authors": [
            "Zhendong Wang",
            "Jonathan J. Hunt",
            "Mingyuan Zhou"
        ],
        "citations": 259,
        "references": 45,
        "year": 2022
    },
    {
        "title": "HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images",
        "abstract": "Diffusion models have emerged as the best approach for generative modeling of 2D images. Part of their success is due to the possibility of training them on millions if not billions of images with a stable learning objective. However, extending these models to 3D remains difficult for two reasons. First, finding a large quantity of 3D training data is much more complex than for 2D images. Second, while it is conceptually trivial to extend the models to operate on 3D rather than 2D grids, the associated cubic growth in memory and compute complexity makes this infeasible. We address the first challenge by introducing a new diffusion setup that can be trained, end-to-end, with only posed 2D images for supervision; and the second challenge by proposing an image formation model that decouples model memory from spatial memory. We evaluate our method on real-world data, using the CO3D dataset which has not been used to train 3D generative models before. We show that our diffusion models are scalable, train robustly, and are competitive in terms of sample quality and fidelity to existing approaches for 3D generative modeling.",
        "authors": [
            "Animesh Karnewar",
            "A. Vedaldi",
            "David Novotný",
            "N. Mitra"
        ],
        "citations": 98,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner",
        "abstract": "Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20\\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app under https://github.com/snap-stanford/med-flamingo.",
        "authors": [
            "Michael Moor",
            "Qian Huang",
            "Shirley Wu",
            "Michihiro Yasunaga",
            "Cyril Zakka",
            "Yashodhara Dalmia",
            "E. Reis",
            "P. Rajpurkar",
            "J. Leskovec"
        ],
        "citations": 162,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
        "abstract": "Prompt engineering is a relatively new field of research that refers to the practice of designing, refining, and implementing prompts or instructions that guide the output of large language models (LLMs) to help in various tasks. With the emergence of LLMs, the most popular one being ChatGPT that has attracted the attention of over a 100 million users in only 2 months, artificial intelligence (AI), especially generative AI, has become accessible for the masses. This is an unprecedented paradigm shift not only because of the use of AI becoming more widespread but also due to the possible implications of LLMs in health care. As more patients and medical professionals use AI-based tools, LLMs being the most popular representatives of that group, it seems inevitable to address the challenge to improve this skill. This paper summarizes the current state of research about prompt engineering and, at the same time, aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs.",
        "authors": [
            "B. Meskó"
        ],
        "citations": 217,
        "references": 7,
        "year": 2023
    },
    {
        "title": "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
        "abstract": "Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed.",
        "authors": [
            "Axel Sauer",
            "Tero Karras",
            "S. Laine",
            "Andreas Geiger",
            "Timo Aila"
        ],
        "citations": 178,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Inpaint Anything: Segment Anything Meets Image Inpainting",
        "abstract": "Modern image inpainting systems, despite the significant progress, often struggle with mask selection and holes filling. Based on Segment-Anything Model (SAM), we make the first attempt to the mask-free image inpainting and propose a new paradigm of ``clicking and filling'', which is named as Inpaint Anything (IA). The core idea behind IA is to combine the strengths of different models in order to build a very powerful and user-friendly pipeline for solving inpainting-related problems. IA supports three main features: (i) Remove Anything: users could click on an object and IA will remove it and smooth the ``hole'' with the context; (ii) Fill Anything: after certain objects removal, users could provide text-based prompts to IA, and then it will fill the hole with the corresponding generative content via driving AIGC models like Stable Diffusion; (iii) Replace Anything: with IA, users have another option to retain the click-selected object and replace the remaining background with the newly generated scenes. We are also very willing to help everyone share and promote new projects based on our Inpaint Anything (IA). Our codes are available at https://github.com/geekyutao/Inpaint-Anything.",
        "authors": [
            "Tao Yu",
            "Runsen Feng",
            "Ruoyu Feng",
            "Jinming Liu",
            "Xin Jin",
            "Wenjun Zeng",
            "Zhibo Chen"
        ],
        "citations": 167,
        "references": 13,
        "year": 2023
    },
    {
        "title": "Fairness And Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, And Mitigation Strategies",
        "abstract": "The significant advancements in applying artificial intelligence (AI) to healthcare decision-making, medical diagnosis, and other domains have simultaneously raised concerns about the fairness and bias of AI systems. This is particularly critical in areas like healthcare, employment, criminal justice, credit scoring, and increasingly, in generative AI models (GenAI) that produce synthetic media. Such systems can lead to unfair outcomes and perpetuate existing inequalities, including generative biases that affect the representation of individuals in synthetic data. This survey study offers a succinct, comprehensive overview of fairness and bias in AI, addressing their sources, impacts, and mitigation strategies. We review sources of bias, such as data, algorithm, and human decision biases—highlighting the emergent issue of generative AI bias, where models may reproduce and amplify societal stereotypes. We assess the societal impact of biased AI systems, focusing on perpetuating inequalities and reinforcing harmful stereotypes, especially as generative AI becomes more prevalent in creating content that influences public perception. We explore various proposed mitigation strategies, discuss the ethical considerations of their implementation, and emphasize the need for interdisciplinary collaboration to ensure effectiveness. Through a systematic literature review spanning multiple academic disciplines, we present definitions of AI bias and its different types, including a detailed look at generative AI bias. We discuss the negative impacts of AI bias on individuals and society and provide an overview of current approaches to mitigate AI bias, including data pre-processing, model selection, and post-processing. We emphasize the unique challenges presented by generative AI models and the importance of strategies specifically tailored to address these. Addressing bias in AI requires a holistic approach involving diverse and representative datasets, enhanced transparency and accountability in AI systems, and the exploration of alternative AI paradigms that prioritize fairness and ethical considerations. This survey contributes to the ongoing discussion on developing fair and unbiased AI systems by providing an overview of the sources, impacts, and mitigation strategies related to AI bias, with a particular focus on the emerging field of generative AI.",
        "authors": [
            "Emilio Ferrara"
        ],
        "citations": 159,
        "references": 60,
        "year": 2023
    },
    {
        "title": "ChatGPT for Education and Research: A Review of Benefits and Risks",
        "abstract": "Generative Pre-trained Transformer (ChatGPT) is an artificial intelligence (AI) tool that can quickly generate detailed responses to prompts and follow-up questions. This emerging AI tool was launched in November 2022 by an American AI research laboratory, called OpenAI, using large language models. In this article, the benefits and risks related to the use of ChatGPT in education and research are discussed. The article argues that ChatGPT has at least five main benefits, such as creating learning assessment, enhancing pedagogical practice, offering virtual personal tutoring, creating an essay or research article outline, and brainstorming ideas. However, there are risks related to academic integrity issues, unfair learning assessment, inaccurate information, and over-reliance on AI. The article offers a set of recommendations for effective use of ChatGPT for educational and research purposes.",
        "authors": [
            "Sarin Sok",
            "Kimkong Heng"
        ],
        "citations": 173,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review",
        "abstract": "Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.",
        "authors": [
            "Aghiles Kebaili",
            "J. Lapuyade-Lahorgue",
            "S. Ruan"
        ],
        "citations": 98,
        "references": 164,
        "year": 2023
    },
    {
        "title": "3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction",
        "abstract": "Rich data and powerful machine learning models allow us to design drugs for a specific protein target \\textit{in silico}. Recently, the inclusion of 3D structures during targeted drug design shows superior performance to other target-free models as the atomic interaction in the 3D space is explicitly modeled. However, current 3D target-aware models either rely on the voxelized atom densities or the autoregressive sampling process, which are not equivariant to rotation or easily violate geometric constraints resulting in unrealistic structures. In this work, we develop a 3D equivariant diffusion model to solve the above challenges. To achieve target-aware molecule design, our method learns a joint generative process of both continuous atom coordinates and categorical atom types with a SE(3)-equivariant network. Moreover, we show that our model can serve as an unsupervised feature extractor to estimate the binding affinity under proper parameterization, which provides an effective way for drug screening. To evaluate our model, we propose a comprehensive framework to evaluate the quality of sampled molecules from different dimensions. Empirical studies show our model could generate molecules with more realistic 3D structures and better affinities towards the protein targets, and improve binding affinity ranking and prediction without retraining.",
        "authors": [
            "Jiaqi Guan",
            "Wesley Wei Qian",
            "Xingang Peng",
            "Yufeng Su",
            "Jian Peng",
            "Jianzhu Ma"
        ],
        "citations": 121,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review",
        "abstract": "Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.",
        "authors": [
            "Aghiles Kebaili",
            "J. Lapuyade-Lahorgue",
            "S. Ruan"
        ],
        "citations": 98,
        "references": 164,
        "year": 2023
    },
    {
        "title": "3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction",
        "abstract": "Rich data and powerful machine learning models allow us to design drugs for a specific protein target \\textit{in silico}. Recently, the inclusion of 3D structures during targeted drug design shows superior performance to other target-free models as the atomic interaction in the 3D space is explicitly modeled. However, current 3D target-aware models either rely on the voxelized atom densities or the autoregressive sampling process, which are not equivariant to rotation or easily violate geometric constraints resulting in unrealistic structures. In this work, we develop a 3D equivariant diffusion model to solve the above challenges. To achieve target-aware molecule design, our method learns a joint generative process of both continuous atom coordinates and categorical atom types with a SE(3)-equivariant network. Moreover, we show that our model can serve as an unsupervised feature extractor to estimate the binding affinity under proper parameterization, which provides an effective way for drug screening. To evaluate our model, we propose a comprehensive framework to evaluate the quality of sampled molecules from different dimensions. Empirical studies show our model could generate molecules with more realistic 3D structures and better affinities towards the protein targets, and improve binding affinity ranking and prediction without retraining.",
        "authors": [
            "Jiaqi Guan",
            "Wesley Wei Qian",
            "Xingang Peng",
            "Yufeng Su",
            "Jian Peng",
            "Jianzhu Ma"
        ],
        "citations": 121,
        "references": 67,
        "year": 2023
    },
    {
        "title": "RingMo: A Remote Sensing Foundation Model With Masked Image Modeling",
        "abstract": "Deep learning approaches have contributed to the rapid development of remote sensing (RS) image interpretation. The most widely used training paradigm is to use ImageNet pretrained models to process RS data for specified tasks. However, there are issues such as domain gap between natural and RS scenes and the poor generalization capacity of RS models. It makes sense to develop a foundation model with general RS feature representation. Since a large amount of unlabeled data is available, the self-supervised method has more development significance than the fully supervised method in RS. However, most of the current self-supervised methods use contrastive learning, whose performance is sensitive to data augmentation, additional information, and selection of positive and negative pairs. In this article, we leverage the benefits of generative self-supervised learning (SSL) for RS images and propose an RS foundation model framework called RingMo, which consists of two parts. First, a large-scale dataset is constructed by collecting two million RS images from satellite and aerial platforms, covering multiple scenes and objects around the world. Second, we propose an RS foundation model training method designed for dense and small objects in complicated RS scenes. We show that the foundation model trained on our dataset with RingMo method achieves state-of-the-art (SOTA) on eight datasets across four downstream tasks, demonstrating the effectiveness of the proposed framework. Through in-depth exploration, we believe it is time for RS researchers to embrace generative SSL and leverage its general representation capabilities to speed up the development of RS applications.",
        "authors": [
            "Xian Sun",
            "Peijin Wang",
            "Wanxuan Lu",
            "Zicong Zhu",
            "Xiaonan Lu",
            "Qi He",
            "Junxi Li",
            "Xuee Rong",
            "Zhujun Yang",
            "Hao Chang",
            "Qinglin He",
            "Guang Yang",
            "Ruiping Wang",
            "Jiwen Lu",
            "Kun Fu"
        ],
        "citations": 156,
        "references": 154,
        "year": 2023
    },
    {
        "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
        "abstract": "We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/",
        "authors": [
            "D. Kondratyuk",
            "Lijun Yu",
            "Xiuye Gu",
            "José Lezama",
            "Jonathan Huang",
            "Rachel Hornung",
            "Hartwig Adam",
            "Hassan Akbari",
            "Y. Alon",
            "Vighnesh Birodkar",
            "Yong Cheng",
            "Ming-Chang Chiu",
            "Josh Dillon",
            "Irfan Essa",
            "Agrim Gupta",
            "Meera Hahn",
            "Anja Hauth",
            "David Hendon",
            "Alonso Martinez",
            "David C. Minnen",
            "David A. Ross",
            "Grant Schindler",
            "Mikhail Sirotenko",
            "Kihyuk Sohn",
            "Krishna Somandepalli",
            "Huisheng Wang",
            "Jimmy Yan",
            "Ming Yang",
            "Xuan Yang",
            "Bryan Seybold",
            "Lu Jiang"
        ],
        "citations": 153,
        "references": 97,
        "year": 2023
    },
    {
        "title": "MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation",
        "abstract": "Video prediction is a challenging task. The quality of video frames from current state-of-the-art (SOTA) generative models tends to be poor and generalization beyond the training data is difficult. Furthermore, existing prediction frameworks are typically not capable of simultaneously handling other video-related tasks such as unconditional generation or interpolation. In this work, we devise a general-purpose framework called Masked Conditional Video Diffusion (MCVD) for all of these video synthesis tasks using a probabilistic conditional score-based denoising diffusion model, conditioned on past and/or future frames. We train the model in a manner where we randomly and independently mask all the past frames or all the future frames. This novel but straightforward setup allows us to train a single model that is capable of executing a broad range of video tasks, specifically: future/past prediction -- when only future/past frames are masked; unconditional generation -- when both past and future frames are masked; and interpolation -- when neither past nor future frames are masked. Our experiments show that this approach can generate high-quality frames for diverse types of videos. Our MCVD models are built from simple non-recurrent 2D-convolutional architectures, conditioning on blocks of frames and generating blocks of frames. We generate videos of arbitrary lengths autoregressively in a block-wise manner. Our approach yields SOTA results across standard video prediction and interpolation benchmarks, with computation times for training models measured in 1-12 days using $\\le$ 4 GPUs. Project page: https://mask-cond-video-diffusion.github.io ; Code : https://github.com/voletiv/mcvd-pytorch",
        "authors": [
            "Vikram S. Voleti",
            "Alexia Jolicoeur-Martineau",
            "C. Pal"
        ],
        "citations": 252,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model",
        "abstract": "Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration.",
        "authors": [
            "Yinhuai Wang",
            "Jiwen Yu",
            "Jian Zhang"
        ],
        "citations": 334,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Multisample Flow Matching: Straightening Flows with Minibatch Couplings",
        "abstract": "Simulation-free methods for training continuous-time generative models construct probability paths that go between noise distributions and individual data samples. Recent works, such as Flow Matching, derived paths that are optimal for each data sample. However, these algorithms rely on independent data and noise samples, and do not exploit underlying structure in the data distribution for constructing probability paths. We propose Multisample Flow Matching, a more general framework that uses non-trivial couplings between data and noise samples while satisfying the correct marginal constraints. At very small overhead costs, this generalization allows us to (i) reduce gradient variance during training, (ii) obtain straighter flows for the learned vector field, which allows us to generate high-quality samples using fewer function evaluations, and (iii) obtain transport maps with lower cost in high dimensions, which has applications beyond generative modeling. Importantly, we do so in a completely simulation-free manner with a simple minimization objective. We show that our proposed methods improve sample consistency on downsampled ImageNet data sets, and lead to better low-cost sample generation.",
        "authors": [
            "Aram-Alexandre Pooladian",
            "Heli Ben-Hamu",
            "Carles Domingo-Enrich",
            "Brandon Amos",
            "Y. Lipman",
            "Ricky T. Q. Chen"
        ],
        "citations": 94,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
        "abstract": "In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.",
        "authors": [
            "Suyu Ge",
            "Yunan Zhang",
            "Liyuan Liu",
            "Minjia Zhang",
            "Jiawei Han",
            "Jianfeng Gao"
        ],
        "citations": 133,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Synthbuster: Towards Detection of Diffusion Model Generated Images",
        "abstract": "Synthetically-generated images are getting increasingly popular. Diffusion models have advanced to the stage where even non-experts can generate photo-realistic images from a simple text prompt. They expand creative horizons but also open a Pandora's box of potential disinformation risks. In this context, the present corpus of synthetic image detection techniques, primarily focusing on older generative models like Generative Adversarial Networks, finds itself ill-equipped to deal with this emerging trend. Recognizing this challenge, we introduce a method specifically designed to detect synthetic images produced by diffusion models. Our approach capitalizes on the inherent frequency artefacts left behind during the diffusion process. Spectral analysis is used to highlight the artefacts in the Fourier transform of a residual image, which are used to distinguish real from fake images. The proposed method can detect diffusion-model-generated images even under mild jpeg compression, and generalizes relatively well to unknown models. By pioneering this novel approach, we aim to fortify forensic methodologies and ignite further research into the detection of AI-generated images.",
        "authors": [
            "Quentin Bammey"
        ],
        "citations": 30,
        "references": 53,
        "year": 2024
    },
    {
        "title": "Artificial Intelligence Can Generate Fraudulent but Authentic-Looking Scientific Medical Articles: Pandora’s Box Has Been Opened",
        "abstract": "Background Artificial intelligence (AI) has advanced substantially in recent years, transforming many industries and improving the way people live and work. In scientific research, AI can enhance the quality and efficiency of data analysis and publication. However, AI has also opened up the possibility of generating high-quality fraudulent papers that are difficult to detect, raising important questions about the integrity of scientific research and the trustworthiness of published papers. Objective The aim of this study was to investigate the capabilities of current AI language models in generating high-quality fraudulent medical articles. We hypothesized that modern AI models can create highly convincing fraudulent papers that can easily deceive readers and even experienced researchers. Methods This proof-of-concept study used ChatGPT (Chat Generative Pre-trained Transformer) powered by the GPT-3 (Generative Pre-trained Transformer 3) language model to generate a fraudulent scientific article related to neurosurgery. GPT-3 is a large language model developed by OpenAI that uses deep learning algorithms to generate human-like text in response to prompts given by users. The model was trained on a massive corpus of text from the internet and is capable of generating high-quality text in a variety of languages and on various topics. The authors posed questions and prompts to the model and refined them iteratively as the model generated the responses. The goal was to create a completely fabricated article including the abstract, introduction, material and methods, discussion, references, charts, etc. Once the article was generated, it was reviewed for accuracy and coherence by experts in the fields of neurosurgery, psychiatry, and statistics and compared to existing similar articles. Results The study found that the AI language model can create a highly convincing fraudulent article that resembled a genuine scientific paper in terms of word usage, sentence structure, and overall composition. The AI-generated article included standard sections such as introduction, material and methods, results, and discussion, as well a data sheet. It consisted of 1992 words and 17 citations, and the whole process of article creation took approximately 1 hour without any special training of the human user. However, there were some concerns and specific mistakes identified in the generated article, specifically in the references. Conclusions The study demonstrates the potential of current AI language models to generate completely fabricated scientific articles. Although the papers look sophisticated and seemingly flawless, expert readers may identify semantic inaccuracies and errors upon closer inspection. We highlight the need for increased vigilance and better detection methods to combat the potential misuse of AI in scientific research. At the same time, it is important to recognize the potential benefits of using AI language models in genuine scientific writing and research, such as manuscript preparation and language editing.",
        "authors": [
            "M. Májovský",
            "Martin Černý",
            "Matěj Kasal",
            "M. Komarc",
            "D. Netuka"
        ],
        "citations": 138,
        "references": 12,
        "year": 2023
    },
    {
        "title": "Performance of GPT-3.5 and GPT-4 on the Japanese Medical Licensing Examination: Comparison Study",
        "abstract": "Background The competence of ChatGPT (Chat Generative Pre-Trained Transformer) in non-English languages is not well studied. Objective This study compared the performances of GPT-3.5 (Generative Pre-trained Transformer) and GPT-4 on the Japanese Medical Licensing Examination (JMLE) to evaluate the reliability of these models for clinical reasoning and medical knowledge in non-English languages. Methods This study used the default mode of ChatGPT, which is based on GPT-3.5; the GPT-4 model of ChatGPT Plus; and the 117th JMLE in 2023. A total of 254 questions were included in the final analysis, which were categorized into 3 types, namely general, clinical, and clinical sentence questions. Results The results indicated that GPT-4 outperformed GPT-3.5 in terms of accuracy, particularly for general, clinical, and clinical sentence questions. GPT-4 also performed better on difficult questions and specific disease questions. Furthermore, GPT-4 achieved the passing criteria for the JMLE, indicating its reliability for clinical reasoning and medical knowledge in non-English languages. Conclusions GPT-4 could become a valuable tool for medical education and clinical support in non–English-speaking regions, such as Japan.",
        "authors": [
            "Soshi Takagi",
            "T. Watari",
            "Ayano Erabi",
            "Kota Sakaguchi"
        ],
        "citations": 146,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos",
        "abstract": "Generating text-editable and pose-controllable character videos have an imperious demand in creating various digital human. Nevertheless, this task has been restricted by the absence of a comprehensive dataset featuring paired video-pose captions and the generative prior models for videos. In this work, we design a novel two-stage training scheme that can utilize easily obtained datasets (i.e., image pose pair and pose-free video) and the pre-trained text-to-image (T2I) model to obtain the pose-controllable character videos. Specifically, in the first stage, only the keypoint image pairs are used only for a controllable text-to-image generation. We learn a zero-initialized convolutional encoder to encode the pose information. In the second stage, we finetune the motion of the above network via a pose-free video dataset by adding the learnable temporal self-attention and reformed cross-frame self-attention blocks. Powered by our new designs, our method successfully generates continuously pose-controllable character videos while keeps the editing and concept composition ability of the pre-trained T2I model. The code and models are available on https://follow-your-pose.github.io/.",
        "authors": [
            "Yue Ma",
            "Yin-Yin He",
            "Xiaodong Cun",
            "Xintao Wang",
            "Ying Shan",
            "Xiu Li",
            "Qifeng Chen"
        ],
        "citations": 130,
        "references": 51,
        "year": 2023
    },
    {
        "title": "LCM-LoRA: A Universal Stable-Diffusion Acceleration Module",
        "abstract": "Latent Consistency Models (LCMs) have achieved impressive performance in accelerating text-to-image generative tasks, producing high-quality images with minimal inference steps. LCMs are distilled from pre-trained latent diffusion models (LDMs), requiring only ~32 A100 GPU training hours. This report further extends LCMs' potential in two aspects: First, by applying LoRA distillation to Stable-Diffusion models including SD-V1.5, SSD-1B, and SDXL, we have expanded LCM's scope to larger models with significantly less memory consumption, achieving superior image generation quality. Second, we identify the LoRA parameters obtained through LCM distillation as a universal Stable-Diffusion acceleration module, named LCM-LoRA. LCM-LoRA can be directly plugged into various Stable-Diffusion fine-tuned models or LoRAs without training, thus representing a universally applicable accelerator for diverse image generation tasks. Compared with previous numerical PF-ODE solvers such as DDIM, DPM-Solver, LCM-LoRA can be viewed as a plug-in neural PF-ODE solver that possesses strong generalization abilities. Project page: https://github.com/luosiallen/latent-consistency-model.",
        "authors": [
            "Simian Luo",
            "Yiqin Tan",
            "Suraj Patil",
            "Daniel Gu",
            "Patrick von Platen",
            "Apolin'ario Passos",
            "Longbo Huang",
            "Jian Li",
            "Hang Zhao"
        ],
        "citations": 119,
        "references": 19,
        "year": 2023
    },
    {
        "title": "SqueezeLLM: Dense-and-Sparse Quantization",
        "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.",
        "authors": [
            "Sehoon Kim",
            "Coleman Hooper",
            "A. Gholami",
            "Zhen Dong",
            "Xiuyu Li",
            "Sheng Shen",
            "Michael W. Mahoney",
            "K. Keutzer"
        ],
        "citations": 126,
        "references": 103,
        "year": 2023
    },
    {
        "title": "Language is All a Graph Needs",
        "abstract": "The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data like images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, languages, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is available at https://github.com/agiresearch/InstructGLM.",
        "authors": [
            "Ruosong Ye",
            "Caiqi Zhang",
            "Runhui Wang",
            "Shuyuan Xu",
            "Yongfeng Zhang"
        ],
        "citations": 126,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Blended Latent Diffusion",
        "abstract": "The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space and eliminating the need for resource-intensive CLIP gradient calculations at each diffusion step. We first enable LDM to perform local image edits by blending the latents at each step, similarly to Blended Diffusion. Next we propose an optimization-based solution for the inherent inability of LDM to accurately reconstruct images. Finally, we address the scenario of performing local edits using thin masks. We evaluate our method against the available baselines both qualitatively and quantitatively and demonstrate that in addition to being faster, it produces more precise results.",
        "authors": [
            "Omri Avrahami",
            "Ohad Fried",
            "Dani Lischinski"
        ],
        "citations": 300,
        "references": 86,
        "year": 2022
    },
    {
        "title": "On Aliased Resizing and Surprising Subtleties in GAN Evaluation",
        "abstract": "Metrics for evaluating generative models aim to measure the discrepancy between real and generated images. The often-used Fréchet Inception Distance (FID) metric, for example, extracts “high-level” features using a deep network from the two sets. However, we find that the differences in “low-level” preprocessing, specifically image resizing and compression, can induce large variations and have unforeseen consequences. For instance, when resizing an image, e.g., with a bilinear or bicubic kernel, signal processing principles mandate adjusting prefilter width depending on the downsampling factor, to antialias to the appropriate bandwidth. However, commonly-used implementations use a fixed-width prefilter, resulting in aliasing artifacts. Such aliasing leads to corruptions in the feature extraction down-stream. Next, lossy compression, such as JPEG, is commonly used to reduce the file size of an image. Although designed to minimally degrade the perceptual quality of an image, the operation also produces variations downstream. Furthermore, we show that if compression is used on real training images, FID can actually improve if the generated images are also subsequently compressed. This paper shows that choices in low-level image processing have been an under-appreciated aspect of generative modeling. We identify and characterize variations in generative modeling development pipelines, provide recommendations based on signal processing principles, and release a reference implementation to facilitate future comparisons.",
        "authors": [
            "Gaurav Parmar",
            "Richard Zhang",
            "Jun-Yan Zhu"
        ],
        "citations": 323,
        "references": 82,
        "year": 2022
    },
    {
        "title": "ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model",
        "abstract": null,
        "authors": [
            "Hanyao Huang",
            "Ou Zheng",
            "Dongdong Wang",
            "Jiayi Yin",
            "Zijin Wang",
            "Shengxuan Ding",
            "H. Yin",
            "Chuan Xu",
            "Renjie Yang",
            "Q. Zheng",
            "B. Shi"
        ],
        "citations": 131,
        "references": 111,
        "year": 2023
    },
    {
        "title": "Who's Harry Potter? Approximate Unlearning in LLMs",
        "abstract": "Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.",
        "authors": [
            "Ronen Eldan",
            "M. Russinovich"
        ],
        "citations": 124,
        "references": 23,
        "year": 2023
    },
    {
        "title": "ChatGPT and the hospitality and tourism industry: an overview of current trends and future research directions",
        "abstract": "ABSTRACT Since its launch, ChatGPT, an artificial intelligence chatbot developed by Open AI based on the premises of generative pre-trained transformer autoregressive language models, has gained widespread popularity and is making significant impact on society with its unique features, such as natural language processing and contextual awareness. ChatGPT is viewed as a major disruptive innovation that is likely to revolutionize the operations in many industries including the hospitality and tourism industry. The adoption of ChatGPT will result in substantial changes throughout the hospitality and tourism industry by disrupting how customer search for information, make decisions, and how businesses produce, create, and deliver customized services and experiences. This conceptual paper provides a comprehensive discussion on generative pre-trained transformers’ (GPTs) benefits, and potential challenges and threats they pose to the hospitality and tourism industry. The feasibility of integrating GPT into different travel stages and decision-making processes is also discussed. The article concludes by proposing a potential future research agenda on using GPT in creating and delivering hospitality and tourism experiences, which can guide further advancements in the field.",
        "authors": [
            "D. Gursoy",
            "Yu Li",
            "Hakjun Song"
        ],
        "citations": 127,
        "references": 38,
        "year": 2023
    },
    {
        "title": "A large-scale comparison of human-written versus ChatGPT-generated essays",
        "abstract": null,
        "authors": [
            "Steffen Herbold",
            "Annette Hautli-Janisz",
            "Ute Heuer",
            "Zlata Kikteva",
            "Alexander Trautsch"
        ],
        "citations": 127,
        "references": 16,
        "year": 2023
    },
    {
        "title": "DepthFM: Fast Monocular Depth Estimation with Flow Matching",
        "abstract": "Current discriminative depth estimation methods often produce blurry artifacts, while generative approaches suffer from slow sampling due to curvatures in the noise-to-depth transport. Our method addresses these challenges by framing depth estimation as a direct transport between image and depth distributions. We are the first to explore flow matching in this field, and we demonstrate that its interpolation trajectories enhance both training and sampling efficiency while preserving high performance. While generative models typically require extensive training data, we mitigate this dependency by integrating external knowledge from a pre-trained image diffusion model, enabling effective transfer even across differing objectives. To further boost our model performance, we employ synthetic data and utilize image-depth pairs generated by a discriminative model on an in-the-wild image dataset. As a generative model, our model can reliably estimate depth confidence, which provides an additional advantage. Our approach achieves competitive zero-shot performance on standard benchmarks of complex natural scenes while improving sampling efficiency and only requiring minimal synthetic data for training.",
        "authors": [
            "Ming Gui",
            "Johannes S. Fischer",
            "Ulrich Prestel",
            "Pingchuan Ma",
            "Dmytro Kotovenko",
            "Olga Grebenkova",
            "S. A. Baumann",
            "Vincent Tao Hu",
            "Bjorn Ommer"
        ],
        "citations": 25,
        "references": 77,
        "year": 2024
    },
    {
        "title": "A Comparative Study on Enhancing Prediction in Social Network Advertisement Through Data Augmentation",
        "abstract": "In the ever-evolving landscape of social network advertising, the volume and accuracy of data play a critical role in the performance of predictive models. However, the development of robust predictive algorithms is often hampered by the limited size and potential bias present in real-world datasets. This study presents and explores a generative augmentation framework of social network advertising data. Our framework explores three generative models for data augmentation - Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Gaussian Mixture Models (GMMs) - to enrich data availability and diversity in the context of social network advertising analytics effectiveness. By performing synthetic extensions of the feature space, we find that through data augmentation, the performance of various classifiers has been quantitatively improved. Furthermore, we compare the relative performance gains brought by each data augmentation technique, providing insights for practitioners to select appropriate techniques to enhance model performance. This paper contributes to the literature by showing that synthetic data augmentation alleviates the limitations imposed by small or imbalanced datasets in the field of social network advertising. At the same time, this article also provides a comparative perspective on the practicality of different data augmentation methods, thereby guiding practitioners to choose appropriate techniques to enhance model performance.",
        "authors": [
            "Qikai Yang",
            "Panfeng Li",
            "Xinhe Xu",
            "Zhicheng Ding",
            "Wenjing Zhou",
            "Yi Nian"
        ],
        "citations": 28,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Long-form music generation with latent diffusion",
        "abstract": "Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure from text prompts. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.",
        "authors": [
            "Zach Evans",
            "Julian Parker",
            "CJ Carr",
            "Zack Zukowski",
            "Josiah Taylor",
            "Jordi Pons"
        ],
        "citations": 25,
        "references": 49,
        "year": 2024
    },
    {
        "title": "I2SB: Image-to-Image Schrödinger Bridge",
        "abstract": "We propose Image-to-Image Schr\\\"odinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schr\\\"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. Moreover, I$^2$SB matches the performance of inverse methods that additionally require the knowledge of the corruption operators. Our work opens up new algorithmic opportunities for developing efficient nonlinear diffusion models on a large scale. scale. Project page and codes: https://i2sb.github.io/",
        "authors": [
            "Guan-Horng Liu",
            "Arash Vahdat",
            "De-An Huang",
            "Evangelos A. Theodorou",
            "Weili Nie",
            "Anima Anandkumar"
        ],
        "citations": 111,
        "references": 78,
        "year": 2023
    },
    {
        "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities",
        "abstract": "Large language models (LMs) offer unprecedented language generation capabilities and exciting opportunities for interaction design. However, their highly context-dependent capabilities are difficult to grasp and are often subjectively interpreted. In this paper, we argue that by curating and analyzing large interaction datasets, the HCI community can foster more incisive examinations of LMs’ generative capabilities. Exemplifying this approach, we present CoAuthor, a dataset designed for revealing GPT-3’s capabilities in assisting creative and argumentative writing. CoAuthor captures rich interactions between 63 writers and four instances of GPT-3 across 1445 writing sessions. We demonstrate that CoAuthor can address questions about GPT-3’s language, ideation, and collaboration capabilities, and reveal its contribution as a writing “collaborator” under various definitions of good collaboration. Finally, we discuss how this work may facilitate a more principled discussion around LMs’ promises and pitfalls in relation to interaction design. The dataset and an interface for replaying the writing sessions are publicly available at https://coauthor.stanford.edu.",
        "authors": [
            "Mina Lee",
            "Percy Liang",
            "Qian Yang"
        ],
        "citations": 308,
        "references": 86,
        "year": 2022
    },
    {
        "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
        "abstract": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.",
        "authors": [
            "Runpei Dong",
            "Chunrui Han",
            "Yuang Peng",
            "Zekun Qi",
            "Zheng Ge",
            "Jinrong Yang",
            "Liang Zhao",
            "Jian‐Yuan Sun",
            "Hongyu Zhou",
            "Hao-Ran Wei",
            "Xiangwen Kong",
            "Xiangyu Zhang",
            "Kaisheng Ma",
            "Li Yi"
        ],
        "citations": 119,
        "references": 188,
        "year": 2023
    },
    {
        "title": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering",
        "abstract": "Medical Visual Question Answering (MedVQA) presents a significant opportunity to enhance diagnostic accuracy and healthcare delivery by leveraging artificial intelligence to interpret and answer questions based on medical images. In this study, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction and propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model. We establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. We train the proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD, SLAKE, and Image-Clef-2019, significantly outperforming existing MedVQA models in generating relevant, accurate free-form answers. In addition, we propose a test set that has undergone manual verification, which is significantly more challenging, serving to better monitor the development of generative MedVQA methods. To facilitate comprehensive evaluation and comparison, we have maintained a leaderboard at https://paperswithcode.com/paper/pmc-vqa-visual-instruction-tuning-for-medical, offering a centralized resource for tracking progress and benchmarking state-of-the-art approaches. The PMC-VQA dataset emerges as a vital resource for the field of research, and the MedVInT presents a significant breakthrough in the area of MedVQA.",
        "authors": [
            "Xiaoman Zhang",
            "Chaoyi Wu",
            "Ziheng Zhao",
            "Weixiong Lin",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "citations": 106,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Leaving Reality to Imagination: Robust Classification via Generated Datasets",
        "abstract": "Recent research on robustness has revealed significant performance gaps between neural image classifiers trained on datasets that are similar to the test set, and those that are from a naturally shifted distribution, such as sketches, paintings, and animations of the object categories observed during training. Prior work focuses on reducing this gap by designing engineered augmentations of training data or through unsupervised pretraining of a single large model on massive in-the-wild training datasets scraped from the Internet. However, the notion of a dataset is also undergoing a paradigm shift in recent years. With drastic improvements in the quality, ease-of-use, and access to modern generative models, generated data is pervading the web. In this light, we study the question: How do these generated datasets influence the natural robustness of image classifiers? We find that Imagenet classifiers trained on real data augmented with generated data achieve higher accuracy and effective robustness than standard training and popular augmentation strategies in the presence of natural distribution shifts. We analyze various factors influencing these results, including the choice of conditioning strategies and the amount of generated data. Additionally, we find that the standard ImageNet classifiers suffer a performance degradation of upto 20\\% on the generated data, indicating their fragility at accurately classifying the objects under novel variations. Lastly, we demonstrate that the image classifiers, which have been trained on real data augmented with generated data from the base generative model, exhibit greater resilience to natural distribution shifts compared to the classifiers trained on real data augmented with generated data from the finetuned generative model on the real data. The code, models, and datasets are available at https://github.com/Hritikbansal/generative-robustness.",
        "authors": [
            "Hritik Bansal",
            "Aditya Grover"
        ],
        "citations": 76,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Compositional 3D Scene Generation using Locally Conditioned Diffusion",
        "abstract": "Designing complex 3D scenes has been a tedious, manual process requiring domain expertise. Emerging text-to-3D generative models show great promise for making this task more intuitive, but existing approaches are limited to object-level generation. We introduce locally conditioned diffusion as an approach to compositional scene diffusion, providing control over semantic parts using text prompts and bounding boxes while ensuring seamless transitions between these parts. We demonstrate a score distillation sampling-based text-to-3D synthesis pipeline that enables compositional 3D scene generation at a higher fidelity than relevant baselines.",
        "authors": [
            "Ryan Po",
            "Gordon Wetzstein"
        ],
        "citations": 73,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "abstract": "Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of “generality” comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.",
        "authors": [
            "Sasha Luccioni",
            "Yacine Jernite",
            "Emma Strubell"
        ],
        "citations": 97,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Aligning Distillation For Cold-start Item Recommendation",
        "abstract": "Recommending cold items in recommendation systems is a longstanding challenge due to the inherent differences between warm items, which are recommended based on user behavior, and cold items, which are recommended based on content features. To tackle this, generative models generate synthetic embeddings from content features, while dropout models enhance the robustness of the recommendation system by randomly dropping behavioral embeddings during training. However, these models primarily focus on handling the recommendation of cold items, but do not effectively address the differences between warm and cold recommendations. As a result, generative models may over-recommend either warm or cold items, neglecting the other type, and dropout models may negatively impact warm item recommendations. To address this, we propose the Aligning Distillation (ALDI) framework, which leverages warm items as \"teachers\" to transfer their behavioral information to cold items, referred to as \"students\". ALDI aligns the students with the teachers by comparing the differences in their recommendation characters, using tailored rating distribution aligning, ranking aligning, and identification aligning losses to narrow these differences. Furthermore, ALDI incorporates a teacher-qualifying weighting structure to prevent students from learning inaccurate information from unreliable teachers. Experiments on three datasets show that our approach outperforms state-of-the-art baselines in terms of overall, warm, and cold recommendation performance with three different recommendation backbones.",
        "authors": [
            "Feiran Huang",
            "Zefan Wang",
            "Xiao Huang",
            "Yu-hong Qian",
            "Zhetao Li",
            "Hao Chen"
        ],
        "citations": 70,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise",
        "abstract": "Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models",
        "authors": [
            "Arpit Bansal",
            "Eitan Borgnia",
            "Hong-Min Chu",
            "Jie Li",
            "Hamid Kazemi",
            "Furong Huang",
            "Micah Goldblum",
            "Jonas Geiping",
            "T. Goldstein"
        ],
        "citations": 225,
        "references": 38,
        "year": 2022
    },
    {
        "title": "Aci-bench: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation",
        "abstract": null,
        "authors": [
            "Wen-wai Yim",
            "Yujuan Fu",
            "Asma Ben Abacha",
            "Neal Snider",
            "Thomas Lin",
            "Meliha Yetisgen"
        ],
        "citations": 64,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Self-Supervised Speech Representation Learning: A Review",
        "abstract": "Although supervised deep learning has revolutionized speech and audio processing, it has necessitated the building of specialist models for individual tasks and application scenarios. It is likewise difficult to apply this to dialects and languages for which only limited labeled data is available. Self-supervised representation learning methods promise a single universal model that would benefit a wide variety of tasks and domains. Such methods have shown success in natural language processing and computer vision domains, achieving new levels of performance while reducing the number of labels required for many downstream scenarios. Speech representation learning is experiencing similar progress in three main categories: generative, contrastive, and predictive methods. Other approaches rely on multi-modal data for pre-training, mixing text or visual data streams with speech. Although self-supervised speech representation is still a nascent research area, it is closely related to acoustic word embedding and learning with zero lexical resources, both of which have seen active research for many years. This review presents approaches for self-supervised speech representation learning and their connection to other research areas. Since many current methods focus solely on automatic speech recognition as a downstream task, we review recent efforts on benchmarking learned representations to extend the application beyond speech recognition.",
        "authors": [
            "Abdel-rahman Mohamed",
            "Hung-yi Lee",
            "Lasse Borgholt",
            "Jakob Drachmann Havtorn",
            "Joakim Edin",
            "C. Igel",
            "K. Kirchhoff",
            "Shang-Wen Li",
            "Karen Livescu",
            "Lars Maaløe",
            "Tara N. Sainath",
            "Shinji Watanabe"
        ],
        "citations": 315,
        "references": 368,
        "year": 2022
    },
    {
        "title": "Diffusion Recommender Model",
        "abstract": "Generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are widely utilized to model the generative process of user interactions. However, they suffer from intrinsic limitations such as the instability of GANs and the restricted representation ability of VAEs. Such limitations hinder the accurate modeling of the complex user interaction generation procedure, such as noisy interactions caused by various interference factors. In light of the impressive advantages of Diffusion Models (DMs) over traditional generative models in image synthesis, we propose a novel Diffusion Recommender Model (named DiffRec) to learn the generative process in a denoising manner. To retain personalized information in user interactions, DiffRec reduces the added noises and avoids corrupting users' interactions into pure noises like in image synthesis. In addition, we extend traditional DMs to tackle the unique challenges in recommendation: high resource costs for large-scale item prediction and temporal shifts of user preference. To this end, we propose two extensions of DiffRec: L-DiffRec clusters items for dimension compression and conducts the diffusion processes in the latent space; and T-DiffRec reweights user interactions based on the interaction timestamps to encode temporal information. We conduct extensive experiments on three datasets under multiple settings (e.g., clean training, noisy training, and temporal training). The empirical results validate the superiority of DiffRec with two extensions over competitive baselines.",
        "authors": [
            "Wenjie Wang",
            "Yiyan Xu",
            "Fuli Feng",
            "Xinyu Lin",
            "X. He",
            "Tat-Seng Chua"
        ],
        "citations": 66,
        "references": 55,
        "year": 2023
    },
    {
        "title": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild",
        "abstract": "Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions, we augment pretrained text-to-image diffusion models and introduce a task-aware HyperNet to modulate the diffusion models, enabling the adaptation to different C2I tasks simultaneously. Trained on nine unique C2I tasks, UniControl demonstrates impressive zero-shot generation abilities with unseen visual conditions. Experimental results show that UniControl often surpasses the performance of single-task-controlled methods of comparable model sizes. This control versatility positions UniControl as a significant advancement in the realm of controllable visual generation.",
        "authors": [
            "Can Qin",
            "Shu Zhang",
            "Ning Yu",
            "Yihao Feng",
            "Xinyi Yang",
            "Yingbo Zhou",
            "Haiquan Wang",
            "Juan Carlos Niebles",
            "Caiming Xiong",
            "S. Savarese",
            "Stefano Ermon",
            "Yun Fu",
            "Ran Xu"
        ],
        "citations": 85,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration",
        "abstract": "Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called\"regression to the mean\"effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models. Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality. While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising.",
        "authors": [
            "M. Delbracio",
            "P. Milanfar"
        ],
        "citations": 90,
        "references": 97,
        "year": 2023
    },
    {
        "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
        "abstract": "Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio",
        "authors": [
            "Dongchao Yang",
            "Jinchuan Tian",
            "Xuejiao Tan",
            "Rongjie Huang",
            "Songxiang Liu",
            "Xuankai Chang",
            "Jiatong Shi",
            "Sheng Zhao",
            "Jiang Bian",
            "Xixin Wu",
            "Zhou Zhao",
            "Helen Meng"
        ],
        "citations": 91,
        "references": 89,
        "year": 2023
    },
    {
        "title": "AudioGen: Textually Guided Audio Generation",
        "abstract": "We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen",
        "authors": [
            "Felix Kreuk",
            "Gabriel Synnaeve",
            "Adam Polyak",
            "Uriel Singer",
            "Alexandre D'efossez",
            "Jade Copet",
            "Devi Parikh",
            "Yaniv Taigman",
            "Yossi Adi"
        ],
        "citations": 245,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds",
        "abstract": "Proteins power a vast array of functional processes in living cells. The capability to create new proteins with designed structures and functions would thus enable the engineering of cellular behavior and development of protein-based therapeutics and materials. Structure-based protein design aims to find structures that are designable (can be realized by a protein sequence), novel (have dissimilar geometry from natural proteins), and diverse (span a wide range of geometries). While advances in protein structure prediction have made it possible to predict structures of novel protein sequences, the combinatorially large space of sequences and structures limits the practicality of search-based methods. Generative models provide a compelling alternative, by implicitly learning the low-dimensional structure of complex data distributions. Here, we leverage recent advances in denoising diffusion probabilistic models and equivariant neural networks to develop Genie, a generative model of protein structures that performs discrete-time diffusion using a cloud of oriented reference frames in 3D space. Through in silico evaluations, we demonstrate that Genie generates protein backbones that are more designable, novel, and diverse than existing models. This indicates that Genie is capturing key aspects of the distribution of protein structure space and facilitates protein design with high success rates. Code for generating new proteins and training new versions of Genie is available at https://github.com/aqlaboratory/genie.",
        "authors": [
            "Yeqing Lin",
            "Mohammed AlQuraishi"
        ],
        "citations": 61,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising",
        "abstract": "Leveraging large-scale image-text datasets and advancements in diffusion models, text-driven generative models have made remarkable strides in the field of image generation and editing. This study explores the potential of extending the text-driven ability to the generation and editing of multi-text conditioned long videos. Current methodologies for video generation and editing, while innovative, are often confined to extremely short videos (typically less than 24 frames) and are limited to a single text condition. These constraints significantly limit their applications given that real-world videos usually consist of multiple segments, each bearing different semantic information. To address this challenge, we introduce a novel paradigm dubbed as Gen-L-Video, capable of extending off-the-shelf short video diffusion models for generating and editing videos comprising hundreds of frames with diverse semantic segments without introducing additional training, all while preserving content consistency. We have implemented three mainstream text-driven video generation and editing methodologies and extended them to accommodate longer videos imbued with a variety of semantic segments with our proposed paradigm. Our experimental outcomes reveal that our approach significantly broadens the generative and editing capabilities of video diffusion models, offering new possibilities for future research and applications. The code is available at https://github.com/G-U-N/Gen-L-Video.",
        "authors": [
            "Fu-Yun Wang",
            "Wenshuo Chen",
            "Guanglu Song",
            "Han-Jia Ye",
            "Yu Liu",
            "Hongsheng Li"
        ],
        "citations": 62,
        "references": 76,
        "year": 2023
    },
    {
        "title": "ObjectStitch: Object Compositing with Diffusion Model",
        "abstract": "Object compositing based on 2D images is a challenging problem since it typically involves multiple processing stages such as color harmonization, geometry correction and shadow generation to generate realistic results. Furthermore, annotating training data pairs for compositing requires substantial manual effort from professionals, and is hardly scalable. Thus, with the recent advances in generative models, in this work, we propose a selfsupervised framework for object compositing by leveraging the power of conditional diffusion models. Our framework can hollistically address the object compositing task in a unified model, transforming the viewpoint, geometry, color and shadow of the generated object while requiring no manual labeling. To preserve the input object's characteristics, we introduce a content adaptor that helps to maintain categori-cal semantics and object appearance. A data augmentation method is further adopted to improve the fidelity of the generator. Our method outperforms relevant baselines in both realism and faithfulness of the synthesized result images in a user study on various real-world images.",
        "authors": [
            "Yi-Zhe Song",
            "Zhifei Zhang",
            "Zhe Lin",
            "Scott D. Cohen",
            "Brian L. Price",
            "Jianming Zhang",
            "S. Kim",
            "Daniel G. Aliaga"
        ],
        "citations": 61,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Audiobox: Unified Audio Generation with Natural Language Prompts",
        "abstract": "Audio is an essential part of our life, but creating it often requires expertise and is time-consuming. Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data. However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like\"a person speaking\"and would only generate mumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities. We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms. We allow transcript, vocal, and other audio styles to be controlled independently when generating speech. To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles. We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks. Our demo is available at https://audiobox.metademolab.com/",
        "authors": [
            "Apoorv Vyas",
            "Bowen Shi",
            "Matt Le",
            "Andros Tjandra",
            "Yi-Chiao Wu",
            "Baishan Guo",
            "Jiemin Zhang",
            "Xinyue Zhang",
            "Robert Adkins",
            "W.K.F. Ngan",
            "Jeff Wang",
            "Ivan Cruz",
            "Bapi Akula",
            "A. Akinyemi",
            "Brian Ellis",
            "Rashel Moritz",
            "Yael Yungster",
            "Alice Rakotoarison",
            "Liang Tan",
            "Chris Summers",
            "Carleigh Wood",
            "Joshua Lane",
            "Mary Williamson",
            "Wei-Ning Hsu"
        ],
        "citations": 60,
        "references": 77,
        "year": 2023
    },
    {
        "title": "LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On",
        "abstract": "The rapidly evolving fields of e-commerce and metaverse continue to seek innovative approaches to enhance the consumer experience. At the same time, recent advancements in the development of diffusion models have enabled generative networks to create remarkably realistic images. In this context, image-based virtual try-on, which consists in generating a novel image of a target model wearing a given in-shop garment, has yet to capitalize on the potential of these powerful generative solutions. This work introduces LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the Virtual Try-ON task. The proposed architecture relies on a latent diffusion model extended with a novel additional autoencoder module that exploits learnable skip connections to enhance the generation process preserving the model's characteristics. To effectively maintain the texture and details of the in-shop garment, we propose a textual inversion component that can map the visual features of the garment to the CLIP token embedding space and thus generate a set of pseudo-word token embeddings capable of conditioning the generation process. Experimental results on Dress Code and VITON-HD datasets demonstrate that our approach outperforms the competitors by a consistent margin, achieving a significant milestone for the task. Source code and trained models are publicly available at: https://github.com/miccunifi/ladi-vton.",
        "authors": [
            "Davide Morelli",
            "Alberto Baldrati",
            "Giuseppe Cartella",
            "Marcella Cornia",
            "Marco Bertini",
            "R. Cucchiara"
        ],
        "citations": 76,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation",
        "abstract": "Preparing training data for deep vision models is a labor-intensive task. To address this, generative models have emerged as an effective solution for generating synthetic data. While current generative models produce image-level category labels, we propose a novel method for generating pixel-level semantic segmentation labels using the text-to-image generative model Stable Diffusion (SD). By utilizing the text prompts, cross-attention, and self-attention of SD, we introduce three new techniques: class-prompt appending, class-prompt cross-attention, and self-attention exponentiation. These techniques enable us to generate segmentation maps corresponding to synthetic images. These maps serve as pseudo-labels for training semantic segmenters, eliminating the need for labor-intensive pixel-wise annotation. To account for the imperfections in our pseudo-labels, we incorporate uncertainty regions into the segmentation, allowing us to disregard loss from those regions. We conduct evaluations on two datasets, PASCAL VOC and MSCOCO, and our approach significantly outperforms concurrent work. Our benchmarks and code will be released at https://github.com/VinAIResearch/Dataset-Diffusion",
        "authors": [
            "Q. Nguyen",
            "T. Vu",
            "A. Tran",
            "Kim Dan Nguyen"
        ],
        "citations": 56,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Investigating Code Generation Performance of ChatGPT with Crowdsourcing Social Data",
        "abstract": "The recent advancements in Artificial Intelligence, particularly in large language models and generative models, are reshaping the field of software engineering by enabling innovative ways of performing various tasks, such as programming, debugging, and testing. However, few existing works have thoroughly explored the potential of AI in code generation and users’ attitudes toward AI-assisted coding tools. This knowledge gap leaves it unclear how AI is transforming software engineering and programming education. This paper presents a scalable crowdsourcing data-driven framework to investigate the code generation performance of generative large language models from diverse perspectives across multiple social media platforms. Specifically, we utilize ChatGPT, a popular generative large language model, as a representative example to reveal its insights and patterns in code generation. First, we propose a hybrid keyword word expansion method that integrates words suggested by topic modeling and expert knowledge to filter relevant social posts of interest on Twitter and Reddit. Then we collect 316K tweets and 3.2K Reddit posts about ChatGPT’s code generation, spanning from Dec. 1, 2022 to January 31, 2023. Our data analytics show that ChatGPT has been used in more than 10 programming languages, with Python and JavaScript being the two most popular, for a diverse range of tasks such as code debugging, interview preparation, and academic assignment solving. Surprisingly, our analysis shows that fear is the dominant emotion associated with ChatGPT’s code generation, overshadowing emotions of happiness, anger, surprise, and sadness. Furthermore, we construct a ChatGPT prompt and corresponding code dataset by analyzing the screen-shots of ChatGPT code generation shared on social media. This dataset enables us to evaluate the quality of the generated code, and we have released this dataset to the public. We believe the insights gained from our work will provide valuable guidance for future research on AI-powered code generation.",
        "authors": [
            "Yunhe Feng",
            "Sreecharan Vanam",
            "Manasa Cherukupally",
            "Weijian Zheng",
            "M. Qiu",
            "Haihua Chen"
        ],
        "citations": 57,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs",
        "abstract": "Large language models (LLMs) are increasingly becoming all-powerful and pervasive via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased, behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously before deployment. Existing auditing tools use either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and interview research experts in safe and fair AI, to build upon the auditing tool: AdaTest [36], which is powered by a generative LLM. Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of AdaTest++, the augmented tool, we conduct user studies with participants auditing two commercial language models: OpenAI’s GPT-3 and Azure’s sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis testing. Further, with our tool, users identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown in formal audits and also those previously under-reported.",
        "authors": [
            "Charvi Rastogi",
            "Marco Tulio Ribeiro",
            "Nicholas King",
            "Saleema Amershi"
        ],
        "citations": 56,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Pocket2Mol: Efficient Molecular Sampling Based on 3D Protein Pockets",
        "abstract": "Deep generative models have achieved tremendous success in designing novel drug molecules in recent years. A new thread of works have shown the great potential in advancing the specificity and success rate of in silico drug design by considering the structure of protein pockets. This setting posts fundamental computational challenges in sampling new chemical compounds that could satisfy multiple geometrical constraints imposed by pockets. Previous sampling algorithms either sample in the graph space or only consider the 3D coordinates of atoms while ignoring other detailed chemical structures such as bond types and functional groups. To address the challenge, we develop Pocket2Mol, an E(3)-equivariant generative network composed of two modules: 1) a new graph neural network capturing both spatial and bonding relationships between atoms of the binding pockets and 2) a new efficient algorithm which samples new drug candidates conditioned on the pocket representations from a tractable distribution without relying on MCMC. Experimental results demonstrate that molecules sampled from Pocket2Mol achieve significantly better binding affinity and other drug properties such as druglikeness and synthetic accessibility.",
        "authors": [
            "Xingang Peng",
            "Shitong Luo",
            "Jiaqi Guan",
            "Qi Xie",
            "Jian Peng",
            "Jianzhu Ma"
        ],
        "citations": 143,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
        "abstract": "The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym -- an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.",
        "authors": [
            "Pascal Notin",
            "M. Dias",
            "J. Frazer",
            "Javier Marchena-Hurtado",
            "Aidan N. Gomez",
            "D. Marks",
            "Y. Gal"
        ],
        "citations": 152,
        "references": 138,
        "year": 2022
    },
    {
        "title": "Game of algorithms: ChatGPT implications for the future of tourism education and research",
        "abstract": "PurposeThe paper aims to evaluate the ways ChatGPT is going to disrupt tourism education and research.Design/methodology/approachThis is a conceptual paper.FindingsChatGPT has the potential to revolutionize tourism education and research because it can do what students and researchers should do, namely, generate text (assignments and research papers). Universities will need to reevaluate their teaching and assessment strategies and incorporate generative language models in teaching. Publishers will need to be more receptive toward manuscripts that are partially generated by artificial intelligence. In the future, digital teachers and research assistants will take over many of the cognitive tasks of tourism educators and researchers.Originality/valueTo the authors’ best knowledge, this is one of the first academic papers that investigates the implications of ChatGPT to tourism education and research.",
        "authors": [
            "Stanislav Ivanov",
            "Mohammad Soliman"
        ],
        "citations": 79,
        "references": 17,
        "year": 2023
    },
    {
        "title": "LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes",
        "abstract": "With the widespread usage of VR devices and contents, demands for 3D scene generation techniques become more popular. Existing 3D scene generation models, however, limit the target scene to specific domain, primarily due to their training strategies using 3D scan dataset that is far from the real-world. To address such limitation, we propose LucidDreamer, a domain-free scene generation pipeline by fully leveraging the power of existing large-scale diffusion-based generative model. Our LucidDreamer has two alternate steps: Dreaming and Alignment. First, to generate multi-view consistent images from inputs, we set the point cloud as a geometrical guideline for each image generation. Specifically, we project a portion of point cloud to the desired view and provide the projection as a guidance for inpainting using the generative model. The inpainted images are lifted to 3D space with estimated depth maps, composing a new points. Second, to aggregate the new points into the 3D scene, we propose an aligning algorithm which harmoniously integrates the portions of newly generated 3D scenes. The finally obtained 3D scene serves as initial points for optimizing Gaussian splats. LucidDreamer produces Gaussian splats that are highly-detailed compared to the previous 3D scene generation methods, with no constraint on domain of the target scene. Project page: https://luciddreamer-cvlab.github.io/",
        "authors": [
            "Jaeyoung Chung",
            "Suyoung Lee",
            "Hyeongjin Nam",
            "Jaerin Lee",
            "Kyoung Mu Lee"
        ],
        "citations": 73,
        "references": 59,
        "year": 2023
    },
    {
        "title": "AI chatbots not yet ready for clinical use",
        "abstract": "As large language models (LLMs) expand and become more advanced, so do the natural language processing capabilities of conversational AI, or “chatbots”. OpenAI's recent release, ChatGPT, uses a transformer-based model to enable human-like text generation and question-answering on general domain knowledge, while a healthcare-specific Large Language Model (LLM) such as GatorTron has focused on the real-world healthcare domain knowledge. As LLMs advance to achieve near human-level performances on medical question and answering benchmarks, it is probable that Conversational AI will soon be developed for use in healthcare. In this article we discuss the potential and compare the performance of two different approaches to generative pretrained transformers—ChatGPT, the most widely used general conversational LLM, and Foresight, a GPT (generative pretrained transformer) based model focused on modelling patients and disorders. The comparison is conducted on the task of forecasting relevant diagnoses based on clinical vignettes. We also discuss important considerations and limitations of transformer-based chatbots for clinical use.",
        "authors": [
            "Joshua Au Yeung",
            "Z. Kraljevic",
            "Akish Luintel",
            "Alfred Balston",
            "Esther Idowu",
            "R. Dobson",
            "J. Teo"
        ],
        "citations": 84,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Protein Design with Guided Discrete Diffusion",
        "abstract": "A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to several therapeutic targets under locality and developability constraints, attaining a 99% expression rate and 40% binding rate in exploratory in vitro experiments.",
        "authors": [
            "Nate Gruver",
            "S. Stanton",
            "Nathan C Frey",
            "Tim G. J. Rudner",
            "I. Hotzel",
            "J. Lafrance-Vanasse",
            "A. Rajpal",
            "Kyunghyun Cho",
            "A. Wilson"
        ],
        "citations": 76,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Why and how to embrace AI such as ChatGPT in your academic life",
        "abstract": "Generative artificial intelligence (AI), including large language models (LLMs), is poised to transform scientific research, enabling researchers to elevate their research productivity. This article presents a how-to guide for employing LLMs in academic settings, focusing on their unique strengths, constraints and implications through the lens of philosophy of science and epistemology. Using ChatGPT as a case study, I identify and elaborate on three attributes contributing to its effectiveness—intelligence, versatility and collaboration—accompanied by tips on crafting effective prompts, practical use cases and a living resource online (https://osf.io/8vpwu/). Next, I evaluate the limitations of generative AI and its implications for ethical use, equality and education. Regarding ethical and responsible use, I argue from technical and epistemic standpoints that there is no need to restrict the scope or nature of AI assistance, provided that its use is transparently disclosed. A pressing challenge, however, lies in detecting fake research, which can be mitigated by embracing open science practices, such as transparent peer review and sharing data, code and materials. Addressing equality, I contend that while generative AI may promote equality for some, it may simultaneously exacerbate disparities for others—an issue with potentially significant yet unclear ramifications as it unfolds. Lastly, I consider the implications for education, advocating for active engagement with LLMs and cultivating students' critical thinking and analytical skills. The how-to guide seeks to empower researchers with the knowledge and resources necessary to effectively harness generative AI while navigating the complex ethical dilemmas intrinsic to its application.",
        "authors": [
            "Zhicheng Lin"
        ],
        "citations": 78,
        "references": 27,
        "year": 2023
    },
    {
        "title": "UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs",
        "abstract": "Text-to-image diffusion models have demonstrated re-markable capabilities in transforming text prompts into co-herent images, yet the computational cost of the multi-step inference remains a persistent challenge. To address this issue, we present UFOGen, a novel generative model de-signed for ultra-fast, one-step text-to-image generation. In contrast to conventional approaches that focus on improving samplers or employing distillation techniques for diffusion models, UFOGen adopts a hybrid methodology, inte-grating diffusion models with a GAN objective. Leveraging a newly introduced diffusion-GAN objective and initialization with pre-trained diffusion models, UFOGen excels in efficiently generating high-quality images conditioned on textual descriptions in a single step. Beyond traditional text-to-image generation, UFOGen showcases versatility in applications. Notably, UFOGen stands among the pioneering models enabling one-step text-to-image generation and diverse downstream tasks, presenting a significant advance-ment in the landscape of efficient generative models.",
        "authors": [
            "Yanwu Xu",
            "Yang Zhao",
            "Zhisheng Xiao",
            "Tingbo Hou"
        ],
        "citations": 74,
        "references": 74,
        "year": 2023
    },
    {
        "title": "ATT3D: Amortized Text-to-3D Object Synthesis",
        "abstract": "Text-to-3D modelling has seen exciting progress by combining generative text-to-image models with image-to-3D methods like Neural Radiance Fields. DreamFusion recently achieved high-quality results but requires a lengthy, per-prompt optimization to create 3D objects. To address this, we amortize optimization over text prompts by training on many prompts simultaneously with a unified model, instead of separately. With this, we share computation across a prompt set, training in less time than per-prompt optimization. Our framework – Amortized Text-to-3D (ATT3D) – enables knowledge sharing between prompts to generalize to unseen setups and smooth interpolations between text for novel assets and simple animations.",
        "authors": [
            "Jonathan Lorraine",
            "Kevin Xie",
            "Xiaohui Zeng",
            "Chen-Hsuan Lin",
            "Towaki Takikawa",
            "Nicholas Sharp",
            "Tsung-Yi Lin",
            "Ming-Yu Liu",
            "S. Fidler",
            "James Lucas"
        ],
        "citations": 70,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Denoising Diffusion Samplers",
        "abstract": "Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal. While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing theoretical results from denoising diffusion models also provide theoretical guarantees for DDS. We discuss the connections between DDS, optimal control and Schr\\\"odinger bridges and finally demonstrate DDS experimentally on a variety of challenging sampling tasks.",
        "authors": [
            "Francisco Vargas",
            "Will Grathwohl",
            "A. Doucet"
        ],
        "citations": 54,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
        "abstract": "As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs' generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods offer a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. By providing an in-depth review, this survey is expected to provide scholars with a deeper understanding of the methods' applications and encourage them to adapt existing techniques to the fast-growing field of LLMs.",
        "authors": [
            "Ruochen Zhao",
            "Hailin Chen",
            "Weishi Wang",
            "Fangkai Jiao",
            "Do Xuan Long",
            "Chengwei Qin",
            "Bosheng Ding",
            "Xiaobao Guo",
            "Minzhi Li",
            "Xingxuan Li",
            "Shafiq R. Joty"
        ],
        "citations": 58,
        "references": 255,
        "year": 2023
    },
    {
        "title": "Conditional Diffusion Probabilistic Model for Speech Enhancement",
        "abstract": "Speech enhancement is a critical component of many user-oriented audio applications, yet current systems still suffer from distorted and unnatural outputs. While generative models have shown strong potential in speech synthesis, they are still lagging behind in speech enhancement. This work leverages recent advances in diffusion probabilistic models, and proposes a novel speech enhancement algorithm that incorporates characteristics of the observed noisy speech signal into the diffusion and reverse processes. More specifically, we propose a generalized formulation of the diffusion probabilistic model named conditional diffusion probabilistic model that, in its reverse process, can adapt to non-Gaussian real noises in the estimated speech signal. In our experiments, we demonstrate strong performance of the proposed approach compared to representative generative models, and investigate the generalization capability of our models to other datasets with noise characteristics unseen during training.",
        "authors": [
            "Yen-Ju Lu",
            "Zhongqiu Wang",
            "Shinji Watanabe",
            "Alexander Richard",
            "Cheng Yu",
            "Yu Tsao"
        ],
        "citations": 146,
        "references": 36,
        "year": 2022
    },
    {
        "title": "Diffusion Probabilistic Modeling for Video Generation",
        "abstract": "Denoising diffusion probabilistic models are a promising new class of generative models that mark a milestone in high-quality image generation. This paper showcases their ability to sequentially generate video, surpassing prior methods in perceptual and probabilistic forecasting metrics. We propose an autoregressive, end-to-end optimized video diffusion model inspired by recent advances in neural video compression. The model successively generates future frames by correcting a deterministic next-frame prediction using a stochastic residual generated by an inverse diffusion process. We compare this approach against six baselines on four datasets involving natural and simulation-based videos. We find significant improvements in terms of perceptual quality and probabilistic frame forecasting ability for all datasets.",
        "authors": [
            "Ruihan Yang",
            "Prakhar Srivastava",
            "S. Mandt"
        ],
        "citations": 229,
        "references": 100,
        "year": 2022
    },
    {
        "title": "ZeroGen: Efficient Zero-shot Learning via Dataset Generation",
        "abstract": "There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZeroGen.Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL).Apart from being annotation-free and efficient, we argue that ZeroGen can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference, show the effectiveness of ZeroGen.",
        "authors": [
            "Jiacheng Ye",
            "Jiahui Gao",
            "Qintong Li",
            "Hang Xu",
            "Jiangtao Feng",
            "Zhiyong Wu",
            "Tao Yu",
            "Lingpeng Kong"
        ],
        "citations": 181,
        "references": 100,
        "year": 2022
    },
    {
        "title": "In-context Examples Selection for Machine Translation",
        "abstract": "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.",
        "authors": [
            "Sweta Agrawal",
            "Chunting Zhou",
            "M. Lewis",
            "Luke Zettlemoyer",
            "Marjan Ghazvininejad"
        ],
        "citations": 163,
        "references": 49,
        "year": 2022
    },
    {
        "title": "FLAME: Free-form Language-based Motion Synthesis & Editing",
        "abstract": "Text-based motion generation models are drawing a surge of interest for their potential for automating the motion-making process in the game, animation, or robot industries. In this paper, we propose a diffusion-based motion synthesis and editing model named FLAME. Inspired by the recent successes in diffusion models, we integrate diffusion-based generative models into the motion domain. FLAME can generate high-fidelity motions well aligned with the given text. Also, it can edit the parts of the motion, both frame-wise and joint-wise, without any fine-tuning. FLAME involves a new transformer-based architecture we devise to better handle motion data, which is found to be crucial to manage variable-length motions and well attend to free-form text. In experiments, we show that FLAME achieves state-of-the-art generation performances on three text-motion datasets: HumanML3D, BABEL, and KIT. We also demonstrate that FLAME’s editing capability can be extended to other tasks such as motion prediction or motion in-betweening, which have been previously covered by dedicated models.",
        "authors": [
            "Jihoon Kim",
            "Jiseob Kim",
            "Sungjoon Choi"
        ],
        "citations": 165,
        "references": 47,
        "year": 2022
    },
    {
        "title": "SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification",
        "abstract": "This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree-based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8× for distributed LLM inference and by 2.6-3.5× for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/",
        "authors": [
            "Xupeng Miao",
            "Gabriele Oliaro",
            "Zhihao Zhang",
            "Xinhao Cheng",
            "Zeyu Wang",
            "Zhengxin Zhang",
            "Rae Ying Yee Wong",
            "Alan Zhu",
            "Lijie Yang",
            "Xiaoxiang Shi",
            "Chunan Shi",
            "Zhuoming Chen",
            "Daiyaan Arfeen",
            "Reyna Abhyankar",
            "Zhihao Jia"
        ],
        "citations": 66,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Social Dynamics of AI Support in Creative Writing",
        "abstract": "Recently, large language models have made huge advances in generating coherent, creative text. While much research focuses on how users can interact with language models, less work considers the social-technical gap that this technology poses. What are the social nuances that underlie receiving support from a generative AI? In this work we ask when and why a creative writer might turn to a computer versus a peer or mentor for support. We interview 20 creative writers about their writing practice and their attitudes towards both human and computer support. We discover three elements that govern a writer’s interaction with support actors: 1) what writers desire help with, 2) how writers perceive potential support actors, and 3) the values writers hold. We align our results with existing frameworks of writing cognition and creativity support, uncovering the social dynamics which modulate user responses to generative technologies.",
        "authors": [
            "K. Gero",
            "Tao Long",
            "Lydia B. Chilton"
        ],
        "citations": 66,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Flexible Diffusion Modeling of Long Videos",
        "abstract": "We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.",
        "authors": [
            "William Harvey",
            "Saeid Naderiparizi",
            "Vaden Masrani",
            "Christian Weilbach",
            "Frank Wood"
        ],
        "citations": 241,
        "references": 44,
        "year": 2022
    },
    {
        "title": "VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids",
        "abstract": "State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to parameterize 3D radiance fields. While demonstrating impressive results, querying an MLP for every sample along each ray leads to slow rendering. Therefore, existing approaches often render low-resolution feature maps and process them with an upsampling network to obtain the final image. Albeit efficient, neural rendering often entangles viewpoint and content such that changing the camera pose results in unwanted changes of geometry or appearance. Motivated by recent results in voxel-based novel view synthesis, we investigate the utility of sparse voxel grid representations for fast and 3D-consistent generative modeling in this paper. Our results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization. To obtain a compact representation of the scene and allow for scaling to higher voxel resolutions, our model disentangles the foreground object (modeled in 3D) from the background (modeled in 2D). In contrast to existing approaches, our method requires only a single forward pass to generate a full 3D scene. It hence allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.",
        "authors": [
            "Katja Schwarz",
            "Axel Sauer",
            "Michael Niemeyer",
            "Yiyi Liao",
            "Andreas Geiger"
        ],
        "citations": 136,
        "references": 57,
        "year": 2022
    },
    {
        "title": "CM3: A Causal Masked Multimodal Model of the Internet",
        "abstract": "We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.",
        "authors": [
            "Armen Aghajanyan",
            "Po-Yao (Bernie) Huang",
            "Candace Ross",
            "Vladimir Karpukhin",
            "Hu Xu",
            "Naman Goyal",
            "Dmytro Okhonko",
            "Mandar Joshi",
            "Gargi Ghosh",
            "M. Lewis",
            "Luke Zettlemoyer"
        ],
        "citations": 145,
        "references": 68,
        "year": 2022
    },
    {
        "title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation",
        "abstract": "Due to the remarkable progress of deep generative models, animating images has become increasingly efficient, whereas associated results have become increasingly realistic. Current animation-approaches commonly exploit structure representation extracted from driving videos. Such structure representation is instrumental in transferring motion from driving videos to still images. However, such approaches fail in case the source image and driving video encompass large appearance variation. Moreover, the extraction of structure information requires additional modules that endow the animation-model with increased complexity. Deviating from such models, we here introduce the Latent Image Animator (LIA), a self-supervised autoencoder that evades need for structure representation. LIA is streamlined to animate images by linear navigation in the latent space. Specifically, motion in generated video is constructed by linear displacement of codes in the latent space. Towards this, we learn a set of orthogonal motion directions simultaneously, and use their linear combination, in order to represent any displacement in the latent space. Extensive quantitative and qualitative analysis suggests that our model systematically and significantly outperforms state-of-art methods on VoxCeleb, Taichi and TED-talk datasets w.r.t. generated quality.",
        "authors": [
            "Yaohui Wang",
            "Di Yang",
            "F. Brémond",
            "A. Dantcheva"
        ],
        "citations": 128,
        "references": 65,
        "year": 2022
    },
    {
        "title": "How AI can distort human beliefs",
        "abstract": "Models can convey biases and false information to users Individual humans form their beliefs by sampling a small subset of the available data in the world. Once those beliefs are formed with high certainty, they can become stubborn to revise. Fabrication and bias in generative artificial intelligence (AI) models are established phenomena that can occur as part of regular system use, in the absence of any malevolent forces seeking to push bias or disinformation. However, transmission of false information and bias from these models to people has been prominently absent from the discourse. Overhyped, unrealistic, and exaggerated capabilities permeate how generative AI models are presented, which contributes to the popular misconception that these models exceed human-level reasoning and exacerbates the risk of transmission of false information and negative stereotypes to people.",
        "authors": [
            "Celeste Kidd",
            "Abeba Birhane"
        ],
        "citations": 63,
        "references": 13,
        "year": 2023
    },
    {
        "title": "Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning",
        "abstract": "Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \\textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, we find \\textsc{MTDiff} outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data synthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given a single demonstration as a prompt, which enhances the low-quality datasets for even unseen tasks.",
        "authors": [
            "Haoran He",
            "Chenjia Bai",
            "Kang Xu",
            "Zhuoran Yang",
            "Weinan Zhang",
            "Dong Wang",
            "Bingyan Zhao",
            "Xuelong Li"
        ],
        "citations": 62,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Posterior Collapse and Latent Variable Non-identifiability",
        "abstract": "Variational autoencoders model high-dimensional data by positing low-dimensional latent variables that are mapped through a flexible distribution parametrized by a neural network. Unfortunately, variational autoencoders often suffer from posterior collapse: the posterior of the latent variables is equal to its prior, rendering the variational autoencoder useless as a means to produce meaningful representations. Existing approaches to posterior collapse often attribute it to the use of neural networks or optimization issues due to variational approximation. In this paper, we consider posterior collapse as a problem of latent variable non-identifiability. We prove that the posterior collapses if and only if the latent variables are non-identifiable in the generative model. This fact implies that posterior collapse is not a phenomenon specific to the use of flexible distributions or approximate inference. Rather, it can occur in classical probabilistic models even with exact inference, which we also demonstrate. Based on these results, we propose a class of latent-identifiable variational autoencoders, deep generative models which enforce identifiability without sacrificing flexibility. This model class resolves the problem of latent variable non-identifiability by leveraging bijective Brenier maps and parameterizing them with input convex neural networks, without special variational inference objectives or optimization tricks. Across synthetic and real datasets, latent-identifiable variational autoencoders outperform existing methods in mitigating posterior collapse and providing meaningful representations of the data.",
        "authors": [
            "Yixin Wang",
            "D. Blei",
            "J. Cunningham"
        ],
        "citations": 62,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Survey on Synthetic Data Generation, Evaluation Methods and GANs",
        "abstract": "Synthetic data consists of artificially generated data. When data are scarce, or of poor quality, synthetic data can be used, for example, to improve the performance of machine learning models. Generative adversarial networks (GANs) are a state-of-the-art deep generative models that can generate novel synthetic samples that follow the underlying data distribution of the original dataset. Reviews on synthetic data generation and on GANs have already been written. However, none in the relevant literature, to the best of our knowledge, has explicitly combined these two topics. This survey aims to fill this gap and provide useful material to new researchers in this field. That is, we aim to provide a survey that combines synthetic data generation and GANs, and that can act as a good and strong starting point for new researchers in the field, so that they have a general overview of the key contributions and useful references. We have conducted a review of the state-of-the-art by querying four major databases: Web of Sciences (WoS), Scopus, IEEE Xplore, and ACM Digital Library. This allowed us to gain insights into the most relevant authors, the most relevant scientific journals in the area, the most cited papers, the most significant research areas, the most important institutions, and the most relevant GAN architectures. GANs were thoroughly reviewed, as well as their most common training problems, their most important breakthroughs, and a focus on GAN architectures for tabular data. Further, the main algorithms for generating synthetic data, their applications and our thoughts on these methods are also expressed. Finally, we reviewed the main techniques for evaluating the quality of synthetic data (especially tabular data) and provided a schematic overview of the information presented in this paper.",
        "authors": [
            "Á. Figueira",
            "Bruno Vaz"
        ],
        "citations": 162,
        "references": 0,
        "year": 2022
    },
    {
        "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
        "abstract": "A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.",
        "authors": [
            "Alisa Liu",
            "Swabha Swayamdipta",
            "Noah A. Smith",
            "Yejin Choi"
        ],
        "citations": 200,
        "references": 83,
        "year": 2022
    },
    {
        "title": "What ChatGPT Tells Us about Gender: A Cautionary Tale about Performativity and Gender Biases in AI",
        "abstract": "Large language models and generative AI, such as ChatGPT, have gained influence over people’s personal lives and work since their launch, and are expected to scale even further. While the promises of generative artificial intelligence are compelling, this technology harbors significant biases, including those related to gender. Gender biases create patterns of behavior and stereotypes that put women, men and gender-diverse people at a disadvantage. Gender inequalities and injustices affect society as a whole. As a social practice, gendering is achieved through the repeated citation of rituals, expectations and norms. Shared understandings are often captured in scripts, including those emerging in and from generative AI, which means that gendered views and gender biases get grafted back into social, political and economic life. This paper’s central argument is that large language models work performatively, which means that they perpetuate and perhaps even amplify old and non-inclusive understandings of gender. Examples from ChatGPT are used here to illustrate some gender biases in AI. However, this paper also puts forward that AI can work to mitigate biases and act to ‘undo gender’.",
        "authors": [
            "Nicole Gross"
        ],
        "citations": 62,
        "references": 69,
        "year": 2023
    },
    {
        "title": "SpectralGPT: Spectral Foundation Model",
        "abstract": "—The foundation model has recently garnered significant attention due to its potential to revolutionize the field of visual representation learning in a self-supervised manner. While most foundation models are tailored to effectively process RGB images for various visual tasks, there is a noticeable gap in research focused on spectral data, which offers valuable information for scene understanding, especially in remote sensing (RS) applications. To fill this gap, we created for the first time a universal RS foundation model, named SpectralGPT, which is purpose-built to handle spectral RS images using a novel 3D generative pretrained transformer (GPT). Compared to existing foundation models, SpectralGPT 1) accommodates input images with varying sizes, resolutions, time series, and regions in a progressive training fashion, enabling full utilization of extensive RS big data; 2) leverages 3D token generation for spatial-spectral coupling; 3) captures spectrally sequential patterns via",
        "authors": [
            "D. Hong",
            "Bing Zhang",
            "Xuyang Li",
            "Yuxuan Li",
            "Chenyu Li",
            "Jing Yao",
            "N. Yokoya",
            "Hao Li",
            "Pedram Ghamisi",
            "Xiuping Jia",
            "Antonio J. Plaza",
            "Paolo Gamba",
            "J. Benediktsson",
            "J. Chanussot"
        ],
        "citations": 61,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus",
        "abstract": "The need for large corpora raw corpora has dramatically increased in recent years with the introduction of transfer learning and semi-supervised learning methods to Natural Language Processing. And while there have been some recent attempts to manually curate the amount of data necessary to train large language models, the main way to obtain this data is still through automatic web crawling. In this paper we take the existing multilingual web corpus OSCAR and its pipeline Ungoliant that extracts and classifies data from Common Crawl at the line level, and propose a set of improvements and automatic annotations in order to produce a new document-oriented version of OSCAR that could prove more suitable to pre-train large generative language models as well as hopefully other applications in Natural Language Processing and Digital Humanities.",
        "authors": [
            "Julien Abadji",
            "Pedro Ortiz Suarez",
            "Laurent Romary",
            "Benoît Sagot"
        ],
        "citations": 139,
        "references": 49,
        "year": 2022
    },
    {
        "title": "Efficiently Scaling Transformer Inference",
        "abstract": "We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.",
        "authors": [
            "Reiner Pope",
            "Sholto Douglas",
            "Aakanksha Chowdhery",
            "Jacob Devlin",
            "James Bradbury",
            "Anselm Levskaya",
            "J. Heek",
            "Kefan Xiao",
            "Shivani Agrawal",
            "J. Dean"
        ],
        "citations": 222,
        "references": 50,
        "year": 2022
    },
    {
        "title": "The Role of ImageNet Classes in Fréchet Inception Distance",
        "abstract": "Fr\\'echet Inception Distance (FID) is the primary metric for ranking models in data-driven generative modeling. While remarkably successful, the metric is known to sometimes disagree with human judgement. We investigate a root cause of these discrepancies, and visualize what FID\"looks at\"in generated images. We show that the feature space that FID is (typically) computed in is so close to the ImageNet classifications that aligning the histograms of Top-$N$ classifications between sets of generated and real images can reduce FID substantially -- without actually improving the quality of results. Thus, we conclude that FID is prone to intentional or accidental distortions. As a practical example of an accidental distortion, we discuss a case where an ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while being worse in terms of human evaluation.",
        "authors": [
            "Tuomas Kynkaanniemi",
            "Tero Karras",
            "M. Aittala",
            "Timo Aila",
            "J. Lehtinen"
        ],
        "citations": 176,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Autoregressive Diffusion Model for Graph Generation",
        "abstract": "Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \\emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \\emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \\emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutation invariance of graph, we show that the two networks can be jointly trained by optimizing a simple lower bound of data likelihood. Our experiments on six diverse generic graph datasets and two molecule datasets show that our model achieves better or comparable generation performance with previous state-of-the-art, and meanwhile enjoys fast generation speed.",
        "authors": [
            "Lingkai Kong",
            "Jiaming Cui",
            "Haotian Sun",
            "Yuchen Zhuang",
            "B. Prakash",
            "Chao Zhang"
        ],
        "citations": 48,
        "references": 54,
        "year": 2023
    },
    {
        "title": "mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections",
        "abstract": "Large-scale pre-trained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from inefficiency and linguistic signal overwhelmed by long visual sequences in cross-modal alignment. To address both problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections.mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, including image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability on vision-language and video-language tasks. The code and pre-trained models are available at https://github.com/alibaba/AliceMind",
        "authors": [
            "Chenliang Li",
            "Haiyang Xu",
            "Junfeng Tian",
            "Wei Wang",
            "Ming Yan",
            "Bin Bi",
            "Jiabo Ye",
            "Hehong Chen",
            "Guohai Xu",
            "Zheng-da Cao",
            "Ji Zhang",
            "Songfang Huang",
            "Feiran Huang",
            "Jingren Zhou",
            "Luo Si"
        ],
        "citations": 185,
        "references": 85,
        "year": 2022
    },
    {
        "title": "PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation",
        "abstract": "Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.",
        "authors": [
            "Yingchaojie Feng",
            "Xingbo Wang",
            "Kamkwai Wong",
            "Sijia Wang",
            "Yuhong Lu",
            "Minfeng Zhu",
            "Baicheng Wang",
            "Wei Chen"
        ],
        "citations": 57,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Fast protein backbone generation with SE(3) flow matching",
        "abstract": "We present FrameFlow, a method for fast protein backbone generation using SE(3) flow matching. Specifically, we adapt FrameDiff, a state-of-the-art diffusion model, to the flow-matching generative modeling paradigm. We show how flow matching can be applied on SE(3) and propose modifications during training to effectively learn the vector field. Compared to FrameDiff, FrameFlow requires five times fewer sampling timesteps while achieving two fold better designability. The ability to generate high quality protein samples at a fraction of the cost of previous methods paves the way towards more efficient generative models in de novo protein design.",
        "authors": [
            "Jason Yim",
            "Andrew Campbell",
            "Andrew Y. K. Foong",
            "Michael Gastegger",
            "Jos'e Jim'enez-Luna",
            "Sarah Lewis",
            "Victor Garcia Satorras",
            "Bastiaan S. Veeling",
            "R. Barzilay",
            "T. Jaakkola",
            "Frank No'e"
        ],
        "citations": 45,
        "references": 27,
        "year": 2023
    },
    {
        "title": "SE(3)-Stochastic Flow Matching for Protein Backbone Generation",
        "abstract": "The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce FoldFlow, a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\\mathrm{D}$ rigid motions -- i.e. the group $\\text{SE}(3)$ -- enabling accurate modeling of protein backbones. We first introduce FoldFlow-Base, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\\text{SE}(3)$. We next accelerate training by incorporating Riemannian optimal transport to create FoldFlow-OT, leading to the construction of both more simple and stable flows. Finally, we design FoldFlow-SFM, coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\\text{SE}(3)$. Our family of FoldFlow, generative models offers several key advantages over previous approaches to the generative modeling of proteins: they are more stable and faster to train than diffusion-based approaches, and our models enjoy the ability to map any invariant source distribution to any invariant target distribution over $\\text{SE}(3)$. Empirically, we validate FoldFlow, on protein backbone generation of up to $300$ amino acids leading to high-quality designable, diverse, and novel samples.",
        "authors": [
            "A. Bose",
            "Tara Akhound-Sadegh",
            "Kilian Fatras",
            "Guillaume Huguet",
            "Jarrid Rector-Brooks",
            "Cheng-Hao Liu",
            "A. Nica",
            "Maksym Korablyov",
            "Michael M. Bronstein",
            "Alexander Tong"
        ],
        "citations": 47,
        "references": 105,
        "year": 2023
    },
    {
        "title": "ITI-Gen: Inclusive Text-to-Image Generation",
        "abstract": "Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that \"a picture is worth a thousand words\". We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-Gen1, that leverages readily available reference images for Inclusive Text-to-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-Gen requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-Gen largely improves over state-of-the-art models to generate inclusive images from a prompt.",
        "authors": [
            "Cheng Zhang",
            "Xuanbai Chen",
            "Siqi Chai",
            "Chen Henry Wu",
            "Dmitry Lagun",
            "T. Beeler",
            "Fernando De la Torre"
        ],
        "citations": 38,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Non-Denoising Forward-Time Diffusions",
        "abstract": "The scope of this paper is generative modeling through diffusion processes. An approach falling within this paradigm is the work of Song et al. (2021), which relies on a time-reversal argument to construct a diffusion process targeting the desired data distribution. We show that the time-reversal argument, common to all denoising diffusion probabilistic modeling proposals, is not necessary. We obtain diffusion processes targeting the desired data distribution by taking appropriate mixtures of diffusion bridges. The resulting transport is exact by construction, allows for greater flexibility in choosing the dynamics of the underlying diffusion, and can be approximated by means of a neural network via novel training objectives. We develop a unifying view of the drift adjustments corresponding to our and to time-reversal approaches and make use of this representation to inspect the inner workings of diffusion-based generative models. Finally, we leverage on scalable simulation and inference techniques common in spatial statistics to move beyond fully factorial distributions in the underlying diffusion dynamics. The methodological advances contained in this work contribute toward establishing a general framework for generative modeling based on diffusion processes.",
        "authors": [
            "Stefano Peluchetti"
        ],
        "citations": 42,
        "references": 25,
        "year": 2023
    },
    {
        "title": "AG3D: Learning to Generate 3D Avatars from 2D Image Collections",
        "abstract": "While progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses. In this paper, we propose a new adversarial generative model of realistic 3D people learned from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient, flexible, articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps. We experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies.",
        "authors": [
            "Zijian Dong",
            "Xu Chen",
            "Jinlong Yang",
            "Michael J. Black",
            "Otmar Hilliges",
            "Andreas Geiger"
        ],
        "citations": 41,
        "references": 75,
        "year": 2023
    },
    {
        "title": "ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting.",
        "authors": [
            "Rongjie Huang",
            "Zhou Zhao",
            "Huadai Liu",
            "Jinglin Liu",
            "Chenye Cui",
            "Yi Ren"
        ],
        "citations": 169,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion",
        "abstract": "Text-to-image generation is a significant domain in modern computer vision and has achieved substantial improvements through the evolution of generative architectures. Among these, there are diffusion-based models that have demonstrated essential quality enhancements. These models are generally split into two categories: pixel-level and latent-level approaches. We present Kandinsky1, a novel exploration of latent diffusion architecture, combining the principles of the image prior models with latent diffusion techniques. The image prior model is trained separately to map text embeddings to image embeddings of CLIP. Another distinct feature of the proposed model is the modified MoVQ implementation, which serves as the image autoencoder component. Overall, the designed model contains 3.3B parameters. We also deployed a user-friendly demo system that supports diverse generative modes such as text-to-image generation, image fusion, text and image fusion, image variations generation, and text-guided inpainting/outpainting. Additionally, we released the source code and checkpoints for the Kandinsky models. Experimental evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking our model as the top open-source performer in terms of measurable image generation quality.",
        "authors": [
            "Anton Razzhigaev",
            "Arseniy Shakhmatov",
            "Anastasia Maltseva",
            "V.Ya. Arkhipkin",
            "Igor Pavlov",
            "Ilya Ryabov",
            "Angelina Kuts",
            "Alexander Panchenko",
            "Andrey Kuznetsov",
            "Denis Dimitrov"
        ],
        "citations": 51,
        "references": 44,
        "year": 2023
    },
    {
        "title": "LayoutDM: Transformer-based Diffusion Model for Layout Generation",
        "abstract": "Automatic layout generation that can synthesize high-quality layouts is an important tool for graphic design in many applications. Though existing methods based on generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) have progressed, they still leave much room for improving the quality and diversity of the results. Inspired by the recent success of diffusion models in generating high-quality images, this paper explores their potential for conditional layout generation and proposes Transformer-based Layout Diffusion Model (LayoutDM) by instantiating the conditional denoising diffusion probabilistic model (DDPM) with a purely transformer-based architecture. Instead of using convolutional neural networks, a transformer-based conditional Layout Denoiser is proposed to learn the reverse diffusion process to generate samples from noised layout data. Benefitting from both transformer and DDPM, our LayoutDM is of desired properties such as high-quality generation, strong sample diversity, faithful distribution coverage, and stationary training in comparison to GANs and VAEs. Quantitative and qualitative experimental results show that our method outperforms state-of-the-art generative models in terms of quality and diversity.",
        "authors": [
            "Shang Chai",
            "Liansheng Zhuang",
            "Feng Yan"
        ],
        "citations": 40,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Dirichlet Diffusion Score Model for Biological Sequence Generation",
        "abstract": "Designing biological sequences is an important challenge that requires satisfying complex constraints and thus is a natural problem to address with deep generative modeling. Diffusion generative models have achieved considerable success in many applications. Score-based generative stochastic differential equations (SDE) model is a continuous-time diffusion model framework that enjoys many benefits, but the originally proposed SDEs are not naturally designed for modeling discrete data. To develop generative SDE models for discrete data such as biological sequences, here we introduce a diffusion process defined in the probability simplex space with stationary distribution being the Dirichlet distribution. This makes diffusion in continuous space natural for modeling discrete data. We refer to this approach as Dirchlet diffusion score model. We demonstrate that this technique can generate samples that satisfy hard constraints using a Sudoku generation task. This generative model can also solve Sudoku, including hard puzzles, without additional training. Finally, we applied this approach to develop the first human promoter DNA sequence design model and showed that designed sequences share similar properties with natural promoter sequences.",
        "authors": [
            "P. Avdeyev",
            "Chenlai Shi",
            "Yuhao Tan",
            "Kseniia Dudnyk",
            "Jian Zhou"
        ],
        "citations": 37,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter",
        "abstract": "The pre-trained text-image discriminative models, such as CLIP, has been explored for open-vocabulary semantic segmentation with unsatisfactory results due to the loss of crucial localization information and awareness of object shapes. Recently, there has been a growing interest in expanding the application of generative models from generation tasks to semantic segmentation. These approaches utilize generative models either for generating annotated data or extracting features to facilitate semantic segmentation. This typically involves generating a considerable amount of synthetic data or requiring additional mask annotations. To this end, we uncover the potential of generative text-to-image diffusion models (e.g., Stable Diffusion) as highly efficient open-vocabulary semantic segmenters, and introduce a novel training-free approach named DiffSegmenter. The insight is that to generate realistic objects that are semantically faithful to the input text, both the complete object shapes and the corresponding semantics are implicitly learned by diffusion models. We discover that the object shapes are characterized by the self-attention maps while the semantics are indicated through the cross-attention maps produced by the denoising U-Net, forming the basis of our segmentation results.Additionally, we carefully design effective textual prompts and a category filtering mechanism to further enhance the segmentation results. Extensive experiments on three benchmark datasets show that the proposed DiffSegmenter achieves impressive results for open-vocabulary semantic segmentation.",
        "authors": [
            "Jinglong Wang",
            "Xiawei Li",
            "Jing Zhang",
            "Qingyuan Xu",
            "Qin Zhou",
            "Qian Yu",
            "Lu Sheng",
            "Dong Xu"
        ],
        "citations": 37,
        "references": 50,
        "year": 2023
    },
    {
        "title": "ResGen is a pocket-aware 3D molecular generation model based on parallel multiscale modelling",
        "abstract": null,
        "authors": [
            "Odin Zhang",
            "Jintu Zhang",
            "Jieyu Jin",
            "Xujun Zhang",
            "Renling Hu",
            "Chao Shen",
            "Hanqun Cao",
            "Hongyan Du",
            "Yu Kang",
            "Yafeng Deng",
            "Furui Liu",
            "Guangyong Chen",
            "Chang-Yu Hsieh",
            "Tingjun Hou"
        ],
        "citations": 40,
        "references": 76,
        "year": 2023
    },
    {
        "title": "DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises",
        "abstract": "While diffusion models have achieved great success in generating continuous signals such as images and audio, it remains elusive for diffusion models in learning discrete sequence data like natural languages. Although recent advances circumvent this challenge of discreteness by embedding discrete tokens as continuous surrogates, they still fall short of satisfactory generation quality. To understand this, we first dive deep into the denoised training protocol of diffusion-based sequence generative models and determine their three severe problems, i.e., 1) failing to learn, 2) lack of scalability, and 3) neglecting source conditions. We argue that these problems can be boiled down to the pitfall of the not completely eliminated discreteness in the embedding space, and the scale of noises is decisive herein. In this paper, we introduce DINOISER to facilitate diffusion models for sequence generation by manipulating noises. We propose to adaptively determine the range of sampled noise scales for counter-discreteness training; and encourage the proposed diffused sequence learner to leverage source conditions with amplified noise scales during inference. Experiments show that DINOISER enables consistent improvement over the baselines of previous diffusion-based sequence generative models on several conditional sequence modeling benchmarks thanks to both effective training and inference strategies. Analyses further verify that DINOISER can make better use of source conditions to govern its generative process.",
        "authors": [
            "Jiasheng Ye",
            "Zaixiang Zheng",
            "Yu Bao",
            "Lihua Qian",
            "Mingxuan Wang"
        ],
        "citations": 38,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Few-shot training LLMs for project-specific code-summarization",
        "abstract": "Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.",
        "authors": [
            "Toufique Ahmed",
            "Prem Devanbu"
        ],
        "citations": 177,
        "references": 25,
        "year": 2022
    },
    {
        "title": "FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hindered their applications to speech synthesis. This paper proposes FastDiff, a fast conditional diffusion model for high-quality speech synthesis. FastDiff employs a stack of time-aware location-variable convolutions of diverse receptive field patterns to efficiently model long-term time dependencies with adaptive conditions. A noise schedule predictor is also adopted to reduce the sampling steps without sacrificing the generation quality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer, FastDiff-TTS, which generates high-fidelity speech waveforms without any intermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff demonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech samples. Also, FastDiff enables a sampling speed of 58x faster than real-time on a V100 GPU, making diffusion models practically applicable to speech synthesis deployment for the first time. We further show that FastDiff generalized well to the mel-spectrogram inversion of unseen speakers, and FastDiff-TTS outperformed other competing methods in end-to-end text-to-speech synthesis. Audio samples are available at https://FastDiff.github.io/.",
        "authors": [
            "Rongjie Huang",
            "Max W. Y. Lam",
            "J. Wang",
            "Dan Su",
            "Dong Yu",
            "Yi Ren",
            "Zhou Zhao"
        ],
        "citations": 144,
        "references": 31,
        "year": 2022
    },
    {
        "title": "Membership Inference Attacks against Synthetic Data through Overfitting Detection",
        "abstract": "Data is the foundation of most science. Unfortunately, sharing data can be obstructed by the risk of violating data privacy, impeding research in fields like healthcare. Synthetic data is a potential solution. It aims to generate data that has the same distribution as the original data, but that does not disclose information about individuals. Membership Inference Attacks (MIAs) are a common privacy attack, in which the attacker attempts to determine whether a particular real sample was used for training of the model. Previous works that propose MIAs against generative models either display low performance -- giving the false impression that data is highly private -- or need to assume access to internal generative model parameters -- a relatively low-risk scenario, as the data publisher often only releases synthetic data, not the model. In this work we argue for a realistic MIA setting that assumes the attacker has some knowledge of the underlying data distribution. We propose DOMIAS, a density-based MIA model that aims to infer membership by targeting local overfitting of the generative model. Experimentally we show that DOMIAS is significantly more successful at MIA than previous work, especially at attacking uncommon samples. The latter is disconcerting since these samples may correspond to underrepresented groups. We also demonstrate how DOMIAS' MIA performance score provides an interpretable metric for privacy, giving data publishers a new tool for achieving the desired privacy-utility trade-off in their synthetic data.",
        "authors": [
            "B. V. Breugel",
            "Hao Sun",
            "Zhaozhi Qian",
            "M. Schaar"
        ],
        "citations": 35,
        "references": 47,
        "year": 2023
    },
    {
        "title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections",
        "abstract": "Inverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512x256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves state-of-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to\"inverse-graphics\"diverse human bodies with a clean framework.",
        "authors": [
            "Fangzhou Hong",
            "Zhaoxi Chen",
            "Yushi Lan",
            "Liang Pan",
            "Ziwei Liu"
        ],
        "citations": 105,
        "references": 75,
        "year": 2022
    },
    {
        "title": "PromptPaint: Steering Text-to-Image Generation Through Paint Medium-like Interactions",
        "abstract": "While diffusion-based text-to-image (T2I) models provide a simple and powerful way to generate images, guiding this generation remains a challenge. For concepts that are difficult to describe through language, users may struggle to create prompts. Moreover, many of these models are built as end-to-end systems, lacking support for iterative shaping of the image. In response, we introduce PromptPaint, which combines T2I generation with interactions that model how we use colored paints. PromptPaint allows users to go beyond language to mix prompts that express challenging concepts. Just as we iteratively tune colors through layered placements of paint on a physical canvas, PromptPaint similarly allows users to apply different prompts to different canvas areas and times of the generative process. Through a set of studies, we characterize different approaches for mixing prompts, design trade-offs, and socio-technical challenges for generative models. With PromptPaint we provide insight into future steerable generative tools.",
        "authors": [
            "John Joon Young Chung",
            "Eytan Adar"
        ],
        "citations": 32,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Computational Scoring and Experimental Evaluation of Enzymes Generated by Neural Networks",
        "abstract": "In recent years, generative protein sequence models have been developed to sample novel sequences. However, predicting whether generated proteins will fold and function remains challenging. We evaluate computational metrics to assess the quality of enzyme sequences produced by three contrasting generative models: ancestral sequence reconstruction, a generative adversarial network, and a protein language model. Focusing on two enzyme families, we expressed and purified over 440 natural and generated sequences with 70-90% identity to the most similar natural sequences to benchmark computational metrics for predicting in vitro enzyme activity. Over three rounds of experiments, we developed a computational filter that improved experimental success rates by 44-100%. Surprisingly, neither sequence identity to natural sequences nor AlphaFold2 residue-confidence scores were predictive of enzyme activity. The proposed metrics and models will drive protein engineering research by serving as a benchmark for generative protein sequence models and helping to select active variants to test experimentally.",
        "authors": [
            "Sean R. Johnson",
            "Xiaozhi Fu",
            "Sandra Viknander",
            "Clara Goldin",
            "Sarah Monaco",
            "Aleksej Zelezniak",
            "Kevin Kaichuang Yang"
        ],
        "citations": 29,
        "references": 85,
        "year": 2023
    },
    {
        "title": "Data Augmentation techniques in time series domain: a survey and taxonomy",
        "abstract": null,
        "authors": [
            "Guillermo Iglesias",
            "Edgar Talavera",
            "Á. González-Prieto",
            "Alberto Mozo",
            "S. Gómez-Canaval"
        ],
        "citations": 110,
        "references": 133,
        "year": 2022
    },
    {
        "title": "An Introduction to Neural Data Compression",
        "abstract": "Neural compression is the application of neural networks and other machine learning methods to data compression. Recent advances in statistical machine learning have opened up new possibilities for data compression, allowing compression algorithms to be learned end-to-end from data using powerful generative models such as normalizing flows, variational autoencoders, diffusion probabilistic models, and generative adversarial networks. The present article aims to introduce this field of research to a broader machine learning audience by reviewing the necessary background in information theory (e.g., entropy coding, rate-distortion theory) and computer vision (e.g., image quality assessment, perceptual metrics), and providing a curated guide through the essential ideas and methods in the literature thus far.",
        "authors": [
            "Yibo Yang",
            "S. Mandt",
            "Lucas Theis"
        ],
        "citations": 99,
        "references": 223,
        "year": 2022
    },
    {
        "title": "GENIE: Higher-Order Denoising Diffusion Solvers",
        "abstract": "Denoising diffusion models (DDMs) have emerged as a powerful class of generative models. A forward diffusion process slowly perturbs the data, while a deep model learns to gradually denoise. Synthesis amounts to solving a differential equation (DE) defined by the learnt model. Solving the DE requires slow iterative solvers for high-quality generation. In this work, we propose Higher-Order Denoising Diffusion Solvers (GENIE): Based on truncated Taylor methods, we derive a novel higher-order solver that significantly accelerates synthesis. Our solver relies on higher-order gradients of the perturbed data distribution, that is, higher-order score functions. In practice, only Jacobian-vector products (JVPs) are required and we propose to extract them from the first-order score network via automatic differentiation. We then distill the JVPs into a separate neural network that allows us to efficiently compute the necessary higher-order terms for our novel sampler during synthesis. We only need to train a small additional head on top of the first-order score network. We validate GENIE on multiple image generation benchmarks and demonstrate that GENIE outperforms all previous solvers. Unlike recent methods that fundamentally alter the generation process in DDMs, our GENIE solves the true generative DE and still enables applications such as encoding and guided sampling. Project page and code: https://nv-tlabs.github.io/GENIE.",
        "authors": [
            "Tim Dockhorn",
            "Arash Vahdat",
            "Karsten Kreis"
        ],
        "citations": 93,
        "references": 120,
        "year": 2022
    },
    {
        "title": "Denoising Diffusion Autoencoders are Unified Self-supervised Learners",
        "abstract": "Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations within its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for generative-and-discriminative dual learning. To validate this, we conduct linear probe and finetuning evaluations. Our diffusion-based approach achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to contrastive learning and masked autoencoders for the first time. Transfer learning from ImageNet also confirms the suitability of DDAE for Vision Transformers, suggesting the potential to scale DDAEs as unified foundation models. Code is available at github.com/FutureXiang/ddae.",
        "authors": [
            "Weilai Xiang",
            "Hongyu Yang",
            "Di Huang",
            "Yunhong Wang"
        ],
        "citations": 44,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions",
        "abstract": "Large-scale generative models enabled the development of AI-powered code completion tools to assist programmers in writing code. However, much like other AI-powered tools, AI-powered code completions are not always accurate, potentially introducing bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers identify potential errors is to highlight uncertain tokens. However, there have been no empirical studies exploring the eﬀectiveness of this technique—nor investigating the diﬀerent and not-yet-agreed-upon notions of uncertainty in the context of generative models. We explore the question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best ﬁts programmers’ needs. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system’s code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We ﬁnd that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any beneﬁt over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools, and ﬁnd that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming. This work contributes to building an understanding of what uncertainty means for generative models and how to convey it eﬀectively.",
        "authors": [
            "Helena Vasconcelos",
            "Gagan Bansal",
            "Adam Fourney",
            "Q. Liao",
            "Jennifer Wortman Vaughan"
        ],
        "citations": 30,
        "references": 61,
        "year": 2023
    },
    {
        "title": "The Prompt Artists",
        "abstract": "This paper examines the art practices, artwork, and motivations of prolific users of the latest generation of text-to-image models. Through interviews, observations, and a user survey, we present a sampling of the artistic styles and describe the developed community of practice around generative AI. We find that: 1) artists hold the text prompt and the resulting image can be considered collectively as a form of artistic expression (prompts as art), and 2) prompt templates (prompts with “slots” for others to fill in with their own words) are developed to create generative art styles. We discover that the value placed by this community on unique outputs leads to artists seeking specialized vocabulary to produce distinctive art pieces (e.g., by reading architectural blogs to find phrases to describe images). We also find that some artists use “glitches” in the model that can be turned into artistic styles of their own right. From these findings, we outline specific implications for design regarding future prompting and image editing options.",
        "authors": [
            "Minsuk Chang",
            "Stefania Druga",
            "Alexander J. Fiannaca",
            "P. Vergani",
            "Chinmay Kulkarni",
            "Carrie J. Cai",
            "Michael Terry"
        ],
        "citations": 43,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling",
        "abstract": "Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation quality: graphs generated by our approach have more similar graph statistics to those of the training graphs.",
        "authors": [
            "Xiaohui Chen",
            "Jiaxing He",
            "Xuhong Han",
            "Liping Liu"
        ],
        "citations": 38,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2",
        "abstract": "The field of image synthesis has made great strides in the last couple of years. Recent models are capable of generating images with astonishing quality. Fine-grained evaluation of these models on some interesting categories such as faces is still missing. Here, we conduct a quantitative comparison of three popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their ability to generate photorealistic faces in the wild. We find that Stable Diffusion generates better faces than the other systems, according to the FID score. We also introduce a dataset of generated faces in the wild dubbed GFW, including a total of 15,076 faces. Furthermore, we hope that our study spurs follow-up research in assessing the generative models and improving them. Data and code are available at data and code, respectively.",
        "authors": [
            "A. Borji"
        ],
        "citations": 101,
        "references": 14,
        "year": 2022
    },
    {
        "title": "A Prompt Log Analysis of Text-to-Image Generation Systems",
        "abstract": "Recent developments in large language models (LLM) and generative AI have unleashed the astonishing capabilities of text-to-image generation systems to synthesize high-quality images that are faithful to a given reference text, known as a “prompt”. These systems have immediately received lots of attention from researchers, creators, and common users. Despite the plenty of efforts to improve the generative models, there is limited work on understanding the information needs of the users of these systems at scale. We conduct the first comprehensive analysis of large-scale prompt logs collected from multiple text-to-image generation systems. Our work is analogous to analyzing the query logs of Web search engines, a line of work that has made critical contributions to the glory of the Web search industry and research. Compared with Web search queries, text-to-image prompts are significantly longer, often organized into special structures that consist of the subject, form, and intent of the generation tasks and present unique categories of information needs. Users make more edits within creation sessions, which present remarkable exploratory patterns. There is also a considerable gap between the user-input prompts and the captions of the images included in the open training data of the generative models. Our findings provide concrete implications on how to improve text-to-image generation systems for creation purposes.",
        "authors": [
            "Yutong Xie",
            "Zhaoying Pan",
            "Jing Ma",
            "Jie Luo",
            "Qiaozhu Mei"
        ],
        "citations": 34,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Educational data augmentation in physics education research using ChatGPT",
        "abstract": "Generative AI technologies such as large language models show novel potentials to enhance educational research. For example, generative large language models were shown to be capable to solve quantitative reasoning tasks in physics and concept tests such as the Force Concept Inventory. Given the importance of such concept inventories for physics education research, and the challenges in developing them such as field testing with representative populations, this study seeks to examine to what extent a generative large language model could be utilized to generate a synthetic data set for the FCI that exhibits content-related variability in responses. We use the recently introduced ChatGPT based on the GPT 4 generative large language model and investigate to what extent ChatGPT could solve the FCI accurately (RQ1) and could be prompted to solve the FCI as-if it were a student belonging to a different cohort (RQ2). Furthermore, we study, to what extent ChatGPT could be prompted to solve the FCI as-if it were a student having a different force- and mechanics-related misconception (RQ3). In alignment with other research, we found the ChatGPT could accurately solve the FCI. We furthermore found that prompting ChatGPT to respond to the inventory as-if it belonged to a different cohort yielded no variance in responses, however, responding as-if it had a certain misconception introduced much variance in responses that approximate real human responses on the FCI in some regards.",
        "authors": [
            "Fabian Kieser",
            "P. Wulff",
            "Jochen Kuhn",
            "S. Küchemann"
        ],
        "citations": 43,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Scalable Diffusion for Materials Generation",
        "abstract": "Generative models trained on internet-scale data are capable of generating novel and realistic texts, images, and videos. A natural next question is whether these models can advance science, for example by generating novel stable materials. Traditionally, models with explicit structures (e.g., graphs) have been used in modeling structural relationships in scientific data (e.g., atoms and bonds in crystals), but generating structures can be difficult to scale to large and complex systems. Another challenge in generating materials is the mismatch between standard generative modeling metrics and downstream applications. For instance, common metrics such as the reconstruction error do not correlate well with the downstream goal of discovering stable materials. In this work, we tackle the scalability challenge by developing a unified crystal representation that can represent any crystal structure (UniMat), followed by training a diffusion probabilistic model on these UniMat representations. Our empirical results suggest that despite the lack of explicit structure modeling, UniMat can generate high fidelity crystal structures from larger and more complex chemical systems, outperforming previous graph-based approaches under various generative modeling metrics. To better connect the generation quality of materials to downstream applications, such as discovering novel stable materials, we propose additional metrics for evaluating generative models of materials, including per-composition formation energy and stability with respect to convex hulls through decomposition energy from Density Function Theory (DFT). Lastly, we show that conditional generation with UniMat can scale to previously established crystal datasets with up to millions of crystals structures, outperforming random structure search (the current leading method for structure discovery) in discovering new stable materials.",
        "authors": [
            "Mengjiao Yang",
            "KwangHwan Cho",
            "Amil Merchant",
            "Pieter Abbeel",
            "D. Schuurmans",
            "Igor Mordatch",
            "E. D. Cubuk"
        ],
        "citations": 27,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Diffusion-Based Signed Distance Fields for 3D Shape Generation",
        "abstract": "We propose a 3D shape generation framework (SDF-Diffusion in short) that uses denoising diffusion models with continuous 3D representation via signed distance fields (SDF). Unlike most existing methods that depend on discontinuous forms, such as point clouds, SDF-Diffusion generates high-resolution 3D shapes while alleviating memory issues by separating the generative process into two-stage: generation and super-resolution. In the first stage, a diffusion-based generative model generates a low-resolution SDF of 3D shapes. Using the estimated low-resolution SDF as a condition, the second stage diffusion model performs super-resolution to generate high-resolution SDF. Our framework can generate a high-fidelity 3D shape despite the extreme spatial complexity. On the ShapeNet dataset, our model shows competitive performance to the state-of-the-art methods and shows applicability on the shape completion task without modification.",
        "authors": [
            "Jaehyeok Shim",
            "Changwoo Kang",
            "Kyungdon Joo"
        ],
        "citations": 38,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images",
        "abstract": "Despite continued advancement in recent years, deep neural networks still rely on large amounts of training data to avoid overfitting. However, labeled training data for real-world applications such as healthcare is limited and difficult to access given longstanding privacy, and strict data sharing policies. By manipulating image datasets in the pixel or feature space, existing data augmentation techniques represent one of the effective ways to improve the quantity and diversity of training data. Here, we look to advance augmentation techniques by building upon the emerging success of text-to-image diffusion probabilistic models in augmenting the training samples of our macroscopic skin disease dataset. We do so by enabling fine-grained control of the image generation process via input text prompts. We demonstrate that this generative data augmentation approach successfully maintains a similar classification accuracy of the visual classifier even when trained on a fully synthetic skin disease dataset. Similar to recent applications of generative models, our study suggests that diffusion models are indeed effective in generating high-quality skin images that do not sacrifice the classifier performance, and can improve the augmentation of training datasets after curation.",
        "authors": [
            "Mohamed Akrout",
            "B'alint Gyepesi",
            "P. Holló",
            "A. Poór",
            "Blága Kincso",
            "Stephen Solis",
            "K. Cirone",
            "J. Kawahara",
            "Dekker Slade",
            "Latif Abid",
            "Máté Kovács",
            "I. Fazekas"
        ],
        "citations": 42,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Artificial intelligence and increasing misinformation",
        "abstract": "Summary With the recent advances in artificial intelligence (AI), patients are increasingly exposed to misleading medical information. Generative AI models, including large language models such as ChatGPT, create and modify text, images, audio and video information based on training data. Commercial use of generative AI is expanding rapidly and the public will routinely receive messages created by generative AI. However, generative AI models may be unreliable, routinely make errors and widely spread misinformation. Misinformation created by generative AI about mental illness may include factual errors, nonsense, fabricated sources and dangerous advice. Psychiatrists need to recognise that patients may receive misinformation online, including about medicine and psychiatry.",
        "authors": [
            "S. Monteith",
            "T. Glenn",
            "John R. Geddes",
            "P. Whybrow",
            "Eric D. Achtyes",
            "Michael Bauer"
        ],
        "citations": 43,
        "references": 6,
        "year": 2023
    },
    {
        "title": "CaloClouds: fast geometry-independent highly-granular calorimeter simulation",
        "abstract": "Simulating showers of particles in highly-granular detectors is a key frontier in the application of machine learning to particle physics. Achieving high accuracy and speed with generative machine learning models would enable them to augment traditional simulations and alleviate a major computing constraint. This work achieves a major breakthrough in this task by, for the first time, directly generating a point cloud of a few thousand space points with energy depositions in the detector in 3D space without relying on a fixed-grid structure. This is made possible by two key innovations: i) Using recent improvements in generative modeling we apply a diffusion model to generate photon showers as high-cardinality point clouds. ii) These point clouds of up to 6,000 space points are largely geometry-independent as they are down-sampled from initial even higher-resolution point clouds of up to 40,000 so-called Geant steps. We showcase the performance of this approach using the specific example of simulating photon showers in the planned electromagnetic calorimeter of the International Large Detector (ILD) and achieve overall good modeling of physically relevant distributions.",
        "authors": [
            "E. Buhmann",
            "S. Diefenbacher",
            "E. Eren",
            "F. Gaede",
            "G. Kasieczka",
            "A. Korol",
            "W. Korcari",
            "K. Krüger",
            "Peter McKeown"
        ],
        "citations": 42,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness",
        "abstract": "Generative pre-trained language models (GPLMs) like ChatGPT encode in the model's parameters knowledge the models observe during the pre-training phase. This knowledge is then used at inference to address the task specified by the user in their prompt. For example, for the question-answering task, the GPLMs leverage the knowledge and linguistic patterns learned at training to produce an answer to a user question. Aside from the knowledge encoded in the model itself, answers produced by GPLMs can also leverage knowledge provided in the prompts. For example, a GPLM can be integrated into a retrieve-then-generate paradigm where a search engine is used to retrieve documents relevant to the question; the content of the documents is then transferred to the GPLM via the prompt. In this paper we study the differences in answer correctness generated by ChatGPT when leveraging the model's knowledge alone vs. in combination with the prompt knowledge. We study this in the context of consumers seeking health advice from the model. Aside from measuring the effectiveness of ChatGPT in this context, we show that the knowledge passed in the prompt can overturn the knowledge encoded in the model and this is, in our experiments, to the detriment of answer correctness. This work has important implications for the development of more robust and transparent question-answering systems based on generative pre-trained language models.",
        "authors": [
            "G. Zuccon",
            "B. Koopman"
        ],
        "citations": 42,
        "references": 15,
        "year": 2023
    },
    {
        "title": "MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model",
        "abstract": "Due to patient privacy protection concerns, machine learning research in healthcare has been undeniably slower and limited than in other application domains. High-quality, realistic, synthetic electronic health records (EHRs) can be leveraged to accelerate methodological developments for research purposes while mitigating privacy concerns associated with data sharing. The current state-of-the-art model for synthetic EHR generation is generative adversarial networks, which are notoriously difficult to train and can suffer from mode collapse. Denoising Diffusion Probabilistic Models, a class of generative models inspired by statistical thermodynamics, have recently been shown to generate high-quality synthetic samples in certain domains. It is unknown whether these can generalize to generation of large-scale, high-dimensional EHRs. In this paper, we present a novel generative model based on diffusion models that is the first successful application on electronic health records. Our model proposes a mechanism to perform class-conditional sampling to preserve label information. We also introduce a new sampling strategy to accelerate the inference speed. We empirically show that our model outperforms existing state-of-the-art synthetic EHR generation methods.",
        "authors": [
            "Huan He",
            "Shifan Zhao",
            "Yuanzhe Xi",
            "Joyce Ho"
        ],
        "citations": 23,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Synthetic data, real errors: how (not) to publish and use synthetic data",
        "abstract": "Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-density regions of the original data, for which the generative uncertainty is largest.",
        "authors": [
            "B. V. Breugel",
            "Zhaozhi Qian",
            "M. Schaar"
        ],
        "citations": 24,
        "references": 62,
        "year": 2023
    },
    {
        "title": "COLD: A Benchmark for Chinese Offensive Language Detection",
        "abstract": "Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark –COLD for Chinese offensive language analysis, including a Chinese Offensive Language Dataset –COLDATASET and a baseline detector –COLDETECTOR which is trained on the dataset. We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources. We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained language models. We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues. Furthermore, we investigate the factors that influence the offensive generations, and we find that anti-bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier.",
        "authors": [
            "Deng Jiawen",
            "Jingyan Zhou",
            "Hao Sun",
            "Chujie Zheng",
            "Fei Mi",
            "Minlie Huang"
        ],
        "citations": 79,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Neural Wavelet-domain Diffusion for 3D Shape Generation",
        "abstract": "This paper presents a new approach for 3D shape generation, enabling direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets, and formulate a pair of neural networks: a generator based on the diffusion model to produce diverse shapes in the form of coarse coefficient volumes; and a detail predictor to further produce compatible detail coefficient volumes for enriching the generated shapes with fine structures and details. Both quantitative and qualitative experimental results manifest the superiority of our approach in generating diverse and high-quality shapes with complex topology and structures, clean surfaces, and fine details, exceeding the 3D generation capabilities of the state-of-the-art models.",
        "authors": [
            "Ka-Hei Hui",
            "Ruihui Li",
            "Jingyu Hu",
            "Chi-Wing Fu"
        ],
        "citations": 106,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Research and Application of Deep Learning in Image Recognition",
        "abstract": "Deep learning is a technical tool with broad application prospects and has an important role in the field of image recognition. In view of the theoretical value and practical significance of image recognition technology in promoting the development of computer vision and artificial intelligence, this paper will review and study the application of deep learning in image recognition. This paper first outlines the development of icon recognition technology, and then introduces three main learning models in deep learning: convolutional neural networks, recurrent neural networks, and generative adversarial networks, and provides a comparative analysis of these three learning models. Finally, the research results of deep learning image recognition application fields, such as face recognition, medical image recognition, and remote sensing image classification, are analyzed and discussed. This paper also analyze the development trend of deep learning in the field of image recognition, and conclude that the future development direction is the effective recognition of video images and the theoretical strengthening of models.",
        "authors": [
            "Yinglong Li"
        ],
        "citations": 112,
        "references": 32,
        "year": 2022
    },
    {
        "title": "RealFill: Reference-Driven Generation for Authentic Image Completion",
        "abstract": "Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions. However, the content these models hallucinate is necessarily inauthentic, since they are unaware of the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging scenarios, and find that it outperforms existing approaches by a large margin. Project page: https://realfill.github.io.",
        "authors": [
            "Luming Tang",
            "Nataniel Ruiz",
            "Q. Chu",
            "Yuanzhen Li",
            "Aleksander Holynski",
            "David E. Jacobs",
            "Bharath Hariharan",
            "Y. Pritch",
            "Neal Wadhwa",
            "Kfir Aberman",
            "Michael Rubinstein"
        ],
        "citations": 33,
        "references": 61,
        "year": 2023
    },
    {
        "title": "ChatGPT and mental healthcare: balancing benefits with risks of harms",
        "abstract": "Against the global need for increased access to mental services, health organisations are looking to technological advances to improve the delivery of care and lower costs. Since November 2022, with the public launch of OpenAI’s ChatGPT, the field of generative artificial intelligence (AI) has received expanding attention. Although generative AI itself is not new, technical advances and the increased accessibility of large language models (LLMs) (eg, OpenAI’s GPT-4 and Google’s Bard) suggest use of these tools could be clinically significant. LLMs are an application of generative AI technology that can summarise and generate content based on training on vast data sets. Unlike search engines, which provide internet links in response to typed entries, chatbots that rely on generative language models can simulate dialogue that resembles human conversations. We examine the potential promise and the risks of using LLMs in mental healthcare today, focusing on their scope to impact mental healthcare, including global equity in the delivery of care. Although we caution that LLMs should not be used to disintermediate mental health clinicians, we signal how—if carefully implemented—in the long term these tools could reap benefits for patients and health professionals.",
        "authors": [
            "C. Blease",
            "J. Torous"
        ],
        "citations": 29,
        "references": 15,
        "year": 2023
    },
    {
        "title": "Latent Space Editing in Transformer-Based Flow Matching",
        "abstract": "This paper strives for image editing via generative models. Flow Matching is an emerging generative modeling technique that offers the advantage of simple and efficient training. Simultaneously, a new transformer-based U-ViT has recently been proposed to replace the commonly used UNet for better scalability and performance in generative modeling. Hence, Flow Matching with a transformer backbone offers the potential for scalable and high-quality generative modeling, but their latent structure and editing ability are as of yet unknown. Hence, we adopt this setting and explore how to edit images through latent space manipulation. We introduce an editing space, which we call u-space, that can be manipulated in a controllable, accumulative, and composable manner. Additionally, we propose a tailored sampling solution to enable sampling with the more efficient adaptive step-size ODE solvers. Lastly, we put forth a straightforward yet powerful method for achieving fine-grained and nuanced editing using text prompts. Our framework is simple and efficient, all while being highly effective at editing images while preserving the essence of the original content. Our code will be publicly available at https://taohu.me/lfm/",
        "authors": [
            "Vincent Tao Hu",
            "David W. Zhang",
            "Meng Tang",
            "P. Mettes",
            "Deli Zhao",
            "Cees G. M. Snoek"
        ],
        "citations": 22,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Diffusion Model as Representation Learner",
        "abstract": "Diffusion Probabilistic Models (DPMs) have recently demonstrated impressive results on various generative tasks. Despite its promises, the learned representations of pre-trained DPMs, however, have not been fully understood. In this paper, we conduct an in-depth investigation of the representation power of DPMs, and propose a novel knowledge transfer method that leverages the knowledge acquired by generative DPMs for recognition tasks. Our study begins by examining the feature space of DPMs, revealing that DPMs are inherently denoising autoencoders that balance the representation learning with regularizing model capacity. To this end, we introduce a novel knowledge transfer paradigm named RepFusion. Our paradigm extracts representations at different time steps from off-the-shelf DPMs and dynamically employs them as supervision for student networks, in which the optimal time is determined through reinforcement learning. We evaluate our approach on several image classification, semantic segmentation, and landmark detection benchmarks, and demonstrate that it outperforms state-of-the-art methods. Our results uncover the potential of DPMs as a powerful tool for representation learning and provide insights into the usefulness of generative models beyond sample generation. The code is available at https://github.com/Adamdad/Repfusion.",
        "authors": [
            "Xingyi Yang",
            "Xinchao Wang"
        ],
        "citations": 32,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Variational Model Inversion Attacks",
        "abstract": "Given the ubiquity of deep neural networks, it is important that these models do not reveal information about sensitive data that they have been trained on. In model inversion attacks, a malicious user attempts to recover the private dataset used to train a supervised neural network. A successful model inversion attack should generate realistic and diverse samples that accurately describe each of the classes in the private dataset. In this work, we provide a probabilistic interpretation of model inversion attacks, and formulate a variational objective that accounts for both diversity and accuracy. In order to optimize this variational objective, we choose a variational family defined in the code space of a deep generative model, trained on a public auxiliary dataset that shares some structural similarity with the target dataset. Empirically, our method substantially improves performance in terms of target attack accuracy, sample realism, and diversity on datasets of faces and chest X-ray images.",
        "authors": [
            "Kuan-Chieh Jackson Wang",
            "Yanzhe Fu",
            "Ke Li",
            "A. Khisti",
            "R. Zemel",
            "Alireza Makhzani"
        ],
        "citations": 82,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Rich and Poor Texture Contrast: A Simple yet Effective Approach for AI-generated Image Detection",
        "abstract": "Recent generative models show impressive performance in generating photographic",
        "authors": [
            "Nan Zhong",
            "Yiran Xu",
            "Zhenxing Qian",
            "Xinpeng Zhang"
        ],
        "citations": 20,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Stochastic Multi-Person 3D Motion Forecasting",
        "abstract": "This paper aims to deal with the ignored real-world complexities in prior work on human motion forecasting, emphasizing the social properties of multi-person motion, the diversity of motion and social interactions, and the complexity of articulated motion. To this end, we introduce a novel task of stochastic multi-person 3D motion forecasting. We propose a dual-level generative modeling framework that separately models independent individual motion at the local level and social interactions at the global level. Notably, this dual-level modeling mechanism can be achieved within a shared generative model, through introducing learnable latent codes that represent intents of future motion and switching the codes' modes of operation at different levels. Our framework is general; we instantiate it with different generative models, including generative adversarial networks and diffusion models, and various multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D, and SoMoF benchmarks show that our approach produces diverse and accurate multi-person predictions, significantly outperforming the state of the art.",
        "authors": [
            "Sirui Xu",
            "Yu-Xiong Wang",
            "Liangyan Gui"
        ],
        "citations": 21,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Towards the Detection of Diffusion Model Deepfakes",
        "abstract": "In the course of the past few years, diffusion models (DMs) have reached an unprecedented level of visual quality. However, relatively little attention has been paid to the detection of DM-generated images, which is critical to prevent adverse impacts on our society. In contrast, generative adversarial networks (GANs), have been extensively studied from a forensic perspective. In this work, we therefore take the natural next step to evaluate whether previous methods can be used to detect images generated by DMs. Our experiments yield two key findings: (1) state-of-the-art GAN detectors are unable to reliably distinguish real from DM-generated images, but (2) re-training them on DM-generated images allows for almost perfect detection, which remarkably even generalizes to GANs. Together with a feature space analysis, our results lead to the hypothesis that DMs produce fewer detectable artifacts and are thus more difficult to detect compared to GANs. One possible reason for this is the absence of grid-like frequency artifacts in DM-generated images, which are a known weakness of GANs. However, we make the interesting observation that diffusion models tend to underestimate high frequencies, which we attribute to the learning objective.",
        "authors": [
            "Jonas Ricker",
            "Simon Damm",
            "Thorsten Holz",
            "Asja Fischer"
        ],
        "citations": 83,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation",
        "abstract": "Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4HINTS-GPT3.5VAL. As a first step, our technique leverages GPT-4 as a “tutor” model to generate hints – it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a “student” model to further validate the hint quality – it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.",
        "authors": [
            "Tung Phung",
            "Victor-Alexandru Pădurean",
            "Anjali Singh",
            "Christopher Brooks",
            "J. Cambronero",
            "Sumit Gulwani",
            "A. Singla",
            "Gustavo Soares"
        ],
        "citations": 26,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Person Image Synthesis via Denoising Diffusion Model",
        "abstract": "The pose-guided person image generation task requires synthesizing photorealistic images of humans in arbitrary poses. The existing approaches use generative adversarial networks that do not necessarily maintain realistic textures or need dense correspondences that struggle to handle complex deformations and severe occlusions. In this work, we show how denoising diffusion models can be applied for high-fidelity person image synthesis with strong sample diversity and enhanced mode coverage of the learnt data distribution. Our proposed Person Image Diffusion Model (PIDM) disintegrates the complex transfer problem into a series of simpler forward-backward denoising steps. This helps in learning plausible source-to-target transformation trajectories that result in faithful textures and undistorted appearance details. We introduce a ‘texture diffusion module’ based on cross-attention to accurately model the correspondences between appearance and pose information available in source and target images. Further, we propose ‘disentangled classifier-free guidance’ to ensure close resemblance between the conditional inputs and the synthesized output in terms of both pose and appearance information. Our extensive results on two large-scale benchmarks and a user study demonstrate the photorealism of our proposed approach under challenging scenarios. We also show how our generated images can help in downstream tasks. Code is available at https://github.com/ankanbhunia/PIDM.",
        "authors": [
            "A. Bhunia",
            "Salman H. Khan",
            "Hisham Cholakkal",
            "R. Anwer",
            "J. Laaksonen",
            "M. Shah",
            "F. Khan"
        ],
        "citations": 79,
        "references": 30,
        "year": 2022
    },
    {
        "title": "Continuous diffusion for categorical data",
        "abstract": "Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.",
        "authors": [
            "S. Dieleman",
            "Laurent Sartran",
            "Arman Roshannai",
            "Nikolay Savinov",
            "Yaroslav Ganin",
            "Pierre H. Richemond",
            "A. Doucet",
            "Robin Strudel",
            "Chris Dyer",
            "Conor Durkan",
            "Curtis Hawthorne",
            "Rémi Leblond",
            "Will Grathwohl",
            "J. Adler"
        ],
        "citations": 78,
        "references": 110,
        "year": 2022
    },
    {
        "title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
        "abstract": null,
        "authors": [
            "Jannis Born",
            "M. Manica"
        ],
        "citations": 70,
        "references": 97,
        "year": 2022
    },
    {
        "title": "New meaning for NLP: the trials and tribulations of natural language processing with GPT-3 in ophthalmology",
        "abstract": "Natural language processing (NLP) is a subfield of machine intelligence focused on the interaction of human language with computer systems. NLP has recently been discussed in the mainstream media and the literature with the advent of Generative Pre-trained Transformer 3 (GPT-3), a language model capable of producing human-like text. The release of GPT-3 has also sparked renewed interest on the applicability of NLP to contemporary healthcare problems. This article provides an overview of NLP models, with a focus on GPT-3, as well as discussion of applications specific to ophthalmology. We also outline the limitations of GPT-3 and the challenges with its integration into routine ophthalmic care.",
        "authors": [
            "Siddharth Nath",
            "Abdullah Marie",
            "S. Ellershaw",
            "Edward Korot",
            "P. Keane"
        ],
        "citations": 79,
        "references": 36,
        "year": 2022
    },
    {
        "title": "A Morphology Focused Diffusion Probabilistic Model for Synthesis of Histopathology Images",
        "abstract": "Visual microscopic study of diseased tissue by pathologists has been the cornerstone for cancer diagnosis and prognostication for more than a century. Recently, deep learning methods have made significant advances in the analysis and classification of tissue images. However, there has been limited work on the utility of such models in generating histopathology images. These synthetic images have several applications in pathology including utilities in education, proficiency testing, privacy, and data sharing. Recently, diffusion probabilistic models were introduced to generate high quality images. Here, for the first time, we investigate the potential use of such models along with prioritized morphology weighting and color normalization to synthesize high quality histopathology images of brain cancer. Our detailed results show that diffusion probabilistic models are capable of synthesizing a wide range of histopathology images and have superior performance compared to generative adversarial networks.",
        "authors": [
            "Puria Azadi Moghadam",
            "Sanne Van Dalen",
            "K. C. Martin",
            "J. Lennerz",
            "Stephen S. F. Yip",
            "H. Farahani",
            "Ali Bashashati"
        ],
        "citations": 71,
        "references": 48,
        "year": 2022
    },
    {
        "title": "SFace: Privacy-friendly and Accurate Face Recognition using Synthetic Data",
        "abstract": "Recent deep face recognition models proposed in the literature utilized large-scale public datasets such as MS-Celeb-1M and VGGFace2 for training very deep neural networks, achieving state-of-the-art performance on mainstream benchmarks. Recently, many of these datasets, e.g., MS-Celeb-1M and VGGFace2, are retracted due to credible privacy and ethical concerns. This motivates this work to propose and investigate the feasibility of using a privacy-friendly synthetically generated face dataset to train face recognition models. Towards this end, we utilize a class-conditional generative adversarial network to generate class-labeled synthetic face images, namely SFace. To address the privacy aspect of using such data to train a face recognition model, we provide extensive evaluation experiments on the identity relation between the synthetic dataset and the original authentic dataset used to train the generative model. Our reported evaluation proved that associating an identity of the authentic dataset to one with the same class label in the synthetic dataset is hardly possible. We also propose to train face recognition on our privacy-friendly dataset, SFace, using three different learning strategies, multi-class classification, label-free knowledge transfer, and combined learning of multi-class classification and knowledge transfer. The reported evaluation results on five authentic face benchmarks demonstrated that the privacy-friendly synthetic dataset has a high potential to be used for training face recognition models, achieving, for example, a verification accuracy of 91.87% on LFW using multi-class classification and 99.13% using the combined learning strategy. The training code and the synthetic face image dataset are publicly released11https://github.com/fdbtrs/SFace-Privacy-friendly-and-Accurate-Face-Recognition-using-Synthetic-Data.",
        "authors": [
            "Fadi Boutros",
            "Marco Huber",
            "Patrick Siebke",
            "Tim Rieber",
            "N. Damer"
        ],
        "citations": 68,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Exploring Chemical Space with Score-based Out-of-distribution Generation",
        "abstract": "A well-known limitation of existing molecular generative models is that the generated molecules highly resemble those in the training set. To generate truly novel molecules that may have even better properties for de novo drug discovery, more powerful exploration in the chemical space is necessary. To this end, we propose Molecular Out-Of-distribution Diffusion(MOOD), a score-based diffusion scheme that incorporates out-of-distribution (OOD) control in the generative stochastic differential equation (SDE) with simple control of a hyperparameter, thus requires no additional costs. Since some novel molecules may not meet the basic requirements of real-world drugs, MOOD performs conditional generation by utilizing the gradients from a property predictor that guides the reverse-time diffusion process to high-scoring regions according to target properties such as protein-ligand interactions, drug-likeness, and synthesizability. This allows MOOD to search for novel and meaningful molecules rather than generating unseen yet trivial ones. We experimentally validate that MOOD is able to explore the chemical space beyond the training distribution, generating molecules that outscore ones found with existing methods, and even the top 0.01% of the original training pool. Our code is available at https://github.com/SeulLee05/MOOD.",
        "authors": [
            "Seul Lee",
            "Jaehyeong Jo",
            "S. Hwang"
        ],
        "citations": 58,
        "references": 79,
        "year": 2022
    },
    {
        "title": "Latent Diffusion Energy-Based Model for Interpretable Text Modeling",
        "abstract": "Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned latent space. Experiments on several challenging tasks demonstrate the superior performance of our model on interpretable text modeling over strong counterparts.",
        "authors": [
            "Peiyu Yu",
            "Sirui Xie",
            "Xiaojian Ma",
            "Baoxiong Jia",
            "Bo Pang",
            "Ruigi Gao",
            "Yixin Zhu",
            "Song-Chun Zhu",
            "Y. Wu"
        ],
        "citations": 72,
        "references": 80,
        "year": 2022
    },
    {
        "title": "DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) are expressive generative models that have been used to solve a variety of speech synthesis problems. However, because of their high sampling costs, DDPMs are difficult to use in real-time speech processing applications. In this paper, we introduce DiffGAN-TTS, a novel DDPM-based text-to-speech (TTS) model achieving high-fidelity and efficient speech synthesis. DiffGAN-TTS is based on denoising diffusion generative adversarial networks (GANs), which adopt an adversarially-trained expressive model to approximate the denoising distribution. We show with multi-speaker TTS experiments that DiffGAN-TTS can generate high-fidelity speech samples within only 4 denoising steps. We present an active shallow diffusion mechanism to further speed up inference. A two-stage training scheme is proposed, with a basic TTS acoustic model trained at stage one providing valuable prior information for a DDPM trained at stage two. Our experiments show that DiffGAN-TTS can achieve high synthesis performance with only 1 denoising step.",
        "authors": [
            "Songxiang Liu",
            "Dan Su",
            "Dong Yu"
        ],
        "citations": 59,
        "references": 53,
        "year": 2022
    },
    {
        "title": "StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis",
        "abstract": "Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 12 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and Swin Transformer). Unlike other benchmarks used in the GAN community, we train representative GANs, including BigGAN and StyleGAN series in a unified training pipeline and quantify generation performance with 7 evaluation metrics. The benchmark evaluates other cutting-edge generative models (e.g., StyleGAN-XL, ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations, training, and evaluation scripts with the pre-trained weights. StudioGAN is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.",
        "authors": [
            "Minguk Kang",
            "Joonghyuk Shin",
            "Jaesik Park"
        ],
        "citations": 58,
        "references": 172,
        "year": 2022
    },
    {
        "title": "PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting",
        "abstract": ". We address the problem of action-conditioned generation of human motion sequences. Existing work falls into two categories: forecast models conditioned on observed past motions, or generative models conditioned on action labels and duration only. In contrast, we generate motion conditioned on observations of arbitrary length, including none. To solve this generalized problem, we propose PoseGPT, an auto-regressive transformer-based approach which internally compresses human motion into quantized latent sequences. An auto-encoder ﬁrst maps human motion to latent index sequences in a discrete space, and vice-versa. Inspired by the Generative Pretrained Transformer (GPT), we propose to train a GPT-like model for next-index prediction in that space; this allows PoseGPT to output distributions on possible futures, with or without conditioning on past motion. The discrete and compressed nature of the latent space allows the GPT-like model to focus on long-range signal, as it removes low-level redundancy in the input signal. Predicting discrete indices also alleviates the common pitfall of predicting averaged poses, a typical failure case when regressing continuous values, as the average of discrete targets is not a target itself. Our experimental results show that our proposed approach achieves state-of-the-art results on HumanAct12, a standard but small scale dataset, as well as on BABEL, a recent large scale MoCap dataset, and on GRAB, a human-object interactions dataset.",
        "authors": [
            "Thomas Lucas",
            "Fabien Baradel",
            "Philippe Weinzaepfel",
            "Grégory Rogez"
        ],
        "citations": 59,
        "references": 75,
        "year": 2022
    },
    {
        "title": "Detecting Generated Images by Real Images",
        "abstract": null,
        "authors": [
            "Bo Liu",
            "Fan Yang",
            "Xiuli Bi",
            "Bin Xiao",
            "Weisheng Li",
            "Xinbo Gao"
        ],
        "citations": 50,
        "references": 28,
        "year": 2022
    },
    {
        "title": "Practical GAN-based synthetic IP header trace generation using NetShare",
        "abstract": "We explore the feasibility of using Generative Adversarial Networks (GANs) to automatically learn generative models to generate synthetic packet- and flow header traces for networking tasks (e.g., telemetry, anomaly detection, provisioning). We identify key fidelity, scalability, and privacy challenges and tradeoffs in existing GAN-based approaches. By synthesizing domain-specific insights with recent advances in machine learning and privacy, we identify design choices to tackle these challenges. Building on these insights, we develop an end-to-end framework, NetShare. We evaluate NetShare on six diverse packet header traces and find that: (1) across all distributional metrics and traces, it achieves 46% more accuracy than baselines and (2) it meets users' requirements of downstream tasks in evaluating accuracy and rank ordering of candidate approaches.",
        "authors": [
            "Yucheng Yin",
            "Zinan Lin",
            "Minhao Jin",
            "G. Fanti",
            "Vyas Sekar"
        ],
        "citations": 51,
        "references": 82,
        "year": 2022
    },
    {
        "title": "CANF-VC: Conditional Augmented Normalizing Flows for Video Compression",
        "abstract": "This paper presents an end-to-end learning-based video compression system, termed CANF-VC, based on conditional augmented normalizing flows (CANF). Most learned video compression systems adopt the same hybrid-based coding architecture as the traditional codecs. Recent research on conditional coding has shown the sub-optimality of the hybrid-based coding and opens up opportunities for deep generative models to take a key role in creating new coding frameworks. CANF-VC represents a new attempt that leverages the conditional ANF to learn a video generative model for conditional inter-frame coding. We choose ANF because it is a special type of generative model, which includes variational autoencoder as a special case and is able to achieve better expressiveness. CANF-VC also extends the idea of conditional coding to motion coding, forming a purely conditional coding framework. Extensive experimental results on commonly used datasets confirm the superiority of CANF-VC to the state-of-the-art methods. The source code of CANF-VC is available at https://github.com/NYCU-MAPL/CANF-VC.",
        "authors": [
            "Yung-Han Ho",
            "Chih-Peng Chang",
            "Peng Chen",
            "Alessandro Gnutti",
            "Wen-Hsiao Peng"
        ],
        "citations": 51,
        "references": 38,
        "year": 2022
    },
    {
        "title": "Lossy Compression with Gaussian Diffusion",
        "abstract": "We consider a novel lossy compression approach based on unconditional diffusion generative models, which we call DiffC. Unlike modern compression schemes which rely on transform coding and quantization to restrict the transmitted information, DiffC relies on the efficient communication of pixels corrupted by Gaussian noise. We implement a proof of concept and find that it works surprisingly well despite the lack of an encoder transform, outperforming the state-of-the-art generative compression method HiFiC on ImageNet 64x64. DiffC only uses a single model to encode and denoise corrupted pixels at arbitrary bitrates. The approach further provides support for progressive coding, that is, decoding from partial bit streams. We perform a rate-distortion analysis to gain a deeper understanding of its performance, providing analytical results for multivariate Gaussian data as well as theoretic bounds for general distributions. Furthermore, we prove that a flow-based reconstruction achieves a 3 dB gain over ancestral sampling at high bitrates.",
        "authors": [
            "Lucas Theis",
            "Tim Salimans",
            "M. Hoffman",
            "Fabian Mentzer"
        ],
        "citations": 64,
        "references": 69,
        "year": 2022
    },
    {
        "title": "MoRF: Morphable Radiance Fields for Multiview Neural Head Modeling",
        "abstract": "Recent research work has developed powerful generative models (e.g., StyleGAN2) that can synthesize complete human head images with impressive photorealism, enabling applications such as photorealistically editing real photographs. While these models can be trained on large collections of unposed images, their lack of explicit 3D knowledge makes it difficult to achieve even basic control over 3D viewpoint without unintentionally altering identity. On the other hand, recent Neural Radiance Field (NeRF) methods have already achieved multiview-consistent, photorealistic renderings but they are so far limited to a single facial identity. In this paper, we propose a new Morphable Radiance Field (MoRF) method that extends a NeRF into a generative neural model that can realistically synthesize multiview-consistent images of complete human heads, with variable and controllable identity. MoRF allows for morphing between particular identities, synthesizing arbitrary new identities, or quickly generating a NeRF from few images of a new subject, all while providing realistic and consistent rendering under novel viewpoints. We train MoRF in a supervised fashion by leveraging a high-quality database of multiview portrait images of several people, captured in studio with polarization-based separation of diffuse and specular reflection. Here, we demonstrate how MoRF is a strong new step forwards towards generative NeRFs for 3D neural head modeling.",
        "authors": [
            "Daoye Wang",
            "Prashanth Chandran",
            "Gaspard Zoss",
            "D. Bradley",
            "Paulo F. U. Gotardo"
        ],
        "citations": 56,
        "references": 51,
        "year": 2022
    },
    {
        "title": "MoVQ: Modulating Quantized Vectors for High-Fidelity Image Generation",
        "abstract": "Although two-stage Vector Quantized (VQ) generative models allow for synthesizing high-fidelity and high-resolution images, their quantization operator encodes similar patches within an image into the same index, resulting in a repeated artifact for similar adjacent regions using existing decoder architectures. To address this issue, we propose to incorporate the spatially conditional normalization to modulate the quantized vectors so as to insert spatially variant information to the embedded index maps, encouraging the decoder to generate more photorealistic images. Moreover, we use multichannel quantization to increase the recombination capability of the discrete codes without increasing the cost of model and codebook. Additionally, to generate discrete tokens at the second stage, we adopt a Masked Generative Image Transformer (MaskGIT) to learn an underlying prior distribution in the compressed latent space, which is much faster than the conventional autoregressive model. Experiments on two benchmark datasets demonstrate that our proposed modulated VQGAN is able to greatly improve the reconstructed image quality as well as provide high-fidelity image generation.",
        "authors": [
            "Chuanxia Zheng",
            "L. Vuong",
            "Jianfei Cai",
            "Dinh Q. Phung"
        ],
        "citations": 53,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Conditional Simulation Using Diffusion Schrödinger Bridges",
        "abstract": "Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr\\\"odinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schr\\\"odinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks. Our code can be found at https://github.com/vdeborto/cdsb.",
        "authors": [
            "Yuyang Shi",
            "Valentin De Bortoli",
            "George Deligiannidis",
            "A. Doucet"
        ],
        "citations": 49,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Learning to Generate Realistic LiDAR Point Clouds",
        "abstract": "We present LiDARGen, a novel, effective, and controllable generative model that produces realistic LiDAR point cloud sensory readings. Our method leverages the powerful score-matching energy-based model and formulates the point cloud generation process as a stochastic denoising process in the equirectangular view. This model allows us to sample diverse and high-quality point cloud samples with guaranteed physical feasibility and controllability. We validate the effectiveness of our method on the challenging KITTI-360 and NuScenes datasets. The quantitative and qualitative results show that our approach produces more realistic samples than other generative models. Furthermore, LiDARGen can sample point clouds conditioned on inputs without retraining. We demonstrate that our proposed generative model could be directly used to densify LiDAR point clouds. Our code is available at: https://www.zyrianov.org/lidargen/",
        "authors": [
            "Vlas Zyrianov",
            "Xiyue Zhu",
            "Shenlong Wang"
        ],
        "citations": 46,
        "references": 83,
        "year": 2022
    },
    {
        "title": "StoRM: A Diffusion-Based Stochastic Regeneration Model for Speech Enhancement and Dereverberation",
        "abstract": "Diffusion models have shown a great ability at bridging the performance gap between predictive and generative approaches for speech enhancement. We have shown that they may even outperform their predictive counterparts for non-additive corruption types or when they are evaluated on mismatched conditions. However, diffusion models suffer from a high computational burden, mainly as they require to run a neural network for each reverse diffusion step, whereas predictive approaches only require one pass. As diffusion models are generative approaches they may also produce vocalizing and breathing artifacts in adverse conditions. In comparison, in such difficult scenarios, predictive models typically do not produce such artifacts but tend to distort the target speech instead, thereby degrading the speech quality. In this work, we present a stochastic regeneration approach where an estimate given by a predictive model is provided as a guide for further diffusion. We show that the proposed approach uses the predictive model to remove the vocalizing and breathing artifacts while producing very high quality samples thanks to the diffusion model, even in adverse conditions. We further show that this approach enables to use lighter sampling schemes with fewer diffusion steps without sacrificing quality, thus lifting the computational burden by an order of magnitude. Source code and audio examples are available online.",
        "authors": [
            "Jean-Marie Lemercier",
            "Julius Richter",
            "Simon Welker",
            "Timo Gerkmann"
        ],
        "citations": 60,
        "references": 73,
        "year": 2022
    },
    {
        "title": "STaSy: Score-based Tabular data Synthesis",
        "abstract": "Tabular data synthesis is a long-standing research topic in machine learning. Many different methods have been proposed over the past decades, ranging from statistical methods to deep generative methods. However, it has not always been successful due to the complicated nature of real-world tabular data. In this paper, we present a new model named Score-based Tabular data Synthesis (STaSy) and its training strategy based on the paradigm of score-based generative modeling. Despite the fact that score-based generative models have resolved many issues in generative models, there still exists room for improvement in tabular data synthesis. Our proposed training strategy includes a self-paced learning technique and a fine-tuning strategy, which further increases the sampling quality and diversity by stabilizing the denoising score matching training. Furthermore, we also conduct rigorous experimental studies in terms of the generative task trilemma: sampling quality, diversity, and time. In our experiments with 15 benchmark tabular datasets and 7 baselines, our method outperforms existing methods in terms of task-dependant evaluations and diversity. Code is available at https://github.com/JayoungKim408/STaSy.",
        "authors": [
            "Jayoung Kim",
            "C. Lee",
            "Noseong Park"
        ],
        "citations": 43,
        "references": 64,
        "year": 2022
    },
    {
        "title": "SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks",
        "abstract": "We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Transformer architectures to encode topological, geometric, and extrusion variations of construction sequences into disentangled codebooks. Autoregressive Transformer decoders generate CAD construction sequences sharing certain properties specified by the codebook vectors. Extensive experiments demonstrate that our disentangled codebook representation generates diverse and high-quality CAD models, enhances user control, and enables efficient exploration of the design space. The code is available at https://samxuxiang.github.io/skexgen.",
        "authors": [
            "Xiang Xu",
            "Karl D. D. Willis",
            "J. Lambourne",
            "Chin-Yi Cheng",
            "P. Jayaraman",
            "Yasutaka Furukawa"
        ],
        "citations": 50,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems",
        "abstract": "With the rapid development of diffusion models and flow-based generative models, there has been a surge of interests in solving noisy linear inverse problems, e.g., super-resolution, deblurring, denoising, colorization, etc, with generative models. However, while remarkable reconstruction performances have been achieved, their inference time is typically too slow since most of them rely on the seminal diffusion posterior sampling (DPS) framework and thus to approximate the intractable likelihood score, time-consuming gradient calculation through back-propagation is needed. To address this issue, this paper provides a fast and effective solution by proposing a simple closed-form approximation to the likelihood score. For both diffusion and flow-based models, extensive experiments are conducted on various noisy linear inverse problems such as noisy super-resolution, denoising, deblurring, and colorization. In all these tasks, our method (namely DMPS) demonstrates highly competitive or even better reconstruction performances while being significantly faster than all the baseline methods.",
        "authors": [
            "Xiangming Meng",
            "Y. Kabashima"
        ],
        "citations": 43,
        "references": 84,
        "year": 2022
    },
    {
        "title": "Better Together? An Evaluation of AI-Supported Code Translation",
        "abstract": "Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced code with fewer errors than when working alone. We also examined how the quality and quantity of AI translations affected the work process and quality of outcomes, and observed that providing multiple translations had a larger impact on the translation process than varying the quality of provided translations. Our results tell a complex, nuanced story about the benefits of generative code models and the challenges software engineers face when working with their outputs. Our work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.",
        "authors": [
            "Justin D. Weisz",
            "Michael J. Muller",
            "Steven I. Ross",
            "Fernando Martinez",
            "Stephanie Houde",
            "Mayank Agarwal",
            "Kartik Talamadupula",
            "John T. Richards"
        ],
        "citations": 55,
        "references": 108,
        "year": 2022
    },
    {
        "title": "What Does DALL-E 2 Know About Radiology?",
        "abstract": "Generative models, such as DALL-E 2 (OpenAI), could represent promising future tools for image generation, augmentation, and manipulation for artificial intelligence research in radiology, provided that these models have sufficient medical domain knowledge. Herein, we show that DALL-E 2 has learned relevant representations of x-ray images, with promising capabilities in terms of zero-shot text-to-image generation of new images, the continuation of an image beyond its original boundaries, and the removal of elements; however, its capabilities for the generation of images with pathological abnormalities (eg, tumors, fractures, and inflammation) or computed tomography, magnetic resonance imaging, or ultrasound images are still limited. The use of generative models for augmenting and generating radiological data thus seems feasible, even if the further fine-tuning and adaptation of these models to their respective domains are required first.",
        "authors": [
            "L. Adams",
            "Felix Busch",
            "D. Truhn",
            "M. Makowski",
            "H. Aerts",
            "K. Bressem"
        ],
        "citations": 43,
        "references": 11,
        "year": 2022
    },
    {
        "title": "Diffusion Model with Detail Complement for Super-Resolution of Remote Sensing",
        "abstract": "Remote sensing super-resolution (RSSR) aims to improve remote sensing (RS) image resolution while providing finer spatial details, which is of great significance for high-quality RS image interpretation. The traditional RSSR is based on the optimization method, which pays insufficient attention to small targets and lacks the ability of model understanding and detail supplement. To alleviate the above problems, we propose the generative Diffusion Model with Detail Complement (DMDC) for RS super-resolution. Firstly, unlike traditional optimization models with insufficient image understanding, we introduce the diffusion model as a generation model into RSSR tasks and regard low-resolution images as condition information to guide image generation. Next, considering that generative models may not be able to accurately recover specific small objects and complex scenes, we propose the detail supplement task to improve the recovery ability of DMDC. Finally, the strong diversity of the diffusion model makes it possibly inappropriate in RSSR, for this purpose, we come up with joint pixel constraint loss and denoise loss to optimize the direction of inverse diffusion. The extensive qualitative and quantitative experiments demonstrate the superiority of our method in RSSR with small and dense targets. Moreover, the results from direct transfer to different datasets also prove the superior generalization ability of DMDC.",
        "authors": [
            "Jinzhe Liu",
            "Zhiqiang Yuan",
            "Zhaoying Pan",
            "Yiqun Fu",
            "Li Liu",
            "Bin Lu"
        ],
        "citations": 41,
        "references": 47,
        "year": 2022
    },
    {
        "title": "Text and Image Guided 3D Avatar Generation and Manipulation",
        "abstract": "The manipulation of latent space has recently become an interesting topic in the field of generative models. Recent research shows that latent directions can be used to manipulate images towards certain attributes. However, controlling the generation process of 3D generative models remains a challenge. In this work, we propose a novel 3D manipulation method that can manipulate both the shape and texture of the model using text or image-based prompts such as ’a young face’ or ’a surprised face’. We leverage the power of Contrastive Language-Image Pre-training (CLIP) model and a pre-trained 3D GAN model designed to generate face avatars and create a fully differentiable rendering pipeline to manipulate meshes. More specifically, our method takes an input latent code and modifies it such that the target attribute specified by a text or image prompt is present or enhanced while leaving other attributes largely unaffected. Our method requires only 5 minutes per manipulation, and we demonstrate the effectiveness of our approach with extensive results and comparisons.",
        "authors": [
            "Zehranaz Canfes",
            "M. Atasoy",
            "Alara Dirik",
            "Pinar Yanardag"
        ],
        "citations": 38,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) are a recent family of generative models that achieve state-of-the-art results. In order to obtain class-conditional generation, it was suggested to guide the diffusion process by gradients from a time-dependent classifier. While the idea is theoretically sound, deep learning-based classifiers are infamously susceptible to gradient-based adversarial attacks. Therefore, while traditional classifiers may achieve good accuracy scores, their gradients are possibly unreliable and might hinder the improvement of the generation results. Recent work discovered that adversarially robust classifiers exhibit gradients that are aligned with human perception, and these could better guide a generative process towards semantically meaningful images. We utilize this observation by defining and training a time-dependent adversarially robust classifier and use it as guidance for a generative diffusion model. In experiments on the highly challenging and diverse ImageNet dataset, our scheme introduces significantly more intelligible intermediate gradients, better alignment with theoretical findings, as well as improved generation results under several evaluation metrics. Furthermore, we conduct an opinion survey whose findings indicate that human raters prefer our method's results.",
        "authors": [
            "Bahjat Kawar",
            "Roy Ganz",
            "Michael Elad"
        ],
        "citations": 35,
        "references": 85,
        "year": 2022
    },
    {
        "title": "Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields",
        "abstract": null,
        "authors": [
            "Yuedong Chen",
            "Qianyi Wu",
            "Chuanxia Zheng",
            "Tat-Jen Cham",
            "Jianfei Cai"
        ],
        "citations": 34,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Retrieval-based Controllable Molecule Generation",
        "abstract": "Generating new molecules with specified chemical and biological properties via generative models has emerged as a promising direction for drug discovery. However, existing methods require extensive training/fine-tuning with a large dataset, often unavailable in real-world generation tasks. In this work, we propose a new retrieval-based framework for controllable molecule generation. We use a small set of exemplar molecules, i.e., those that (partially) satisfy the design criteria, to steer the pre-trained generative model towards synthesizing molecules that satisfy the given design criteria. We design a retrieval mechanism that retrieves and fuses the exemplar molecules with the input molecule, which is trained by a new self-supervised objective that predicts the nearest neighbor of the input molecule. We also propose an iterative refinement process to dynamically update the generated molecules and retrieval database for better generalization. Our approach is agnostic to the choice of generative models and requires no task-specific fine-tuning. On various tasks ranging from simple design criteria to a challenging real-world scenario for designing lead compounds that bind to the SARS-CoV-2 main protease, we demonstrate our approach extrapolates well beyond the retrieval database, and achieves better performance and wider applicability than previous methods. Code is available at https://github.com/NVlabs/RetMol.",
        "authors": [
            "Zichao Wang",
            "Weili Nie",
            "Zhuoran Qiao",
            "Chaowei Xiao",
            "Richard Baraniuk",
            "Anima Anandkumar"
        ],
        "citations": 30,
        "references": 100,
        "year": 2022
    },
    {
        "title": "Full-Band General Audio Synthesis with Score-Based Diffusion",
        "abstract": "Recent works have shown the capability of deep generative models to tackle general audio synthesis from a single label, producing a variety of impulsive, tonal, and environmental sounds. Such models operate on band-limited signals and, as a result of an autoregressive approach, they are typically conformed by pre-trained latent encoders and/or several cascaded modules. In this work, we propose a diffusion-based generative model for general audio synthesis, named DAG, which deals with full-band signals end-to-end in the waveform domain. Results show the superiority of DAG over existing label-conditioned generators in terms of both quality and diversity. More specifically, when compared to the state of the art, the band-limited and full-band versions of DAG achieve relative improvements that go up to 40 and 65%, respectively. We believe DAG is flexible enough to accommodate different conditioning schemas while providing good quality synthesis.",
        "authors": [
            "Santiago Pascual",
            "Gautam Bhattacharya",
            "Chunghsin Yeh",
            "Jordi Pons",
            "J. Serrà"
        ],
        "citations": 32,
        "references": 29,
        "year": 2022
    },
    {
        "title": "Improved Masked Image Generation with Token-Critic",
        "abstract": "Non-autoregressive generative transformers recently demonstrated impressive image generation performance, and orders of magnitude faster sampling than their autoregressive counterparts. However, optimal parallel sampling from the true joint distribution of visual tokens remains an open challenge. In this paper we introduce Token-Critic, an auxiliary model to guide the sampling of a non-autoregressive generative transformer. Given a masked-and-reconstructed real image, the Token-Critic model is trained to distinguish which visual tokens belong to the original image and which were sampled by the generative transformer. During non-autoregressive iterative sampling, Token-Critic is used to select which tokens to accept and which to reject and resample. Coupled with Token-Critic, a state-of-the-art generative transformer significantly improves its performance, and outperforms recent diffusion models and GANs in terms of the trade-off between generated image quality and diversity, in the challenging class-conditional ImageNet generation.",
        "authors": [
            "José Lezama",
            "Huiwen Chang",
            "Lu Jiang",
            "Irfan Essa"
        ],
        "citations": 38,
        "references": 41,
        "year": 2022
    },
    {
        "title": "3DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design",
        "abstract": "Deep learning has achieved tremendous success in designing novel chemical compounds with desirable pharmaceutical properties. In this work, we focus on a new type of drug design problem -- generating a small\"linker\"to physically attach two independent molecules with their distinct functions. The main computational challenges include: 1) the generation of linkers is conditional on the two given molecules, in contrast to generating full molecules from scratch in previous works; 2) linkers heavily depend on the anchor atoms of the two molecules to be connected, which are not known beforehand; 3) 3D structures and orientations of the molecules need to be considered to avoid atom clashes, for which equivariance to E(3) group are necessary. To address these problems, we propose a conditional generative model, named 3DLinker, which is able to predict anchor atoms and jointly generate linker graphs and their 3D structures based on an E(3) equivariant graph variational autoencoder. So far as we know, there are no previous models that could achieve this task. We compare our model with multiple conditional generative models modified from other molecular design tasks and find that our model has a significantly higher rate in recovering molecular graphs, and more importantly, accurately predicting the 3D coordinates of all the atoms.",
        "authors": [
            "Yinan Huang",
            "Xing Peng",
            "Jianzhu Ma",
            "Muhan Zhang"
        ],
        "citations": 39,
        "references": 60,
        "year": 2022
    },
    {
        "title": "DP-CTGAN: Differentially Private Medical Data Generation Using CTGANs",
        "abstract": null,
        "authors": [
            "Mei-Ling Fang",
            "D. Dhami",
            "K. Kersting"
        ],
        "citations": 40,
        "references": 32,
        "year": 2022
    },
    {
        "title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization",
        "abstract": "Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.",
        "authors": [
            "Zheng Li",
            "Zijian Wang",
            "Ming Tan",
            "Ramesh Nallapati",
            "Parminder Bhatia",
            "Andrew O. Arnold",
            "Bing Xiang",
            "D. Roth"
        ],
        "citations": 37,
        "references": 33,
        "year": 2022
    },
    {
        "title": "Rewriting geometric rules of a GAN",
        "abstract": "Deep generative models make visual content creation more accessible to novice users by automating the synthesis of diverse, realistic content based on a collected dataset. However, the current machine learning approaches miss a key element of the creative process - the ability to synthesize things that go far beyond the data distribution and everyday experience. To begin to address this issue, we enable a user to \"warp\" a given model by editing just a handful of original model outputs with desired geometric changes. Our method applies a low-rank update to a single model layer to reconstruct edited examples. Furthermore, to combat overfitting, we propose a latent space augmentation method based on style-mixing. Our method allows a user to create a model that synthesizes endless objects with defined geometric changes, enabling the creation of a new generative model without the burden of curating a large-scale dataset. We also demonstrate that edited models can be composed to achieve aggregated effects, and we present an interactive interface to enable users to create new models through composition. Empirical measurements on multiple test cases suggest the advantage of our method against recent GAN fine-tuning methods. Finally, we showcase several applications using the edited models, including latent space interpolation and image editing.",
        "authors": [
            "Sheng-Yu Wang",
            "David Bau",
            "Jun-Yan Zhu"
        ],
        "citations": 32,
        "references": 89,
        "year": 2022
    },
    {
        "title": "UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog",
        "abstract": "Visual Dialog aims to answer multi-round, interactive questions based on the dialog history and image content. Existing methods either consider answer ranking and generating individually or only weakly capture the relation across the two tasks implicitly by two separate models. The research on a universal framework that jointly learns to rank and generate answers in a single model is seldom explored. In this paper, we propose a contrastive learning-based framework UTC to unify and facilitate both discriminative and generative tasks in visual dialog with a single model. Specifically, considering the inherent limitation of the previous learning paradigm, we devise two inter-task contrastive losses i.e., context contrastive loss and answer contrastive loss to make the discriminative and generative tasks mutually reinforce each other. These two com-plementary contrastive losses exploit dialog context and target answer as anchor points to provide representation learning signals from different perspectives. We evaluate our proposed UTC on the VisDial v1.0 dataset, where our method outperforms the state-of-the-art on both discriminative and generative tasks and surpasses previous state-of-the-art generative methods by more than 2 absolute points on Recall@1.",
        "authors": [
            "Cheng Chen",
            "Yudong Zhu",
            "Zhenshan Tan",
            "Qingrong Cheng",
            "Xin Jiang",
            "Qun Liu",
            "X. Gu"
        ],
        "citations": 36,
        "references": 25,
        "year": 2022
    },
    {
        "title": "DiffusER: Discrete Diffusion via Edit-based Reconstruction",
        "abstract": "In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DiffusER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models -- a class of models that use a Markov chain of denoising steps to incrementally generate data. DiffusER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DiffusER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps.",
        "authors": [
            "Machel Reid",
            "Vincent J. Hellendoorn",
            "Graham Neubig"
        ],
        "citations": 35,
        "references": 34,
        "year": 2022
    },
    {
        "title": "Cycle-Consistent Counterfactuals by Latent Transformations",
        "abstract": "CounterFactual (CF) visual explanations try to find images similar to the query image that change the decision of a vision system to a specified outcome. Existing methods either require inference-time optimization or joint training with a generative adversarial model which makes them time-consuming and difficult to use in practice. We propose a novel approach, Cycle-Consistent Counterfactuals by Latent Transformations (C3LT), which learns a latent transformation that automatically generates visual CFs by steering in the latent space of generative models. Our method uses cycle consistency between the query and CF latent representations which helps our training to find better solutions. C3LT can be easily plugged into any state-of-the-art pretrained generative network. This enables our method to generate high-quality and interpretable CF images at high resolution such as those in ImageNet. In addition to several established metrics for evaluating CF explanations, we introduce a novel metric tailored to assess the quality of the generated CF examples and validate the effectiveness of our method on an extensive set of experiments.",
        "authors": [
            "S. Khorram",
            "Li Fuxin"
        ],
        "citations": 29,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Learning to Generate 3D Shapes from a Single Example",
        "abstract": "Existing generative models for 3D shapes are typically trained on a large 3D dataset, often of a specific object category. In this paper, we investigate the deep generative model that learns from only a single reference 3D shape. Specifically, we present a multi-scale GAN-based model designed to capture the input shape's geometric features across a range of spatial scales. To avoid large memory and computational cost induced by operating on the 3D volume, we build our generator atop the tri-plane hybrid representation, which requires only 2D convolutions. We train our generative model on a voxel pyramid of the reference shape, without the need of any external supervision or manual annotation. Once trained, our model can generate diverse and high-quality 3D shapes possibly of different sizes and aspect ratios. The resulting shapes present variations across different scales, and at the same time retain the global structure of the reference shape. Through extensive evaluation, both qualitative and quantitative, we demonstrate that our model can generate 3D shapes of various types.1",
        "authors": [
            "Rundi Wu",
            "Changxi Zheng"
        ],
        "citations": 29,
        "references": 81,
        "year": 2022
    },
    {
        "title": "Stochastic normalizing flows as non-equilibrium transformations",
        "abstract": null,
        "authors": [
            "M. Caselle",
            "E. Cellini",
            "A. Nada",
            "M. Panero"
        ],
        "citations": 29,
        "references": 101,
        "year": 2022
    },
    {
        "title": "Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images",
        "abstract": "Evaluation metrics in image synthesis play a key role to measure performances of generative models. However, most metrics mainly focus on image fidelity. Existing diversity metrics are derived by comparing distributions, and thus they cannot quantify the diversity or rarity degree of each generated image. In this work, we propose a new evaluation metric, called `rarity score', to measure the individual rarity of each image synthesized by generative models. We first show empirical observation that common samples are close to each other and rare samples are far from each other in nearest-neighbor distances of feature space. We then use our metric to demonstrate that the extent to which different generative models produce rare images can be effectively compared. We also propose a method to compare rarities between datasets that share the same concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in different designs of feature spaces to better understand the relationship between feature spaces and resulting sparse images. Code will be publicly available online for the research community.",
        "authors": [
            "Jiyeon Han",
            "Hwanil Choi",
            "Yunjey Choi",
            "Jae Hyun Kim",
            "Jung-Woo Ha",
            "Jaesik Choi"
        ],
        "citations": 27,
        "references": 34,
        "year": 2022
    },
    {
        "title": "A GAN-Based Short-Term Link Traffic Prediction Approach for Urban Road Networks Under a Parallel Learning Framework",
        "abstract": "Road link speed is often employed as an essential measure of traffic state in the operation of an urban traffic network. Not only real-time traffic demand but also signal timings and other local planning factors are major influential factors. This paper proposes a short-term traffic speed prediction approach, called PL-WGAN, for urban road networks, which is considered an important part of a novel parallel learning framework for traffic control and operation. The proposed method applies Wasserstein Generative Adversarial Nets (WGAN) for robust data-driven traffic modeling using a combination of generative neural network and discriminative neural network. The generative neural network models the road link features of the adjacent intersections and the control parameters of intersections using a hybrid graph block. In addition, the spatial-temporal relations are captured by stacking a graph convolutional network (GCN), a recurrent neural network (RNN), and an attention mechanism. A comprehensive computational experiment was carried out including comparing model prediction and computational performances with several state-of-the-art deep learning models. The proposed approach has been implemented and applied for predicting short-term link traffic speed in a large-scale urban road network in Hangzhou, China. The results suggest that it provides a scalable and effective traffic prediction solution for urban road networks.",
        "authors": [
            "Junchen Jin",
            "Dingding Rong",
            "Tong Zhang",
            "Qingyuan Ji",
            "Haifeng Guo",
            "Yisheng Lv",
            "Xiaoliang Ma",
            "Fei Wang"
        ],
        "citations": 32,
        "references": 40,
        "year": 2022
    },
    {
        "title": "FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations",
        "abstract": null,
        "authors": [
            "Cemre Karakas",
            "Alara Dirik",
            "Eylül Yalçınkaya",
            "Pinar Yanardag"
        ],
        "citations": 28,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Image Super-Resolution With Deep Variational Autoencoders",
        "abstract": "Image super-resolution (SR) techniques are used to generate a high-resolution image from a low-resolution image. Until now, deep generative models such as autoregressive models and Generative Adversarial Networks (GANs) have proven to be effective at modelling high-resolution images. VAE-based models have often been criticised for their feeble generative performance, but with new advancements such as VDVAE, there is now strong evidence that deep VAEs have the potential to outperform current state-of-the-art models for high-resolution image generation. In this paper, we introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE methodologies to improve upon the results of similar models. VDVAE-SR tackles image super-resolution using transfer learning on pretrained VDVAEs. The presented model is competitive with other state-of-the-art models, having comparable results on image quality metrics.",
        "authors": [
            "Darius Chira",
            "Ilian Haralampiev",
            "O. Winther",
            "Andrea Dittadi",
            "Valentin Li'evin"
        ],
        "citations": 27,
        "references": 47,
        "year": 2022
    },
    {
        "title": "SOS: Score-based Oversampling for Tabular Data",
        "abstract": "Score-based generative models (SGMs) are a recent breakthrough in generating fake images. SGMs are known to surpass other generative models, e.g., generative adversarial networks (GANs) and variational autoencoders (VAEs). Being inspired by their big success, in this work, we fully customize them for generating fake tabular data. In particular, we are interested in oversampling minor classes since imbalanced classes frequently lead to sub-optimal training outcomes. To our knowledge, we are the first presenting a score-based tabular data oversampling method. Firstly, we re-design our own score network since we have to process tabular data. Secondly, we propose two options for our generation method: the former is equivalent to a style transfer for tabular data and the latter uses the standard generative policy of SGMs. Lastly, we define a fine-tuning method, which further enhances the oversampling quality. In our experiments with 6 datasets and 10 baselines, our method outperforms other oversampling methods in all cases.",
        "authors": [
            "Jayoung Kim",
            "Chae-Eun Lee",
            "Yehjin Shin",
            "Sewon Park",
            "Minjung Kim",
            "Noseong Park",
            "Jihoon Cho"
        ],
        "citations": 25,
        "references": 46,
        "year": 2022
    },
    {
        "title": "Wasserstein Iterative Networks for Barycenter Estimation",
        "abstract": "Wasserstein barycenters have become popular due to their ability to represent the average of probability measures in a geometrically meaningful way. In this paper, we present an algorithm to approximate the Wasserstein-2 barycenters of continuous measures via a generative model. Previous approaches rely on regularization (entropic/quadratic) which introduces bias or on input convex neural networks which are not expressive enough for large-scale tasks. In contrast, our algorithm does not introduce bias and allows using arbitrary neural networks. In addition, based on the celebrity faces dataset, we construct Ave, celeba! dataset which can be used for quantitative evaluation of barycenter algorithms by using standard metrics of generative models such as FID.",
        "authors": [
            "Alexander Korotin",
            "Vage Egiazarian",
            "Lingxiao Li",
            "Evgeny Burnaev"
        ],
        "citations": 25,
        "references": 75,
        "year": 2022
    },
    {
        "title": "A Hybrid Method for Keystroke Biometric User Identification",
        "abstract": "The generative model and discriminative model are the two categories of statistical models used in keystroke biometric areas. Generative models have the trait of handling missing or irregular data, and perform well for limited training data. Discriminative models are fast in making predictions for new data, resulting in faster classification of new data compared to the generative models. In an attempt to build an efficient model for keystroke biometric user identification, this study proposes a hybrid POHMM/SVM method taking advantage of both generative and discriminative models. The partially observable hidden Markov model (POHMM) is an extension of the hidden Markov model (HMM), which has shown promising performance in user verification and handling missing or infrequent data. On the other hand, the support vector machine (SVM) has been a widely used discriminative model in keystroke biometric systems for the last decade and achieved a higher accuracy rate for large data sets. In the proposed model, features are extracted using the POHMM model, and a one-class support vector machine is used as the anomaly detector. For user identification, the study examines POHMM parameters using five different discriminative classifiers: support vector machines, k-nearest neighbor, random forest, multilayer perceptron (MLP) neural network, and logistic regression. The best accuracy of 91.3% (mean 0.868, SD 0.132) is achieved by the proposed hybrid POHMM/SVM approach among all generative and discriminative models.",
        "authors": [
            "M. Ali",
            "Kutub Thakur",
            "Muath A. Obaidat"
        ],
        "citations": 24,
        "references": 62,
        "year": 2022
    },
    {
        "title": "Can There be Art Without an Artist?",
        "abstract": "Generative AI based art has proliferated in the past year, with increasingly impressive use cases from generating fake human faces to the creation of systems that can generate thousands of artistic images from text prompts - some of these images have even been\"good\"enough to win accolades from qualified judges. In this paper, we explore how Generative Models have impacted artistry, not only from a qualitative point of view, but also from an angle of exploitation of artists -- both via plagiarism, where models are trained on their artwork without permission, and via profit shifting, where profits in the art market have shifted from art creators to model owners. However, we posit that if deployed responsibly, AI generative models have the possibility of being a positive, new modality in art that does not displace or harm existing artists.",
        "authors": [
            "A. Ghosh",
            "Genoveva Fossas"
        ],
        "citations": 23,
        "references": 26,
        "year": 2022
    },
    {
        "title": "Scaling Up Probabilistic Circuits by Latent Variable Distillation",
        "abstract": "Probabilistic Circuits (PCs) are a unified framework for tractable probabilistic models that support efficient computation of various probabilistic queries (e.g., marginal probabilities). One key challenge is to scale PCs to model large and high-dimensional real-world datasets: we observe that as the number of parameters in PCs increases, their performance immediately plateaus. This phenomenon suggests that the existing optimizers fail to exploit the full expressive power of large PCs. We propose to overcome such bottleneck by latent variable distillation: we leverage the less tractable but more expressive deep generative models to provide extra supervision over the latent variables of PCs. Specifically, we extract information from Transformer-based generative models to assign values to latent variables of PCs, providing guidance to PC optimizers. Experiments on both image and language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent variable distillation substantially boosts the performance of large PCs compared to their counterparts without latent variable distillation. In particular, on the image modeling benchmarks, PCs achieve competitive performance against some of the widely-used deep generative models, including variational autoencoders and flow-based models, opening up new avenues for tractable generative modeling. Our code can be found at https://github.com/UCLA-StarAI/LVD.",
        "authors": [
            "Anji Liu",
            "Honghua Zhang",
            "Guy Van den Broeck"
        ],
        "citations": 19,
        "references": 38,
        "year": 2022
    },
    {
        "title": "FingerprintNet: Synthesized Fingerprints for Generated Image Detection",
        "abstract": null,
        "authors": [
            "Yonghyun Jeong",
            "Doyeon Kim",
            "Youngmin Ro",
            "Pyounggeon Kim",
            "Jongwon Choi"
        ],
        "citations": 19,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Semi-Parametric Neural Image Synthesis",
        "abstract": "Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Much of this success is due to the scalability of these architectures and hence caused by a dramatic increase in model complexity and in the computational resources invested in training these models. Our work questions the underlying paradigm of compressing large training data into ever growing parametric representations. We rather present an orthogonal, semi-parametric approach. We complement comparably small diffusion or autoregressive models with a separate image database and a retrieval strategy. During training we retrieve a set of nearest neighbors from this external database for each training instance and condition the generative model on these informative samples. While the retrieval approach is providing the (local) content, the model is focusing on learning the composition of scenes based on this content. As demonstrated by our experiments, simply swapping the database for one with different contents transfers a trained model post-hoc to a novel domain. The evaluation shows competitive performance on tasks which the generative model has not been trained on, such as class-conditional synthesis, zero-shot stylization or text-to-image synthesis without requiring paired text-image data. With negligible memory and computational overhead for the external database and retrieval we can significantly reduce the parameter count of the generative model and still outperform the state-of-the-art.",
        "authors": [
            "A. Blattmann",
            "Robin Rombach",
            "Kaan Oktay",
            "Jonas Muller",
            "B. Ommer"
        ],
        "citations": 26,
        "references": 96,
        "year": 2022
    },
    {
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.",
        "authors": [
            "J. Park",
            "Joseph C. O'Brien",
            "Carrie J. Cai",
            "M. Morris",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "citations": 1000,
        "references": 113,
        "year": 2023
    },
    {
        "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation",
        "abstract": "Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.",
        "authors": [
            "Jiaxiang Tang",
            "Jiawei Ren",
            "Hang Zhou",
            "Ziwei Liu",
            "Gang Zeng"
        ],
        "citations": 454,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Genie: Generative Interactive Environments",
        "abstract": "We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",
        "authors": [
            "Jake Bruce",
            "Michael D. Dennis",
            "Ashley Edwards",
            "Jack Parker-Holder",
            "Yuge Shi",
            "Edward Hughes",
            "Matthew Lai",
            "Aditi Mavalankar",
            "Richie Steigerwald",
            "Chris Apps",
            "Y. Aytar",
            "Sarah Bechtle",
            "Feryal M. P. Behbahani",
            "Stephanie Chan",
            "N. Heess",
            "Lucy Gonzalez",
            "Simon Osindero",
            "Sherjil Ozair",
            "Scott Reed",
            "Jingwei Zhang",
            "Konrad Zolna",
            "Jeff Clune",
            "Nando de Freitas",
            "Satinder Singh",
            "Tim Rocktaschel"
        ],
        "citations": 80,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Generative AI in healthcare: an implementation science informed translational path on application, integration and governance",
        "abstract": null,
        "authors": [
            "S. Reddy"
        ],
        "citations": 68,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Examining Science Education in ChatGPT: An Exploratory Study of Generative Artificial Intelligence",
        "abstract": null,
        "authors": [
            "G. Cooper"
        ],
        "citations": 491,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Students’ voices on generative AI: perceptions, benefits, and challenges in higher education",
        "abstract": null,
        "authors": [
            "C. Chan",
            "Wenjie Hu"
        ],
        "citations": 407,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold",
        "abstract": "Synthesizing visual content that meets users’ needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to \"drag\" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object’s rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.",
        "authors": [
            "Xingang Pan",
            "A. Tewari",
            "Thomas Leimkühler",
            "Lingjie Liu",
            "Abhimitra Meka",
            "C. Theobalt"
        ],
        "citations": 185,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.",
        "abstract": "\n This study assesses the diagnostic accuracy of the Generative Pre-trained Transformer 4 (GPT-4) artificial intelligence (AI) model in a series of challenging cases.\n",
        "authors": [
            "Zahir Kanjee",
            "Byron Crowe",
            "A. Rodman"
        ],
        "citations": 175,
        "references": 5,
        "year": 2023
    },
    {
        "title": "EscherNet: A Generative Model for Scalable View Synthesis",
        "abstract": "We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis ─it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: https://kxhit.github.io/EscherNet.",
        "authors": [
            "Xin Kong",
            "Shikun Liu",
            "Xiaoyang Lyu",
            "Marwan Taher",
            "Xiaojuan Qi",
            "Andrew J. Davison"
        ],
        "citations": 25,
        "references": 63,
        "year": 2024
    },
    {
        "title": "GAIA-1: A Generative World Model for Autonomous Driving",
        "abstract": "Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves. To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representation that captures expectations of future events, combined with its ability to generate realistic samples, provides new possibilities for innovation in the field of autonomy, enabling enhanced and accelerated training of autonomous driving technology.",
        "authors": [
            "Anthony Hu",
            "Lloyd Russell",
            "Hudson Yeo",
            "Zak Murez",
            "George Fedoseev",
            "Alex Kendall",
            "Jamie Shotton",
            "Gianluca Corrado"
        ],
        "citations": 149,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Modelling Generative AI Acceptance, Perceived Teachers' Enthusiasm and Self‐Efficacy to English as a Foreign Language Learners' Well‐Being in the Digital Era",
        "abstract": "As artificial intelligence (AI) has been integrated into foreign language (FL) education, learners' well‐being is influenced by various factors, including technological, personal and contextual elements. However, few studies explored how external and internal factors jointly shape FL learners' well‐being in the era of generative AI. To fill this gap, this study explores the effects of generative AI acceptance, perceived teachers' enthusiasm and self‐efficacy on FL learners' well‐being by investigating 613 university learners of English as a foreign language (EFL). The structural equation modelling results reveal that (1) generative AI acceptance positively predicts EFL learners' well‐being and self‐efficacy; (2) perceived teachers' enthusiasm does not predict learners' well‐being and positively predicts EFL learners' self‐efficacy; and (3) the self‐efficacy for receptive skills mediates the relationship between generative AI acceptance/perceived teachers' enthusiasm and EFL learners' well‐being, whereas self‐efficacy for productive skills does not play the mediation role. This research broadens the understanding of the antecedents of EFL learners' well‐being and extends the application of self‐efficacy theory in the AI‐driven educational environment, providing significant pedagogical implications.",
        "authors": [
            "Fangwei Huang",
            "Yongliang Wang",
            "Haijing Zhang"
        ],
        "citations": 28,
        "references": 70,
        "year": 2024
    },
    {
        "title": "GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling",
        "abstract": "3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation. Project page: https://gaussiancube.github.io/.",
        "authors": [
            "Bowen Zhang",
            "Yiji Cheng",
            "Jiaolong Yang",
            "Chunyu Wang",
            "Feng Zhao",
            "Yansong Tang",
            "Dong Chen",
            "Baining Guo"
        ],
        "citations": 20,
        "references": 66,
        "year": 2024
    },
    {
        "title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?",
        "abstract": "As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and self-supervised pretraining to generative modeling methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including text, images, videos, 3D content, etc., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream industries, such as education and creativity content. Finally, we discuss the challenges currently faced and present an outlook on how generative AI might evolve in the near future.",
        "authors": [
            "Chaoning Zhang",
            "Chenshuang Zhang",
            "Sheng Zheng",
            "Yu Qiao",
            "Chenghao Li",
            "Mengchun Zhang",
            "Sumit Kumar Dam",
            "Chu Myaet Thwal",
            "Ye Lin Tun",
            "Le Luang Huy",
            "Donguk kim",
            "S. Bae",
            "Lik-Hang Lee",
            "Yang Yang",
            "Heng Tao Shen",
            "In-So Kweon",
            "Choong-Seon Hong"
        ],
        "citations": 139,
        "references": 530,
        "year": 2023
    },
    {
        "title": "Generative Diffusion Prior for Unified Image Restoration and Enhancement",
        "abstract": "Existing image restoration methods mostly leverage the posterior distribution of natural images. However, they often assume known degradation and also require supervised training, which restricts their adaptation to complex real applications. In this work, we propose the Generative Diffusion Prior (GDP) to effectively model the posterior distributions in an unsupervised sampling manner. GDP utilizes a pre-train denoising diffusion generative model (DDPM) for solving linear inverse, non-linear, or blind problems. Specifically, GDP systematically explores a protocol of conditional guidance, which is verified more practical than the commonly used guidance way. Furthermore, GDP is strength at optimizing the parameters of degradation model during the denoising process, achieving blind image restoration. Besides, we devise hierarchical guidance and patch-based methods, enabling the GDP to generate images of arbitrary resolutions. Experimentally, we demonstrate GDP's versatility on several image datasets for linear problems, such as super-resolution, deblurring, inpainting, and colorization, as well as non-linear and blind issues, such as low-light enhancement and HDR image recovery. GDP outperforms the current leading unsupervised methods on the diverse benchmarks in reconstruction quality and perceptual quality. Moreover, GDP also generalizes well for natural images or synthesized images with arbitrary sizes from various tasks out of the distribution of the ImageNet training set. The project page is available at https://generativediffusionprior.github.io/",
        "authors": [
            "Ben Fei",
            "Zhaoyang Lyu",
            "Liang Pan",
            "Junzhe Zhang",
            "Weidong Yang",
            "Tian-jian Luo",
            "Bo Zhang",
            "Bo Dai"
        ],
        "citations": 131,
        "references": 108,
        "year": 2023
    },
    {
        "title": "MeshDiffusion: Score-based Generative 3D Mesh Modeling",
        "abstract": "We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.",
        "authors": [
            "Zhen Liu",
            "Yao Feng",
            "Michael J. Black",
            "D. Nowrouzezahrai",
            "L. Paull",
            "Wei-yu Liu"
        ],
        "citations": 124,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Splitwise: Efficient Generative LLM Inference Using Phase Splitting",
        "abstract": "Generative large language model (LLM) applications are growing rapidly, leading to large-scale deployments of expensive and power-hungry GPUs. Our characterization of LLM inference shows that each inference request undergoes two phases: a compute-intensive prompt computation phase and a memory intensive token generation phase, each with distinct latency, throughput, memory, and power characteristics. Despite state-of-the-art batching and scheduling, the token generation phase underutilizes compute resources. Unlike prompt computation, token generation does not need the compute capability of the latest GPUs and can be run with lower power and cost. Based on these insights, we propose Splitwise, a model deployment and scheduling technique that splits the two phases of LLM inference requests on to separate machines. Splitwise enables phase-specific resource management using hardware that is well suited for each phase. Request state is transferred efficiently between machines using optimized network libraries on the fast back-plane interconnects available in today’s GPU clusters. Using Splitwise, we design homogeneous and heterogeneous LLM inference clusters optimized for throughput, cost, and power Compared to current designs, Splitwise clusters achieve up to $1.4 \\times$ higher throughput at $\\mathbf{2 0 \\%}$ lower cost. Alternatively, they can deliver $2.35 \\times$ more throughput under the same power and cost budgets.",
        "authors": [
            "Pratyush Patel",
            "Esha Choukse",
            "Chaojie Zhang",
            "Íñigo Goiri",
            "Aashaka Shah",
            "Saeed Maleki",
            "Ricardo Bianchini"
        ],
        "citations": 101,
        "references": 85,
        "year": 2023
    },
    {
        "title": "The GenAI is out of the bottle: generative artificial intelligence from a business model innovation perspective",
        "abstract": null,
        "authors": [
            "Dominik K. Kanbach",
            "Louisa Heiduk",
            "Georg Blueher",
            "Maximilian Schreiter",
            "Alexander Lahmann"
        ],
        "citations": 118,
        "references": 29,
        "year": 2023
    },
    {
        "title": "The GenAI is out of the bottle: generative artificial intelligence from a business model innovation perspective",
        "abstract": null,
        "authors": [
            "Dominik K. Kanbach",
            "Louisa Heiduk",
            "Georg Blueher",
            "Maximilian Schreiter",
            "Alexander Lahmann"
        ],
        "citations": 118,
        "references": 29,
        "year": 2023
    },
    {
        "title": "SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction",
        "abstract": "Recently video generation has achieved substantial progress with realistic results. Nevertheless, existing AI-generated videos are usually very short clips (\"shot-level\") depicting a single scene. To deliver a coherent long video (\"story-level\"), it is desirable to have creative transition and prediction effects across different clips. This paper presents a short-to-long video diffusion model, SEINE, that focuses on generative transition and prediction. The goal is to generate high-quality long videos with smooth and creative transitions between scenes and varying lengths of shot-level videos. Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions. By providing the images of different scenes as inputs, combined with text-based control, our model generates transition videos that ensure coherence and visual quality. Furthermore, the model can be readily extended to various tasks such as image-to-video animation and autoregressive video prediction. To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment. Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos. Project page: https://vchitect.github.io/SEINE-project/ .",
        "authors": [
            "Xinyuan Chen",
            "Yaohui Wang",
            "Lingjun Zhang",
            "Shaobin Zhuang",
            "Xin Ma",
            "Jiashuo Yu",
            "Yali Wang",
            "Dahua Lin",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "citations": 84,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Evaluating the Social Impact of Generative AI Systems in Systems and Society",
        "abstract": "Generative AI systems across modalities, ranging from text (including code), image, audio, and video, have broad social impacts, but there is no official standard for means of evaluating those impacts or for which impacts should be evaluated. In this paper, we present a guide that moves toward a standard approach in evaluating a base generative AI system for any modality in two overarching categories: what can be evaluated in a base system independent of context and what can be evaluated in a societal context. Importantly, this refers to base systems that have no predetermined application or deployment context, including a model itself, as well as system components, such as training data. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to listed generative modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what can be evaluated in a broader societal context, each with its own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm.",
        "authors": [
            "Irene Solaiman",
            "Zeerak Talat",
            "William Agnew",
            "Lama Ahmad",
            "Dylan Baker",
            "Su Lin Blodgett",
            "Canyu Chen",
            "Hal Daum'e",
            "Jesse Dodge",
            "Ellie Evans",
            "Felix Friedrich",
            "Usman Gohar",
            "Sara Hooker",
            "Yacine Jernite",
            "A. Luccioni",
            "Alberto Lusoli",
            "Jennifer Mickel",
            "Margaret Mitchell",
            "J. Newman",
            "Marie-Therese Png",
            "A. Strait",
            "Apostol T. Vassilev",
            "Arjun Subramonian"
        ],
        "citations": 81,
        "references": 270,
        "year": 2023
    },
    {
        "title": "Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement",
        "abstract": "Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.",
        "authors": [
            "Y. Li",
            "Xin-xin Lu",
            "Yaqing Wang",
            "De-Yu Dou"
        ],
        "citations": 80,
        "references": 110,
        "year": 2023
    },
    {
        "title": "MultiVI: deep generative model for the integration of multimodal data",
        "abstract": null,
        "authors": [
            "Tal Ashuach",
            "Mariano I. Gabitto",
            "Rohan V. Koodli",
            "Giuseppe-Antonio Saldi",
            "Michael I. Jordan",
            "N. Yosef"
        ],
        "citations": 71,
        "references": 38,
        "year": 2023
    },
    {
        "title": "A theory of continuous generative flow networks",
        "abstract": "Generative flow networks (GFlowNets) are amortized variational inference algorithms that are trained to sample from unnormalized target distributions over compositional objects. A key limitation of GFlowNets until this time has been that they are restricted to discrete spaces. We present a theory for generalized GFlowNets, which encompasses both existing discrete GFlowNets and ones with continuous or hybrid state spaces, and perform experiments with two goals in mind. First, we illustrate critical points of the theory and the importance of various assumptions. Second, we empirically demonstrate how observations about discrete GFlowNets transfer to the continuous case and show strong results compared to non-GFlowNet baselines on several previously studied tasks. This work greatly widens the perspectives for the application of GFlowNets in probabilistic inference and various modeling settings.",
        "authors": [
            "Salem Lahlou",
            "T. Deleu",
            "Pablo Lemos",
            "Dinghuai Zhang",
            "Alexandra Volokhova",
            "Alex Hernandez-Garcia",
            "L'ena N'ehale Ezzine",
            "Y. Bengio",
            "Nikolay Malkin"
        ],
        "citations": 68,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model",
        "abstract": "In this paper, we rethink the low-light image enhancement task and propose a physically explainable and generative diffusion model for low-light image enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the physical model and the generative network. Furthermore, we hope to supplement and even deduce the information missing in the low-light image through the generative network. Therefore, Diff-Retinex formulates the lowlight image enhancement problem into Retinex decomposition and conditional image generation. In the Retinex decomposition, we integrate the superiority of attention in Transformer and meticulously design a Retinex Transformer decomposition network (TDN) to decompose the image into illumination and reflectance maps. Then, we design multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution and solve the various degradations in these components respectively, including dark illumination, noise, color deviation, loss of scene contents, etc. Owing to generative diffusion model, Diff-Retinex puts the restoration of low-light subtle detail into practice. Extensive experiments conducted on real-world low-light datasets qualitatively and quantitatively demonstrate the effectiveness, superiority, and generalization of the proposed method.",
        "authors": [
            "Xunpeng Yi",
            "Han Xu",
            "H. Zhang",
            "Linfeng Tang",
            "Jiayi Ma"
        ],
        "citations": 69,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Reducing the Carbon Impact of Generative AI Inference (today and in 2035)",
        "abstract": "Generative AI, exemplified in ChatGPT, Dall-E 2, and Stable Diffusion, are exciting new applications consuming growing quantities of computing. We study the compute, energy, and carbon impacts of generative AI inference. Using ChatGPT as an exemplar, we create a workload model and compare request direction approaches (Local, Balance, CarbonMin), assessing their power use and carbon impacts. Our workload model shows that for ChatGPT-like services, inference dominates emissions, in one year producing 25x the carbon-emissions of training GPT-3. The workload model characterizes user experience, and experiments show that carbon emissions-aware algorithms (CarbonMin) can both maintain user experience and reduce carbon emissions dramatically (35%). We also consider a future scenario (2035 workload and power grids), and show that CarbonMin can reduce emissions by 56%. In both cases, the key is intelligent direction of requests to locations with low-carbon power. Combined with hardware technology advances, CarbonMin can keep emissions increase to only 20% compared to 2022 levels for 55x greater workload. Finally we consider datacenter headroom to increase effectiveness of shifting. With headroom, CarbonMin reduces 2035 emissions by 71%.",
        "authors": [
            "A. Chien",
            "Liuzixuan Lin",
            "H. Nguyen",
            "V. Rao",
            "Tristan Sharma",
            "Rajini Wijayawardana"
        ],
        "citations": 62,
        "references": 61,
        "year": 2023
    },
    {
        "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
        "abstract": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning.",
        "authors": [
            "Zequn Liu",
            "W. Zhang",
            "Yingce Xia",
            "Lijun Wu",
            "Shufang Xie",
            "Tao Qin",
            "M. Zhang",
            "Tie-Yan Liu"
        ],
        "citations": 60,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Distribution Bias Aware Collaborative Generative Adversarial Network for Imbalanced Deep Learning in Industrial IoT",
        "abstract": "The impact of Internet of Things (IoT) has become increasingly significant in smart manufacturing, while deep generative model (DGM) is viewed as a promising learning technique to work with large amount of continuously generated industrial Big Data in facilitating modern industrial applications. However, it is still challenging to handle the imbalanced data when using conventional Generative Adversarial Network (GAN) based learning strategies. In this article, we propose a distribution bias aware collaborative GAN (DB-CGAN) model for imbalanced deep learning in industrial IoT, especially to solve limitations caused by distribution bias issue between the generated data and original data, via a more robust data augmentation. An integrated data augmentation framework is constructed by introducing a complementary classifier into the basic GAN model. Specifically, a conditional generator with random labels is designed and trained adversarially with the classifier to effectively enhance augmentation of the number of data samples in minority classes, while a weight sharing scheme is newly designed between two separated feature extractors, enabling the collaborative adversarial training among generator, discriminator, and classifier. An augmentation algorithm is then developed for intelligent anomaly detection in imbalanced learning, which can significantly improve the classification accuracy based on the correction of distribution bias using the rebalanced data. Compared with five baseline methods, experiment evaluations based on two real-world imbalanced datasets demonstrate the outstanding performance of our proposed model in tackling the distribution bias issue for multiclass classification in imbalanced learning for industrial IoT applications.",
        "authors": [
            "Xiaokang Zhou",
            "Yiyong Hu",
            "Jiayi Wu",
            "Wei Liang",
            "Jianhua Ma",
            "Qun Jin"
        ],
        "citations": 92,
        "references": 29,
        "year": 2023
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "abstract": "Popularized by the Differentiable Search Index, the emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document corpora on the order of 100k in size. We conduct the first empirical study of generative retrieval techniques across various corpus scales, ultimately scaling up to the entire MS MARCO passage ranking task with a corpus of 8.8M passages and evaluating model sizes up to 11B parameters. We uncover several findings about scaling generative retrieval to millions of passages; notably, the central importance of using synthetic queries as document representations during indexing, the ineffectiveness of existing proposed architecture modifications when accounting for compute cost, and the limits of naively scaling model parameters with respect to retrieval performance. While we find that generative retrieval is competitive with state-of-the-art dual encoders on small corpora, scaling to millions of passages remains an important and unsolved challenge. We believe these findings will be valuable for the community to clarify the current state of generative retrieval, highlight the unique challenges, and inspire new research directions.",
        "authors": [
            "Ronak Pradeep",
            "Kai Hui",
            "Jai Gupta",
            "Á. Lelkes",
            "Honglei Zhuang",
            "Jimmy J. Lin",
            "Donald Metzler",
            "Vinh Q. Tran"
        ],
        "citations": 52,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Learning to Tokenize for Generative Retrieval",
        "abstract": "Conventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-to-end modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in GenRet: (i) a tokenization model that produces docids for documents; (ii) a reconstruction model that learns to reconstruct a document based on a docid; and (iii) a sequence-to-sequence retrieval model that generates relevant document identifiers directly for a designated query. By using an auto-encoding framework, GenRet learns semantic docids in a fully end-to-end manner. We also develop a progressive training scheme to capture the autoregressive nature of docids and to stabilize training. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets to assess the effectiveness of GenRet. GenRet establishes the new state-of-the-art on the NQ320K dataset. Especially, compared to generative retrieval baselines, GenRet can achieve significant improvements on the unseen documents. GenRet also outperforms comparable baselines on MS MARCO and BEIR, demonstrating the method's generalizability.",
        "authors": [
            "Weiwei Sun",
            "Lingyong Yan",
            "Zheng Chen",
            "Shuaiqiang Wang",
            "Haichao Zhu",
            "Pengjie Ren",
            "Zhumin Chen",
            "Dawei Yin",
            "M. de Rijke",
            "Z. Ren"
        ],
        "citations": 53,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Generative Pretraining in Multimodality",
        "abstract": "We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.",
        "authors": [
            "Quan Sun",
            "Qiying Yu",
            "Yufeng Cui",
            "Fan Zhang",
            "Xiaosong Zhang",
            "Yueze Wang",
            "Hongcheng Gao",
            "Jingjing Liu",
            "Tiejun Huang",
            "Xinlong Wang"
        ],
        "citations": 110,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Integrated Generative Model for Industrial Anomaly Detection via Bidirectional LSTM and Attention Mechanism",
        "abstract": "For emerging industrial Internet of Things (IIoT), intelligent anomaly detection is a key step to build smart industry. Especially, explosive time-series data pose enormous challenges to the information mining and processing for modern industry. How to identify and detect the multidimensional industrial time-series anomaly is an important issue. However, most of the existing studies fail to handle with large amounts of unlabeled data, thus generating the undesirable results. In this article, we propose a novel integrated deep generative model, which is built by generative adversarial networks based on bidirectional long short-term memory and attention mechanism (AMBi-GAN). The structure for the generator and the discriminator is the bidirectional long short-term memory with attention mechanism, which can capture time-series dependence. Reconstruction loss and generation loss test the input of sample training space and random latent space. Experimental results show that the detection performance of our proposed AMBi-GAN has the potential to improve the detection accuracy of industrial multidimensional time-series anomaly toward IIoT in the era of artificial intelligence.",
        "authors": [
            "Fanhui Kong",
            "Jianqiang Li",
            "Bin Jiang",
            "Huihui Wang",
            "H. Song"
        ],
        "citations": 68,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Better Generative Replay for Continual Federated Learning",
        "abstract": "Federated learning is a technique that enables a centralized server to learn from distributed clients via communications without accessing the client local data. However, existing federated learning works mainly focus on a single task scenario with static data. In this paper, we introduce the problem of continual federated learning, where clients incrementally learn new tasks and history data cannot be stored due to certain reasons, such as limited storage and data retention policy. Generative replay based methods are effective for continual learning without storing history data, but adapting them for this setting is challenging. By analyzing the behaviors of clients during training, we find that the unstable training process caused by distributed training on non-IID data leads to a notable performance degradation. To address this problem, we propose our FedCIL model with two simple but effective solutions: model consolidation and consistency enforcement. Our experimental results on multiple benchmark datasets demonstrate that our method significantly outperforms baselines.",
        "authors": [
            "Daiqing Qi",
            "Handong Zhao",
            "Sheng Li"
        ],
        "citations": 41,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Navigating the Complexity of Generative AI Adoption in Software Engineering",
        "abstract": "This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares–Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.",
        "authors": [
            "Daniel Russo"
        ],
        "citations": 39,
        "references": 153,
        "year": 2023
    },
    {
        "title": "Diffusion Bridge Mixture Transports, Schrödinger Bridge Problems and Generative Modeling",
        "abstract": "The dynamic Schr\\\"odinger bridge problem seeks a stochastic process that defines a transport between two target probability measures, while optimally satisfying the criteria of being closest, in terms of Kullback-Leibler divergence, to a reference process. We propose a novel sampling-based iterative algorithm, the iterated diffusion bridge mixture (IDBM) procedure, aimed at solving the dynamic Schr\\\"odinger bridge problem. The IDBM procedure exhibits the attractive property of realizing a valid transport between the target probability measures at each iteration. We perform an initial theoretical investigation of the IDBM procedure, establishing its convergence properties. The theoretical findings are complemented by numerical experiments illustrating the competitive performance of the IDBM procedure. Recent advancements in generative modeling employ the time-reversal of a diffusion process to define a generative process that approximately transports a simple distribution to the data distribution. As an alternative, we propose utilizing the first iteration of the IDBM procedure as an approximation-free method for realizing this transport. This approach offers greater flexibility in selecting the generative process dynamics and exhibits accelerated training and superior sample quality over larger discretization intervals. In terms of implementation, the necessary modifications are minimally intrusive, being limited to the training loss definition.",
        "authors": [
            "Stefano Peluchetti"
        ],
        "citations": 38,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Unifying the design space and optimizing linear and nonlinear truss metamaterials by generative modeling",
        "abstract": null,
        "authors": [
            "Li Zheng",
            "K. Karapiperis",
            "Siddhant Kumar",
            "D. Kochmann"
        ],
        "citations": 58,
        "references": 110,
        "year": 2023
    },
    {
        "title": "DreamGaussian4D: Generative 4D Gaussian Splatting",
        "abstract": "4D content generation has achieved remarkable progress recently. However, existing methods suffer from long optimization times, a lack of motion controllability, and a low quality of details. In this paper, we introduce DreamGaussian4D (DG4D), an efficient 4D generation framework that builds on Gaussian Splatting (GS). Our key insight is that combining explicit modeling of spatial transformations with static GS makes an efficient and powerful representation for 4D generation. Moreover, video generation methods have the potential to offer valuable spatial-temporal priors, enhancing the high-quality 4D generation. Specifically, we propose an integral framework with two major modules: 1) Image-to-4D GS - we initially generate static GS with DreamGaussianHD, followed by HexPlane-based dynamic generation with Gaussian deformation; and 2) Video-to-Video Texture Refinement - we refine the generated UV-space texture maps and meanwhile enhance their temporal consistency by utilizing a pre-trained image-to-video diffusion model. Notably, DG4D reduces the optimization time from several hours to just a few minutes, allows the generated 3D motion to be visually controlled, and produces animated meshes that can be realistically rendered in 3D engines.",
        "authors": [
            "Jiawei Ren",
            "Liang Pan",
            "Jiaxiang Tang",
            "Chi Zhang",
            "Ang Cao",
            "Gang Zeng",
            "Ziwei Liu"
        ],
        "citations": 72,
        "references": 59,
        "year": 2023
    },
    {
        "title": "ContactGen: Generative Contact Modeling for Grasp Generation",
        "abstract": "This paper presents a novel object-centric contact representation ContactGen for hand-object interaction. The ContactGen comprises 3 components: a contact map indicates the contact location, a part map represents the contact hand part, and a direction map tells the contact direction within each part. Given an input object, we propose a conditional generative model to predict ContactGen and adopt model-based optimization to predict diverse and geometrically feasible grasps. Experimental results demonstrate our method can generate high-fidelity and diverse human grasps for various objects.",
        "authors": [
            "Shaowei Liu",
            "Yang Zhou",
            "Jimei Yang",
            "Saurabh Gupta",
            "Shenlong Wang"
        ],
        "citations": 24,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Generative AI for Cyber Threat-Hunting in 6G-enabled IoT Networks",
        "abstract": "The next generation of cellular technology, 6G, is being developed to enable a wide range of new applications and services for the Internet of Things (IoT). One of 6G’s main advantages for IoT applications is its ability to support much higher data rates and bandwidth as well as to support ultralow latency. However, with this increased connectivity will come to an increased risk of cyber threats, as attackers will be able to exploit the large network of connected devices. Generative Artificial Intelligence (AI) can be used to detect and prevent cyber attacks by continuously learning and adapting to new threats and vulnerabilities. In this paper, we discuss the use of generative AI for cyber threat-hunting (CTH) in 6G-enabled IoT networks. Then, we propose a new generative adversarial network (GAN) and Transformer-based model for CTH in 6Genabled IoT Networks. The experimental analysis results with a new cyber security dataset demonstrate that the Transformer-based security model for CTH can detect IoT attacks with a high overall accuracy of 95%. We examine the challenges and opportunities and conclude by highlighting the potential of generative AI in enhancing the security of 6G-enabled IoT networks and call for further research to be conducted in this area.",
        "authors": [
            "M. Ferrag",
            "M. Debbah",
            "Muna Al-Hawawreh"
        ],
        "citations": 25,
        "references": 17,
        "year": 2023
    },
    {
        "title": "CityDreamer: Compositional Generative Model of Unbounded 3D Cities",
        "abstract": "3D city generation is a desirable yet challenging task, since humans are more sensitive to structural distortions in urban environments. Additionally, generating 3D cities is more complex than 3D natural scenes since buildings, as objects of the same class, exhibit a wider range of appear-ances compared to the relatively consistent appearance of objects like trees in natural scenes. To address these challenges, we propose CityDreamer, a compositional generative model designed specifically for unbounded 3D cities. Our key insight is that 3D city generation should be a com-position of different types of neural fields: 1) various building instances, and 2) background stuff, such as roads and green lands. Specifically, we adopt the bird's eye view scene representation and employ a volumetric render for both instance-oriented and stuff-oriented neural fields. The generative hash grid and periodic positional embedding are tailored as scene parameterization to suit the distinct characteristics of building instances and background stuff. Furthermore, we contribute a suite of CityGen Datasets, including OSM and GoogleEarth, which comprises a vast amount of real-world city imagery to enhance the realism of the generated 3D cities both in their layouts and appear-ances. CityDreamer achieves state-of-the-art performance not only in generating realistic 3D cities but also in local-ized editing within the generated cities.",
        "authors": [
            "Haozhe Xie",
            "Zhaoxi Chen",
            "Fangzhou Hong",
            "Ziwei Liu"
        ],
        "citations": 26,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Conditional Style-Based Generative Adversarial Networks for Renewable Scenario Generation",
        "abstract": "Day-ahead scenario generationof renewable power plays an important role in short-term power system operations due to considerable output uncertainty included. In this paper, a deep renewable scenario generation model using style-based generative adversarial networks followed by a sequence encoder network, is developed to generate accurate and reliable day-ahead scenarios directly from historical data through different-level scenario style controlling and mixing, thus achieving better characterization of renewable spatial-temporal dynamics. Meanwhile, the integration of meteorological information serving as conditions enables our model to precisely capture the complex diurnal pattern and seasonality difference of renewable power. From wind and photovoltaic power perspectives, the effectiveness of the proposed model is validated on two real-world datasets reflecting region aggregation level and distributed power station level respectively. Numerical results demonstrate the superiority of model performance through both the statistical and power system scheduling analysis, compared to three benchmarks.",
        "authors": [
            "Ran Yuan",
            "Bo Wang",
            "Yeqi Sun",
            "Xuanning Song",
            "J. Watada"
        ],
        "citations": 44,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Adopting and expanding ethical principles for generative artificial intelligence from military to healthcare",
        "abstract": null,
        "authors": [
            "David Oniani",
            "Jordan Hilsman",
            "Yifan Peng",
            "Ronald K. Poropatich",
            "Jeremy C. Pamplin",
            "Gary L Legault",
            "Yanshan Wang"
        ],
        "citations": 39,
        "references": 110,
        "year": 2023
    },
    {
        "title": "Ten years of generative adversarial nets (GANs): a survey of the state-of-the-art",
        "abstract": "Generative adversarial networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas, since their inception in 2014. Consisting of a discriminative network and a generative network engaged in a minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the ‘Top Ten Global Breakthrough Technologies List’ issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, cycle-consistent GAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely recognized variants. We also delve into recent theoretical developments, exploring the profound connection between the adversarial principle underlying GAN and Jensen–Shannon divergence while discussing the optimality characteristics of the GAN framework. The efficiency of GAN variants and their model architectures will be evaluated along with training obstacles as well as training solutions. In addition, a detailed discussion will be provided, examining the integration of GANs with newly developed deep learning frameworks such as transformers, physics-informed neural networks, large language models, and diffusion models. Finally, we reveal several issues as well as future research outlines in this field.",
        "authors": [
            "Tanujit Chakraborty",
            "Ujjwal Reddy K S",
            "Shraddha M. Naik",
            "Madhurima Panja",
            "B. Manvitha"
        ],
        "citations": 37,
        "references": 252,
        "year": 2023
    },
    {
        "title": "Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks",
        "abstract": "In the deep learning era, long video generation of high-quality still remains challenging due to the spatio-temporal complexity and continuity of videos. Existing prior works have attempted to model video distribution by representing videos as 3D grids of RGB values, which impedes the scale of generated videos and neglects continuous dynamics. In this paper, we found that the recent emerging paradigm of implicit neural representations (INRs) that encodes a continuous signal into a parameterized neural network effectively mitigates the issue. By utilizing INRs of video, we propose dynamics-aware implicit generative adversarial network (DIGAN), a novel generative adversarial network for video generation. Specifically, we introduce (a) an INR-based video generator that improves the motion dynamics by manipulating the space and time coordinates differently and (b) a motion discriminator that efficiently identifies the unnatural motions without observing the entire long frame sequences. We demonstrate the superiority of DIGAN under various datasets, along with multiple intriguing properties, e.g., long video synthesis, video extrapolation, and non-autoregressive video generation. For example, DIGAN improves the previous state-of-the-art FVD score on UCF-101 by 30.7% and can be trained on 128 frame videos of 128x128 resolution, 80 frames longer than the 48 frames of the previous state-of-the-art method.",
        "authors": [
            "Sihyun Yu",
            "Jihoon Tack",
            "Sangwoo Mo",
            "Hyunsu Kim",
            "Junho Kim",
            "Jung-Woo Ha",
            "Jinwoo Shin"
        ],
        "citations": 181,
        "references": 88,
        "year": 2022
    },
    {
        "title": "MoMask: Generative Masked Modeling of 3D Human Motions",
        "abstract": "We introduce MoMask, a novel masked modeling framework for text-driven 3D human motion generation. In Mo-Mask, a hierarchical quantization scheme is employed to represent human motion as multi-layer discrete motion tokens with high-fidelity details. Starting at the base layer, with a sequence of motion tokens obtained by vector quan-tization, the residual tokens of increasing orders are de-rived and stored at the subsequent layers of the hierar-chy. This is consequently followed by two distinct bidirectional transformers. For the base-layer motion tokens, a Masked Transformer is designated to predict randomly masked motion tokens conditioned on text input at training stage. During generation (i. e. inference) stage, starting from an empty sequence, our Masked Transformer iteratively fills up the missing tokens; Subsequently, a Residual Transformer learns to progressively predict the next-layer tokens based on the results from current layer. Extensive experiments demonstrate that MoMask outperforms the state-of-art methods on the text-to-motion generation task, with an FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the HumanML3D dataset, and 0.228 (vs 0.514) on KIT-ML, respectively. MoMask can also be seamlessly applied in related tasks without further model fine-tuning, such as text-guided temporal inpainting.",
        "authors": [
            "Chuan Guo",
            "Yuxuan Mu",
            "Muhammad Gohar Javed",
            "Sen Wang",
            "Li Cheng"
        ],
        "citations": 53,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Generative Semantic Segmentation",
        "abstract": "We present Generative Semantic Segmentation (GSS), a generative learning approach for semantic segmentation. Uniquely, we cast semantic segmentation as an image-conditioned mask generation problem. This is achieved by replacing the conventional per-pixel discriminative learning with a latent prior learning process. Specifically, we model the variational posterior distribution of latent vari-ables given the segmentation mask. To that end, the segmentation mask is expressed with a special type of image (dubbed as maskige). This posterior distribution allows to generate segmentation masks unconditionally. To achieve semantic segmentation on a given image, we further intro-duce a conditioning network. It is optimized by minimizing the divergence between the posterior distribution of maskige (i.e. segmentation masks) and the latent prior distribution of input training images. Extensive experiments on standard benchmarks show that our GSS can perform competitively to prior art alternatives in the standard semantic segmentation setting, whilst achieving a new state of the art in the more challenging cross-domain setting.",
        "authors": [
            "Jia-Qing Chen",
            "Jiachen Lu",
            "Xiatian Zhu",
            "Li Zhang"
        ],
        "citations": 29,
        "references": 62,
        "year": 2023
    },
    {
        "title": "RODIN: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion",
        "abstract": "This paper presents a 3D diffusion model that automatically generates 3D digital avatars represented as neural radiance fields (NeRFs). A significant challenge for 3D diffusion is that the memory and processing costs are prohibitive for producing high-quality results with rich details. To tackle this problem, we propose the roll-out diffusion network (RODIN), which takes a 3D NeRF model represented as multiple 2D feature maps and rolls out them onto a single 2D feature plane within which we perform 3D-aware diffusion. The RODIN model brings much-needed computational efficiency while preserving the integrity of 3D diffusion by using 3D-aware convolution that attends to projected features in the 2D plane according to their original relationships in 3D. We also use latent conditioning to orchestrate the feature generation with global coherence, leading to high-fidelity avatars and enabling semantic editing based on text prompts. Finally, we use hierarchical synthesis to further enhance details. The 3D avatars generated by our model compare favorably with those produced by existing techniques. We can generate highly detailed avatars with realistic hairstyles and facial hair. We also demonstrate 3D avatar generation from image or text, as well as text-guided editability.",
        "authors": [
            "Tengfei Wang",
            "Bo Zhang",
            "Ting Zhang",
            "Shuyang Gu",
            "Jianmin Bao",
            "T. Baltrušaitis",
            "Jingjing Shen",
            "Dong Chen",
            "Fang Wen",
            "Qifeng Chen",
            "B. Guo"
        ],
        "citations": 240,
        "references": 83,
        "year": 2022
    },
    {
        "title": "CodeKGC: Code Language Model for Generative Knowledge Graph Construction",
        "abstract": "Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provide intermediate steps, thereby improving knowledge extraction abilities. Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines.1",
        "authors": [
            "Zhen Bi",
            "Jing Chen",
            "Yinuo Jiang",
            "Feiyu Xiong",
            "Wei Guo",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "citations": 24,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Discovering highly potent antimicrobial peptides with deep generative model HydrAMP",
        "abstract": null,
        "authors": [],
        "citations": 45,
        "references": 0,
        "year": 2023
    },
    {
        "title": "An all-atom protein generative model",
        "abstract": "Proteins mediate their functions through chemical interactions; modeling these interactions, which are typically through sidechains, is an important need in protein design. However, constructing an all-atom generative model requires an appropriate scheme for managing the jointly continuous and discrete nature of proteins encoded in the structure and sequence. We describe an all-atom diffusion model of protein structure, Protpardelle, which instantiates a “superposition” over the possible sidechain states, and collapses it to conduct reverse diffusion for sample generation. When combined with sequence design methods, our model is able to co-design all-atom protein structure and sequence. Generated proteins are of good quality under the typical quality, diversity, and novelty metrics, and sidechains reproduce the chemical features and behavior of natural proteins. Finally, we explore the potential of our model conduct all-atom protein design and scaffold functional motifs in a backbone- and rotamer-free way.",
        "authors": [
            "Alexander E. Chu",
            "Lucy Cheng",
            "Gina El Nesr",
            "Minkai Xu",
            "Po-Ssu Huang"
        ],
        "citations": 22,
        "references": 67,
        "year": 2023
    },
    {
        "title": "A novel model watermarking for protecting generative adversarial network",
        "abstract": null,
        "authors": [
            "T. Qiao",
            "Yuyan Ma",
            "Ning Zheng",
            "Hanzhou Wu",
            "Yanli Chen",
            "Ming Xu",
            "Xiangyang Luo"
        ],
        "citations": 32,
        "references": 44,
        "year": 2023
    },
    {
        "title": "End-to-end Generative Pretraining for Multimodal Video Captioning",
        "abstract": "Recent video and language pretraining frameworks lack the ability to generate sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new pretraining framework for learning from unlabelled videos which can be effectively used for generative tasks such as multimodal video captioning. Unlike recent video-language pretraining frameworks, our framework trains both a multimodal video encoder and a sentence decoder jointly. To overcome the lack of captions in unlabelled videos, we leverage the future utterance as an additional text source and propose a bidirectional generation objective - we generate future utterances given the present mulitmodal context, and also the present utterance given future observations. With this objective, we train an encoder-decoder model end-to-end to generate a caption from raw pixels and transcribed speech directly. Our model achieves state-of the-art performance for multimodal video captioning on four standard benchmarks, as well as for other video understanding tasks such as VideoQA, video retrieval and action classification.",
        "authors": [
            "P. H. Seo",
            "Arsha Nagrani",
            "Anurag Arnab",
            "C. Schmid"
        ],
        "citations": 148,
        "references": 65,
        "year": 2022
    },
    {
        "title": "Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions",
        "abstract": "We give an improved theoretical analysis of score-based generative modeling. Under a score estimate with small $L^2$ error (averaged across timesteps), we provide efficient convergence guarantees for any data distribution with second-order moment, by either employing early stopping or assuming smoothness condition on the score function of the data distribution. Our result does not rely on any log-concavity or functional inequality assumption and has a logarithmic dependence on the smoothness. In particular, we show that under only a finite second moment condition, approximating the following in reverse KL divergence in $\\epsilon$-accuracy can be done in $\\tilde O\\left(\\frac{d \\log (1/\\delta)}{\\epsilon}\\right)$ steps: 1) the variance-$\\delta$ Gaussian perturbation of any data distribution; 2) data distributions with $1/\\delta$-smooth score functions. Our analysis also provides a quantitative comparison between different discrete approximations and may guide the choice of discretization points in practice.",
        "authors": [
            "Hongrui Chen",
            "Holden Lee",
            "Jianfeng Lu"
        ],
        "citations": 99,
        "references": 40,
        "year": 2022
    },
    {
        "title": "HoloFusion: Towards Photo-realistic 3D Generative Modeling",
        "abstract": "Diffusion-based image generators can now produce high-quality and diverse samples, but their success has yet to fully translate to 3D generation: existing diffusion methods can either generate low-resolution but 3D consistent outputs, or detailed 2D views of 3D objects but with potential structural defects and lacking view consistency or realism. We present HoloFusion, a method that combines the best of these approaches to produce high-fidelity, plausible, and diverse 3D samples while learning from a collection of multi-view 2D images only. The method first generates coarse 3D samples using a variant of the recently proposed HoloDiffusion generator. Then, it independently renders and upsamples a large number of views of the coarse 3D model, super-resolves them to add detail, and distills those into a single, high-fidelity implicit 3D representation, which also ensures view-consistency of the final renders. The super-resolution network is trained as an integral part of HoloFusion, end-to-end, and the final distillation uses a new sampling scheme to capture the space of super-resolved signals. We compare our method against existing baselines, including DreamFusion, Get3D, EG3D, and HoloDiffusion, and achieve, to the best of our knowledge, the most realistic results on the challenging CO3Dv2 dataset.",
        "authors": [
            "Animesh Karnewar",
            "N. Mitra",
            "A. Vedaldi",
            "David Novotný"
        ],
        "citations": 26,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning",
        "abstract": "Recently, CLIP-guided image synthesis has shown appealing performance on adapting a pre-trained source-domain generator to an unseen target domain. It does not require any target-domain samples but only the textual domain labels. The training is highly efficient, e.g., a few minutes. However, existing methods still have some limitations in the quality of generated images and may suffer from the mode collapse issue. A key reason is that a fixed adaptation direction is applied for all cross-domain image pairs, which leads to identical supervision signals. To address this issue, we propose an Image-specific Prompt Learning (IPL) method, which learns specific prompt vectors for each source-domain image. This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility. Qualitative and quantitative evaluations on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse. Moreover, IPL is independent of the structure of the generative model, such as generative adversarial networks or diffusion models. Code is available at https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation.",
        "authors": [
            "Jiayi Guo",
            "Chaofei Wang",
            "You Wu",
            "Eric Zhang",
            "Kai Wang",
            "Xingqian Xu",
            "S. Song",
            "Humphrey Shi",
            "Gao Huang"
        ],
        "citations": 25,
        "references": 66,
        "year": 2023
    },
    {
        "title": "A Survey on Large Language Models for Recommendation",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, https://github.com/WLiK/LLM4Rec.",
        "authors": [
            "Likang Wu",
            "Zhilan Zheng",
            "Zhaopeng Qiu",
            "Hao Wang",
            "Hongchao Gu",
            "Tingjia Shen",
            "Chuan Qin",
            "Chen Zhu",
            "Hengshu Zhu",
            "Qi Liu",
            "Hui Xiong",
            "Enhong Chen"
        ],
        "citations": 232,
        "references": 139,
        "year": 2023
    },
    {
        "title": "The Multiclass Fault Diagnosis of Wind Turbine Bearing Based on Multisource Signal Fusion and Deep Learning Generative Model",
        "abstract": "Low fault diagnosis accuracy in case of insufficient and imbalanced samples is a major problem in the wind turbine fault diagnosis. The imbalance of samples refers to the large difference in the number of samples of different categories or the lack of a certain fault sample, which requires good learning of the characteristics of a small number of samples. Sample generation in the deep learning generation model can effectively solve this problem. In this study, we proposed a novel multiclass wind turbine bearing fault diagnosis strategy based on the conditional variational generative adversarial network (CVAE-GAN) model combining multisource signals fusion. This strategy converts multisource 1-D vibration signals into 2-D signals, and the multisource 2-D signals were fused by using wavelet transform. The CVAE-GAN model was developed by merging the variational autoencoder (VAE) with the generative adversarial network (GAN). The VAE encoder was introduced as the front end of the GAN generator. The sample label was introduced as the model input to improve the model’s training efficiency. Finally, the sample set was used to train encoder, generator, and discriminator in the CVAE-GAN model to supplement the number of the fault samples. In the classifier, the sample set is used to do experimental analysis under various sample circumstances. The results show that the proposed strategy can increase wind turbine bearing fault diagnostic accuracy in complex scenarios.",
        "authors": [
            "Liang Zhang",
            "Hao Zhang",
            "G. Cai"
        ],
        "citations": 120,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Application of generative adversarial networks (GAN) for ophthalmology image domains: a survey",
        "abstract": null,
        "authors": [
            "Aram You",
            "Jin Kuk Kim",
            "I. Ryu",
            "T. Yoo"
        ],
        "citations": 118,
        "references": 109,
        "year": 2022
    },
    {
        "title": "Convergence for score-based generative modeling with polynomial complexity",
        "abstract": "Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density $p$ given a score estimate (an estimate of $\\nabla \\ln p$) that is accurate in $L^2(p)$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using annealing to obtain a warm start at each step. Moreover, we show that a predictor-corrector algorithm gives better convergence than using either portion alone.",
        "authors": [
            "Holden Lee",
            "Jianfeng Lu",
            "Yixin Tan"
        ],
        "citations": 110,
        "references": 43,
        "year": 2022
    },
    {
        "title": "Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars",
        "abstract": "3D-aware generative adversarial networks (GANs) synthesize high-fidelity and multi-view-consistent facial images using only collections of single-view 2D imagery. Towards fine-grained control over facial attributes, recent efforts incorporate 3D Morphable Face Model (3DMM) to describe deformation in generative radiance fields either explicitly or implicitly. Explicit methods provide fine-grained expression control but cannot handle topological changes caused by hair and accessories, while implicit ones can model varied topologies but have limited generalization caused by the unconstrained deformation fields. We propose a novel 3D GAN framework for unsupervised learning of generative, high-quality and 3D-consistent facial avatars from unstructured 2D images. To achieve both deformation accuracy and topological flexibility, we propose a 3D representation called Generative Texture-Rasterized Tri-planes. The proposed representation learns Generative Neural Textures on top of parametric mesh templates and then projects them into three orthogonal-viewed feature planes through rasterization, forming a tri-plane feature representation for volume rendering. In this way, we combine both fine-grained expression control of mesh-guided explicit deformation and the flexibility of implicit volumetric representation. We further propose specific modules for modeling mouth interior which is not taken into account by 3DMM. Our method demonstrates state-of-the-art 3D-aware synthesis quality and animation ability through extensive experiments. Furthermore, serving as 3D prior, our animatable 3D representation boosts multiple applications including one-shot facial avatars and 3D-aware stylization. Project page: https://mrtornado24.github.io/Next3D/. Code: https://github.com/MrTornado24/Next3D.",
        "authors": [
            "Jingxiang Sun",
            "Xuanxia Wang",
            "Lizhen Wang",
            "Xiaoyu Li",
            "Yong Zhang",
            "Hongwen Zhang",
            "Yebin Liu"
        ],
        "citations": 97,
        "references": 88,
        "year": 2022
    },
    {
        "title": "GRiT: A Generative Region-to-text Transformer for Object Understanding",
        "abstract": "This paper presents a Generative RegIon-to-Text transformer, GRiT, for object understanding. The spirit of GRiT is to formulate object understanding aspairs, where region locates objects and text describes objects. For example, the text in object detection denotes class names while that in dense captioning refers to descriptive sentences. Specifically, GRiT consists of a visual encoder to extract image features, a foreground object extractor to localize objects, and a text decoder to generate open-set object descriptions. With the same model architecture, GRiT can understand objects via not only simple nouns, but also rich descriptive sentences including object attributes or actions. Experimentally, we apply GRiT to object detection and dense captioning tasks. GRiT achieves 60.4 AP on COCO 2017 test-dev for object detection and 15.5 mAP on Visual Genome for dense captioning. Code is available at https://github.com/JialianW/GRiT",
        "authors": [
            "Jialian Wu",
            "Jianfeng Wang",
            "Zhengyuan Yang",
            "Zhe Gan",
            "Zicheng Liu",
            "Junsong Yuan",
            "Lijuan Wang"
        ],
        "citations": 94,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage",
        "abstract": "Federated Learning (FL) framework brings privacy benefits to distributed learning systems by allowing multiple clients to participate in a learning task under the coordination of a central server without exchanging their private data. However, recent studies have revealed that private information can still be leaked through shared gradient information. To further protect user's privacy, several defense mechanisms have been proposed to prevent privacy leakage via gradient information degradation methods, such as using additive noise or gradient compression before sharing it with the server. In this work, we validate that the private training data can still be leaked under certain defense settings with a new type of leakage, i.e., Generative Gradient Leakage (GGL). Unlike existing methods that only rely on gradient information to reconstruct data, our method leverages the latent space of generative adversarial networks (GAN) learned from public image datasets as a prior to compensate for the informational loss during gradient degradation. To address the nonlinearity caused by the gradient operator and the GAN model, we explore various gradient-free optimization methods (e.g., evolution strategies and Bayesian optimization) and empirically show their superiority in reconstructing high-quality images from gradients compared to gradient-based optimizers. We hope the proposed method can serve as a tool for empirically measuring the amount of privacy leakage to facilitate the design of more robust defense mechanisms.",
        "authors": [
            "Zhuohang Li",
            "Jiaxin Zhang",
            "Lu Liu",
            "Jian Liu"
        ],
        "citations": 94,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Dynamic Prefix-Tuning for Generative Template-based Event Extraction",
        "abstract": "We consider event extraction in a generative manner with template-based conditional generation.Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information.In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DynPref) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context.Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE.Additionally, our model is proven to be portable to new types of events effectively.",
        "authors": [
            "Xiao Liu",
            "Heyan Huang",
            "Ge Shi",
            "Bo Wang"
        ],
        "citations": 88,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Generative Flow Networks for Discrete Probabilistic Modeling",
        "abstract": "We present energy-based generative flow networks (EB-GFN), a novel probabilistic modeling algorithm for high-dimensional discrete data. Building upon the theory of generative flow networks (GFlowNets), we model the generation process by a stochastic data construction policy and thus amortize expensive MCMC exploration into a fixed number of actions sampled from a GFlowNet. We show how GFlowNets can approximately perform large-block Gibbs sampling to mix between modes. We propose a framework to jointly train a GFlowNet with an energy function, so that the GFlowNet learns to sample from the energy distribution, while the energy learns with an approximate MLE objective with negative samples from the GFlowNet. We demonstrate EB-GFN's effectiveness on various probabilistic modeling tasks. Code is publicly available at https://github.com/zdhNarsil/EB_GFN.",
        "authors": [
            "Dinghuai Zhang",
            "Nikolay Malkin",
            "Z. Liu",
            "Alexandra Volokhova",
            "Aaron C. Courville",
            "Y. Bengio"
        ],
        "citations": 88,
        "references": 61,
        "year": 2022
    },
    {
        "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack",
        "abstract": "Training text-to-image models with web scale image-text pairs enables the generation of a wide range of visual concepts from text. However, these pre-trained models often face challenges when it comes to generating highly aesthetic images. This creates the need for aesthetic alignment post pre-training. In this paper, we propose quality-tuning to effectively guide a pre-trained model to exclusively generate highly visually appealing images, while maintaining generality across visual concepts. Our key insight is that supervised fine-tuning with a set of surprisingly small but extremely visually appealing images can significantly improve the generation quality. We pre-train a latent diffusion model on $1.1$ billion image-text pairs and fine-tune it with only a few thousand carefully selected high-quality images. The resulting model, Emu, achieves a win rate of $82.9\\%$ compared with its pre-trained only counterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred $68.4\\%$ and $71.3\\%$ of the time on visual appeal on the standard PartiPrompts and our Open User Input benchmark based on the real-world usage of text-to-image models. In addition, we show that quality-tuning is a generic approach that is also effective for other architectures, including pixel diffusion and masked generative transformer models.",
        "authors": [
            "Xiaoliang Dai",
            "Ji Hou",
            "Chih-Yao Ma",
            "Sam S. Tsai",
            "Jialiang Wang",
            "Rui Wang",
            "Peizhao Zhang",
            "Simon Vandenhende",
            "Xiaofang Wang",
            "Abhimanyu Dubey",
            "Matthew Yu",
            "Abhishek Kadian",
            "Filip Radenovic",
            "D. Mahajan",
            "Kunpeng Li",
            "Yue Zhao",
            "Vladan Petrovic",
            "Mitesh Kumar Singh",
            "Simran Motwani",
            "Yiqian Wen",
            "Yi-Zhe Song",
            "Roshan Sumbaly",
            "Vignesh Ramanathan",
            "Zijian He",
            "Péter Vajda",
            "Devi Parikh"
        ],
        "citations": 157,
        "references": 36,
        "year": 2023
    },
    {
        "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
        "abstract": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.",
        "authors": [
            "A. Maharana",
            "Mohit Bansal"
        ],
        "citations": 87,
        "references": 85,
        "year": 2022
    },
    {
        "title": "TTS-GAN: A Transformer-based Time-Series Generative Adversarial Network",
        "abstract": null,
        "authors": [
            "Xiaomin Li",
            "V. Metsis",
            "Huan Wang",
            "Anne H. H. Ngu"
        ],
        "citations": 81,
        "references": 23,
        "year": 2022
    },
    {
        "title": "Channel Attention Image Steganography With Generative Adversarial Networks",
        "abstract": "Recently, extensive research has revealed the enormous potential of deep learning in the application of image steganography. However, some defects still exist in previous studies on deep learning-based steganography. In this paper, we propose a novel end-to-end network architecture for image steganography with channel attention mechanisms based on generative adversarial networks, which can yield perceptually indistinguishable stego images at various capacities. Three subnetworks constitute our model, where a generator embeds the payload into cover images, an extractor extracts it from stego images, and a powerful steganalyzer acts as a discriminator to enhance steganographic security. We design a specific channel attention module, which tunes channel-wise features in the deep representation of images dynamically by exploiting channel interdependencies. The experimental results demonstrate that the channel attention strategy is conducive to improving the quality of generated stego images and the accuracy of message extraction. To tackle the inevitable issue of extraction errors, we resort to error correction codes, with which our model achieves the maximum effective embedding rates over 4 bits per pixel. Finally, we verify that the proposed model outperforms current GAN-based steganographic schemes on two datasets and the undetectability is superior to traditional algorithms when the steganalyst cannot access model hyperparameters.",
        "authors": [
            "Jin-Lan Tan",
            "Xin Liao",
            "Jiate Liu",
            "Yun Cao",
            "Hongbo Jiang"
        ],
        "citations": 78,
        "references": 0,
        "year": 2022
    },
    {
        "title": "gDNA: Towards Generative Detailed Neural Avatars",
        "abstract": "To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is chal-lenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochas-tic geometric detail in clothing. Hence, current methods that represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skin-ning weights. Specifically, we devise a multi-subject forward skinning module that is learned from only a few posed, unrigged scans per subject. To capture the stochastic nature of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evi-dence that this leads to realistic generation of local details such as wrinkles. We show that our model is able to gen-erate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of fitting human models to raw scans, out-performing the previous state-of-the-art.",
        "authors": [
            "Xu Chen",
            "Tianjian Jiang",
            "Jie Song",
            "Jinlong Yang",
            "Michael J. Black",
            "Andreas Geiger",
            "Otmar Hilliges"
        ],
        "citations": 78,
        "references": 64,
        "year": 2022
    },
    {
        "title": "Self-Supervised Attentive Generative Adversarial Networks for Video Anomaly Detection",
        "abstract": "Video anomaly detection (VAD) refers to the discrimination of unexpected events in videos. The deep generative model (DGM)-based method learns the regular patterns on normal videos and expects the learned model to yield larger generative errors for abnormal frames. However, DGM cannot always do so, since it usually captures the shared patterns between normal and abnormal events, which results in similar generative errors for them. In this article, we propose a novel self-supervised framework for unsupervised VAD to tackle the above-mentioned problem. To this end, we design a novel self-supervised attentive generative adversarial network (SSAGAN), which is composed of the self-attentive predictor, the vanilla discriminator, and the self-supervised discriminator. On the one hand, the self-attentive predictor can capture the long-term dependences for improving the prediction qualities of normal frames. On the other hand, the predicted frames are fed to the vanilla discriminator and self-supervised discriminator for performing true–false discrimination and self-supervised rotation detection, respectively. Essentially, the role of the self-supervised task is to enable the predictor to encode semantic information into the predicted normal frames via adversarial training, in order for the angles of rotated normal frames can be detected. As a result, our self-supervised framework lessens the generalization ability of the model to abnormal frames, resulting in larger detection errors for abnormal frames. Extensive experimental results indicate that SSAGAN outperforms other state-of-the-art methods, which demonstrates the validity and advancement of SSAGAN.",
        "authors": [
            "Chao Huang",
            "Jie Wen",
            "Yong Xu",
            "Qiuping Jiang",
            "Jian Yang",
            "Yaowei Wang",
            "David Zhang"
        ],
        "citations": 80,
        "references": 88,
        "year": 2022
    },
    {
        "title": "AvatarGen: a 3D Generative Model for Animatable Human Avatars",
        "abstract": "Unsupervised generation of 3D-aware clothed humans with various appearances and controllable geometries is important for creating virtual human avatars and other AR/VR applications. Existing methods are either limited to rigid object modeling, or not generative and thus unable to generate high-quality virtual humans and animate them. In this work, we propose AvatarGen, the first method that enables not only geometry-aware clothed human synthesis with high-fidelity appearances but also disentangled human animation controllability, while only requiring 2D images for training. Specifically, we decompose the generative 3D human synthesis into pose-guided mapping and canonical representation with predefined human pose and shape, such that the canonical representation can be explicitly driven to different poses and shapes with the guidance of a 3D parametric human model SMPL. AvatarGen further introduces a deformation network to learn non-rigid deformations for modeling fine-grained geometric details and pose-dependent dynamics. To improve the geometry quality of the generated human avatars, it leverages the signed distance field as geometric proxy, which allows more direct regularization from the 3D geometric priors of SMPL. Benefiting from these designs, our method can generate animatable 3D human avatars with high-quality appearance and geometry modeling, significantly outperforming previous 3D GANs. Furthermore, it is competent for many applications, e.g., single-view reconstruction, re-animation, and text-guided synthesis/editing. Code and pre-trained model will be available at http://jeff95.me/projects/avatargen.html.",
        "authors": [
            "Jianfeng Zhang",
            "Zihang Jiang",
            "Dingdong Yang",
            "Hongyi Xu",
            "Yichun Shi",
            "Guoxian Song",
            "Zhongcong Xu",
            "Xinchao Wang",
            "Jiashi Feng"
        ],
        "citations": 73,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Concealed Attack for Robust Watermarking Based on Generative Model and Perceptual Loss",
        "abstract": "While existing watermarking attack methods can disturb the correct extraction of watermark information, the visual quality of watermarked images will be greatly damaged. Therefore, a concealed attack based on generative adversarial network and perceptual losses for robust watermarking is proposed. First, the watermarked image is utilized as the input of generative networks, and its generating target (i.e. attacked watermarked image) is the original image. Inspired by the U-Net network, the generative networks consist of encoder-decoder architecture with skip connection, which can combine the low-level and high-level information to ensure the imperceptibility of the generated image. Next, to further improve the imperceptibility of the generated image, instead of the loss function based on MSE, a perceptual loss based on feature extraction is introduced. In addition, a discriminative network is also introduced to make the appearance and distribution of generated image similar to those of the original image. The addition of the discriminative network can remove watermark information effectively. Extensive experiments are conducted to verify the feasibility of the proposed concealed attack method. Experimental and analysis results demonstrate that the proposed concealed attack method has better imperceptibility and attack ability in comparison to the existing watermarking attack methods.",
        "authors": [
            "Qi Li",
            "Xingyuan Wang",
            "Bin Ma",
            "Xiaoyu Wang",
            "Chun-peng Wang",
            "Suo Gao",
            "Yunqing Shi"
        ],
        "citations": 71,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Physics guided deep learning for generative design of crystal materials with symmetry constraints",
        "abstract": null,
        "authors": [
            "Yong Zhao",
            "E. Siriwardane",
            "Zhenyao Wu",
            "Nihang Fu",
            "Mohammed Al-fahdi",
            "Ming Hu",
            "Jianjun Hu"
        ],
        "citations": 52,
        "references": 47,
        "year": 2022
    },
    {
        "title": "DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-aware Scene Synthesis",
        "abstract": "Existing 3D-aware image synthesis approaches mainly focus on generating a single canonical object and show limited capacity in composing a complex scene containing a variety of objects. This work presents DisCoScene: a 3D-aware generative model for high-quality and controllable scene synthesis. The key ingredient of our method is a very abstract object-level representation (i.e., 3D bounding boxes without semantic annotation) as the scene layout prior, which is simple to obtain, general to describe various scene contents, and yet informative to disentangle objects and background. Moreover, it serves as an intuitive user control for scene editing. Based on such a prior, the proposed model spatially disentangles the whole scene into object-centric generative radiance fields by learning on only 2D images with the global-local discrimination. Our model obtains the generation fidelity and editing flexibility of individual objects while being able to efficiently compose objects and the background into a complete scene. We demonstrate state-of-the-art performance on many scene datasets, including the challenging Waymo outdoor dataset. Project page can be found here.",
        "authors": [
            "Yinghao Xu",
            "Menglei Chai",
            "Zifan Shi",
            "Sida Peng",
            "Ivan Skorokhodov",
            "Aliaksandr Siarohin",
            "Ceyuan Yang",
            "Yujun Shen",
            "Hsin-Ying Lee",
            "Bolei Zhou",
            "S. Tulyakov"
        ],
        "citations": 53,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Memristor-Based Hyperchaotic Maps and Application in Auxiliary Classifier Generative Adversarial Nets",
        "abstract": "With the nonlinearity and plasticity, memristors are widely used as nonlinear devices for chaotic oscillations or as biological synapses for neuromorphic computations. But discrete memristors (DMs) and their coupling maps have not received much attention, yet. Using a DM model, this article presents a general three-dimensional discrete memristor-based (3-D-DM) map model. By coupling the DM with four 2-D discrete maps, four examples of 3-D-DM maps with no or infinitely many fixed points are generated. We simulate the coupling coefficient-depended and memristor initial-boosted bifurcation behaviors of these 3-D-DM maps using numerical measures. The results demonstrate that the memristor can enhance the chaos complexity of existing discrete maps and its coupling maps can display hyperchaos. Furthermore, a hardware platform is developed to implement the 3-D-DM maps and the acquired hyperchaotic sequences have high randomness. Particularly, these hyperchaotic sequences can be applied to the auxiliary classifier generative adversarial nets for greatly improving the discriminator accuracy.",
        "authors": [
            "H. Bao",
            "Zhongyun Hua",
            "Houzhen Li",
            "Mo Chen",
            "Bo-cheng Bao"
        ],
        "citations": 55,
        "references": 38,
        "year": 2022
    },
    {
        "title": "Deep generative modeling of transcriptional dynamics for RNA velocity analysis in single cells",
        "abstract": null,
        "authors": [
            "Adam Gayoso",
            "P. Weiler",
            "M. Lotfollahi",
            "Dominik K. Klein",
            "Justin Hong",
            "Aaron M. Streets",
            "F. Theis",
            "N. Yosef"
        ],
        "citations": 49,
        "references": 59,
        "year": 2022
    },
    {
        "title": "PerFED-GAN: Personalized Federated Learning via Generative Adversarial Networks",
        "abstract": "Federated learning is gaining popularity as a distributed machine learning method that can be used to deploy AI-dependent Internet of Things applications while protecting client data privacy and security. Due to the differences of clients, a single global model may not perform well on all clients, so the personalized federated learning method, which trains a personalized model for each client that better suits its individual needs, becomes a research hotspot. Most personalized federated learning research, however, focuses on data heterogeneity while ignoring the need for model architecture heterogeneity. Most existing federated learning methods uniformly set the model architecture of all clients participating in federated learning, which is inconvenient for each client’s individual model and local data distribution requirements, and also increases the risk of client model leakage. This article proposes a federated learning method based on co-training and generative adversarial networks (GANs) that allows each client to design its own model to participate in federated learning training independently without sharing any model architecture or parameter information with other clients or a center. In our experiments, the proposed method outperforms the existing methods in mean test accuracy by 42% when the client’s model architecture and data distribution vary significantly.",
        "authors": [
            "Xingjian Cao",
            "Gang Sun",
            "Hongfang Yu",
            "M. Guizani"
        ],
        "citations": 47,
        "references": 41,
        "year": 2022
    },
    {
        "title": "Secret-to-Image Reversible Transformation for Generative Steganography",
        "abstract": "Recently, generative steganography that transforms secret information to a generated image has been a promising technique to resist steganalysis detection. However, due to the inefficiency and irreversibility of the secret-to-image transformation, it is hard to find a good trade-off between the information hiding capacity and extraction accuracy. To address this issue, we propose a secret-to-image reversible transformation (S2IRT) scheme for generative steganography. The proposed S2IRT scheme is based on a generative model, i.e., Glow model, which enables a bijective-mapping between latent space with multivariate Gaussian distribution and image space with a complex distribution. In the process of S2I transformation, guided by a given secret message, we construct a latent vector and then map it to a generated image by the Glow model, so that the secret message is finally transformed to the generated image. Owing to good efficiency and reversibility of S2IRT scheme, the proposed steganographic approach achieves both high hiding capacity and accurate extraction of secret message from generated image. Furthermore, a separate encoding-based S2IRT (SE-S2IRT) scheme is also proposed to improve the robustness to common image attacks. The experiments demonstrate the proposed steganographic approaches can achieve high hiding capacity (up to 4 bpp) and accurate information extraction (almost 100% accuracy rate) simultaneously, while maintaining desirable anti-detectability and imperceptibility.",
        "authors": [
            "Zhili Zhou",
            "Yuecheng Su",
            "Jin Li",
            "K. Yu",
            "Q. M. Jonathan Wu",
            "Zhangjie Fu",
            "Yunqing Shi"
        ],
        "citations": 40,
        "references": 53,
        "year": 2022
    },
    {
        "title": "De-Bias for Generative Extraction in Unified NER Task",
        "abstract": "Named entity recognition (NER) is a fundamental task to recognize specific types of entities from a given sentence. Depending on how the entities appear in the sentence, it can be divided into three subtasks, namely, Flat NER, Nested NER, and Discontinuous NER. Among the existing approaches, only the generative model can be uniformly adapted to these three subtasks. However, when the generative model is applied to NER, its optimization objective is not consistent with the task, which makes the model vulnerable to the incorrect biases. In this paper, we analyze the incorrect biases in the generation process from a causality perspective and attribute them to two confounders: pre-context confounder and entity-order confounder. Furthermore, we design Intra- and Inter-entity Deconfounding Data Augmentation methods to eliminate the above confounders according to the theory of backdoor adjustment. Experiments show that our method can improve the performance of the generative NER model in various datasets.",
        "authors": [
            "Shuai Zhang",
            "Yongliang Shen",
            "Zeqi Tan",
            "Yiquan Wu",
            "Weiming Lu"
        ],
        "citations": 42,
        "references": 56,
        "year": 2022
    },
    {
        "title": "VL-BEiT: Generative Vision-Language Pretraining",
        "abstract": "We introduce a vision-language foundation model called VL-BEiT, which is a bidirectional multimodal Transformer learned by generative pretraining. Our minimalist solution conducts masked prediction on both monomodal and multimodal data with a shared Transformer. Specifically, we perform masked vision-language modeling on image-text pairs, masked language modeling on texts, and masked image modeling on images. VL-BEiT is learned from scratch with one unified pretraining task, one shared backbone, and one-stage training. Our method is conceptually simple and empirically effective. Experimental results show that VL-BEiT obtains strong results on various vision-language benchmarks, such as visual question answering, visual reasoning, and image-text retrieval. Moreover, our method learns transferable visual features, achieving competitive performance on image classification, and semantic segmentation.",
        "authors": [
            "Hangbo Bao",
            "Wenhui Wang",
            "Li Dong",
            "Furu Wei"
        ],
        "citations": 43,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Voids Filling of DEM with Multiattention Generative Adversarial Network Model",
        "abstract": "The digital elevation model (DEM) acquired through photogrammetry or LiDAR usually exposes voids due to phenomena such as instrumentation artifact, ground occlusion, etc. For this reason, this paper proposes a multiattention generative adversarial network model to fill the voids. In this model, a multiscale feature fusion generation network is proposed to initially fill the voids, and then a multiattention filling network is proposed to recover the detailed features of the terrain surrounding the void area, and the channel-spatial cropping attention mechanism module is proposed as an enhancement of the network. Spectral normalization is added to each convolution layer in the discriminator network. Finally, the training of the model by a combined loss function, including reconstruction loss and adversarial loss, is optimized. Three groups of experiments with four different types of terrains, hillsides, valleys, ridges and hills, are conducted for validation of the proposed model. The experimental results show that (1) the structural similarity surrounding terrestrial voids in the three types of terrains (i.e., hillside, valley, and ridge) can reach 80–90%, which implies that the DEM accuracy can be improved by at least 10% relative to the traditional interpolation methods (i.e., Kriging, IDW, and Spline), and can reach 57.4%, while other deep learning models (i.e., CE, GL and CR) only reach 43.2%, 17.1% and 11.4% in the hilly areas, respectively. Therefore, it can be concluded that the structural similarity surrounding the terrestrial voids filled using the model proposed in this paper can reach 60–90% upon the types of terrain, such as hillside, valley, ridge, and hill.",
        "authors": [
            "Guoqing Zhou",
            "B. Song",
            "P. Liang",
            "Jiasheng Xu",
            "Tao Yue"
        ],
        "citations": 40,
        "references": 35,
        "year": 2022
    },
    {
        "title": "Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative",
        "abstract": "This paper targets at improving the generalizability of hypergraph neural networks in the low-label regime, through applying the contrastive learning approach from images/graphs (we refer to it as HyperGCL). We focus on the following question: How to construct contrastive views for hypergraphs via augmentations? We provide the solutions in two folds. First, guided by domain knowledge, we fabricate two schemes to augment hyperedges with higher-order relations encoded, and adopt three vertex augmentation strategies from graph-structured data. Second, in search of more effective views in a data-driven manner, we for the first time propose a hypergraph generative model to generate augmented views, and then an end-to-end differentiable pipeline to jointly learn hypergraph augmentations and model parameters. Our technical innovations are reflected in designing both fabricated and generative augmentations of hypergraphs. The experimental findings include: (i) Among fabricated augmentations in HyperGCL, augmenting hyperedges provides the most numerical gains, implying that higher-order information in structures is usually more downstream-relevant; (ii) Generative augmentations do better in preserving higher-order information to further benefit generalizability; (iii) HyperGCL also boosts robustness and fairness in hypergraph representation learning. Codes are released at https://github.com/weitianxin/HyperGCL.",
        "authors": [
            "Tianxin Wei",
            "Yuning You",
            "Tianlong Chen",
            "Yang Shen",
            "Jingrui He",
            "Zhangyang Wang"
        ],
        "citations": 36,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Review of Generative Adversarial Networks in Image Generation",
        "abstract": "Generative adversarial network (GAN) model generates and discriminates images using an adversarial competitive strategy to generate high-quality images. The implementation of GAN in different fields is helpful for generating samples that are not easy to obtain. Image generation can help machine learning to balance data and improve the accuracy of the classifier. This paper introduces the principles of the GAN model and analyzes the advantages and disadvantages of improving GANs. The applications of GANs in image generation are analyzed. Finally, the problems of GANs in image generation are summarized.",
        "authors": [
            "Wanle Chi",
            "Yun-Huoy Choo",
            "O. Goh"
        ],
        "citations": 36,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds",
        "abstract": null,
        "authors": [
            "M. Korshunova",
            "Niles Huang",
            "Stephen J. Capuzzi",
            "D. Radchenko",
            "Olena V. Savych",
            "Yuriy S. Moroz",
            "C. Wells",
            "T. Willson",
            "A. Tropsha",
            "O. Isayev"
        ],
        "citations": 38,
        "references": 53,
        "year": 2022
    },
    {
        "title": "GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks",
        "abstract": "Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation. The official implementation can be found at https://github.com/yolandalalala/GNNInterpreter.",
        "authors": [
            "Xiaoqi Wang",
            "Hang Shen"
        ],
        "citations": 33,
        "references": 43,
        "year": 2022
    },
    {
        "title": "Diffusion-Based Generative Speech Source Separation",
        "abstract": "We propose DiffSep, a new single channel source separation method based on score-matching of a stochastic differential equation (SDE). We craft a tailored continuous time diffusion-mixing process starting from the separated sources and converging to a Gaussian distribution centered on their mixture. This formulation lets us apply the machinery of score-based generative modelling. First, we train a neural network to approximate the score function of the marginal probabilities of the diffusion-mixing process. Then, we use it to solve the reverse time SDE that progressively separates the sources starting from their mixture. We propose a modified training strategy to handle model mismatch and source permutation ambiguity. Experiments on the WSJ0_2mix dataset demonstrate the potential of the method. Furthermore, the method is also suitable for speech enhancement and shows performance competitive with prior work on the VoiceBank-DEMAND dataset.",
        "authors": [
            "Robin Scheibler",
            "Youna Ji",
            "Soo-Whan Chung",
            "J. Byun",
            "Soyeon Choe",
            "Min-Seok Choi"
        ],
        "citations": 32,
        "references": 43,
        "year": 2022
    },
    {
        "title": "High-throughput property-driven generative design of functional organic molecules",
        "abstract": null,
        "authors": [
            "Julia Westermayr",
            "Joe Gilkes",
            "Rhyan Barrett",
            "R. Maurer"
        ],
        "citations": 34,
        "references": 80,
        "year": 2022
    },
    {
        "title": "Generative aptamer discovery using RaptGen",
        "abstract": null,
        "authors": [
            "Natsuki Iwano",
            "Tatsuo Adachi",
            "Kazuteru Aoki",
            "Yoshikazu Nakamura",
            "M. Hamada"
        ],
        "citations": 34,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Generative Model based Highly Efficient Semantic Communication Approach for Image Transmission",
        "abstract": "Deep learning (DL) based semantic communication methods have been explored to transmit images efficiently in recent years. In this paper, we propose a generative model based semantic communication to further improve the efficiency of image transmission and protect private information. In particular, the transmitter extracts the interpretable latent representation from the original image by a generative model exploiting the GAN inversion method. We also employ a privacy filter and a knowledge base to erase private information and replace it with natural features in the knowledge base. The simulation results indicate that our proposed method achieves comparable quality of received images while significantly reducing communication costs compared to the existing methods.",
        "authors": [
            "Tian Han",
            "Jiancheng Tang",
            "Qianqian Yang",
            "Yiping Duan",
            "Zhaoyang Zhang",
            "Zhiguo Shi"
        ],
        "citations": 30,
        "references": 20,
        "year": 2022
    },
    {
        "title": "Design of potent antimalarials with generative chemistry",
        "abstract": null,
        "authors": [
            "William J. Godinez",
            "Eric J. Ma",
            "Alexander T. Chao",
            "Luying Pei",
            "Peter Skewes-Cox",
            "Stephen M. Canham",
            "J. Jenkins",
            "Joseph M. Young",
            "E. Martin",
            "Armand Guiguemde"
        ],
        "citations": 30,
        "references": 32,
        "year": 2022
    },
    {
        "title": "Generative Adversarial Neural Operators",
        "abstract": "We propose the generative adversarial neural operator (GANO), a generative model paradigm for learning probabilities on infinite-dimensional function spaces. The natural sciences and engineering are known to have many types of data that are sampled from infinite-dimensional function spaces, where classical finite-dimensional deep generative adversarial networks (GANs) may not be directly applicable. GANO generalizes the GAN framework and allows for the sampling of functions by learning push-forward operator maps in infinite-dimensional spaces. GANO consists of two main components, a generator neural operator and a discriminator neural functional. The inputs to the generator are samples of functions from a user-specified probability measure, e.g., Gaussian random field (GRF), and the generator outputs are synthetic data functions. The input to the discriminator is either a real or synthetic data function. In this work, we instantiate GANO using the Wasserstein criterion and show how the Wasserstein loss can be computed in infinite-dimensional spaces. We empirically study GANO in controlled cases where both input and output functions are samples from GRFs and compare its performance to the finite-dimensional counterpart GAN. We empirically study the efficacy of GANO on real-world function data of volcanic activities and show its superior performance over GAN.",
        "authors": [
            "Md Ashiqur Rahman",
            "Manuel A. Florez",
            "Anima Anandkumar",
            "Z. Ross",
            "K. Azizzadenesheli"
        ],
        "citations": 31,
        "references": 31,
        "year": 2022
    },
    {
        "title": "Generative Steganography Network",
        "abstract": "Steganography usually modifies cover media to embed secret data. A new steganographic approach called generative steganography (GS) has emerged recently, in which stego images (images containing secret data) are generated from secret data directly without cover media. However, existing GS schemes are often criticized for their poor performances. In this paper, we propose an advanced generative steganography network (GSN) that can generate realistic stego images without using cover images. We firstly introduce the mutual information mechanism in GS, which helps to achieve high secret extraction accuracy. Our model contains four sub-networks, i.e., an image generator (G), a discriminator (D), a steganalyzer (S), and a data extractor (E). D and S act as two adversarial discriminators to ensure the visual quality and security of generated stego images. E is to extract the hidden secret from generated stego images. The generator G is flexibly constructed to synthesize either cover or stego images with different inputs. It facilitates covert communication by concealing the function of generating stego images in a normal generator. A module named secret block is designed to hide secret data in the feature maps during image generation, with which high hiding capacity and image fidelity are achieved. In addition, a novel hierarchical gradient decay (HGD) skill is developed to resist steganalysis detection. Experiments demonstrate the superiority of our work over existing methods.",
        "authors": [
            "Ping Wei",
            "Sheng Li",
            "Xinpeng Zhang",
            "Ge Luo",
            "Zhenxing Qian",
            "Qing Zhou"
        ],
        "citations": 34,
        "references": 49,
        "year": 2022
    },
    {
        "title": "Equivalent Circuit Theory-Assisted Deep Learning for Accelerated Generative Design of Metasurfaces",
        "abstract": "In this article, we propose an equivalent circuit theory-assisted deep learning approach to accelerate the design of metasurfaces. By combining the filter equivalent circuit theory and a sophisticated deep learning model, designers can achieve efficient metasurface designs. Compared with most existing metasurface generative design methods that rely on arbitrarily generated training dataset (TDS), the proposed method can adaptively produce highly relevant and low-noise training samples under the guidance of filter equivalent circuit theory, resulting in a significantly narrowed target solution space and improved model training efficiency. Furthermore, we select the variational autoencoder (VAE) as a generative model, which can compress the raw training samples into a lower-dimensional latent space where optimization methods, such as genetic algorithm, can be more efficiently executed to find the optimal design than a brute-force search. To verify the effectiveness of the proposed method, we apply it in the creation of three examples of frequency selective surfaces (FSSs), presenting wide-band, dual-band, and band-stop responses. Experimental results show that the proposed method can realize much faster and more stable convergence than deep learning design methods without domain knowledge.",
        "authors": [
            "Zhaohui Wei",
            "Zhao Zhou",
            "Peng Wang",
            "Jian-lin Ren",
            "Ying-Zheng Yin",
            "G. Pedersen",
            "M. Shen"
        ],
        "citations": 35,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Generative Spoken Dialogue Language Modeling",
        "abstract": "We introduce dGSLM, the first “textless” model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. We show that our model is able to generate speech, laughter, and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn taking compared to a text-based cascaded model.1,2",
        "authors": [
            "Tu Nguyen",
            "E. Kharitonov",
            "Jade Copet",
            "Yossi Adi",
            "Wei-Ning Hsu",
            "A. Elkahky",
            "Paden Tomasello",
            "Robin Algayres",
            "Benoît Sagot",
            "Abdel-rahman Mohamed",
            "Emmanuel Dupoux"
        ],
        "citations": 75,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Generative Modeling in Sinogram Domain for Sparse-View CT Reconstruction",
        "abstract": "The radiation dose in computed tomography (CT) examinations is harmful for patients but can be significantly reduced by intuitively decreasing the number of projection views. Reducing projection views usually leads to severe aliasing artifacts in reconstructed images. Previous deep learning (DL) techniques with sparse-view data require sparse-view/full-view CT image pairs to train the network with supervised manners. When the number of projection view changes, the DL network should be retrained with updated sparse-view/full-view CT image pairs. To relieve this limitation, we present a fully unsupervised score-based generative model in sinogram domain for sparse-view CT reconstruction. Specifically, we first train a score-based generative model on full-view sinogram data and use multichannel strategy to form high-dimensional tensor as the network input to capture their prior distribution. Then, at the inference stage, the stochastic differential equation (SDE) solver and data-consistency step were performed iteratively to achieve full-view projection. The filtered back projection (FBP) algorithm was used to achieve the final image reconstruction. Qualitative and quantitative studies were implemented to evaluate the presented method with several CT data. Experimental results demonstrated that our method achieved comparable or better performance than the supervised learning counterparts.",
        "authors": [
            "Bing Guan",
            "Cailian Yang",
            "Liu Zhang",
            "S. Niu",
            "Minghui Zhang",
            "Yuhao Wang",
            "Weiwen Wu",
            "Qiegen Liu"
        ],
        "citations": 28,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities",
        "abstract": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.",
        "authors": [
            "Sherry Yang",
            "Ofir Nachum",
            "Yilun Du",
            "Jason Wei",
            "P. Abbeel",
            "D. Schuurmans"
        ],
        "citations": 126,
        "references": 231,
        "year": 2023
    },
    {
        "title": "Deep Generative Modeling",
        "abstract": null,
        "authors": [
            "J. Tomczak"
        ],
        "citations": 69,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Variational Autoencoder Generative Adversarial Network for Synthetic Data Generation in Smart Home",
        "abstract": "Data is the fuel of data science and machine learning techniques for smart grid applications, similar to many other fields. However, the availability of data can be an issue due to privacy concerns, data size, data quality, and so on. To this end, in this paper, we propose a Variational AutoEncoder Generative Adversarial Network (VAE-GAN) as a smart grid data generative model which is capable of learning various types of data distributions and generating plausible samples from the same distribution without performing any prior analysis on the data before the training phase. We compared the Kullback–Leibler (KL) divergence, maximum mean discrepancy (MMD), and Wasserstein distance between the synthetic data (electrical load and PV production) distribution generated by the proposed model, vanilla GAN network, and the real data distribution, to evaluate the performance of our model. Furthermore, we used five key statistical parameters to describe the smart grid data distribution and compared them between synthetic data generated by both models and real data. Experiments indicate that the proposed synthetic data generative model outperforms the vanilla GAN network. The distribution of VAE-GAN synthetic data is the most comparable to that of real data.",
        "authors": [
            "Mina Razghandi",
            "Hao Zhou",
            "M. Erol-Kantarci",
            "D. Turgut"
        ],
        "citations": 22,
        "references": 18,
        "year": 2022
    },
    {
        "title": "DDE-GAN: Integrating a Data-driven Design Evaluator into Generative Adversarial Networks for Desirable and Diverse Concept Generation",
        "abstract": "\n Generative Adversarial Networks (GANs) have shown remarkable success in various generative design tasks, from topology optimization to material design, and shape parametrization. However, most generative design approaches based on GANs lack evaluation mechanisms to ensure the generation of diverse samples. In addition, no GAN-based generative design model incorporates user sentiments in the loss function to generate samples with high desirability from the aggregate perspectives of users. Motivated by these knowledge gaps, this paper builds and validates a novel GAN-based generative design model with an offline design evaluation function to generate samples that are not only realistic, but also diverse and desirable. A multimodal Data-driven Design Evaluation (DDE) model is developed to guide the generative process by automatically predicting user sentiments for the generated samples based on large-scale user reviews of previous designs. This paper incorporates DDE into the StyleGAN structure, a state-of-the-art GAN model, to enable data-driven generative processes that are innovative and user-centered. The results of experiments conducted on a large dataset of footwear products demonstrate the effectiveness of the proposed DDE-GAN in generating high-quality, diverse, and desirable concepts.",
        "authors": [
            "Chenxi Yuan",
            "T. Marion",
            "Mohsen Moghaddam"
        ],
        "citations": 16,
        "references": 0,
        "year": 2022
    },
    {
        "title": "BIM-based graph data model for automatic generative design of modular buildings",
        "abstract": null,
        "authors": [
            "V. Gan"
        ],
        "citations": 65,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Brain Tumor Detection and Classification Using Cycle Generative Adversarial Networks",
        "abstract": null,
        "authors": [
            "R. Gupta",
            "S. Bharti",
            "Nilesh Kunhare",
            "Yatendra Sahu",
            "Nikhlesh Pathik"
        ],
        "citations": 55,
        "references": 34,
        "year": 2022
    },
    {
        "title": "ClinicalT5: A Generative Language Model for Clinical Text",
        "abstract": ",",
        "authors": [
            "Qiuhao Lu",
            "D. Dou",
            "Thien Huu Nguyen"
        ],
        "citations": 48,
        "references": 34,
        "year": 2022
    },
    {
        "title": "Coarse-to-Fine Generative Modeling for Graphic Layouts",
        "abstract": "Even though graphic layout generation has attracted growing attention recently, it is still challenging to synthesis realistic and diverse layouts, due to the complicated element relationships and varied element arrangements. In this work, we seek to improve the performance of layout generation by incorporating the concept of regions, which consist of a smaller number of elements and appears like a simple layout, into the generation process. Specifically, we leverage Variational Autoencoder (VAE) as the overall architecture and decompose the decoding process into two stages. The first stage predicts representations for regions, and the second stage fills in the detailed position for each element within the region based on the predicted region representation. Compared to prior studies that merely abstract the layout into a list of elements and generate all the element positions in one go, our approach has at least two advantages. First, by the two-stage decoding, our approach decouples the complex layout generation task into several simple layout generation tasks, which reduces the problem difficulty. Second, the predicted regions can help the model roughly know what the graphic layout looks like and serve as global context to improve the generation of detailed element positions. Qualitative and quantitative experiments demonstrate that our approach significantly outperforms the existing methods, especially on the complex graphic layouts.",
        "authors": [
            "Zhaoyun Jiang",
            "Shizhao Sun",
            "Jihua Zhu",
            "Jian-Guang Lou",
            "D. Zhang"
        ],
        "citations": 31,
        "references": 29,
        "year": 2022
    },
    {
        "title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
        "abstract": "Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations. To address these limitations, we propose TitanFuzz – the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38%/50.84% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs. This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.",
        "authors": [
            "Yinlin Deng",
            "Chun Xia",
            "Haoran Peng",
            "Chenyuan Yang",
            "Lingming Zhang"
        ],
        "citations": 146,
        "references": 93,
        "year": 2022
    },
    {
        "title": "Controllable protein design with language models",
        "abstract": null,
        "authors": [
            "Noelia Ferruz",
            "B. Höcker"
        ],
        "citations": 127,
        "references": 102,
        "year": 2022
    },
    {
        "title": "Building Face Ageing Model Using Face Synthesis",
        "abstract": "Advancements in face synthesis technology have enabled innovative methods for modeling facial aging. This research paper focuses primarily on creating a robust face aging model using deep learning and Generative Adversarial Networks (GANs), trained on a diverse dataset of facial images. The proposed approach captures both global features and local textures to produce realistic age-progressed images while preserving the subject's identity. This paper also examines face synthesis techniques, with specific emphasis for the various practical usage of GANs. The key objective of our project is to upgrade both the discriminator and the generator parts of GANs to generate more realistic, age- progressed face images. We evaluated the model using quantitative metrics and qualitative assessments, demonstrating its effectiveness. Additionally, we address ethical considerations, proposing guidelines for responsible use. Our study offers a novel framework with significant applications in security, forensics, and entertainment, and suggests future research directions to improve accuracy and ethical standards.",
        "authors": [
            "Shraddha Mishra",
            "Manvi Chahar",
            "Shivani Jaswal"
        ],
        "citations": 703,
        "references": 52,
        "year": 2024
    },
    {
        "title": "TripoSR: Fast 3D Object Reconstruction from a Single Image",
        "abstract": "This technical report introduces TripoSR, a 3D reconstruction model leveraging transformer architecture for fast feed-forward 3D generation, producing 3D mesh from a single image in under 0.5 seconds. Building upon the LRM network architecture, TripoSR integrates substantial improvements in data processing, model design, and training techniques. Evaluations on public datasets show that TripoSR exhibits superior performance, both quantitatively and qualitatively, compared to other open-source alternatives. Released under the MIT license, TripoSR is intended to empower researchers, developers, and creatives with the latest advancements in 3D generative AI.",
        "authors": [
            "Dmitry Tochilkin",
            "David Pankratz",
            "Zexiang Liu",
            "Zixuan Huang",
            "Adam Letts",
            "Yangguang Li",
            "Ding Liang",
            "Christian Laforte",
            "Varun Jampani",
            "Yan-Pei Cao"
        ],
        "citations": 79,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild",
        "abstract": "We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Lever-aging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabil-ities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each en-riched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capac-ity to manipulate restoration through textual prompts.",
        "authors": [
            "Fanghua Yu",
            "Jinjin Gu",
            "Zheyuan Li",
            "Jinfan Hu",
            "Xiangtao Kong",
            "Xintao Wang",
            "Jingwen He",
            "Yu Qiao",
            "Chao Dong"
        ],
        "citations": 67,
        "references": 103,
        "year": 2024
    },
    {
        "title": "Chatting about ChatGPT: How May AI and GPT Impact Academia and Libraries?",
        "abstract": "\nPurpose\nThis paper aims to provide an overview of key definitions related to ChatGPT, a public tool developed by OpenAI, and its underlying technology, Generative Pretrained Transformer (GPT).\n\n\nDesign/methodology/approach\nThis paper includes an interview with ChatGPT on its potential impact on academia and libraries. The interview discusses the benefits of ChatGPT such as improving search and discovery, reference and information services; cataloging and metadata generation; and content creation, as well as the ethical considerations that need to be taken into account, such as privacy and bias.\n\n\nFindings\nChatGPT has considerable power to advance academia and librarianship in both anxiety-provoking and exciting new ways. However, it is important to consider how to use this technology responsibly and ethically, and to uncover how we, as professionals, can work alongside this technology to improve our work, rather than to abuse it or allow it to abuse us in the race to create new scholarly knowledge and educate future professionals.\n\n\nOriginality/value\nThis paper discusses the history and technology of GPT, including its generative pretrained transformer model, its ability to perform a wide range of language-based tasks and how ChatGPT uses this technology to function as a sophisticated chatbot.\n",
        "authors": [
            "B. Lund",
            "Wang Ting"
        ],
        "citations": 607,
        "references": 22,
        "year": 2023
    },
    {
        "title": "Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware",
        "abstract": "Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/",
        "authors": [
            "Tony Zhao",
            "Vikash Kumar",
            "S. Levine",
            "Chelsea Finn"
        ],
        "citations": 348,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Can artificial intelligence help for scientific writing?",
        "abstract": null,
        "authors": [
            "Michele Salvagno",
            "F. Taccone",
            "A. Gerli"
        ],
        "citations": 453,
        "references": 9,
        "year": 2023
    },
    {
        "title": "Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance",
        "abstract": "In this study, we introduce a methodology for human image animation by leveraging a 3D human parametric model within a latent diffusion framework to enhance shape alignment and motion guidance in curernt human generative techniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear) model as the 3D human parametric model to establish a unified representation of body shape and pose. This facilitates the accurate capture of intricate human geometry and motion characteristics from source videos. Specifically, we incorporate rendered depth images, normal maps, and semantic maps obtained from SMPL sequences, alongside skeleton-based motion guidance, to enrich the conditions to the latent diffusion model with comprehensive 3D shape and detailed pose attributes. A multi-layer motion fusion module, integrating self-attention mechanisms, is employed to fuse the shape and motion latent representations in the spatial domain. By representing the 3D human parametric model as the motion guidance, we can perform parametric shape alignment of the human body between the reference image and the source video motion. Experimental evaluations conducted on benchmark datasets demonstrate the methodology's superior ability to generate high-quality human animations that accurately capture both pose and shape variations. Furthermore, our approach also exhibits superior generalization capabilities on the proposed in-the-wild dataset. Project page: https://fudan-generative-vision.github.io/champ.",
        "authors": [
            "Shenhao Zhu",
            "Junming Leo Chen",
            "Zuozhuo Dai",
            "Yinghui Xu",
            "Xun Cao",
            "Yao Yao",
            "Hao Zhu",
            "Siyu Zhu"
        ],
        "citations": 45,
        "references": 65,
        "year": 2024
    },
    {
        "title": "Reinforced Self-Training (ReST) for Language Modeling",
        "abstract": "Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.",
        "authors": [
            "Caglar Gulcehre",
            "T. Paine",
            "S. Srinivasan",
            "Ksenia Konyushkova",
            "L. Weerts",
            "Abhishek Sharma",
            "Aditya Siddhant",
            "Alexa Ahern",
            "Miaosen Wang",
            "Chenjie Gu",
            "Wolfgang Macherey",
            "A. Doucet",
            "Orhan Firat",
            "Nando de Freitas"
        ],
        "citations": 217,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Any-to-Any Generation via Composable Diffusion",
        "abstract": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io",
        "authors": [
            "Zineng Tang",
            "Ziyi Yang",
            "Chenguang Zhu",
            "Michael Zeng",
            "Mohit Bansal"
        ],
        "citations": 133,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
        "abstract": "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot’s visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 15 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details are available (diffusion-policy.cs.columbia.edu).",
        "authors": [
            "Cheng Chi",
            "S. Feng",
            "Yilun Du",
            "Zhenjia Xu",
            "Eric A. Cousineau",
            "B. Burchfiel",
            "Shuran Song"
        ],
        "citations": 659,
        "references": 59,
        "year": 2023
    },
    {
        "title": "DDP: Diffusion Model for Dense Visual Prediction",
        "abstract": "We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional diffusion pipeline. Our approach follows a \"noise-to-map\" generative paradigm for prediction by progressively removing noise from a random Gaussian distribution, guided by the image. The method, called DDP, efficiently extends the denoising diffusion process into the modern perception pipeline. Without task-specific design and architecture customization, DDP is easy to generalize to most dense prediction tasks, e.g., semantic segmentation and depth estimation. In addition, DDP shows attractive properties such as dynamic inference and uncertainty awareness, in contrast to previous single-step discriminative methods. We show top results on three representative tasks with six diverse benchmarks, without tricks, DDP achieves state-of-the-art or competitive performance on each task compared to the specialist counterparts. For example, semantic segmentation (83.9 mIoU on Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation (0.05 REL on KITTI). We hope that our approach will serve as a solid baseline and facilitate future research.",
        "authors": [
            "Yuanfeng Ji",
            "Zhe Chen",
            "Enze Xie",
            "Lanqing Hong",
            "Xihui Liu",
            "Zhaoqiang Liu",
            "Tong Lu",
            "Zhenguo Li",
            "P. Luo"
        ],
        "citations": 101,
        "references": 98,
        "year": 2023
    },
    {
        "title": "HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion",
        "abstract": "Implicit neural fields, typically encoded by a multilayer perceptron (MLP) that maps from coordinates (e.g., xyz) to signals (e.g., signed distances), have shown remarkable promise as a high-fidelity and compact representation. However, the lack of a regular and explicit grid structure also makes it challenging to apply generative modeling directly on implicit neural fields in order to synthesize new data. To this end, we propose HyperDiffusion, a novel approach for unconditional generative modeling of implicit neural fields. HyperDiffusion operates directly on MLP weights and generates new neural implicit fields encoded by synthesized MLP parameters. Specifically, a collection of MLPs is first optimized to faithfully represent individual data samples. Subsequently, a diffusion process is trained in this MLP weight space to model the underlying distribution of neural implicit fields. HyperDiffusion enables diffusion modeling over a implicit, compact, and yet high-fidelity representation of complex signals across 3D shapes and 4D mesh animations within one single unified framework.",
        "authors": [
            "Ziya Erkoç",
            "Fangchang Ma",
            "Qi Shan",
            "M. Nießner",
            "Angela Dai"
        ],
        "citations": 104,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Equivariant Diffusion for Molecule Generation in 3D",
        "abstract": "This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and efficiency at training time.",
        "authors": [
            "Emiel Hoogeboom",
            "Victor Garcia Satorras",
            "Clément Vignac",
            "M. Welling"
        ],
        "citations": 486,
        "references": 53,
        "year": 2022
    }
]