[
    {
        "title": "Code Llama: Open Foundation Models for Code",
        "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",
        "authors": [
            "Baptiste Rozière",
            "Jonas Gehring",
            "Fabian Gloeckle",
            "Sten Sootla",
            "Itai Gat",
            "Xiaoqing Tan",
            "Yossi Adi",
            "Jingyu Liu",
            "Tal Remez",
            "J. Rapin",
            "Artyom Kozhevnikov",
            "I. Evtimov",
            "Joanna Bitton",
            "Manish P Bhatt",
            "Cristian Cantón Ferrer",
            "Aaron Grattafiori",
            "Wenhan Xiong",
            "Alexandre D'efossez",
            "Jade Copet",
            "F. Azhar",
            "Hugo Touvron",
            "Louis Martin",
            "Nicolas Usunier",
            "Thomas Scialom",
            "Gabriel Synnaeve"
        ],
        "citations": 1000,
        "references": 92,
        "year": 2023
    },
    {
        "title": "Yi: Open Foundation Models by 01.AI",
        "abstract": "We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.",
        "authors": [
            "01.AI Alex Young",
            "Bei Chen",
            "Chao Li",
            "Chengen Huang",
            "Ge Zhang",
            "Guanwei Zhang",
            "Heng Li",
            "Jiangcheng Zhu",
            "Jianqun Chen",
            "Jing Chang",
            "Kaidong Yu",
            "Peng Liu",
            "Qiang Liu",
            "Shawn Yue",
            "Senbin Yang",
            "Shiming Yang",
            "Tao Yu",
            "Wen Xie",
            "Wenhao Huang",
            "Xiaohui Hu",
            "Xiaoyi Ren",
            "Xinyao Niu",
            "Pengcheng Nie",
            "Yuchi Xu",
            "Yudong Liu",
            "Yue Wang",
            "Yuxuan Cai",
            "Zhenyu Gu",
            "Zhiyuan Liu",
            "Zonghong Dai"
        ],
        "citations": 380,
        "references": 92,
        "year": 2024
    },
    {
        "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
        "abstract": "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models.",
        "authors": [
            "Zhe Chen",
            "Jiannan Wu",
            "Wenhai Wang",
            "Weijie Su",
            "Guo Chen",
            "Sen Xing",
            "Zhong Muyan",
            "Qinglong Zhang",
            "Xizhou Zhu",
            "Lewei Lu",
            "Bin Li",
            "Ping Luo",
            "Tong Lu",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "citations": 455,
        "references": 190,
        "year": 2023
    },
    {
        "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
        "abstract": "Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.",
        "authors": [
            "Jiahui Yu",
            "Zirui Wang",
            "Vijay Vasudevan",
            "Legg Yeung",
            "Mojtaba Seyedhosseini",
            "Yonghui Wu"
        ],
        "citations": 1000,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
        "abstract": "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}.",
        "authors": [
            "Chenfei Wu",
            "Sheng-Kai Yin",
            "Weizhen Qi",
            "Xiaodong Wang",
            "Zecheng Tang",
            "Nan Duan"
        ],
        "citations": 552,
        "references": 61,
        "year": 2023
    },
    {
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
        "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.",
        "authors": [
            "Yuzhen Huang",
            "Yuzhuo Bai",
            "Zhihao Zhu",
            "Junlei Zhang",
            "Jinghan Zhang",
            "Tangjun Su",
            "Junteng Liu",
            "Chuancheng Lv",
            "Yikai Zhang",
            "Jiayi Lei",
            "Fanchao Qi",
            "Yao Fu",
            "Maosong Sun",
            "Junxian He"
        ],
        "citations": 406,
        "references": 48,
        "year": 2023
    },
    {
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
        "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",
        "authors": [
            "Pan Lu",
            "Hritik Bansal",
            "Tony Xia",
            "Jiacheng Liu",
            "Chun-yue Li",
            "Hannaneh Hajishirzi",
            "Hao Cheng",
            "Kai-Wei Chang",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "citations": 311,
        "references": 96,
        "year": 2023
    },
    {
        "title": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics",
        "abstract": "Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learning between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, ranging from 50M up to 2.5B parameters and integrating information from 3,202 diverse human genomes, as well as 850 genomes selected across diverse phyla, including both model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the developed models can be fine-tuned at low cost and despite low available data regime to solve a variety of genomics applications. Despite no supervision, the transformer models learned to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence. Code and weights available on GitHub in Jax and HuggingFace in Pytorch. Example notebooks to apply these models to any downstream task are available on HuggingFace.",
        "authors": [
            "Hugo Dalla-torre",
            "Liam Gonzalez",
            "Javier Mendoza Revilla",
            "Nicolás López Carranza",
            "Adam Henryk Grzywaczewski",
            "Francesco Oteri",
            "Christian Dallago",
            "Evan Trop",
            "Hassan Sirelkhatim",
            "Guillaume Richard",
            "Marcin J. Skwark",
            "Karim Beguir",
            "Marie Lopez",
            "Thomas Pierrot"
        ],
        "citations": 143,
        "references": 84,
        "year": 2024
    },
    {
        "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
        "abstract": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",
        "authors": [
            "Chameleon Team"
        ],
        "citations": 118,
        "references": 60,
        "year": 2024
    },
    {
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
        "authors": [
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "M. Lachaux",
            "Timothée Lacroix",
            "Baptiste Rozière",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar",
            "Aurélien Rodriguez",
            "Armand Joulin",
            "Edouard Grave",
            "Guillaume Lample"
        ],
        "citations": 1000,
        "references": 80,
        "year": 2023
    },
    {
        "title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models",
        "abstract": "We present a modern formulation of Embodied Question Answering (EQA) as the task of understanding an environment well enough to answer questions about it in natural language. An agent can achieve such an understanding by either drawing upon episodic memory, exemplified by agents on smart glasses, or by actively exploring the environment, as in the case of mobile robots. We accompany our formulation with OpenEQA - the first open-vocabulary benchmark dataset for EQA supporting both episodic memory and active exploration use cases. OpenEQA contains over 1600 high-quality human generated questions drawn from over 180 real-world environments. In addition to the dataset, we also provide an automatic LLM-powered evaluation protocol that has excellent correlation with human judgement. Using this dataset and evaluation protocol, we evaluate several state-of-the-art foundation models including GPT-4V, and find that they significantly lag behind human-level performance. Consequently, OpenEQA stands out as a straightforward, measurable, and practically rele-vant benchmark that poses a considerable challenge to current generation offoundation models. We hope this inspires and stimulates future research at the intersection of Embod-ied AI, conversational agents, and world models.",
        "authors": [
            "Arjun Majumdar",
            "Anurag Ajay",
            "Xiaohan Zhang",
            "Pranav Putta",
            "Sriram Yenamandra",
            "Mikael Henaff",
            "Sneha Silwal",
            "Paul Mcvay",
            "Oleksandr Maksymets",
            "Sergio Arnaud",
            "Karmesh Yadav",
            "Qiyang Li",
            "Ben Newman",
            "Mohit Sharma",
            "Vincent-Pierre Berges",
            "Shiqi Zhang",
            "Pulkit Agrawal",
            "Yonatan Bisk",
            "Dhruv Batra",
            "Mrinal Kalakrishnan",
            "Franziska Meier",
            "Chris Paxton",
            "Alexander Sax",
            "A. Rajeswaran"
        ],
        "citations": 59,
        "references": 65,
        "year": 2024
    },
    {
        "title": "MOMENT: A Family of Open Time-series Foundation Models",
        "abstract": "We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on Huggingface.",
        "authors": [
            "Mononito Goswami",
            "Konrad Szafer",
            "Arjun Choudhry",
            "Yifu Cai",
            "Shuo Li",
            "Artur Dubrawski"
        ],
        "citations": 60,
        "references": 88,
        "year": 2024
    },
    {
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
        "authors": [
            "Hugo Touvron",
            "Louis Martin",
            "Kevin R. Stone",
            "Peter Albert",
            "Amjad Almahairi",
            "Yasmine Babaei",
            "Nikolay Bashlykov",
            "Soumya Batra",
            "Prajjwal Bhargava",
            "Shruti Bhosale",
            "D. Bikel",
            "Lukas Blecher",
            "Cristian Cantón Ferrer",
            "Moya Chen",
            "Guillem Cucurull",
            "David Esiobu",
            "Jude Fernandes",
            "Jeremy Fu",
            "Wenyin Fu",
            "Brian Fuller",
            "Cynthia Gao",
            "Vedanuj Goswami",
            "Naman Goyal",
            "A. Hartshorn",
            "Saghar Hosseini",
            "Rui Hou",
            "Hakan Inan",
            "Marcin Kardas",
            "Viktor Kerkez",
            "Madian Khabsa",
            "Isabel M. Kloumann",
            "A. Korenev",
            "Punit Singh Koura",
            "M. Lachaux",
            "Thibaut Lavril",
            "Jenya Lee",
            "Diana Liskovich",
            "Yinghai Lu",
            "Yuning Mao",
            "Xavier Martinet",
            "Todor Mihaylov",
            "Pushkar Mishra",
            "Igor Molybog",
            "Yixin Nie",
            "Andrew Poulton",
            "Jeremy Reizenstein",
            "Rashi Rungta",
            "Kalyan Saladi",
            "Alan Schelten",
            "Ruan Silva",
            "Eric Michael Smith",
            "R. Subramanian",
            "Xia Tan",
            "Binh Tang",
            "Ross Taylor",
            "Adina Williams",
            "Jian Xiang Kuan",
            "Puxin Xu",
            "Zhengxu Yan",
            "Iliyan Zarov",
            "Yuchen Zhang",
            "Angela Fan",
            "M. Kambadur",
            "Sharan Narang",
            "Aurélien Rodriguez",
            "Robert Stojnic",
            "Sergey Edunov",
            "Thomas Scialom"
        ],
        "citations": 1000,
        "references": 131,
        "year": 2023
    },
    {
        "title": "Movie Gen: A Cast of Media Foundation Models",
        "abstract": "We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",
        "authors": [
            "Adam Polyak",
            "Amit Zohar",
            "Andrew Brown",
            "Andros Tjandra",
            "Animesh Sinha",
            "Ann Lee",
            "Apoorv Vyas",
            "Bowen Shi",
            "Chih-Yao Ma",
            "Ching-Yao Chuang",
            "David Yan",
            "Dhruv Choudhary",
            "Dingkang Wang",
            "Geet Sethi",
            "Guan Pang",
            "Haoyu Ma",
            "Ishan Misra",
            "Ji Hou",
            "Jialiang Wang",
            "Ki-ran Jagadeesh",
            "Kunpeng Li",
            "Luxin Zhang",
            "Mannat Singh",
            "Mary Williamson",
            "Matt Le",
            "Matthew Yu",
            "Mitesh Kumar Singh",
            "Peizhao Zhang",
            "Peter Vajda",
            "Quentin Duval",
            "Rohit Girdhar",
            "Roshan Sumbaly",
            "Sai Saketh Rambhatla",
            "Sam S. Tsai",
            "S. Azadi",
            "Samyak Datta",
            "Sanyuan Chen",
            "Sean Bell",
            "Sharadh Ramaswamy",
            "Shelly Sheynin",
            "Siddharth Bhattacharya",
            "Simran Motwani",
            "Tao Xu",
            "Tianhe Li",
            "Tingbo Hou",
            "Wei-Ning Hsu",
            "Xi Yin",
            "Xiaoliang Dai",
            "Yaniv Taigman",
            "Yaqiao Luo",
            "Yen-Cheng Liu",
            "Yi-Chiao Wu",
            "Yue Zhao",
            "Yuval Kirstain",
            "Zecheng He",
            "Zijian He",
            "Albert Pumarola",
            "Ali K. Thabet",
            "A. Sanakoyeu",
            "Arun Mallya",
            "Baishan Guo",
            "Boris Araya",
            "Breena Kerr",
            "Carleigh Wood",
            "Ce Liu",
            "Cen Peng",
            "Dimitry Vengertsev",
            "Edgar Schonfeld",
            "Elliot Blanchard",
            "Felix Juefei-Xu",
            "Fraylie Nord",
            "Jeff Liang",
            "John Hoffman",
            "Jonas Kohler",
            "Kaolin Fire",
            "Karthik Sivakumar",
            "Lawrence Chen",
            "Licheng Yu",
            "Luya Gao",
            "Markos Georgopoulos",
            "Rashel Moritz",
            "Sara K. Sampson",
            "Shikai Li",
            "Simone Parmeggiani",
            "Steve Fine",
            "Tara Fowler",
            "Vladan Petrovic",
            "Yuming Du"
        ],
        "citations": 46,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Probing the 3D Awareness of Visual Foundation Models",
        "abstract": "Recent advances in large-scale pretraining have yielded visual foundation models with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and local- ize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual foundation models. We posit that 3D awareness implies that representations (1) encode the 3D structure of the scene and (2) consistently represent the surface across views. We conduct a series of experiments using task-specific probes and zero-shot inference procedures on frozen fea- tures. Our experiments reveal several limitations of the current models. Our code and analysis can be found at https://github.com/mbanani/probe3d.",
        "authors": [
            "Mohamed El Banani",
            "Amit Raj",
            "Kevis-Kokitsi Maninis",
            "Abhishek Kar",
            "Yuanzhen Li",
            "Michael Rubinstein",
            "Deqing Sun",
            "Leonidas J. Guibas",
            "Justin Johnson",
            "Varun Jampani"
        ],
        "citations": 49,
        "references": 102,
        "year": 2024
    },
    {
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
        "abstract": "We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.",
        "authors": [
            "Rui Yang",
            "Xiaoman Pan",
            "Feng Luo",
            "Shuang Qiu",
            "Han Zhong",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "citations": 40,
        "references": 73,
        "year": 2024
    },
    {
        "title": "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models",
        "abstract": "Foundation models (FMs) adapt surprisingly well to downstream tasks with fine-tuning. However, their colossal parameter space prohibits their training on resource-constrained edge-devices. For federated fine-tuning, we need to consider the smaller FMs of few billion parameters at most, namely on-device FMs (ODFMs), which can be deployed on-device. Federated fine-tuning of ODFMs has unique challenges non-present in standard fine-tuning: i) ODFMs poorly generalize to downstream tasks due to their limited sizes making proper fine-tuning imperative to their performance, and ii) devices have limited and heterogeneous system capabilities and data that can deter the performance of fine-tuning.Tackling these challenges, we propose HetLoRA, a feasible and effective federated fine-tuning method for ODFMs that leverages the system and data heterogeneity at the edge. HetLoRA allows heterogeneous LoRA ranks across clients for their individual system resources, and efficiently aggregates and distributes these LoRA modules in a data-aware manner by applying rank self-pruning locally and sparsity-weighted aggregation at the server. It combines the advantages of high and low-rank LoRAs, achieving improved convergence speed and final performance compared to homogeneous LoRA. Furthermore, HetLoRA has enhanced computation and communication efficiency compared to full fine-tuning making it more feasible for the edge.",
        "authors": [
            "Yae Jee Cho",
            "Luyang Liu",
            "Zheng Xu",
            "Aldi Fahrezi",
            "Gauri Joshi"
        ],
        "citations": 26,
        "references": 44,
        "year": 2024
    },
    {
        "title": "CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models",
        "abstract": "Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object’s grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: copa-2024.github.io",
        "authors": [
            "Haoxu Huang",
            "Fanqi Lin",
            "Yingdong Hu",
            "Shengjie Wang",
            "Yang Gao"
        ],
        "citations": 26,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Blind Baselines Beat Membership Inference Attacks for Foundation Models",
        "abstract": "Membership inference (MI) attacks try to determine if a data sample was used to train a machine learning model. For foundation models trained on unknown Web data, MI attacks can be used to detect copyrighted training materials, measure test set contamination, or audit machine unlearning. Unfortunately, we find that evaluations of MI attacks for foundation models are flawed, because they sample members and non-members from different distributions. For 8 published MI evaluation datasets, we show that blind attacks -- that distinguish the member and non-member distributions without looking at any trained model -- outperform state-of-the-art MI attacks. Existing evaluations thus tell us nothing about membership leakage of a foundation model's training data.",
        "authors": [
            "Debeshee Das",
            "Jie Zhang",
            "F. Tramèr"
        ],
        "citations": 19,
        "references": 38,
        "year": 2024
    },
    {
        "title": "InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding",
        "abstract": "data level, we prioritize the spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. We scale both data and model size for our InternVideo2 . Through extensive experiments, we validate our designs and demonstrate the state-of-the-art performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related captioning, dialogue, and long video understanding benchmarks, highlighting its ability to reason and comprehend long temporal contexts.",
        "authors": [
            "Yi Wang",
            "Kunchang Li",
            "Xinhao Li",
            "Jiashuo Yu",
            "Yinan He",
            "Guo Chen",
            "Baoqi Pei",
            "Rongkun Zheng",
            "Jilan Xu",
            "Zun Wang",
            "Yansong Shi",
            "Tianxiang Jiang",
            "Songze Li",
            "Hongjie Zhang",
            "Yifei Huang",
            "Yu Qiao",
            "Yali Wang",
            "Limin Wang"
        ],
        "citations": 80,
        "references": 134,
        "year": 2024
    },
    {
        "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
        "abstract": "Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/ruixiangcui/AGIEval.",
        "authors": [
            "Wanjun Zhong",
            "Ruixiang Cui",
            "Yiduo Guo",
            "Yaobo Liang",
            "Shuai Lu",
            "Yanlin Wang",
            "A. Saied",
            "Weizhu Chen",
            "Nan Duan"
        ],
        "citations": 380,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine",
        "abstract": "Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4's capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model's out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology.",
        "authors": [
            "Harsha Nori",
            "Yin Tat Lee",
            "Sheng Zhang",
            "Dean Carignan",
            "Richard Edgar",
            "Nicoló Fusi",
            "Nicholas King",
            "Jonathan Larson",
            "Yuanzhi Li",
            "Weishung Liu",
            "Renqian Luo",
            "S. McKinney",
            "Robert Osazuwa Ness",
            "Hoifung Poon",
            "Tao Qin",
            "Naoto Usuyama",
            "Chris White",
            "Eric Horvitz"
        ],
        "citations": 228,
        "references": 38,
        "year": 2023
    },
    {
        "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models",
        "abstract": "Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.",
        "authors": [
            "Mengwei Xu",
            "Wangsong Yin",
            "Dongqi Cai",
            "Rongjie Yi",
            "Daliang Xu",
            "Qipeng Wang",
            "Bingyang Wu",
            "Yihao Zhao",
            "Chen Yang",
            "Shihe Wang",
            "Qiyang Zhang",
            "Zhenyan Lu",
            "Li Zhang",
            "Shangguang Wang",
            "Yuanchun Li",
            "Yunxin Liu",
            "Xin Jin",
            "Xuanzhe Liu"
        ],
        "citations": 52,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Foundation Models for Time Series Analysis: A Tutorial and Survey",
        "abstract": "Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration.",
        "authors": [
            "Yuxuan Liang",
            "Haomin Wen",
            "Yuqi Nie",
            "Yushan Jiang",
            "Ming Jin",
            "Dongjin Song",
            "Shirui Pan",
            "Qingsong Wen"
        ],
        "citations": 51,
        "references": 120,
        "year": 2024
    },
    {
        "title": "On the Societal Impact of Open Foundation Models",
        "abstract": "Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties (e.g. greater customizability, poor monitoring) of open foundation models that lead to both their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks.",
        "authors": [
            "Sayash Kapoor",
            "Rishi Bommasani",
            "Kevin Klyman",
            "Shayne Longpre",
            "Ashwin Ramaswami",
            "Peter Cihon",
            "Aspen Hopkins",
            "Kevin Bankston",
            "Stella Biderman",
            "Miranda Bogen",
            "Rumman Chowdhury",
            "Alex Engler",
            "Peter Henderson",
            "Yacine Jernite",
            "Seth Lazar",
            "Stefano Maffulli",
            "Alondra Nelson",
            "Joelle Pineau",
            "Aviya Skowron",
            "Dawn Song",
            "Victor Storchan",
            "Daniel Zhang",
            "Daniel E. Ho",
            "Percy Liang",
            "Arvind Narayanan"
        ],
        "citations": 40,
        "references": 138,
        "year": 2024
    },
    {
        "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
        "abstract": "Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such\"in-the-wild\"data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.",
        "authors": [
            "Michael Ahn",
            "Debidatta Dwibedi",
            "Chelsea Finn",
            "Montse Gonzalez Arenas",
            "K. Gopalakrishnan",
            "Karol Hausman",
            "Brian Ichter",
            "A. Irpan",
            "Nikhil J. Joshi",
            "Ryan C. Julian",
            "Sean Kirmani",
            "Isabel Leal",
            "T. Lee",
            "Sergey Levine",
            "Yao Lu",
            "Sharath Maddineni",
            "Kanishka Rao",
            "Dorsa Sadigh",
            "Pannag R. Sanketi",
            "P. Sermanet",
            "Q. Vuong",
            "Stefan Welker",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Steve Xu",
            "Zhuo Xu"
        ],
        "citations": 33,
        "references": 39,
        "year": 2024
    },
    {
        "title": "Foundation models for generalist medical artificial intelligence",
        "abstract": null,
        "authors": [
            "Michael Moor",
            "Oishi Banerjee",
            "Zahra F H Abad",
            "H. Krumholz",
            "J. Leskovec",
            "E. Topol",
            "P. Rajpurkar"
        ],
        "citations": 689,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Real-World Robot Applications of Foundation Models: A Review",
        "abstract": "Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.",
        "authors": [
            "Kento Kawaharazuka",
            "T. Matsushima",
            "Andrew Gambardella",
            "Jiaxian Guo",
            "Chris Paxton",
            "Andy Zeng"
        ],
        "citations": 30,
        "references": 237,
        "year": 2024
    },
    {
        "title": "MeshXL: Neural Coordinate Field for Generative 3D Foundation Models",
        "abstract": "The polygon mesh representation of 3D data exhibits great flexibility, fast rendering speed, and storage efficiency, which is widely preferred in various applications. However, given its unstructured graph representation, the direct generation of high-fidelity 3D meshes is challenging. Fortunately, with a pre-defined ordering strategy, 3D meshes can be represented as sequences, and the generation process can be seamlessly treated as an auto-regressive problem. In this paper, we validate the Neural Coordinate Field (NeurCF), an explicit coordinate representation with implicit neural embeddings, is a simple-yet-effective representation for large-scale sequential mesh modeling. After that, we present MeshXL, a family of generative pre-trained auto-regressive models, which addresses the process of 3D mesh generation with modern large language model approaches. Extensive experiments show that MeshXL is able to generate high-quality 3D meshes, and can also serve as foundation models for various down-stream applications.",
        "authors": [
            "Sijin Chen",
            "Xin Chen",
            "Anqi Pang",
            "Xianfang Zeng",
            "Wei Cheng",
            "Yijun Fu",
            "Fukun Yin",
            "Yanru Wang",
            "Zhibin Wang",
            "C. Zhang",
            "Jingyi Yu",
            "Gang Yu",
            "Bin Fu",
            "Tao Chen"
        ],
        "citations": 13,
        "references": 102,
        "year": 2024
    },
    {
        "title": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
        "abstract": "Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, andADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs.",
        "authors": [
            "Wenhai Wang",
            "Jifeng Dai",
            "Zhe Chen",
            "Zhenhang Huang",
            "Zhiqi Li",
            "Xizhou Zhu",
            "Xiaowei Hu",
            "Tong Lu",
            "Lewei Lu",
            "Hongsheng Li",
            "Xiaogang Wang",
            "Y. Qiao"
        ],
        "citations": 519,
        "references": 107,
        "year": 2022
    },
    {
        "title": "OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning",
        "abstract": "Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as One Tracker. One- Tracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker. This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object. Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker. Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter- efficient finetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our One- Tracker outperforms other models and achieves state-of-the-art performance.",
        "authors": [
            "Lingyi Hong",
            "Shilin Yan",
            "Renrui Zhang",
            "Wanyun Li",
            "Xinyu Zhou",
            "Pinxue Guo",
            "Kaixun Jiang",
            "Yiting Chen",
            "Jinglun Li",
            "Zhaoyu Chen",
            "Wenqiang Zhang"
        ],
        "citations": 22,
        "references": 118,
        "year": 2024
    },
    {
        "title": "Developing Generalist Foundation Models from a Multimodal Dataset for 3D Computed Tomography",
        "abstract": "While computer vision has achieved tremendous success with multimodal encoding and direct textual interaction with images via chat-based large language models, similar advancements in medical imaging AI, particularly in 3D imaging, have been limited due to the scarcity of comprehensive datasets. To address this critical gap, we introduce CT-RATE, the first dataset that pairs 3D medical images with corresponding textual reports. CT-RATE comprises 25,692 non-contrast 3D chest CT scans from 21,304 unique patients. Through various reconstructions, these scans are expanded to 50,188 volumes, totaling over 14.3 million 2D slices. Each scan is accompanied by its corresponding radiology report. Leveraging CT-RATE, we develop CT-CLIP, a CT-focused contrastive language-image pretraining framework designed for broad applications without the need for task-specific training. We demonstrate how CT-CLIP can be used in two tasks: multi-abnormality detection and case retrieval. Remarkably, in multi-abnormality detection, CT-CLIP outperforms state-of-the-art fully supervised models across all key metrics, effectively eliminating the need for manual annotation. In case retrieval, it efficiently retrieves relevant cases using either image or textual queries, thereby enhancing knowledge dissemination. By combining CT-CLIP's vision encoder with a pretrained large language model, we create CT-CHAT, a vision-language foundational chat model for 3D chest CT volumes. Finetuned on over 2.7 million question-answer pairs derived from the CT-RATE dataset, CT-CHAT surpasses other multimodal AI assistants, underscoring the necessity for specialized methods in 3D medical imaging. Collectively, the open-source release of CT-RATE, CT-CLIP, and CT-CHAT not only addresses critical challenges in 3D medical imaging but also lays the groundwork for future innovations in medical AI and improved patient care.",
        "authors": [
            "Ibrahim Ethem Hamamci",
            "Sezgin Er",
            "Furkan Almas",
            "Ayse Gulnihan Simsek",
            "S. Esirgun",
            "Irem Dogan",
            "M. F. Dasdelen",
            "Bastian Wittmann",
            "Enis Simsar",
            "Mehmet Simsar",
            "Emine Bensu Erdemir",
            "Abdullah Alanbay",
            "A. Sekuboyina",
            "Berkan Lafci",
            "M. K. Ozdemir",
            "Bjoern H Menze"
        ],
        "citations": 17,
        "references": 61,
        "year": 2024
    },
    {
        "title": "DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models",
        "abstract": "We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its endeffector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.",
        "authors": [
            "Norman Di Palo",
            "Edward Johns"
        ],
        "citations": 16,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Why Tabular Foundation Models Should Be a Research Priority",
        "abstract": "Recent text and image foundation models are incredibly impressive, and these models are attracting an ever-increasing portion of research resources. In this position piece we aim to shift the ML research community's priorities ever so slightly to a different modality: tabular data. Tabular data is the dominant modality in many fields, yet it is given hardly any research attention and significantly lags behind in terms of scale and power. We believe the time is now to start developing tabular foundation models, or what we coin a Large Tabular Model (LTM). LTMs could revolutionise the way science and ML use tabular data: not as single datasets that are analyzed in a vacuum, but contextualized with respect to related datasets. The potential impact is far-reaching: from few-shot tabular models to automating data science; from out-of-distribution synthetic data to empowering multidisciplinary scientific discovery. We intend to excite reflections on the modalities we study, and convince some researchers to study large tabular models.",
        "authors": [
            "B. V. Breugel",
            "M. Schaar"
        ],
        "citations": 19,
        "references": 102,
        "year": 2024
    },
    {
        "title": "Asymmetry in Low-Rank Adapters of Foundation Models",
        "abstract": "Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound. We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.",
        "authors": [
            "Jiacheng Zhu",
            "K. Greenewald",
            "Kimia Nadjahi",
            "Haitz S'aez de Oc'ariz Borde",
            "Rickard Brüel Gabrielsson",
            "Leshem Choshen",
            "Marzyeh Ghassemi",
            "M. Yurochkin",
            "Justin Solomon"
        ],
        "citations": 18,
        "references": 40,
        "year": 2024
    },
    {
        "title": "Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models",
        "abstract": "\n We propose masked particle modeling (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains.",
        "authors": [
            "L. Heinrich",
            "T. Golling",
            "M. Kagan",
            "Samuel Klein",
            "Matthew Leigh",
            "Margarita Osadchy",
            "J. A. Raine"
        ],
        "citations": 15,
        "references": 63,
        "year": 2024
    },
    {
        "title": "Data-Centric Foundation Models in Computational Healthcare: A Survey",
        "abstract": "The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare-related foundation models and datasets at https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .",
        "authors": [
            "Yunkun Zhang",
            "Jin Gao",
            "Zheling Tan",
            "Lingfeng Zhou",
            "Kexin Ding",
            "Mu Zhou",
            "Shaoting Zhang",
            "Dequan Wang"
        ],
        "citations": 17,
        "references": 340,
        "year": 2024
    },
    {
        "title": "A Survey for Foundation Models in Autonomous Driving",
        "abstract": "The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the methods employed in current research. It identifies the gaps between existing foundation models and cutting-edge AD approaches, thereby charting future research directions and proposing a roadmap for bridging these gaps.",
        "authors": [
            "Haoxiang Gao",
            "Yaqian Li",
            "Kaiwen Long",
            "Ming Yang",
            "Yiqing Shen"
        ],
        "citations": 16,
        "references": 70,
        "year": 2024
    },
    {
        "title": "On the Opportunities and Challenges of Foundation Models for GeoAI (Vision Paper)",
        "abstract": "Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have not yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial domains, including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality, such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, the task-agnostic large learning models (LLMs) can outperform task-specific fully supervised models in a zero-shot or few-shot learning setting. However, on other geospatial tasks, especially tasks that involve multiple data modalities (e.g., POI-based urban function classification, street view image–based urban noise intensity classification, and remote sensing image scene classification), existing FMs still underperform task-specific models. Based on these observations, we propose that one of the major challenges of developing an FM for GeoAI is to address the multimodal nature of geospatial tasks. After discussing the distinct challenges of each geospatial data modality, we suggest the possibility of a multimodal FM that can reason over various types of geospatial data through geospatial alignments. We conclude this article by discussing the unique risks and challenges to developing such a model for GeoAI.",
        "authors": [
            "Gengchen Mai",
            "Weiming Huang",
            "Jin Sun",
            "Suhang Song",
            "Deepak Mishra",
            "Ninghao Liu",
            "Song Gao",
            "Tianming Liu",
            "Gao Cong",
            "Yingjie Hu",
            "Chris Cundy",
            "Ziyuan Li",
            "Rui Zhu",
            "Ni Lao"
        ],
        "citations": 18,
        "references": 231,
        "year": 2024
    },
    {
        "title": "Sora-Based Parallel Vision for Smart Sensing of Intelligent Vehicles: From Foundation Models to Foundation Intelligence",
        "abstract": "There are a large number of functional sensors installed on the modern intelligent vehicles. Many Artificial Intelligence based foundation models have been proposed for smart sensing to recognize the known object classes in the new but similar scenarios. However, it is still challenging for the foundation models of smart sensing to detect all the object classes in both seen and unseen scenarios. This letter aims at pushing the boundary of smart sensing research for intelligent vehicles. We first summarize the current widely-used foundation models and the foundation intelligence needed for smart sensing of intelligent vehicles. We then explain Sora-based Parallel Vision to boost the foundation models of smart sensing from basic intelligence (1.0) to enhanced intelligence (2.0) and final generalized intelligence (3.0). Several representative case studies are discussed to show the potential usages of Sora-based Parallel Vision, followed by its future research direction.",
        "authors": [
            "Hongkai Yu",
            "Xinyu Liu",
            "Yonglin Tian",
            "Yutong Wang",
            "Chao Gou",
            "Fei-Yue Wang"
        ],
        "citations": 17,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Foundation Models",
        "abstract": null,
        "authors": [
            "Johannes Schneider",
            "Christian Meske",
            "P. Kuss"
        ],
        "citations": 54,
        "references": 26,
        "year": 2024
    },
    {
        "title": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence",
        "abstract": "Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.",
        "authors": [
            "Mayank Mishra",
            "Matt Stallone",
            "Gaoyuan Zhang",
            "Yikang Shen",
            "Aditya Prasad",
            "Adriana Meza Soria",
            "Michele Merler",
            "Parameswaran Selvam",
            "Saptha Surendran",
            "Shivdeep Singh",
            "Manish Sethi",
            "Xuan-Hong Dang",
            "Pengyuan Li",
            "Kun-Lung Wu",
            "Syed Zawad",
            "Andrew Coleman",
            "Matthew White",
            "Mark Lewis",
            "Raju Pavuluri",
            "Y. Koyfman",
            "Boris Lublinsky",
            "M. D. Bayser",
            "Ibrahim Abdelaziz",
            "Kinjal Basu",
            "Mayank Agarwal",
            "Yi Zhou",
            "Chris Johnson",
            "Aanchal Goyal",
            "Hima Patel",
            "Yousaf Shah",
            "Petros Zerfos",
            "Heiko Ludwig",
            "Asim Munawar",
            "M. Crouse",
            "P. Kapanipathi",
            "Shweta Salaria",
            "Bob Calio",
            "Sophia Wen",
            "Seetharami R. Seelam",
            "Brian M. Belgodere",
            "Carlos Fonseca",
            "Amith Singhee",
            "Nirmit Desai",
            "David D. Cox",
            "Ruchir Puri",
            "Rameswar Panda"
        ],
        "citations": 34,
        "references": 59,
        "year": 2024
    },
    {
        "title": "Multimodal artificial intelligence foundation models: Unleashing the power of remote sensing big data in earth observation",
        "abstract": null,
        "authors": [
            "D. Hong",
            "Chenyu Li",
            "Bing Zhang",
            "N. Yokoya",
            "J. Benediktsson",
            "J. Chanussot"
        ],
        "citations": 41,
        "references": 2,
        "year": 2024
    },
    {
        "title": "InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
        "abstract": "The foundation models have recently shown excellent performance on a variety of downstream tasks in computer vision. However, most existing vision foundation models simply focus on image-level pretraining and adpation, which are limited for dynamic and complex video-level understanding tasks. To fill the gap, we present general video foundation models, InternVideo, by taking advantage of both generative and discriminative self-supervised video learning. Specifically, InternVideo efficiently explores masked video modeling and video-language contrastive learning as the pretraining objectives, and selectively coordinates video representations of these two complementary frameworks in a learnable manner to boost various video applications. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets from extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications. Especially, our methods can obtain 91.1% and 77.2% top-1 accuracy on the challenging Kinetics-400 and Something-Something V2 benchmarks, respectively. All of these results effectively show the generality of our InternVideo for video understanding. The code will be released at https://github.com/OpenGVLab/InternVideo .",
        "authors": [
            "Yi Wang",
            "Kunchang Li",
            "Yizhuo Li",
            "Yinan He",
            "Bingkun Huang",
            "Zhiyu Zhao",
            "Hongjie Zhang",
            "Jilan Xu",
            "Yi Liu",
            "Zun Wang",
            "Sen Xing",
            "Guo Chen",
            "Junting Pan",
            "Jiashuo Yu",
            "Yali Wang",
            "Limin Wang",
            "Yu Qiao"
        ],
        "citations": 258,
        "references": 110,
        "year": 2022
    },
    {
        "title": "FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs",
        "abstract": "This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual speech recognition, emotion recognition, and audio event detection; and CosyVoice, which facilitates natural speech generation with control over multiple languages, timbre, speaking style, and speaker identity. SenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and SenseVoice-Large supports high-precision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities. The models related to SenseVoice and CosyVoice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interaction technology. Demos are available at https://fun-audio-llm.github.io, and the code can be accessed at https://github.com/FunAudioLLM.",
        "authors": [
            "Keyu An",
            "Qian Chen",
            "Chong Deng",
            "Zhihao Du",
            "Changfeng Gao",
            "Zhifu Gao",
            "Yue Gu",
            "Ting He",
            "Hangrui Hu",
            "Kai Hu",
            "Shengpeng Ji",
            "Yabin Li",
            "Zerui Li",
            "Heng Lu",
            "Xiang Lv",
            "Bin Ma",
            "Ziyang Ma",
            "Chongjia Ni",
            "Changhe Song",
            "Jiaqi Shi",
            "Xian Shi",
            "Hao Wang",
            "Wen Wang",
            "Yuxuan Wang",
            "Zhangyu Xiao",
            "Zhijie Yan",
            "Yexin Yang",
            "Bin Zhang",
            "Qingling Zhang",
            "Shi-Ya Zhang",
            "Nan Zhao",
            "Siqi Zheng"
        ],
        "citations": 19,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Poseidon: Efficient Foundation Models for PDEs",
        "abstract": "We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.",
        "authors": [
            "Maximilian Herde",
            "Bogdan Raoni'c",
            "Tobias Rohner",
            "R. Käppeli",
            "Roberto Molinaro",
            "Emmanuel de B'ezenac",
            "Siddhartha Mishra"
        ],
        "citations": 15,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Few-shot Adaptation of Multi-modal Foundation Models: A Survey",
        "abstract": "Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models. These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: 1) prompt-based methods, 2) adapter-based methods, and 3) external knowledge-based methods. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: 1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive knowledge utilization.",
        "authors": [
            "Fan Liu",
            "Tianshu Zhang",
            "Wenwen Dai",
            "Wenwen Cai Xiaocong Zhou",
            "Delong Chen"
        ],
        "citations": 14,
        "references": 104,
        "year": 2024
    },
    {
        "title": "Many-Shot In-Context Learning in Multimodal Foundation Models",
        "abstract": "Large language models are effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have enabled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (image classification, visual QA, and object localization). We observe that many-shot ICL, including up to almost 2,000 demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets. We also find open-weights multimodal foundation models like Llama 3.2-Vision do not benefit from the demonstrating examples, highlighting an important gap between open and closed multimodal foundation models. Given the high inference costs required for many-shot ICL, we also explore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and many-shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro learns more quickly than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL .",
        "authors": [
            "Yixing Jiang",
            "Jeremy Irvin",
            "Ji Hun Wang",
            "Muhammad Ahmed Chaudhry",
            "Jonathan H. Chen",
            "Andrew Y. Ng"
        ],
        "citations": 14,
        "references": 38,
        "year": 2024
    },
    {
        "title": "Position: Graph Foundation Models Are Already Here",
        "abstract": "Graph Foundation Models (GFMs) are emerging as a significant research topic in the graph domain, aiming to develop graph models trained on extensive and diverse data to enhance their applicability across various tasks and domains. Developing GFMs presents unique challenges over traditional Graph Neural Networks (GNNs), which are typically trained from scratch for specific tasks on particular datasets. The primary challenge in constructing GFMs lies in effectively leveraging vast and diverse graph data to achieve positive transfer. Drawing inspiration from existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, expressiveness, and stability. Such a vocabulary perspective can potentially advance the future GFM design in line with the neural scaling laws. All relevant resources with GFM design can be found here.",
        "authors": [
            "Haitao Mao",
            "Zhikai Chen",
            "Wenzhuo Tang",
            "Jianan Zhao",
            "Yao Ma",
            "Tong Zhao",
            "Neil Shah",
            "Mikhail Galkin",
            "Jiliang Tang"
        ],
        "citations": 14,
        "references": 194,
        "year": 2024
    },
    {
        "title": "Large language models and multimodal foundation models for precision oncology",
        "abstract": null,
        "authors": [
            "Daniel Truhn",
            "Jan-Niklas Eckardt",
            "Dyke Ferber",
            "J. N. Kather"
        ],
        "citations": 23,
        "references": 28,
        "year": 2024
    },
    {
        "title": "Surgical-DINO: adapter learning of foundation models for depth estimation in endoscopic surgery",
        "abstract": null,
        "authors": [
            "Beilei Cui",
            "Mobarakol Islam",
            "Long Bai",
            "Hongliang Ren"
        ],
        "citations": 27,
        "references": 28,
        "year": 2024
    },
    {
        "title": "OpenGraph: Towards Open Graph Foundation Models",
        "abstract": "Graph learning has become essential in various domains, including recommendation systems and social network analysis. Graph Neural Networks (GNNs) have emerged as promising techniques for encoding structural information and improving performance in tasks like link prediction and node classification. However, a key challenge remains: the difficulty of generalizing to unseen graph data with different properties. In this work, we propose a novel graph foundation model, called OpenGraph, to address this challenge. Our approach tackles several technical obstacles. Firstly, we enhance data augmentation using a large language model (LLM) to overcome data scarcity in real-world scenarios. Secondly, we introduce a unified graph tokenizer that enables the model to generalize effectively to diverse graph data, even when encountering unseen properties during training. Thirdly, our developed scalable graph transformer captures node-wise dependencies within the global topological context. Extensive experiments validate the effectiveness of our framework. By adapting OpenGraph to new graph characteristics and comprehending diverse graphs, our approach achieves remarkable zero-shot graph learning performance across various settings. We release the model implementation at https://github.com/HKUDS/OpenGraph.",
        "authors": [
            "Lianghao Xia",
            "Ben Kao",
            "Chao Huang"
        ],
        "citations": 19,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities",
        "abstract": "The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving.",
        "authors": [
            "Xu Yan",
            "Haiming Zhang",
            "Yingjie Cai",
            "Jingming Guo",
            "Weichao Qiu",
            "Bin Gao",
            "Kaiqiang Zhou",
            "Yue Zhao",
            "Huan Jin",
            "Jiantao Gao",
            "Zhen Li",
            "Lihui Jiang",
            "Wei Zhang",
            "Hongbo Zhang",
            "Dengxin Dai",
            "Bingbing Liu"
        ],
        "citations": 13,
        "references": 258,
        "year": 2024
    },
    {
        "title": "A Survey on Robotics with Foundation Models: toward Embodied AI",
        "abstract": "While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.",
        "authors": [
            "Zhiyuan Xu",
            "Kun Wu",
            "Junjie Wen",
            "Jinming Li",
            "Ning Liu",
            "Zhengping Che",
            "Jian Tang"
        ],
        "citations": 13,
        "references": 63,
        "year": 2024
    },
    {
        "title": "A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model",
        "abstract": "Time series data are ubiquitous across various domains, making time series analysis critically important. Traditional time series models are task-specific, featuring singular functionality and limited generalization capacity. Recently, large language foundation models have unveiled their remarkable capabilities for cross-task transferability, zero-shot/few-shot learning, and decision-making explainability. This success has sparked interest in the exploration of foundation models to solve multiple time series challenges simultaneously. There are two main research lines, namely pre-training foundation models from scratch for time series and adapting large language foundation models for time series. They both contribute to the development of a unified model that is highly generalizable, versatile, and comprehensible for time series analysis. This survey offers a 3E analytical framework for comprehensive examination of related research. Specifically, we examine existing works from three dimensions, namely Effectiveness, Efficiency and Explainability. In each dimension, we focus on discussing how related works devise tailored solution by considering unique challenges in the realm of time series. Furthermore, we provide a domain taxonomy to help followers keep up with the domain-specific advancements. In addition, we introduce extensive resources to facilitate the field's development, including datasets, open-source, time series libraries. A GitHub repository is also maintained for resource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).",
        "authors": [
            "Jiexia Ye",
            "Weiqi Zhang",
            "Ke Yi",
            "Yongzi Yu",
            "Ziyue Li",
            "Jia Li",
            "F. Tsung"
        ],
        "citations": 14,
        "references": 189,
        "year": 2024
    },
    {
        "title": "Heterogeneous Contrastive Learning for Foundation Models and Beyond",
        "abstract": "In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how contrastive learning is applied to train and fine-tune the multi-view foundation models. Then, we move to contrastive learning methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with contrastive learning loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of contrastive learning.",
        "authors": [
            "Lecheng Zheng",
            "Baoyu Jing",
            "Zihao Li",
            "Hanghang Tong",
            "Jingrui He"
        ],
        "citations": 13,
        "references": 64,
        "year": 2024
    },
    {
        "title": "Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models",
        "abstract": "To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.",
        "authors": [
            "Goutham Rajendran",
            "Simon Buchholz",
            "Bryon Aragam",
            "Bernhard Schölkopf",
            "Pradeep Ravikumar"
        ],
        "citations": 13,
        "references": 120,
        "year": 2024
    },
    {
        "title": "On the Foundations of Earth and Climate Foundation Models",
        "abstract": "Foundation models have enormous potential in advancing Earth and climate sciences, however, current approaches may not be optimal as they focus on a few basic features of a desirable Earth and climate foundation model. Crafting the ideal Earth foundation model, we define eleven features which would allow such a foundation model to be beneficial for any geoscientific downstream application in an environmental- and human-centric manner.We further shed light on the way forward to achieve the ideal model and to evaluate Earth foundation models. What comes after foundation models? Energy efficient adaptation, adversarial defenses, and interpretability are among the emerging directions.",
        "authors": [
            "Xiao Xiang Zhu",
            "Zhitong Xiong",
            "Yi Wang",
            "Adam J. Stewart",
            "Konrad Heidler",
            "Yuanyuan Wang",
            "Zhenghang Yuan",
            "Thomas Dujardin",
            "Qingsong Xu",
            "Yilei Shi"
        ],
        "citations": 13,
        "references": 134,
        "year": 2024
    },
    {
        "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
        "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
        "authors": [
            "Ce Zhou",
            "Qian Li",
            "Chen Li",
            "Jun Yu",
            "Yixin Liu",
            "Guangjing Wang",
            "Kaichao Zhang",
            "Cheng Ji",
            "Qi Yan",
            "Lifang He",
            "Hao Peng",
            "Jianxin Li",
            "Jia Wu",
            "Ziwei Liu",
            "P. Xie",
            "Caiming Xiong",
            "Jian Pei",
            "Philip S. Yu",
            "Lichao Sun Michigan State University",
            "B. University",
            "Lehigh University",
            "M. University",
            "Nanyang Technological University",
            "University of California at San Diego",
            "Duke University",
            "U. Chicago",
            "Salesforce Research"
        ],
        "citations": 418,
        "references": 0,
        "year": 2023
    },
    {
        "title": "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs",
        "abstract": "Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce TaskMatrix.AI as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, TaskMatrix.AI focuses more on using existing foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.",
        "authors": [
            "Yaobo Liang",
            "Chenfei Wu",
            "Ting Song",
            "Wenshan Wu",
            "Yan Xia",
            "Yu Liu",
            "Yangyiwen Ou",
            "Shuai Lu",
            "Lei Ji",
            "Shaoguang Mao",
            "Yuntao Wang",
            "Linjun Shou",
            "Ming Gong",
            "Nan Duan"
        ],
        "citations": 172,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Multimodal Foundation Models: From Specialists to General-Purpose Assistants",
        "abstract": "This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.",
        "authors": [
            "Chunyuan Li",
            "Zhe Gan",
            "Zhengyuan Yang",
            "Jianwei Yang",
            "Linjie Li",
            "Lijuan Wang",
            "Jianfeng Gao"
        ],
        "citations": 181,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Tool Learning with Foundation Models",
        "abstract": "\n Humans possess an extraordinary ability to create and utilize tools. With the advent of foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as\n tool learning with foundation models\n , combines the strengths of tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. This paper presents a systematic investigation and comprehensive review of tool learning. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research and formulate a general framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate generalization in tool learning. Finally, we discuss several open problems that require further investigation, such as ensuring trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. Overall, we hope this paper could inspire future research in integrating tools with foundation models.\n",
        "authors": [
            "Yujia Qin",
            "Shengding Hu",
            "Yankai Lin",
            "Weize Chen",
            "Ning Ding",
            "Ganqu Cui",
            "Zheni Zeng",
            "Yufei Huang",
            "Chaojun Xiao",
            "Chi Han",
            "Y. Fung",
            "Yusheng Su",
            "Huadong Wang",
            "Cheng Qian",
            "Runchu Tian",
            "Kunlun Zhu",
            "Shi Liang",
            "Xingyu Shen",
            "Bokai Xu",
            "Zhen Zhang",
            "Yining Ye",
            "Bo Li",
            "Ziwei Tang",
            "Jing Yi",
            "Yu Zhu",
            "Zhenning Dai",
            "Lan Yan",
            "Xin Cong",
            "Ya-Ting Lu",
            "Weilin Zhao",
            "Yuxiang Huang",
            "Junxi Yan",
            "Xu Han",
            "Xian Sun",
            "Dahai Li",
            "Jason Phang",
            "Cheng Yang",
            "Tongshuang Wu",
            "Heng Ji",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "citations": 173,
        "references": 241,
        "year": 2023
    },
    {
        "title": "A Survey of Hallucination in Large Foundation Models",
        "abstract": "Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.",
        "authors": [
            "Vipula Rawte",
            "A. Sheth",
            "Amitava Das"
        ],
        "citations": 274,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems",
        "abstract": "Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6 G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the unique needs of next-generation wireless systems, thereby paving the way towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.",
        "authors": [
            "Shengzhe Xu",
            "Christo Kurisummoottil Thomas",
            "Omar Hashash",
            "N. Muralidhar",
            "Walid Saad",
            "Naren Ramakrishnan"
        ],
        "citations": 12,
        "references": 21,
        "year": 2024
    },
    {
        "title": "Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning",
        "abstract": "Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our theoretical claims with extensive empirical evidence. Further, we present results affirming our task selection algorithm adeptly chooses related finetuning tasks, providing advantages to the model performance on target tasks. We believe our study shed new light on the effective adaptation of foundation models to new tasks that lack abundant labels. Our code is available at https://github.com/OliverXUZY/Foudation-Model_Multitask.",
        "authors": [
            "Zhuoyan Xu",
            "Zhenmei Shi",
            "Junyi Wei",
            "Fangzhou Mu",
            "Yin Li",
            "Yingyu Liang"
        ],
        "citations": 12,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Q-Bench<inline-formula><tex-math notation=\"LaTeX\">$^+$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3445770.gif\"/></alternatives></inline-formula>: A Benchmark for Multi-Modal Foundation Models on Low-Level Visi",
        "abstract": "The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile foundational models. However, evaluating MLLMs in <italic>low-level visual perception and understanding</italic> remains a yet-to-explore domain. To this end, we design benchmark settings to <italic>emulate human language responses</italic> related to low-level vision: the low-level visual <italic>perception</italic> (<underline>A1</underline>) <italic>via</italic> visual question answering related to low-level attributes (<italic>e.g. clarity, lighting</italic>); and the low-level visual <italic>description</italic> (<underline>A2</underline>), on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related question-answering and description evaluations of MLLMs from single images to <italic>image pairs</italic>. Specifically, for <italic>perception</italic> (A1), we carry out the LLVisionQA<inline-formula><tex-math notation=\"LaTeX\">$^{+}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3445770.gif\"/></alternatives></inline-formula> dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended question about its low-level features; for <bold/><italic>description</italic><bold/> (A2), we propose the LLDescribe<inline-formula><tex-math notation=\"LaTeX\">$^{+}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3445770.gif\"/></alternatives></inline-formula> dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on <bold/><italic>assessment</italic><bold/> (A3) ability, <italic>i.e.</italic> predicting score, by employing a softmax-based approach to enable all MLLMs to generate <italic>quantifiable</italic> quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level visual competencies on single images, but only GPT-4V exhibits higher accuracy on pairwise comparisons than single image evaluations (<italic>like humans</italic>). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs.",
        "authors": [
            "Zicheng Zhang",
            "Haoning Wu",
            "Erli Zhang",
            "Guangtao Zhai",
            "Weisi Lin"
        ],
        "citations": 14,
        "references": 74,
        "year": 2024
    },
    {
        "title": "The shaky foundations of large language models and foundation models for electronic health records",
        "abstract": null,
        "authors": [
            "Michael Wornow",
            "Yizhe Xu",
            "Rahul Thapa",
            "Birju S. Patel",
            "E. Steinberg",
            "S. Fleming",
            "M. Pfeffer",
            "J. Fries",
            "N. Shah"
        ],
        "citations": 139,
        "references": 113,
        "year": 2023
    },
    {
        "title": "Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners",
        "abstract": "Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pretraining paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by ‘Prompt, Generate, then Cache’. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of-the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.",
        "authors": [
            "Renrui Zhang",
            "Xiangfei Hu",
            "Bohao Li",
            "Siyuan Huang",
            "Hanqiu Deng",
            "Hongsheng Li",
            "Y. Qiao",
            "Peng Gao"
        ],
        "citations": 139,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
        "abstract": "Video Foundation Models (VFMs) have received limited exploration due to high computational costs and data scarcity. Previous VFMs rely on Image Foundation Models (IFMs), which face challenges in transferring to the video domain. Although VideoMAE has trained a robust ViT from limited data, its low-level reconstruction poses convergence difficulties and conflicts with high-level cross-modal alignment. This paper proposes a training-efficient method for temporal-sensitive VFMs that integrates the benefits of existing methods. To increase data efficiency, we mask out most of the low-semantics video tokens, but selectively align the unmasked tokens with IFM, which serves as the UnMasked Teacher (UMT). By providing semantic guidance, our method enables faster convergence and multi-modal friendliness. With a progressive pre-training framework, our model can handle various tasks including scene-related, temporal-related, and complex video-language understanding. Using only public sources for pre-training in 6 days on 32 A100 GPUs, our scratch-built ViT-L/16 achieves state-of-the-art performances on various video tasks.",
        "authors": [
            "Kunchang Li",
            "Yali Wang",
            "Yizhuo Li",
            "Yi Wang",
            "Yinan He",
            "Limin Wang",
            "Yu Qiao"
        ],
        "citations": 109,
        "references": 111,
        "year": 2023
    },
    {
        "title": "On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence",
        "abstract": "Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial subdomains including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, these task-agnostic LLMs can outperform task-specific fully-supervised models in a zero-shot or few-shot learning setting. However, on other geospatial tasks, especially tasks that involve multiple data modalities (e.g., POI-based urban function classification, street view image-based urban noise intensity classification, and remote sensing image scene classification), existing foundation models still underperform task-specific models. Based on these observations, we propose that one of the major challenges of developing a FM for GeoAI is to address the multimodality nature of geospatial tasks. After discussing the distinct challenges of each geospatial data modality, we suggest the possibility of a multimodal foundation model which can reason over various types of geospatial data through geospatial alignments. We conclude this paper by discussing the unique risks and challenges to develop such a model for GeoAI.",
        "authors": [
            "Gengchen Mai",
            "Weiming Huang",
            "Jin Sun",
            "Suhang Song",
            "Deepak Mishra",
            "Ninghao Liu",
            "Song Gao",
            "Tianming Liu",
            "G. Cong",
            "Yingjie Hu",
            "Chris Cundy",
            "Ziyuan Li",
            "Rui Zhu",
            "Ni Lao"
        ],
        "citations": 102,
        "references": 168,
        "year": 2023
    },
    {
        "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
        "abstract": "Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: https://lmexam.com.",
        "authors": [
            "Yushi Bai",
            "Jiahao Ying",
            "Yixin Cao",
            "Xin Lv",
            "Yuze He",
            "Xiaozhi Wang",
            "Jifan Yu",
            "Kaisheng Zeng",
            "Yijia Xiao",
            "Haozhe Lyu",
            "Jiayin Zhang",
            "Juanzi Li",
            "Lei Hou"
        ],
        "citations": 105,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities",
        "abstract": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.",
        "authors": [
            "Sherry Yang",
            "Ofir Nachum",
            "Yilun Du",
            "Jason Wei",
            "P. Abbeel",
            "D. Schuurmans"
        ],
        "citations": 126,
        "references": 231,
        "year": 2023
    },
    {
        "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision",
        "abstract": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. Project Page: https://q-future.github.io/Q-Bench.",
        "authors": [
            "Haoning Wu",
            "Zicheng Zhang",
            "Erli Zhang",
            "Chaofeng Chen",
            "Liang Liao",
            "Annan Wang",
            "Chunyi Li",
            "Wenxiu Sun",
            "Qiong Yan",
            "Guangtao Zhai",
            "Weisi Lin"
        ],
        "citations": 105,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Foundation Models and Fair Use",
        "abstract": "Existing foundation models are trained on copyrighted material. Deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. In the United States and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. However, there is a caveat: If the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. In this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. First, we survey the potential risks of developing and deploying foundation models based on copyrighted content. We review relevant U.S. case law, drawing parallels to existing and potential applications for generating text, source code, and visual art. Experiments confirm that popular foundation models can generate content considerably similar to copyrighted material. Second, we discuss technical mitigations that can help foundation models stay in line with fair use. We argue that more research is needed to align mitigation strategies with the current state of the law. Lastly, we suggest that the law and technical mitigations should co-evolve. For example, coupled with other policy mechanisms, the law could more explicitly consider safe harbors when strong technical tools are used to mitigate infringement harms. This co-evolution may help strike a balance between intellectual property and innovation, which speaks to the original goal of fair use. But we emphasize that the strategies we describe here are not a panacea and more work is needed to develop policies that address the potential harms of foundation models.",
        "authors": [
            "Peter Henderson",
            "Xuechen Li",
            "Dan Jurafsky",
            "Tatsunori Hashimoto",
            "Mark A. Lemley",
            "Percy Liang"
        ],
        "citations": 95,
        "references": 144,
        "year": 2023
    },
    {
        "title": "A Large-Scale Evaluation of Speech Foundation Models",
        "abstract": "The foundation model paradigm leverages a shared foundation model to achieve state-of-the-art (SOTA) performance for various tasks, requiring minimal downstream-specific data collection and modeling. This approach has proven crucial in the field of Natural Language Processing (NLP). However, the speech processing community lacks a similar setup to explore the paradigm systematically. To bridge this gap, we establish the Speech processing Universal PERformance Benchmark (SUPERB). SUPERB represents an ecosystem designed to evaluate foundation models across a wide range of speech processing tasks, facilitating the sharing of results on an online leaderboard and fostering collaboration through a community-driven benchmark database that aids in new development cycles. We present a unified learning framework for solving the speech processing tasks in SUPERB with the frozen foundation model followed by task-specialized lightweight prediction heads. Combining our results with community submissions, we verify that the framework is simple yet effective, as the best-performing foundation model shows competitive generalizability across most SUPERB tasks. Finally, we conduct a series of analyses to offer an in-depth understanding of SUPERB and speech foundation models, including information flows across tasks inside the models and the statistical significance and robustness of the benchmark.",
        "authors": [
            "Shu-Wen Yang",
            "Heng-Jui Chang",
            "Zili Huang",
            "Andy T. Liu",
            "Cheng-I Lai",
            "Haibin Wu",
            "Jiatong Shi",
            "Xuankai Chang",
            "Hsiang-Sheng Tsai",
            "Wen-Chin Huang",
            "Tzu-hsun Feng",
            "Po-Han Chi",
            "Yist Y. Lin",
            "Yung-Sung Chuang",
            "Tzu-hsien Huang",
            "Wei-Cheng Tseng",
            "Kushal Lakhotia",
            "Shang-Wen Li",
            "Abdelrahman Mohamed",
            "Shinji Watanabe",
            "Hung-yi Lee"
        ],
        "citations": 11,
        "references": 123,
        "year": 2024
    },
    {
        "title": "Distributed Foundation Models for Multi-Modal Learning in 6G Wireless Networks",
        "abstract": "Benefiting from the ability to process and integrate data from various modalities, multi-modal foundation models (FMs) facilitate potential applications across a range of fields, including computer vision (CV), natural language processing (NLP), and diverse multi-modal applications such as imagetext retrieval. Currently, FMs are deployed on computing clusters for training and inference to meet their considerable computational demands. In the foreseeable future, the parameter size of FMs is expected to evolve further, posing challenges to both computation resources and energy supply. Fortunately, leveraging the next-generation wireless networks (6G) to aggregate substantial computation resources and multi-modal data from myriad wireless devices holds promise for handling the aforementioned challenges. In this work, we delve into state-of-the-art artificial intelligence (AI) techniques, specifically focusing on pipeline parallelism, data parallelism, and multi-modal learning, with the aim of supporting the sustainable development of distributed multi-modal FMs in the 6G era. In the context of pipeline parallelism, compressing activations and gradients while intelligently allocating communication resources can overcome communication bottlenecks caused by unstable wireless links. For data parallelism, federated learning (FL) with over-the-air computation (AirComp) seamlessly integrates communication and computation, significantly expediting gradient aggregation. Furthermore, by following the recent success of large language models (LLMs) and incorporating multi-modal learning into FMs, we can seamlessly integrate NLP and CV, along with the broader AI community, establishing the cornerstone for the intrinsic AI within 6G wireless networks.",
        "authors": [
            "Jun Du",
            "Tianyi Lin",
            "Chunxiao Jiang",
            "Qianqian Yang",
            "Faouzi Bader",
            "Zhu Han"
        ],
        "citations": 11,
        "references": 13,
        "year": 2024
    },
    {
        "title": "Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
        "abstract": "science (CS) domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs",
        "authors": [
            "Jiajia Li",
            "Mingle Xu",
            "Lirong Xiang",
            "Dong Chen",
            "Weichao Zhuang",
            "Xunyuan Yin",
            "Zhao Li"
        ],
        "citations": 13,
        "references": 173,
        "year": 2024
    },
    {
        "title": "Considerations for governing open foundation models",
        "abstract": "Different policy proposals may disproportionately affect the innovation ecosystem Foundation models (e.g., GPT-4 and Llama 3.1) are at the epicenter of artificial intelligence (AI), driving technological innovation and billions of dollars in investment. This has sparked widespread demands for regulation. Central to the debate about how to regulate foundation models is the process by which foundation models are released (1)—whether they are made available only to the model developers, fully open to the public, or somewhere in between. Open foundation models can benefit society by promoting competition, accelerating innovation, and distributing power. However, an emerging concern is whether open foundation models pose distinct risks to society (2). In general, although most policy proposals and regulations do not mention open foundation models by name, they may have an uneven impact on open and closed foundation models. We illustrate tensions that surface—and that policy-makers should consider—regarding different policy proposals that may disproportionately damage the innovation ecosystem around open foundation models.",
        "authors": [
            "Rishi Bommasani",
            "Sayash Kapoor",
            "Kevin Klyman",
            "Shayne Longpre",
            "Ashwin Ramaswami",
            "Daniel Zhang",
            "Marietje Schaake",
            "Daniel E. Ho",
            "Arvind Narayanan",
            "Percy Liang"
        ],
        "citations": 10,
        "references": 8,
        "year": 2024
    },
    {
        "title": "From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models",
        "abstract": "Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models, have revolutionized various natural language processing tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. We review fundamental building blocks crucial for studying chart understanding tasks. Additionally, we explore various tasks and their evaluation metrics and sources of both charts and textual inputs. Various modeling strategies are then examined, encompassing both classification-based and generation-based approaches, along with tool augmentation techniques that enhance chart understanding performance. Furthermore, we discuss the state-of-the-art performance of each task and discuss how we can improve the performance. Challenges and future directions are addressed, highlighting the importance of several topics, such as domain-specific charts, lack of efforts in developing evaluation metrics, and agent-oriented settings. This survey paper serves as a comprehensive resource for researchers and practitioners in the fields of natural language processing, computer vision, and data analysis, providing valuable insights and directions for future research in chart understanding leveraging large foundation models. The studies mentioned in this paper, along with emerging new research, will be continually updated at: https://github.com/khuangaf/Awesome-Chart-Understanding.",
        "authors": [
            "Kung-Hsiang Huang",
            "Hou Pong Chan",
            "Y. Fung",
            "Haoyi Qiu",
            "Mingyang Zhou",
            "Shafiq R. Joty",
            "Shih-Fu Chang",
            "Heng Ji"
        ],
        "citations": 11,
        "references": 131,
        "year": 2024
    },
    {
        "title": "Foundation Models for Video Understanding: A Survey",
        "abstract": "Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks. Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data. This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories. Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks. We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework. By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding. Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks. Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks. We share the comprehensive list of ViFMs studied in this work at: \\url{https://github.com/NeeluMadan/ViFM_Survey.git}",
        "authors": [
            "Neelu Madan",
            "Andreas Møgelmose",
            "Rajat Modi",
            "Y. S. Rawat",
            "T. Moeslund"
        ],
        "citations": 11,
        "references": 0,
        "year": 2024
    },
    {
        "title": "One for All: Toward Unified Foundation Models for Earth Vision",
        "abstract": "Foundation models characterized by extensive parameters and trained on large-scale datasets have demonstrated remarkable efficacy across various downstream tasks for remote sensing data. Current remote sensing foundation models typically specialize in a single modality or a specific spatial resolution range, limiting their versatility for downstream datasets. While there have been attempts to develop multi-modal remote sensing foundation models, they typically employ separate vision encoders for each modality or spatial resolution, necessitating a switch in backbones contingent upon the input data. To address this issue, we introduce a simple yet effective method, termed OFA-Net (One-For-All Network): employing a single, shared Transformer backbone for multiple data modalities with different spatial resolutions. Using the masked image modeling mechanism, we pre-train a single Transformer backbone on a curated multi-modal dataset with this simple design. Then the backbone model can be used in different downstream tasks, thus forging a path towards a unified foundation backbone model in Earth vision. The proposed method is evaluated on 12 distinct downstream tasks and demonstrates promising performance.",
        "authors": [
            "Zhitong Xiong",
            "Yi Wang",
            "Fahong Zhang",
            "Xiao Xiang Zhu"
        ],
        "citations": 11,
        "references": 19,
        "year": 2024
    },
    {
        "title": "On Catastrophic Inheritance of Large Foundation Models",
        "abstract": "Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims to unite both the machine learning and social sciences communities for more responsible and promising AI development and deployment.",
        "authors": [
            "Hao Chen",
            "Bhiksha Raj",
            "Xing Xie",
            "Jindong Wang"
        ],
        "citations": 10,
        "references": 207,
        "year": 2024
    },
    {
        "title": "A Survey of Deep Learning and Foundation Models for Time Series Forecasting",
        "abstract": "Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided.",
        "authors": [
            "John A. Miller",
            "Mohammed Aldosari",
            "Farah Saeed",
            "Nasid Habib Barna",
            "Subas Rana",
            "I. Arpinar",
            "Ninghao Liu"
        ],
        "citations": 10,
        "references": 141,
        "year": 2024
    },
    {
        "title": "How to Index Item IDs for Recommendation Foundation Models",
        "abstract": "Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item as in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text and hallucinated recommendations when deciding which item(s) to recommend, creating LLM-compatible item IDs to uniquely identify each item is essential for recommendation foundation models. In this study, we systematically examine the item ID creation and indexing problem for recommendation foundation models, using P5 as an example of the backbone LLM. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as random indexing, title indexing, and independent indexing. We then propose four simple yet effective solutions, including sequential indexing, collaborative indexing, semantic (content-based) indexing, and hybrid indexing. Our study highlights the significant influence of item indexing methods on the performance of LLM-based recommendation, and our results on real-world datasets validate the effectiveness of our proposed solutions. The research also demonstrates how recent advances on language modeling and traditional IR principles such as indexing can help each other for better learning and inference. Source code and data are available at https://github.com/Wenyueh/LLM-RecSys-ID.",
        "authors": [
            "Wenyue Hua",
            "Shuyuan Xu",
            "Yingqiang Ge",
            "Yongfeng Zhang"
        ],
        "citations": 80,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Towards Graph Foundation Models: A Survey and Beyond",
        "abstract": "Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is witnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation models to generalize and adapt motivate graph machine learning researchers to discuss the potential of developing a new graph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various graph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this new domain. To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation of their key characteristics and underlying technologies. We proceed to classify the existing work related to GFMs into three distinct categories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough review of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.",
        "authors": [
            "Jiawei Liu",
            "Cheng Yang",
            "Zhiyuan Lu",
            "Junze Chen",
            "Yibo Li",
            "Mengmei Zhang",
            "Ting Bai",
            "Yuan Fang",
            "Lichao Sun",
            "Philip S. Yu",
            "Chuan Shi"
        ],
        "citations": 70,
        "references": 236,
        "year": 2023
    },
    {
        "title": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
        "abstract": "The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded multimodal perception, HTML comprehension, and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB, we improve over the previous best offline methods by more than 45.8%, even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the WebShop benchmark, our 3-billion-parameter model achieves superior performance to the existing SoTA, PaLM-540B. Furthermore, WebGUM exhibits strong positive transfer to the real-world planning tasks on the Mind2Web. We also collect 347K high-quality demonstrations using our trained models, 38 times larger than prior work, and make them available to promote future research in this direction.",
        "authors": [
            "Hiroki Furuta",
            "Ofir Nachum",
            "Kuang-Huei Lee",
            "Yutaka Matsuo",
            "S. Gu",
            "Izzeddin Gur"
        ],
        "citations": 67,
        "references": 114,
        "year": 2023
    },
    {
        "title": "Foundation Models in Robotics: Applications, Challenges, and the Future",
        "abstract": "We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models .",
        "authors": [
            "Roya Firoozi",
            "Johnathan Tucker",
            "Stephen Tian",
            "Anirudha Majumdar",
            "Jiankai Sun",
            "Weiyu Liu",
            "Yuke Zhu",
            "Shuran Song",
            "Ashish Kapoor",
            "Karol Hausman",
            "Brian Ichter",
            "Danny Driess",
            "Jiajun Wu",
            "Cewu Lu",
            "Mac Schwager"
        ],
        "citations": 83,
        "references": 232,
        "year": 2023
    },
    {
        "title": "Vision-Language Foundation Models as Effective Robot Imitators",
        "abstract": "Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.",
        "authors": [
            "Xinghang Li",
            "Minghuan Liu",
            "Hanbo Zhang",
            "Cunjun Yu",
            "Jie Xu",
            "Hongtao Wu",
            "Chi-Hou Cheang",
            "Ya Jing",
            "Weinan Zhang",
            "Huaping Liu",
            "Hang Li",
            "Tao Kong"
        ],
        "citations": 80,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Effective Long-Context Scaling of Foundation Models",
        "abstract": "We present an effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens. Our models are built through continual pretraining from Llama 2 checkpoints with longer text sequences and on a dataset where long texts are upsampled. We perform extensive evaluation using language modeling, synthetic context probing tasks, and a wide range of downstream benchmarks. Across all evaluations, our models achieve consistent improvements on most regular-context tasks and significant improvements on long-context tasks over Llama 2. Moreover, with a cost-effective instruction tuning procedure that is free of expensive annotation, the presented models can already surpass \\texttt{gpt-3.5-turbo-16k}‘s overall performance on long-context benchmarks. Alongside these results, we provide an in-depth analysis on each individual component of our method. We delve into Llama’s position encodings and discuss its key limitation in modeling long data. We examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths – ablation results suggest that having abundant long texts in the pretrain dataset is \\textit{not} the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.",
        "authors": [
            "Wenhan Xiong",
            "Jingyu Liu",
            "Igor Molybog",
            "Hejia Zhang",
            "Prajjwal Bhargava",
            "Rui Hou",
            "Louis Martin",
            "Rashi Rungta",
            "Karthik Abinav Sankararaman",
            "Barlas Oğuz",
            "Madian Khabsa",
            "Han Fang",
            "Yashar Mehdad",
            "Sharan Narang",
            "Kshitiz Malik",
            "Angela Fan",
            "Shruti Bhosale",
            "Sergey Edunov",
            "Mike Lewis",
            "Sinong Wang",
            "Hao Ma"
        ],
        "citations": 154,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Lag-Llama: Towards Foundation Models for Time Series Forecasting",
        "abstract": "Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama , a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen “out-of-distribution” time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws [7] to fit and predict model scaling behavior. The open source code is made available at https://github",
        "authors": [
            "Kashif Rasul",
            "Arjun Ashok",
            "Andrew Robert Williams",
            "Arian Khorasani",
            "George Adamopoulos",
            "Rishika Bhagwatkar",
            "Marin Bilos",
            "Hena Ghonia",
            "N. Hassen",
            "Anderson Schneider",
            "Sahil Garg",
            "Alexandre Drouin",
            "Nicolas Chapados",
            "Yuriy Nevmyvaka",
            "Irina Rish"
        ],
        "citations": 58,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models",
        "abstract": "Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, obviating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment regularization stages, facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notably, Seal achieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing random initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU. Moreover, Seal demonstrates significant performance gains over existing methods across 20 different few-shot fine-tuning tasks on all eleven tested point cloud datasets.",
        "authors": [
            "You-Chen Liu",
            "Lingdong Kong",
            "Jun Cen",
            "Runnan Chen",
            "Wenwei Zhang",
            "Liang Pan",
            "Kai Chen",
            "Ziwei Liu"
        ],
        "citations": 61,
        "references": 122,
        "year": 2023
    },
    {
        "title": "Foundation Models for Generalist Geospatial Artificial Intelligence",
        "abstract": "Significant progress in the development of highly adaptable and reusable Artificial Intelligence (AI) models is expected to have a significant impact on Earth science and remote sensing. Foundation models are pre-trained on large unlabeled datasets through self-supervision, and then fine-tuned for various downstream tasks with small labeled datasets. This paper introduces a first-of-a-kind framework for the efficient pre-training and fine-tuning of foundational models on extensive geospatial data. We have utilized this framework to create Prithvi, a transformer-based geospatial foundational model pre-trained on more than 1TB of multispectral satellite imagery from the Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates the efficacy of our framework in successfully fine-tuning Prithvi to a range of Earth observation tasks that have not been tackled by previous work on foundation models involving multi-temporal cloud gap imputation, flood mapping, wildfire scar segmentation, and multi-temporal crop segmentation. Our experiments show that the pre-trained model accelerates the fine-tuning process compared to leveraging randomly initialized weights. In addition, pre-trained Prithvi compares well against the state-of-the-art, e.g., outperforming a conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%) in the structural similarity index. Finally, due to the limited availability of labeled data in the field of Earth observation, we gradually reduce the quantity of available labeled data for refining the model to evaluate data efficiency and demonstrate that data can be decreased significantly without affecting the model's accuracy. The pre-trained 100 million parameter model and corresponding fine-tuning workflows have been released publicly as open source contributions to the global Earth sciences community through Hugging Face.",
        "authors": [
            "Johannes Jakubik",
            "Sujit Roy",
            "C. Phillips",
            "P. Fraccaro",
            "Denys Godwin",
            "Bianca Zadrozny",
            "D. Szwarcman",
            "Carlos Gomes",
            "Gabby Nyirjesy",
            "Blair Edwards",
            "Daiki Kimura",
            "Naomi Simumba",
            "Linsong Chu",
            "S. K. Mukkavilli",
            "Devyani Lambhate",
            "Kamal Das",
            "Ranjini Bangalore",
            "Dario Oliveira",
            "Michal Muszynski",
            "Kumar Ankur",
            "Muthukumaran Ramasubramanian",
            "I. Gurung",
            "Sam Khallaghi",
            "Hanxi Li",
            "Michael Cecil",
            "Maryam Ahmadi",
            "Fatemeh Kordi",
            "H. Alemohammad",
            "M. Maskey",
            "R. Ganti",
            "Kommy Weldemariam",
            "Rahul Ramachandran"
        ],
        "citations": 55,
        "references": 59,
        "year": 2023
    },
    {
        "title": "LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models",
        "abstract": "Foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more foundation models have become publicly available.However, most of those models exhibit a major deficiency in specialized-domain and specialized-task applications, where the step of domain- and task-aware finetuning is still required to obtain scientific language models. As the number of available foundation models and specialized tasks keeps growing, the job of training scientific language models becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the domain- and task-aware finetuning of general foundation models.LMFlow offers a complete finetuning workflow for a foundation model to support specialized training with limited computing resources.Furthermore, it supports continuous pretraining, instruction tuning, parameter-efficient finetuning, alignment tuning, inference acceleration, long context generalization, model customization, and even multimodal finetuning, along with carefully designed and extensible APIs. This toolkit has been thoroughly tested and is available at https://github.com/OptimalScale/LMFlow.",
        "authors": [
            "Shizhe Diao",
            "Rui Pan",
            "Hanze Dong",
            "Kashun Shum",
            "Jipeng Zhang",
            "Wei Xiong",
            "T. Zhang"
        ],
        "citations": 59,
        "references": 71,
        "year": 2023
    },
    {
        "title": "A Survey of Reasoning with Foundation Models",
        "abstract": "Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI.",
        "authors": [
            "Jiankai Sun",
            "Chuanyang Zheng",
            "E. Xie",
            "Zhengying Liu",
            "Ruihang Chu",
            "Jianing Qiu",
            "Jiaqi Xu",
            "Mingyu Ding",
            "Hongyang Li",
            "Mengzhe Geng",
            "Yue Wu",
            "Wenhai Wang",
            "Junsong Chen",
            "Zhangyue Yin",
            "Xiaozhe Ren",
            "Jie Fu",
            "Junxian He",
            "Wu Yuan",
            "Qi Liu",
            "Xihui Liu",
            "Yu Li",
            "Hao Dong",
            "Yu Cheng",
            "Ming Zhang",
            "P. Heng",
            "Jifeng Dai",
            "Ping Luo",
            "Jingdong Wang",
            "Jingwei Wen",
            "Xipeng Qiu",
            "Yi-Chen Guo",
            "Hui Xiong",
            "Qun Liu",
            "Zhenguo Li"
        ],
        "citations": 46,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Towards Geospatial Foundation Models via Continual Pretraining",
        "abstract": "Geospatial technologies are becoming increasingly essential in our world for a wide range of applications, including agriculture, urban planning, and disaster response. To help improve the applicability and performance of deep learning models on these geospatial tasks, various works have begun investigating foundation models for this domain. Researchers have explored two prominent approaches for introducing such models in geospatial applications, but both have drawbacks in terms of limited performance benefit or prohibitive training cost. Therefore, in this work, we propose a novel paradigm for building highly effective geospatial foundation models with minimal resource cost and carbon impact. We first construct a compact yet diverse dataset from multiple sources to promote feature diversity, which we term GeoPile. Then, we investigate the potential of continual pretraining from large-scale ImageNet-22k models and propose a multi-objective continual pretraining paradigm, which leverages the strong representations of ImageNet while simultaneously providing the freedom to learn valuable in-domain features. Our approach outperforms previous state-of-the-art geospatial pretraining methods in an extensive evaluation on seven downstream datasets covering various tasks such as change detection, classification, multi-label classification, semantic segmentation, and super-resolution. Code is available at https://github.com/mmendiet/GFM.",
        "authors": [
            "Mat'ias Mendieta",
            "Boran Han",
            "Xingjian Shi",
            "Yi Zhu",
            "Chen Chen",
            "Mu Li"
        ],
        "citations": 44,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Compositional Foundation Models for Hierarchical Planning",
        "abstract": "To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks.",
        "authors": [
            "Anurag Ajay",
            "Seung-Jun Han",
            "Yilun Du",
            "Shaung Li",
            "Abhishek Gupta",
            "T. Jaakkola",
            "Josh Tenenbaum",
            "L. Kaelbling",
            "Akash Srivastava",
            "Pulkit Agrawal"
        ],
        "citations": 44,
        "references": 39,
        "year": 2023
    },
    {
        "title": "SLM: Bridge the Thin Gap Between Speech and Text Foundation Models",
        "abstract": "We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1% (156M) of the foundation models’ parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as automatic speech recognition (ASR) and automatic speech translation (AST), but also unlocks the novel capability of zero-shot instruction-following for more diverse tasks. Given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering. Our approach demonstrates that the representational gap between pretrained speech and language models is narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already present in foundation models of different modalities.",
        "authors": [
            "Mingqiu Wang",
            "Wei Han",
            "Izhak Shafran",
            "Zelin Wu",
            "Chung-Cheng Chiu",
            "Yuan Cao",
            "Yongqiang Wang",
            "Nanxin Chen",
            "Yu Zhang",
            "H. Soltau",
            "P. Rubenstein",
            "Lukás Zilka",
            "Dian Yu",
            "Zhong Meng",
            "G. Pundak",
            "Nikhil Siddhartha",
            "J. Schalkwyk",
            "Yonghui Wu"
        ],
        "citations": 47,
        "references": 43,
        "year": 2023
    },
    {
        "title": "SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding",
        "abstract": "The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that absorbs their expertise. Our method integrates techniques of multitask learning, continual learning, and distillation. Further, it demands significantly less computational cost compared to traditional multi-task training from scratch, and it only needs a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we obtain SAM-CLIP : a unified model that combines the capabilities of SAM and CLIP into a single vision transformer. Compared with deploying SAM and CLIP independently, our merged model, SAM-CLIP, reduces storage and compute costs for inference, making it well-suited for edge device applications. We show that SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also introduces synergistic functionalities, notably in zero-shot semantic segmentation, where SAM-CLIP establishes new state-of-the-art results on 5 benchmarks. It outperforms previous models that are specifically designed for this task by a large margin, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.",
        "authors": [
            "Haoxiang Wang",
            "Pavan Kumar Anasosalu Vasu",
            "Fartash Faghri",
            "Raviteja Vemulapalli",
            "Mehrdad Farajtabar",
            "Sachin Mehta",
            "Mohammad Rastegari",
            "Oncel Tuzel",
            "Hadi Pouransari"
        ],
        "citations": 44,
        "references": 103,
        "year": 2023
    },
    {
        "title": "Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models",
        "abstract": "Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment. In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits. We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision. Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings. The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets. This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models. The code is available at https://github.com/ggjy/vision_weak_to_strong.",
        "authors": [
            "Jianyuan Guo",
            "Hanting Chen",
            "Chengcheng Wang",
            "Kai Han",
            "Chang Xu",
            "Yunhe Wang"
        ],
        "citations": 9,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning",
        "abstract": "Vision-based robot policy learning, which maps visual inputs to actions, necessitates a holistic understanding of diverse visual tasks beyond single-task needs like classification or segmentation. Inspired by this, we introduce Theia, a vision foundation model for robot learning that distills multiple off-the-shelf vision foundation models trained on varied vision tasks. Theia's rich visual representations encode diverse visual knowledge, enhancing downstream robot learning. Extensive experiments demonstrate that Theia outperforms its teacher models and prior robot learning models using less training data and smaller model sizes. Additionally, we quantify the quality of pre-trained visual representations and hypothesize that higher entropy in feature norm distributions leads to improved robot learning performance. Code, models, and demo are available at https://theia.theaiinstitute.com.",
        "authors": [
            "Jinghuan Shang",
            "Karl Schmeckpeper",
            "Brandon B. May",
            "M. Minniti",
            "Tarik Kelestemur",
            "David Watkins",
            "Laura Herlant"
        ],
        "citations": 9,
        "references": 74,
        "year": 2024
    },
    {
        "title": "Evaluating the Utilities of Foundation Models in Single-cell Data Analysis",
        "abstract": "Foundation Models (FMs) have made significant strides in both industrial and scientific domains. In this paper, we evaluate the performance of FMs for single-cell sequencing data analysis through comprehensive experiments across eight downstream tasks pertinent to single-cell data. Overall, the top FMs include scGPT, Geneformer, and CellPLM by considering model performances and user accessibility among ten single-cell FMs. However, by comparing these FMs with task-specific methods, we found that single-cell FMs may not consistently excel than task-specific methods in all tasks, which challenges the necessity of developing foundation models for single-cell analysis. In addition, we evaluated the effects of hyper-parameters, initial settings, and stability for training single-cell FMs based on a proposed scEval framework, and provide guidelines for pre-training and fine-tuning, to enhance the performances of single-cell FMs. Our work summarizes the current state of single-cell FMs, points to their constraints and avenues for future development, and offers a freely available evaluation pipeline to benchmark new models and improve method development.",
        "authors": [
            "Tianyu Liu",
            "Kexing Li",
            "Yuge Wang",
            "Hongyu Li",
            "Hongyu Zhao"
        ],
        "citations": 9,
        "references": 149,
        "year": 2024
    },
    {
        "title": "Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees",
        "abstract": "Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values. For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making. This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion. It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution. Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor. It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy. Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data. En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor.",
        "authors": [
            "Yu Gui",
            "Ying Jin",
            "Zhimei Ren"
        ],
        "citations": 9,
        "references": 57,
        "year": 2024
    },
    {
        "title": "AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models",
        "abstract": "AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning. Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundation models with just three lines of code. Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation. Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes.",
        "authors": [
            "Zhiqiang Tang",
            "Haoyang Fang",
            "Su Zhou",
            "Taojiannan Yang",
            "Zihan Zhong",
            "Tony Hu",
            "Katrin Kirchhoff",
            "George Karypis"
        ],
        "citations": 9,
        "references": 127,
        "year": 2024
    },
    {
        "title": "Participation in the age of foundation models",
        "abstract": "Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of services, from banking to healthcare. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to historically marginalized groups. The larger scale and domain-agnostic manner in which these models operate further heightens the stakes: any errors or harms are liable to reoccur across use cases. In AI & ML more broadly, participatory approaches hold promise to lend agency and decision-making power to marginalized stakeholders, leading to systems that better benefit justice through equitable and distributed governance. But existing approaches in participatory AI/ML are typically grounded in a specific application and set of relevant stakeholders, and it is not straightforward how to apply these lessons to the context of foundation models. Our paper aims to fill this gap. First, we examine existing attempts at incorporating participation into foundation models. We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable. In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation. In addition to the “foundation” layer, our framework proposes the “subfloor” layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain such as clinical care, journalism, or finance, and the “surface” (or application) layer, in which affected communities shape the use of a foundation model for a specific downstream task. The intermediate “subfloor” layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention. At the same time, it avoids duplicative effort by scaling input across relevant use cases. Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer.",
        "authors": [
            "Harini Suresh",
            "Emily Tseng",
            "Meg Young",
            "Mary L. Gray",
            "Emma Pierson",
            "Karen Levy"
        ],
        "citations": 9,
        "references": 106,
        "year": 2024
    },
    {
        "title": "PhilEO Bench: Evaluating Geo-Spatial Foundation Models",
        "abstract": "Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at multiple n-shots and convergence rates.",
        "authors": [
            "Casper Fibaek",
            "Luke Camilleri",
            "Andreas Luyts",
            "Nikolaos Dionelis",
            "B. L. Saux"
        ],
        "citations": 9,
        "references": 38,
        "year": 2024
    },
    {
        "title": "Weaver: Foundation Models for Creative Writing",
        "abstract": "This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.",
        "authors": [
            "Tiannan Wang",
            "Jiamin Chen",
            "Qingrui Jia",
            "Shuai Wang",
            "Ruoyu Fang",
            "Huilin Wang",
            "Zhaowei Gao",
            "Chunzhao Xie",
            "Chuou Xu",
            "Jihong Dai",
            "Yibin Liu",
            "Jialong Wu",
            "Shengwei Ding",
            "Long Li",
            "Zhiwei Huang",
            "Xinle Deng",
            "Teng Yu",
            "Gangan Ma",
            "Han Xiao",
            "Z. Chen",
            "Danjun Xiang",
            "Yunxia Wang",
            "Yuanyuan Zhu",
            "Yichen Xiao",
            "Jing Wang",
            "Yiru Wang",
            "Siran Ding",
            "Jiayang Huang",
            "Jiayi Xu",
            "Yilihamujiang Tayier",
            "Zhenyu Hu",
            "Yuan Gao",
            "Chengfeng Zheng",
            "Yu-Jie Ye",
            "Yihan Li",
            "Lei Wan",
            "Xinyue Jiang",
            "Yujie Wang",
            "Siyuan Cheng",
            "Zhule Song",
            "Xiangru Tang",
            "Xiaohua Xu",
            "Ningyu Zhang",
            "Huajun Chen",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou"
        ],
        "citations": 12,
        "references": 61,
        "year": 2024
    },
    {
        "title": "A Novel Scenarios Engineering Methodology for Foundation Models in Metaverse",
        "abstract": "Foundation models are used to train a broad system of general data to build adaptations to new bottlenecks. Typically, they contain hundreds of billions of hyperparameters that have been trained with hundreds of gigabytes of data. However, this type of black-box vulnerability places foundation models at risk of data poisoning attacks that are designed to pass on misinformation or purposely introduce machine bias. Moreover, ordinary researchers have not been able to completely participate due to the rise in deployment standards. This study introduces the theoretical framework of scenarios engineering (SE) for building accessible and reliable foundation models in metaverse, namely, “SE-enabled foundation models in metaverse.” Particularly, the research framework comprises a six-layer architecture (infrastructure layer, operation layer, knowledge layer, intelligence layer, management layer, and interaction layer), which can provide controllability, trustworthiness, and interactivity for the foundation models in metaverse. This creates closed-loop, virtual–real, and human–machine environments that provides the best indices and goals for the foundation models, which allows us to fully validate and calibrate the corresponding models. Then, examples of use cases from the automotive industry are listed to provide transparency on the possible use and benefits of our approach. Finally, the open research topics of related frameworks are discussed.",
        "authors": [
            "Xuan Li",
            "Yonglin Tian",
            "Peijun Ye",
            "Haibin Duan",
            "Fei-yue Wang"
        ],
        "citations": 71,
        "references": 68,
        "year": 2023
    },
    {
        "title": "On the Challenges and Perspectives of Foundation Models for Medical Image Analysis",
        "abstract": "This article discusses the opportunities, applications and future directions of large-scale pretrained models, i.e., foundation models, which promise to significantly improve the analysis of medical images. Medical foundation models have immense potential in solving a wide range of downstream tasks, as they can help to accelerate the development of accurate and robust models, reduce the dependence on large amounts of labeled data, preserve the privacy and confidentiality of patient data. Specifically, we illustrate the \"spectrum\" of medical foundation models, ranging from general imaging models, modality-specific models, to organ/task-specific models, and highlight their challenges, opportunities and applications. We also discuss how foundation models can be leveraged in downstream medical tasks to enhance the accuracy and efficiency of medical image analysis, leading to more precise diagnosis and treatment decisions.",
        "authors": [
            "Shaoting Zhang",
            "Dimitris N. Metaxas"
        ],
        "citations": 83,
        "references": 88,
        "year": 2023
    },
    {
        "title": "GEO-Bench: Toward Foundation Models for Earth Monitoring",
        "abstract": "Recent progress in self-supervision has shown that pre-training large neural networks on vast amounts of unsupervised data can lead to substantial increases in generalization to downstream tasks. Such models, recently coined foundation models, have been transformational to the field of natural language processing. Variants have also been proposed for image data, but their applicability to remote sensing tasks is limited. To stimulate the development of foundation models for Earth monitoring, we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation. We accompany this benchmark with a robust methodology for evaluating models and reporting aggregated results to enable a reliable assessment of progress. Finally, we report results for 20 baselines to gain information about the performance of existing models. We believe that this benchmark will be a driver of progress across a variety of Earth monitoring tasks.",
        "authors": [
            "Alexandre Lacoste",
            "Nils Lehmann",
            "Pau Rodríguez López",
            "Evan D. Sherwin",
            "H. Kerner",
            "Bjorn Lutjens",
            "J. Irvin",
            "David Dao",
            "H. Alemohammad",
            "Alexandre Drouin",
            "Mehmet Gunturkun",
            "Gabriel Huang",
            "David Vázquez",
            "Dava Newman",
            "Y. Bengio",
            "Stefano Ermon",
            "Xiao Xiang Zhu"
        ],
        "citations": 37,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Towards Label-free Scene Understanding by Vision Foundation Models",
        "abstract": "Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4\\% and 33.5\\% mIoU on ScanNet, improving 4.7\\% and 7.9\\%, respectively. For nuImages and nuScenes datasets, the performance is 22.1\\% and 26.8\\% with improvements of 3.5\\% and 6.0\\%, respectively. Code is available. (https://github.com/runnanchen/Label-Free-Scene-Understanding).",
        "authors": [
            "Runnan Chen",
            "You-Chen Liu",
            "Lingdong Kong",
            "Nenglun Chen",
            "Xinge Zhu",
            "Yuexin Ma",
            "Tongliang Liu",
            "Wenping Wang"
        ],
        "citations": 38,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Equivariant Similarity for Vision-Language Foundation Models",
        "abstract": "This study explores the concept of equivariance in vision-language foundation models (VLMs), focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. Unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched pairs as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. This allows VLMs to generalize better to nuanced and unseen multimodal compositions. However, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. For example, given an image-text pair about a dog, it is unclear to what extent the similarity changes when the pixel is changed from dog to cat? To this end, we propose EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we present a new challenging benchmark EqBen. Compared to the existing evaluation sets, EqBen is the first to focus on \"visual-minimal change\". Extensive experiments show the lack of equivariance in current VLMs1 and validate the effectiveness of EqSim2.",
        "authors": [
            "Tan Wang",
            "Kevin Lin",
            "Linjie Li",
            "Chung-Ching Lin",
            "Zhengyuan Yang",
            "Hanwang Zhang",
            "Zicheng Liu",
            "Lijuan Wang"
        ],
        "citations": 41,
        "references": 83,
        "year": 2023
    },
    {
        "title": "Acceptable Use Policies for Foundation Models",
        "abstract": "As foundation models have accumulated hundreds of millions of users, developers have begun to take steps to prevent harmful types of uses. One salient intervention that foundation model developers adopt is acceptable use policies—legally binding policies that prohibit users from using a model for specific purposes. This paper identifies acceptable use policies from 30 foundation model developers, analyzes the use restrictions they contain, and argues that acceptable use policies are an important lens for understanding the regulation of foundation models. Taken together, developers’ acceptable use policies include 127 distinct use restrictions; the wide variety in the number and type of use restrictions may create fragmentation across the AI supply chain. Companies also employ acceptable use policies to prevent competitors or specific industries from making use of their models. Developers alone decide what constitutes acceptable use, and rarely provide transparency about how they enforce their policies. In practice, acceptable use policies are difficult to enforce, and scrupulous enforcement can act as a barrier to researcher access and limit beneficial uses of foundation models. Acceptable use policies for foundation models are an early example of self-regulation that have a significant impact on the market for foundation models and the AI ecosystem.",
        "authors": [
            "Kevin Klyman"
        ],
        "citations": 6,
        "references": 174,
        "year": 2024
    },
    {
        "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
        "abstract": "We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models.",
        "authors": [
            "Musashi Hinck",
            "M. L. Olson",
            "David Cobbley",
            "Shao-Yen Tseng",
            "Vasudev Lal"
        ],
        "citations": 8,
        "references": 24,
        "year": 2024
    },
    {
        "title": "A comprehensive survey on pretrained foundation models: a history from BERT to ChatGPT",
        "abstract": null,
        "authors": [
            "Ce Zhou",
            "Qian Li",
            "Chen Li",
            "Jun Yu",
            "Yixin Liu",
            "Guangjing Wang",
            "Kai Zhang",
            "Cheng Ji",
            "Qiben Yan",
            "Lifang He",
            "Hao Peng",
            "Jianxin Li",
            "Jia Wu",
            "Ziwei Liu",
            "Pengtao Xie",
            "Caiming Xiong",
            "Jianfeng Pei",
            "Philip S. Yu",
            "Lichao Sun"
        ],
        "citations": 12,
        "references": 139,
        "year": 2024
    },
    {
        "title": "Towards Foundation Models for Knowledge Graph Reasoning",
        "abstract": "Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.",
        "authors": [
            "Mikhail Galkin",
            "Xinyu Yuan",
            "Hesham Mostafa",
            "Jian Tang",
            "Zhaocheng Zhu"
        ],
        "citations": 31,
        "references": 69,
        "year": 2023
    },
    {
        "title": "FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models",
        "abstract": "Recent works on generalizable NeRFs have shown promising results on novel view synthesis from single or few images. However, such models have rarely been applied on other downstream tasks beyond synthesis such as semantic understanding and parsing. In this paper, we propose a novel framework named FeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision foundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D pre-trained foundation models to 3D space via neural rendering, and then extract deep features for 3D query points from NeRF MLPs. Consequently, it allows to map 2D images to continuous 3D semantic feature volumes, which can be used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D semantic keypoint transfer and 2D/3D object part segmentation. Our extensive experiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D semantic feature extractor. Our project page is available at https://jianglongye.com/featurenerf/.",
        "authors": [
            "Jianglong Ye",
            "Naiyan Wang",
            "X. Wang"
        ],
        "citations": 32,
        "references": 73,
        "year": 2023
    },
    {
        "title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations",
        "abstract": "Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse. Finally, we present two prompting techniques, $\\textit{IsoCombination}$ and $\\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations.",
        "authors": [
            "Deqing Fu",
            "Ruohao Guo",
            "Ghazal Khalighinejad",
            "Ollie Liu",
            "Bhuwan Dhingra",
            "Dani Yogatama",
            "Robin Jia",
            "W. Neiswanger"
        ],
        "citations": 8,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models",
        "abstract": "Vision-and-Language Navigation (VLN) has gained increasing attention over recent years and many approaches have emerged to advance their development. The remarkable achievements of foundation models have shaped the challenges and proposed methods for VLN research. In this survey, we provide a top-down review that adopts a principled framework for embodied planning and reasoning, and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges. We hope our in-depth discussions could provide valuable resources and insights: on one hand, to milestone the progress and explore opportunities and potential roles for foundation models in this field, and on the other, to organize different challenges and solutions in VLN to foundation model researchers.",
        "authors": [
            "Yue Zhang",
            "Ziqiao Ma",
            "Jialu Li",
            "Yanyuan Qiao",
            "Zun Wang",
            "Joyce Chai",
            "Qi Wu",
            "Mohit Bansal",
            "Parisa Kordjamshidi"
        ],
        "citations": 8,
        "references": 220,
        "year": 2024
    },
    {
        "title": "State Space Models as Foundation Models: A Control Theoretic Overview",
        "abstract": "In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.",
        "authors": [
            "Carmen Amo Alonso",
            "Jerome Sieber",
            "M. Zeilinger"
        ],
        "citations": 8,
        "references": 27,
        "year": 2024
    },
    {
        "title": "SegmentNT: annotating the genome at single-nucleotide resolution with DNA foundation models",
        "abstract": "Foundation models have achieved remarkable success in several fields such as natural language processing, computer vision and more recently biology. DNA foundation models in particular are emerging as a promising approach for genomics. However, so far no model has delivered granular, nucleotide-level predictions across a wide range of genomic and regulatory elements, limiting their practical usefulness. In this paper, we build on our previous work on the Nucleotide Transformer (NT) to develop a segmentation model, SegmentNT, that processes input DNA sequences up to 30kb-long to predict 14 different classes of genomic elements at single nucleotide resolution. By utilizing pre-trained weights from NT, SegmentNT surpasses the performance of several ablation models, including convolution networks with one-hot encoded nucleotide sequences and models trained from scratch. SegmentNT can process multiple sequence lengths with zero-shot generalization for sequences of up to 50kb. We show improved performance on the detection of splice sites throughout the genome and demonstrate strong nucleotide-level precision. Because it evaluates all gene elements simultaneously, SegmentNT can predict the impact of sequence variants not only on splice site changes but also on exon and intron rearrangements in transcript isoforms. Finally, we show that a SegmentNT model trained on human genomic elements can generalize to elements of different human and plant species and that a trained multispecies SegmentNT model achieves stronger generalization for all genic elements on unseen species. In summary, SegmentNT demonstrates that DNA foundation models can tackle complex, granular tasks in genomics at a single-nucleotide resolution. SegmentNT can be easily extended to additional genomic elements and species, thus representing a new paradigm on how we analyze and interpret DNA. We make our SegmentNT-30kb human and multispecies models available on our github repository in Jax and HuggingFace space in Pytorch.",
        "authors": [
            "B. P. de Almeida",
            "Hugo Dalla-torre",
            "Guillaume Richard",
            "Christopher Blum",
            "Lorenz Hexemer",
            "Maxence Gélard",
            "Javier Mendoza-Revilla",
            "Priyanka Pandey",
            "Stefan Laurent",
            "Marie Lopez",
            "Alexandre Laterre",
            "Maren Lang",
            "U. Sahin",
            "Karim Beguir",
            "Thomas Pierrot"
        ],
        "citations": 8,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Foundation models are platform models: Prompting and the political economy of AI",
        "abstract": "A recent innovation in the field of machine learning has been the creation of very large pre-trained models, also referred to as ‘foundation models’, that draw on much larger and broader sets of data than typical deep learning systems and can be applied to a wide variety of tasks. Underpinning text-based systems such as OpenAI's ChatGPT and image generators such as Midjourney, these models have received extraordinary amounts of public attention, in part due to their reliance on prompting as the main technique to direct and apply them. This paper thus uses prompting as an entry point into the critical study of foundation models and their implications. The paper proceeds as follows: In the first section, we introduce foundation models in more detail, outline some of the main critiques, and present our general approach. We then discuss prompting as an algorithmic technique, show how it makes foundation models programmable, and explain how it enables different audiences to use these models as (computational) platforms. In the third section, we link the material properties of the technologies under scrutiny to questions of political economy, discussing, in turn, deep user interactions, reordered cost structures, and centralization and lock-in. We conclude by arguing that foundation models and prompting further strengthen Big Tech's dominance over the field of computing and, through their broad applicability, many other economic sectors, challenging our capacities for critical appraisal and regulatory response.",
        "authors": [
            "Sarah Burkhardt",
            "Bernhard Rieder"
        ],
        "citations": 8,
        "references": 22,
        "year": 2024
    },
    {
        "title": "Foundation models in ophthalmology",
        "abstract": "Foundation models represent a paradigm shift in artificial intelligence (AI), evolving from narrow models designed for specific tasks to versatile, generalisable models adaptable to a myriad of diverse applications. Ophthalmology as a specialty has the potential to act as an exemplar for other medical specialties, offering a blueprint for integrating foundation models broadly into clinical practice. This review hopes to serve as a roadmap for eyecare professionals seeking to better understand foundation models, while equipping readers with the tools to explore the use of foundation models in their own research and practice. We begin by outlining the key concepts and technological advances which have enabled the development of these models, providing an overview of novel training approaches and modern AI architectures. Next, we summarise existing literature on the topic of foundation models in ophthalmology, encompassing progress in vision foundation models, large language models and large multimodal models. Finally, we outline major challenges relating to privacy, bias and clinical validation, and propose key steps forward to maximise the benefit of this powerful technology.",
        "authors": [
            "Mark A. Chia",
            "F. Antaki",
            "Yukun Zhou",
            "A. Turner",
            "Aaron Y. Lee",
            "P. Keane"
        ],
        "citations": 6,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Foundation Models for Music: A Survey",
        "abstract": "In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.",
        "authors": [
            "Ying-Chao Ma",
            "Anders Oland",
            "Anton Ragni",
            "B. M. D. Sette",
            "C. Saitis",
            "Chris Donahue",
            "Chenghua Lin",
            "Christos Plachouras",
            "Emmanouil Benetos",
            "Elio Quinton",
            "Elona Shatri",
            "Fabio Morreale",
            "Ge Zhang",
            "Gyorgy Fazekas",
            "Gus G. Xia",
            "Huan Zhang",
            "Ilaria Manco",
            "Jiawen Huang",
            "Julien Guinot",
            "Liwei Lin",
            "Luca Marinelli",
            "Max W. Y. Lam",
            "Megha Sharma",
            "Qiuqiang Kong",
            "Roger B. Dannenberg",
            "Ruibin Yuan",
            "Shangda Wu",
            "Shih-Lun Wu",
            "Shu-Yuan Dai",
            "Shunwei Lei",
            "Shiyin Kang",
            "Simon Dixon",
            "Wenhu Chen",
            "Wehhao Huang",
            "Xin Du",
            "Xingwei Qu",
            "Xu Tan",
            "Yizhi Li",
            "Zeyue Tian",
            "Zhi-Xin Wu",
            "Zhizheng Wu",
            "Ziyang Ma",
            "Ziyu Wang"
        ],
        "citations": 8,
        "references": 0,
        "year": 2024
    },
    {
        "title": "A Clinical Benchmark of Public Self-Supervised Pathology Foundation Models",
        "abstract": "The use of self-supervised learning (SSL) to train pathology foundation models has increased substantially in the past few years. Notably, several models trained on large quantities of clinical data have been made publicly available in recent months. This will significantly enhance scientific research in computational pathology and help bridge the gap between research and clinical deployment. With the increase in availability of public foundation models of different sizes, trained using different algorithms on different datasets, it becomes important to establish a benchmark to compare the performance of such models on a variety of clinically relevant tasks spanning multiple organs and diseases. In this work, we present a collection of pathology datasets comprising clinical slides associated with clinically relevant endpoints including cancer diagnoses and a variety of biomarkers generated during standard hospital operation from two medical centers. We leverage these datasets to systematically assess the performance of public pathology foundation models and provide insights into best practices for training new foundation models and selecting appropriate pretrained models.",
        "authors": [
            "Gabriele Campanella",
            "Shengjia Chen",
            "Ruchika Verma",
            "Jennifer Zeng",
            "A. Stock",
            "Matthew Croken",
            "Brandon Veremis",
            "Abdulkadir Elmas",
            "Kuan-lin Huang",
            "Ricky Kwan",
            "Jane Houldsworth",
            "Adam J. Schoenfeld",
            "Chad M. Vanderbilt"
        ],
        "citations": 7,
        "references": 25,
        "year": 2024
    },
    {
        "title": "Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models",
        "abstract": "The integration of machine learning techniques has become a cornerstone in the development of intelligent urban services, significantly contributing to the enhancement of urban efficiency, sustainability, and overall livability. Recent advancements in foundational models, such as ChatGPT, have introduced a paradigm shift within the fields of machine learning and artificial intelligence. These models, with their exceptional capacity for contextual comprehension, problem-solving, and task adaptability, present a transformative opportunity to reshape the future of smart cities and drive progress toward Urban General Intelligence (UGI). Despite increasing attention to Urban Foundation Models (UFMs), this rapidly evolving field faces critical challenges, including the lack of clear definitions, systematic reviews, and universalizable solutions. To address these issues, this paper first introduces the definition and concept of UFMs and highlights the distinctive challenges involved in their development. Furthermore, we present a data-centric taxonomy that classifies existing research on UFMs according to the various urban data modalities and types. In addition, we propose a prospective framework designed to facilitate the realization of versatile UFMs, aimed at overcoming the identified challenges and driving further progress in this field. Finally, this paper explores the wide-ranging applications of UFMs within urban contexts, illustrating their potential to significantly impact and transform urban systems. A comprehensive collection of relevant research papers and open-source resources have been collated and are continuously updated at: https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.",
        "authors": [
            "Weijiao Zhang",
            "Jindong Han",
            "Zhao Xu",
            "Hang Ni",
            "Hao Liu",
            "Hui Xiong"
        ],
        "citations": 8,
        "references": 385,
        "year": 2024
    },
    {
        "title": "OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine",
        "abstract": "The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models. It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data. Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab.",
        "authors": [
            "Xiaosong Wang",
            "Xiaofan Zhang",
            "Guotai Wang",
            "Junjun He",
            "Zhongyu Li",
            "Wentao Zhu",
            "Yi Guo",
            "Qi Dou",
            "Xiaoxiao Li",
            "Dequan Wang",
            "Liang Hong",
            "Qicheng Lao",
            "Tong Ruan",
            "Yukun Zhou",
            "Yixue Li",
            "Jie Zhao",
            "Kang Li",
            "Xin Sun",
            "Lifeng Zhu",
            "Shaoting Zhang"
        ],
        "citations": 6,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Dual-Personalizing Adapter for Federated Foundation Models",
        "abstract": "Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning diverse instruction data. Notably, federated foundation models (FedFM) emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to FedFM for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications, and conventional methods for test-time distribution shifts in personalized FL are less effective for FedFM due to their failure to adapt to complex distribution shift scenarios and the requirement to train all parameters. To bridge this gap, we refine the setting in FedFM, termed test-time personalization, which aims to learn personalized federated foundation models on clients while effectively handling test-time distribution shifts simultaneously. To address challenges in this setting, we explore a simple yet effective solution, a Federated Dual-Personalizing Adapter (FedDPA) architecture. By co-working with a foundation model, a global adapter and a local adapter jointly tackle the test-time distribution shifts and client-specific personalization. Additionally, we introduce an instance-wise dynamic weighting mechanism that dynamically integrates the global and local adapters for each test instance during inference, facilitating effective test-time personalization. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks.",
        "authors": [
            "Yiyuan Yang",
            "Guodong Long",
            "Taoshu Shen",
            "Jing Jiang",
            "Michael Blumenstein"
        ],
        "citations": 6,
        "references": 44,
        "year": 2024
    },
    {
        "title": "Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models",
        "abstract": "Low-Rank Adaptation (LoRA) emerges as a popular parameter-efficient fine-tuning (PEFT) method, which proposes to freeze pretrained model weights and update an additive low-rank trainable matrix. In this work, we study the enhancement of LoRA training by introducing an $r \\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. We theoretically verify that the proposed preconditioner stabilizes feature learning with LoRA under infinite-width NN setting. Empirically, the implementation of this new preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with this new preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. The new preconditioner can be derived from a novel Riemannian metric in low-rank matrix field. Code can be accessed at https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.",
        "authors": [
            "Fangzhao Zhang",
            "Mert Pilanci"
        ],
        "citations": 10,
        "references": 35,
        "year": 2024
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "abstract": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under a shared format. Additionally, we propose a parameter-efficient training method for foundation models, which involves freezing the P5 backbone and fine-tuning lightweight adapters, resulting in improved recommendation performance and increased efficiency in terms of training time and memory usage. Code and data of VIP5 are available at https://github.com/jeykigung/VIP5.",
        "authors": [
            "Shijie Geng",
            "Juntao Tan",
            "Shuchang Liu",
            "Zuohui Fu",
            "Yongfeng Zhang"
        ],
        "citations": 56,
        "references": 96,
        "year": 2023
    },
    {
        "title": "On the Adversarial Robustness of Multi-Modal Foundation Models",
        "abstract": "Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images $\\left({{\\varepsilon _\\infty } = 1/255}\\right)$ in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model. Note: This paper contains fake information to illustrate the outcome of our attacks. It does not reflect the opinion of the authors.",
        "authors": [
            "Christian Schlarmann",
            "Matthias Hein"
        ],
        "citations": 58,
        "references": 42,
        "year": 2023
    },
    {
        "title": "TrafficGPT: Viewing, processing and interacting with traffic foundation models",
        "abstract": null,
        "authors": [
            "Siyao Zhang",
            "Daocheng Fu",
            "Wenzhe Liang",
            "Zhao Zhang",
            "Bin Yu",
            "Pinlong Cai",
            "Baozhen Yao"
        ],
        "citations": 11,
        "references": 26,
        "year": 2024
    },
    {
        "title": "Augmenting research methods with foundation models and generative AI",
        "abstract": null,
        "authors": [
            "Sippo Rossi",
            "Matti Rossi",
            "R. Mukkamala",
            "J. Thatcher",
            "Yogesh K. Dwivedi"
        ],
        "citations": 10,
        "references": 37,
        "year": 2024
    },
    {
        "title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning",
        "abstract": "We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Many models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingually-trained models have not attained “balanced multilingual” capabilities. Our endeavors underscore the need for more generalizable semantic representations and enhanced multilingual contextualization. SeaEval can serve as a launchpad for more thorough investigations and evaluations for multilingual and multicultural scenarios.",
        "authors": [
            "Bin Wang",
            "Zhengyuan Liu",
            "Xin Huang",
            "Fangkai Jiao",
            "Yang Ding",
            "A. Aw",
            "Nancy F. Chen"
        ],
        "citations": 50,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting",
        "abstract": "Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization. However, despite the success of foundation models in modalities such as natural language processing and computer vision, the development of foundation models for time series forecasting has lagged behind. We present Lag-Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates. Lag-Llama is pretrained on a large corpus of diverse time series data from several domains, and demonstrates strong zero-shot generalization capabilities compared to a wide range of forecasting models on downstream datasets across domains. Moreover, when fine-tuned on relatively small fractions of such previously unseen datasets, Lag-Llama achieves state-of-the-art performance, outperforming prior deep learning approaches, emerging as the best general-purpose model on average. Lag-Llama serves as a strong contender to the current state-of-art in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.",
        "authors": [
            "Kashif Rasul",
            "Arjun Ashok",
            "Andrew Robert Williams",
            "Arian Khorasani",
            "George Adamopoulos",
            "Rishika Bhagwatkar",
            "Marin Bilovs",
            "Hena Ghonia",
            "N. Hassen",
            "Anderson Schneider",
            "Sahil Garg",
            "Alexandre Drouin",
            "Nicolas Chapados",
            "Yuriy Nevmyvaka",
            "I. Rish"
        ],
        "citations": 30,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill",
        "abstract": "Zero-shot object navigation is a challenging task for home-assistance robots. This task emphasizes visual grounding, commonsense inference and locomotion abilities, where the first two are inherent in foundation models. But for the locomotion part, most works still depend on map-based planning approaches. The gap between RGB space and map space makes it difficult to directly transfer the knowledge from foundation models to navigation tasks. In this work, we propose a Pixel-guided Navigation skill (PixNav), which bridges the gap between the foundation models and the embodied navigation task. It is straightforward for recent foundation models to indicate an object by pixels, and with pixels as the goal specification, our method becomes a versatile navigation policy towards all different kinds of objects. Besides, our PixNav is a pure RGB-based policy that can reduce the cost of homeassistance robots. Experiments demonstrate the robustness of the PixNav which achieves 80+% success rate in the local path-planning task. To perform long-horizon object navigation, we design an LLM-based planner to utilize the commonsense knowledge between objects and rooms to select the best waypoint. Evaluations across both photorealistic indoor simulators and real-world environments validate the effectiveness of our proposed navigation strategy. More details are accessible via our project website https://sites.google.com/view/pixnav/.",
        "authors": [
            "Wenzhe Cai",
            "Siyuan Huang",
            "Guangran Cheng",
            "Yuxing Long",
            "Peng Gao",
            "Changyin Sun",
            "Hao Dong"
        ],
        "citations": 28,
        "references": 40,
        "year": 2023
    },
    {
        "title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models",
        "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively. Supplementary materials such as audio samples are provided at our demo website: https://v2a-mapper.github.io/.",
        "authors": [
            "Heng Wang",
            "Jianbo Ma",
            "Santiago Pascual",
            "Richard Cartwright",
            "Weidong (Tom) Cai"
        ],
        "citations": 24,
        "references": 68,
        "year": 2023
    },
    {
        "title": "A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models",
        "abstract": "We study the task of zero-shot vision-and-language navigation (ZS-VLN), a practical yet challenging problem in which an agent learns to navigate following a path described by language instructions without requiring any path-instruction annotation data. Normally, the instructions have complex grammatical structures and often contain various action descriptions (e.g.,\"proceed beyond\",\"depart from\"). How to correctly understand and execute these action demands is a critical problem, and the absence of annotated data makes it even more challenging. Note that a well-educated human being can easily understand path instructions without the need for any special training. In this paper, we propose an action-aware zero-shot VLN method ($A^2$Nav) by exploiting the vision-and-language ability of foundation models. Specifically, the proposed method consists of an instruction parser and an action-aware navigation policy. The instruction parser utilizes the advanced reasoning ability of large language models (e.g., GPT-3) to decompose complex navigation instructions into a sequence of action-specific object navigation sub-tasks. Each sub-task requires the agent to localize the object and navigate to a specific goal position according to the associated action demand. To accomplish these sub-tasks, an action-aware navigation policy is learned from freely collected action-specific datasets that reveal distinct characteristics of each action demand. We use the learned navigation policy for executing sub-tasks sequentially to follow the navigation instruction. Extensive experiments show $A^2$Nav achieves promising ZS-VLN performance and even surpasses the supervised learning methods on R2R-Habitat and RxR-Habitat datasets.",
        "authors": [
            "Peihao Chen",
            "Xinyu Sun",
            "Hongyan Zhi",
            "Runhao Zeng",
            "Thomas H. Li",
            "Gaowen Liu",
            "Mingkui Tan",
            "Chuang Gan"
        ],
        "citations": 25,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis",
        "abstract": "Building general-purpose robots that operate seamlessly in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. However, as a community, we have been constraining most robotic systems by designing them for specific tasks, training them on specific datasets, and deploying them within specific environments. These systems require extensively-labeled data and task-specific models. When deployed in real-world scenarios, such systems face several generalization issues and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of general-purpose robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing a generalized formulation of how foundation models are used in robotics, and the fundamental barriers to making generalist robots universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository 2 of resources, including papers reviewed in this survey, as well as related projects and repositories for developing foundation models for robotics.",
        "authors": [
            "Yafei Hu",
            "Quanting Xie",
            "Vidhi Jain",
            "Jonathan Francis",
            "Jay Patrikar",
            "Nikhil Varma Keetha",
            "Seungchan Kim",
            "Yaqi Xie",
            "Tianyi Zhang",
            "Shibo Zhao",
            "Yu Quan Chong",
            "Chen Wang",
            "Katia P. Sycara",
            "Matthew Johnson-Roberson",
            "Dhruv Batra",
            "Xiaolong Wang",
            "Sebastian Scherer",
            "Z. Kira",
            "Fei Xia",
            "Yonatan Bisk"
        ],
        "citations": 42,
        "references": 373,
        "year": 2023
    },
    {
        "title": "Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models",
        "abstract": "Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in many domains. In this paper, we propose the Federated Foundation Models (FFMs) paradigm, which combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple end-users. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further outline potential future research avenues in FFM, including FFM pre-training, FFM fine-tuning, and federated prompt tuning, which allow the development of more personalized and context-aware models while ensuring data privacy. Moreover, we explore the possibility of continual/lifelong learning in FFMs, as increased computational power at the edge may unlock the potential for optimizing FMs using newly generated private data close to the data source. The proposed FFM concepts offer a flexible and scalable framework for training large language models in a privacy-preserving manner, setting the stage for subsequent advancements in both FM training and federated learning.",
        "authors": [
            "Sixing Yu",
            "J. P. Muñoz",
            "A. Jannesari"
        ],
        "citations": 37,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives",
        "abstract": "Recent decisions by leading AI labs to either open-source their models or to restrict access to their models has sparked debate about whether, and how, increasingly capable AI models should be shared. Open-sourcing in AI typically refers to making model architecture and weights freely and publicly accessible for anyone to modify, study, build on, and use. This offers advantages such as enabling external oversight, accelerating progress, and decentralizing control over AI development and use. However, it also presents a growing potential for misuse and unintended consequences. This paper offers an examination of the risks and benefits of open-sourcing highly capable foundation models. While open-sourcing has historically provided substantial net benefits for most software and AI development processes, we argue that for some highly capable foundation models likely to be developed in the near future, open-sourcing may pose sufficiently extreme risks to outweigh the benefits. In such a case, highly capable foundation models should not be open-sourced, at least not initially. Alternative strategies, including non-open-source model sharing options, are explored. The paper concludes with recommendations for developers, standard-setting bodies, and governments for establishing safe and responsible model sharing practices and preserving open-source benefits where safe.",
        "authors": [
            "Elizabeth Seger",
            "Noemi Dreksler",
            "Richard Moulange",
            "Emily Dardaman",
            "Jonas Schuett",
            "K. Wei",
            "Christoph Winter",
            "Mackenzie Arnold",
            "Anton Korinek",
            "Markus Anderljung",
            "Ben Bucknall",
            "Alan Chan",
            "Eoghan Stafford",
            "Leonie Koessler",
            "Aviv Ovadya",
            "Ben Garfinkel",
            "Emma Bluemke",
            "Michael Aird",
            "Patrick Levermore",
            "Julian Hazell",
            "Abhishek Gupta",
            "Andrew Trask",
            "Ben Cottier",
            "Herbie Bradley",
            "Irene Solaiman",
            "Norman Johnson",
            "Peter Cihon",
            "S. Avin",
            "Stella Biderman"
        ],
        "citations": 34,
        "references": 218,
        "year": 2023
    },
    {
        "title": "ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps",
        "abstract": "Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT represents a landmark achievement in this research paradigm, offering hope for general artificial intelligence due to its highly intelligent natural language understanding ability. However, the PHM field lacks a consensus on how to respond to this significant change in the AI field, and a systematic review and roadmap is required to elucidate future development directions. To fill this gap, this paper systematically expounds on the key components and latest developments of LSF-Models. Then, we systematically answered how to build the LSF-Model applicable to PHM tasks and outlined the challenges and future development roadmaps for this research paradigm.",
        "authors": [
            "Yanfang Li",
            "Huan Wang",
            "Muxia Sun"
        ],
        "citations": 32,
        "references": 305,
        "year": 2023
    },
    {
        "title": "A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models",
        "abstract": "—Prompt engineering is a technique that involves augmenting a large pre-trained model with task-speciﬁc hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been well-studied in natural language processing. Recently, it has also been intensively studied in vision-language modeling. However, there is currently a lack of a systematic overview of prompt engineering on pre-trained vision-language models. This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models ( e.g., Flamingo), image-text matching models ( e.g., CLIP), and text-to-image generation models ( e.g., Stable Diffusion). For each type of model, a brief model summary, prompting methods, prompting-based applications, and the corresponding responsibility and integrity issues are summarized and discussed. Furthermore, the commonalities and differences between prompting on vision-language models, language models, and vision models are also discussed. The challenges, future directions, and research opportunities are summarized to foster future research on this topic.",
        "authors": [
            "Jindong Gu",
            "Zhen Han",
            "Shuo Chen",
            "Ahmad Beirami",
            "Bailan He",
            "Gengyuan Zhang",
            "Ruotong Liao",
            "Yao Qin",
            "Volker Tresp",
            "Philip H. S. Torr"
        ],
        "citations": 105,
        "references": 216,
        "year": 2023
    },
    {
        "title": "Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT",
        "abstract": "ChatGPT has shown the potential of emerging general artificial intelligence capabilities, as it has demonstrated competent performance across many natural language processing tasks. In this work, we evaluate the capabilities of ChatGPT to perform text classification on three affective computing problems, namely, big-five personality prediction, sentiment analysis, and suicide tendency detection. We utilize three baselines, a robust language model (RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and a simple bag-of-words (BoW) baseline. Results show that the RoBERTa model trained for a specific downstream task generally has a superior performance. On the other hand, ChatGPT provides decent results and is relatively comparable to the Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy data, where the Word2Vec model achieves worse results due to noise. Results indicate that ChatGPT is a good generalist model that is capable of achieving good results across various problems without any specialized training; however, it is not as good as a specialized model for a downstream task.",
        "authors": [
            "Mostafa M. Amin",
            "E. Cambria",
            "Björn Schuller",
            "E. Cambria"
        ],
        "citations": 67,
        "references": 34,
        "year": 2023
    },
    {
        "title": "π-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation",
        "abstract": "Foundation models have achieved great advances in multi-task learning with a unified interface of unimodal and multimodal tasks. However, the potential of such multi-task learners has not been exploited during transfer learning. In this work, we present a universal parameter-efficient transfer learning method, termed Predict-Interpolate Tuning ($\\pi$-Tuning), for vision, language, and vision-language tasks. It aggregates the parameters of lightweight task-specific experts learned from similar tasks to aid the target downstream task. The task similarities are predicted in a unified modality-independent space, yielding a scalable graph to demonstrate task relationships. $\\pi$-Tuning has several appealing benefits. First, it flexibly explores both intra- and inter-modal transferability between similar tasks to improve the accuracy and robustness of transfer learning, especially in data-scarce scenarios. Second, it offers a systematical solution for transfer learning with multi-task prediction-and-then-interpolation, compatible with diverse types of parameter-efficient experts, such as prompt and adapter. Third, an extensive study of task-level mutual benefits on 14 unimodal and 6 multimodal datasets shows that $\\pi$-Tuning surpasses fine-tuning and other parameter-efficient transfer learning methods both in full-shot and low-shot regimes. The task graph also enables an in-depth interpretable analysis of task transferability across modalities. The code will be available at https://github.com/TencentARC/pi-Tuning.",
        "authors": [
            "Chengyue Wu",
            "Teng Wang",
            "Yixiao Ge",
            "Zeyu Lu",
            "Rui-Zhi Zhou",
            "Ping Luo",
            "Ying Shan"
        ],
        "citations": 33,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Q-Instruct: Improving Low-Level Visual Abilities for Multi-Modality Foundation Models",
        "abstract": "Multi-modality large language models (MLLMs), as represented by GPT-4V, have introduced a paradigm shift for visual perception and understanding tasks, that a variety of abilities can be achieved within one foundation model. While current MLLMs demonstrate primary low-level visual abilities from the identification of low-level visual attributes (e.g., clarity, brightness) to the evaluation on image quality, there's still an imperative to further improve the accuracy of MLLMs to substantially alleviate human burdens. To address this, we collect the first dataset consisting of human natural language feedback on low-level vision. Each feedback offers a comprehensive description of an image's low-level visual attributes, culminating in an overall quality assessment. The constructed Q-Pathway dataset includes 58K detailed human feedbacks on 18,973 multi-sourced images with diverse low-level appearance. To ensure MLLMs can adeptly handle diverse queries, we further propose a GPT-participated transformation to convert these feedbacks into a rich set of 200K instruction-response pairs, termed Q-Instruct. Experimental results indicate that the Q-Instruct consistently elevates various low-level visual capabilities across multiple base models. We anticipate that our datasets can pave the way for a future that foundation models can assist humans on low-level visual tasks.",
        "authors": [
            "Haoning Wu",
            "Zicheng Zhang",
            "Erli Zhang",
            "Chaofeng Chen",
            "Liang Liao",
            "Annan Wang",
            "Kaixin Xu",
            "Chunyi Li",
            "Jingwen Hou",
            "Guangtao Zhai",
            "Geng Xue",
            "Wenxiu Sun",
            "Qiong Yan",
            "Weisi Lin"
        ],
        "citations": 50,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Towards A Unified Agent with Foundation Models",
        "abstract": "Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.",
        "authors": [
            "Norman Di Palo",
            "Arunkumar Byravan",
            "Leonard Hasenclever",
            "Markus Wulfmeier",
            "N. Heess",
            "Martin A. Riedmiller"
        ],
        "citations": 51,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Adversarial Prompting for Black Box Foundation Models",
        "abstract": "Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to signiﬁcant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce speciﬁc behaviors into the generative process, such as generating images of a particular object or biasing the frequency of speciﬁc letters in the generated text.",
        "authors": [
            "N. Maus",
            "Patrick Chao",
            "Eric Wong",
            "Jacob R. Gardner"
        ],
        "citations": 45,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models",
        "abstract": "As autonomous driving technology matures, end-to-end methodologies have emerged as a leading strategy, promising seamless integration from perception to control via deep learning. However, existing systems grapple with challenges such as unexpected open set environments and the complexity of black-box models. At the same time, the evolution of deep learning introduces larger, multimodal foundational models, offering multi-modal visual and textual understanding. In this paper, we harness these multimodal foundation models to enhance the robustness and adaptability of autonomous driving systems. We introduce a method to extract nuanced spatial features from transformers and the incorporation of latent space simulation for improved training and policy debugging. We use pixel/patch-aligned feature descriptors to expand foundational model capabilities to create an end-to-end multimodal driving model, demonstrating unparalleled results in diverse tests. Our solution combines language with visual perception and achieves significantly greater robustness on out-of-distribution situations.",
        "authors": [
            "Tsun-Hsuan Wang",
            "Alaa Maalouf",
            "Wei Xiao",
            "Yutong Ban",
            "Alexander Amini",
            "G. Rosman",
            "S. Karaman",
            "Daniela Rus"
        ],
        "citations": 29,
        "references": 51,
        "year": 2023
    },
    {
        "title": "TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",
        "abstract": "With the promotion of chatgpt to the public, Large language models indeed showcase remarkable common sense, reasoning, and planning skills, frequently providing insightful guidance. These capabilities hold significant promise for their application in urban traffic management and control. However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges. In parallel, specialized traffic foundation models exist but are typically designed for specific tasks with limited input-output interactions. Combining these models with LLMs presents an opportunity to enhance their capacity for tackling complex traffic-related problems and providing insightful suggestions. To bridge this gap, we present TrafficGPT, a fusion of ChatGPT and traffic foundation models. This integration yields the following key enhancements: 1) empowering ChatGPT with the capacity to view, analyze, process traffic data, and provide insightful decision support for urban transportation system management; 2) facilitating the intelligent deconstruction of broad and complex tasks and sequential utilization of traffic foundation models for their gradual completion; 3) aiding human decision-making in traffic control through natural language dialogues; and 4) enabling interactive feedback and solicitation of revised outcomes. By seamlessly intertwining large language model and traffic expertise, TrafficGPT not only advances traffic management but also offers a novel approach to leveraging AI capabilities in this domain. The TrafficGPT demo can be found in https://github.com/lijlansg/TrafficGPT.git.",
        "authors": [
            "Siyao Zhang",
            "Daocheng Fu",
            "Zhao Zhang",
            "Bin Yu",
            "Pinlong Cai"
        ],
        "citations": 30,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation",
        "abstract": "In this paper, we first assess and harness various Vision Foundation Models (VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS). Driven by the motivation that Leveraging Stronger pre-trained models and Fewer trainable parameters for Superior generalizability, we introduce a robust fine-tuning approach, namely “Rein”, to parameter-efficiently harness VFMs for DGSS. Built upon a set of trainable tokens, each linked to distinct instances, Rein precisely refines and forwards the feature maps from each layer to the next layer within the backbone. This process produces diverse refinements for different categories within a single image. With fewer trainable parameters, Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full parameter fine-tuning. Extensive experiments across various settings demonstrate that Rein significantly outperforms state-of-the-art methods. Remarkably, with just an extra 1% of trainable parameters within the frozen backbone, Rein achieves a mIoU of 78.4% on the Cityscapes, without accessing any real urban-scene datasets. Code is available at https://github.com/w1oves/Rein.git.",
        "authors": [
            "Zhixiang Wei",
            "Lin Chen",
            "Yi Jin",
            "Xiaoxiao Ma",
            "Tianle Liu",
            "Pengyang Lin",
            "Ben Wang",
            "H. Chen",
            "Jinjin Zheng"
        ],
        "citations": 22,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Foundation Models Meet Visualizations: Challenges and Opportunities",
        "abstract": "Recent studies have indicated that foundation models, such as BERT and GPT, excel at adapting to various downstream tasks. This adaptability has made them a dominant force in building artificial intelligence (AI) systems. Moreover, a new research paradigm has emerged as visualization techniques are incorporated into these models. This study divides these intersections into two research areas: visualization for foundation model (VIS4FM) and foundation model for visualization (FM4VIS). In terms of VIS4FM, we explore the primary role of visualizations in understanding, refining, and evaluating these intricate foundation models. VIS4FM addresses the pressing need for transparency, explainability, fairness, and robustness. Conversely, in terms of FM4VIS, we highlight how foundation models can be used to advance the visualization field itself. The intersection of foundation models with visualizations is promising but also introduces a set of challenges. By highlighting these challenges and promising opportunities, this study aims to provide a starting point for the continued exploration of this research avenue.",
        "authors": [
            "Weikai Yang",
            "Mengchen Liu",
            "Zheng Wang",
            "Shixia Liu"
        ],
        "citations": 29,
        "references": 130,
        "year": 2023
    },
    {
        "title": "Towards foundation models of biological image segmentation",
        "abstract": null,
        "authors": [
            "Jun Ma",
            "Bo Wang"
        ],
        "citations": 29,
        "references": 16,
        "year": 2023
    },
    {
        "title": "EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models",
        "abstract": "While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaptation. Our model and dataset are available via a research data use agreement from the Stanford AIMI Center. Code to reproduce our results are available at our Github repo: https://github.com/som-shahlab/ehrshot-benchmark",
        "authors": [
            "Michael Wornow",
            "Rahul Thapa",
            "E. Steinberg",
            "J. Fries",
            "N. Shah"
        ],
        "citations": 21,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Scaling Laws for Sparsely-Connected Foundation Models",
        "abstract": "We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e.,\"foundation models\"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data, which we validate empirically across model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the\"optimal sparsity\", the sparsity level which yields the best performance for a given effective model size and training budget. For a fixed number of non-zero parameters, we identify that the optimal sparsity increases with the amount of data used for training. We also extend our study to different sparsity structures (such as the hardware-friendly n:m pattern) and strategies (such as starting from a pretrained dense model). Our findings shed light on the power and limitations of weight sparsity across various parameter and computational settings, offering both theoretical understanding and practical implications for leveraging sparsity towards computational efficiency improvements.",
        "authors": [
            "Elias Frantar",
            "C. Riquelme",
            "N. Houlsby",
            "Dan Alistarh",
            "Utku Evci"
        ],
        "citations": 21,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Foundation Model Engineering: Engineering Foundation Models Just as Engineering Software",
        "abstract": "By treating data and models as the source code, Foundation Models (FMs) become a new type of software. Mirroring the concept of software crisis, the increasing complexity of FMs making FM crisis a tangible concern in the coming decade, appealing for new theories and methodologies from the field of software engineering. In this paper, we outline our vision of introducing Foundation Model (FM) engineering, a strategic response to the anticipated FM crisis with principled engineering methodologies. FM engineering aims to mitigate potential issues in FM development and application through the introduction of declarative, automated, and unified programming interfaces for both data and model management, reducing the complexities involved in working with FMs by providing a more structured and intuitive process for developers. Through the establishment of FM engineering, we aim to provide a robust, automated, and extensible framework that addresses the imminent challenges, and discovering new research opportunities for the software engineering field.",
        "authors": [
            "Dezhi Ran",
            "Mengzhou Wu",
            "Wei Yang",
            "Tao Xie"
        ],
        "citations": 1,
        "references": 55,
        "year": 2024
    },
    {
        "title": "Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models",
        "abstract": "Foundation models have achieved remarkable results in 2D and language tasks like image segmentation, object detection, and visual-language understanding. However, their potential to enrich 3D scene representation learning is largely untapped due to the existence of the domain gap. In this work, we propose an innovative methodology called Bridge3D to address this gap by pre-training 3D models using features, semantic masks, and captions sourced from foundation models. Specifically, our method employs semantic masks from foundation models to guide the masking and reconstruction process for the masked autoencoder, enabling more focused attention on foreground representations. Moreover, we bridge the 3D-text gap at the scene level using image captioning foundation models, thereby facilitating scene-level knowledge distillation. We further extend this bridging effort by introducing an innovative object-level knowledge distillation method that harnesses highly accurate object-level masks and semantic text data from foundation models. Our methodology significantly surpasses the performance of existing state-of-the-art methods in 3D object detection and semantic segmentation tasks. For instance, on the ScanNet dataset, Bridge3D improves the baseline by a notable margin of 6.3%. Code will be available at: https://github.com/Zhimin-C/Bridge3D",
        "authors": [
            "Zhimin Chen",
            "Bing Li"
        ],
        "citations": 23,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots",
        "abstract": ": Improving the generalization capabilities of general-purpose robotic agents has long been a signiﬁcant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efﬁciency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efﬁcient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach consists of two distinct steps. First, we introduce a series of foundation models to accurately ground natural language demands across multiple tasks. Second, we develop a Multi-modal Multi-view Policy Model that incorporates inputs such as RGB images, semantic masks, and robot proprioception states to jointly predict precise and executable robot actions. Extensive real-world experiments conducted on a Franka Emika robot arm validate the effectiveness of our proposed paradigm. Real-world demos are shown in YouTube/Bilibili.",
        "authors": [
            "Jiange Yang",
            "Wenhui Tan",
            "Chuhao Jin",
            "Bei Liu",
            "Jianlong Fu",
            "Ruihua Song",
            "Limin Wang"
        ],
        "citations": 24,
        "references": 53,
        "year": 2023
    },
    {
        "title": "CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks",
        "abstract": "Distributed training of foundation models, especially large language models (LLMs), is communication-intensive and so has heavily relied on centralized data centers with fast interconnects. Can we train on slow networks and unlock the potential of decentralized infrastructure for foundation models? In this paper, we propose C OCKTAIL SGD, a novel communication-efficient training framework that combines three distinct compression techniques—random sparsification, top-K sparsification, and quantization—to achieve much greater compression than each individual technique alone. We justify the benefit of such a hybrid approach through a theoretical analysis of convergence. Empirically, we show that C OCKTAIL SGD achieves up to 117 × compression in fine-tuning LLMs up to 20 billion parameters without hurting convergence. On a 500Mbps network, C OCKTAIL SGD only incurs ∼ 1 . 2 × slowdown compared with data center networks.",
        "authors": [
            "Jue Wang",
            "Yucheng Lu",
            "Binhang Yuan",
            "Beidi Chen",
            "Percy Liang",
            "Christopher De Sa",
            "Christopher Ré",
            "Ce Zhang"
        ],
        "citations": 23,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Ecosystem Graphs: The Social Footprint of Foundation Models",
        "abstract": "Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful abstraction and interface for achieving the minimum transparency required to address myriad use cases. Therefore, we envision Ecosystem Graphs will be a community-maintained resource that provides value to stakeholders spanning AI researchers, industry professionals, social scientists, auditors and policymakers.",
        "authors": [
            "Rishi Bommasani",
            "Dilara Soylu",
            "Thomas Liao",
            "Kathleen A. Creel",
            "Percy Liang"
        ],
        "citations": 26,
        "references": 107,
        "year": 2023
    },
    {
        "title": "A Framework and Operational Procedures for Metaverses-Based Industrial Foundation Models",
        "abstract": "Industrial processes are typical cyber–physical–social systems (CPSSs), where the effective management of employees and the efficient control of machines play important roles. Traditional industries heavily rely on human labor and neglect the development of collection–utilization–transmission integrated information loops, thereby leading to high costs and low efficiency in operational procedures. To facilitate the natural interactions and smart operations for humans and machines, industrial foundation models (IFMs) based on metaverses are proposed in this article, serving as the operating systems of industrial parallel machines that provide sustainable data resources and scenarios for management and control experiments. On this basis, IFM comprised of vision foundation models, language foundation models, as well as operational foundation models, are constructed to manage resources in industrial parallel machines and provides comprehensive services for industrial procedures. On the one hand, IFM can efficiently manage various resources including computing power, digital assets, enterprise resources, and platform I/O via the proposed CPSS-based competing, sharing, scheduling, monitoring, allocating, and recovering mechanisms. On the other hand, imaginative intelligence, linguistic intelligence, and algorithmic intelligence can be achieved through vivid visualization of vision foundation models, natural conversations of language foundation models, and smart manipulation of operational foundation models. With the proposed IFM, cyber–physical–social intelligence (CPSI) can be achieved to enhance the efficient management and control of industrial processes.",
        "authors": [
            "Jiangong Wang",
            "Yonglin Tian",
            "Yutong Wang",
            "Jing Yang",
            "Xingxia Wang",
            "Sanjin Wang",
            "Oliver Kwan"
        ],
        "citations": 24,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Foundation Models for Natural Language Processing: Pre-trained Language Models Integrating Media",
        "abstract": null,
        "authors": [
            "G. Paass",
            "Sven Giesselbach"
        ],
        "citations": 24,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.",
        "authors": [
            "Chris Cummins",
            "Volker Seeker",
            "Dejan Grubisic",
            "Baptiste Rozière",
            "Jonas Gehring",
            "Gabriele Synnaeve",
            "Hugh Leather"
        ],
        "citations": 8,
        "references": 60,
        "year": 2024
    },
    {
        "title": "The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs",
        "abstract": "The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.",
        "authors": [
            "Michael Wornow",
            "Yizhe Xu",
            "Rahul Thapa",
            "Birju S. Patel",
            "E. Steinberg",
            "S. Fleming",
            "M. Pfeffer",
            "Jason Alan Fries",
            "N. Shah"
        ],
        "citations": 25,
        "references": 96,
        "year": 2023
    },
    {
        "title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
        "abstract": "ChatGPT has shown the potential of emerging general artificial intelligence capabilities, as it has demonstrated competent performance across many natural language processing tasks. In this work, we evaluate the capabilities of ChatGPT to perform text classification on three affective computing problems, namely, big-five personality prediction, sentiment analysis, and suicide tendency detection. We utilise three baselines, a robust language model (RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and a simple bag-of-words baseline (BoW). Results show that the RoBERTa trained for a specific downstream task generally has a superior performance. On the other hand, ChatGPT provides decent results, and is relatively comparable to the Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy data, where Word2Vec models achieve worse results due to noise. Results indicate that ChatGPT is a good generalist model that is capable of achieving good results across various problems without any specialised training, however, it is not as good as a specialised model for a downstream task.",
        "authors": [
            "Mostafa M. Amin",
            "E. Cambria",
            "Björn Schuller"
        ],
        "citations": 64,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
        "abstract": "In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. ms-GFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. ms-GFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, under-scoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities. Code can be found at https://github.com/boranhan/Geospatial_Foundation_Models",
        "authors": [
            "Boran Han",
            "Shuai Zhang",
            "Xingjian Shi",
            "M. Reichstein"
        ],
        "citations": 7,
        "references": 64,
        "year": 2024
    },
    {
        "title": "Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning",
        "abstract": "Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks. However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability. Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand. In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning. Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output. We also categorize several state-of-the-art approaches into agent interaction protocols, highlighting the nature and extent of involvement of the various agent roles. The proposed framework provides guidance for future directions to further realize the power of FMs in practical AI systems.",
        "authors": [
            "D. Bhattacharjya",
            "Junkyu Lee",
            "Don Joven Agravante",
            "Balaji Ganesan",
            "Radu Marinescu"
        ],
        "citations": 0,
        "references": 70,
        "year": 2024
    },
    {
        "title": "Pathology Foundation Models",
        "abstract": "Pathology has played a crucial role in the diagnosis and evaluation of patient tissue samples obtained from surgeries and biopsies for many years. The advent of Whole Slide Scanners and the development of deep learning technologies have significantly advanced the field, leading to extensive research and development in pathology AI (Artificial Intelligence). These advancements have contributed to reducing the workload of pathologists and supporting decision-making in treatment plans. Recently, large-scale AI models known as Foundation Models (FMs), which are more accurate and applicable to a wide range of tasks compared to traditional AI, have emerged, and expanded their application scope in the healthcare field. Numerous FMs have been developed in pathology, and there are reported cases of their application in various tasks, such as disease diagnosis, rare cancer diagnosis, patient survival prognosis prediction, biomarker expression prediction, and the scoring of immunohistochemical expression intensity. However, several challenges remain for the clinical application of FMs, which healthcare professionals, as users, must be aware of. Research is ongoing to address these challenges. In the future, it is expected that the development of Generalist Medical AI, which integrates pathology FMs with FMs from other medical domains, will progress, leading to the effective utilization of AI in real clinical settings to promote precision and personalized medicine.",
        "authors": [
            "Mieko Ochi",
            "D. Komura",
            "Shumpei Ishikawa"
        ],
        "citations": 0,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation",
        "abstract": "Medical image analysis plays an important role in clinical diagnosis. In this paper, we examine the recent Segment Anything Model (SAM) on medical images, and report both quantitative and qualitative zero-shot segmentation results on nine medical image segmentation benchmarks, covering various imaging modalities, such as optical coherence tomography (OCT), magnetic resonance imaging (MRI), and computed tomography (CT), as well as different applications including dermatology, ophthalmology, and radiology. Those benchmarks are representative and commonly used in model development. Our experimental results indicate that while SAM presents remarkable segmentation performance on images from the general domain, its zero-shot segmentation ability remains restricted for out-of-distribution images, e.g., medical images. In addition, SAM exhibits inconsistent zero-shot segmentation performance across different unseen medical domains. For certain structured targets, e.g., blood vessels, the zero-shot segmentation of SAM completely failed. In contrast, a simple fine-tuning of it with a small amount of data could lead to remarkable improvement of the segmentation quality, showing the great potential and feasibility of using fine-tuned SAM to achieve accurate medical image segmentation for a precision diagnostics. Our study indicates the versatility of generalist vision foundation models on medical imaging, and their great potential to achieve desired performance through fine-turning and eventually address the challenges associated with accessing large and diverse medical datasets in support of clinical diagnostics.",
        "authors": [
            "Peilun Shi",
            "Jianing Qiu",
            "Sai Mu Dalike Abaxi",
            "Hao Wei",
            "F. P. Lo",
            "Wu Yuan"
        ],
        "citations": 74,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Foundation Models for Biomedical Image Segmentation: A Survey",
        "abstract": "Recent advancements in biomedical image analysis have been significantly driven by the Segment Anything Model (SAM). This transformative technology, originally developed for general-purpose computer vision, has found rapid application in medical image processing. Within the last year, marked by over 100 publications, SAM has demonstrated its prowess in zero-shot learning adaptations for medical imaging. The fundamental premise of SAM lies in its capability to segment or identify objects in images without prior knowledge of the object type or imaging modality. This approach aligns well with tasks achievable by the human visual system, though its application in non-biological vision contexts remains more theoretically challenging. A notable feature of SAM is its ability to adjust segmentation according to a specified resolution scale or area of interest, akin to semantic priming. This adaptability has spurred a wave of creativity and innovation in applying SAM to medical imaging. Our review focuses on the period from April 1, 2023, to September 30, 2023, a critical first six months post-initial publication. We examine the adaptations and integrations of SAM necessary to address longstanding clinical challenges, particularly in the context of 33 open datasets covered in our analysis. While SAM approaches or achieves state-of-the-art performance in numerous applications, it falls short in certain areas, such as segmentation of the carotid artery, adrenal glands, optic nerve, and mandible bone. Our survey delves into the innovative techniques where SAM's foundational approach excels and explores the core concepts in translating and applying these models effectively in diverse medical imaging scenarios.",
        "authors": [
            "Ho Hin Lee",
            "Yu Gu",
            "Theodore Zhao",
            "Yanbo Xu",
            "Jianwei Yang",
            "Naoto Usuyama",
            "Cliff Wong",
            "Mu-Hsin Wei",
            "Bennett A. Landman",
            "Yuankai Huo",
            "Alberto Santamaría-Pang",
            "Hoifung Poon"
        ],
        "citations": 8,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Molecular causality in the advent of foundation models",
        "abstract": null,
        "authors": [
            "Sebastian Lobentanzer",
            "P. Rodríguez-Mier",
            "Stefan Bauer",
            "Julio Saez-Rodriguez"
        ],
        "citations": 6,
        "references": 94,
        "year": 2024
    },
    {
        "title": "The Promises and Perils of Foundation Models in Dermatology.",
        "abstract": null,
        "authors": [
            "Haiwen Gui",
            "J. Omiye",
            "Crystal T. Chang",
            "Roxana Daneshjou"
        ],
        "citations": 6,
        "references": 43,
        "year": 2024
    },
    {
        "title": "Verifiably Following Complex Robot Instructions with Foundation Models",
        "abstract": "Enabling mobile robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), an approach that enables robots to verifiably follow expressive and complex open-ended instructions in real-world environments without prebuilt semantic maps. LIMP constructs a symbolic instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that are correct-by-construction. We perform a large scale evaluation and demonstrate our approach on 150 instructions in five real-world environments showing the generality of our approach and the ease of deployment in novel unstructured domains. In our experiments, LIMP performs comparably with state-of-the-art LLM task planners and LLM code-writing planners on standard open vocabulary tasks and additionally achieves 79\\% success rate on complex spatiotemporal instructions while LLM and Code-writing planners both achieve 38\\%. See supplementary materials and demo videos at https://robotlimp.github.io",
        "authors": [
            "Benedict Quartey",
            "Eric Rosen",
            "Stefanie Tellex",
            "G. Konidaris"
        ],
        "citations": 7,
        "references": 126,
        "year": 2024
    },
    {
        "title": "Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior",
        "abstract": "Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We also find that fine-tuning these models yields more performance gains as model size increases, compared to training from scratch on new downstream tasks. These results hold for a broad range of PDE learning tasks. All in all, our results demonstrate the potential of the\"pre-train and fine-tune\"paradigm for SciML problems, demonstrating a path towards building SciML foundation models. We open-source our code for reproducibility.",
        "authors": [
            "Shashank Subramanian",
            "P. Harrington",
            "K. Keutzer",
            "W. Bhimji",
            "D. Morozov",
            "Michael W. Mahoney",
            "A. Gholami"
        ],
        "citations": 44,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Computational Pathology at Health System Scale - Self-Supervised Foundation Models from Three Billion Images",
        "abstract": "Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and DINO algorithms. We evaluated performance on six clinically relevant tasks from three anatomic sites and two institutions: breast cancer detection, inflammatory bowel disease detection, breast cancer estrogen receptor prediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer immunotherapy response prediction. Our results demonstrate that pre-training on pathology data is beneficial for downstream performance compared to pre-training on natural images. Additionally, the DINO algorithm achieved better generalization performance across all tasks tested. The presented results signify a phase change in computational pathology research, paving the way into a new era of more performant models based on large-scale, parallel pre-training at the billion-image scale.",
        "authors": [
            "Gabriele Campanella",
            "Ricky Kwan",
            "Eugene Fluder",
            "Jennifer Zeng",
            "A. Stock",
            "Brandon Veremis",
            "A. Polydorides",
            "Cyrus Hedvat",
            "Adam J. Schoenfeld",
            "Chad M. Vanderbilt",
            "P. Kovatch",
            "Carlos Cordon-Cardo",
            "Thomas J. Fuchs"
        ],
        "citations": 22,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Large-scale Training of Foundation Models for Wearable Biosignals",
        "abstract": "Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from ~141K participants spanning ~3 years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices $\\unicode{x2013}$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health.",
        "authors": [
            "Salar Abbaspourazad",
            "Oussama Elachqar",
            "Andrew C. Miller",
            "Saba Emrani",
            "Udhyakumar Nallasamy",
            "Ian Shapiro"
        ],
        "citations": 18,
        "references": 49,
        "year": 2023
    },
    {
        "title": "CHORUS: Foundation Models for Unified Data Discovery and Exploration",
        "abstract": "We apply foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMS) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. We investigate the fundamental characteristics of this approach including generalizability to several foundation models and the impact of non-determinism on the outputs. All in all, this suggests a future direction in which disparate data management tasks can be unified under foundation models.",
        "authors": [
            "Moe Kayali",
            "A. Lykov",
            "Ilias Fountalis",
            "N. Vasiloglou",
            "Dan Olteanu",
            "Dan Suciu"
        ],
        "citations": 21,
        "references": 85,
        "year": 2023
    },
    {
        "title": "Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text",
        "abstract": "Knowledge graphs can represent information about the real-world using entities and their relations in a structured and semantically rich manner and they enable a variety of downstream applications such as question-answering, recommendation systems, semantic search, and advanced analytics. However, at the moment, building a knowledge graph involves a lot of manual effort and thus hinders their application in some situations and the automation of this process might benefit especially for small organizations. Automatically generating structured knowledge graphs from a large volume of natural language is still a challenging task and the research on sub-tasks such as named entity extraction, relation extraction, entity and relation linking, and knowledge graph construction aims to improve the state of the art of automatic construction and completion of knowledge graphs from text. The recent advancement of foundation models with billions of parameters trained in a self-supervised manner with large volumes of training data that can be adapted to a variety of downstream tasks has helped to demonstrate high performance on a large range of Natural Language Processing (NLP) tasks. In this context, one emerging paradigm is in-context learning where a language model is used as it is with a prompt that provides instructions and some examples to perform a task without changing the parameters of the model using traditional approaches such as fine-tuning. This way, no computing resources are needed for re-training/fine-tuning the models and the engineering effort is minimal. Thus, it would be beneficial to utilize such capabilities for generating knowledge graphs from text.",
        "authors": [
            "H. Khorashadizadeh",
            "Nandana Mihindukulasooriya",
            "S. Tiwari",
            "Jinghua Groppe",
            "Sven Groppe"
        ],
        "citations": 20,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Assessing the limits of zero-shot foundation models in single-cell biology",
        "abstract": "The advent and success of foundation models such as GPT has sparked growing interest in their application to single-cell biology. Models like Geneformer and scGPT have emerged with the promise of serving as versatile tools for this specialized field. However, the efficacy of these models, particularly in zero-shot settings where models are not fine-tuned but used without any further training, remains an open question, especially as practical constraints require useful models to function in settings that preclude fine-tuning (e.g., discovery settings where labels are not fully known). This paper presents a rigorous evaluation of the zero-shot performance of these proposed single-cell foundation models. We assess their utility in tasks such as cell type clustering and batch effect correction, and evaluate the generality of their pretraining objectives. Our results indicate that both Geneformer and scGPT exhibit limited reliability in zero-shot settings and often underperform compared to simpler methods. These findings serve as a cautionary note for the deployment of proposed single-cell foundation models and highlight the need for more focused research to realize their potential.2",
        "authors": [
            "Kasia Z. Kedzierska",
            "Lorin Crawford",
            "Ava P. Amini",
            "Alex X. Lu"
        ],
        "citations": 22,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Grasp-Anything: Large-scale Grasp Dataset from Foundation Models",
        "abstract": "Foundation models such as ChatGPT have made significant strides in robotic tasks due to their universal representation of real-world domains. In this paper, we leverage foundation models to tackle grasp detection, a persistent challenge in robotics with broad industrial applications. Despite numerous grasp datasets, their object diversity remains limited compared to real-world figures. Fortunately, foundation models possess an extensive repository of real-world knowledge, including objects we encounter in our daily lives. As a consequence, a promising solution to the limited representation in previous grasp datasets is to harness the universal knowledge embedded in these foundation models. We present Grasp-Anything, a new large-scale grasp dataset synthesized from foundation models to implement this solution. Grasp-Anything excels in diversity and magnitude, boasting 1M samples with text descriptions and more than 3M objects, surpassing prior datasets. Empirically, we show that Grasp-Anything successfully facilitates zero-shot grasp detection on vision-based tasks and real-world robotic experiments. Our dataset and code are available at https://airvlab.github.io/grasp-anything/.",
        "authors": [
            "An Vuong",
            "Minh N. Vu",
            "Hieu Le",
            "Baoru Huang",
            "B. Huynh",
            "T. Vo",
            "Andreas Kugi",
            "Anh Nguyen"
        ],
        "citations": 21,
        "references": 71,
        "year": 2023
    },
    {
        "title": "SSL4EO-L: Datasets and Foundation Models for Landsat Imagery",
        "abstract": "The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery captured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Landsat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat family of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4-5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models for Landsat imagery using SSL4EO-L and evaluate their performance on multiple semantic segmentation tasks. All datasets and model weights are available via the TorchGeo (https://github.com/microsoft/torchgeo) library, making reproducibility and experimentation easy, and enabling scientific advancements in the burgeoning field of remote sensing for a multitude of downstream applications.",
        "authors": [
            "A. Stewart",
            "Nils Lehmann",
            "I. Corley",
            "Yi Wang",
            "Yi Chang",
            "Nassim Ait Ali Braham",
            "Shradha Sehgal",
            "Caleb Robinson",
            "Arindam Banerjee"
        ],
        "citations": 20,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment",
        "abstract": "We introduce a method to train vision-language models for remote-sensing images without using any textual annotations. Our key insight is to use co-located internet imagery taken on the ground as an intermediary for connecting remote-sensing images and language. Specifically, we train an image encoder for remote sensing images to align with the image encoder of CLIP using a large amount of paired internet and satellite images. Our unsupervised approach enables the training of a first-of-its-kind large-scale vision language model (VLM) for remote sensing images at two different resolutions. We show that these VLMs enable zero-shot, open-vocabulary image classification, retrieval, segmentation and visual question answering for satellite images. On each of these tasks, our VLM trained without textual annotations outperforms existing VLMs trained with supervision, with gains of up to 20% for classification and 80% for segmentation.",
        "authors": [
            "Utkarsh Mall",
            "Cheng Perng Phoo",
            "Meilin Kelsey Liu",
            "Carl Vondrick",
            "B. Hariharan",
            "Kavita Bala"
        ],
        "citations": 25,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models",
        "abstract": "Remote sensing imagery has attracted significant attention in recent years due to its instrumental role in global environmental monitoring, land usage monitoring, and more. As image databases grow each year, performing automatic segmentation with deep learning models has gradually become the standard approach for processing the data. Despite the improved performance of current models, certain limitations remain unresolved. Firstly, training deep learning models for segmentation requires per-pixel annotations. Given the large size of datasets, only a small portion is fully annotated and ready for training. Additionally, the high intra-dataset variance in remote sensing data limits the transfer learning ability of such models. Although recently proposed generic segmentation models like SAM have shown promising results in zero-shot instance-level segmentation, adapting them to semantic segmentation is a non-trivial task. To tackle these challenges, we propose a novel method named Text2Seg for remote sensing semantic segmentation. Text2Seg overcomes the dependency on extensive annotations by employing an automatic prompt generation process using different visual foundation models (VFMs), which are trained to understand semantic information in various ways. This approach not only reduces the need for fully annotated datasets but also enhances the model's ability to generalize across diverse datasets. Evaluations on four widely adopted remote sensing datasets demonstrate that Text2Seg significantly improves zero-shot prediction performance compared to the vanilla SAM model, with relative improvements ranging from 31% to 225%. Our code is available at https://github.com/Douglas2Code/Text2Seg.",
        "authors": [
            "Jielu Zhang",
            "Zhongliang Zhou",
            "Gengchen Mai",
            "Lan Mu",
            "Mengxuan Hu",
            "Sheng Li"
        ],
        "citations": 38,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Black Box Adversarial Prompting for Foundation Models",
        "abstract": "Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.",
        "authors": [
            "N. Maus",
            "Patrick Chao",
            "Eric Wong",
            "Jacob R. Gardner"
        ],
        "citations": 41,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Efficient Domain Adaptation for Speech Foundation Models",
        "abstract": "Foundation models (FMs), that are trained on broad data at scale and are adaptable to a wide range of downstream tasks, have brought large interest in the research community. Benefiting from the diverse data sources such as different modalities, languages and application domains, foundation models have demonstrated strong generalization and knowledge transfer capabilities. In this paper, we present a pioneering study towards building an efficient solution for FM-based speech recognition systems. We adopt the recently developed self-supervised BEST-RQ for pretraining, and extend the joint training strategy JUST Hydra for finetuning using both source and unsuper-vised target domain data. The FM encoder adapter and decoder are then finetuned to the target domain with a small amount of super-vised in-domain data. On a large-scale YouTube and Voice Search task, our method is shown to be both data and model parameter efficient. It achieves the same quality with only 21.6M supervised in-domain data and 130.8M finetuned parameters, compared to the 731.1M model trained from scratch on additional 300M supervised in-domain data.",
        "authors": [
            "Bo Li",
            "DongSeon Hwang",
            "Zhouyuan Huo",
            "Junwen Bai",
            "Guru Prakash",
            "Tara N. Sainath",
            "K. Sim",
            "Yu Zhang",
            "Wei Han",
            "Trevor Strohman",
            "F. Beaufays"
        ],
        "citations": 19,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Revolutionizing Digital Pathology with the Power of Generative Artificial Intelligence and Foundation Models.",
        "abstract": null,
        "authors": [
            "Asim Waqas",
            "Marilyn M. Bui",
            "E. Glassy",
            "I. E. El Naqa",
            "Piotr Borkowski",
            "Andrew A Borkowski",
            "Ghulam Rasool"
        ],
        "citations": 34,
        "references": 75,
        "year": 2023
    },
    {
        "title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
        "abstract": "Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.",
        "authors": [
            "Tuan Dung Nguyen",
            "Yuan-Sen Ting",
            "I. Ciucă",
            "Charlie O'Neill",
            "Ze-Chang Sun",
            "Maja Jabłońska",
            "S. Kruk",
            "Ernest Perkowski",
            "Jack William Miller",
            "Jason Li",
            "J. Peek",
            "Kartheik G. Iyer",
            "Tomasz R'o.za'nski",
            "P. Khetarpal",
            "Sharaf Zaman",
            "D. Brodrick",
            "Sergio J. Rodr'iguez M'endez",
            "Thang Bui",
            "Alyssa Goodman",
            "A. Accomazzi",
            "J. P. Naiman",
            "Jesse Cranney",
            "K. Schawinski",
            "UniverseTBD"
        ],
        "citations": 15,
        "references": 17,
        "year": 2023
    },
    {
        "title": "MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
        "abstract": "Foundation models have shown outstanding performance and generalization capabilities across domains. Since most studies on foundation models mainly focus on the pretraining phase, a naive strategy to minimize a single task-specific loss is adopted for fine-tuning. However, such fine-tuning methods do not fully leverage other losses that are potentially beneficial for the target task. Therefore, we propose MEta Loss TRansformer (MELTR), a plug-in module that automatically and non-linearly combines various loss functions to aid learning the target task via auxiliary learning. We formulate the auxiliary learning as a bi-level optimization problem and present an efficient optimization algorithm based on Approximate Implicit Differentiation (AID). For evaluation, we apply our framework to various video foundation models (UniVL, Violet and All-in-one), and show significant performance gain on all four downstream tasks: text-to-video retrieval, video question answering, video captioning, and multimodal sentiment analysis. Our qualitative analyses demonstrate that MELTR adequately 'transforms' individual loss functions and 'melts' them into an effective unified loss. Code is available at https://github.com/mlvlab/MELTR.",
        "authors": [
            "Dohwan Ko",
            "Joon-Young Choi",
            "Hyeong Kyu Choi",
            "Kyoung-Woon On",
            "Byungseok Roh",
            "Hyunwoo J. Kim"
        ],
        "citations": 15,
        "references": 91,
        "year": 2023
    },
    {
        "title": "Multimodal Foundation Models Exploit Text to Make Medical Image Predictions",
        "abstract": "Multimodal foundation models have shown compelling but conflicting performance in medical image interpretation. However, the mechanisms by which these models integrate and prioritize different data modalities, including images and text, remain poorly understood. Here, using a diverse collection of 1014 multimodal medical cases, we evaluate the unimodal and multimodal image interpretation abilities of proprietary (GPT-4, Gemini Pro 1.0) and open-source (Llama-3.2-90B, LLaVA-Med-v1.5) multimodal foundational models with and without the use of text descriptions. Across all models, image predictions were largely driven by exploiting text, with accuracy increasing monotonically with the amount of informative text. By contrast, human performance on medical image interpretation did not improve with informative text. Exploitation of text is a double-edged sword; we show that even mild suggestions of an incorrect diagnosis in text diminishes image-based classification, reducing performance dramatically in cases the model could previously answer with images alone. Finally, we conducted a physician evaluation of model performance on long-form medical cases, finding that the provision of images either reduced or had no effect on model performance when text is already highly informative. Our results suggest that multimodal AI models may be useful in medical diagnostic reasoning but that their accuracy is largely driven, for better and worse, by their exploitation of text.",
        "authors": [
            "Thomas A. Buckley",
            "James A. Diao",
            "P. Rajpurkar",
            "Adam Rodman",
            "Arjun K. Manrai"
        ],
        "citations": 15,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models",
        "abstract": "Foundation models like CLIP allow zero-shot transfer on various tasks without additional training data. Yet, the zero-shot performance is less competitive than a fully supervised one. Thus, to enhance the performance, fine-tuning and ensembling are also commonly adopted to better fit the downstream tasks. However, we argue that such prior work has overlooked the inherent biases in foundation models. Due to the highly imbalanced Web-scale training set, these foundation models are inevitably skewed toward frequent semantics, and thus the subsequent fine-tuning or ensembling is still biased. In this study, we systematically examine the biases in foundation models and demonstrate the efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that bias estimation in foundation models is challenging, as most pre-train data cannot be explicitly accessed like in traditional long-tailed classification tasks. To this end, GLA has an optimization-based bias estimation approach for debiasing foundation models. As our work resolves a fundamental flaw in the pre-training, the proposed GLA demonstrates significant improvements across a diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on long-tailed classification. Codes are in \\url{https://github.com/BeierZhu/GLA}.",
        "authors": [
            "Beier Zhu",
            "Kaihua Tang",
            "Qianru Sun",
            "Hanwang Zhang"
        ],
        "citations": 15,
        "references": 63,
        "year": 2023
    },
    {
        "title": "AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models",
        "abstract": "We present AstroCLIP, a strategy to facilitate the construction of astronomical foundation models that bridge the gap between diverse observational modalities. We demonstrate that a cross-modal contrastive learning approach between images and optical spectra of galaxies yields highly informative embeddings of both modalities. In particular, we apply our method on multi-band images and optical spectra from the Dark Energy Spectroscopic Instrument (DESI), and show that: (1) these embeddings are well-aligned between modalities and can be used for accurate cross-modal searches, and (2) these embeddings encode valuable physical information about the galaxies in particular redshift and stellar mass that can be used to achieve competitive zeroand fewshot predictions without further finetuning. Additionally, in the process of developing our approach, we also construct a novel, transformer-based model and pretraining approach for processing galaxy spectra.",
        "authors": [
            "François Lanusse",
            "Liam Parker",
            "Siavash Golkar",
            "M. Cranmer",
            "Alberto Bietti",
            "Michael Eickenberg",
            "G. Krawezik",
            "Michael McCabe",
            "Ruben Ohana",
            "Mariel Pettee",
            "Bruno Régaldo-Saint Blancard",
            "Tiberiu Teşileanu",
            "Kyunghyun Cho",
            "Shirley Ho"
        ],
        "citations": 17,
        "references": 36,
        "year": 2023
    },
    {
        "title": "GFM: Building Geospatial Foundation Models via Continual Pretraining",
        "abstract": "Geospatial technologies are becoming increasingly essential in our world for a wide range of applications, including agriculture, urban planning, and disaster response. To help improve the applicability and performance of deep learning models on these geospatial tasks, various works have begun investigating foundation models for this domain. Researchers have explored two prominent approaches for introducing such models in geospatial applications, but both have drawbacks in terms of limited performance beneﬁt or prohibitive training cost. Therefore, in this work, we propose a novel paradigm for building highly effective geospatial foundation models with minimal resource cost and carbon impact. We ﬁrst construct a compact yet diverse dataset from multiple sources to promote feature diversity, which we term GeoPile. Then, we investigate the potential of continual pretraining from large-scale ImageNet-22k models and propose a multi-objective continual pretraining paradigm, which leverages the strong representations of ImageNet while simultaneously providing the freedom to learn valuable in-domain features. Our approach outperforms previous state-of-the-art geospatial pretraining methods in an extensive evaluation on seven downstream datasets covering various tasks such as change detection, classiﬁcation, multi-label classiﬁcation, semantic segmentation, and super-resolution.",
        "authors": [
            "Mat'ias Mendieta",
            "Boran Han",
            "Xingjian Shi",
            "Yi Zhu",
            "Chen Chen",
            "Mu Li"
        ],
        "citations": 16,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Prompt Federated Learning for Weather Forecasting: Toward Foundation Models on Meteorological Data",
        "abstract": "To tackle the global climate challenge, it urgently needs to develop a collaborative platform for comprehensive weather forecasting on large-scale meteorological data. Despite urgency, heterogeneous meteorological sensors across countries and regions, inevitably causing multivariate heterogeneity and data exposure, become the main barrier. This paper develops a foundation model across regions capable of understanding complex meteorological data and providing weather forecasting. To relieve the data exposure concern across regions, a novel federated learning approach has been proposed to collaboratively learn a brand-new spatio-temporal Transformer-based foundation model across participants with heterogeneous meteorological data. Moreover, a novel prompt learning mechanism has been adopted to satisfy low-resourced sensors' communication and computational constraints. The effectiveness of the proposed method has been demonstrated on classical weather forecasting tasks using three meteorological datasets with multivariate time series.",
        "authors": [
            "Shen Chen",
            "Guodong Long",
            "Tao Shen",
            "Jing Jiang"
        ],
        "citations": 28,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning",
        "abstract": "We introduce Dataset Grouper, a library to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library facilitates the creation of group-structured versions of existing datasets based on user-specified partitions and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper enables large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work, allowing for federated training of language models with hundreds of millions, and even billions, of parameters. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empirical risk minimization methods at this scale, suggesting their utility in downstream personalization and task-specific adaptation. Dataset Grouper is available at https://github.com/google-research/dataset_grouper.",
        "authors": [
            "Zachary B. Charles",
            "Nicole Mitchell",
            "Krishna Pillutla",
            "Michael Reneer",
            "Zachary Garrett"
        ],
        "citations": 18,
        "references": 92,
        "year": 2023
    },
    {
        "title": "Batched Low-Rank Adaptation of Foundation Models",
        "abstract": "Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and a multilingual speech recognition task across 6 languages.",
        "authors": [
            "Yeming Wen",
            "Swarat Chaudhuri"
        ],
        "citations": 16,
        "references": 57,
        "year": 2023
    },
    {
        "title": "A Deep Dive into Single-Cell RNA Sequencing Foundation Models",
        "abstract": "Large-scale foundation models, which are pre-trained on massive, unlabeled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and biology. In this paper, we explore two foundation models recently developed for single-cell RNA sequencing data, scBERT and scGPT. Focusing on the fine-tuning task of cell type annotation, we explore the relative performance of pre-trained models compared to a simple baseline, L1-regularized logistic regression, including in the few-shot setting. We perform ablation studies to understand whether pretraining improves model performance and to better understand the difficulty of the pre-training task in scBERT. Finally, using scBERT as an example, we demonstrate the potential sensitivity of fine-tuning to hyperparameter settings and parameter initializations. Taken together, our results highlight the importance of rigorously testing foundation models against well established baselines, establishing challenging fine-tuning tasks on which to benchmark foundation models, and performing deep introspection into the embeddings learned by the model in order to more effectively harness these models to transform single-cell data analysis. Code is available at https://github.com/clinicalml/sc-foundation-eval.",
        "authors": [
            "Rebecca Boiarsky",
            "Nalini M. Singh",
            "Alejandro Buendia",
            "Gad Getz",
            "David Sontag"
        ],
        "citations": 15,
        "references": 25,
        "year": 2023
    },
    {
        "title": "DIME-FM : DIstilling Multimodal and Efficient Foundation Models",
        "abstract": "Large Vision-Language Foundation Models (VLFM), such as CLIP, ALIGN and Florence, are trained on large-scale datasets of image-caption pairs and achieve superior transferability and robustness on downstream tasks, but they are difficult to use in many practical applications due to their large size, high latency and fixed architectures. Unfortunately, recent work shows training a small custom VLFM for resource-limited applications is currently very difficult using public and smaller-scale data. In this paper, we introduce a new distillation mechanism (DIME-FM) that allows us to transfer the knowledge contained in large VLFMs to smaller, customized foundation models using a relatively small amount of inexpensive, unpaired images and sentences. We transfer the knowledge from the pre-trained CLIP-ViT-L/14 model to a ViT-B/32 model, with only 40M public images and 28.4M unpaired public sentences. The resulting model \"Distill-ViT-B/32\" rivals the CLIP-ViT-B/32 model pre-trained on its private WiT dataset (400M image-text pairs): Distill-ViT-B/32 achieves similar results in terms of zero-shot and linear-probing performance on both Ima-geNet and the ELEVATER (20 image classification tasks) benchmarks. It also displays comparable robustness when evaluated on five datasets with natural distribution shifts from ImageNet. Please refer to our project page for code and more details.",
        "authors": [
            "Ximeng Sun",
            "Pengchuan Zhang",
            "Peizhao Zhang",
            "Hardik Shah",
            "Kate Saenko",
            "Xide Xia"
        ],
        "citations": 17,
        "references": 112,
        "year": 2023
    },
    {
        "title": "Robot Learning in the Era of Foundation Models: A Survey",
        "abstract": "The proliferation of Large Language Models (LLMs) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence (AI). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.",
        "authors": [
            "Xuan Xiao",
            "Jiahang Liu",
            "Zhipeng Wang",
            "Yanmin Zhou",
            "Yong Qi",
            "Qian Cheng",
            "Bin He",
            "Shuo Jiang"
        ],
        "citations": 16,
        "references": 358,
        "year": 2023
    },
    {
        "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
        "abstract": "Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like\"left\"can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.",
        "authors": [
            "Joy Hsu",
            "Jiayuan Mao",
            "J. B. Tenenbaum",
            "Jiajun Wu"
        ],
        "citations": 16,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Are Natural Domain Foundation Models Useful for Medical Image Classification?",
        "abstract": "The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely Sam, Seem, Dinov2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. Dinov2 consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.",
        "authors": [
            "Joana Palés Huix",
            "Adithya Raju Ganeshan",
            "Johan Fredin Haslum",
            "Magnus Söderberg",
            "Christos Matsoukas",
            "Kevin Smith"
        ],
        "citations": 14,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Foundation Models in Healthcare: Opportunities, Risks & Strategies Forward",
        "abstract": "Foundation models (FMs) are a new paradigm in AI. First pretrained on broad data at immense scale and subsequently adapted to more specific tasks, they achieve high performances and unlock powerful new capabilities to be leveraged in many domains, including healthcare. This SIG will bring together researchers and practitioners within the CHI community interested in such emerging technology and healthcare. Drawing attention to the rapid evolution of these models and proposals for their wide-spread adoption, we aim to demonstrate their strengths whilst simultaneously highlighting deficiencies and limitations that give raise to ethical and societal concerns. In particular, we will invite the community to actively debate how the field of HCI – with its research frameworks and methods – can help address some of these existing challenges and mitigate risks to ensure the safe and ethical use of the end-product; a requirement to realize many of the ambitious visions for how these models can positively transform healthcare delivery. This conversation will benefit from a diversity of voices, critical perspectives, and open debate, which are necessary to bring about the right norms and best practices, and to identify a path forward in devising responsible approaches to future FM design and use in healthcare.",
        "authors": [
            "Anja Thieme",
            "A. Nori",
            "M. Ghassemi",
            "Rishi Bommasani",
            "T. Andersen",
            "E. Luger"
        ],
        "citations": 14,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Paxion: Patching Action Knowledge in Video-Language Foundation Models",
        "abstract": "Action knowledge involves the understanding of textual, visual, and temporal aspects of actions. We introduce the Action Dynamics Benchmark (ActionBench) containing two carefully designed probing tasks: Action Antonym and Video Reversal, which targets multimodal alignment capabilities and temporal understanding skills of the model, respectively. Despite recent video-language models' (VidLM) impressive performance on various benchmark tasks, our diagnostic tasks reveal their surprising deficiency (near-random performance) in action knowledge, suggesting that current models rely on object recognition abilities as a shortcut for action understanding. To remedy this, we propose a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher network to encode new action knowledge and a Knowledge Fuser component to integrate the Patcher into frozen VidLMs without compromising their existing capabilities. Due to limitations of the widely-used Video-Text Contrastive (VTC) loss for learning action knowledge, we introduce the DVDM objective to train the Knowledge Patcher. DVDM forces the model to encode the correlation between the action text and the correct ordering of video frames. Our extensive analyses show that Paxion and DVDM together effectively fill the gap in action knowledge understanding (~50% to 80%), while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks. The code and data will be made publicly available for research purposes at https://github.com/MikeWangWZHL/Paxion.git.",
        "authors": [
            "Zhenhailong Wang",
            "Ansel Blume",
            "Sha Li",
            "Genglin Liu",
            "Jaemin Cho",
            "Zineng Tang",
            "Mohit Bansal",
            "Heng Ji"
        ],
        "citations": 19,
        "references": 63,
        "year": 2023
    },
    {
        "title": "AI Foundation Models for Weather and Climate: Applications, Design, and Implementation",
        "abstract": "Machine learning and deep learning methods have been widely explored in understanding the chaotic behavior of the atmosphere and furthering weather forecasting. There has been increasing interest from technology companies, government institutions, and meteorological agencies in building digital twins of the Earth. Recent approaches using transformers, physics-informed machine learning, and graph neural networks have demonstrated state-of-the-art performance on relatively narrow spatiotemporal scales and specific tasks. With the recent success of generative artificial intelligence (AI) using pre-trained transformers for language modeling and vision with prompt engineering and fine-tuning, we are now moving towards generalizable AI. In particular, we are witnessing the rise of AI foundation models that can perform competitively on multiple domain-specific downstream tasks. Despite this progress, we are still in the nascent stages of a generalizable AI model for global Earth system models, regional climate models, and mesoscale weather models. Here, we review current state-of-the-art AI approaches, primarily from transformer and operator learning literature in the context of meteorology. We provide our perspective on criteria for success towards a family of foundation models for nowcasting and forecasting weather and climate predictions. We also discuss how such models can perform competitively on downstream tasks such as downscaling (super-resolution), identifying conditions conducive to the occurrence of wildfires, and predicting consequential meteorological phenomena across various spatiotemporal scales such as hurricanes and atmospheric rivers. In particular, we examine current AI methodologies and contend they have matured enough to design and implement a weather foundation model.",
        "authors": [
            "S. K. Mukkavilli",
            "D. S. Civitarese",
            "J. Schmude",
            "Johannes Jakubik",
            "Anne Jones",
            "Nam Nguyen",
            "C. Phillips",
            "Sujit Roy",
            "Shraddha Singh",
            "Campbell Watson",
            "R. Ganti",
            "Hendrik F. Hamann",
            "U. Nair",
            "Rahul Ramachandran",
            "Kommy Weldemariam"
        ],
        "citations": 14,
        "references": 115,
        "year": 2023
    },
    {
        "title": "Market Concentration Implications of Foundation Models",
        "abstract": "We analyze the structure of the market for foundation models, i.e., large AI models such as those that power ChatGPT and that are adaptable to downstream uses, and we examine the implications for competition policy and regulation. We observe that the most capable models will have a tendency towards natural monopoly and may have potentially vast markets. This calls for a two-pronged regulatory response: (i) Antitrust authorities need to ensure the contestability of the market by tackling strategic behavior, in particular by ensuring that monopolies do not propagate vertically to downstream uses, and (ii) given the diminished potential for market discipline, there is a role for regulators to ensure that the most capable models meet sufficient quality standards (including safety, privacy, non-discrimination, reliability and interoperability standards) to maximally contribute to social welfare. Regulators should also ensure a level regulatory playing field between AI and non-AI applications in all sectors of the economy. For models that are behind the frontier, we expect competition to be quite intense, implying a more limited role for competition policy, although a role for regulation remains.",
        "authors": [
            "Jai Vipra",
            "Anton Korinek"
        ],
        "citations": 14,
        "references": 112,
        "year": 2023
    },
    {
        "title": "Grounding Foundation Models through Federated Transfer Learning: A General Framework",
        "abstract": "Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated learning setting, construct a detailed taxonomy based on the FTL-FM framework to categorize state-of-the-art FTL-FM works, and comprehensively overview FTL-FM works based on the proposed taxonomy. We also establish correspondences between FTL-FM and conventional phases of adapting FM so that FM practitioners can align their research works with FTL-FM. In addition, we overview advanced efficiency-improving and privacy-preserving techniques because efficiency and privacy are critical concerns in FTL-FM. Last, we discuss opportunities and future research directions of FTL-FM.",
        "authors": [
            "Yan Kang",
            "Tao Fan",
            "Hanlin Gu",
            "Lixin Fan",
            "Qiang Yang"
        ],
        "citations": 14,
        "references": 171,
        "year": 2023
    },
    {
        "title": "Few-Shot Panoptic Segmentation With Foundation Models",
        "abstract": "Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de.",
        "authors": [
            "Markus Kappeler",
            "Kürsat Petek",
            "Niclas Vodisch",
            "Wolfram Burgard",
            "Abhinav Valada"
        ],
        "citations": 14,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models (Vision Paper)",
        "abstract": "In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.",
        "authors": [
            "Jinmeng Rao",
            "Song Gao",
            "Gengchen Mai",
            "Krzysztof Janowicz"
        ],
        "citations": 14,
        "references": 39,
        "year": 2023
    },
    {
        "title": "FORGE: Pre-Training Open Foundation Models for Science",
        "abstract": "Large language models (LLMs) are poised to revolutionize the way we conduct scientific research. However, both model complexity and pre-training cost are impeding effective adoption for the wider science community. Identifying suitable scientific use cases, finding the optimal balance between model and data sizes, and scaling up model training are among the most pressing issues that need to be addressed. In this study, we provide practical solutions for building and using LLM-based foundation models targeting scientific research use cases. We present an end-to-end examination of the effectiveness of LLMs in scientific research, including their scaling behavior and computational requirements on Frontier, the first Exascale supercomputer. We have also developed for release to the scientific community a suite of open foundation models called FORGE with up to 26B parameters using 257B tokens from over 200M scientific articles, with performance either on par or superior to other state-of-the-art comparable models. We have demonstrated the use and effectiveness of FORGE on scientific downstream tasks. Our research establishes best practices that can be applied across various fields to take advantage of LLMs for scientific discovery.",
        "authors": [
            "Junqi Yin",
            "Sajal Dash",
            "Feiyi Wang",
            "M. Shankar"
        ],
        "citations": 14,
        "references": 42,
        "year": 2023
    },
    {
        "title": "3D Open-vocabulary Segmentation with Foundation Models",
        "abstract": "Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.",
        "authors": [
            "Kunhao Liu",
            "Fangneng Zhan",
            "Jiahui Zhang",
            "Muyu Xu",
            "Yingchen Yu",
            "Abdulmotaleb El-Saddik",
            "C. Theobalt",
            "Eric P. Xing",
            "Shijian Lu"
        ],
        "citations": 13,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
        "abstract": "Foundation models or pre-trained models have substantially improved the performance of various language, vision, and vision-language understanding tasks. However, existing foundation models can only perform the best in one type of tasks, namely language, vision, or vision-language. It is still an open question whether it is possible to construct a foundation model performing the best for all the understanding tasks, which we call a general foundation model. In this paper, we propose a new general foundation model, X-FM (the X-Foundation Model). X-FM has one language encoder, one vision encoder, and one fusion encoder, as well as a new training method. The training method includes two new techniques for learning X-FM from text, image, and image-text pair data. One is to stop gradients from the vision-language training when learning the language encoder. The other is to leverage the vision-language training to guide the learning of the vision encoder. Extensive experiments on benchmark datasets show that X-FM can significantly outperform existing general foundation models and perform better than or comparable to existing foundation models specifically for language, vision, or vision-language understanding. Code and pre-trained models are released at https://github.com/zhangxinsong-nlp/XFM.",
        "authors": [
            "Xinsong Zhang",
            "Yan Zeng",
            "Jipeng Zhang",
            "Hang Li"
        ],
        "citations": 13,
        "references": 100,
        "year": 2023
    },
    {
        "title": "Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models",
        "abstract": "Human-object interaction (HOI) detection aims to comprehend the intricate relationships between humans and objects, predicting $$ triplets, and serving as the foundation for numerous computer vision tasks. The complexity and diversity of human-object interactions in the real world, however, pose significant challenges for both annotation and recognition, particularly in recognizing interactions within an open world context. This study explores the universal interaction recognition in an open-world setting through the use of Vision-Language (VL) foundation models and large language models (LLMs). The proposed method is dubbed as \\emph{\\textbf{UniHOI}}. We conduct a deep analysis of the three hierarchical features inherent in visual HOI detectors and propose a method for high-level relation extraction aimed at VL foundation models, which we call HO prompt-based learning. Our design includes an HO Prompt-guided Decoder (HOPD), facilitates the association of high-level relation representations in the foundation model with various HO pairs within the image. Furthermore, we utilize a LLM (\\emph{i.e.} GPT) for interaction interpretation, generating a richer linguistic understanding for complex HOIs. For open-category interaction recognition, our method supports either of two input types: interaction phrase or interpretive sentence. Our efficient architecture design and learning methods effectively unleash the potential of the VL foundation models and LLMs, allowing UniHOI to surpass all existing methods with a substantial margin, under both supervised and zero-shot settings. The code and pre-trained weights are available at: \\url{https://github.com/Caoyichao/UniHOI}.",
        "authors": [
            "Yichao Cao",
            "Qingfei Tang",
            "Xiu Su",
            "Chen Song",
            "Shan You",
            "Xiaobo Lu",
            "Chang Xu"
        ],
        "citations": 13,
        "references": 53,
        "year": 2023
    },
    {
        "title": "One-Shot Open Affordance Learning with Foundation Models",
        "abstract": "We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances. While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances. To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess the potential for data-limited affordance learning. We then propose a vision-language framework with simple and effective designs that boost the alignment between visual features and affordance text embeddings. Experiments on two affordance segmentation benchmarks show that the proposed method outperforms state-of-the-art models with less than 1% of the full training data, and exhibits reasonable generalization capability on unseen objects and affordances. Project page: https://reagan1311.github.io/ooal.",
        "authors": [
            "Gen Li",
            "Deqing Sun",
            "Laura Sevilla-Lara",
            "Varun Jampani"
        ],
        "citations": 13,
        "references": 68,
        "year": 2023
    },
    {
        "title": "FM-Loc: Using Foundation Models for Improved Vision-Based Localization",
        "abstract": "Visual place recognition is essential for vision-based robot localization and SLAM. Despite the tremendous progress made in recent years, place recognition in changing environments remains challenging. A promising approach to cope with appearance variations is to leverage high-level semantic features like objects or place categories. In this paper, we propose FM-Loc which is a novel image-based localization approach based on Foundation Models. Our approach uses the Large Language Model GPT-3 in combination with the Visual-Language Model CLIP to construct a semantic image descriptor that is robust to severe changes in scene geometry and camera viewpoint. We deploy CLIP to detect objects in an image, GPT-3 to suggest potential room labels based on the detected objects, and CLIP again to propose the most likely location label. The object labels and the scene label constitute an image descriptor that we use to calculate a similarity score between the query and database images. We validate our approach on real-world data that exhibit significant changes in camera viewpoints and object placement between the database and query trajectories. The experimental results demonstrate that our method is applicable to a wide range of indoor scenarios without the need for training or fine-tuning.",
        "authors": [
            "Reihaneh Mirjalili",
            "Michael Krawez",
            "Wolfram Burgard"
        ],
        "citations": 12,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Large Foundation Models for Power Systems",
        "abstract": "Foundation models, such as Large Language Models (LLMs), can respond to a wide range of format-free queries without any task-specific data collection or model training, creating various research and application opportunities for the modeling and operation of large-scale power systems. In this paper, we outline how such large foundation model such as GPT-4 are developed, and discuss how they can be leveraged in challenging power and energy system tasks. We first investigate the potential of existing foundation models by validating their performance on four representative tasks across power system domains, including the optimal power flow (OPF), electric vehicle (EV) scheduling, knowledge retrieval for power engineering technical reports, and situation awareness. Our results indicate strong capabilities of such foundation models on boosting the efficiency and reliability of power system operational pipelines. We also provide suggestions and projections on future deployment of foundation models in power system applications.",
        "authors": [
            "Chenghao Huang",
            "Siyang Li",
            "Ruohong Liu",
            "Hao Wang",
            "Yize Chen"
        ],
        "citations": 12,
        "references": 33,
        "year": 2023
    },
    {
        "title": "BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs",
        "abstract": "Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relations. Additionally, we also show that BioBridge presents itself as a general purpose retriever that can aid biomedical multimodal question answering as well as enhance the guided generation of novel drugs.",
        "authors": [
            "Zifeng Wang",
            "Zichen Wang",
            "Balasubramaniam Srinivasan",
            "V. Ioannidis",
            "H. Rangwala",
            "Rishita Anubhai"
        ],
        "citations": 12,
        "references": 67,
        "year": 2023
    },
    {
        "title": "RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes",
        "abstract": "Can foundation models (such as ChatGPT) clean your data? In this proposal, we demonstrate that indeed ChatGPT can assist in data cleaning by suggesting corrections for specific cells in a data table (scenario 1). However, ChatGPT may struggle with datasets it has never encountered before (e.g., local enterprise data) or when the user requires an explanation of the source of the suggested clean values. To address these issues, we developed a retrieval-based method that complements ChatGPT’s power with a user-provided data lake. The data lake is first indexed, we then retrieve the top-𝑘 relevant tuples to the user’s query tuple and finally leverage Chat-GPT to infer the correct value (scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally hosted model, might not be feasible for privacy reasons. To assist with this scenario, we developed a custom RoBERTa-based foundation model that can be locally deployed. By fine-tuning it on a small number of examples, it can effectively make value inferences based on the retrieved tuples (scenario 3). Our proposed system, RetClean , seamlessly supports all three scenarios and provides a user-friendly GUI that enables the VLDB audience to explore and experiment with the system.",
        "authors": [
            "M. Ahmad",
            "Z. Naeem",
            "M. Eltabakh",
            "M. Ouzzani",
            "N. Tang"
        ],
        "citations": 11,
        "references": 6,
        "year": 2023
    },
    {
        "title": "Improving Foundation Models",
        "abstract": null,
        "authors": [
            "Aran Komatsuzaki"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Geo-Foundation Models: Reality, Gaps and Opportunities",
        "abstract": "With the recent rapid advances of revolutionary AI models such as ChatGPT, foundation models have become a main topic for the discussion of future AI. Despite the excitement, the success is still limited to specific types of tasks. Particularly, ChatGPT and similar foundation models have unique characteristics that are difficult to replicate for most geospatial tasks. This paper envisions several major challenges and opportunities in the creation of geospatial foundation (geo-foundation) models, as well as potential future adoption scenarios. We also expect that a major success story is necessary for geo-foundation models to take off in the long term.",
        "authors": [
            "Yiqun Xie",
            "Zhaonan Wang",
            "Gengchen Mai",
            "Yanhua Li",
            "Xiaowei Jia",
            "Song Gao",
            "Shaowen Wang"
        ],
        "citations": 11,
        "references": 22,
        "year": 2023
    },
    {
        "title": "Applications of Large Scale Foundation Models for Autonomous Driving",
        "abstract": "Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007, autonomous driving has been the most active field of AI applications. Recently powered by large language models (LLMs), chat systems, such as chatGPT and PaLM, emerge and rapidly become a promising direction to achieve artificial general intelligence (AGI) in natural language processing (NLP). There comes a natural thinking that we could employ these abilities to reformulate autonomous driving. By combining LLM with foundation models, it is possible to utilize the human knowledge, commonsense and reasoning to rebuild autonomous driving systems from the current long-tailed AI dilemma. In this paper, we investigate the techniques of foundation models and LLMs applied for autonomous driving, categorized as simulation, world model, data annotation and planning or E2E solutions etc.",
        "authors": [
            "Yu Huang",
            "Yue Chen",
            "Zhu Li"
        ],
        "citations": 10,
        "references": 248,
        "year": 2023
    },
    {
        "title": "Federated Prompt Learning for Weather Foundation Models on Devices",
        "abstract": "On-device intelligence for weather forecasting uses local deep learning models to analyze weather patterns without centralized cloud computing, holds significance for supporting human activates. Federated Learning is a promising solution for such forecasting by enabling collaborative model training without sharing raw data. However, it faces three main challenges that hinder its reliability: (1) data heterogeneity among devices due to geographic differences; (2) data homogeneity within individual devices and (3) communication overload from sending large model parameters for collaboration. To address these challenges, this paper propose Federated Prompt learning for Weather Foundation Models on Devices (FedPoD), which enables devices to obtain highly customized models while maintaining communication efficiency. Concretely, our Adaptive Prompt Tuning leverages lightweight prompts guide frozen foundation model to generate more precise predictions, also conducts prompt-based multi-level communication to encourage multi-source knowledge fusion and regulate optimization. Additionally, Dynamic Graph Modeling constructs graphs from prompts, prioritizing collaborative training among devices with similar data distributions to against heterogeneity. Extensive experiments demonstrates FedPoD leads the performance among state-of-the-art baselines across various setting in real-world on-device weather forecasting datasets.",
        "authors": [
            "Shen Chen",
            "Guodong Long",
            "Tao Shen",
            "Tianyi Zhou",
            "Jing Jiang"
        ],
        "citations": 9,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Customizing General-Purpose Foundation Models for Medical Report Generation",
        "abstract": "Medical caption prediction which can be regarded as a task of medical report generation (MRG), requires the automatic generation of coherent and accurate captions for the given medical images. However, the scarcity of labelled medical image-report pairs presents great challenges in the development of deep and large-scale neural networks capable of harnessing the potential artificial general intelligence power like large language models (LLMs). In this work, we propose customizing off-the-shelf general-purpose large-scale pre-trained models, i.e., foundation models (FMs), in computer vision and natural language processing with a specific focus on medical report generation. Specifically, following BLIP-2, a state-of-the-art vision-language pre-training approach, we introduce our encoder-decoder-based MRG model. This model utilizes a lightweight query Transformer to connect two FMs: the giant vision Transformer EVA-ViT-g and a bilingual LLM trained to align with human intentions (referred to as ChatGLM-6B). Furthermore, we conduct ablative experiments on the trainable components of the model to identify the crucial factors for effective transfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn medical image representations, followed by parameter-efficient training of ChatGLM-6B to capture the writing styles of medical reports, is essential for achieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and the 2nd, respectively, out of 13 participating teams, based on the BERTScore and ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction Task competition.",
        "authors": [
            "Bang Yang",
            "Asif Raza",
            "Yuexian Zou",
            "Tong Zhang"
        ],
        "citations": 9,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Integrating Visual Foundation Models for Enhanced Robot Manipulation and Motion Planning: A Layered Approach",
        "abstract": "This paper presents a novel layered framework that integrates visual foundation models to improve robot manipulation tasks and motion planning. The framework consists of five layers: Perception, Cognition, Planning, Execution, and Learning. Using visual foundation models, we enhance the robot's perception of its environment, enabling more efficient task understanding and accurate motion planning. This approach allows for real-time adjustments and continual learning, leading to significant improvements in task execution. Experimental results demonstrate the effectiveness of the proposed framework in various robot manipulation tasks and motion planning scenarios, highlighting its potential for practical deployment in dynamic environments.",
        "authors": [
            "Chenguang Yang",
            "Peng Zhou",
            "Jiaming Qi"
        ],
        "citations": 9,
        "references": 33,
        "year": 2023
    },
    {
        "title": "Foundation models such as ChatGPT through the prism of the UNESCO Recommendation on the Ethics of Artificial Intelligence",
        "abstract": "The release into the public domain and massive growth in the user base of artificial intelligence (AI) foundation models for text, images, and audio is fuelling debate about the risks they pose to work, education, scientific research, and democracy, as well as their potential negative impacts on cultural diversity and cross-cultural interactions, among other areas. Foundation models are AI systems that are characterized by the use of very large machine learning models trained on massive unlabelled data sets using considerable compute resources. Examples include large language models (LLMs) such as the GPT series and Bard, and image generator tools such as DALL·E 2 and Stable Diffusion. This discussion paper focuses on a widely used foundation model, ChatGPT, as a case study, but many of the points below are applicable to other LLMs and foundation models more broadly. UNESCO Catno: 0000385629",
        "authors": [],
        "citations": 7,
        "references": 0,
        "year": 2023
    },
    {
        "title": "MIS-FM: 3D Medical Image Segmentation using Foundation Models Pretrained on a Large-Scale Unannotated Dataset",
        "abstract": "Pretraining with large-scale 3D volumes has a potential for improving the segmentation performance on a target medical image dataset where the training images and annotations are limited. Due to the high cost of acquiring pixel-level segmentation annotations on the large-scale pretraining dataset, pretraining with unannotated images is highly desirable. In this work, we propose a novel self-supervised learning strategy named Volume Fusion (VF) for pretraining 3D segmentation models. It fuses several random patches from a foreground sub-volume to a background sub-volume based on a predefined set of discrete fusion coefficients, and forces the model to predict the fusion coefficient of each voxel, which is formulated as a self-supervised segmentation task without manual annotations. Additionally, we propose a novel network architecture based on parallel convolution and transformer blocks that is suitable to be transferred to different downstream segmentation tasks with various scales of organs and lesions. The proposed model was pretrained with 110k unannotated 3D CT volumes, and experiments with different downstream segmentation targets including head and neck organs, thoracic/abdominal organs showed that our pretrained model largely outperformed training from scratch and several state-of-the-art self-supervised training methods and segmentation models. The code and pretrained model are available at https://github.com/openmedlab/MIS-FM.",
        "authors": [
            "Guotai Wang",
            "Jianghao Wu",
            "Xiangde Luo",
            "Xinglong Liu",
            "Kang Li",
            "Shaoting Zhang"
        ],
        "citations": 20,
        "references": 54,
        "year": 2023
    },
    {
        "title": "INSTRUCTION-FINETUNED FOUNDATION MODELS FOR MULTIMODAL WEB NAVIGATION",
        "abstract": "We propose an instruction-aligned multimodal agent for autonomous web navigation – i.e., sequential decision making tasks employing a computer interface. Our approach is based on supervised finetuning of vision and language foundation models on a large corpus of web data consisting of webpage screenshots and HTML. Specifically, we use vision transformers on sequences of web page screenshots to extract patch-level image features. These features are concatenated with embedding of tokens in HTML documents. Using an instruction-finetuned large language model, we jointly encode both vision and HTML modalities and decode web actions such as click and type. We show that our method outperforms previous approaches by a significant margin, even in handling out-of-distribution HTML and compositional tasks. On the MiniWoB benchmark, we improve previous approaches using only HTML input by more than 17.7%, even surpassing the performance of RL-finetuned models. On the recent WebShop benchmark, our 3-billion-parameter model achieves superior performance to the existing state-ofthe-art PaLM-540B. We also collect 347K gold demonstrations using our trained models, 29 times larger than prior work, and make them available to promote future research in this area. We believe that our work is a step towards building capable and generalist decision making agents for computer interface.",
        "authors": [
            "Hiroki Furuta",
            "Ofir Nachum",
            "Kuang-Huei Lee",
            "Yutaka Matsuo",
            "S. Gu"
        ],
        "citations": 7,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Adapting Vision Foundation Models for Plant Phenotyping",
        "abstract": "Foundation models are large models pre-trained on tremendous amount of data. They can be typically adapted to diverse downstream tasks with minimal effort. However, as foundation models are usually pre-trained on images or texts sourced from the Internet, their performance in specialized domains, such as plant phenotyping, comes into question. In addition, fully fine-tuning foundation models is time-consuming and requires high computational power. This paper investigates the efficient adaptation of foundation models for plant phenotyping settings and tasks. We perform extensive experiments on fine-tuning three foundation models, MAE, DINO, and DINOv2 on three essential plant phenotyping tasks: leaf counting, instance segmentation, and disease classification. In particular, the pretrained backbones are kept frozen, while two distinct fine-tuning methods are evaluated, namely adapter tuning (using LoRA) and decoder tuning. The experimental results show that a foundation model can be efficiently adapted to multiple plant phenotyping tasks, yielding similar performance as the state-of-the-art (SoTA) models specifically designed or trained for each task. Despite exhibiting great transferability over different tasks, the fine-tuned foundation models perform slightly worse than the SoTA task-specific models in some scenarios, which requires further investigation.",
        "authors": [
            "Feng Chen",
            "M. Giuffrida",
            "S. Tsaftaris"
        ],
        "citations": 7,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Open World Object Detection in the Era of Foundation Models",
        "abstract": "Object detection is integral to a bevy of real-world applications, from robotics to medical image analysis. To be used reliably in such applications, models must be capable of handling unexpected - or novel - objects. The open world object detection (OWD) paradigm addresses this challenge by enabling models to detect unknown objects and learn discovered ones incrementally. However, OWD method development is hindered due to the stringent benchmark and task definitions. These definitions effectively prohibit foundation models. Here, we aim to relax these definitions and investigate the utilization of pre-trained foundation models in OWD. First, we show that existing benchmarks are insufficient in evaluating methods that utilize foundation models, as even naive integration methods nearly saturate these benchmarks. This result motivated us to curate a new and challenging benchmark for these models. Therefore, we introduce a new benchmark that includes five real-world application-driven datasets, including challenging domains such as aerial and surgical images, and establish baselines. We exploit the inherent connection between classes in application-driven datasets and introduce a novel method, Foundation Object detection Model for the Open world, or FOMO, which identifies unknown objects based on their shared attributes with the base known objects. FOMO has ~3x unknown object mAP compared to baselines on our benchmark. However, our results indicate a significant place for improvement - suggesting a great research opportunity in further scaling object detection methods to real-world domains. Our code and benchmark are available at https://orrzohar.github.io/projects/fomo/.",
        "authors": [
            "O. Zohar",
            "Alejandro Lozano",
            "Shelly Goel",
            "Serena Yeung",
            "Kuan-Chieh Wang"
        ],
        "citations": 8,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Universal Domain Adaptation from Foundation Models",
        "abstract": "Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target data distillation on the CLIP model, and achieves consistent improvement over the baseline across all the UniDA benchmarks. Our studies are under a newly proposed evaluation metric of universal classification rate (UCR), which is threshold- and ratio-free and addresses the threshold-sensitive issue encountered when using the existing H-score metric.",
        "authors": [
            "Bin Deng",
            "K. Jia"
        ],
        "citations": 7,
        "references": 43,
        "year": 2023
    },
    {
        "title": "VideoGLUE: Video General Understanding Evaluation of Foundation Models",
        "abstract": "We evaluate the video understanding capabilities of existing foundation models (FMs) using a carefully designed experiment protocol consisting of three hallmark tasks (action recognition,temporal localization, and spatiotemporal localization), eight datasets well received by the community, and four adaptation methods tailoring an FM for downstream tasks. Furthermore, we jointly profile FMs' efficacy and efficiency when adapting to general video understanding tasks using cost measurements during both training and inference. Our main findings areas follows. First, task-specialized models significantly outperform the seven FMs studied in this work, in sharp contrast to what FMs have achieved in natural language and image understanding. Second, video-native FMs, whose pretraining data mainly contains the video modality, are generally better than image-native FMs in classifying motion-rich videos, localizing actions in time, and understanding a video of more than one action. Third, the video-native FMs can perform well on video tasks under light adaptations to downstream tasks (e.g., freezing the FM backbones), while image-native FMs win in full end-to-end finetuning. The first two observations reveal the need and tremendous opportunities to conduct research on video-focused FMs, and the last confirms that both tasks and adaptation methods matter when it comes to the evaluation of FMs. Our code is released under: https://github.com/tensorflow/models/tree/master/official/projects/videoglue.",
        "authors": [
            "Liangzhe Yuan",
            "N. B. Gundavarapu",
            "Long Zhao",
            "Hao Zhou",
            "Yin Cui",
            "Lu Jiang",
            "Xu Yang",
            "Menglin Jia",
            "Tobias Weyand",
            "Luke Friedman",
            "Mikhail Sirotenko",
            "H. Wang",
            "Florian Schroff",
            "Hartwig Adam",
            "Ming Yang",
            "Ting Liu",
            "Boqing Gong"
        ],
        "citations": 7,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Can Foundation Models Wrangle Your Data?",
        "abstract": "Foundation Models (FMs) are models trained on large corpora of data that, at very large scale, can generalize to new tasks without any task-specific finetuning. As these models continue to grow in size, innovations continue to push the boundaries of what these models can do on language and image tasks. This paper aims to understand an underexplored area of FMs: classical data tasks like cleaning and integration. As a proof-of-concept, we cast five data cleaning and integration tasks as prompting tasks and evaluate the performance of FMs on these tasks. We find that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks. We identify specific research challenges and opportunities that these models present, including challenges with private and domain specific data, and opportunities to make data management systems more accessible to non-experts. We make our code and experiments publicly available at: https://github.com/HazyResearch/fm_data_tasks.",
        "authors": [
            "A. Narayan",
            "Ines Chami",
            "Laurel J. Orr",
            "Christopher R'e"
        ],
        "citations": 166,
        "references": 100,
        "year": 2022
    },
    {
        "title": "Backdoor Attacks to Pre-trained Unified Foundation Models",
        "abstract": "The rise of pre-trained unified foundation models breaks down the barriers between different modalities and tasks, providing comprehensive support to users with unified architectures. However, the backdoor attack on pre-trained models poses a serious threat to their security. Previous research on backdoor attacks has been limited to uni-modal tasks or single tasks across modalities, making it inapplicable to unified foundation models. In this paper, we make proof-of-concept level research on the backdoor attack for pre-trained unified foundation models. Through preliminary experiments on NLP and CV classification tasks, we reveal the vulnerability of these models and suggest future research directions for enhancing the attack approach.",
        "authors": [
            "Zenghui Yuan",
            "Yixin Liu",
            "Kai Zhang",
            "Pan Zhou",
            "Lichao Sun"
        ],
        "citations": 8,
        "references": 8,
        "year": 2023
    },
    {
        "title": "Multimodal Foundation Models For Echocardiogram Interpretation",
        "abstract": "Multimodal deep learning foundation models can learn the relationship between images and text. In the context of medical imaging, mapping images to language concepts reflects the clinical task of diagnostic image interpretation, however current general-purpose foundation models do not perform well in this context because their training corpus have limited medical text and images. To address this challenge and account for the range of cardiac physiology, we leverage 1,032,975 cardiac ultrasound videos and corresponding expert interpretations to develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP displays strong zero-shot (not explicitly trained) performance in cardiac function assessment (external validation left ventricular ejection fraction mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and artificial heart valves). We also developed a long-context variant (EchoCLIP-R) with a custom echocardiography report text tokenizer which can accurately identify unique patients across multiple videos (AUC of 0.86), identify clinical changes such as orthotopic heart transplants (AUC of 0.79) or cardiac surgery (AUC 0.77), and enable robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These emergent capabilities can be used for preliminary assessment and summarization of echocardiographic findings.",
        "authors": [
            "M. Christensen",
            "Milos Vukadinovic",
            "N. Yuan",
            "David Ouyang"
        ],
        "citations": 7,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Module-wise Adaptive Distillation for Multimodality Foundation Models",
        "abstract": "Pre-trained multimodal foundation models have demonstrated remarkable generalizability but pose challenges for deployment due to their large sizes. One effective approach to reducing their sizes is layerwise distillation, wherein small student models are trained to match the hidden representations of large teacher models at each layer. Motivated by our observation that certain architecture components, referred to as modules, contribute more significantly to the student's performance than others, we propose to track the contributions of individual modules by recording the loss decrement after distillation each module and choose the module with a greater contribution to distill more frequently. Such an approach can be naturally formulated as a multi-armed bandit (MAB) problem, where modules and loss decrements are considered as arms and rewards, respectively. We then develop a modified-Thompson sampling algorithm named OPTIMA to address the nonstationarity of module contributions resulting from model updating. Specifically, we leverage the observed contributions in recent history to estimate the changing contribution of each module and select modules based on these estimations to maximize the cumulative contribution. We evaluate the effectiveness of OPTIMA through distillation experiments on various multimodal understanding and image captioning tasks, using the CoCa-Large model (Yu et al., 2022) as the teacher model.",
        "authors": [
            "Chen Liang",
            "Jiahui Yu",
            "Ming-Hsuan Yang",
            "Matthew Brown",
            "Yin Cui",
            "Tuo Zhao",
            "Boqing Gong",
            "Tianyi Zhou"
        ],
        "citations": 8,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Semantic Communications Using Foundation Models: Design Approaches and Open Issues",
        "abstract": "Foundation models (FMs), including large language models, have become increasingly popular due to their wide-ranging applicability and ability to understand human-like semantics. While previous research has explored the use of FMs in semantic communications to improve semantic extraction and reconstruction, the impact of these models on different system layers, considering computation and memory complexity, requires further analysis. This study focuses on integrating FMs at the task application, semantic coding, and physical transmission layers, using universal knowledge to profoundly transform system design. Additionally, it examines the use of compact models to balance performance and complexity, comparing three separate approaches that employ FMs. Ultimately, the study highlights unresolved issues in the fields that need addressing.",
        "authors": [
            "Peiwen Jiang",
            "Chao-Kai Wen",
            "Xinping Yi",
            "X. Li",
            "Shimei Jin",
            "Jun Zhang"
        ],
        "citations": 8,
        "references": 21,
        "year": 2023
    },
    {
        "title": "Towards Foundation Models for Relational Databases [Vision Paper]",
        "abstract": "Tabular representation learning has recently gained a lot of attention. However, existing approaches only learn a representation from a single table, and thus ignore the potential to learn from the full structure of relational databases, including neighboring tables that can contain important information for a contextualized representation. Moreover, current models are significantly limited in scale, which prevents that they learn from large databases. In this paper, we thus introduce our vision of relational representation learning, that can not only learn from the full relational structure, but also can scale to larger database sizes that are commonly found in real-world. Moreover, we also discuss opportunities and challenges we see along the way to enable this vision and present initial very promising results. Overall, we argue that this direction can lead to foundation models for relational databases that are today only available for text and images.",
        "authors": [
            "Liane Vogel",
            "Benjamin Hilprecht",
            "Carsten Binnig"
        ],
        "citations": 6,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Granite Foundation Models",
        "abstract": "—We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint, testing and evaluation, socio-technical harms and mitigations, and usage policies.",
        "authors": [
            "Ibm Research"
        ],
        "citations": 5,
        "references": 133,
        "year": 2023
    },
    {
        "title": "Danish Foundation Models",
        "abstract": "Large language models, sometimes referred to as foundation models, have transformed multiple fields of research. However, smaller languages risk falling behind due to high training costs and small incentives for large companies to train these models. To combat this, the Danish Foundation Models project seeks to provide and maintain open, well-documented, and high-quality foundation models for the Danish language. This is achieved through broad cooperation with public and private institutions, to ensure high data quality and applicability of the trained models. We present the motivation of the project, the current status, and future perspectives.",
        "authors": [
            "K. Enevoldsen",
            "Lasse Hansen",
            "Dan S. Nielsen",
            "R. A. F. Egebæk",
            "Soren V. Holm",
            "Martin C. Nielsen",
            "M. Bernstorff",
            "Rasmus Larsen",
            "Peter B. Jorgensen",
            "Malte Højmark-Bertelsen",
            "P. B. Vahlstrup",
            "Per Moldrup-Dalum",
            "Kristoffer L. Nielbo"
        ],
        "citations": 2,
        "references": 0,
        "year": 2023
    },
    {
        "title": "DeFACT in ManuVerse for Parallel Manufacturing: Foundation Models and Parallel Workers in Smart Factories",
        "abstract": "In cyber–physical–social systems, smart manufacturing has to overcome challenges, such as uncertainty, diversity, complexity in modeling, long-delayed responses to market changes, and human engineer dependency. DeFACT is a framework of parallel manufacturing in ManuVerse where the Decentralized Autonomous Organization-based interactions between parallel workers consisting of robotic, digital, and human workers are elaborated to transform from professional division to real-virtual division. In DeFACT, human workers are only responsible for 5% physical and mental work that is complex and creative, and the robotic and digital workers can take care of the rest. The perceptual and cognitive intelligence of digital workers are intensified by a manufacturing foundation model (MF-PC), where calibration and certification (C&C), and verification and validation (V&V) guarantee not only the accuracy of task models, but also the interpretability and controllability of feature learning. As a case study, the workflow of customized shoes of SANBODY Technology Company is illustrated to show how DeFACT breaks the time and space constraints, avoids production waste caused by aesthetic discrepancies with consumers, and truly realizes flexible manufacturing.",
        "authors": [
            "Jing Yang",
            "Shimeng Li",
            "Xiaoxi Wang",
            "Jingwei Lu",
            "Huaiyu Wu",
            "Xiao Wang"
        ],
        "citations": 17,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey",
        "abstract": "As artificial intelligence (AI) continues to rapidly evolve, the realm of Earth and atmospheric sciences is increasingly adopting data-driven models, powered by progressive developments in deep learning (DL). Specifically, DL techniques are extensively utilized to decode the chaotic and nonlinear aspects of Earth systems, and to address climate challenges via understanding weather and climate data. Cutting-edge performance on specific tasks within narrower spatio-temporal scales has been achieved recently through DL. The rise of large models, specifically large language models (LLMs), has enabled fine-tuning processes that yield remarkable outcomes across various downstream tasks, thereby propelling the advancement of general AI. However, we are still navigating the initial stages of crafting general AI for weather and climate. In this survey, we offer an exhaustive, timely overview of state-of-the-art AI methodologies specifically engineered for weather and climate data, with a special focus on time series and text data. Our primary coverage encompasses four critical aspects: types of weather and climate data, principal model architectures, model scopes and applications, and datasets for weather and climate. Furthermore, in relation to the creation and application of foundation models for weather and climate data understanding, we delve into the field's prevailing challenges, offer crucial insights, and propose detailed avenues for future research. This comprehensive approach equips practitioners with the requisite knowledge to make substantial progress in this domain. Our survey encapsulates the most recent breakthroughs in research on large, data-driven models for weather and climate data understanding, emphasizing robust foundations, current advancements, practical applications, crucial resources, and prospective research opportunities.",
        "authors": [
            "Shengchao Chen",
            "Guodong Long",
            "Jing Jiang",
            "Dikai Liu",
            "Chengqi Zhang"
        ],
        "citations": 16,
        "references": 345,
        "year": 2023
    },
    {
        "title": "Federated Generative Learning with Foundation Models",
        "abstract": "Existing approaches in Federated Learning (FL) mainly focus on sending model parameters or gradients from clients to a server. However, these methods are plagued by significant inefficiency, privacy, and security concerns. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning. In this framework, each client can create text embeddings that are tailored to their local data, and send embeddings to the server. Then the informative training data can be synthesized remotely on the server using foundation generative models with these embeddings, which can benefit FL tasks. Our proposed framework offers several advantages, including increased communication efficiency, robustness to data heterogeneity, substantial performance improvements, and enhanced privacy protection. We validate these benefits through extensive experiments conducted on 12 datasets. For example, on the ImageNet100 dataset with a highly skewed data distribution, our method outperforms FedAvg by 12% in a single communication round, compared to FedAvg's performance over 200 communication rounds. We have released the code for all experiments conducted in this study.",
        "authors": [
            "J. Zhang",
            "Xiaohua Qi",
            "Bo-Lu Zhao"
        ],
        "citations": 14,
        "references": 91,
        "year": 2023
    },
    {
        "title": "Promise: Prompt-Driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models",
        "abstract": "To address prevalent issues in medical imaging, such as data acquisition challenges and label availability, transfer learning from natural to medical image domains serves as a viable strategy to produce reliable segmentation results. However, several existing barriers between domains need to be broken down, including addressing contrast discrepancies, managing anatomical variability, and adapting 2D pretrained models for 3D segmentation tasks. In this paper, we propose ProMISe, a prompt-driven 3D medical image segmentation model using only a single point prompt to leverage knowledge from a pretrained 2D image foundation model. In particular, we use the pretrained vision transformer from the Segment Anything Model (SAM) and integrate lightweight adapters to extract depth-related (3D) spatial context without updating the pretrained weights. For robust results, a hybrid network with complementary encoders is designed, and a boundary-aware loss is proposed to achieve precise boundaries. We evaluate our model on two public datasets for colon and pancreas tumor segmentations, respectively. Compared to the state-of- the-art segmentation methods with and without prompt engineering, our proposed method achieves superior performance. The code is publicly available at https://github.com/MedICL-VU/ProMISe",
        "authors": [
            "Hao Li",
            "Han Liu",
            "Dewei Hu",
            "Jiacheng Wang",
            "I. Oguz"
        ],
        "citations": 14,
        "references": 21,
        "year": 2023
    },
    {
        "title": "Decentralized Training of Foundation Models in Heterogeneous Environments",
        "abstract": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational\"tasklets\"in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron).",
        "authors": [
            "Binhang Yuan",
            "Yongjun He",
            "Jared Davis",
            "Tianyi Zhang",
            "Tri Dao",
            "Beidi Chen",
            "Percy Liang",
            "Christopher Ré",
            "Ce Zhang"
        ],
        "citations": 75,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Gesture-Informed Robot Assistance via Foundation Models",
        "abstract": "Gestures serve as a fundamental and significant mode of non-verbal communication among humans. Deictic gestures (such as pointing towards an object), in particular, offer valuable means of efficiently expressing intent in situations where language is inaccessible, restricted, or highly specialized. As a result, it is essential for robots to comprehend gestures in order to infer human intentions and establish more effective coordination with them. Prior work often rely on a rigid hand-coded library of gestures along with their meanings. However, interpretation of gestures is often context-dependent, requiring more flexibility and common-sense reasoning. In this work, we propose a framework, GIRAF, for more flexibly interpreting gesture and language instructions by leveraging the power of large language models. Our framework is able to accurately infer human intent and contextualize the meaning of their gestures for more effective human-robot collaboration. We instantiate the framework for interpreting deictic gestures in table-top manipulation tasks and demonstrate that it is both effective and preferred by users, achieving 70% higher success rates than the baseline. We further demonstrate GIRAF's ability on reasoning about diverse types of gestures by curating a GestureInstruct dataset consisting of 36 different task scenarios. GIRAF achieved 81% success rate on finding the correct plan for tasks in GestureInstruct. Website: https://tinyurl.com/giraf23",
        "authors": [
            "Li-Heng Lin",
            "Yuchen Cui",
            "Yilun Hao",
            "Fei Xia",
            "Dorsa Sadigh"
        ],
        "citations": 13,
        "references": 33,
        "year": 2023
    },
    {
        "title": "Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants",
        "abstract": "Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.",
        "authors": [
            "Tianyu Yu",
            "Jinyi Hu",
            "Yuan Yao",
            "Haoye Zhang",
            "Yue Zhao",
            "Chongyi Wang",
            "Shanonan Wang",
            "Yinxv Pan",
            "Jiao Xue",
            "Dahai Li",
            "Zhiyuan Liu",
            "Hai-Tao Zheng",
            "Maosong Sun"
        ],
        "citations": 13,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Zero-Shot Robustification of Zero-Shot Models With Foundation Models",
        "abstract": "Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and show an average improvement of 15.98% on worst group accuracy, with trivial decrease in overall accuracy over several zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible with a variety of pretrained and language models and propose a way to further boost performance with a zero-shot adaptation variant.",
        "authors": [
            "Dyah Adila",
            "Changho Shin",
            "Lin Cai",
            "Frederic Sala"
        ],
        "citations": 12,
        "references": 56,
        "year": 2023
    },
    {
        "title": "One-shot Localization and Segmentation of Medical Images with Foundation Models",
        "abstract": "Recent advances in Vision Transformers (ViT) and Stable Diffusion (SD) models with their ability to capture rich semantic features of the image have been used for image correspondence tasks on natural images. In this paper, we examine the ability of a variety of pre-trained ViT (DINO, DINOv2, SAM, CLIP) and SD models, trained exclusively on natural images, for solving the correspondence problems on medical images. While many works have made a case for in-domain training, we show that the models trained on natural images can offer good performance on medical images across different modalities (CT,MR,Ultrasound) sourced from various manufacturers, over multiple anatomical regions (brain, thorax, abdomen, extremities), and on wide variety of tasks. Further, we leverage the correspondence with respect to a template image to prompt a Segment Anything (SAM) model to arrive at single shot segmentation, achieving dice range of 62%-90% across tasks, using just one image as reference. We also show that our single-shot method outperforms the recently proposed few-shot segmentation method - UniverSeg (Dice range 47%-80%) on most of the semantic segmentation tasks(six out of seven) across medical imaging modalities.",
        "authors": [
            "Deepa Anand",
            "Gurunath Reddy",
            "Vanika Singhal",
            "D. Shanbhag",
            "KS Shriram",
            "Uday Patil",
            "C. Bhushan",
            "Kavitha Manickam",
            "Dawei Gui",
            "R. Mullick",
            "Avinash Gopal",
            "Parminder Bhatia",
            "Taha A. Kass-Hout"
        ],
        "citations": 10,
        "references": 14,
        "year": 2023
    },
    {
        "title": "Leveraging language foundation models for human mobility forecasting",
        "abstract": "In this paper, we propose a novel pipeline that leverages language foundation models for temporal sequential pattern mining, such as for human mobility forecasting tasks. For example, in the task of predicting Place-of-Interest (POI) customer flows, typically the number of visits is extracted from historical logs, and only the numerical data are used to predict visitor flows. In this research, we perform the forecasting task directly on the natural language input that includes all kinds of information such as numerical values and contextual semantic information. Specific prompts are introduced to transform numerical temporal sequences into sentences so that existing language models can be directly applied. We design an AuxMobLCast pipeline for predicting the number of visitors in each POI, integrating an auxiliary POI category classification task with the encoder-decoder architecture. This research provides empirical evidence of the effectiveness of the proposed AuxMobLCast pipeline to discover sequential patterns in mobility forecasting tasks. The results, evaluated on three real-world datasets, demonstrate that pre-trained language foundation models also have good performance in forecasting temporal sequences. This study could provide visionary insights and lead to new research directions for predicting human mobility.",
        "authors": [
            "Hao Xue",
            "Bhanu Prakash Voutharoja",
            "Flora D. Salim"
        ],
        "citations": 53,
        "references": 24,
        "year": 2022
    },
    {
        "title": "ChemBERTa-2: Towards Chemical Foundation Models",
        "abstract": "Large pretrained models such as GPT-3 have had tremendous impact on modern natural language processing by leveraging self-supervised learning to learn salient representations that can be used to readily finetune on a wide variety of downstream tasks. We investigate the possibility of transferring such advances to molecular machine learning by building a chemical foundation model, ChemBERTa-2, using the language of SMILES. While labeled data for molecular prediction tasks is typically scarce, libraries of SMILES strings are readily available. In this work, we build upon ChemBERTa by optimizing the pretraining process. We compare multi-task and self-supervised pretraining by varying hyperparameters and pretraining dataset size, up to 77M compounds from PubChem. To our knowledge, the 77M set constitutes one of the largest datasets used for molecular pretraining to date. We find that with these pretraining improvements, we are competitive with existing state-of-the-art architectures on the MoleculeNet benchmark suite. We analyze the degree to which improvements in pretraining translate to improvement on downstream tasks.",
        "authors": [
            "Walid Ahmad",
            "Elana Simon",
            "Seyone Chithrananda",
            "Gabriel Grand",
            "Bharath Ramsundar"
        ],
        "citations": 106,
        "references": 21,
        "year": 2022
    },
    {
        "title": "Brain-Inspired Remote Sensing Foundation Models and Open Problems: A Comprehensive Survey",
        "abstract": "The foundation model (FM) has garnered significant attention for its remarkable transfer performance in downstream tasks. Typically, it undergoes task-agnostic pretraining on a large dataset and can be efficiently adapted to various downstream applications through fine-tuning. While FMs have been extensively explored in language and other domains, their potential in remote sensing has also begun to attract scholarly interest. However, comprehensive investigations and performance comparisons of these models on remote sensing tasks are currently lacking. In this survey, we provide essential background knowledge by introducing key technologies and recent developments in FMs. Subsequently, we explore essential downstream applications in remote sensing, covering classification, localization, and understanding. Our analysis encompasses over 30 FMs in both natural and remote sensing fields, and we conduct extensive experiments on more than 10 datasets, evaluating global feature representation, local feature representation, and target localization. Through quantitative assessments, we highlight the distinctions among various FMs and confirm that pretrained large-scale natural FMs can also deliver outstanding performance in remote sensing tasks. After that, we systematically presented a brain-inspired framework for remote sensing foundation models (RSFMs). We delve into the brain-inspired characteristics in this framework, including structure, perception, learning, and cognition. To conclude, we summarize 12 open problems in RSFMs, providing potential research directions. Our survey offers valuable insights into the burgeoning field of RSFMs and aims to foster further advancements in this exciting area.",
        "authors": [
            "Licheng Jiao",
            "Zhongjian Huang",
            "Xiaoqiang Lu",
            "Xu Liu",
            "Yuting Yang",
            "Jiaxuan Zhao",
            "Jinyue Zhang",
            "B. Hou",
            "Shuyuan Yang",
            "F. Liu",
            "Wenping Ma",
            "Lingling Li",
            "Xiangrong Zhang",
            "Puhua Chen",
            "Zhixi Feng",
            "Xu Tang",
            "Yuwei Guo",
            "Dou Quan",
            "Shuang Wang",
            "Weibin Li",
            "Jing Bai",
            "Yangyang Li",
            "Ronghua Shang",
            "Jie Feng"
        ],
        "citations": 8,
        "references": 307,
        "year": 2023
    },
    {
        "title": "Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models",
        "abstract": "A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. We present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (MLAC). In a small-scale experiment, we show MLAC can largely prevent a BERT-style model from being re-purposed to perform gender identification without harming the model’s ability to perform profession classification.",
        "authors": [
            "E. Mitchell",
            "Peter Henderson",
            "Christopher D. Manning",
            "Dan Jurafsky",
            "Chelsea Finn"
        ],
        "citations": 42,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Continual Learning with Foundation Models: An Empirical Study of Latent Replay",
        "abstract": "Rapid development of large-scale pre-training has resulted in foundation models that can act as effective feature extractors on a variety of downstream tasks and domains. Motivated by this, we study the efficacy of pre-trained vision models as a foundation for downstream continual learning (CL) scenarios. Our goal is twofold. First, we want to understand the compute-accuracy trade-off between CL in the raw-data space and in the latent space of pre-trained encoders. Second, we investigate how the characteristics of the encoder, the pre-training algorithm and data, as well as of the resulting latent space affect CL performance. For this, we compare the efficacy of various pre-trained models in large-scale benchmarking scenarios with a vanilla replay setting applied in the latent and in the raw-data space. Notably, this study shows how transfer, forgetting, task similarity and learning are dependent on the input data characteristics and not necessarily on the CL algorithms. First, we show that under some circumstances reasonable CL performance can readily be achieved with a non-parametric classifier at negligible compute. We then show how models pre-trained on broader data result in better performance for various replay sizes. We explain this with representational similarity and transfer properties of these representations. Finally, we show the effectiveness of self-supervised pre-training for downstream domains that are out-of-distribution as compared to the pre-training domain. We point out and validate several research directions that can further increase the efficacy of latent CL including representation ensembling. The diverse set of datasets used in this study can serve as a compute-efficient playground for further CL research. The codebase is available under https://github.com/oleksost/latent_CL.",
        "authors": [
            "O. Ostapenko",
            "Timothée Lesort",
            "P. Rodr'iguez",
            "Md Rifat Arefin",
            "Arthur Douillard",
            "I. Rish",
            "Laurent Charlin"
        ],
        "citations": 39,
        "references": 98,
        "year": 2022
    },
    {
        "title": "@ CREPE: Can Vision-Language Foundation Models Reason Compositionally?",
        "abstract": "A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, we find that-across 7 architectures trained with 4 algorithms on massive datasets-they struggle at compositionality. To arrive at this conclusion, we introduce a new compositionality evaluation benchmark, @ CREPE, which measures two important aspects of compositionality identified by cognitive science literature: system-aticity and productivity. To measure systematicity, CREPE consists of a test dataset containing over 370K image-text pairs and three different seen-unseen splits. The three splits are designed to test models trained on three popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also generate 325K, 316K, and 309K hard negative captions for a subset of the pairs. To test productivity, CREPE contains 17 K image-text pairs with nine different complexities plus 278K hard negative captions with atomic, swapping and negation foils. The datasets are generated by repurposing the Visual Genome scene graphs and region descriptions and applying handcrafted templates and GPT-3. For systematicity, we find that model performance decreases consistently when novel compositions dominate the retrieval set, with Recall@1 dropping by up to 9%. For productivity, models' retrieval success decays as complexity increases, frequently nearing random chance at high complexity. These results hold regardless of model and training dataset size.",
        "authors": [
            "Zixian Ma",
            "Jerry Hong",
            "Mustafa Omer Gul",
            "Mona Gandhi",
            "Irena Gao",
            "Ranjay Krishna"
        ],
        "citations": 100,
        "references": 102,
        "year": 2022
    },
    {
        "title": "EHR foundation models improve robustness in the presence of temporal distribution shift",
        "abstract": null,
        "authors": [
            "L. Guo",
            "E. Steinberg",
            "S. Fleming",
            "J. Posada",
            "J. Lemmon",
            "S. Pfohl",
            "N. Shah",
            "J. Fries",
            "L. Sung"
        ],
        "citations": 30,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Assemble Foundation Models for Automatic Code Summarization",
        "abstract": "Automatic code summarization is beneficial to software development and maintenance since it reduces the burden of manual tasks. Currently, artificial intelligence is undergoing a paradigm shift. The foundation models pretrained on massive data and finetuned to downstream tasks surpass specially customized models. This trend inspired us to consider reusing foundation models instead of learning from scratch. Based on this, we propose a flexible and robust approach for automatic code summarization based on neural networks. We assemble available foundation models, such as CodeBERT and GPT-2, into a single model named AdaMo. Moreover, we utilize Gaussian noise as the simulation of contextual information to optimize the latent representation. Furthermore, we introduce two adaptive schemes from the perspective of knowledge transfer, namely continuous pretraining and intermediate finetuning, and design intermediate stage tasks for general sequence-to-sequence learning. Finally, we evaluate AdaMo against a benchmark dataset for code summarization, by comparing it with state-of-the-art models.",
        "authors": [
            "Jian Gu",
            "P. Salza",
            "H. Gall"
        ],
        "citations": 31,
        "references": 75,
        "year": 2022
    },
    {
        "title": "Merak: An Efficient Distributed DNN Training Framework With Automated 3D Parallelism for Giant Foundation Models",
        "abstract": "Foundation models are in the process of becoming the dominant deep learning technology. Pretraining a foundation model is always time-consuming due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the pretraining process is extremely memory- and communication-intensive. These challenges make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism, and tensor model parallelism, to achieve high training efficiency. However, current 3D parallelism frameworks still encounter two issues: i) they are not transparent to model developers, requiring manual model modification to parallelize training, and ii) their utilization of computation resources, GPU memory, and network bandwidth is insufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automatically deploys 3D parallelism with an automatic model partitioner, which includes a graph-sharding algorithm and proxy node-based model graph. Merak also offers a non-intrusive API to scale out foundation model training with minimal code modification. In addition, we design a high-performance 3D parallel runtime engine that employs several techniques to exploit available training resources, including a shifted critical path pipeline schedule that increases computation utilization, stage-aware recomputation that makes use of idle worker memory, and sub-pipelined tensor model parallelism that overlaps communication and computation. Experiments on 64 GPUs demonstrate Merak's capability to speed up training performance over state-of-the-art 3D parallelism frameworks of models with 1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42, 1.39, 1.43, and 1.61×, respectively.",
        "authors": [
            "Zhiquan Lai",
            "Shengwei Li",
            "Xudong Tang",
            "Ke-shi Ge",
            "Weijie Liu",
            "Yabo Duan",
            "Linbo Qiao",
            "Dongsheng Li"
        ],
        "citations": 32,
        "references": 55,
        "year": 2022
    },
    {
        "title": "A Survey of Deep Active Learning for Foundation Models",
        "abstract": null,
        "authors": [
            "Tianjiao Wan",
            "Kele Xu",
            "Ting Yu",
            "Xu Wang",
            "Dawei Feng",
            "Bo Ding",
            "Huaimin Wang"
        ],
        "citations": 8,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Parallel Transportation in TransVerse: From Foundation Models to DeCAST",
        "abstract": "Rapid development of AI technologies has propelled the seamless integration of physical and cyber worlds with various kinds of online/offline information collected from millions of multimodal sensing systems. The complexity, diversity and uncertainty inherited in such systems, such as Intelligent Transportation Systems (ITSs), have gone far beyond human capacity of managing and controlling. Our team is among the first to propose the idea of utilizing the nearly unlimited computational resources in cyberspace to construct a bottom-up and top-down combined artificial ITSs for testing, experimenting, representation, verification, and validation of physical ITSs. Especially, the parallel transportation has been developed for safer, smarter, greener, and more reliable transportation services. After three decades of research and field studies, the DeCAST in Transverse, i.e., Decentralized/Distributed Autonomous Operations/Organizations (DAO) in transportation systems, has been envisioned. In this paper, we introduce its architecture, operational processes, software and hardware platforms, and real world applications. Specifically, a transportation foundation model driven by artificial transportation systems, parallel learning and federated intelligence, named TengYun, is outlined for DeCAST.",
        "authors": [
            "Chen Zhao",
            "Xiao Wang",
            "Yisheng Lv",
            "Yonglin Tian",
            "Yilun Lin",
            "Fei Wang"
        ],
        "citations": 6,
        "references": 158,
        "year": 2023
    },
    {
        "title": "Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?",
        "abstract": "Task specification is at the core of programming autonomous robots. A low-effort modality for task specification is critical for engagement of non-expert end-users and ultimate adoption of personalized robot agents. A widely studied approach to task specification is through goals, using either compact state vectors or goal images from the same robot scene. The former is hard to interpret for non-experts and necessitates detailed state estimation and scene understanding. The latter requires the generation of desired goal image, which often requires a human to complete the task, defeating the purpose of having autonomous robots. In this work, we explore alternate and more general forms of goal specification that are expected to be easier for humans to specify and use such as images obtained from the internet, hand sketches that provide a visual description of the desired task, or simple language descriptions. As a preliminary step towards this, we investigate the capabilities of large scale pre-trained models (foundation models) for zero-shot goal specification, and find promising results in a collection of simulated robot manipulation tasks and real-world datasets.",
        "authors": [
            "Yuchen Cui",
            "S. Niekum",
            "Abhi Gupta",
            "Vikash Kumar",
            "A. Rajeswaran"
        ],
        "citations": 66,
        "references": 46,
        "year": 2022
    },
    {
        "title": "Exploring Parameter-Efficient Fine-Tuning to Enable Foundation Models in Federated Learning",
        "abstract": "Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown to be effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters, known as the \"Foundation Models.\" In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fine-tuning in federated learning and thus introduce a new framework: FedPEFT. Specifically, we systemically evaluate the performance of FedPEFT across a variety of client stability, data distribution, and differential privacy settings. By only locally tuning and globally sharing a small portion of the model weights, significant reductions in the total communication overhead can be achieved while maintaining competitive or even better performance in a wide range of federated learning scenarios, providing insight into a new paradigm for practical and effective federated systems.",
        "authors": [
            "Guangyu Sun",
            "Mat'ias Mendieta",
            "Taojiannan Yang",
            "Chen Chen"
        ],
        "citations": 14,
        "references": 63,
        "year": 2022
    },
    {
        "title": "Can Foundation Models Talk Causality?",
        "abstract": "Foundation models are subject to an ongoing heated debate, leaving open the question of progress towards AGI and dividing the community into two camps: the ones who see the arguably impressive results as evidence to the scaling hypothesis, and the others who are worried about the lack of interpretability and reasoning capabilities. By investigating to which extent causal representations might be captured by these large scale language models, we make a humble efforts towards resolving the ongoing philosophical conflicts.",
        "authors": [
            "Moritz Willig",
            "M. Zecevic",
            "D. Dhami",
            "K. Kersting"
        ],
        "citations": 21,
        "references": 44,
        "year": 2022
    },
    {
        "title": "NamedMask: Distilling Segmenters from Complementary Foundation Models",
        "abstract": "The goal of this work is to segment and name regions of images without access to pixel-level labels during training. To tackle this task, we construct segmenters by distilling the complementary strengths of two foundation models. The first, CLIP [26], exhibits the ability to assign names to image content but lacks an accessible representation of object structure. The second, DINO [5], captures the spatial extent of objects but has no knowledge of object names. Our method, termed NamedMask, begins by using CLIP to construct category-specific archives of images. These images are pseudo-labelled with a category-agnostic salient object detector bootstrapped from DINO, then refined by category-specific segmenters using the CLIP archive labels. Thanks to the high quality of the refined masks, we show that a standard segmentation architecture trained on these archives with appropriate data augmentation achieves impressive semantic segmentation abilities for both single-object and multi-object images. As a result, our proposed NamedMask performs favourably against a range of prior work on five benchmarks including the VOC2012, COCO and large-scale ImageNet-S datasets.",
        "authors": [
            "Gyungin Shin",
            "Weidi Xie",
            "Samuel Albanie"
        ],
        "citations": 20,
        "references": 46,
        "year": 2022
    },
    {
        "title": "FETA: Towards Specializing Foundation Models for Expert Task Applications",
        "abstract": "Foundation Models (FMs) have demonstrated unprecedented capabilities including zero-shot learning, high fidelity data synthesis, and out of domain generalization. However, as we show in this paper, FMs still have poor out-of-the-box performance on expert tasks (e.g. retrieval of car manuals technical illustrations from language queries), data for which is either unseen or belonging to a long-tail part of the data distribution of the huge datasets used for FM pre-training. This underlines the necessity to explicitly evaluate and finetune FMs on such expert tasks, arguably ones that appear the most in practical real-world applications. In this paper, we propose a first of its kind FETA benchmark built around the task of teaching FMs to understand technical documentation, via learning to match their graphical illustrations to corresponding language descriptions. Our FETA benchmark focuses on text-to-image and image-to-text retrieval in public car manuals and sales catalogue brochures. FETA is equipped with a procedure for completely automatic annotation extraction (code would be released upon acceptance), allowing easy extension of FETA to more documentation types and application domains in the future. Our automatic annotation leads to an automated performance metric shown to be consistent with metrics computed on human-curated annotations (also released). We provide multiple baselines and analysis of popular FMs on FETA leading to several interesting findings that we believe would be very valuable to the FM community, paving the way towards real-world application of FMs for practical expert tasks currently 'overlooked' by standard benchmarks focusing on common objects.",
        "authors": [
            "Amit Alfassy",
            "Assaf Arbelle",
            "Oshri Halimi",
            "Sivan Harary",
            "Roei Herzig",
            "Eli Schwartz",
            "Rameswar Panda",
            "Michele Dolfi",
            "Christoph Auer",
            "Kate Saenko",
            "P. Staar",
            "R. Feris",
            "Leonid Karlinsky"
        ],
        "citations": 18,
        "references": 81,
        "year": 2022
    },
    {
        "title": "Quantifying Uncertainty in Foundation Models via Ensembles",
        "abstract": "As large pre-trained foundation models begin to proliferate and have increasing impact in real-world applications, it is of utmost importance to guarantee that these models are trustworthy and reliable. An important capability towards this is for models to know what they don’t know models that are capable of quantifying uncertainty about their own outputs (potentially on inputs very different from what they were trained on). Uncertainty quantification can give guidance on when to trust the model. It can also lead to new capabilities, such as active learning (Settles, 2009) or exploration behavior in decision-making agents (Baranes & Oudeyer, 2013) driven by the goal of reducing uncertainty.",
        "authors": [
            "Meiqi Sun",
            "Wilson Yan",
            "P. Abbeel",
            "Igor Mordatch"
        ],
        "citations": 16,
        "references": 14,
        "year": 2022
    },
    {
        "title": "Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned",
        "abstract": "Foundation models pre-trained on large corpora demonstrate significant gains across many natural language processing tasks and domains e.g., law, healthcare, education, etc. However, only limited efforts have investigated the opportunities and limitations of applying these powerful models to science and security applications. In this work, we develop foundation models of scientific knowledge for chemistry to augment scientists with the advanced ability to perceive and reason at scale previously unimagined. Specifically, we build large-scale (1.47B parameter) general-purpose models for chemistry that can be effectively used to perform a wide range of in-domain and out-of-domain tasks. Evaluating these models in a zero-shot setting, we analyze the effect of model and data scaling, knowledge depth, and temporality on model performance in context of model training efficiency. Our novel findings demonstrate that (1) model size significantly contributes to the task performance when evaluated in a zero-shot setting; (2) data quality (aka diversity) affects model performance more than data quantity; (3) similarly, unlike previous work, temporal order of the documents in the corpus boosts model performance only for specific tasks, e.g., SciQ; and (4) models pre-trained from scratch perform better on in-domain tasks than those tuned from general-purpose models like Open AI’s GPT-2.",
        "authors": [
            "Sameera Horawalavithana",
            "Ellyn Ayton",
            "Shivam Sharma",
            "Scott Howland",
            "Megha Subramanian",
            "Scott Vasquez",
            "Robin Cosbey",
            "M. Glenski",
            "Svitlana Volkova"
        ],
        "citations": 17,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Differentially Private Bias-Term Fine-tuning of Foundation Models",
        "abstract": "We study the problem of differentially private (DP) fine-tuning of large pre-trained models -- a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about 0.1% of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is 2~30X faster and uses 2~8X less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods. We open-source our code at FastDP (https://github.com/awslabs/fast-differential-privacy).",
        "authors": [
            "Zhiqi Bu",
            "Yu-Xiang Wang",
            "Sheng Zha",
            "G. Karypis"
        ],
        "citations": 43,
        "references": 70,
        "year": 2022
    },
    {
        "title": "On the power of foundation models",
        "abstract": "With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be seen as a new type of generalization theorem, showing that the foundation model can generate unseen objects from the target category (e.g., images) using the structural information from the source category (e.g., texts). Along the way, we provide a categorical framework for supervised and self-supervised learning, which might be of independent interest.",
        "authors": [
            "Yang Yuan"
        ],
        "citations": 29,
        "references": 83,
        "year": 2022
    },
    {
        "title": "Risk of Bias in Chest Radiography Deep Learning Foundation Models",
        "abstract": "Purpose To analyze a recently published chest radiography foundation model for the presence of biases that could lead to subgroup performance disparities across biologic sex and race. Materials and Methods This Health Insurance Portability and Accountability Act–compliant retrospective study used 127 118 chest radiographs from 42 884 patients (mean age, 63 years ± 17 [SD]; 23 623 male, 19 261 female) from the CheXpert dataset that were collected between October 2002 and July 2017. To determine the presence of bias in features generated by a chest radiography foundation model and baseline deep learning model, dimensionality reduction methods together with two-sample Kolmogorov–Smirnov tests were used to detect distribution shifts across sex and race. A comprehensive disease detection performance analysis was then performed to associate any biases in the features to specific disparities in classification performance across patient subgroups. Results Ten of 12 pairwise comparisons across biologic sex and race showed statistically significant differences in the studied foundation model, compared with four significant tests in the baseline model. Significant differences were found between male and female (P < .001) and Asian and Black (P < .001) patients in the feature projections that primarily capture disease. Compared with average model performance across all subgroups, classification performance on the “no finding” label decreased between 6.8% and 7.8% for female patients, and performance in detecting “pleural effusion” decreased between 10.7% and 11.6% for Black patients. Conclusion The studied chest radiography foundation model demonstrated racial and sex-related bias, which led to disparate performance across patient subgroups; thus, this model may be unsafe for clinical applications. Keywords: Conventional Radiography, Computer Application-Detection/Diagnosis, Chest Radiography, Bias, Foundation Models Supplemental material is available for this article. Published under a CC BY 4.0 license. See also commentary by Czum and Parr in this issue.",
        "authors": [
            "B. Glocker",
            "Charles Jones",
            "Mélanie Roschewitz",
            "S. Winzeck"
        ],
        "citations": 28,
        "references": 35,
        "year": 2022
    },
    {
        "title": "On the Opportunities and Risks of Foundation Models for Natural Language Processing in Radiology.",
        "abstract": null,
        "authors": [
            "Walter F. Wiggins",
            "Ali S. Tejani"
        ],
        "citations": 30,
        "references": 4,
        "year": 2022
    },
    {
        "title": "Language-Aware Soft Prompting for Vision & Language Foundation Models",
        "abstract": "This paper is on soft prompt learning for Vision & Language (V&L) models. Similarly to their NLP counterparts, V&L models can be adapted to a downstream task by learning soft continuous prompts using a few training examples. Current methods learn the soft prompts by minimizing a cross-entropy loss using as class weights the features obtained by passing the prompts plus the class names through the text encoder. Such methods, however, signiﬁcantly overﬁt the training data suffering from large accuracy degradation when tested on unseen classes from the same domain. Our main contribution, in this paper, is a surprisingly simple approach to alleviate this problem: we use a second cross entropy loss to minimize the distance between the learned soft prompts and a set of hand-engineered manual prompts (obtained by prompt engineering). The proposed loss can be interpreted in multiple ways including as a regularizer, as a means for language-based augmentation, and as a way of learning more discriminative class centroids. Importantly, our formulation is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through extensive evaluations on 11 datasets, we show that our approach (a) signiﬁcantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the ﬁrst time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for the majority of the test datasets. Code will be made available.",
        "authors": [
            "Adrian Bulat",
            "Georgios Tzimiropoulos"
        ],
        "citations": 17,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Foundation Models for Transportation Intelligence: ITS Convergence in TransVerse",
        "abstract": "Smart cities are our aspiration for a better life where transportation intelligence is indispensable. Recent technological advances in intelligent transportation systems have opened up new possibilities for smart mobility in smart cities. Here we present TengYun, a transportation foundation model designed and developed with parallel learning and federated intelligence for our transportation metaverse called TransVerse. TengYun enables decentralized/distributed autonomous organizations with decentralized/ distributed operations, as well as various federated technologies, from federated security, federated control, federated management, federated services, to federated ecology for transportation intelligence in smart cities. An example for a federation of transportation transformers is discussed for illustrating the operating procedure of TengYun.",
        "authors": [
            "Chen Zhao",
            "Xingyuan Dai",
            "Yisheng Lv",
            "Yonglin Tian",
            "Yuhai Ren",
            "Fei Wang",
            "Fei Wang"
        ],
        "citations": 18,
        "references": 20,
        "year": 2022
    },
    {
        "title": "Rethinking data-driven networking with foundation models: challenges and opportunities",
        "abstract": "Foundational models have caused a paradigm shift in the way artificial intelligence (AI) systems are built. They have had a major impact in natural language processing (NLP), and several other domains, not only reducing the amount of required labeled data or even eliminating the need for it, but also significantly improving performance on a wide range of tasks. We argue foundation models can have a similar profound impact on network traffic analysis, and management. More specifically, we show that network data shares several of the properties that are behind the success of foundational models in linguistics. For example, network data contains rich semantic content, and several of the networking tasks (e.g., traffic classification, generation of protocol implementations from specification text, anomaly detection) can find similar counterparts in NLP (e.g., sentiment analysis, translation from natural language to code, out-of-distribution). However, network settings also present unique characteristics and challenges that must be overcome. Our contribution is in highlighting the opportunities and challenges at the intersection of foundation models and networking.",
        "authors": [
            "Franck Le",
            "M. Srivatsa",
            "R. Ganti",
            "Vyas Sekar"
        ],
        "citations": 11,
        "references": 109,
        "year": 2022
    },
    {
        "title": "Foundation Models in Healthcare: Opportunities, Biases and Regulatory Prospects in Europe",
        "abstract": null,
        "authors": [
            "M. Wójcik"
        ],
        "citations": 15,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Towards Galaxy Foundation Models with Hybrid Contrastive Learning",
        "abstract": "New astronomical tasks are often related to earlier tasks for which labels have already been collected. We adapt the contrastive framework BYOL to leverage those labels as a pretraining task while also enforcing augmentation invariance. For large-scale pretraining, we introduce GZ-Evo v0.1, a set of 96.5M volunteer responses for 552k galaxy images plus a further 1.34M comparable unlabelled galaxies. Most of the 206 GZ-Evo answers are unknown for any given galaxy, and so our pretraining task uses a Dirichlet loss that naturally handles unknown answers. GZ-Evo pretraining, with or without hybrid learning, improves on direct training even with plentiful downstream labels (+4% accuracy with 44k labels). Our hybrid pretraining/contrastive method further improves downstream accuracy vs. pretraining or contrastive learning, especially in the low-label transfer regime (+6% accuracy with 750 labels).",
        "authors": [
            "Mike Walmsley",
            "I. V. Slijepcevic",
            "Micah Bowles",
            "A. Scaife"
        ],
        "citations": 14,
        "references": 26,
        "year": 2022
    },
    {
        "title": "Risk of Bias in Chest X-ray Foundation Models",
        "abstract": "Foundation models are considered a breakthrough in all applications of AI, promising robust and reusable mechanisms for feature extraction, alleviating the need for large amounts of high quality annotated training data for task-specific prediction models. However, foundation models may potentially encode and even reinforce existing biases present in historic datasets. Given the limited ability to scrutinize foundation models, it remains unclear whether the opportunities outweigh the risks in safety critical applications such as clinical decision making. In our statistical bias analysis of a recently published, and publicly accessible chest X-ray foundation model, we found reasons for concern as the model seems to encode protected characteristics including biological sex and racial identity. When used for the downstream application of disease detection, we observed substantial degradation of performance of the foundation model compared to a standard model with specific disparities in protected subgroups. While research into foundation models for healthcare applications is in an early stage, we hope to raise awareness of the risks by highlighting the importance of conducting thorough bias and subgroup performance analyses.",
        "authors": [
            "B. Glocker",
            "Charles Jones",
            "Mélanie Bernhardt",
            "S. Winzeck"
        ],
        "citations": 8,
        "references": 24,
        "year": 2022
    },
    {
        "title": "Multimodal foundation models are better simulators of the human brain",
        "abstract": "Multimodal learning, especially large-scale multimodal pre-training, has developed rapidly over the past few years and led to the greatest advances in artificial intelligence (AI). Despite its effectiveness, understanding the underlying mechanism of multimodal pre-training models still remains a grand challenge. Revealing the explainability of such models is likely to enable breakthroughs of novel learning paradigms in the AI field. To this end, given the multimodal nature of the human brain, we propose to explore the explainability of multimodal learning models with the aid of non-invasive brain imaging technologies such as functional magnetic resonance imaging (fMRI). Concretely, we first present a newly-designed multimodal foundation model pre-trained on 15 million image-text pairs, which has shown strong multimodal understanding and generalization abilities in a variety of cognitive downstream tasks. Further, from the perspective of neural encoding (based on our foundation model), we find that both visual and lingual encoders trained multimodally are more brain-like compared with unimodal ones. Particularly, we identify a number of brain regions where multimodally-trained encoders demonstrate better neural encoding performance. This is consistent with the findings in existing studies on exploring brain multi-sensory integration. Therefore, we believe that multimodal foundation models are more suitable tools for neuroscientists to study the multimodal signal processing mechanisms in the human brain. Our findings also demonstrate the potential of multimodal foundation models as ideal computational simulators to promote both AI-for-brain and brain-for-AI research.",
        "authors": [
            "Haoyu Lu",
            "Qiongyi Zhou",
            "Nanyi Fei",
            "Zhiwu Lu",
            "Mingyu Ding",
            "Jingyuan Wen",
            "Changde Du",
            "Xin Zhao",
            "Haoran Sun",
            "Huiguang He",
            "J. Wen"
        ],
        "citations": 12,
        "references": 90,
        "year": 2022
    },
    {
        "title": "Foundation models in brief: A historical, socio-technical focus",
        "abstract": "Foundation models can be disruptive for future AI development by scaling up deep learning in terms of model size and training data's breadth and size. These models achieve state-of-the-art performance (often through further adaptation) on a variety of tasks in domains such as natural language processing and computer vision. Foundational models exhibit a novel {emergent behavior}: {In-context learning} enables users to provide a query and a few examples from which a model derives an answer without being trained on such queries. Additionally, {homogenization} of models might replace a myriad of task-specific models with fewer very large models controlled by few corporations leading to a shift in power and control over AI. This paper provides a short introduction to foundation models. It contributes by crafting a crisp distinction between foundation models and prior deep learning models, providing a history of machine learning leading to foundation models, elaborating more on socio-technical aspects, i.e., organizational issues and end-user interaction, and a discussion of future research.",
        "authors": [
            "Johannes Schneider"
        ],
        "citations": 7,
        "references": 63,
        "year": 2022
    },
    {
        "title": "A Case for Business Process-Specific Foundation Models",
        "abstract": "The inception of large language models has helped advance state-of-the-art performance on numerous natural language tasks. This has also opened the door for the development of foundation models for other domains and data modalities such as images, code, and music. In this paper, we argue that business process data representations have unique characteristics that warrant the development of a new class of foundation models to handle tasks like process mining, optimization, and decision making. These models should also tackle the unique challenges of applying AI to business processes which include data scarcity, multi-modal representations, domain specific terminology, and privacy concerns.",
        "authors": [
            "Yara Rizk",
            "P. Venkateswaran",
            "Vatche Isahagian",
            "Vinod Muthusamy"
        ],
        "citations": 7,
        "references": 53,
        "year": 2022
    },
    {
        "title": "Foundation Models for Semantic Novelty in Reinforcement Learning",
        "abstract": "Effectively exploring the environment is a key challenge in reinforcement learning (RL). We address this challenge by defining a novel intrinsic reward based on a foundation model, such as contrastive language image pretraining (CLIP), which can encode a wealth of domain-independent semantic visual-language knowledge about the world. Specifically, our intrinsic reward is defined based on pre-trained CLIP embeddings without any fine-tuning or learning on the target RL task. We demonstrate that CLIP-based intrinsic rewards can drive exploration towards semantically meaningful states and outperform state-of-the-art methods in challenging sparse-reward procedurally-generated environments.",
        "authors": [
            "Tarun Gupta",
            "Peter Karkus",
            "Tong Che",
            "Danfei Xu",
            "M. Pavone"
        ],
        "citations": 7,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Data Management Opportunities for Foundation Models",
        "abstract": "a paradigm shift in industrial machine learning",
        "authors": [
            "Laurel J. Orr",
            "Karan Goel",
            "Christopher Ré",
            "Stanford"
        ],
        "citations": 6,
        "references": 19,
        "year": 2022
    },
    {
        "title": "Me-LLaMA: Foundation Large Language Models for Medical Applications",
        "abstract": "Recent advancements in large language models (LLMs) such as ChatGPT and LLaMA have hinted at their potential to revolutionize medical applications, yet their application in clinical settings often reveals limitations due to a lack of specialized training on medical-specific data. In response to this challenge, this study introduces Me-LLaMA, a novel medical LLM family that includes foundation models – Me-LLaMA 13/70B, along with their chat-enhanced versions – Me-LLaMA 13/70B-chat, developed through continual pre-training and instruction tuning of LLaMA2 using large medical datasets. Our methodology leverages a comprehensive domain-specific data suite, including a large-scale, continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a new medical evaluation benchmark (MIBE) across six critical medical tasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me-LLaMA models achieve overall better performance than existing open-source medical LLMs in zero-shot, few-shot and supervised learning abilities. With task-specific instruction tuning, Me-LLaMA models outperform ChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets. In addition, we investigated the catastrophic forgetting problem, and our results show that Me-LLaMA models outperform other open-source medical LLMs in mitigating this issue. Me-LLaMA is one of the largest open-source medical foundation LLMs that use both biomedical and clinical data. It exhibits superior performance across both general and medical tasks compared to other open-source medical LLMs, rendering it an attractive choice for medical AI applications. We release our models, datasets, and evaluation scripts at: https://github.com/BIDS-Xu-Lab/Me-LLaMA.",
        "authors": [
            "Qianqian Xie",
            "Qingyu Chen",
            "Aokun Chen",
            "C.A.I. Peng",
            "Yan Hu",
            "Fongci Lin",
            "Xueqing Peng",
            "Jimin Huang",
            "Jeffrey Zhang",
            "V. Keloth",
            "Xingyu Zhou",
            "Huan He",
            "Lucila Ohno-Machido",
            "Yonghui Wu",
            "Hua Xu",
            "Jiang Bian"
        ],
        "citations": 31,
        "references": 70,
        "year": 2024
    },
    {
        "title": "The Llama 3 Herd of Models",
        "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
        "authors": [
            "Abhimanyu Dubey",
            "Abhinav Jauhri",
            "Abhinav Pandey",
            "Abhishek Kadian",
            "Ahmad Al-Dahle",
            "Aiesha Letman",
            "Akhil Mathur",
            "Alan Schelten",
            "Amy Yang",
            "Angela Fan",
            "Anirudh Goyal",
            "Anthony S. Hartshorn",
            "Aobo Yang",
            "Archi Mitra",
            "Archie Sravankumar",
            "Artem Korenev",
            "Arthur Hinsvark",
            "Arun Rao",
            "Aston Zhang",
            "Aurélien Rodriguez",
            "Austen Gregerson",
            "Ava Spataru",
            "Baptiste Rozière",
            "Bethany Biron",
            "Binh Tang",
            "Bobbie Chern",
            "C. Caucheteux",
            "Chaya Nayak",
            "Chloe Bi",
            "Chris Marra",
            "Chris McConnell",
            "Christian Keller",
            "Christophe Touret",
            "Chunyang Wu",
            "Corinne Wong",
            "Cristian Cantón Ferrer",
            "Cyrus Nikolaidis",
            "Damien Allonsius",
            "Daniel Song",
            "Danielle Pintz",
            "Danny Livshits",
            "David Esiobu",
            "Dhruv Choudhary",
            "Dhruv Mahajan",
            "Diego Garcia-Olano",
            "Diego Perino",
            "Dieuwke Hupkes",
            "Egor Lakomkin",
            "Ehab A. AlBadawy",
            "Elina Lobanova",
            "Emily Dinan",
            "Eric Michael Smith",
            "Filip Radenovic",
            "Frank Zhang",
            "Gabriele Synnaeve",
            "Gabrielle Lee",
            "Georgia Lewis Anderson",
            "Graeme Nail",
            "Grégoire Mialon",
            "Guanglong Pang",
            "Guillem Cucurell",
            "Hailey Nguyen",
            "Hannah Korevaar",
            "Hu Xu",
            "Hugo Touvron",
            "Iliyan Zarov",
            "Imanol Arrieta Ibarra",
            "Isabel M. Kloumann",
            "Ishan Misra",
            "Ivan Evtimov",
            "Jade Copet",
            "Jaewon Lee",
            "J. Geffert",
            "Jana Vranes",
            "Jason Park",
            "Jay Mahadeokar",
            "Jeet Shah",
            "J. V. D. Linde",
            "Jennifer Billock",
            "Jenny Hong",
            "Jenya Lee",
            "Jeremy Fu",
            "Jianfeng Chi",
            "Jianyu Huang",
            "Jiawen Liu",
            "Jie Wang",
            "Jiecao Yu",
            "Joanna Bitton",
            "Joe Spisak",
            "Jongsoo Park",
            "Joseph Rocca",
            "Joshua Johnstun",
            "Joshua Saxe",
            "Ju-Qing Jia",
            "Kalyan Vasuden Alwala",
            "K. Upasani",
            "Kate Plawiak",
            "Keqian Li",
            "K. Heafield",
            "Kevin Stone",
            "Khalid El-Arini",
            "Krithika Iyer",
            "Kshitiz Malik",
            "Kuenley Chiu",
            "Kunal Bhalla",
            "Lauren Rantala-Yeary",
            "L. Maaten",
            "Lawrence Chen",
            "Liang Tan",
            "Liz Jenkins",
            "Louis Martin",
            "Lovish Madaan",
            "Lubo Malo",
            "Lukas Blecher",
            "Lukas Landzaat",
            "Luke de Oliveira",
            "Madeline Muzzi",
            "M. Pasupuleti",
            "Mannat Singh",
            "Manohar Paluri",
            "Marcin Kardas",
            "Mathew Oldham",
            "Mathieu Rita",
            "Maya Pavlova",
            "M. Kambadur",
            "Mike Lewis",
            "Min Si",
            "Mitesh Kumar Singh",
            "Mona Hassan",
            "Naman Goyal",
            "Narjes Torabi",
            "Nikolay Bashlykov",
            "Nikolay Bogoychev",
            "Niladri S. Chatterji",
            "Olivier Duchenne",
            "Onur cCelebi",
            "Patrick Alrassy",
            "Pengchuan Zhang",
            "Pengwei Li",
            "Petar Vasić",
            "Peter Weng",
            "Prajjwal Bhargava",
            "Pratik Dubal",
            "Praveen Krishnan",
            "Punit Singh Koura",
            "Puxin Xu",
            "Qing He",
            "Qingxiao Dong",
            "Ragavan Srinivasan",
            "Raj Ganapathy",
            "Ramon Calderer",
            "Ricardo Silveira Cabral",
            "Robert Stojnic",
            "Roberta Raileanu",
            "Rohit Girdhar",
            "Rohit Patel",
            "R. Sauvestre",
            "Ronnie Polidoro",
            "Roshan Sumbaly",
            "Ross Taylor",
            "Ruan Silva",
            "Rui Hou",
            "Rui Wang",
            "Saghar Hosseini",
            "Sahana Chennabasappa",
            "Sanjay Singh",
            "Sean Bell",
            "Seohyun Sonia Kim",
            "Sergey Edunov",
            "Shaoliang Nie",
            "Sharan Narang",
            "S. Raparthy",
            "Sheng Shen",
            "Shengye Wan",
            "Shruti Bhosale",
            "Shun Zhang",
            "Simon Vandenhende",
            "Soumya Batra",
            "Spencer Whitman",
            "Sten Sootla",
            "Stephane Collot",
            "Suchin Gururangan",
            "S. Borodinsky",
            "Tamar Herman",
            "Tara Fowler",
            "Tarek Sheasha",
            "Thomas Georgiou",
            "Thomas Scialom",
            "Tobias Speckbacher",
            "Todor Mihaylov",
            "Tong Xiao",
            "Ujjwal Karn",
            "Vedanuj Goswami",
            "Vibhor Gupta",
            "Vignesh Ramanathan",
            "Viktor Kerkez",
            "Vincent Gonguet",
            "Virginie Do",
            "Vish Vogeti",
            "Vladan Petrovic",
            "Weiwei Chu",
            "Wenhan Xiong",
            "Wenyin Fu",
            "Whit-ney Meers",
            "Xavier Martinet",
            "Xiaodong Wang",
            "Xiaoqing Ellen Tan",
            "Xinfeng Xie",
            "Xuchao Jia",
            "Xuewei Wang",
            "Yaelle Goldschlag",
            "Yashesh Gaur",
            "Yasmine Babaei",
            "Yiqian Wen",
            "Yiwen Song",
            "Yuchen Zhang",
            "Yue Li",
            "Yuning Mao",
            "Zacharie Delpierre Coudert",
            "Zhengxu Yan",
            "Zhengxing Chen",
            "Zoe Papakipos",
            "Aaditya K. Singh",
            "Aaron Grattafiori",
            "Abha Jain",
            "Adam Kelsey",
            "Adam Shajnfeld",
            "Adi Gangidi",
            "Adolfo Victoria",
            "Ahuva Goldstand",
            "Ajay Menon",
            "Ajay Sharma",
            "Alex Boesenberg",
            "Alex Vaughan",
            "Alexei Baevski",
            "Allie Feinstein",
            "A. Kallet",
            "Amit Sangani",
            "Anam Yunus",
            "Andrei Lupu",
            "Andres Alvarado",
            "Andrew Caples",
            "Andrew Gu",
            "Andrew Ho",
            "Andrew Poulton",
            "Andrew Ryan",
            "Ankit Ramchandani",
            "Annie Franco",
            "Aparajita Saraf",
            "Arkabandhu Chowdhury",
            "Ashley Gabriel",
            "Ashwin Bharambe",
            "Assaf Eisenman",
            "Azadeh Yazdan",
            "Beau James",
            "Ben Maurer",
            "Ben Leonhardi",
            "Po-Yao (Bernie) Huang",
            "Beth Loyd",
            "Beto De Paola",
            "Bhargavi Paranjape",
            "Bing Liu",
            "Bo Wu",
            "Boyu Ni",
            "Braden Hancock",
            "Bram Wasti",
            "Brandon Spence",
            "Brani Stojkovic",
            "Brian Gamido",
            "Britt Montalvo",
            "Carl Parker",
            "Carly Burton",
            "Catalina Mejia",
            "Changhan Wang",
            "Changkyu Kim",
            "Chao Zhou",
            "Chester Hu",
            "Ching-Hsiang Chu",
            "Chris Cai",
            "Chris Tindal",
            "Christoph Feichtenhofer",
            "Damon Civin",
            "Dana Beaty",
            "Daniel Kreymer",
            "Shang-Wen Li",
            "Danny Wyatt",
            "David Adkins",
            "David Xu",
            "Davide Testuggine",
            "Delia David",
            "Devi Parikh",
            "Diana Liskovich",
            "Didem Foss",
            "Dingkang Wang",
            "Duc Le",
            "Dustin Holland",
            "Edward Dowling",
            "Eissa Jamil",
            "Elaine Montgomery",
            "Eleonora Presani",
            "Emily Hahn",
            "Emily Wood",
            "Erik Brinkman",
            "Esteban Arcaute",
            "Evan Dunbar",
            "Evan Smothers",
            "Fei Sun",
            "Felix Kreuk",
            "Feng Tian",
            "Firat Ozgenel",
            "Francesco Caggioni",
            "Francisco Guzm'an",
            "Frank J. Kanayet",
            "Frank Seide",
            "Gabriela Medina Florez",
            "Gabriella Schwarz",
            "Gada Badeer",
            "Georgia Swee",
            "Gil Halpern",
            "G. Thattai",
            "Grant Herman",
            "G. Sizov",
            "Guangyi Zhang",
            "Guna Lakshminarayanan",
            "Hamid Shojanazeri",
            "Han Zou",
            "Hannah Wang",
            "Han Zha",
            "Haroun Habeeb",
            "Harrison Rudolph",
            "Helen Suk",
            "Henry Aspegren",
            "Hunter Goldman",
            "Igor Molybog",
            "Igor Tufanov",
            "Irina-Elena Veliche",
            "Itai Gat",
            "Jake Weissman",
            "James Geboski",
            "James Kohli",
            "Japhet Asher",
            "Jean-Baptiste Gaya",
            "Jeff Marcus",
            "Jeff Tang",
            "Jennifer Chan",
            "Jenny Zhen",
            "Jeremy Reizenstein",
            "Jeremy Teboul",
            "Jessica Zhong",
            "Jian Jin",
            "Jingyi Yang",
            "Joe Cummings",
            "Jon Carvill",
            "Jon Shepard",
            "Jonathan McPhie",
            "Jonathan Torres",
            "Josh Ginsburg",
            "Junjie Wang",
            "Kaixing(Kai) Wu",
            "U. KamHou",
            "Karan Saxena",
            "Karthik Prasad",
            "Kartikay Khandelwal",
            "Katayoun Zand",
            "Kathy Matosich",
            "K. Veeraraghavan",
            "Kelly Michelena",
            "Keqian Li",
            "Kun Huang",
            "Kunal Chawla",
            "Kushal Lakhotia",
            "Kyle Huang",
            "Lailin Chen",
            "Lakshya Garg",
            "A. Lavender",
            "Leandro Silva",
            "Lee Bell",
            "Lei Zhang",
            "Liangpeng Guo",
            "Licheng Yu",
            "Liron Moshkovich",
            "Luca Wehrstedt",
            "Madian Khabsa",
            "Manav Avalani",
            "Manish Bhatt",
            "M. Tsimpoukelli",
            "Martynas Mankus",
            "Matan Hasson",
            "M. Lennie",
            "Matthias Reso",
            "Maxim Groshev",
            "Maxim Naumov",
            "Maya Lathi",
            "Meghan Keneally",
            "M. Seltzer",
            "Michal Valko",
            "Michelle Restrepo",
            "Mihir Patel",
            "Mik Vyatskov",
            "Mikayel Samvelyan",
            "Mike Clark",
            "Mike Macey",
            "Mike Wang",
            "Miquel Jubert Hermoso",
            "Mo Metanat",
            "Mohammad Rastegari",
            "Munish Bansal",
            "Nandhini Santhanam",
            "Natascha Parks",
            "Natasha White",
            "Navyata Bawa",
            "Nayan Singhal",
            "Nick Egebo",
            "Nicolas Usunier",
            "Nikolay Pavlovich Laptev",
            "Ning Dong",
            "Ning Zhang",
            "Norman Cheng",
            "Oleg Chernoguz",
            "Olivia Hart",
            "Omkar Salpekar",
            "Ozlem Kalinli",
            "Parkin Kent",
            "Parth Parekh",
            "Paul Saab",
            "Pavan Balaji",
            "Pedro Rittner",
            "Philip Bontrager",
            "Pierre Roux",
            "Piotr Dollár",
            "Polina Zvyagina",
            "Prashant Ratanchandani",
            "Pritish Yuvraj",
            "Qian Liang",
            "Rachad Alao",
            "Rachel Rodriguez",
            "Rafi Ayub",
            "Raghotham Murthy",
            "Raghu Nayani",
            "Rahul Mitra",
            "Raymond Li",
            "Rebekkah Hogan",
            "Robin Battey",
            "Rocky Wang",
            "Rohan Maheswari",
            "Russ Howes",
            "Ruty Rinott",
            "Sai Jayesh Bondu",
            "Samyak Datta",
            "Sara Chugh",
            "Sara Hunt",
            "Sargun Dhillon",
            "Sasha Sidorov",
            "Satadru Pan",
            "Saurabh Verma",
            "Seiji Yamamoto",
            "Sharadh Ramaswamy",
            "Shaun Lindsay",
            "Sheng Feng",
            "Shenghao Lin",
            "S. Zha",
            "Shiva Shankar",
            "Shuqiang Zhang",
            "Sinong Wang",
            "Sneha Agarwal",
            "S. Sajuyigbe",
            "Soumith Chintala",
            "Stephanie Max",
            "Stephen Chen",
            "Steve Kehoe",
            "Steve Satterfield",
            "Sudarshan Govindaprasad",
            "Sumit Gupta",
            "Sung-Bae Cho",
            "Sunny Virk",
            "Suraj Subramanian",
            "Sy Choudhury",
            "Sydney Goldman",
            "Tal Remez",
            "Tamar Glaser",
            "Tamara Best",
            "Thilo Kohler",
            "Thomas Robinson",
            "Tianhe Li",
            "Tianjun Zhang",
            "Tim Matthews",
            "Timothy Chou",
            "Tzook Shaked",
            "Varun Vontimitta",
            "Victoria Ajayi",
            "Victoria Montanez",
            "Vijai Mohan",
            "Vinay Satish Kumar",
            "Vishal Mangla",
            "Vlad Ionescu",
            "V. Poenaru",
            "Vlad T. Mihailescu",
            "Vladimir Ivanov",
            "Wei Li",
            "Wenchen Wang"
        ],
        "citations": 1000,
        "references": 0,
        "year": 2024
    },
    {
        "title": "FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model",
        "abstract": "The fast simulation of dynamical systems is a key challenge in many scientific and engineering applications, such as weather forecasting, disease control, and drug discovery. With the recent success of deep learning, there is increasing interest in using neural networks to solve differential equations in a data-driven manner. However, existing methods are either limited to specific types of differential equations or require large amounts of data for training. This restricts their practicality in many real-world applications, where data is often scarce or expensive to obtain. To address this, we propose a novel multi-modal foundation model, named \\textbf{FMint} (\\textbf{F}oundation \\textbf{M}odel based on \\textbf{In}i\\textbf{t}ialization), to bridge the gap between human-designed and data-driven models for the fast simulation of dynamical systems. Built on a decoder-only transformer architecture with in-context learning, FMint utilizes both numerical and textual data to learn a universal error correction scheme for dynamical systems, using prompted sequences of coarse solutions from traditional solvers. The model is pre-trained on a corpus of 40K ODEs, and we perform extensive experiments on challenging ODEs that exhibit chaotic behavior and of high dimensionality. Our results demonstrate the effectiveness of the proposed model in terms of both accuracy and efficiency compared to classical numerical solvers, highlighting FMint's potential as a general-purpose solver for dynamical systems. Our approach achieves an accuracy improvement of 1 to 2 orders of magnitude over state-of-the-art dynamical system simulators, and delivers a 5X speedup compared to traditional numerical algorithms. The code for FMint is available at \\url{https://github.com/margotyjx/FMint}.",
        "authors": [
            "Zezheng Song",
            "Jiaxin Yuan",
            "Haizhao Yang"
        ],
        "citations": 15,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Apple Intelligence Foundation Language Models",
        "abstract": "We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",
        "authors": [
            "Tom Gunter",
            "Zirui Wang",
            "Chong Wang",
            "Ruoming Pang",
            "Andy Narayanan",
            "Aonan Zhang",
            "Bowen Zhang",
            "Chen Chen",
            "Chung-Cheng Chiu",
            "David Qiu",
            "Deepak Gopinath",
            "Dian Ang Yap",
            "Dong Yin",
            "Feng Nan",
            "Floris Weers",
            "Guoli Yin",
            "Haoshuo Huang",
            "Jianyu Wang",
            "Jiarui Lu",
            "John Peebles",
            "Kewei Ye",
            "Mark Lee",
            "Nan Du",
            "Qibin Chen",
            "Quentin Keunebroek",
            "Sam Wiseman",
            "Syd Evans",
            "Tao Lei",
            "Vivek Rathod",
            "Xiang Kong",
            "Xianzhi Du",
            "Yanghao Li",
            "Yongqiang Wang",
            "Yuan Gao",
            "Zaid Ahmed",
            "Zhaoyang Xu",
            "Zhiyun Lu",
            "Al Rashid",
            "Albin Madappally Jose",
            "Alec Doane",
            "Alfredo Bencomo",
            "Allison Vanderby",
            "Andrew Hansen",
            "Ankur Jain",
            "A. Anupama",
            "Areeba Kamal",
            "Bugu Wu",
            "Carolina Brum",
            "Charlie Maalouf",
            "Chinguun Erdenebileg",
            "Chris Dulhanty",
            "Dominik Moritz",
            "Doug Kang",
            "Eduardo Jimenez",
            "Evan Ladd",
            "Fang Shi",
            "Felix Bai",
            "Frank Chu",
            "Fred Hohman",
            "Hadas Kotek",
            "Hannah Gillis Coleman",
            "Jane Li",
            "Jeffrey P. Bigham",
            "Jeffery Cao",
            "Jeff Lai",
            "Jessica Cheung",
            "Jiulong Shan",
            "Joe Zhou",
            "John Li",
            "Jun Qin",
            "Karanjeet Singh",
            "Karla Vega",
            "Kelvin Zou",
            "Laura Heckman",
            "Lauren Gardiner",
            "Margit Bowler",
            "Maria Cordell",
            "Meng Cao",
            "Nicole Hay",
            "Nilesh Shahdadpuri",
            "Otto Godwin",
            "Pranay Dighe",
            "Pushyami Rachapudi",
            "Ramsey Tantawi",
            "Roman Frigg",
            "Sam Davarnia",
            "Sanskruti Shah",
            "Saptarshi Guha",
            "Sasha Sirovica",
            "Shen Ma",
            "Shuang Ma",
            "Simon Wang",
            "Sulgi Kim",
            "Suma Jayaram",
            "Vaishaal Shankar",
            "Varsha Paidi",
            "Vivek Kumar",
            "Xin Wang",
            "Xin Zheng",
            "Walker Cheng",
            "Y. Shrager",
            "Yang Ye",
            "Yasu Tanaka",
            "Yihao Guo",
            "Yunsong Meng",
            "Zhaoping Luo",
            "Ouyang Zhi",
            "Alp Aygar",
            "Alvin Wan",
            "Andrew D. Walkingshaw",
            "Tzu-Hsiang Lin",
            "Arsalan Farooq",
            "Brent Ramerth",
            "Colorado Reed",
            "Chris Bartels",
            "Chris Chaney",
            "David Riazati",
            "Eric Liang Yang",
            "Erin Feldman",
            "Gabriel Hochstrasser",
            "Guillaume Seguin",
            "Irina Belousova",
            "J. Pelemans",
            "Karen Yang",
            "Keivan Alizadeh Vahid",
            "Liangliang Cao",
            "Mahyar Najibi",
            "Marco Zuliani",
            "Max Horton",
            "Minsik Cho",
            "Nikhil Bhendawade",
            "Patrick Dong",
            "Piotr Maj",
            "Pulkit Agrawal",
            "Qi Shan",
            "Qichen Fu",
            "R. Poston",
            "Sam Xu",
            "Shuangning Liu",
            "Sushma Rao",
            "Tashweena Heeramun",
            "Thomas Merth",
            "Uday Rayala",
            "Victor Cui",
            "Vivek Rangarajan Sridhar",
            "Wencong Zhang",
            "Wenqi Zhang",
            "Wentao Wu",
            "Xingyu Zhou",
            "Xinwen Liu",
            "Yang Zhao",
            "Yin Xia",
            "Zhile Ren",
            "Zhongzheng Ren"
        ],
        "citations": 17,
        "references": 60,
        "year": 2024
    },
    {
        "title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents",
        "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train \\&test data, and part of fine-tuned open LMMs are available at \\url{https://github.com/THUDM/VisualAgentBench}.",
        "authors": [
            "Xiao Liu",
            "Tianjie Zhang",
            "Yu Gu",
            "Iat Long Iong",
            "Yifan Xu",
            "Xixuan Song",
            "Shudan Zhang",
            "Hanyu Lai",
            "Xinyi Liu",
            "Hanlin Zhao",
            "Jiadai Sun",
            "Xinyue Yang",
            "Yu Yang",
            "Zehan Qi",
            "Shuntian Yao",
            "Xueqiao Sun",
            "Siyi Cheng",
            "Qi Zheng",
            "Hao Yu",
            "Hanchen Zhang",
            "Wenyi Hong",
            "Ming Ding",
            "Lihang Pan",
            "Xiaotao Gu",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chan Hee Song",
            "Yu Su",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "citations": 14,
        "references": 130,
        "year": 2024
    },
    {
        "title": "The Essential Role of Causality in Foundation World Models for Embodied AI",
        "abstract": "Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents will require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions and are therefore insufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitating meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.",
        "authors": [
            "Tarun Gupta",
            "Wenbo Gong",
            "Chao Ma",
            "Nick Pawlowski",
            "Agrin Hilmkil",
            "Meyer Scetbon",
            "Ade Famoti",
            "A. Llorens",
            "Jianfeng Gao",
            "Stefan Bauer",
            "Danica Kragic",
            "Bernhard Schölkopf",
            "Cheng Zhang"
        ],
        "citations": 10,
        "references": 202,
        "year": 2024
    },
    {
        "title": "scGPT: toward building a foundation model for single-cell multi-omics using generative AI.",
        "abstract": null,
        "authors": [
            "Haotian Cui",
            "Chloe X. Wang",
            "Hassaan Maan",
            "Kuan Pang",
            "Fengning Luo",
            "Nan Duan",
            "Bo Wang"
        ],
        "citations": 196,
        "references": 51,
        "year": 2024
    },
    {
        "title": "A visual-language foundation model for computational pathology.",
        "abstract": null,
        "authors": [
            "Ming Y. Lu",
            "Bowen Chen",
            "Drew F. K. Williamson",
            "Richard J. Chen",
            "Ivy Liang",
            "Tong Ding",
            "Guillaume Jaume",
            "Igor Odintsov",
            "L. Le",
            "Georg K. Gerber",
            "Anil V. Parwani",
            "Andrew Zhang",
            "Faisal Mahmood"
        ],
        "citations": 115,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Sapiens: Foundation for Human Vision Models",
        "abstract": "We present Sapiens, a family of models for four fundamental human-centric vision tasks -- 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability -- model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error. Project page: https://about.meta.com/realitylabs/codecavatars/sapiens.",
        "authors": [
            "Rawal Khirodkar",
            "Timur Bagautdinov",
            "Julieta Martinez",
            "Zhaoen Su",
            "Austin James",
            "Peter Selednik",
            "Stuart Anderson",
            "Shunsuke Saito"
        ],
        "citations": 8,
        "references": 121,
        "year": 2024
    },
    {
        "title": "New Control Paradigm for Industry 5.0: From Big Models to Foundation Control and Management",
        "abstract": null,
        "authors": [
            "Fei-Yue Wang"
        ],
        "citations": 36,
        "references": 8,
        "year": 2023
    },
    {
        "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites",
        "abstract": "In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.",
        "authors": [
            "Zhe Chen",
            "Weiyun Wang",
            "Hao Tian",
            "Shenglong Ye",
            "Zhangwei Gao",
            "Erfei Cui",
            "Wenwen Tong",
            "Kongzhi Hu",
            "Jiapeng Luo",
            "Zheng Ma",
            "Ji Ma",
            "Jiaqi Wang",
            "Xiao-wen Dong",
            "Hang Yan",
            "Hewei Guo",
            "Conghui He",
            "Zhenjiang Jin",
            "Chaochao Xu",
            "Bin Wang",
            "Xingjian Wei",
            "Wei Li",
            "Wenjian Zhang",
            "Bo Zhang",
            "Lewei Lu",
            "Xizhou Zhu",
            "Tong Lu",
            "Dahua Lin",
            "Yu Qiao"
        ],
        "citations": 272,
        "references": 140,
        "year": 2024
    },
    {
        "title": "Foundation and large language models: fundamentals, challenges, opportunities, and social impacts",
        "abstract": null,
        "authors": [
            "Devon Myers",
            "Rami Mohawesh",
            "Venkata Ishwarya Chellaboina",
            "Anantha Lakshmi Sathvik",
            "Praveen Venkatesh",
            "Yi-Hui Ho",
            "Hanna Henshaw",
            "Muna Alhawawreh",
            "David Berdik",
            "Yaser Jararweh"
        ],
        "citations": 34,
        "references": 84,
        "year": 2023
    },
    {
        "title": "A whole-slide foundation model for digital pathology from real-world data",
        "abstract": null,
        "authors": [
            "Hanwen Xu",
            "Naoto Usuyama",
            "Jaspreet Bagga",
            "Sheng Zhang",
            "Rajesh Rao",
            "Tristan Naumann",
            "Cliff Wong",
            "Zelalem Gero",
            "Javier González",
            "Yu Gu",
            "Yanbo Xu",
            "Mu-Hsin Wei",
            "Wenhui Wang",
            "Shuming Ma",
            "Furu Wei",
            "Jianwei Yang",
            "Chun-yue Li",
            "Jianfeng Gao",
            "Jaylen Rosemon",
            "Tucker Bower",
            "Soohee Lee",
            "R. Weerasinghe",
            "Bill J Wright",
            "Ari Robicsek",
            "B. Piening",
            "Carlo Bifulco",
            "Sheng Wang",
            "Hoifung Poon"
        ],
        "citations": 100,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
        "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
        "authors": [
            "Neha Sengupta",
            "Sunil Kumar Sahu",
            "Bokang Jia",
            "Satheesh Katipomu",
            "Haonan Li",
            "Fajri Koto",
            "Osama Mohammed Afzal",
            "Samta Kamboj",
            "O. Pandit",
            "Rahul Pal",
            "Lalit Pradhan",
            "Zain Muhammad Mujahid",
            "Massa Baali",
            "Xudong Han",
            "Alham Fikri Aji",
            "Zhengzhong Liu",
            "Andy Hock",
            "Andrew Feldman",
            "Jonathan Lee",
            "A. Jackson",
            "Preslav Nakov",
            "Timothy Baldwin",
            "Eric P. Xing"
        ],
        "citations": 30,
        "references": 119,
        "year": 2023
    },
    {
        "title": "Making Large Language Models A Better Foundation For Dense Retrieval",
        "abstract": "Dense retrieval needs to learn discriminative text embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs' strong capability on semantic understanding. However, the LLMs are pre-trained by text generation tasks, whose working pattern is completely different from representing texts as embeddings. As a result, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. In this paper, we propose a novel approach, called LLaRA (LLM adapted for dense RetrievAl), which works as a post-hoc adaptation of LLM for the dense retrieval application. LLaRA consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the text embeddings from LLM are used to reconstruct the tokens for the input sentence and predict the tokens for the next sentence, respectively. LLaRA turns out to be simple, lightweight, and highly effective. It is applied to adapt LLaMA-2-7B (base) on the Wikipedia corpus, where it substantially improves the model's fine-tuned performances on a variety of dense retrieval benchmarks, like MSMARCO and BEIR. Our model and code will be made publicly available at BGE repository.",
        "authors": [
            "Chaofan Li",
            "Zheng Liu",
            "Shitao Xiao",
            "Yingxia Shao"
        ],
        "citations": 24,
        "references": 36,
        "year": 2023
    },
    {
        "title": "PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models – Federated Learning in Age of Foundation Model",
        "abstract": "Quick global aggregation of effective distributed parameters is crucial to federated learning (FL), which requires adequate bandwidth for parameters communication and sufficient user data for local training. Otherwise, FL may cost excessive training time for convergence and produce inaccurate models. In this paper, we propose a brand-new FL framework, PromptFL, that replaces the federated model training with the federated prompt training, i.e., let federated participants train prompts instead of a shared model, to simultaneously achieve the efficient global aggregation and local training on insufficient data by exploiting the power of foundation models (FM) in a distributed way. PromptFL ships an off-the-shelf FM, i.e., CLIP, to distributed clients who would cooperatively train shared soft prompts based on very few local data. Since PromptFL only needs to update the prompts instead of the whole model, both the local training and the global aggregation can be significantly accelerated. And FM trained over large scale data can provide strong adaptation capability to distributed users tasks with the trained soft prompts. We empirically analyze the PromptFL via extensive experiments, and show its superiority in terms of system feasibility, user privacy, and performance.",
        "authors": [
            "Tao Guo",
            "Song Guo",
            "Junxiao Wang",
            "Xueyang Tang",
            "Wenchao Xu"
        ],
        "citations": 88,
        "references": 65,
        "year": 2022
    },
    {
        "title": "Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts",
        "abstract": null,
        "authors": [
            "Adam Kolides",
            "Alyna Nawaz",
            "Anshu Rathor",
            "Denzel Beeman",
            "Muzammil Hashmi",
            "Sana Fatima",
            "David Berdik",
            "M. Al-Ayyoub",
            "Yaser Jararweh"
        ],
        "citations": 31,
        "references": 73,
        "year": 2023
    },
    {
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
        "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.",
        "authors": [
            "Hanze Dong",
            "Wei Xiong",
            "Deepanshu Goyal",
            "Rui Pan",
            "Shizhe Diao",
            "Jipeng Zhang",
            "Kashun Shum",
            "T. Zhang"
        ],
        "citations": 323,
        "references": 81,
        "year": 2023
    },
    {
        "title": "A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation",
        "abstract": "Over 1.4 billion chest X-rays (CXRs) are performed annually due to their cost-effectiveness as an initial diagnostic test. This scale of radiological studies provides a significant opportunity to streamline CXR interpretation and documentation. While foundation models are a promising solution, the lack of publicly available large-scale datasets and benchmarks inhibits their iterative development and real-world evaluation. To overcome these challenges, we constructed a large-scale dataset (CheXinstruct), which we utilized to train a vision-language foundation model (CheXagent). We systematically demonstrated competitive performance across eight distinct task types on our novel evaluation benchmark (CheXbench). Beyond technical validation, we assessed the real-world utility of CheXagent in directly drafting radiology reports. Our clinical assessment with eight radiologists revealed a 36% time saving for residents using CheXagent-drafted reports, while attending radiologists showed no significant time difference editing resident-drafted or CheXagent-drafted reports. The CheXagent-drafted reports improved the writing efficiency of both radiology residents and attending radiologists in 81% and 61% of cases, respectively, without loss of quality. Overall, we demonstrate that CheXagent can effectively perform a variety of CXR interpretation tasks and holds potential to assist radiologists in routine clinical workflows.",
        "authors": [
            "Zhihong Chen",
            "Maya Varma",
            "Jean-Benoit Delbrouck",
            "Magdalini Paschali",
            "Louis Blankemeier",
            "Dave Van Veen",
            "Jeya Maria Jose Valanarasu",
            "Alaa Youssef",
            "Joseph Paul Cohen",
            "E. Reis",
            "Emily B. Tsai",
            "Andrew Johnston",
            "Cameron Olsen",
            "Tanishq Mathew Abraham",
            "S. Gatidis",
            "Akshay S. Chaudhari",
            "Curtis P. Langlotz"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.",
        "authors": [
            "Yanis Labrak",
            "Adrien Bazoge",
            "Emmanuel Morin",
            "P. Gourraud",
            "Mickael Rouvier",
            "Richard Dufour"
        ],
        "citations": 131,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Predicting resilient modulus of flexible pavement foundation using extreme gradient boosting based optimised models",
        "abstract": "ABSTRACT Resilient modulus ($M_R$MR) plays the most critical role in the evaluation and design of flexible pavement foundations. $M_R$MR is utilised as the principal parameter for representing stiffness and behaviour of flexible pavement foundation in experimental and semi-empirical approaches. To determine $M_R$MR, cyclic triaxial compressive experiments under different confining pressures and deviatoric stresses are needed. However, such experiments are costly and time-consuming. In the present study, an extreme gradient boosting-based ($XGB$XGB) model is presented for predicting the resilient modulus of flexible pavement foundations. The model is optimised using four different optimisation methods (particle swarm optimisation ($PSO$PSO), social spider optimisation ($SSO$SSO), sine cosine algorithm ($SCA$SCA), and multi-verse optimisation ($MVO$MVO)) and a database collected from previously published technical literature. The outcomes present that all developed designs have good workability in estimating the $M_R$MR of flexible pavement foundation, but the $PSO-XGB$PSO−XGB models have the best prediction accuracy considering both training and testing datasets.",
        "authors": [
            "Reza Sarkhani Benemaran",
            "M. Esmaeili‐Falak",
            "A. Javadi"
        ],
        "citations": 86,
        "references": 74,
        "year": 2022
    },
    {
        "title": "SpectralGPT: Spectral Remote Sensing Foundation Model",
        "abstract": "The foundation model has recently garnered significant attention due to its potential to revolutionize the field of visual representation learning in a self-supervised manner. While most foundation models are tailored to effectively process RGB images for various visual tasks, there is a noticeable gap in research focused on spectral data, which offers valuable information for scene understanding, especially in remote sensing (RS) applications. To fill this gap, we created for the first time a universal RS foundation model, named SpectralGPT, which is purpose-built to handle spectral RS images using a novel 3D generative pretrained transformer (GPT). Compared to existing foundation models, SpectralGPT 1) accommodates input images with varying sizes, resolutions, time series, and regions in a progressive training fashion, enabling full utilization of extensive RS Big Data; 2) leverages 3D token generation for spatial-spectral coupling; 3) captures spectrally sequential patterns via multi-target reconstruction; and 4) trains on one million spectral RS images, yielding models with over 600 million parameters. Our evaluation highlights significant performance improvements with pretrained SpectralGPT models, signifying substantial potential in advancing spectral RS Big Data applications within the field of geoscience across four downstream tasks: single/multi-label scene classification, semantic segmentation, and change detection.",
        "authors": [
            "D. Hong",
            "Bing Zhang",
            "Xuyang Li",
            "Yuxuan Li",
            "Chenyu Li",
            "Jing Yao",
            "N. Yokoya",
            "Hao Li",
            "Pedram Ghamisi",
            "Xiuping Jia",
            "Antonio J. Plaza",
            "Paolo Gamba",
            "J. Benediktsson",
            "J. Chanussot"
        ],
        "citations": 275,
        "references": 53,
        "year": 2023
    },
    {
        "title": "A foundation model for clinical-grade computational pathology and rare cancers detection",
        "abstract": null,
        "authors": [
            "Eugene Vorontsov",
            "A. Bozkurt",
            "Adam Casson",
            "George Shaikovski",
            "Michal Zelechowski",
            "Kristen Severson",
            "Eric Zimmermann",
            "James Hall",
            "Neil Tenenholtz",
            "Nicolò Fusi",
            "Ellen Yang",
            "Philippe Mathieu",
            "A. van Eck",
            "Donghun Lee",
            "Julian Viret",
            "Eric Robert",
            "Yi Kan Wang",
            "J. Kunz",
            "Matthew C H Lee",
            "Jan H Bernhard",
            "R. Godrich",
            "Gerard Oakley",
            "Ewan Millar",
            "Matthew G Hanna",
            "Hannah Y Wen",
            "Juan A Retamero",
            "William A. Moye",
            "Razik Yousfi",
            "C. Kanan",
            "D.S. Klimstra",
            "B. Rothrock",
            "Siqi Liu",
            "Thomas J Fuchs"
        ],
        "citations": 51,
        "references": 59,
        "year": 2024
    },
    {
        "title": "Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-Shot Metric Depth and Surface Normal Estimation",
        "abstract": "We introduce Metric3D v2, a geometric foundation model designed for zero-shot metric depth and surface normal estimation from single images, critical for accurate 3D recovery. Depth and normal estimation, though complementary, present distinct challenges. State-of-the-art monocular depth methods achieve zero-shot generalization through affine-invariant depths, but fail to recover real-world metric scale. Conversely, current normal estimation techniques struggle with zero-shot performance due to insufficient labeled data. We propose targeted solutions for both metric depth and normal estimation. For metric depth, we present a canonical camera space transformation module that resolves metric ambiguity across various camera models and large-scale datasets, which can be easily integrated into existing monocular models. For surface normal estimation, we introduce a joint depth-normal optimization module that leverages diverse data from metric depth, allowing normal estimators to improve beyond traditional labels. Our model, trained on over 16 million images from thousands of camera models with varied annotations, excels in zero-shot generalization to new camera settings. As shown in Fig. 1, It ranks the 1st in multiple zero-shot and standard benchmarks for metric depth and surface normal prediction. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. Our model also relieves the scale drift issues of monocular-SLAM (Fig. 3), leading to high-quality metric scale dense mapping. Such applications highlight the versatility of Metric3D v2 models as geometric foundation models.",
        "authors": [
            "Mu Hu",
            "Wei Yin",
            "C. Zhang",
            "Zhipeng Cai",
            "Xiaoxiao Long",
            "Hao Chen",
            "Kaixuan Wang",
            "Gang Yu",
            "Chunhua Shen",
            "Shaojie Shen"
        ],
        "citations": 41,
        "references": 137,
        "year": 2024
    },
    {
        "title": "Revolutionizing Healthcare with Foundation AI Models",
        "abstract": "ChatGPT is a foundation Artificial Intelligence (AI) model that has opened up new opportunities in digital healthcare. Particularly, it can serve as a co-pilot tool for doctors in the interpretation, summarization, and completion of reports. Furthermore, it can build upon the ability to access the large literature and knowledge on the internet. So, chatGPT could generate acceptable responses for the medical examination. Hence. It offers the possibility of enhancing healthcare accessibility, expandability, and effectiveness. Nonetheless, chatGPT is vulnerable to inaccuracies, false information, and bias. This paper briefly describes the potential of Foundation AI models to transform future healthcare by presenting ChatGPT as an example tool.",
        "authors": [
            "Hazrat Ali",
            "Junaid Qadir",
            "T. Alam",
            "Mowafa J Househ",
            "Zubair Shah"
        ],
        "citations": 6,
        "references": 7,
        "year": 2023
    },
    {
        "title": "Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models",
        "abstract": "Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model's performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle (e.g., the human users), thus a good evaluation protocol is probably to evaluate the models' behaviors in comparison to the oracle. In this paper, we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model). Besides, we design a simple method that can accomplish the evaluation beyond the scope of the benchmarks. Our method extends the image datasets with new samples that are sufficiently perturbed to be distinct from the ones in the original sets, but are still bounded within the same image-label structure the original test image represents, constrained by a foundation model pretrained with a large amount of samples. As a result, our new method will offer us a new way to evaluate the models' robustness performance, free of limitations of fixed benchmarks or constrained perturbations, although scoped by the power of the oracle. In addition to the evaluation results, we also leverage our generated data to understand the behaviors of the model and our new evaluation strategies.",
        "authors": [
            "Peiyan Zhang",
            "Hao Liu",
            "Chaozhuo Li",
            "Xing Xie",
            "Sunghun Kim",
            "Haohan Wang"
        ],
        "citations": 7,
        "references": 113,
        "year": 2023
    },
    {
        "title": "RudolfV: A Foundation Model by Pathologists for Pathologists",
        "abstract": "Artificial intelligence has started to transform histopathology impacting clinical diagnostics and biomedical research. However, while many computational pathology approaches have been proposed, most current AI models are limited with respect to generalization, application variety, and handling rare diseases. Recent efforts introduced self-supervised foundation models to address these challenges, yet existing approaches do not leverage pathologist knowledge by design. In this study, we present a novel approach to designing foundation models for computational pathology, incorporating pathologist expertise, semi-automated data curation, and a diverse dataset from over 15 laboratories, including 58 tissue types, and encompassing 129 different histochemical and immunohistochemical staining modalities. We demonstrate that our model\"RudolfV\"surpasses existing state-of-the-art foundation models across different benchmarks focused on tumor microenvironment profiling, biomarker evaluation, and reference case search while exhibiting favorable robustness properties. Our study shows how domain-specific knowledge can increase the efficiency and performance of pathology foundation models and enable novel application areas.",
        "authors": [
            "Jonas Dippel",
            "Barbara Feulner",
            "Tobias Winterhoff",
            "S. Schallenberg",
            "Gabriel Dernbach",
            "Andreas Kunft",
            "Stephan Tietz",
            "Philipp Jurmeister",
            "David Horst",
            "Lukas Ruff",
            "Klaus-Robert Müller",
            "Frederick Klauschen",
            "Maximilian Alber"
        ],
        "citations": 18,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
        "authors": [
            "Yunfan Gao",
            "Yun Xiong",
            "Xinyu Gao",
            "Kangxiang Jia",
            "Jinliu Pan",
            "Yuxi Bi",
            "Yi Dai",
            "Jiawei Sun",
            "Qianyu Guo",
            "Meng Wang",
            "Haofen Wang"
        ],
        "citations": 915,
        "references": 229,
        "year": 2023
    },
    {
        "title": "A foundation model for generalizable disease detection from retinal images",
        "abstract": null,
        "authors": [
            "Yukun Zhou",
            "Mark A. Chia",
            "Siegfried K. Wagner",
            "Murat S. Ayhan",
            "Dominic J. Williamson",
            "R. Struyven",
            "Timing Liu",
            "Moucheng Xu",
            "Mateo G. Lozano",
            "Peter Woodward-Court",
            "Y. Kihara",
            "Naomi John E. J. Thomas Tariq Paul Graeme Panagiotis Den Allen Gallacher Littlejohns Aslam Bishop Black Ser",
            "N. Allen",
            "John Gallacher",
            "T. Littlejohns",
            "Tariq Aslam",
            "P. Bishop",
            "Graeme Black",
            "P. Sergouniotis",
            "Denize Atan",
            "Andrew D. Dick",
            "Cathy Williams",
            "S. Barman",
            "J. Barrett",
            "S. Mackie",
            "Tasanee Braithwaite",
            "R. Carare",
            "Sarah Ennis",
            "Jane Gibson",
            "A. Lotery",
            "Jay Self",
            "U. Chakravarthy",
            "Ruth E. Hogg",
            "E. Paterson",
            "J. Woodside",
            "T. Peto",
            "G. McKay",
            "Bernadette Mcguinness",
            "P. Foster",
            "Konstantinos Balaskas",
            "A. Khawaja",
            "N. Pontikos",
            "J. Rahi",
            "G. Lascaratos",
            "Praveen J. Patel",
            "Michelle Chan",
            "S. Chua",
            "A. Day",
            "Parul Desai",
            "Catherine A Egan",
            "Marcus Fruttiger",
            "D. Garway-Heath",
            "A. Hardcastle",
            "S. P. T. Khaw",
            "T. Moore",
            "S. Sivaprasad",
            "N. Strouthidis",
            "D. Thomas",
            "A. Tufail",
            "Ananth C. Viswanathan",
            "B. Dhillon",
            "T. Macgillivray",
            "C. Sudlow",
            "V. Vitart",
            "Alexander Doney",
            "E. Trucco",
            "Jeremy A. Guggeinheim",
            "James E. Morgan",
            "C. Hammond",
            "Katie M. Williams",
            "P. Hysi",
            "Simon Harding",
            "Yalin Zheng",
            "R. Luben",
            "P. Luthert",
            "Zihan Sun",
            "Martin McKibbin",
            "Eoin O'sullivan",
            "R. Oram",
            "Mike Weedon",
            "Chris G. Owen",
            "A. Rudnicka",
            "N. Sattar",
            "David Steel",
            "Irene Stratton",
            "Robyn Tapp",
            "M. Yates",
            "A. Petzold",
            "S. Madhusudhan",
            "A. Altmann",
            "Aaron Y. Lee",
            "E. Topol",
            "A. Denniston",
            "Daniel C Alexander",
            "P. Keane"
        ],
        "citations": 228,
        "references": 67,
        "year": 2023
    },
    {
        "title": "ClimaX: A foundation model for weather and climate",
        "abstract": "Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute while maintaining general utility. ClimaX is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6. The pre-trained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. Compared to existing data-driven baselines, we show that this generality in ClimaX results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets. The source code is available at https://github.com/microsoft/ClimaX.",
        "authors": [
            "Tung Nguyen",
            "Johannes Brandstetter",
            "Ashish Kapoor",
            "Jayesh K. Gupta",
            "Aditya Grover"
        ],
        "citations": 204,
        "references": 126,
        "year": 2023
    },
    {
        "title": "Holistic Evaluation of Language Models",
        "abstract": "Language models (LMs) like GPT‐3, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade‐offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top‐level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/.",
        "authors": [
            "Percy Liang",
            "Rishi Bommasani",
            "Tony Lee",
            "Dimitris Tsipras",
            "Dilara Soylu",
            "Michihiro Yasunaga",
            "Yian Zhang",
            "D. Narayanan",
            "Yuhuai Wu",
            "Ananya Kumar",
            "Benjamin Newman",
            "Binhang Yuan",
            "Bobby Yan",
            "Ce Zhang",
            "Christian Cosgrove",
            "Christopher D. Manning",
            "Christopher R'e",
            "Diana Acosta-Navas",
            "Drew A. Hudson",
            "E. Zelikman",
            "Esin Durmus",
            "Faisal Ladhak",
            "Frieda Rong",
            "Hongyu Ren",
            "Huaxiu Yao",
            "Jue Wang",
            "Keshav Santhanam",
            "Laurel J. Orr",
            "Lucia Zheng",
            "Mert Yuksekgonul",
            "Mirac Suzgun",
            "Nathan S. Kim",
            "Neel Guha",
            "Niladri S. Chatterji",
            "O. Khattab",
            "Peter Henderson",
            "Qian Huang",
            "Ryan Chi",
            "Sang Michael Xie",
            "Shibani Santurkar",
            "S. Ganguli",
            "Tatsunori Hashimoto",
            "Thomas F. Icard",
            "Tianyi Zhang",
            "Vishrav Chaudhary",
            "William Wang",
            "Xuechen Li",
            "Yifan Mai",
            "Yuhui Zhang",
            "Yuta Koreeda"
        ],
        "citations": 779,
        "references": 69,
        "year": 2023
    },
    {
        "title": "State of art soft computing based simulation models for bearing capacity of pile foundation: a comparative study of hybrid ANNs and conventional models",
        "abstract": null,
        "authors": [
            "Manish Kumar",
            "Vinay T. Kumar",
            "B. Rajagopal",
            "P. Samui",
            "A. Burman"
        ],
        "citations": 23,
        "references": 98,
        "year": 2022
    },
    {
        "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
        "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.",
        "authors": [
            "Qinghao Ye",
            "Haiyang Xu",
            "Guohai Xu",
            "Jiabo Ye",
            "Ming Yan",
            "Yi Zhou",
            "Junyan Wang",
            "Anwen Hu",
            "Pengcheng Shi",
            "Yaya Shi",
            "Chenliang Li",
            "Yuanhong Xu",
            "Hehong Chen",
            "Junfeng Tian",
            "Qiang Qi",
            "Ji Zhang",
            "Feiyan Huang"
        ],
        "citations": 755,
        "references": 36,
        "year": 2023
    },
    {
        "title": "RingMo: A Remote Sensing Foundation Model With Masked Image Modeling",
        "abstract": "Deep learning approaches have contributed to the rapid development of remote sensing (RS) image interpretation. The most widely used training paradigm is to use ImageNet pretrained models to process RS data for specified tasks. However, there are issues such as domain gap between natural and RS scenes and the poor generalization capacity of RS models. It makes sense to develop a foundation model with general RS feature representation. Since a large amount of unlabeled data is available, the self-supervised method has more development significance than the fully supervised method in RS. However, most of the current self-supervised methods use contrastive learning, whose performance is sensitive to data augmentation, additional information, and selection of positive and negative pairs. In this article, we leverage the benefits of generative self-supervised learning (SSL) for RS images and propose an RS foundation model framework called RingMo, which consists of two parts. First, a large-scale dataset is constructed by collecting two million RS images from satellite and aerial platforms, covering multiple scenes and objects around the world. Second, we propose an RS foundation model training method designed for dense and small objects in complicated RS scenes. We show that the foundation model trained on our dataset with RingMo method achieves state-of-the-art (SOTA) on eight datasets across four downstream tasks, demonstrating the effectiveness of the proposed framework. Through in-depth exploration, we believe it is time for RS researchers to embrace generative SSL and leverage its general representation capabilities to speed up the development of RS applications.",
        "authors": [
            "Xian Sun",
            "Peijin Wang",
            "Wanxuan Lu",
            "Zicong Zhu",
            "Xiaonan Lu",
            "Qi He",
            "Junxi Li",
            "Xuee Rong",
            "Zhujun Yang",
            "Hao Chang",
            "Qinglin He",
            "Guang Yang",
            "Ruiping Wang",
            "Jiwen Lu",
            "Kun Fu"
        ],
        "citations": 156,
        "references": 154,
        "year": 2023
    },
    {
        "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding",
        "abstract": "Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present Foundation Model Embedded Gaussian Splatting (FMGS), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of the same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that we are 851X faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code on the project page.",
        "authors": [
            "Xingxing Zuo",
            "Pouya Samangouei",
            "Yunwen Zhou",
            "Yan Di",
            "Mingyang Li"
        ],
        "citations": 25,
        "references": 63,
        "year": 2024
    },
    {
        "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
        "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
        "authors": [
            "Yufei Wang",
            "Zhanyi Sun",
            "Jesse Zhang",
            "Zhou Xian",
            "Erdem Biyik",
            "David Held",
            "Zackory Erickson"
        ],
        "citations": 25,
        "references": 54,
        "year": 2024
    },
    {
        "title": "ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning",
        "abstract": "Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models. Both ChartX and ChartVLM are available at: https://github.com/Alpha-Innovator/ChartVLM",
        "authors": [
            "Renqiu Xia",
            "Bo Zhang",
            "Hancheng Ye",
            "Xiangchao Yan",
            "Qi Liu",
            "Hongbin Zhou",
            "Zijun Chen",
            "Min Dou",
            "Botian Shi",
            "Junchi Yan",
            "Yu Qiao"
        ],
        "citations": 30,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
        "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.",
        "authors": [
            "Zhiliang Peng",
            "Wenhui Wang",
            "Li Dong",
            "Y. Hao",
            "Shaohan Huang",
            "Shuming Ma",
            "Furu Wei"
        ],
        "citations": 561,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
        "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",
        "authors": [
            "Ming Jin",
            "Shiyu Wang",
            "Lintao Ma",
            "Zhixuan Chu",
            "James Y. Zhang",
            "X. Shi",
            "Pin-Yu Chen",
            "Yuxuan Liang",
            "Yuan-Fang Li",
            "Shirui Pan",
            "Qingsong Wen"
        ],
        "citations": 224,
        "references": 63,
        "year": 2023
    },
    {
        "title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation",
        "abstract": "VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.",
        "authors": [
            "Yecheng Wu",
            "Zhuoyang Zhang",
            "Junyu Chen",
            "Haotian Tang",
            "Dacheng Li",
            "Yunhao Fang",
            "Ligeng Zhu",
            "Enze Xie",
            "Hongxu Yin",
            "Li Yi",
            "Song Han",
            "Yao Lu"
        ],
        "citations": 20,
        "references": 67,
        "year": 2024
    },
    {
        "title": "ViNT: A Foundation Model for Visual Navigation",
        "abstract": "General-purpose pre-trained models (\"foundation models\") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic platforms, and exhibits positive transfer, outperforming specialist models trained on singular datasets. ViNT can be augmented with diffusion-based subgoal proposals to explore novel environments, and can solve kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced by an encoding of another task modality (e.g., GPS waypoints or routing commands) embedded into the same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem domains establishes ViNT as an effective foundation model for mobile robotics. For videos, code, and model checkpoints, see our project page at https://visualnav-transformer.github.io.",
        "authors": [
            "Dhruv Shah",
            "A. Sridhar",
            "Nitish Dashora",
            "Kyle Stachowicz",
            "Kevin Black",
            "N. Hirose",
            "S. Levine"
        ],
        "citations": 101,
        "references": 64,
        "year": 2023
    },
    {
        "title": "RemoteCLIP: A Vision Language Foundation Model for Remote Sensing",
        "abstract": "General-purpose foundation models have led to recent breakthroughs in artificial intelligence (AI). In remote sensing, self-supervised learning (SSL) and masked image modeling (MIM) have been adopted to build foundation models. However, these models primarily learn low-level features and require annotated data for fine-tuning. Moreover, they are inapplicable for retrieval and zero-shot applications due to the lack of language understanding. To address these limitations, we propose RemoteCLIP, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics and aligned text embeddings for seamless downstream application. To address the scarcity of pretraining data, we leverage data scaling which converts heterogeneous annotations into a unified image-caption data format based on box-to-caption (B2C) and mask-to-box (M2B) conversion. By further incorporating unmanned aerial vehicle (UAV) imagery, we produce a $12\\times $ larger pretraining dataset than the combination of all available datasets. RemoteCLIP can be applied to a variety of downstream tasks, including zero-shot image classification, linear probing, k-NN classification, few-shot classification, image-text retrieval, and object counting in remote sensing images. Evaluation of 16 datasets, including a newly introduced RemoteCount benchmark to test the object counting ability, shows that RemoteCLIP consistently outperforms baseline foundation models across different model scales. Impressively, RemoteCLIP beats the state-of-the-art (SOTA) method by 9.14% mean recall on the RSITMD dataset and 8.92% on the RSICD dataset. For zero-shot classification, our RemoteCLIP outperforms the contrastive language image pretraining (CLIP) baseline by up to 6.39% average accuracy on 12 downstream datasets.",
        "authors": [
            "F. Liu",
            "Delong Chen",
            "Zhan-Rong Guan",
            "Xiaocong Zhou",
            "Jiale Zhu",
            "Jun Zhou"
        ],
        "citations": 105,
        "references": 139,
        "year": 2023
    },
    {
        "title": "Large Scale Foundation Model on Single-cell Transcriptomics",
        "abstract": "Large-scale pretrained models have become foundation models leading to breakthroughs in natural language processing and related fields. Developing foundation models in life science for deciphering the “languages” of cells and facilitating biomedical research is promising yet challenging. We developed a large-scale pretrained model scFoundation with 100M parameters for this purpose. scFoundation was trained on over 50 million human single-cell transcriptomics data, which contain high-throughput observations on the complex molecular features in all known types of cells. scFoundation is currently the largest model in terms of the size of trainable parameters, dimensionality of genes and the number of cells used in the pre-training. Experiments showed that scFoundation can serve as a foundation model for single-cell transcriptomics and achieve state-of-the-art performances in a diverse array of downstream tasks, such as gene expression enhancement, tissue drug response prediction, single-cell drug response classification, and single-cell perturbation prediction.",
        "authors": [
            "Minsheng Hao",
            "Jing Gong",
            "Xin Zeng",
            "Chiming Liu",
            "Yucheng Guo",
            "Xingyi Cheng",
            "Taifeng Wang",
            "Jianzhu Ma",
            "Leo T. Song",
            "Xuegong Zhang"
        ],
        "citations": 99,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities",
        "abstract": "The development of foundation models has revolutionized our ability to interpret the Earth’s surface using satellite observational data. Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyper-spectral, each with its own unique characteristics. This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources. Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively. This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile Transformer jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining. DOFA’s innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of multimodal Earth observation data.",
        "authors": [
            "Zhitong Xiong",
            "Yi Wang",
            "Fahong Zhang",
            "Adam J. Stewart",
            "Joelle Hanna",
            "Damian Borth",
            "Ioannis Papoutsis",
            "B. L. Saux",
            "G. Camps-Valls",
            "Xiao Xiang Zhu"
        ],
        "citations": 17,
        "references": 88,
        "year": 2024
    },
    {
        "title": "Foundation model for cancer imaging biomarkers",
        "abstract": null,
        "authors": [
            "Suraj Pai",
            "Dennis Bontempi",
            "Ibrahim Hadzic",
            "Vasco Prudente",
            "M. Sokač",
            "T. Chaunzwa",
            "S. Bernatz",
            "A. Hosny",
            "Raymond H. Mak",
            "Nicolai J. Birkbak",
            "Hugo J W L Aerts"
        ],
        "citations": 22,
        "references": 18,
        "year": 2024
    },
    {
        "title": "MTP: Advancing Remote Sensing Foundation Model via Multitask Pretraining",
        "abstract": "Foundation models have reshaped the landscape of remote sensing (RS) by enhancing various image interpretation tasks. Pretraining is an active research topic, encompassing supervised and self-supervised learning methods to initialize model weights effectively. However, transferring the pretrained models to downstream tasks may encounter task discrepancy due to their formulation of pretraining as image classification or object discrimination tasks. In this study, we explore the multitask pretraining (MTP) paradigm for RS foundation models to address this issue. Using a shared encoder and task-specific decoder architecture, we conduct multitask supervised pretraining on the segment anything model annotated remote sensing segmentation dataset, encompassing semantic segmentation, instance segmentation, and rotated object detection. MTP supports both convolutional neural networks and vision transformer foundation models with over 300 million parameters. The pretrained models are finetuned on various RS downstream tasks, such as scene classification, horizontal, and rotated object detection, semantic segmentation, and change detection. Extensive experiments across 14 datasets demonstrate the superiority of our models over existing ones of similar size and their competitive performance compared to larger state-of-the-art models, thus validating the effectiveness of MTP.",
        "authors": [
            "Di Wang",
            "Jing Zhang",
            "Minqiang Xu",
            "Lin Liu",
            "Dongsheng Wang",
            "Erzhong Gao",
            "Chengxi Han",
            "Haonan Guo",
            "Bo Du",
            "Dacheng Tao",
            "L. Zhang"
        ],
        "citations": 18,
        "references": 201,
        "year": 2024
    },
    {
        "title": "Standing on the shoulders of giants: Online formative assessments as the foundation for predictive learning analytics models",
        "abstract": null,
        "authors": [
            "O. Bulut",
            "Guher Gorgun",
            "S. Yildirim-Erbasli",
            "Tarid Wongvorachan",
            "L. Daniels",
            "Yizhu Gao",
            "Ka Wing Lai",
            "Jinnie Shin"
        ],
        "citations": 27,
        "references": 68,
        "year": 2022
    },
    {
        "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome",
        "abstract": "Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates $36$ distinct datasets across $9$ tasks, with input lengths ranging from $70$ to $10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with $21 \\times$ fewer parameters and approximately $92 \\times$ less GPU time in pre-training.",
        "authors": [
            "Zhihan Zhou",
            "Yanrong Ji",
            "Weijian Li",
            "Pratik Dutta",
            "R. Davuluri",
            "Han Liu"
        ],
        "citations": 120,
        "references": 38,
        "year": 2023
    },
    {
        "title": "A pathology foundation model for cancer diagnosis and prognosis prediction.",
        "abstract": null,
        "authors": [
            "Xiyue Wang",
            "Junhan Zhao",
            "Eliana Marostica",
            "Wei Yuan",
            "Jietian Jin",
            "Jiayu Zhang",
            "Ruijiang Li",
            "Hongping Tang",
            "Kanran Wang",
            "Yu Li",
            "Fang Wang",
            "Yulong Peng",
            "Junyou Zhu",
            "Jing Zhang",
            "Christopher R Jackson",
            "Jun Zhang",
            "Deborah Dillon",
            "Nancy U Lin",
            "L. Sholl",
            "Thomas Denize",
            "David Meredith",
            "Keith L. Ligon",
            "S. Signoretti",
            "S. Ogino",
            "Jeffrey A Golden",
            "MacLean P Nasrallah",
            "Xiao Han",
            "Sen Yang",
            "Kun-Hsing Yu"
        ],
        "citations": 32,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Vision–language foundation model for echocardiogram interpretation",
        "abstract": null,
        "authors": [
            "M. Christensen",
            "Milos Vukadinovic",
            "Neal Yuan",
            "D. Ouyang"
        ],
        "citations": 32,
        "references": 39,
        "year": 2024
    },
    {
        "title": "Rethinking Machine Unlearning for Large Language Models",
        "abstract": "We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.",
        "authors": [
            "Sijia Liu",
            "Yuanshun Yao",
            "Jinghan Jia",
            "Stephen Casper",
            "Nathalie Baracaldo",
            "Peter Hase",
            "Xiaojun Xu",
            "Yuguang Yao",
            "Chris Liu",
            "Hang Li",
            "Kush R. Varshney",
            "Mohit Bansal",
            "Sanmi Koyejo",
            "Yang Liu"
        ],
        "citations": 49,
        "references": 151,
        "year": 2024
    },
    {
        "title": "scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI",
        "abstract": "Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between linguistic constructs and cellular biology — where texts comprise words, similarly, cells are defined by genes — our study probes the applicability of foundation models to advance cellular biology and genetics research. Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell-type annotation, multi-batch integration, multi-omic integration, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at https://github.com/bowang-lab/scGPT.",
        "authors": [
            "Haotian Cui",
            "Chloe X. Wang",
            "Hassaan Maan",
            "Kuan Pang",
            "Fengning Luo",
            "Bo Wang"
        ],
        "citations": 71,
        "references": 105,
        "year": 2023
    },
    {
        "title": "A Survey on Multimodal Large Language Models for Autonomous Driving",
        "abstract": "With the emergence of Large Language Models (LLMs) and Vision Foundation Models (VFMs), multimodal AI systems benefiting from large models have the potential to equally perceive the real world, make decisions, and control tools as humans. In recent months, LLMs have shown widespread attention in autonomous driving and map systems. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors to apply in LLM driving systems. In this paper, we present a systematic investigation in this field. We first introduce the background of Multimodal Large Language Models (MLLMs), the multimodal models development using LLMs, and the history of autonomous driving. Then, we overview existing MLLM tools for driving, transportation, and map systems together with existing datasets and benchmarks. Moreover, we summarized the works in The 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD), which is the first workshop of its kind regarding LLMs in autonomous driving. To further promote the development of this field, we also discuss several important problems regarding using MLLMs in autonomous driving systems that need to be solved by both academia and industry.",
        "authors": [
            "Can Cui",
            "Yunsheng Ma",
            "Xu Cao",
            "Wenqian Ye",
            "Yang Zhou",
            "Kaizhao Liang",
            "Jintai Chen",
            "Juanwu Lu",
            "Zichong Yang",
            "Kuei-Da Liao",
            "Tianren Gao",
            "Erlong Li",
            "Kun Tang",
            "Zhipeng Cao",
            "Tongxi Zhou",
            "Ao Liu",
            "Xinrui Yan",
            "Shuqi Mei",
            "Jianguo Cao",
            "Ziran Wang",
            "Chao Zheng"
        ],
        "citations": 163,
        "references": 209,
        "year": 2023
    },
    {
        "title": "CogVLM: Visual Expert for Pretrained Language Models",
        "abstract": "We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.",
        "authors": [
            "Weihan Wang",
            "Qingsong Lv",
            "Wenmeng Yu",
            "Wenyi Hong",
            "Ji Qi",
            "Yan Wang",
            "Junhui Ji",
            "Zhuoyi Yang",
            "Lei Zhao",
            "Xixuan Song",
            "Jiazheng Xu",
            "Bin Xu",
            "Juanzi Li",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "citations": 345,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Towards Generalist Foundation Model for Radiology",
        "abstract": "In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM. We consider the construction of foundational models from three perspectives, namely, dataset construction, model design, and thorough evaluation. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases. To the best of our knowledge, this is the first large-scale, high-quality, medical visual-language dataset, with both 2D and 3D scans; (ii), we propose an architecture that enables visually conditioned generative pre-training, i.e., allowing for integration of text input with 2D or 3D medical scans, and generate responses for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently fine-tuned on the domain-specific dataset, which is a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs, termed as RadMD; (iii), we propose a new evaluation benchmark, RadBench, that comprises five tasks, including modality recognition, disease diagnosis, visual question answering, report generation and rationale diagnosis, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems. We conduct both automatic and human evaluation on RadBench, in both cases, RadFM outperforms existing multi-modal foundation models, that are publicaly accessible, including Openflamingo, MedFlamingo, MedVInT and GPT-4V. Additionally, we also adapt RadFM for different public benchmarks, surpassing existing SOTAs on diverse datasets. All codes, data, and model checkpoint will all be made publicly available to promote further research and development in the field.",
        "authors": [
            "Chaoyi Wu",
            "Xiaoman Zhang",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "citations": 110,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Transparent medical image AI via an image-text foundation model grounded in medical literature.",
        "abstract": null,
        "authors": [
            "Chanwoo Kim",
            "S. U. Gadgil",
            "A. DeGrave",
            "J. Omiye",
            "Zhuo Ran Cai",
            "Roxana Daneshjou",
            "Su-In Lee"
        ],
        "citations": 22,
        "references": 34,
        "year": 2024
    },
    {
        "title": "Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions",
        "abstract": "Foundation model, trained on a diverse range of data and adaptable to a myriad of tasks, is advancing healthcare. It fosters the development of healthcare artificial intelligence (AI) models tailored to the intricacies of the medical field, bridging the gap between limited AI models and the varied nature of healthcare practices. The advancement of a healthcare foundation model (HFM) brings forth tremendous potential to augment intelligent healthcare services across a broad spectrum of scenarios. However, despite the imminent widespread deployment of HFMs, there is currently a lack of clear understanding regarding their operation in the healthcare field, their existing challenges, and their future trajectory. To answer these critical inquiries, we present a comprehensive and in-depth examination that delves into the landscape of HFMs. It begins with a comprehensive overview of HFMs, encompassing their methods, data, and applications, to provide a quick understanding of the current progress. Subsequently, it delves into a thorough exploration of the challenges associated with data, algorithms, and computing infrastructures in constructing and widely applying foundation models in healthcare. Furthermore, this survey identifies promising directions for future development in this field. We believe that this survey will enhance the community's understanding of the current progress of HFMs and serve as a valuable source of guidance for future advancements in this domain. For the latest HFM papers and related resources, please refer to our website.",
        "authors": [
            "Yuting He",
            "Fuxiang Huang",
            "Xinrui Jiang",
            "Yuxiang Nie",
            "Minghao Wang",
            "Jiguang Wang",
            "Hao Chen"
        ],
        "citations": 16,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Foundation Intelligence for Smart Infrastructure Services in Transportation 5.0",
        "abstract": "This perspective paper delves into the concept of foundation intelligence that shapes the future of smart infrastructure services as the transportation sector transitions into the era of Transportation 5.0. First, the discussion focuses on a suite of emerging technologies essential for foundation intelligence. These technologies encompass digital twinning, parallel intelligence, large vision-language models, traffic simulation and transportation systems modeling, vehicle-to-everything (V2X) connectivity, and decentralized/distributed systems. Next, the paper introduces the present landscape of Transportation 5.0 applications as illuminated by the foundational intelligence, and casts a vision towards the future including cooperative driving automation, smart intersection/infrastructure, parallel traffic management, virtual drivers, and mobility systems planning and operations, laying out prospects that are poised to redefine the mobility ecosystem. Last, through a comprehensive outlook, this paper aspires to offer a guiding framework for the intelligent evolution in data generation and model calibration, digital twinning and simulation, scenario development and experimentation, feedback loop for management and control, and continuous learning and adaptation, fostering safety, efficiency, reliability, and sustainability in the future smart transportation infrastructure.",
        "authors": [
            "Xu Han",
            "Zonglin Meng",
            "Xin Xia",
            "Xishun Liao",
            "Yueshuai He",
            "Zhaoliang Zheng",
            "Yutong Wang",
            "Hao Xiang",
            "Zewei Zhou",
            "Letian Gao",
            "Lili Fan",
            "Yuke Li",
            "Jiaqi Ma"
        ],
        "citations": 16,
        "references": 43,
        "year": 2024
    },
    {
        "title": "Retentive Network: A Successor to Transformer for Large Language Models",
        "abstract": "In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.",
        "authors": [
            "Yutao Sun",
            "Li Dong",
            "Shaohan Huang",
            "Shuming Ma",
            "Yuqing Xia",
            "Jilong Xue",
            "Jianyong Wang",
            "Furu Wei"
        ],
        "citations": 228,
        "references": 44,
        "year": 2023
    },
    {
        "title": "A decoder-only foundation model for time-series forecasting",
        "abstract": "Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.",
        "authors": [
            "Abhimanyu Das",
            "Weihao Kong",
            "Rajat Sen",
            "Yichen Zhou"
        ],
        "citations": 124,
        "references": 58,
        "year": 2023
    },
    {
        "title": "VideoCrafter1: Open Diffusion Models for High-Quality Video Generation",
        "abstract": "Video generation has increasingly gained interest in both academia and industry. Although commercial tools can generate plausible videos, there is a limited number of open-source models available for researchers and engineers. In this work, we introduce two diffusion models for high-quality video generation, namely text-to-video (T2V) and image-to-video (I2V) models. T2V models synthesize a video based on a given text input, while I2V models incorporate an additional image input. Our proposed T2V model can generate realistic and cinematic-quality videos with a resolution of $1024 \\times 576$, outperforming other open-source T2V models in terms of quality. The I2V model is designed to produce videos that strictly adhere to the content of the provided reference image, preserving its content, structure, and style. This model is the first open-source I2V foundation model capable of transforming a given image into a video clip while maintaining content preservation constraints. We believe that these open-source video generation models will contribute significantly to the technological advancements within the community.",
        "authors": [
            "Haoxin Chen",
            "Menghan Xia",
            "Yin-Yin He",
            "Yong Zhang",
            "Xiaodong Cun",
            "Shaoshu Yang",
            "Jinbo Xing",
            "Yaofang Liu",
            "Qifeng Chen",
            "Xintao Wang",
            "Chao-Liang Weng",
            "Ying Shan"
        ],
        "citations": 193,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models",
        "abstract": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.",
        "authors": [
            "Philip Anastassiou",
            "Jiawei Chen",
            "Jitong Chen",
            "Yuanzhe Chen",
            "Zhuo Chen",
            "Ziyi Chen",
            "Jian Cong",
            "Lelai Deng",
            "Chuang Ding",
            "Lu Gao",
            "Mingqing Gong",
            "Peisong Huang",
            "Qingqing Huang",
            "Zhiying Huang",
            "Yuanyuan Huo",
            "Dongya Jia",
            "Chumin Li",
            "Feiya Li",
            "Hui Li",
            "Jiaxin Li",
            "Xiaoyang Li",
            "Xingxing Li",
            "Lin Liu",
            "Shouda Liu",
            "Sichao Liu",
            "Xudong Liu",
            "Yuchen Liu",
            "Zhengxi Liu",
            "Lu Lu",
            "Junjie Pan",
            "Xin Wang",
            "Yuping Wang",
            "Yuxuan Wang",
            "Zhengnan Wei",
            "Jian Wu",
            "Chao Yao",
            "Yifeng Yang",
            "Yuan-Qiu-Qiang Yi",
            "Junteng Zhang",
            "Qidi Zhang",
            "Shuo Zhang",
            "Wenjie Zhang",
            "Yang Zhang",
            "Zilin Zhao",
            "Dejian Zhong",
            "Xiaobin Zhuang"
        ],
        "citations": 37,
        "references": 73,
        "year": 2024
    },
    {
        "title": "OmniJet-α: The first cross-task foundation model for particle physics",
        "abstract": "\n Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data. We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) and a classic supervised task (jet tagging) with our new OmniJet-α model. This is the first successful transfer between two different and actively studied classes of tasks and constitutes a major step in the building of foundation models for particle physics.",
        "authors": [
            "Joschka Birk",
            "Anna Hallin",
            "G. Kasieczka"
        ],
        "citations": 13,
        "references": 77,
        "year": 2024
    },
    {
        "title": "SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation",
        "abstract": "The rapid evolution of multimodal foundation model has demonstrated significant progresses in vision-language understanding and generation, e.g., our previous work SEED-LLaMA. However, there remains a gap between its capability and the real-world applicability, primarily due to the model's limited capacity to effectively respond to various user instructions and interact with diverse visual data. In this work, we focus on bridging this gap through integrating two enhanced features: (1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-granularity image generation. We present a unified and versatile foundation model, namely, SEED-X, which is able to model multi-granularity visual semantics for comprehension and generation tasks. Besides the competitive results on public benchmarks, SEED-X demonstrates its effectiveness in handling real-world applications across various domains after instruction tuning. We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications. The models, codes, and datasets will be released in https://github.com/AILab-CVC/SEED-X.",
        "authors": [
            "Yuying Ge",
            "Sijie Zhao",
            "Jinguo Zhu",
            "Yixiao Ge",
            "Kun Yi",
            "Lin Song",
            "Chen Li",
            "Xiaohan Ding",
            "Ying Shan"
        ],
        "citations": 46,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
        "abstract": "Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe.",
        "authors": [
            "Chulin Xie",
            "Zinan Lin",
            "A. Backurs",
            "Sivakanth Gopi",
            "Da Yu",
            "Huseyin A. Inan",
            "Harsha Nori",
            "Haotian Jiang",
            "Huishuai Zhang",
            "Yin Tat Lee",
            "Bo Li",
            "Sergey Yekhanin"
        ],
        "citations": 16,
        "references": 51,
        "year": 2024
    },
    {
        "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
        "abstract": "This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen initiative on foundation AI models. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our pre-trained base model exhibits strong in-context learning capabilities and the instruction-tuned model demonstrates competitive performance among open-source LMMs with similar model sizes. In addition, we introduce a safety-tuned model with DPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. We open-source our models, curated large-scale datasets, and our fine-tuning codebase to facilitate further advancements in LMM research. Associated resources will be available on our project page above.",
        "authors": [
            "Le Xue",
            "Manli Shu",
            "Anas Awadalla",
            "Jun Wang",
            "An Yan",
            "Senthil Purushwalkam",
            "Honglu Zhou",
            "Viraj Prabhu",
            "Yutong Dai",
            "Michael S. Ryoo",
            "Shrikant B. Kendre",
            "Jieyu Zhang",
            "Can Qin",
            "Shu Zhen Zhang",
            "Chia-Chih Chen",
            "Ning Yu",
            "Juntao Tan",
            "Tulika Awalgaonkar",
            "Shelby Heinecke",
            "Huan Wang",
            "Yejin Choi",
            "Ludwig Schmidt",
            "Zeyuan Chen",
            "Silvio Savarese",
            "Juan Carlos Niebles",
            "Caiming Xiong",
            "Ran Xu"
        ],
        "citations": 34,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Foundation Policies with Hilbert Representations",
        "abstract": "Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment, and then to span this learned latent space with directional movements, which enables various zero-shot policy\"prompting\"schemes for downstream tasks. Through our experiments on simulated robotic locomotion and manipulation benchmarks, we show that our unsupervised policies can solve goal-conditioned and general RL tasks in a zero-shot fashion, even often outperforming prior methods designed specifically for each setting. Our code and videos are available at https://seohong.me/projects/hilp/.",
        "authors": [
            "Seohong Park",
            "Tobias Kreiman",
            "Sergey Levine"
        ],
        "citations": 13,
        "references": 80,
        "year": 2024
    },
    {
        "title": "A generalist vision-language foundation model for diverse biomedical tasks.",
        "abstract": null,
        "authors": [
            "Kai Zhang",
            "Jun Yu",
            "Zhilin Yan",
            "Yixin Liu",
            "Eashan Adhikarla",
            "S. Fu",
            "Xun Chen",
            "Chen Chen",
            "Yuyin Zhou",
            "Xiang Li",
            "Lifang He",
            "B. Davison",
            "Quanzheng Li",
            "Yong Chen",
            "Hongfang Liu",
            "Lichao Sun"
        ],
        "citations": 88,
        "references": 26,
        "year": 2023
    },
    {
        "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
        "abstract": "Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio",
        "authors": [
            "Dongchao Yang",
            "Jinchuan Tian",
            "Xuejiao Tan",
            "Rongjie Huang",
            "Songxiang Liu",
            "Xuankai Chang",
            "Jiatong Shi",
            "Sheng Zhao",
            "Jiang Bian",
            "Xixin Wu",
            "Zhou Zhao",
            "Helen Meng"
        ],
        "citations": 91,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Large Language Models are Geographically Biased",
        "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
        "authors": [
            "Rohin Manvi",
            "Samar Khanna",
            "Marshall Burke",
            "David B. Lobell",
            "Stefano Ermon"
        ],
        "citations": 32,
        "references": 61,
        "year": 2024
    },
    {
        "title": "OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer",
        "abstract": "Recent studies have highlighted the importance of fully open foundation models. The Open Whisper-style Speech Model (OWSM) is an initial step towards reproducing OpenAI Whisper using public data and open-source toolkits. However, previous versions of OWSM (v1 to v3) are still based on standard Transformer, which might lead to inferior performance compared to state-of-the-art speech encoder architectures. This work aims to improve the performance and efficiency of OWSM without additional data. We present a series of E-Branchformer-based models named OWSM v3.1, ranging from 100M to 1B parameters. OWSM v3.1 outperforms its predecessor, OWSM v3, in most evaluation benchmarks, while showing an improved inference speed of up to 25%. We further reveal the emergent ability of OWSM v3.1 in zero-shot contextual biasing speech recognition. We also provide a model trained on a subset of data with low license restrictions. We will publicly release the code, pre-trained models, and training logs.",
        "authors": [
            "Yifan Peng",
            "Jinchuan Tian",
            "William Chen",
            "Siddhant Arora",
            "Brian Yan",
            "Yui Sudo",
            "Muhammad Shakeel",
            "Kwanghee Choi",
            "Jiatong Shi",
            "Xuankai Chang",
            "Jee-weon Jung",
            "Shinji Watanabe"
        ],
        "citations": 30,
        "references": 59,
        "year": 2024
    },
    {
        "title": "Skywork: A More Open Bilingual Foundation Model",
        "abstract": "In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves \\emph{state of the art} performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",
        "authors": [
            "Tianwen Wei",
            "Liang Zhao",
            "Lichang Zhang",
            "Bo Zhu",
            "Lijie Wang",
            "Haihua Yang",
            "Biye Li",
            "Cheng Cheng",
            "Weiwei Lü",
            "Rui Hu",
            "Chenxia Li",
            "Liu Yang",
            "Xilin Luo",
            "X. Wu",
            "Lunan Liu",
            "Wenjun Cheng",
            "Peng Cheng",
            "Jianhao Zhang",
            "Xiaoyu Zhang",
            "Lei Lin",
            "Xiaokun Wang",
            "Yutuan Ma",
            "Chuanhai Dong",
            "Yanqi Sun",
            "Yifu Chen",
            "Yongyi Peng",
            "Xiaojuan Liang",
            "Shuicheng Yan",
            "Han Fang",
            "Yahui Zhou"
        ],
        "citations": 83,
        "references": 56,
        "year": 2023
    },
    {
        "title": "SpectralGPT: Spectral Foundation Model",
        "abstract": "—The foundation model has recently garnered significant attention due to its potential to revolutionize the field of visual representation learning in a self-supervised manner. While most foundation models are tailored to effectively process RGB images for various visual tasks, there is a noticeable gap in research focused on spectral data, which offers valuable information for scene understanding, especially in remote sensing (RS) applications. To fill this gap, we created for the first time a universal RS foundation model, named SpectralGPT, which is purpose-built to handle spectral RS images using a novel 3D generative pretrained transformer (GPT). Compared to existing foundation models, SpectralGPT 1) accommodates input images with varying sizes, resolutions, time series, and regions in a progressive training fashion, enabling full utilization of extensive RS big data; 2) leverages 3D token generation for spatial-spectral coupling; 3) captures spectrally sequential patterns via",
        "authors": [
            "D. Hong",
            "Bing Zhang",
            "Xuyang Li",
            "Yuxuan Li",
            "Chenyu Li",
            "Jing Yao",
            "N. Yokoya",
            "Hao Li",
            "Pedram Ghamisi",
            "Xiuping Jia",
            "Antonio J. Plaza",
            "Paolo Gamba",
            "J. Benediktsson",
            "J. Chanussot"
        ],
        "citations": 61,
        "references": 42,
        "year": 2023
    },
    {
        "title": "EndoDAC: Efficient Adapting Foundation Model for Self-Supervised Depth Estimation from Any Endoscopic Camera",
        "abstract": "Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization. Despite the significant achievements of foundation models in vision tasks, including depth estimation, their direct application to the medical domain often results in suboptimal performance. This highlights the need for efficient adaptation methods to adapt these models to endoscopic depth estimation. We propose Endoscopic Depth Any Camera (EndoDAC) which is an efficient self-supervised depth estimation framework that adapts foundation models to endoscopic scenes. Specifically, we develop the Dynamic Vector-Based Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor the foundational model to the surgical domain, utilizing remarkably few trainable parameters. Given that camera information is not always accessible, we also introduce a self-supervised adaptation strategy that estimates camera intrinsics using the pose encoder. Our framework is capable of being trained solely on monocular surgical videos from any camera, ensuring minimal training costs. Experiments demonstrate that our approach obtains superior performance even with fewer training epochs and unaware of the ground truth camera intrinsics. Code is available at https://github.com/BeileiCui/EndoDAC.",
        "authors": [
            "Beilei Cui",
            "Mobarakol Islam",
            "Long Bai",
            "An-Chi Wang",
            "Hongliang Ren"
        ],
        "citations": 9,
        "references": 34,
        "year": 2024
    },
    {
        "title": "PRISM: A Multi-Modal Generative Foundation Model for Slide-Level Histopathology",
        "abstract": "Foundation models in computational pathology promise to unlock the development of new clinical decision support systems and models for precision medicine. However, there is a mismatch between most clinical analysis, which is defined at the level of one or more whole slide images, and foundation models to date, which process the thousands of image tiles contained in a whole slide image separately. The requirement to train a network to aggregate information across a large number of tiles in multiple whole slide images limits these models' impact. In this work, we present a slide-level foundation model for H&E-stained histopathology, PRISM, that builds on Virchow tile embeddings and leverages clinical report text for pre-training. Using the tile embeddings, PRISM produces slide-level embeddings with the ability to generate clinical reports, resulting in several modes of use. Using text prompts, PRISM achieves zero-shot cancer detection and sub-typing performance approaching and surpassing that of a supervised aggregator model. Using the slide embeddings with linear classifiers, PRISM surpasses supervised aggregator models. Furthermore, we demonstrate that fine-tuning of the PRISM slide encoder yields label-efficient training for biomarker prediction, a task that typically suffers from low availability of training data; an aggregator initialized with PRISM and trained on as little as 10% of the training data can outperform a supervised baseline that uses all of the data.",
        "authors": [
            "George Shaikovski",
            "Adam Casson",
            "Kristen Severson",
            "Eric Zimmermann",
            "Yi Kan Wang",
            "J. Kunz",
            "J. Retamero",
            "Gerard Oakley",
            "D. Klimstra",
            "C. Kanan",
            "Matthew G Hanna",
            "Michal Zelechowski",
            "Julian Viret",
            "Neil Tenenholtz",
            "James Hall",
            "Nicolò Fusi",
            "Razik Yousfi",
            "Peter Hamilton",
            "William A. Moye",
            "Eugene Vorontsov",
            "Siqi Liu",
            "Thomas J Fuchs"
        ],
        "citations": 12,
        "references": 48,
        "year": 2024
    },
    {
        "title": "A Survey on Efficient Federated Learning Methods for Foundation Model Training",
        "abstract": "Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training across a multitude of clients. However, new approaches to FL often discuss their contributions involving small deep-learning models only and focus on training full models on clients. In the wake of Foundation Models (FM), the reality is different for many deep learning applications. Typically, FMs have already been pre-trained across a wide variety of tasks and can be fine-tuned to specific downstream tasks over significantly smaller datasets than required for full model training. However, access to such datasets is often challenging. By its design, FL can help to open data silos. With this survey, we introduce a novel taxonomy focused on computational and communication efficiency, the vital elements to make use of FMs in FL systems. We discuss the benefits and drawbacks of parameter-efficient fine-tuning (PEFT) for FL applications, elaborate on the readiness of FL frameworks to work with FMs and provide future research opportunities on how to evaluate generative models in FL as well as the interplay of privacy and PEFT.",
        "authors": [
            "Herbert Woisetschläger",
            "Alexander Isenko",
            "Shiqiang Wang",
            "R. Mayer",
            "Hans-Arno Jacobsen"
        ],
        "citations": 12,
        "references": 79,
        "year": 2024
    },
    {
        "title": "A Survey on Vision Mamba: Models, Applications and Challenges",
        "abstract": "—Mamba, a recent selective structured state space model, performs excellently on long sequence modeling tasks. Mamba mitigates the modeling constraints of convolutional neural networks and offers advanced modeling capabilities similar to those of Transformers, through global receptive fields and dynamic weighting. Crucially, it achieves this without incurring the quadratic computational complexity typically associated with Transformers. Due to its advantages over the former two mainstream foundation models, Mamba exhibits great potential to be a visual foundation model. Researchers are actively applying Mamba to various computer vision tasks, leading to numerous emerging works. To help keep pace with the rapid advancements in computer vision, this paper aims to provide a comprehensive review of visual Mamba approaches. This paper begins by delineating the formulation of the original Mamba model. Subsequently, our review of visual Mamba delves into several representative backbone networks to elucidate the core insights of the visual Mamba. We then categorize related works using different modalities, including image, video, point cloud, multi-modal, and others. Specifically, for image applications, we further organize them into distinct tasks to facilitate a more structured discussion. Finally, we discuss the challenges and future research directions for visual Mamba, providing insights for future research in this quickly evolving area. A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.",
        "authors": [
            "Rui Xu",
            "Shu Yang",
            "Yihui Wang",
            "Bo Du",
            "Hao Chen"
        ],
        "citations": 23,
        "references": 114,
        "year": 2024
    },
    {
        "title": "A Billion-scale Foundation Model for Remote Sensing Images",
        "abstract": "As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation model in the remote sensing field. Furthermore, we propose an effective method for scaling up and fine-tuning a vision transformer in the remote sensing field. To evaluate general performance in downstream tasks, we employed the DOTA v2.0 and DIOR-R benchmark datasets for rotated object detection, and the Potsdam and LoveDA datasets for semantic segmentation. Experimental results demonstrated that, across all benchmark datasets and downstream tasks, the performance of the foundation models and data efficiency improved as the number of parameters increased. Moreover, our models achieve the state-of-the-art performance on several datasets including DIOR-R, Postdam, and LoveDA.",
        "authors": [
            "Keumgang Cha",
            "Junghoon Seo",
            "Taekyung Lee"
        ],
        "citations": 45,
        "references": 147,
        "year": 2023
    },
    {
        "title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
        "abstract": "Recent advances in Foundation Models such as Large Language Models (LLMs) have propelled them to the forefront of Recommender Systems (RS). Despite their utility, there is a growing concern that LLMs might inadvertently perpetuate societal stereotypes, resulting in unfair recommendations. Since fairness is critical for RS as many users take it for decision-making and demand fulfillment, this paper focuses on user-side fairness for LLM-based recommendation where the users may require a recommender system to be fair on specific sensitive features such as gender or age. In this paper, we dive into the extent of unfairness exhibited by LLM-based recommender models based on both T5 and LLaMA backbones, and discuss appropriate methods for promoting equitable treatment of users in LLM-based recommendation models. We introduce a novel Counterfactually-Fair-Prompt (CFP) method towards Unbiased Foundation mOdels (UFO) for fairness-aware LLM-based recommendation. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and compared with both matching-based and sequential-based fairness-aware recommendation models. Results show that CFP achieves better recommendation performance with a high level of fairness.",
        "authors": [
            "Wenyue Hua",
            "Yingqiang Ge",
            "Shuyuan Xu",
            "Jianchao Ji",
            "Yongfeng Zhang"
        ],
        "citations": 40,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Generative ConvNet Foundation Model With Sparse Modeling and Low-Frequency Reconstruction for Remote Sensing Image Interpretation",
        "abstract": "Foundation models offer a highly versatile and precise solution for intelligent interpretation of remote sensing images, thus greatly facilitating various remote sensing applications. Nevertheless, conventional remote sensing foundational models based on generative transformers neglect the consideration of multiscale features and frequency information, limiting their potential for dense prediction tasks in remote sensing scenarios. In this article, we make the first attempt to propose a generative convolutional neural network (ConvNet) foundation model tailored for remote sensing scenarios, which comprises two key components: First, a large dataset named GeoSense, containing approximately nine million diverse remote sensing images, is constructed to enhance the robustness and generalization of the foundation model during the pretraining phase. Second, a sparse modeling and low-frequency reconstruction (SMLFR) framework is designed for self-supervised representation learning of the ConvNet foundation model. Specifically, a sparse modeling strategy is proposed in masked image modeling (MIM), which allows ConvNet to process variable-length sequences by treating unmasked patches as voxels and sparsifying the encoder. In addition, a low-frequency reconstruction target is designed to guide the model’s attention toward essential ground object features in remote sensing images, while mitigating unnecessary detail interference. To evaluate the general performance of our proposed foundation model, comprehensive experiments have been carried out on five datasets across three downstream tasks. Experimental results demonstrate that our method consistently achieves state-of-the-art performance across all the benchmark datasets and downstream tasks. The code and pretrained models will be available at https://github.com/HIT-SIRS/SMLFR.",
        "authors": [
            "Zhe Dong",
            "Yanfeng Gu",
            "Tianzhu Liu"
        ],
        "citations": 10,
        "references": 115,
        "year": 2024
    },
    {
        "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping",
        "abstract": "This paper assesses trending AI foundation models, especially emerging computer vision foundation models and their performance in natural landscape feature segmentation. While the term foundation model has quickly garnered interest from the geospatial domain, its definition remains vague. Hence, this paper will first introduce AI foundation models and their defining characteristics. Built upon the tremendous success achieved by Large Language Models (LLMs) as the foundation models for language tasks, this paper discusses the challenges of building foundation models for geospatial artificial intelligence (GeoAI) vision tasks. To evaluate the performance of large AI vision models, especially Meta’s Segment Anything Model (SAM), we implemented different instance segmentation pipelines that minimize the changes to SAM to leverage its power as a foundation model. A series of prompt strategies were developed to test SAM’s performance regarding its theoretical upper bound of predictive accuracy, zero-shot performance, and domain adaptability through fine-tuning. The analysis used two permafrost feature datasets, ice-wedge polygons and retrogressive thaw slumps because (1) these landform features are more challenging to segment than man-made features due to their complicated formation mechanisms, diverse forms, and vague boundaries; (2) their presence and changes are important indicators for Arctic warming and climate change. The results show that although promising, SAM still has room for improvement to support AI-augmented terrain mapping. The spatial and domain generalizability of this finding is further validated using a more general dataset EuroCrops for agricultural field mapping. Finally, we discuss future research directions that strengthen SAM’s applicability in challenging geospatial domains.",
        "authors": [
            "Wenwen Li",
            "Chia-Yu Hsu",
            "Sizhe Wang",
            "Yezhou Yang",
            "Hyunho Lee",
            "A. Liljedahl",
            "C. Witharana",
            "Yili Yang",
            "B. Rogers",
            "S. Arundel",
            "Matthew B. Jones",
            "Kenton McHenry",
            "Patricia Solis"
        ],
        "citations": 10,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Towards a Foundation Model for Partial Differential Equations: Multi-Operator Learning and Extrapolation",
        "abstract": "Foundation models, such as large language models, have demonstrated success in addressing various language and image processing tasks. In this work, we introduce a multi-modal foundation model for scientific problems, named PROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a multi-operator learning approach which can predict future states of spatiotemporal systems while concurrently learning the underlying governing equations of the physical system. Specifically, we focus on multi-operator learning by training distinct one-dimensional time-dependent nonlinear constant coefficient partial differential equations, with potential applications to many physical applications including physics, geology, and biology. More importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE can generalize physical features through the robust training of multiple operators and that the proposed model can extrapolate to predict PDE solutions whose models or data were unseen during the training. Furthermore, we show through systematic numerical experiments that the utilization of the symbolic modality in our model effectively resolves the well-posedness problems with training multiple operators and thus enhances our model's predictive capabilities.",
        "authors": [
            "Jingmin Sun",
            "Yuxuan Liu",
            "Zecheng Zhang",
            "Hayden Schaeffer"
        ],
        "citations": 10,
        "references": 78,
        "year": 2024
    },
    {
        "title": "Foundation Model-Based Multimodal Remote Sensing Data Classification",
        "abstract": "With the increasing availability and openness of remote sensing (RS) data collected from diverse sensors, there has been a growing interest in multimodal RS data classification. Nowadays, in the area of deep learning, there is a paradigm shift with the rise of foundation models, which are trained on large-scale datasets and are adaptable to a wide range of downstream tasks. In this study, the potential and effectiveness of foundation models for multimodal RS data classification is investigated. The training datasets of foundation models and multimodal RS datasets are quite different, and therefore, it is difficult to use a pretrained foundation model for multimodal RS data classification directly. To mitigate this difficulty, this article proposes a foundation model adaptation (FMA) framework for multimodal RS data classification without fine-tuning the parameters. Specifically, two learnable modules, i.e., cross-spatial interaction module and cross-channel interaction module, are proposed to add to the foundation model for extracting multimodal-specific representations. The cross-spatial and cross-channel interaction modules extract the characteristics of unimodal features along the spatial dimension and channel dimension, respectively. To effectively tackle the disparities among various RS modalities, an alignment approach (FMA2) is further explored based on the FMA. The FMA2 describes dependencies between different modalities by establishing a coupling score function, which can further enhance classification performance. To demonstrate the effectiveness and superiority of the FMA framework, comprehensive experiments are conducted on three multimodal RS datasets, showing improvement over the advanced multimodal RS data classification image methods.",
        "authors": [
            "Xin He",
            "Yushi Chen",
            "Lin Huang",
            "Danfeng Hong",
            "Q. Du"
        ],
        "citations": 11,
        "references": 59,
        "year": 2024
    },
    {
        "title": "The Foundation Model Transparency Index v1.1: May 2024",
        "abstract": "Foundation models are increasingly consequential yet extremely opaque. To characterize the status quo, the Foundation Model Transparency Index was launched in October 2023 to measure the transparency of leading foundation model developers. The October 2023 Index (v1.0) assessed 10 major foundation model developers (e.g. OpenAI, Google) on 100 transparency indicators (e.g. does the developer disclose the wages it pays for data labor?). At the time, developers publicly disclosed very limited information with the average score being 37 out of 100. To understand how the status quo has changed, we conduct a follow-up study (v1.1) after 6 months: we score 14 developers against the same 100 indicators. While in v1.0 we searched for publicly available information, in v1.1 developers submit reports on the 100 transparency indicators, potentially including information that was not previously public. We find that developers now score 58 out of 100 on average, a 21 point improvement over v1.0. Much of this increase is driven by developers disclosing information during the v1.1 process: on average, developers disclosed information related to 16.6 indicators that was not previously public. We observe regions of sustained (i.e. across v1.0 and v1.1) and systemic (i.e. across most or all developers) opacity such as on copyright status, data access, data labor, and downstream impact. We publish transparency reports for each developer that consolidate information disclosures: these reports are based on the information disclosed to us via developers. Our findings demonstrate that transparency can be improved in this nascent ecosystem, the Foundation Model Transparency Index likely contributes to these improvements, and policymakers should consider interventions in areas where transparency has not improved.",
        "authors": [
            "Rishi Bommasani",
            "Kevin Klyman",
            "Sayash Kapoor",
            "Shayne Longpre",
            "Betty Xiong",
            "Nestor Maslej",
            "Percy Liang"
        ],
        "citations": 10,
        "references": 81,
        "year": 2024
    },
    {
        "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
        "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available at https://github.com/chs20/RobustVLM",
        "authors": [
            "Christian Schlarmann",
            "Naman D. Singh",
            "Francesco Croce",
            "Matthias Hein"
        ],
        "citations": 15,
        "references": 68,
        "year": 2024
    },
    {
        "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
        "abstract": "Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vision-text, audio-text, and multi-modal video-text tasks (retrieval, captioning and QA). Extensive experiments have been conducted to demonstrate the effectiveness of our proposed VAST-27M corpus and VAST foundation model. VAST achieves 22 new state-of-the-art results on various cross-modality benchmarks. Code, model and dataset will be released at https://github.com/TXH-mercury/VAST.",
        "authors": [
            "Sihan Chen",
            "Handong Li",
            "Qunbo Wang",
            "Zijia Zhao",
            "Ming-Ting Sun",
            "Xinxin Zhu",
            "J. Liu"
        ],
        "citations": 68,
        "references": 102,
        "year": 2023
    },
    {
        "title": "MolFM: A Multimodal Molecular Foundation Model",
        "abstract": "Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures local and global molecular knowledge by minimizing the distance in the feature space between different modalities of the same molecule, as well as molecules sharing similar structures or functions. MolFM achieves state-of-the-art performance on various downstream tasks. On cross-modal retrieval, MolFM outperforms existing models with 12.13% and 5.04% absolute gains under the zero-shot and fine-tuning settings, respectively. Furthermore, qualitative analysis showcases MolFM's implicit ability to provide grounding from molecular substructures and knowledge graphs. Code and models are available on https://github.com/BioFM/OpenBioMed.",
        "authors": [
            "Yi Luo",
            "Kai Yang",
            "Massimo Hong",
            "Xingyi Liu",
            "Zaiqing Nie"
        ],
        "citations": 31,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Text-guided Foundation Model Adaptation for Pathological Image Classification",
        "abstract": "The recent surge of foundation models in computer vision and natural language processing opens up perspectives in utilizing multi-modal clinical data to train large models with strong generalizability. Yet pathological image datasets often lack biomedical text annotation and enrichment. Guiding data-efficient image diagnosis from the use of biomedical text knowledge becomes a substantial interest. In this paper, we propose to Connect Image and Text Embeddings (CITE) to enhance pathological image classification. CITE injects text insights gained from language models pre-trained with a broad range of biomedical texts, leading to adapt foundation models towards pathological image understanding. Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce. CITE offers insights into leveraging in-domain text knowledge to reinforce data-efficient pathological image classification. Code is available at https://github.com/Yunkun-Zhang/CITE.",
        "authors": [
            "Yunkun Zhang",
            "Jinglei Gao",
            "Mu Zhou",
            "Xiaosong Wang",
            "Y. Qiao",
            "Shaoting Zhang",
            "Dequan Wang"
        ],
        "citations": 32,
        "references": 31,
        "year": 2023
    },
    {
        "title": "The Foundation Model Transparency Index",
        "abstract": "Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts. While the societal impact of foundation models is growing, transparency is on the decline, mirroring the opacity that has plagued past digital technologies (e.g. social media). Reversing this trend is essential: transparency is a vital precondition for public accountability, scientific innovation, and effective governance. To assess the transparency of the foundation model ecosystem and help improve transparency over time, we introduce the Foundation Model Transparency Index. The Foundation Model Transparency Index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). We score 10 major foundation model developers (e.g. OpenAI, Google, Meta) against the 100 indicators to assess their transparency. To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. Overall, the Foundation Model Transparency Index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.",
        "authors": [
            "Rishi Bommasani",
            "Kevin Klyman",
            "S. Longpre",
            "Sayash Kapoor",
            "Nestor Maslej",
            "Betty Xiong",
            "Daniel Zhang",
            "Percy Liang"
        ],
        "citations": 48,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
        "abstract": "As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.",
        "authors": [
            "Yao Fu",
            "Litu Ou",
            "Mingyu Chen",
            "Yuhao Wan",
            "Hao-Chun Peng",
            "Tushar Khot"
        ],
        "citations": 97,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model",
        "abstract": "The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical image data, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image input for commonly-used medical image segmentation models (e.g., U-Net). Experiments on three segmentation tasks show the effectiveness of our proposed SAMAug method. The code is available at \\url{https://github.com/yizhezhang2000/SAMAug}.",
        "authors": [
            "Yizhe Zhang",
            "Tao Zhou",
            "Peixian Liang",
            "Da Chen"
        ],
        "citations": 66,
        "references": 33,
        "year": 2023
    },
    {
        "title": "Virchow: A Million-Slide Digital Pathology Foundation Model",
        "abstract": "The use of artificial intelligence to enable precision medicine and decision support systems through the analysis of pathology images has the potential to revolutionize the diagnosis and treatment of cancer. Such applications will depend on models' abilities to capture the diverse patterns observed in pathology images. To address this challenge, we present Virchow, a foundation model for computational pathology. Using self-supervised learning empowered by the DINOv2 algorithm, Virchow is a vision transformer model with 632 million parameters trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue and specimen types, which is orders of magnitude more data than previous works. The Virchow model enables the development of a pan-cancer detection system with 0.949 overall specimen-level AUC across 17 different cancer types, while also achieving 0.937 AUC on 7 rare cancer types. The Virchow model sets the state-of-the-art on the internal and external image tile level benchmarks and slide level biomarker prediction tasks. The gains in performance highlight the importance of training on massive pathology image datasets, suggesting scaling up the data and network architecture can improve the accuracy for many high-impact computational pathology applications where limited amounts of training data are available.",
        "authors": [
            "Eugene Vorontsov",
            "A. Bozkurt",
            "Adam Casson",
            "George Shaikovski",
            "Michal Zelechowski",
            "Siqi Liu",
            "Eric Zimmermann",
            "Philippe Mathieu",
            "Alexander van Eck",
            "Donghun Lee",
            "Julian Viret",
            "Eric Robert",
            "Yi Kan Wang",
            "Jeremy D. Kun",
            "Matthew C. H. Le",
            "Jan H Bernhard",
            "R. Godrich",
            "Gerard Oakley",
            "Ewan Millar",
            "Matthew G Hanna",
            "J. Retamero",
            "William A. Moye",
            "Razik Yousfi",
            "Christopher Kanan",
            "D. Klimstra",
            "B. Rothrock",
            "Thomas J. Fuchs"
        ],
        "citations": 52,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train",
        "abstract": "Foundation models have exhibited remarkable success in various applications, such as disease diagnosis and text report generation. To date, a foundation model for endoscopic video analysis is still lacking. In this paper, we propose Endo-FM, a foundation model specifically developed using massive endoscopic video data. First, we build a video transformer, which captures both local and global long-range dependencies across spatial and temporal dimensions. Second, we pre-train our transformer model using global and local views via a self-supervised manner, aiming to make it robust to spatial-temporal variations and discriminative across different scenes. To develop the foundation model, we construct a large-scale endoscopy video dataset by combining 9 publicly available datasets and a privately collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K video clips with up to 5 million frames, encompassing various protocols, target organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a given downstream task via fine-tuning by serving as the backbone. With experiments on 3 different types of downstream tasks, including classification, segmentation, and detection, our Endo-FM surpasses the current state-of-the-art (SOTA) self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6% Dice, and 9.9% F1 for classification, segmentation, and detection). Code, datasets, and models are released at https://github.com/med-air/Endo-FM.",
        "authors": [
            "Zhao Wang",
            "Chang Liu",
            "Shaoting Zhang",
            "Q. Dou"
        ],
        "citations": 38,
        "references": 37,
        "year": 2023
    },
    {
        "title": "scGPT: Towards Building a Foundation Model for Single-Cell 1 Multi-omics Using Generative AI",
        "abstract": "10 Generative pre-trained models have achieved remarkable success in various domains such as nat-11 ural language processing and computer vision. Specifically, the combination of large-scale diverse 12 datasets and pre-trained transformers has emerged as a promising approach for developing founda-13 tion models. While texts are made up of words, cells can be characterized by genes. This analogy 14 inspires us to explore the potential of foundation models for cell and gene biology. By leveraging the",
        "authors": [
            "Haotian Cui",
            "Chloe X. Wang",
            "Hassaan Maan",
            "Bo Wang",
            "C. E. D. Masked-Attention"
        ],
        "citations": 30,
        "references": 66,
        "year": 2023
    },
    {
        "title": "RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model",
        "abstract": "Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we tried several Parameter-Efficient Fine-Tuning methods on RS5M to implement the DFM. Experimental results show that our proposed dataset is highly effective for various tasks, improving upon the baseline by 8% „ 16% in zero-shot classification tasks, and obtaining good results in both Vision-Language Retrieval and Semantic Localization tasks. https",
        "authors": [
            "Zilun Zhang",
            "Tiancheng Zhao",
            "Yulong Guo",
            "Jianwei Yin"
        ],
        "citations": 30,
        "references": 130,
        "year": 2023
    },
    {
        "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
        "abstract": "This paper presents a novel method for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 90% of the time.",
        "authors": [
            "Junlin Han",
            "Filippos Kokkinos",
            "Philip Torr"
        ],
        "citations": 27,
        "references": 73,
        "year": 2024
    },
    {
        "title": "A Variational Perspective on Solving Inverse Problems with Diffusion Models",
        "abstract": "Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-Diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-to-noise-ratio (SNR). Our approach provides a new variational perspective for solving inverse problems with diffusion models, allowing us to formulate sampling as stochastic optimization, where one can simply apply off-the-shelf solvers with lightweight iterates. Our experiments for image restoration tasks such as inpainting and superresolution demonstrate the strengths of our method compared with state-of-the-art sampling-based diffusion models.",
        "authors": [
            "M. Mardani",
            "Jiaming Song",
            "J. Kautz",
            "Arash Vahdat"
        ],
        "citations": 92,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "abstract": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc. Project page: https://lotus3d.github.io/.",
        "authors": [
            "Jing He",
            "Haodong Li",
            "Wei Yin",
            "Yixun Liang",
            "Leheng Li",
            "Kaiqiang Zhou",
            "Hongbo Liu",
            "Bingbing Liu",
            "Ying-Cong Chen"
        ],
        "citations": 12,
        "references": 66,
        "year": 2024
    },
    {
        "title": "OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification",
        "abstract": "There has been an increasing interest in large speech models that can perform multiple tasks in a single model. Such models usually adopt an encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up to 24% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form ASR result with 20x speed-up. We will publicly release our code, pre-trained model, and training logs to promote open science in speech foundation models.",
        "authors": [
            "Yifan Peng",
            "Yui Sudo",
            "Muhammad Shakeel",
            "Shinji Watanabe"
        ],
        "citations": 12,
        "references": 84,
        "year": 2024
    },
    {
        "title": "AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception",
        "abstract": "The highly abstract nature of image aesthetics perception (IAP) poses significant challenge for current multimodal large language models (MLLMs). The lack of human-annotated multi-modality aesthetic data further exacerbates this dilemma, resulting in MLLMs falling short of aesthetics perception capabilities. To address the above challenge, we first introduce a comprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT) dataset, which serves as the footstone for building multi-modality aesthetics foundation models. Specifically, to align MLLMs with human aesthetics perception, we construct a corpus-rich aesthetic critique database with 21,904 diverse-sourced images and 88K human natural language feedbacks, which are collected via progressive questions, ranging from coarse-grained aesthetic grades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle diverse queries, we further prompt GPT to refine the aesthetic critiques and assemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT, which consists of 409K multi-typed instructions to activate stronger aesthetic capabilities. Based on the AesMMIT database, we fine-tune the open-sourced general foundation models, achieving multi-modality Aesthetic Expert models, dubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert models deliver significantly better aesthetic perception performances than the state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision. Project homepage: https://yipoh.github.io/aes-expert/.",
        "authors": [
            "Yipo Huang",
            "Xiangfei Sheng",
            "Zhichao Yang",
            "Quan Yuan",
            "Zhichao Duan",
            "Pengfei Chen",
            "Leida Li",
            "Weisi Lin",
            "Guangming Shi"
        ],
        "citations": 9,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Endora: Video Generation Models as Endoscopy Simulators",
        "abstract": "Generative models hold promise for revolutionizing medical education, robot-assisted surgery, and data augmentation for machine learning. Despite progress in generating 2D medical images, the complex domain of clinical video generation has largely remained untapped.This paper introduces \\model, an innovative approach to generate medical videos that simulate clinical endoscopy scenes. We present a novel generative model design that integrates a meticulously crafted spatial-temporal video transformer with advanced 2D vision foundation model priors, explicitly modeling spatial-temporal dynamics during video generation. We also pioneer the first public benchmark for endoscopy simulation with video generation models, adapting existing state-of-the-art methods for this endeavor.Endora demonstrates exceptional visual quality in generating endoscopy videos, surpassing state-of-the-art methods in extensive testing. Moreover, we explore how this endoscopy simulator can empower downstream video analysis tasks and even generate 3D medical scenes with multi-view consistency. In a nutshell, Endora marks a notable breakthrough in the deployment of generative AI for clinical endoscopy research, setting a substantial stage for further advances in medical content generation. For more details, please visit our project page: https://endora-medvidgen.github.io/.",
        "authors": [
            "Chenxin Li",
            "Hengyu Liu",
            "Yifan Liu",
            "Brandon Y. Feng",
            "Wuyang Li",
            "Xinyu Liu",
            "Zhen Chen",
            "Jing Shao",
            "Yixuan Yuan"
        ],
        "citations": 23,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Advancing Plain Vision Transformer Toward Remote Sensing Foundation Model",
        "abstract": "Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers (ViTs) being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this article, we resort to plain ViTs with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mean average precision (mAP) on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring. The code and models will be released at https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA.",
        "authors": [
            "Di Wang",
            "Qiming Zhang",
            "Yufei Xu",
            "Jing Zhang",
            "Bo Du",
            "Dacheng Tao",
            "L. Zhang"
        ],
        "citations": 186,
        "references": 90,
        "year": 2022
    },
    {
        "title": "A New Learning Paradigm for Foundation Model-Based Remote-Sensing Change Detection",
        "abstract": "Change detection (CD) is a critical task to observe and analyze dynamic processes of land cover. Although numerous deep-learning (DL)-based CD models have performed excellently, their further performance improvements are constrained by the limited knowledge extracted from the given labeled data. On the other hand, the foundation models that emerged recently contain a huge amount of knowledge by scaling up across data modalities and proxy tasks. In this article, we propose a bi-temporal adapter network (BAN), which is a universal foundation model-based CD adaptation framework aiming to extract the knowledge of foundation models for CD. The proposed BAN contains three parts, that is, the frozen foundation model (e.g., CLIP), bi-temporal adapter branch (Bi-TAB), and bridging modules between them. Specifically, BAN extracts general features through a frozen foundation model, which are then selected, aligned, and injected into Bi-TAB via the bridging modules. Bi-TAB is designed as a model-agnostic concept to extract task/domain-specific features, which can be either an existing arbitrary CD model or some hand-crafted stacked blocks. Beyond current customized models, BAN is the first extensive attempt to adapt the foundation model to the CD task. Experimental results show the effectiveness of our BAN in improving the performance of existing CD methods (e.g., up to 4.08% IoU improvement) with only a few additional learnable parameters. More importantly, these successful practices show us the potential of foundation models for remote-sensing (RS) CD. The code is available at https://github.com/likyoo/BAN and will be supported in our Open-CD.",
        "authors": [
            "Kaiyu Li",
            "Xiangyong Cao",
            "Deyu Meng"
        ],
        "citations": 32,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology",
        "abstract": "Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the ex-isting MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foun-dation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R2T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R2T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R2T can introduce more signifi-cant performance improvements to various MIL models; 3) R2T-MIL, as an R2T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at: https://github.com/DearCaat/RRT-MIL.",
        "authors": [
            "Wenhao Tang",
            "Fengtao Zhou",
            "Shengyue Huang",
            "Xiang Zhu",
            "Yi Zhang",
            "Bo Liu"
        ],
        "citations": 12,
        "references": 53,
        "year": 2024
    },
    {
        "title": "Foundation Model Transparency Reports",
        "abstract": "Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g. the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House.",
        "authors": [
            "Rishi Bommasani",
            "Kevin Klyman",
            "Shayne Longpre",
            "Betty Xiong",
            "Sayash Kapoor",
            "Nestor Maslej",
            "Arvind Narayanan",
            "Percy Liang"
        ],
        "citations": 8,
        "references": 73,
        "year": 2024
    },
    {
        "title": "RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation",
        "abstract": "Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1~5 demonstrations, and effectively handles complex, dexterous tasks. We refer to https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.",
        "authors": [
            "Songming Liu",
            "Lingxuan Wu",
            "Bangguo Li",
            "Hengkai Tan",
            "Huayu Chen",
            "Zhengyi Wang",
            "Ke Xu",
            "Hang Su",
            "Jun Zhu"
        ],
        "citations": 8,
        "references": 107,
        "year": 2024
    },
    {
        "title": "EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging",
        "abstract": "Artificial intelligence (AI) is vital in ophthalmology, tackling tasks like diagnosis, classification, and visual question answering (VQA). However, existing AI models in this domain often require extensive annotation and are task-specific, limiting their clinical utility. While recent developments have brought about foundation models for ophthalmology, they are limited by the need to train separate weights for each imaging modality, preventing a comprehensive representation of multi-modal features. This highlights the need for versatile foundation models capable of handling various tasks and modalities in ophthalmology. To address this gap, we present EyeFound, a multimodal foundation model for ophthalmic images. Unlike existing models, EyeFound learns generalizable representations from unlabeled multimodal retinal images, enabling efficient model adaptation across multiple applications. Trained on 2.78 million images from 227 hospitals across 11 ophthalmic modalities, EyeFound facilitates generalist representations and diverse multimodal downstream tasks, even for detecting challenging rare diseases. It outperforms previous work RETFound in diagnosing eye diseases, predicting systemic disease incidents, and zero-shot multimodal VQA. EyeFound provides a generalizable solution to improve model performance and lessen the annotation burden on experts, facilitating widespread clinical AI applications for retinal imaging.",
        "authors": [
            "Danli Shi",
            "Weiyi Zhang",
            "Xiaolan Chen",
            "Yexin Liu",
            "Jianchen Yang",
            "Siyu Huang",
            "Y. Tham",
            "Yingfeng Zheng",
            "M. He"
        ],
        "citations": 8,
        "references": 17,
        "year": 2024
    },
    {
        "title": "ECG-FM: An Open Electrocardiogram Foundation Model",
        "abstract": "The electrocardiogram (ECG) is a ubiquitous diagnostic test. Conventional task-specific ECG analysis models require large numbers of expensive ECG annotations or associated labels to train. Transfer learning techniques have been shown to improve generalization and reduce reliance on labeled data. We present ECG-FM, an open foundation model for ECG analysis, and conduct a comprehensive study performed on a dataset of 1.66 million ECGs sourced from both publicly available and private institutional sources. ECG-FM adopts a transformer-based architecture and is pretrained on 2.5 million samples using ECG-specific augmentations and contrastive learning, as well as a continuous signal masking objective. Our transparent evaluation includes a diverse range of downstream tasks, where we predict ECG interpretation labels, reduced left ventricular ejection fraction, and abnormal cardiac troponin. Affirming ECG-FM's effectiveness as a foundation model, we demonstrate how its command of contextual information results in strong performance, rich pretrained embeddings, and reliable interpretability. Due to a lack of open-weight practices, we highlight how ECG analysis is lagging behind other medical machine learning subfields in terms of foundation model adoption. Our code is available at https://github.com/bowang-lab/ECG-FM/.",
        "authors": [
            "Kaden McKeen",
            "Laura Oliva",
            "Sameer Masood",
            "Augustin Toma",
            "Barry Rubin",
            "Bo Wang"
        ],
        "citations": 10,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Playing to Vision Foundation Model's Strengths in Stereo Matching",
        "abstract": "Stereo matching has become a key technique for 3D environment perception in intelligent vehicles. For a considerable time, convolutional neural networks (CNNs) have remained the mainstream choice for feature extraction in this domain. Nonetheless, there is a growing consensus that the existing paradigm should evolve towards vision foundation models (VFM), particularly those developed based on vision Transformers (ViTs) and pre-trained through self-supervision on extensive, unlabeled datasets. While VFMs are adept at extracting informative, general-purpose visual features, specifically for dense prediction tasks, their performance often lacks in geometric vision tasks. This study serves as the first exploration of a viable approach for adapting VFMs to stereo matching. Our ViT adapter, referred to as ViTAS, is constructed upon three types of modules: spatial differentiation, patch attention fusion, and cross-attention. The first module initializes feature pyramids, while the latter two aggregate stereo and multi-scale contextual information into fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost volume-based stereo matching back-end processes, achieves the top rank on the KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by approximately 7.9% in terms of the percentage of error pixels, with a tolerance of 3 pixels. Additional experiments across diverse scenarios further demonstrate its superior generalizability compared to all other state-of-the-art approaches. We believe this new paradigm will pave the way for the next generation of stereo matching networks.",
        "authors": [
            "Chuangwei Liu",
            "Qijun Chen",
            "Rui Fan"
        ],
        "citations": 7,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Nicheformer: a foundation model for single-cell and spatial omics",
        "abstract": "Tissue makeup relies fundamentally on the cellular microenvironment. Spatial single-cell genomics allows probing the underlying cellular interactions in an unbiased, scalable fashion. To learn a unified cell representation that accounts for local dependencies in the cellular microenvironment, we propose Nicheformer, a transformer-based foundation model that combines human and mouse dissociated single-cell and targeted spatial transcriptomics data. Pretrained on over 57 million dissociated and 53 million spatially resolved cells across 73 tissues on cellular reconstruction, the model is fine-tuned on spatial tasks for spatial omics data to decode spatially resolved cellular information. Nicheformer excels in linear-probing and fine-tuning scenarios for a novel set of downstream tasks, in particular spatial composition prediction and spatial label prediction. We further show that existing foundation models trained on dissociated single-cell data alone are not capable of recapitulating the spatial complexity of cells in their microenvironments, indicating that multiscale models are required to understand complex local dependencies at scale. Nicheformer enables the prediction of the spatial context of dissociated cells, allowing the transfer of rich spatial information to scRNA-seq datasets. Overall, Nicheformer sets the stage for the next generation of machine-learning models in spatial single-cell analysis. Extended Abstract Tissue makeup and the corresponding orchestration of vital biological activities, ranging from development and differentiation to immune response and regeneration, rely fundamentally on the cellular microenvironment and the interactions between cells. Spatial single-cell genomics allows probing such interactions in an unbiased and, increasingly, scalable fashion. To learn a unified cell representation that accounts for local dependencies in the cellular microenvironment and the underlying cell interactions, we propose to generalize recent foundation modeling approaches for disassociated single-cell transcriptomics to the spatial omics setting. Our model, Nicheformer, is a transformer-based foundation model that combines human and mouse dissociated single-cell and targeted spatial transcriptomics data to learn a cellular representation useful for a large variety of downstream tasks. Nicheformer is pretrained on over 57 million dissociated and 53 million spatially resolved cells across 73 tissues from both human and mouse. Subsequently, the model is fine-tuned on spatial tasks for spatial omics data to decode spatially resolved cellular information. We demonstrate the usefulness of Nicheformer in both linear-probing as well as fine-tuning scenarios on a novel set of spatially-relevant downstream tasks such as spatial density prediction or niche and region label prediction. In particular, we show that Nicheformer enables the prediction of the spatial context of dissociated cells, allowing the transfer of rich spatial information to scRNA-seq datasets. We define a series of novel spatial prediction problems and observe consistent top performance of Nicheformer, demonstrating the advantage of the improved model capacity of the underlying transformer. Additionally, we benchmarked Nicheformer in these tasks against scGPT1, Geneformer2, scVI3 and PCA and show that the Nicheformer architecture excels in these tasks. Altogether, our large-scale resource of more than 110 million cells in a partial spatial context, together with the set of novel spatial learning tasks and the Nicheformer model itself, will pave the way for the next generation of machine-learning models for spatial single-cell analysis.",
        "authors": [
            "A. C. Schaar",
            "Alejandro Tejada-Lapuerta",
            "G. Palla",
            "Robert M Gutgesell",
            "Lennard Halle",
            "M. Minaeva",
            "L. Vornholz",
            "L. Dony",
            "Francesca Drummer",
            "Mojtaba Bahrami",
            "F. Theis"
        ],
        "citations": 7,
        "references": 111,
        "year": 2024
    },
    {
        "title": "CosmicMan: A Text-to-Image Foundation Model for Humans",
        "abstract": "We present CosmicMan, a text-to-image foundation model specialized for generating high-fidelity human images. Unlike current general-purpose foundation models that are stuck in the dilemma of inferior quality and text-image misalignment for humans, CosmicMan enables generating photo-realistic human images with meticulous appearance, reasonable structure, and precise text-image alignment with detailed dense descriptions. At the heart of CosmicMan's success are the new reflections and perspectives on data and models: (1) We found that data quality and a scalable data production flow are essential for the final results from trained models. Hence, we propose a new data production paradigm, Annotate Anyone, which serves as a perpetual data flywheel to produce high-quality data with accurate yet cost-effective annotations over time. Based on this, we constructed a large-scale dataset, CosmicMan-HQ 1.0, with 6 Million high-quality real-world human images in a mean resolution of $1488 \\times 1255$, and attached with precise text annotations deriving from 115 Million attributes in diverse granularities. (2) We argue that a text-to-image foundation model specialized for humans must be pragmatic – easy to integrate into down-streaming tasks while effective in producing high-quality human images. Hence, we propose to model the relationship between dense text descriptions and image pixels in a decomposed manner, and present Decomposed-Attention-Refocusing (Daring) training framework. It seamlessly decomposes the cross-attention features in existing text-to-image diffusion model, and enforces attention refocusing without adding extra modules. Through Daring, we show that explicitly discretizing continuous text space into several basic groups that align with human body structure is the key to tackling the misalignment problem in a breeze. Project page: https://cosmicman-cvpr2024.github.io/.",
        "authors": [
            "Shikai Li",
            "Jianglin Fu",
            "Kaiyuan Liu",
            "Wentao Wang",
            "Kwan-Yee Lin",
            "Wayne Wu"
        ],
        "citations": 7,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Towards Vision-Language Geo-Foundation Model: A Survey",
        "abstract": "Vision-Language Foundation Models (VLFMs) have made remarkable progress on various multimodal tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding. However, most methods rely on training with general image datasets, and the lack of geospatial data leads to poor performance on earth observation. Numerous geospatial image-text pair datasets and VLFMs fine-tuned on them have been proposed recently. These new approaches aim to leverage large-scale, multimodal geospatial data to build versatile intelligent models with diverse geo-perceptive capabilities, which we refer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper thoroughly reviews VLGFMs, summarizing and analyzing recent developments in the field. In particular, we introduce the background and motivation behind the rise of VLGFMs, highlighting their unique research significance. Then, we systematically summarize the core technologies employed in VLGFMs, including data construction, model architectures, and applications of various multimodal geospatial tasks. Finally, we conclude with insights, issues, and discussions regarding future research directions. To the best of our knowledge, this is the first comprehensive literature review of VLGFMs. We keep tracing related works at https://github.com/zytx121/Awesome-VLGFM.",
        "authors": [
            "Yue Zhou",
            "Litong Feng",
            "Yiping Ke",
            "Xue Jiang",
            "Junchi Yan",
            "Xue Yang",
            "Wayne Zhang"
        ],
        "citations": 7,
        "references": 106,
        "year": 2024
    },
    {
        "title": "Federated Adaptation for Foundation Model-based Recommendations",
        "abstract": "With the recent success of large language models, particularly foundation models with generalization abilities, applying foundation models for recommendations becomes a new paradigm to improve existing recommendation systems. It becomes a new open challenge to enable the foundation model to capture user preference changes in a timely manner with reasonable communication and computation costs while preserving privacy. This paper proposes a novel federated adaptation mechanism to enhance the foundation model-based recommendation system in a privacy-preserving manner. Specifically, each client will learn a lightweight personalized adapter using its private data. The adapter then collaborates with pre-trained foundation models to provide recommendation service efficiently with fine-grained manners. Importantly, users' private behavioral data remains secure as it is not shared with the server. This data localization-based privacy preservation is embodied via the federated learning framework. The model can ensure that shared knowledge is incorporated into all adapters while simultaneously preserving each user's personal preferences. Experimental results on four benchmark datasets demonstrate our method's superior performance. The code is available.",
        "authors": [
            "Chunxu Zhang",
            "Guodong Long",
            "Hongkuan Guo",
            "Xiao Fang",
            "Yang Song",
            "Zhaojie Liu",
            "Guorui Zhou",
            "Zijian Zhang",
            "Yang Liu",
            "Bo Yang"
        ],
        "citations": 6,
        "references": 38,
        "year": 2024
    },
    {
        "title": "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability",
        "abstract": "Causal abstraction provides a theoretical foundation for mechanistic interpretability, the field concerned with providing intelligible algorithms that are faithful simplifications of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for the core concepts of modular features, polysemantic neurons, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methodologies in the common language of causal abstraction, namely activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, differential binary masking, distributed alignment search, and activation steering.",
        "authors": [
            "Atticus Geiger",
            "D. Ibeling",
            "Amir Zur",
            "Maheep Chaudhary",
            "Sonakshi Chauhan",
            "Jing Huang",
            "Aryaman Arora",
            "Zhengxuan Wu",
            "Noah D. Goodman",
            "Christopher Potts",
            "Thomas F. Icard"
        ],
        "citations": 38,
        "references": 136,
        "year": 2023
    },
    {
        "title": "Towards a Visual-Language Foundation Model for Computational Pathology",
        "abstract": "The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-the-art performance on histology image classification, segmentation, captioning, text-to-image and image-to-text retrieval. CONCH represents a substantial leap over concurrent visual-language pretrained systems for histopathology, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.",
        "authors": [
            "Ming Y. Lu",
            "Bowen Chen",
            "Drew F. K. Williamson",
            "Richard J. Chen",
            "Ivy Liang",
            "Tong Ding",
            "Guillaume Jaume",
            "Igor Odintsov",
            "Andrew Zhang",
            "L. Le",
            "G. Gerber",
            "A. Parwani",
            "Faisal Mahmood"
        ],
        "citations": 39,
        "references": 105,
        "year": 2023
    },
    {
        "title": "Text-to-Image Diffusion Models are Zero-Shot Classifiers",
        "abstract": "The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data. However, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks. We investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers. The key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood. We apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image classification datasets. Additionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot. Although generative pre-training is prevalent in NLP, visual foundation models often use other methods such as contrastive learning. Based on our findings, we argue that generative pre-training should be explored as a compelling alternative for vision-language tasks.",
        "authors": [
            "Kevin Clark",
            "P. Jaini"
        ],
        "citations": 81,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Debiasing Vision-Language Models via Biased Prompts",
        "abstract": "Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.",
        "authors": [
            "Ching-Yao Chuang",
            "Varun Jampani",
            "Yuanzhen Li",
            "A. Torralba",
            "S. Jegelka"
        ],
        "citations": 76,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Ignore Previous Prompt: Attack Techniques For Language Models",
        "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.",
        "authors": [
            "Fábio Perez",
            "Ian Ribeiro"
        ],
        "citations": 318,
        "references": 42,
        "year": 2022
    },
    {
        "title": "A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4",
        "abstract": "Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI GPT-3 model, and the popularity of LLMs is increasing exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GPT-3 family large language models.",
        "authors": [
            "Katikapalli Subramanyam Kalyan"
        ],
        "citations": 140,
        "references": 146,
        "year": 2023
    },
    {
        "title": "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models",
        "abstract": "The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better visual dog classifier by reading about dogs and listening to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning, adapters, and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification. Project site at link.",
        "authors": [
            "Zhiqiu Lin",
            "Samuel Yu",
            "Zhiyi Kuang",
            "Deepak Pathak",
            "Deva Ramana"
        ],
        "citations": 71,
        "references": 114,
        "year": 2023
    },
    {
        "title": "ChemDFM: A Large Language Foundation Model for Chemistry",
        "abstract": "Artificial intelligence (AI) has played an increasingly important role in chemical research. However, most models currently used in chemistry are specialist models that require training and tuning for specific tasks. A more generic and efficient solution would be an AI model that could address many tasks and support free-form dialogue in the broad field of chemistry. In its utmost form, such a generalist AI chemist could be referred to as Chemical General Intelligence. Large language models (LLMs) have recently logged tremendous success in the general domain of natural language processing, showing emerging task generalization and free-form dialogue capabilities. However, domain knowledge of chemistry is largely missing when training general-domain LLMs. The lack of such knowledge greatly hinders the performance of generalist LLMs in the field of chemistry. To this end, we develop ChemDFM, a pioneering LLM for chemistry trained on 34B tokens from chemical literature and textbooks, and fine-tuned using 2.7M instructions. As a result, it can understand and reason with chemical knowledge in free-form dialogue. Quantitative evaluations show that ChemDFM significantly surpasses most representative open-source LLMs. It outperforms GPT-4 on a great portion of chemical tasks, despite the substantial size difference. We have open-sourced the inference codes, evaluation datasets, and model weights of ChemDFM on Huggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B).",
        "authors": [
            "Zihan Zhao",
            "Da Ma",
            "Lu Chen",
            "Liangtai Sun",
            "Zihao Li",
            "Hongshen Xu",
            "Zichen Zhu",
            "Su Zhu",
            "Shuai Fan",
            "Guodong Shen",
            "Xin Chen",
            "Kai Yu"
        ],
        "citations": 11,
        "references": 82,
        "year": 2024
    },
    {
        "title": "A Survey on Model Compression for Large Language Models",
        "abstract": "Abstract Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.",
        "authors": [
            "Xunyu Zhu",
            "Jian Li",
            "Yong Liu",
            "Can Ma",
            "Weiping Wang"
        ],
        "citations": 132,
        "references": 201,
        "year": 2023
    },
    {
        "title": "PDEformer: Towards a Foundation Model for One-Dimensional Partial Differential Equations",
        "abstract": "This paper introduces PDEformer, a neural solver for partial differential equations (PDEs) capable of simultaneously addressing various types of PDEs. We propose to represent the PDE in the form of a computational graph, facilitating the seamless integration of both symbolic and numerical information inherent in a PDE. A graph Transformer and an implicit neural representation (INR) are employed to generate mesh-free predicted solutions. Following pretraining on data exhibiting a certain level of diversity, our model achieves zero-shot accuracies on benchmark datasets that is comparable to those of specifically trained expert models. Additionally, PDEformer demonstrates promising results in the inverse problem of PDE coefficient recovery.",
        "authors": [
            "Zhanhong Ye",
            "Xiang Huang",
            "Leheng Chen",
            "Hongsheng Liu",
            "Zidong Wang",
            "Bin Dong"
        ],
        "citations": 11,
        "references": 29,
        "year": 2024
    },
    {
        "title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography",
        "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current shortage of both general and specialized radiologists, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies while simultaneously using the images to extract novel physiological insights. Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs) that utilize both the image and the corresponding textual radiology reports. However, current medical VLMs are generally limited to 2D images and short reports. To overcome these shortcomings for abdominal CT interpretation, we introduce Merlin - a 3D VLM that leverages both structured electronic health records (EHR) and unstructured radiology reports for pretraining without requiring additional manual annotations. We train Merlin using a high-quality clinical dataset of paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens) for training. We comprehensively evaluate Merlin on 6 task types and 752 individual tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year chronic disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs). We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines. We derive data scaling laws to empirically assess training data needs for requisite downstream task performance. Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU. This computationally efficient design can help democratize foundation model training, especially for health systems with compute constraints. We plan to release our trained models, code, and dataset, pending manual removal of all protected health information.",
        "authors": [
            "Louis Blankemeier",
            "Joseph Paul Cohen",
            "Ashwin Kumar",
            "Dave Van Veen",
            "Syed Jamal Safdar Gardezi",
            "Magdalini Paschali",
            "Zhihong Chen",
            "Jean-Benoit Delbrouck",
            "E. Reis",
            "C. Truyts",
            "Christian Bluethgen",
            "Malte E. K. Jensen",
            "Sophie Ostmeier",
            "Maya Varma",
            "Jeya Maria Jose Valanarasu",
            "Zhongnan Fang",
            "Zepeng Huo",
            "Zaid Nabulsi",
            "Diego Ardila",
            "Wei-Hung Weng",
            "Edson Amaro Junior",
            "Neera Ahuja",
            "J. Fries",
            "Nigam H. Shah",
            "Andrew Johnston",
            "Robert D. Boutin",
            "Andrew Wentland",
            "C. Langlotz",
            "Jason Hom",
            "S. Gatidis",
            "Akshay S. Chaudhari"
        ],
        "citations": 11,
        "references": 94,
        "year": 2024
    },
    {
        "title": "Large AI Models in Health Informatics: Applications, Challenges, and the Future",
        "abstract": "Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which can reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A prime example is ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our lives. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multi-modal data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents a comprehensive review of large AI models, from background to their applications. We identify seven key sectors in which large AI models are applicable and might have substantial influence, including: 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics.",
        "authors": [
            "Jianing Qiu",
            "Lin Li",
            "Jiankai Sun",
            "Jiachuan Peng",
            "Peilun Shi",
            "Rui Zhang",
            "Yinzhao Dong",
            "K. Lam",
            "F. P. Lo",
            "Bo Xiao",
            "Wu Yuan",
            "Dong Xu",
            "Benny P. L. Lo"
        ],
        "citations": 95,
        "references": 346,
        "year": 2023
    },
    {
        "title": "A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering",
        "abstract": "The Segment Anything Model (SAM), developed by Meta AI Research, represents a significant breakthrough in computer vision, offering a robust framework for image and video segmentation. This survey provides a comprehensive exploration of the SAM family, including SAM and SAM 2, highlighting their advancements in granularity and contextual understanding. Our study demonstrates SAM's versatility across a wide range of applications while identifying areas where improvements are needed, particularly in scenarios requiring high granularity and in the absence of explicit prompts. By mapping the evolution and capabilities of SAM models, we offer insights into their strengths and limitations and suggest future research directions, including domain-specific adaptations and enhanced memory and propagation mechanisms. We believe that this survey comprehensively covers the breadth of SAM's applications and challenges, setting the stage for ongoing advancements in segmentation technology.",
        "authors": [
            "Chaoning Zhang",
            "Sheng Zheng",
            "Chenghao Li",
            "Yu Qiao",
            "Taegoo Kang",
            "Xinru Shan",
            "Chenshuang Zhang",
            "Caiyan Qin",
            "François Rameau",
            "S. Bae",
            "Choong-Seon Hong"
        ],
        "citations": 54,
        "references": 307,
        "year": 2023
    },
    {
        "title": "Data Portraits: Recording Foundation Model Training Data",
        "abstract": "Foundation models are trained on increasingly immense and opaque datasets. Even while these models are now key in AI system building, it can be difficult to answer the straightforward question: has the model already encountered a given example during training? We therefore propose a widespread adoption of Data Portraits: artifacts that record training data and allow for downstream inspection. First we outline the properties of such an artifact and discuss how existing solutions can be used to increase transparency. We then propose and implement a solution based on data sketching, stressing fast and space efficient querying. Using our tools, we document a popular language modeling corpus (The Pile) and a recently released code modeling dataset (The Stack). We show that our solution enables answering questions about test set leakage and model plagiarism. Our tool is lightweight and fast, costing only 3% of the dataset size in overhead. We release a live interface of our tools at https://dataportraits.org/ and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.",
        "authors": [
            "Marc Marone",
            "Benjamin Van Durme"
        ],
        "citations": 28,
        "references": 42,
        "year": 2023
    },
    {
        "title": "FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning",
        "abstract": "Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL.",
        "authors": [
            "Haokun Chen",
            "Yao Zhang",
            "Denis Krompass",
            "Jindong Gu",
            "Volker Tresp"
        ],
        "citations": 23,
        "references": 57,
        "year": 2023
    },
    {
        "title": "An RNA foundation model enables discovery of disease mechanisms and candidate therapeutics",
        "abstract": "Accurately modeling and predicting RNA biology has been a long-standing challenge, bearing significant clinical ramifications for variant interpretation and the formulation of tailored therapeutics. We describe a foundation model for RNA biology, “BigRNA”, which was trained on thousands of genome-matched datasets to predict tissue-specific RNA expression, splicing, microRNA sites, and RNA binding protein specificity from DNA sequence. Unlike approaches that are restricted to missense variants, BigRNA can identify pathogenic non-coding variant effects across diverse mechanisms, including polyadenylation, exon skipping and intron retention. BigRNA accurately predicted the effects of steric blocking oligonucleotides (SBOs) on increasing the expression of 4 out of 4 genes, and on splicing for 18 out of 18 exons across 14 genes, including those involved in Wilson disease and spinal muscular atrophy. We anticipate that BigRNA and foundation models like it will have widespread applications in the field of personalized RNA therapeutics.",
        "authors": [
            "Albi Celaj",
            "Alice Jiexin Gao",
            "Tammy T. Y. Lau",
            "Erle M. Holgersen",
            "Alston Lo",
            "Varun Lodaya",
            "Christopher B. Cole",
            "R. Denroche",
            "Carl Spickett",
            "Omar Wagih",
            "Pedro O. Pinheiro",
            "Parth Vora",
            "P. Mohammadi-Shemirani",
            "Steve Chan",
            "Zach Nussbaum",
            "Xi Zhang",
            "Helen Zhu",
            "Easwaran Ramamurthy",
            "Bhargav Kanuparthi",
            "Michael Iacocca",
            "Diane Ly",
            "Ken Kron",
            "Marta Verby",
            "Kahlin Cheung-Ong",
            "Zvi Shalev",
            "Brandon Vaz",
            "Sakshi Bhargava",
            "Farhan Yusuf",
            "Sharon Samuel",
            "Sabriyeh Alibai",
            "Zahra Baghestani",
            "Xinwen He",
            "Kirsten Krastel",
            "Oladipo Oladapo",
            "Amrudha Mohan",
            "Arathi Shanavas",
            "Magdalena Bugno",
            "Jovanka J Bogojeski",
            "F. Schmitges",
            "Carolyn Kim",
            "Solomon Grant",
            "Rachana Jayaraman",
            "Tehmina Masud",
            "A. Deshwar",
            "Shreshth Gandhi",
            "Brendan J. Frey"
        ],
        "citations": 23,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images",
        "abstract": "Generating differentially private (DP) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are only accessible via their inference APIs. However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider. In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its initial promise on synthetic images. Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods without any model training. For example, on CIFAR10 (with ImageNet as the public data), we achieve FID<= 7.9 with privacy cost {\\epsilon} = 0.67, significantly improving the previous SOTA from {\\epsilon} = 32. We further demonstrate the promise of applying PE on large foundation models such as Stable Diffusion to tackle challenging private datasets with a small number of high-resolution images. The code and data are released at https://github.com/microsoft/DPSDA.",
        "authors": [
            "Zi-Han Lin",
            "Sivakanth Gopi",
            "Janardhan Kulkarni",
            "Harsha Nori",
            "S. Yekhanin"
        ],
        "citations": 21,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
        "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
        "authors": [
            "Andy Zeng",
            "Adrian S. Wong",
            "Stefan Welker",
            "K. Choromanski",
            "F. Tombari",
            "Aveek Purohit",
            "M. Ryoo",
            "Vikas Sindhwani",
            "Johnny Lee",
            "Vincent Vanhoucke",
            "Peter R. Florence"
        ],
        "citations": 524,
        "references": 165,
        "year": 2022
    },
    {
        "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
        "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Jiaying Lu",
            "Chengyuan Deng",
            "Can Zheng",
            "Junxiang Wang",
            "Tanmoy Chowdhury",
            "Yun-Qing Li",
            "Hejie Cui",
            "Xuchao Zhang",
            "Tian-yu Zhao",
            "Amit Panalkar",
            "Wei Cheng",
            "Haoyu Wang",
            "Yanchi Liu",
            "Zhengzhang Chen",
            "Haifeng Chen",
            "Chris White",
            "Quanquan Gu",
            "Jian Pei",
            "Carl Yang",
            "Liang Zhao"
        ],
        "citations": 100,
        "references": 331,
        "year": 2023
    },
    {
        "title": "Arc2Face: A Foundation Model for ID-Consistent Human Faces",
        "abstract": null,
        "authors": [
            "Foivos Paraperas Papantoniou",
            "Alexandros Lattas",
            "Stylianos Moschoglou",
            "Jiankang Deng",
            "Bernhard Kainz",
            "S. Zafeiriou"
        ],
        "citations": 9,
        "references": 100,
        "year": 2024
    },
    {
        "title": "An Interactive Agent Foundation Model",
        "abstract": "The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems.",
        "authors": [
            "Zane Durante",
            "Bidipta Sarkar",
            "Ran Gong",
            "Rohan Taori",
            "Yusuke Noda",
            "Paul Tang",
            "Ehsan Adeli",
            "S. Lakshmikanth",
            "Kevin Schulman",
            "Arnold Milstein",
            "D. Terzopoulos",
            "Ade Famoti",
            "N. Kuno",
            "A. Llorens",
            "Hoi Vo",
            "Katsushi Ikeuchi",
            "Fei-Fei Li",
            "Jianfeng Gao",
            "Naoki Wake",
            "Qiuyuan Huang"
        ],
        "citations": 9,
        "references": 60,
        "year": 2024
    },
    {
        "title": "PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm",
        "abstract": "In contrast to numerous NLP and 2D vision foundational models, learning a 3D foundational model poses considerably greater challenges. This is primarily due to the inherent data variability and diversity of downstream tasks. In this paper, we introduce a novel universal 3D pre-training framework designed to facilitate the acquisition of efficient 3D representation, thereby establishing a pathway to 3D foundational models. Considering that informative 3D features should encode rich geometry and appearance cues that can be utilized to render realistic images, we propose to learn 3D representations by differentiable neural rendering. We train a 3D backbone with a devised volumetric neural renderer by comparing the rendered with the real images. Notably, our approach seamlessly integrates the learned 3D encoder into various downstream tasks. These tasks encompass not only high-level challenges such as 3D detection and segmentation but also low-level objectives like 3D reconstruction and image synthesis, spanning both indoor and outdoor scenarios. Besides, we also illustrate the capability of pre-training a 2D backbone using the proposed methodology, surpassing conventional pre-training methods by a large margin. For the first time, PonderV2 achieves state-of-the-art performance on 11 indoor and outdoor benchmarks, implying its effectiveness. Code and models are available at https://github.com/OpenGVLab/PonderV2.",
        "authors": [
            "Haoyi Zhu",
            "Honghui Yang",
            "Xiaoyang Wu",
            "Di Huang",
            "Sha Zhang",
            "Xianglong He",
            "Tong He",
            "Hengshuang Zhao",
            "Chunhua Shen",
            "Yu Qiao",
            "Wanli Ouyang"
        ],
        "citations": 31,
        "references": 141,
        "year": 2023
    },
    {
        "title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
        "abstract": "Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization.We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2",
        "authors": [
            "Cheng Deng",
            "Tianhang Zhang",
            "Zhongmou He",
            "Yi Xu",
            "Qiyuan Chen",
            "Yuanyuan Shi",
            "Le Zhou",
            "Luoyi Fu",
            "Weinan Zhang",
            "Xinbing Wang",
            "Cheng Zhou",
            "Zhouhan Lin",
            "Junxian He"
        ],
        "citations": 43,
        "references": 59,
        "year": 2023
    },
    {
        "title": "RanPAC: Random Projections and Pre-trained Models for Continual Learning",
        "abstract": "Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning. Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 20% and 62% on seven class-incremental benchmarks, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast CL has not hitherto been fully tapped. Code is at github.com/RanPAC/RanPAC.",
        "authors": [
            "M. McDonnell",
            "Dong Gong",
            "Amin Parvaneh",
            "Ehsan Abbasnejad",
            "A. Hengel"
        ],
        "citations": 56,
        "references": 66,
        "year": 2023
    },
    {
        "title": "PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology",
        "abstract": "As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We open-source our dataset, as well as a comprehensive toolkit for extensive pathology data collection and preprocessing at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology.",
        "authors": [
            "Yuxuan Sun",
            "Chenglu Zhu",
            "S. Zheng",
            "Kai Zhang",
            "Zhongyi Shui",
            "Xiaoxuan Yu",
            "Yizhi Zhao",
            "Honglin Li",
            "Yunlong Zhang",
            "Ruojia Zhao",
            "Xinheng Lyu",
            "Lin Yang"
        ],
        "citations": 28,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI",
        "abstract": null,
        "authors": [
            "Mahyar Abbasian",
            "Elahe Khatibi",
            "Iman Azimi",
            "David Oniani",
            "Zahra Shakeri Hossein Abad",
            "Alexander Thieme",
            "Ram Sriram",
            "Zhongqi Yang",
            "Yanshan Wang",
            "Bryant Lin",
            "Olivier Gevaert",
            "Li-Jia Li",
            "Ramesh C. Jain",
            "Amir M. Rahmani"
        ],
        "citations": 41,
        "references": 151,
        "year": 2023
    },
    {
        "title": "Human Preference Score: Better Aligning Text-to-image Models with Human Preference",
        "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human preferences. Our experiments show that HPS outperforms CLIP in predicting human choices and has good generalization capability toward images generated from other models. By tuning Stable Diffusion with the guidance of HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/alignsd-web/.",
        "authors": [
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Feng Zhu",
            "Rui Zhao",
            "Hongsheng Li"
        ],
        "citations": 94,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Better Aligning Text-to-Image Models with Human Preference",
        "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classiﬁer with the collected dataset and derive a Human Preference Score (HPS) based on the classiﬁer. Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. By tuning Stable Diffusion with the guidance of the HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/align sd web/.",
        "authors": [
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Feng Zhu",
            "Rui Zhao",
            "Hongsheng Li"
        ],
        "citations": 87,
        "references": 42,
        "year": 2023
    },
    {
        "title": "A Recipe for Watermarking Diffusion Models",
        "abstract": "Diffusion models (DMs) have demonstrated advantageous potential on generative tasks. Widespread interest exists in incorporating DMs into downstream applications, such as producing or editing photorealistic images. However, practical deployment and unprecedented power of DMs raise legal issues, including copyright protection and monitoring of generated content. In this regard, watermarking has been a proven solution for copyright protection and content monitoring, but it is underexplored in the DMs literature. Specifically, DMs generate samples from longer tracks and may have newly designed multimodal structures, necessitating the modification of conventional watermarking pipelines. To this end, we conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe is straightforward but involves empirically ablated implementation details, providing a foundation for future research on watermarking DMs. The code is available at https://github.com/yunqing-me/WatermarkDM.",
        "authors": [
            "Yunqing Zhao",
            "Tianyu Pang",
            "Chao Du",
            "Xiao Yang",
            "Ngai-Man Cheung",
            "Min Lin"
        ],
        "citations": 86,
        "references": 80,
        "year": 2023
    },
    {
        "title": "A Real-world Dataset and Benchmark For Foundation Model Adaptation in Medical Image Classification",
        "abstract": null,
        "authors": [
            "Dequan Wang",
            "Xiaosong Wang",
            "Lilong Wang",
            "Mengzhang Li",
            "Q. Da",
            "Xiaoqiang Liu",
            "Xiangyu Gao",
            "Jun Shen",
            "Junjun He",
            "Tian Shen",
            "Qi Duan",
            "Jie Zhao",
            "Kang Li",
            "Y. Qiao",
            "Shaoting Zhang"
        ],
        "citations": 20,
        "references": 34,
        "year": 2023
    },
    {
        "title": "A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language",
        "abstract": "Although artificial intelligence (AI) has made significant progress in understanding molecules in a wide range of fields, existing models generally acquire the single cognitive ability from the single molecular modality. Since the hierarchy of molecular knowledge is profound, even humans learn from different modalities including both intuitive diagrams and professional texts to assist their understanding. Inspired by this, we propose a molecular multimodal foundation model which is pretrained from molecular graphs and their semantically related textual data (crawled from published Scientific Citation Index papers) via contrastive learning. This AI model represents a critical attempt that directly bridges molecular graphs and natural language. Importantly, through capturing the specific and complementary information of the two modalities, our proposed model can better grasp molecular expertise. Experimental results show that our model not only exhibits promising performance in cross-modal tasks such as cross-modal retrieval and molecule caption, but also enhances molecular property prediction and possesses capability to generate meaningful molecular graphs from natural language descriptions. We believe that our model would have a broad impact on AI-empowered fields across disciplines such as biology, chemistry, materials, environment, and medicine, among others.",
        "authors": [
            "Bing Su",
            "Dazhao Du",
            "Zhao-Qing Yang",
            "Yujie Zhou",
            "Jiangmeng Li",
            "Anyi Rao",
            "Haoran Sun",
            "Zhiwu Lu",
            "Ji-rong Wen"
        ],
        "citations": 94,
        "references": 80,
        "year": 2022
    },
    {
        "title": "DiffusionSat: A Generative Foundation Model for Satellite Imagery",
        "abstract": "Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets. As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, superresolution given multi-spectral inputs and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale generative foundation model for satellite imagery. The project website can be found here: https://samar-khanna.github.io/DiffusionSat/",
        "authors": [
            "Samar Khanna",
            "Patrick Liu",
            "Linqi Zhou",
            "Chenlin Meng",
            "Robin Rombach",
            "Marshall Burke",
            "David B. Lobell",
            "Stefano Ermon"
        ],
        "citations": 34,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
        "abstract": "Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",
        "authors": [
            "Luke Bailey",
            "Euan Ong",
            "Stuart Russell",
            "Scott Emmons"
        ],
        "citations": 54,
        "references": 53,
        "year": 2023
    },
    {
        "title": "AnyPredict: Foundation Model for Tabular Prediction",
        "abstract": "Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated signiﬁcant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains. This paper proposes a method for building training data at scale for tabular prediction foundation models ( AnyPredict ) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a “learn, annotate, and audit” pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular dataset in the domain without ﬁne-tuning, resulting in signiﬁcant improvements over supervised baselines: it reaches an average ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3 trial outcome prediction datasets, respectively. In addition, AnyPredict exhibits impressive zero-shot performances: it outperforms supervised XGBoost models by 8 . 9% and 17 . 2% on average in two prediction tasks, respectively.",
        "authors": [
            "Zifeng Wang",
            "Chufan Gao",
            "Cao Xiao",
            "Jimeng Sun"
        ],
        "citations": 20,
        "references": 45,
        "year": 2023
    },
    {
        "title": "The New Zealand Community Fault Model – version 1.0: an improved geological foundation for seismic hazard modelling",
        "abstract": "ABSTRACT The New Zealand Community Fault Model (NZ CFM) is a publicly available representation of New Zealand fault zones that have the potential to produce damaging earthquakes. Compiled through collaborative engagement between New Zealand earthquake-science experts, this first edition (version 1.0) of the NZ CFM builds upon previous compilations of earthquake-source active fault models with the addition of new and modified information. Developed primarily to support an update of the New Zealand National Seismic Hazard Model, the NZ CFM comprises two principal components. The first dataset is a two-dimensional map representation of the surface traces of 880 generalised fault zones. Each fault zone is assigned specific geometric and kinematic attributes, including uncertainties, supplemented with a subjective quality ranking focused primarily on the confidence in assigned slip rates. The second component is a three-dimensional representation of the fault zones as triangulated mesh surfaces that are projected down-dip from the two-dimensional mapped traces to a geophysically-defined maximum fault rupture depth. This article summarises the compilation and parameterisation of the NZ CFM, along with background on its relation to predecessor datasets, and forward applications to probabilistic seismic hazard assessment and physics-based earthquake models currently being developed for Aotearoa New Zealand.",
        "authors": [
            "H. Seebeck",
            "R. Dissen",
            "N. Litchfield",
            "P. Barnes",
            "A. Nicol",
            "R. Langridge",
            "D. Barrell",
            "P. Villamor",
            "S. Ellis",
            "M. Rattenbury",
            "Stephen Bannister",
            "M. Gerstenberger",
            "F. Ghisetti",
            "R. Sutherland",
            "H. Hirschberg",
            "J. Fraser",
            "S. Nodder",
            "M. Stirling",
            "Jade Humphrey",
            "K. J. Bland",
            "A. Howell",
            "J. Mountjoy",
            "V. Moon",
            "T. Stahl",
            "F. Spinardi",
            "D. Townsend",
            "K. Clark",
            "I. Hamling",
            "S. Cox",
            "W. D. de Lange",
            "P. Wopereis",
            "M. Johnston",
            "R. Morgenstern",
            "G. Coffey",
            "J. Eccles",
            "T. Little",
            "B. Fry",
            "J. Griffin",
            "John Townend",
            "N. Mortimer",
            "S. Alcaraz",
            "C. Massiot",
            "J. Rowland",
            "James Muirhead",
            "P. Upton",
            "Julie Lee"
        ],
        "citations": 35,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text",
        "abstract": "Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.",
        "authors": [
            "Sebastian Gehrmann",
            "Elizabeth Clark",
            "Thibault Sellam"
        ],
        "citations": 169,
        "references": 304,
        "year": 2022
    },
    {
        "title": "AM-RADIO: Agglomerative Vision Foundation Model Reduce All Domains Into One",
        "abstract": "A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are trained with distinct objectives, exhibiting unique characteristics for various downstream tasks. We find that despite their conceptual differences, these models can be effectively merged into a unified model through multi-teacher distillation. We name this approach AM-RADIO (Agglomerative Model - Reduce All Domains Into One). This integrative approach not only surpasses the performance of individual teacher models but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel-level understanding, and open vocabulary segmentation capabilities. Additionally, in pursuit of the most hardware-efficient backbone, we evaluated numerous architectures in our multi-teacher distillation pipeline using the same training recipe. This led to the development of a novel architecture (E-RADIO) that exceeds the performance of its predecessors and is at least 6x faster than the teacher models at matched resolution. Our comprehensive benchmarking process covers downstream tasks including ImageNet classification, semantic segmentation linear probing, COCO object detection and integration into LLaVa-1.5. Code: https://github.com/NVlabs/RADIO.",
        "authors": [
            "Michael Ranzinger",
            "Greg Heinrich",
            "Jan Kautz",
            "Pavlo Molchanov"
        ],
        "citations": 14,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence",
        "abstract": "Nowadays, foundation models become one of fundamental infrastructures in artificial intelligence, paving ways to the general intelligence. However, the reality presents two urgent challenges: existing foundation models are dominated by the English-language community; users are often given limited resources and thus cannot always use foundation models. To support the development of the Chinese-language community, we introduce an open-source project, called Fengshenbang, which leads by the research center for Cognitive Computing and Natural Language (CCNL). Our project has comprehensive capabilities, including large pre-trained models, user-friendly APIs, benchmarks, datasets, and others. We wrap all these in three sub-projects: the Fengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An open-source roadmap, Fengshenbang, aims to re-evaluate the open-source community of Chinese pre-trained large-scale models, prompting the development of the entire Chinese large-scale model community. We also want to build a user-centered open-source ecosystem to allow individuals to access the desired models to match their computing resources. Furthermore, we invite companies, colleges, and research institutions to collaborate with us to build the large-scale open-source model-based ecosystem. We hope that this project will be the foundation of Chinese cognitive intelligence.",
        "authors": [
            "Junjie Wang",
            "Yuxiang Zhang",
            "Lin Zhang",
            "Ping Yang",
            "Xinyu Gao",
            "Ziwei Wu",
            "Xiaoqun Dong",
            "Junqing He",
            "Jianheng Zhuo",
            "Qi Yang",
            "Yongfeng Huang",
            "Xiayu Li",
            "Yan-Ze Wu",
            "Junyu Lu",
            "Xinyu Zhu",
            "Weifeng Chen",
            "Ting-Ting Han",
            "Kunhao Pan",
            "Rui Wang",
            "Hao Wang",
            "Xiaojun Wu",
            "Zhong Zeng",
            "Chong-An Chen",
            "Ruyi Gan",
            "Jiaxing Zhang"
        ],
        "citations": 99,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Changen2: Multi-Temporal Remote Sensing Generative Change Foundation Model",
        "abstract": "Our understanding of the temporal dynamics of the Earth's surface has been significantly advanced by deep vision models, which often require a massive amount of labeled multi-temporal images for training. However, collecting, preprocessing, and annotating multi-temporal remote sensing images at scale is non-trivial since it is expensive and knowledge-intensive. In this paper, we present scalable multi-temporal change data generators based on generative models, which are cheap and automatic, alleviating these data problems. Our main idea is to simulate a stochastic change process over time. We describe the stochastic change process as a probabilistic graphical model, namely the generative probabilistic change model (GPCM), which factorizes the complex simulation problem into two more tractable sub-problems, i.e., condition-level change event simulation and image-level semantic change synthesis. To solve these two problems, we present Changen2, a GPCM implemented with a resolution-scalable diffusion transformer which can generate time series of remote sensing images and corresponding semantic and change labels from labeled and even unlabeled single-temporal images. Changen2 is a “generative change foundation model” that can be trained at scale via self-supervision, and is capable of producing change supervisory signals from unlabeled single-temporal images. Unlike existing “foundation models”, our generative change foundation model synthesizes change data to train task-specific foundation models for change detection. The resulting model possesses inherent zero-shot change detection capabilities and excellent transferability. Comprehensive experiments suggest Changen2 has superior spatiotemporal scalability in data generation, e.g., Changen2 model trained on 256<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zheng-ieq1-3475824.gif\"/></alternatives></inline-formula> pixel single-temporal images can yield time series of any length and resolutions of 1,024<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zheng-ieq2-3475824.gif\"/></alternatives></inline-formula> pixels. Changen2 pre-trained models exhibit superior zero-shot performance (narrowing the performance gap to 3% on LEVIR-CD and approximately 10% on both S2Looking and SECOND, compared to fully supervised counterpart) and transferability across multiple types of change tasks, including ordinary and off-nadir building change, land-use/land-cover change, and disaster assessment.",
        "authors": [
            "Zhuo Zheng",
            "Stefano Ermon",
            "Dongjun Kim",
            "Liangpei Zhang",
            "Yanfei Zhong"
        ],
        "citations": 8,
        "references": 83,
        "year": 2024
    },
    {
        "title": "An Improved Traditional Chinese Evaluation Suite for Foundation Model",
        "abstract": "We present TMMLU+, a new benchmark designed for Traditional Chinese language understanding. TMMLU+ is a multi-choice question-answering dataset with 66 subjects from elementary to professional level. It is six times larger and boasts a more balanced subject distribution than its predecessor, Taiwan Massive Multitask Language Understanding (TMMLU). We also benchmark closed-source models and 26 open-weight Chinese large language models (LLMs) of parameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal that (1.) Traditional Chinese models still trail behind their Simplified Chinese counterparts, highlighting a need for more focused advancements in LLMs catering to Traditional Chinese. (2.) Current LLMs still fall short of human performance in average scores, indicating a potential need for future research to delve deeper into social science and humanities subjects. (3.) Among all the tokenization compression metrics examined, we identify that only the fertility score uniquely demonstrates strong correlations with our benchmark results. We foresee that TMMLU+ will pinpoint areas for future model improvement, thereby narrowing the gap between machine and human linguistic capabilities and supporting researchers in developing Traditional Chinese LLMs. Our dataset, along with the benchmark source code, is accessible at huggingface.co/datasets/ikala/tmmluplus.",
        "authors": [
            "Zhi Rui Tam",
            "Ya-Ting Pai",
            "Yen-Wei Lee",
            "Sega Cheng",
            "Hong-Han Shuai"
        ],
        "citations": 8,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models",
        "abstract": "Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffusion process, which we show to be more robust in comparing distributions with misaligned supports. We also reveal non-trivial connections of our method to existing works such as DreamFusion, and generative adversarial training. To demonstrate the effectiveness and universality of Diff-Instruct, we consider two scenarios: distilling pre-trained diffusion models and refining existing GAN models. The experiments on distilling pre-trained diffusion models show that Diff-Instruct results in state-of-the-art single-step diffusion-based models. The experiments on refining GAN models show that the Diff-Instruct can consistently improve the pre-trained generators of GAN models across various settings.",
        "authors": [
            "Weijian Luo",
            "Tianyang Hu",
            "Shifeng Zhang",
            "Jiacheng Sun",
            "Zhenguo Li",
            "Zhihua Zhang"
        ],
        "citations": 79,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts",
        "abstract": "The fine-tuning paradigm in addressing long-tail learning tasks has sparked significant interest since the emergence of foundation models. Nonetheless, how fine-tuning impacts performance in long-tail learning was not explicitly quantified. In this paper, we disclose that heavy fine-tuning may even lead to non-negligible performance deterioration on tail classes, and lightweight fine-tuning is more effective. The reason is attributed to inconsistent class conditions caused by heavy fine-tuning. With the observation above, we develop a low-complexity and accurate long-tail learning algorithms LIFT with the goal of facilitating fast prediction and compact models by adaptive lightweight fine-tuning. Experiments clearly verify that both the training time and the learned parameters are significantly reduced with more accurate predictive performance compared with state-of-the-art approaches. The implementation code is available at https://github.com/shijxcs/LIFT.",
        "authors": [
            "Jiang-Xin Shi",
            "Tong Wei",
            "Zhi Zhou",
            "Xin-Yan Han",
            "Jiejing Shao",
            "Yu-Feng Li"
        ],
        "citations": 14,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Open-Vocabulary Segmentation",
        "abstract": null,
        "authors": [
            "Laurynas Karazija",
            "Iro Laina",
            "A. Vedaldi",
            "C. Rupprecht"
        ],
        "citations": 47,
        "references": 85,
        "year": 2023
    },
    {
        "title": "PMC-LLaMA: Towards Building Open-source Language Models for Medicine",
        "abstract": "Recently, Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight PMCLLaMA, which consists of only 13 billion parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.",
        "authors": [
            "Chaoyi Wu",
            "Xiaoman Zhang",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "citations": 65,
        "references": 29,
        "year": 2023
    },
    {
        "title": "In-Context Learning Unlocked for Diffusion Models",
        "abstract": "We present Prompt Diffusion, a framework for enabling in-context learning in diffusion-based generative models. Given a pair of task-specific example images, such as depth from/to image and scribble from/to image, and a text guidance, our model automatically understands the underlying task and performs the same task on a new query image following the text guidance. To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input. The diffusion model is trained jointly over six different tasks using these prompts. The resulting Prompt Diffusion model is the first diffusion-based vision-language foundation model capable of in-context learning. It demonstrates high-quality in-context generation on the trained tasks and generalizes effectively to new, unseen vision tasks with their respective prompts. Our model also shows compelling text-guided image editing results. Our framework aims to facilitate research into in-context learning for computer vision. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Prompt-Diffusion.",
        "authors": [
            "Zhendong Wang",
            "Yifan Jiang",
            "Yadong Lu",
            "Yelong Shen",
            "Pengcheng He",
            "Weizhu Chen",
            "Zhangyang Wang",
            "Mingyuan Zhou"
        ],
        "citations": 63,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
        "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
        "authors": [
            "Xingxuan Li",
            "Ruochen Zhao",
            "Yew Ken Chia",
            "Bosheng Ding",
            "Shafiq R. Joty",
            "Soujanya Poria",
            "Lidong Bing"
        ],
        "citations": 68,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Neural general circulation models for weather and climate",
        "abstract": null,
        "authors": [
            "Dmitrii Kochkov",
            "J. Yuval",
            "I. Langmore",
            "Peter C. Norgaard",
            "Jamie A. Smith",
            "G. Mooers",
            "Milan Klöwer",
            "James Lottes",
            "S. Rasp",
            "P. Düben",
            "Samuel Hatfield",
            "Peter W. Battaglia",
            "Alvaro Sanchez-Gonzalez",
            "Matthew Willson",
            "Michael P. Brenner",
            "Stephan Hoyer"
        ],
        "citations": 68,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Foundation Model Assisted Weakly Supervised Semantic Segmentation",
        "abstract": "This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAMbased seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014. Our code will be released upon acceptance.",
        "authors": [
            "Xiaobo Yang",
            "Xiaojin Gong"
        ],
        "citations": 16,
        "references": 51,
        "year": 2023
    },
    {
        "title": "EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge",
        "abstract": "Deep Learning (DL) models have been widely deployed on IoT devices with the help of advancements in DL algorithms and chips. However, the limited resources of edge devices make these on-device DL models hard to be generalizable to diverse environments and tasks. Although the recently emerged foundation models (FMs) show impressive generalization power, how to effectively leverage the rich knowledge of FMs on resource-limited edge devices is still not explored. In this paper, we propose EdgeFM, a novel edge-cloud cooperative system with open-set recognition capability. EdgeFM selectively uploads unlabeled data to query the FM on the cloud and customizes the specific knowledge and architectures for edge models. Meanwhile, EdgeFM conducts dynamic model switching at run-time taking into account both data uncertainty and dynamic network variations, which ensures the accuracy always close to the original FM. We implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on three public datasets and two self-collected datasets. Results show that EdgeFM can reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy increase compared with the baseline.",
        "authors": [
            "Bufang Yang",
            "Lixing He",
            "Neiwen Ling",
            "Zhenyu Yan",
            "Guoliang Xing",
            "Xian Shuai",
            "Xiaozhe Ren",
            "Xin Jiang"
        ],
        "citations": 15,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Joint Foundation Model Caching and Inference of Generative AI Services for Edge Intelligence",
        "abstract": "With the rapid development of artificial general intelligence (AGI), various multimedia services based on pretrained foundation models (PFMs) need to be effectively deployed. With edge servers that have cloud-level computing power, edge intelligence can extend the capabilities of AGI to mobile edge networks. However, compared with cloud data centers, resource-limited edge servers can only cache and execute a small number of PFMs, which typically consist of billions of parameters and require intensive computing power and GPU memory during inference. To address this challenge, in this paper, we propose a joint foundation model caching and inference framework that aims to balance the tradeoff among inference latency, accuracy, and resource consumption by managing cached PFMs and user requests efficiently during the provisioning of generative AI services. Specifically, considering the in-context learning ability of PFMs, a new metric named the Age of Context (AoC), is proposed to model the freshness and relevance between examples in past demonstrations and current service requests. Based on the AoC, we propose a least context caching algorithm to manage cached PFMs at edge servers with historical prompts and inference results. The numerical results demonstrate that the proposed algorithm can reduce system costs compared with existing baselines by effectively utilizing contextual information.",
        "authors": [
            "Minrui Xu",
            "D. Niyato",
            "Hongliang Zhang",
            "Jiawen Kang",
            "Zehui Xiong",
            "Shiwen Mao",
            "Zhu Han"
        ],
        "citations": 16,
        "references": 14,
        "year": 2023
    },
    {
        "title": "Fine-tuning can cripple your foundation model; preserving features may be the solution",
        "abstract": "Pre-trained foundation models, due to their enormous capacity and exposure to vast amounts of data during pre-training, are known to have learned plenty of real-world concepts. An important step in making these pre-trained models effective on downstream tasks is to fine-tune them on related datasets. While various fine-tuning methods have been devised and have been shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is an undesirable effect of fine-tuning as a substantial amount of resources was used to learn these pre-trained concepts in the first place. We call this phenomenon ''concept forgetting'' and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we propose a simple fix to this problem by designing a new fine-tuning method called $\\textit{LDIFS}$ (short for $\\ell_2$ distance in feature space) that, while learning new concepts related to the downstream task, allows a model to preserve its pre-trained knowledge as well. Through extensive experiments on 10 fine-tuning tasks we show that $\\textit{LDIFS}$ significantly reduces concept forgetting. Additionally, we show that LDIFS is highly effective in performing continual fine-tuning on a sequence of tasks as well, in comparison with both fine-tuning as well as continual learning baselines.",
        "authors": [
            "Jishnu Mukhoti",
            "Y. Gal",
            "Philip H. S. Torr",
            "P. Dokania"
        ],
        "citations": 17,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Time Travelling Pixels: Bitemporal Features Integration with Foundation Model for Remote Sensing Image Change Detection",
        "abstract": "Change detection, a prominent research area in remote sensing, is pivotal in observing and analyzing surface transformations. Despite significant advancements achieved through deep learning-based methods, executing high-precision change detection in spatiotemporally complex remote sensing scenarios still presents a substantial challenge. The recent emergence of foundation models, with their powerful universality and generalization capabilities, offers potential solutions. However, bridging the gap of data and tasks remains a significant obstacle. In this paper, we introduce Time Travelling Pixels (TTP), a novel approach that integrates the latent knowledge of the SAM foundation model into change detection. TTP can effectively address the domain shift in general knowledge transfer and the challenge of expressing homogeneous and heterogeneous characteristics of multi-temporal images. The state-of-the-art results obtained on the LEVIR-CD underscore the efficacy of the TTP. The code has been made publicly available at https://github.com/KyanChen/TTP.",
        "authors": [
            "Keyan Chen",
            "Chengyang Liu",
            "Wenyuan Li",
            "Zili Liu",
            "Hao Chen",
            "Haotian Zhang",
            "Zhengxia Zou",
            "Z. Shi"
        ],
        "citations": 15,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Building Transportation Foundation Model via Generative Graph Transformer",
        "abstract": "In recent years, researchers have made notable advancements in various disciplines using large-scale foundation models. However, foundation models in the transportation system have not received adequate attention. To address this gap, we propose the Generative Graph Transformer (GGT), a transportation foundation model (TFM) that leverages graph structure and dynamic graph generation algorithms. The primary objective of our TFM is to capture participant behavior and interaction in the transportation system, at various scales, and establish a large-scale neural network to comprehend the entire system. The GGT-based TFM can overcom challenges of structural complexity and model accuracy in conventional traffic models. This approach holds promise for addressing complex traffic issues by utilizing up-to-date real traffic data. To demonstrate the capabilities of GGT, a simulation experiment was conducted.",
        "authors": [
            "Xuhong Wang",
            "Ding Wang",
            "Liang Chen",
            "Yilun Lin"
        ],
        "citations": 15,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Insect-Foundation: A Foundation Model and Large-Scale 1M Dataset for Visual Insect Understanding",
        "abstract": "In precision agriculture, the detection and recognition of insects play an essential role in the ability of crops to grow healthy and produce a high-quality yield. The current machine vision model requires a large volume of data to achieve high performance. However, there are approximately 5.5 million different insect species in the world. None of the existing insect datasets can cover even a fraction of them due to varying geographic locations and acquisition costs. In this paper, we introduce a novel “Insect-1M” dataset, a game-changing resource poised to revolutionize insect-related foundation model training. Covering a vast spectrum of insect species, our dataset, including 1 million images with dense identification labels of taxonomy hierarchy and insect descriptions, offers a panoramic view of entomology, enabling foundation models to comprehend visual and semantic information about insects like never before. Then, to efficiently establish an Insect Foundation Model, we develop a micro-feature self-supervised learning method with a Patch-wise Relevant Attention mechanism capable of discerning the subtle differences among insect images. In addition, we introduce Description Consistency loss to improve micro-feature modeling via insect descriptions. Through our experiments, we illustrate the effectiveness of our proposed approach in insect modeling and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks. Our Insect Foundation Model and Dataset promise to empower the next generation of insect-related vision models, bringing them closer to the ultimate goal of precision agriculture.",
        "authors": [
            "Hoang-Quan Nguyen",
            "Thanh-Dat Truong",
            "Xuan-Bac Nguyen",
            "Ashley Dowling",
            "Xin Li",
            "Khoa Luu"
        ],
        "citations": 15,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Assessment of a new GeoAI foundation model for flood inundation mapping",
        "abstract": "Vision foundation models are a new frontier in Geospatial Artificial Intelligence (GeoAI), an interdisciplinary research area that applies and extends AI for geospatial problem solving and geographic knowledge discovery, because of their potential to enable powerful image analysis by learning and extracting important image features from vast amounts of geospatial data. This paper evaluates the performance of the first-of-its-kind geospatial foundation model, IBM-NASA's Prithvi, to support a crucial geospatial analysis task: flood inundation mapping. This model is compared with convolutional neural network and vision transformer-based architectures in terms of mapping accuracy for flooded areas. A benchmark dataset, Sen1Floods11, is used in the experiments, and the models' predictability, generalizability, and transferability are evaluated based on both a test dataset and a dataset that is completely unseen by the model. Results show the good transferability of the Prithvi model, highlighting its performance advantages in segmenting flooded areas in previously unseen regions. The findings also indicate areas for improvement for the Prithvi model in terms of adopting multi-scale representation learning, developing more end-to-end pipelines for high-level image analysis tasks, and offering more flexibility in terms of input data bands.",
        "authors": [
            "Wenwen Li",
            "Hyunho Lee",
            "Sizhe Wang",
            "Chia-Yu Hsu",
            "S. Arundel"
        ],
        "citations": 15,
        "references": 39,
        "year": 2023
    },
    {
        "title": "GenePT: A Simple But Effective Foundation Model for Genes and Cells Built From ChatGPT",
        "abstract": "There has been significant recent progress in leveraging large-scale gene expression data to develop foundation models for single-cell biology. Models such as Geneformer and scGPT implicitly learn gene and cellular functions from the gene expression profiles of millions of cells, which requires extensive data curation and resource-intensive training. Here we explore a much simpler alternative by leveraging ChatGPT embeddings of genes based on literature. Our proposal, GenePT, uses NCBI text descriptions of individual genes with GPT-3.5 to generate gene embeddings. From there, GenePT generates single-cell embeddings in two ways: (i) by averaging the gene embeddings, weighted by each gene’s expression level; or (ii) by creating a sentence embedding for each cell, using gene names ordered by the expression level. Without the need for dataset curation and additional pretraining, GenePT is efficient and easy to use. On many downstream tasks used to evaluate recent single-cell foundation models — e.g., classifying gene properties and cell types — GenePT achieves comparable, and often better, performance than Geneformer and other models. GenePT demonstrates that large language model embedding of literature is a simple and effective path for biological foundation models.",
        "authors": [
            "Yiqun T. Chen",
            "James Zou"
        ],
        "citations": 14,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Enhancing representation in radiography-reports foundation model: a granular alignment algorithm using masked contrastive learning",
        "abstract": null,
        "authors": [
            "Weijian Huang",
            "Cheng Li",
            "Hao Yang",
            "Jiarun Liu",
            "Shanshan Wang"
        ],
        "citations": 14,
        "references": 56,
        "year": 2023
    },
    {
        "title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
        "abstract": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",
        "authors": [
            "Xikun Zhang",
            "Antoine Bosselut",
            "Michihiro Yasunaga",
            "Hongyu Ren",
            "Percy Liang",
            "Christopher D. Manning",
            "J. Leskovec"
        ],
        "citations": 188,
        "references": 49,
        "year": 2022
    },
    {
        "title": "Transformer models: an introduction and catalog",
        "abstract": "In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).",
        "authors": [
            "X. Amatriain"
        ],
        "citations": 33,
        "references": 105,
        "year": 2023
    },
    {
        "title": "Matte Anything: Interactive Natural Image Matting with Segment Anything Models",
        "abstract": "Natural image matting algorithms aim to predict the transparency map (alpha-matte) with the trimap guidance. However, the production of trimap often requires significant labor, which limits the widespread application of matting algorithms on a large scale. To address the issue, we propose Matte Anything (MatAny), an interactive natural image matting model that could produce high-quality alpha-matte with various simple hints. The key insight of MatAny is to generate pseudo trimap automatically with contour and transparency prediction. In our work, we leverage vision foundation models to enhance the performance of natural image matting. Specifically, we use the segment anything model to predict high-quality contour with user interaction and an open-vocabulary detector to predict the transparency of any object. Subsequently, a pre-trained image matting model generates alpha mattes with pseudo trimaps. MatAny is the interactive matting algorithm with the most supported interaction methods and the best performance to date. It consists of orthogonal vision models without any additional training. We evaluate the performance of MatAny against several current image matting algorithms. MatAny has 58.3% improvement on MSE and 40.6% improvement on SAD compared to the previous image matting methods with simple guidance, achieving new state-of-the-art (SOTA) performance. The source codes and pre-trained models are available at https://github.com/hustvl/Matte-Anything.",
        "authors": [
            "J. Yao",
            "Xinggang Wang",
            "Lang Ye",
            "Wenyu Liu"
        ],
        "citations": 31,
        "references": 54,
        "year": 2023
    },
    {
        "title": "BAVS: Bootstrapping Audio-Visual Segmentation by Integrating Foundation Knowledge",
        "abstract": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate sounding sources by predicting pixel-wise maps. Previous methods assume that each sound component in an audio signal always has a visual counterpart in the image. However, this assumption overlooks that off-screen sounds and background noise often contaminate the audio recordings in real-world scenarios. They impose significant challenges on building a consistent semantic mapping between audio and visual signals for AVS models and thus impede precise sound localization. In this work, we propose a two-stage bootstrapping audio-visual segmentation framework by incorporating multi-modal foundation knowledge. In a nutshell, our BAVS is designed to eliminate the interference of background noise or off-screen sounds in segmentation by establishing the audio-visual correspondences in an explicit manner. In the first stage, we employ a segmentation model to localize potential sounding objects from visual data without being affected by contaminated audio signals. Meanwhile, we also utilize a foundation audio classification model to discern audio semantics. Considering the audio tags provided by the audio foundation model are noisy, associating object masks with audio tags is not trivial. Thus, in the second stage, we develop an audio-visual semantic integration strategy (AVIS) to localize the authentic-sounding objects. Here, we construct an audio-visual tree based on the hierarchical correspondence between sounds and object categories. We then examine the label concurrency between the localized objects and classified audio tags by tracing the audio-visual tree. With AVIS, we can effectively segment real-sounding objects. Extensive experiments demonstrate the superiority of our method on AVS datasets, particularly in scenarios involving background noise.",
        "authors": [
            "Chen Liu",
            "Peike Li",
            "Hu Zhang",
            "Lincheng Li",
            "Zi Huang",
            "Dadong Wang",
            "Xin Yu"
        ],
        "citations": 18,
        "references": 62,
        "year": 2023
    },
    {
        "title": "FoundPose: Unseen Object Pose Estimation with Foundation Features",
        "abstract": "We propose FoundPose, a model-based method for 6D pose estimation of unseen objects from a single RGB image. The method can quickly onboard new objects using their 3D models without requiring any object- or task-specific training. In contrast, existing methods typically pre-train on large-scale, task-specific datasets in order to generalize to new objects and to bridge the image-to-model domain gap. We demonstrate that such generalization capabilities can be observed in a recent vision foundation model trained in a self-supervised manner. Specifically, our method estimates the object pose from image-to-model 2D-3D correspondences, which are established by matching patch descriptors from the recent DINOv2 model between the image and pre-rendered object templates. We find that reliable correspondences can be established by kNN matching of patch descriptors from an intermediate DINOv2 layer. Such descriptors carry stronger positional information than descriptors from the last layer, and we show their importance when semantic information is ambiguous due to object symmetries or a lack of texture. To avoid establishing correspondences against all object templates, we develop an efficient template retrieval approach that integrates the patch descriptors into the bag-of-words representation and can promptly propose a handful of similarly looking templates. Additionally, we apply featuremetric alignment to compensate for discrepancies in the 2D-3D correspondences caused by coarse patch sampling. The resulting method noticeably outperforms existing RGB methods for refinement-free pose estimation on the standard BOP benchmark with seven diverse datasets and can be seamlessly combined with an existing render-and-compare refinement method to achieve RGB-only state-of-the-art results. Project page: evinpinar.github.io/foundpose.",
        "authors": [
            "Evin Pınar Örnek",
            "Yann Labb'e",
            "Bugra Tekin",
            "Lingni Ma",
            "Cem Keskin",
            "Christian Forster",
            "Tomás Hodan"
        ],
        "citations": 21,
        "references": 120,
        "year": 2023
    },
    {
        "title": "A Foundation Model for Music Informatics",
        "abstract": "This paper investigates foundation models tailored for music informatics, a domain currently challenged by the scarcity of labeled data and generalization issues. To this end, we conduct an in-depth comparative study among various foundation model variants, examining key determinants such as model architectures, tokenization methods, temporal resolution, data, and model scalability. This research aims to bridge the existing knowledge gap by elucidating how these individual factors contribute to the success of foundation models in music informatics. Employing a careful evaluation frame-work, we assess the performance of these models across diverse downstream tasks in music information retrieval, with a particular focus on frame-level and sequence-level classification. Our results reveal that our model demonstrates robust performance, surpassing existing models in specific key metrics. These findings contribute to the understanding of self-supervised learning in music informatics and pave the way for developing more effective and versatile foundation models in the field. A pretrained version of our model is publicly available to foster reproducibility and future research.",
        "authors": [
            "Minz Won",
            "Yun-Ning Hung",
            "Duc Le"
        ],
        "citations": 13,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Mobile Foundation Model as Firmware",
        "abstract": "In the current AI era, mobile devices such as smartphones are tasked with executing a myriad of deep neural networks (DNNs) locally. It presents a complex landscape, as these models are highly fragmented in terms of architecture, operators, and implementations. Such fragmentation poses significant challenges to the co-optimization of hardware, systems, and algorithms for efficient and scalable mobile AI. Inspired by the recent groundbreaking progress in large foundation models, this work introduces a novel paradigm for mobile AI, where mobile OS and hardware jointly manage a foundation model that is capable of serving a wide array of mobile AI tasks. This foundation model functions akin to firmware, unmodifiable by apps or the OS, exposed as a system service to Apps. They can invoke this foundation model through a small, offline fine-tuned \"adapter\" for various downstream tasks. We propose a tangible design of this vision called M4, and prototype it from publicly available pre-trained models. To assess its capability, we also build a comprehensive benchmark consisting of 38 mobile AI tasks and 50 datasets, spanning 5 multimodal inputs. Extensive experiments demonstrate M4's remarkable results: it achieves comparable accuracy in 85% of tasks, offers enhanced scalability regarding storage and memory, and has much simpler operations. In broader terms, this work paves a new way towards efficient and scalable mobile AI in the post-LLM era.",
        "authors": [
            "Jinliang Yuan",
            "Chenchen Yang",
            "Dongqi Cai",
            "Shihe Wang",
            "Xin Yuan",
            "Zeling Zhang",
            "Xiang Li",
            "Di Zhang",
            "Hanzi Mei",
            "Xianqing Jia",
            "Shangguang Wang",
            "Mengwei Xu"
        ],
        "citations": 13,
        "references": 125,
        "year": 2023
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "abstract": "Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that TRICE can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools.",
        "authors": [
            "Shuofei Qiao",
            "Honghao Gui",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "citations": 38,
        "references": 71,
        "year": 2023
    },
    {
        "title": "An Unified Search and Recommendation Foundation Model for Cold-Start Scenario",
        "abstract": "In modern commercial search engines and recommendation systems, data from multiple domains is available to jointly train the multi-domain model. Traditional methods train multi-domain models in the multi-task setting, with shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of features, labels, and sample distributions of individual tasks. With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks. We propose a novel framework called S&R Multi-Domain Foundation, which uses LLM to extract domain invariant features, and Aspect Gating Fusion to merge the ID feature, domain invariant text features and task-specific heterogeneous sparse features to obtain the representations of query and item. Additionally, samples from multiple search and recommendation scenarios are trained jointly with Domain Adaptive Multi-Task module to obtain the multi-domain foundation model. We apply the S&R Multi-Domain foundation model to cold start scenarios in the pretrain-finetune manner, which achieves better performance than other SOTA transfer learning methods. The S&R Multi-Domain Foundation model has been successfully deployed in Alipay Mobile Application's online services, such as content query recommendation and service card recommendation, etc.",
        "authors": [
            "Yuqi Gong",
            "Xichen Ding",
            "Yehui Su",
            "Kaiming Shen",
            "Zhongyi Liu",
            "Guannan Zhang"
        ],
        "citations": 20,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Toward a Foundation Model for Time Series Data",
        "abstract": "A foundation model is a machine learning model trained on a large and diverse set of data, typically using self-supervised learning-based pre-training techniques, that can be adapted to various downstream tasks. However, current research on time series pre-training has predominantly focused on models trained exclusively on data from a single domain. As a result, these models possess domain-specific knowledge that may not be easily transferable to time series from other domains. In this paper, we aim to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains. To achieve this, we repurposed the publicly available UCR Archive and evaluated four existing self-supervised learning-based pre-training methods, along with a novel method, on the datasets. We tested these methods using four popular neural network architectures for time series to understand how the pre-training methods interact with different network designs. Our experimental results show that pre-training improves downstream classification tasks by enhancing the convergence of the fine-tuning process. Furthermore, we found that the proposed pre-training method, when combined with the Transformer, outperforms the alternatives. The proposed method outperforms or achieves equal performance compared to the second best method in ~93% of downstream tasks.",
        "authors": [
            "Chin-Chia Michael Yeh",
            "Xin Dai",
            "Huiyuan Chen",
            "Yan Zheng",
            "Yujie Fan",
            "Audrey Der",
            "Vivian Lai",
            "Zhongfang Zhuang",
            "Junpeng Wang",
            "Liang Wang",
            "Wei Zhang"
        ],
        "citations": 21,
        "references": 41,
        "year": 2023
    },
    {
        "title": "A Coupled Model for Dam Foundation Seepage Behavior Monitoring and Forecasting Based on Variational Mode Decomposition and Improved Temporal Convolutional Network",
        "abstract": "Grasping the change behavior of dam foundation seepage pressure is of great significance for ensuring the safety of concrete dams. Because of the environmental complexity of the dam location, the prototypical seepage pressure data are easy to be contaminated by noise, which brings challenges to accurate prediction. Traditional denoising methods will lose the detailed characteristics of the objects, resulting in prediction models with limited flexibility and prediction accuracy. To address these problems, the prototypical data with noise are denoised using the variational mode decomposition (VMD)-wavelet packet denoising method. Then, an improved temporal convolutional network (ITCN) model is built for dam foundation seepage pressure data prediction. A hysteresis experiment is carried out to optimize the model structure by correlating the receptive field size of the ITCN model with the hysteresis of the dam foundation seepage pressure. Finally, the optimal ITCN dam foundation seepage pressure prediction model of each measurement point is obtained after the training. Three state-of-the-art methods in dam seepage monitoring are used as benchmark methods to compare the prediction performance of the proposed method. Four evaluation indicators are introduced to quantitatively evaluate and compare the prediction performance of the proposed method. The experimental results prove that the proposed method achieves high prediction accuracy flexibility. The indicator values of the ITCN model are only 50%–90% of those of LSTM and RNN models and 15%–40% of those of the stepwise regression model, and the values are all small.",
        "authors": [
            "Yantao Zhu",
            "Zhiduan Zhang",
            "C. Gu",
            "Yangtao Li",
            "Kang Zhang",
            "Mingxia Xie"
        ],
        "citations": 20,
        "references": 40,
        "year": 2023
    },
    {
        "title": "An Overview on Language Models: Recent Developments and Outlook",
        "abstract": "Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner, while pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and fine-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era.",
        "authors": [
            "Chengwei Wei",
            "Yun Cheng Wang",
            "Bin Wang",
            "C.-C. Jay Kuo"
        ],
        "citations": 38,
        "references": 221,
        "year": 2023
    },
    {
        "title": "Retrieval-based Language Models and Applications",
        "abstract": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
        "authors": [
            "Akari Asai",
            "Sewon Min",
            "Zexuan Zhong",
            "Danqi Chen"
        ],
        "citations": 63,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Similarity of Neural Network Models: A Survey of Functional and Representational Measures",
        "abstract": "Measuring similarity of neural networks to understand and improve their behavior has become an issue of great importance and research interest. In this survey, we provide a comprehensive overview of two complementary perspectives of measuring neural network similarity: (i) representational similarity, which considers how activations of intermediate layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties of and relationships between these measures, and point to open research problems. We hope our work lays a foundation for more systematic research on the properties and applicability of similarity measures for neural network models.",
        "authors": [
            "Max Klabunde",
            "Tobias Schumacher",
            "M. Strohmaier",
            "Florian Lemmerich"
        ],
        "citations": 54,
        "references": 245,
        "year": 2023
    },
    {
        "title": "InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models",
        "abstract": "Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. However, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and lack of holistic evaluation. To address these challenges, we present InstructEval, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and training methods. Our findings reveal that the quality of instruction data is a crucial factor in scaling model performance. While open-source models demonstrate impressive writing abilities, there is substantial room for improvement in problem-solving and alignment.",
        "authors": [
            "Yew Ken Chia",
            "Pengfei Hong",
            "Lidong Bing",
            "Soujanya Poria"
        ],
        "citations": 53,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Having Beer after Prayer? Measuring Cultural Bias in Large Language Models",
        "abstract": "As the reach of large language models (LMs) expands globally, their ability to cater to diverse cultural contexts becomes crucial. Despite advancements in multilingual capabilities, models are not designed with appropriate cultural nuances. In this paper, we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. We introduce CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities spanning eight types that contrast Arab and Western cultures. CAMeL provides a foundation for measuring cultural biases in LMs through both extrinsic and intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance in Arabic of 16 different LMs on tasks such as story generation, NER, and sentiment analysis, where we find concerning cases of stereotyping and cultural unfairness. We further test their text-infilling performance, revealing the incapability of appropriate adaptation to Arab cultural contexts. Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment. We will make CAMeL publicly available at: https://github.com/tareknaous/camel",
        "authors": [
            "Tarek Naous",
            "Michael Joseph Ryan",
            "Wei Xu"
        ],
        "citations": 53,
        "references": 122,
        "year": 2023
    },
    {
        "title": "Prompt a Robot to Walk with Large Language Models",
        "abstract": "Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively generate low-level control commands for robots without task-specific fine-tuning. Experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can proficiently function as low-level feedback controllers for dynamic motion control even in high-dimensional robotic systems. The project website and source code can be found at: https://prompt2walk.github.io/ .",
        "authors": [
            "Yen-Jen Wang",
            "Bike Zhang",
            "Jianyu Chen",
            "K. Sreenath"
        ],
        "citations": 36,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Composite Backdoor Attacks Against Large Language Models",
        "abstract": "Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered Rate (FTR) below $2.06\\%$ and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.",
        "authors": [
            "Hai Huang",
            "Zhengyu Zhao",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "citations": 30,
        "references": 36,
        "year": 2023
    },
    {
        "title": "LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching",
        "abstract": "Obtaining large pre-trained models that can be fine-tuned to new tasks with limited annotated samples has remained an open challenge for medical imaging data. While pre-trained deep networks on ImageNet and vision-language foundation models trained on web-scale data are prevailing approaches, their effectiveness on medical tasks is limited due to the significant domain shift between natural and medical images. To bridge this gap, we introduce LVM-Med, the first family of deep networks trained on large-scale medical datasets. We have collected approximately 1.3 million medical images from 55 publicly available datasets, covering a large number of organs and modalities such as CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art self-supervised algorithms on this dataset and propose a novel self-supervised contrastive learning algorithm using a graph-matching formulation. The proposed approach makes three contributions: (i) it integrates prior pair-wise image similarity metrics based on local and global information; (ii) it captures the structural constraints of feature embeddings through a loss function constructed via a combinatorial graph-matching objective; and (iii) it can be trained efficiently end-to-end using modern gradient-estimation techniques for black-box solvers. We thoroughly evaluate the proposed LVM-Med on 15 downstream medical tasks ranging from segmentation and classification to object detection, and both for the in and out-of-distribution settings. LVM-Med empirically outperforms a number of state-of-the-art supervised, self-supervised, and foundation models. For challenging tasks such as Brain Tumor Classification or Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models trained on 1 billion masks by 6-7% while using only a ResNet-50.",
        "authors": [
            "D. M. Nguyen",
            "Hoang Nguyen",
            "N. T. Diep",
            "T. Pham",
            "T. Cao",
            "Binh Duc Nguyen",
            "P. Swoboda",
            "Nhat Ho",
            "Shadi Albarqouni",
            "P. Xie",
            "Daniel Sonntag",
            "Mathias Niepert"
        ],
        "citations": 29,
        "references": 138,
        "year": 2023
    },
    {
        "title": "Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision",
        "abstract": "Foundation models, large-scale, pre-trained deep-learning models adapted to a wide range of downstream tasks have gained significant interest lately in various deep-learning problems undergoing a paradigm shift with the rise of these models. Trained on large-scale dataset to bridge the gap between different modalities, foundation models facilitate contextual reasoning, generalization, and prompt capabilities at test time. The predictions of these models can be adjusted for new tasks by augmenting the model input with task-specific hints called prompts without requiring extensive labeled data and retraining. Capitalizing on the advances in computer vision, medical imaging has also marked a growing interest in these models. To assist researchers in navigating this direction, this survey intends to provide a comprehensive overview of foundation models in the domain of medical imaging. Specifically, we initiate our exploration by providing an exposition of the fundamental concepts forming the basis of foundation models. Subsequently, we offer a methodical taxonomy of foundation models within the medical domain, proposing a classification system primarily structured around training strategies, while also incorporating additional facets such as application domains, imaging modalities, specific organs of interest, and the algorithms integral to these models. Furthermore, we emphasize the practical use case of some selected approaches and then discuss the opportunities, applications, and future directions of these large-scale pre-trained models, for analyzing medical images. In the same vein, we address the prevailing challenges and research pathways associated with foundational models in medical imaging. These encompass the areas of interpretability, data management, computational requirements, and the nuanced issue of contextual comprehension.",
        "authors": [
            "Bobby Azad",
            "Reza Azad",
            "Sania Eskandari",
            "Afshin Bozorgpour",
            "A. Kazerouni",
            "I. Rekik",
            "D. Merhof"
        ],
        "citations": 37,
        "references": 92,
        "year": 2023
    },
    {
        "title": "FM-OV3D: Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection",
        "abstract": "The superior performances of pre-trained foundation models in various visual tasks underscore their potential to enhance the 2D models' open-vocabulary ability. Existing methods explore analogous applications in the 3D space. However, most of them only center around knowledge extraction from singular foundation models, which limits the open-vocabulary ability of 3D models. We hypothesize that leveraging complementary pre-trained knowledge from various foundation models can improve knowledge transfer from 2D pre-trained visual language models to the 3D space. In this work, we propose FM-OV3D, a method of Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection, which improves the open-vocabulary localization and recognition abilities of 3D model by blending knowledge from multiple pre-trained foundation models, achieving true open-vocabulary without facing constraints from original 3D datasets. Specifically, to learn the open-vocabulary 3D localization ability, we adopt the open-vocabulary localization knowledge of the Grounded-Segment-Anything model. For open-vocabulary 3D recognition ability, We leverage the knowledge of generative foundation models, including GPT-3 and Stable Diffusion models, and cross-modal discriminative models like CLIP. The experimental results on two popular benchmarks for open-vocabulary 3D object detection show that our model efficiently learns knowledge from multiple foundation models to enhance the open-vocabulary ability of the 3D model and successfully achieves state-of-the-art performance in open-vocabulary 3D object detection tasks. Code is released at https://github.com/dmzhang0425/FM-OV3D.git.",
        "authors": [
            "Dongmei Zhang",
            "Chang Li",
            "Ray Zhang",
            "Shenghao Xie",
            "Wei Xue",
            "Xiaodong Xie",
            "Shanghang Zhang"
        ],
        "citations": 10,
        "references": 37,
        "year": 2023
    },
    {
        "title": "LLark: A Multimodal Foundation Model for Music",
        "abstract": "Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLARK, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLARK, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model’s responses in captioning and reasoning tasks. LLARK is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https: //github.com/spotify-research/llark .",
        "authors": [
            "Josh Gardner",
            "Simon Durand",
            "Daniel Stoller",
            "Rachel M. Bittner"
        ],
        "citations": 21,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Consistency-guided Prompt Learning for Vision-Language Models",
        "abstract": "We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and output spaces. This facilitates more effective adaptation to downstream tasks in a few-shot learning setting. Experiments show that CoPrompt outperforms existing methods on a range of evaluation suites, including base-to-novel generalization, domain generalization, and cross-dataset evaluation. On generalization, CoPrompt improves the state-of-the-art on zero-shot tasks and the overall harmonic mean over 11 datasets. Detailed ablation studies show the effectiveness of each of the components in CoPrompt. We make our code available at https://github.com/ShuvenduRoy/CoPrompt.",
        "authors": [
            "Shuvendu Roy",
            "A. Etemad"
        ],
        "citations": 25,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Perspective: Advances, Challenges, and Insight for Predictive Coarse-Grained Models.",
        "abstract": "By averaging over atomic details, coarse-grained (CG) models provide profound computational and conceptual advantages for studying soft materials. In particular, bottom-up approaches develop CG models based upon information obtained from atomically detailed models. At least in principle, a bottom-up model can reproduce all the properties of an atomically detailed model that are observable at the resolution of the CG model. Historically, bottom-up approaches have accurately modeled the structure of liquids, polymers, and other amorphous soft materials, but have provided lower structural fidelity for more complex biomolecular systems. Moreover, they have also been plagued by unpredictable transferability and a poor description of thermodynamic properties. Fortunately, recent studies have reported dramatic advances in addressing these prior limitations. This Perspective reviews this remarkable progress, while focusing on its foundation in the basic theory of coarse-graining. In particular, we describe recent insights and advances for treating the CG mapping, for modeling many-body interactions, for addressing the state-point dependence of effective potentials, and even for reproducing atomic observables that are beyond the resolution of the CG model. We also outline outstanding challenges and promising directions in the field. We anticipate that the synthesis of rigorous theory and modern computational tools will result in practical bottom-up methods that not only are accurate and transferable but also provide predictive insight for complex systems.",
        "authors": [
            "W. Noid"
        ],
        "citations": 60,
        "references": 337,
        "year": 2023
    },
    {
        "title": "Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation",
        "abstract": "Modern incremental learning for semantic segmentation methods usually learn new categories based on dense annotations. Although achieve promising results, pixel-by-pixel labeling is costly and time-consuming. Weakly incremental learning for semantic segmentation (WILSS) is a novel and attractive task, which aims at learning to segment new classes from cheap and widely available image-level labels. Despite the comparable results, the image-level labels can not provide details to locate each segment, which limits the performance of WILSS. This inspires us to think how to improve and effectively utilize the supervision of new classes given image-level labels while avoiding forgetting old ones. In this work, we propose a novel and data-efficient frame-work for WILSS, named FMWISS. Specifically, we propose pre-training based co-segmentation to distill the knowledge of complementary foundation models for generating dense pseudo labels. We further optimize the noisy pseudo masks with a teacher-student architecture, where a plug-in teacher is optimized with a proposed dense contrastive loss. Moreover, we introduce memory-based copy-paste augmentation to improve the catastrophic forgetting problem of old classes. Extensive experiments on Pascal VOC and COCO datasets demonstrate the superior performance of our framework, e.g., FMWISS achieves 70.7% and 73.3% in the 15–5 VOC setting, outperforming the state-of-the-art method by 3.4% and 6.1%, respectively.",
        "authors": [
            "Chaohui Yu",
            "Qiang-feng Zhou",
            "Jingliang Li",
            "Jia-Chao Yuan",
            "Zhibin Wang",
            "Fan Wang"
        ],
        "citations": 9,
        "references": 55,
        "year": 2023
    },
    {
        "title": "ViP: A Differentially Private Foundation Model for Computer Vision",
        "abstract": "Artificial intelligence (AI) has seen a tremendous surge in capabilities thanks to the use of foundation models trained on internet-scale data. On the flip side, the uncurated nature of internet-scale data also poses significant privacy and legal risks, as they often contain personal information or copyrighted material that should not be trained on without permission. In this work, we propose as a mitigation measure a recipe to train foundation vision models with differential privacy (DP) guarantee. We identify masked autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and train ViP -- a Vision transformer with differential Privacy -- under a strict privacy budget of $\\epsilon=8$ on the LAION400M dataset. We evaluate the quality of representation learned by ViP using standard downstream vision tasks; in particular, ViP achieves a (non-private) linear probing accuracy of $55.7\\%$ on ImageNet, comparable to that of end-to-end trained AlexNet (trained and evaluated on ImageNet). Our result suggests that scaling to internet-scale data can be practical for private learning. Code is available at \\url{https://github.com/facebookresearch/ViP-MAE}.",
        "authors": [
            "Yaodong Yu",
            "Maziar Sanjabi",
            "Y. Ma",
            "Kamalika Chaudhuri",
            "Chuan Guo"
        ],
        "citations": 10,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Seismic Foundation Model (SFM): a next generation deep learning model in geophysics",
        "abstract": "While computer science has seen remarkable advancements in foundation models, they remain underexplored in geoscience. Addressing this gap, we introduce a workflow to develop geophysical foundation models, including data preparation, model pre-training, and adaption to downstream tasks. From 192 globally collected 3-D seismic volumes, we create a carefully curated dataset of 2,286,422 2-D seismic images. Fully using these unlabeled images, we employ the self-supervised learning to pre-train a Transformer-based Seismic Foundation Model (SFM) for producing all-purpose seismic features that work across various tasks and surveys. Through experiments on seismic facies classification, geobody identification, interpolation, denoising, and inversion, our pre-trained model demonstrates versatility, generalization, scalability, and superior performance over baseline models. In conclusion, we provide a foundation model and vast dataset to advance AI in geophysics, addressing challenges (poor generalization, lacking labels, and repetitive training for task-specified models) of applying AI in geophysics and paving the way for future innovations in geoscience.",
        "authors": [
            "Hanlin Sheng",
            "Xinming Wu",
            "Xu Si",
            "Jintao Li",
            "Sibo Zhang",
            "X. Duan"
        ],
        "citations": 10,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Towards Causal Foundation Model: on Duality between Causal Inference and Attention",
        "abstract": "Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for treatment effect estimations. We propose a novel, theoretically justified method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset methodologies. These results provide compelling evidence that our method has the potential to serve as a stepping stone for the development of causal foundation models.",
        "authors": [
            "Jiaqi Zhang",
            "Joel Jennings",
            "Cheng Zhang",
            "Chao Ma"
        ],
        "citations": 11,
        "references": 92,
        "year": 2023
    },
    {
        "title": "A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture",
        "abstract": "Large language model (LLM) based chatbots, such as ChatGPT, have attracted huge interest in foundation models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. However, the architecture design of foundation model based systems has not yet been systematically explored. There is limited understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and system design options. Our taxonomy comprises three categories: the pretraining and adaptation of foundation models, the architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy can serve as concrete guidance for designing foundation model based systems.CCS CONCEPTS•Software and its engineering → Software design engineering;•Computer systems organization → Architectures;•Computing methodologies → Artificial intelligence.",
        "authors": [
            "Qinghua Lu",
            "Liming Zhu",
            "Xiwei Xu",
            "Yue Liu",
            "Zhenchang Xing",
            "Jon Whittle"
        ],
        "citations": 10,
        "references": 25,
        "year": 2023
    },
    {
        "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
        "abstract": "Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their\"black box\"nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about \\textit{Large Language Models for Autonomous Driving (LLM4AD)}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.",
        "authors": [
            "Zhenjie Yang",
            "Xiaosong Jia",
            "Hongyang Li",
            "Junchi Yan"
        ],
        "citations": 57,
        "references": 162,
        "year": 2023
    },
    {
        "title": "Foundation Model for Material Science",
        "abstract": "Foundation models (FMs) are achieving remarkable successes to realize complex downstream tasks in domains including natural language and visions. In this paper, we propose building an FM for material science, which is trained with massive data across a wide variety of material domains and data modalities. Nowadays machine learning models play key roles in material discovery, particularly for property prediction and structure generation. However, those models have been independently developed to address only specific tasks without sharing more global knowledge. Development of an FM for material science will enable overarching modeling across material domains and data modalities by sharing their feature representations. We discuss fundamental challenges and required technologies to build an FM from the aspects of data preparation, model development, and downstream tasks.",
        "authors": [
            "Seiji Takeda",
            "Akihiro Kishimoto",
            "Lisa Hamada",
            "D. Nakano",
            "John Smith"
        ],
        "citations": 9,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Towards General Purpose Medical AI: Continual Learning Medical Foundation Model",
        "abstract": "Inevitable domain and task discrepancies in real-world scenarios can impair the generalization performance of the pre-trained deep models for medical data. Therefore, we audaciously propose that we should build a general-purpose medical AI system that can be seamlessly adapted to downstream domains/tasks. Since the domain/task adaption procedures usually involve additional labeling work for the target data, designing a data-efficient adaption algorithm is desired to save the cost of transferring the learned knowledge. Our recent work found that vision-language models (VLMs) are efficient learners with extraordinary cross-domain ability. Therefore, in this work, we further explore the possibility of leveraging pre-trained VLMs as medical foundation models for building general-purpose medical AI, where we thoroughly investigate three machine-learning paradigms, i.e., domain/task-specialized learning, joint learning, and continual learning, for training the VLMs and evaluate their generalization performance on cross-domain and cross-task test sets. To alleviate the catastrophic forgetting during sequential training, we employ rehearsal learning and receive a sharp boost in terms of generalization capability. In a nutshell, our empirical evidence suggests that continual learning may be a practical and efficient learning paradigm for the medical foundation model. And we hope researchers can use our empirical evidence as basement to further explore the path toward medical foundation model.",
        "authors": [
            "Huahui Yi",
            "Ziyuan Qin",
            "Qicheng Lao",
            "Wei Xu",
            "Zekun Jiang",
            "Dequan Wang",
            "Shaoting Zhang",
            "Kang Li"
        ],
        "citations": 9,
        "references": 41,
        "year": 2023
    },
    {
        "title": "A multi-center study on the adaptability of a shared foundation model for electronic health records",
        "abstract": null,
        "authors": [
            "L. Guo",
            "J. Fries",
            "E. Steinberg",
            "Scott L. Fleming",
            "Keith Morse",
            "Catherine Aftandilian",
            "J. Posada",
            "Nigam H. Shah",
            "L. Sung"
        ],
        "citations": 9,
        "references": 42,
        "year": 2023
    },
    {
        "title": "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation",
        "abstract": "Multimodal models trained on large natural image-text pair datasets have exhibited astounding abilities in generating high-quality images. Medical imaging data is fundamentally different to natural images, and the language used to succinctly capture relevant details in medical data uses a different, narrow but semantically rich, domain-specific vocabulary. Not surprisingly, multi-modal models trained on natural image-text pairs do not tend to generalize well to the medical domain. Developing generative imaging models faithfully representing medical concepts while providing compositional diversity could mitigate the existing paucity of high-quality, annotated medical imaging datasets. In this work, we develop a strategy to overcome the large natural-medical distributional shift by adapting a pre-trained latent diffusion model on a corpus of publicly available chest x-rays (CXR) and their corresponding radiology (text) reports. We investigate the model's ability to generate high-fidelity, diverse synthetic CXR conditioned on text prompts. We assess the model outputs quantitatively using image quality metrics, and evaluate image quality and text-image alignment by human domain experts. We present evidence that the resulting model (RoentGen) is able to create visually convincing, diverse synthetic CXR images, and that the output can be controlled to a new extent by using free-form text prompts including radiology-specific language. Fine-tuning this model on a fixed training set and using it as a data augmentation method, we measure a 5% improvement of a classifier trained jointly on synthetic and real images, and a 3% improvement when trained on a larger but purely synthetic training set. Finally, we observe that this fine-tuning distills in-domain knowledge in the text-encoder and can improve its representation capabilities of certain diseases like pneumothorax by 25%.",
        "authors": [
            "P. Chambon",
            "Christian Blüthgen",
            "Jean-Benoit Delbrouck",
            "Rogier van der Sluijs",
            "M. Polacin",
            "Juan Manuel Zambrano Chaves",
            "T. Abraham",
            "Shivanshu Purohit",
            "C. Langlotz",
            "Akshay Chaudhari"
        ],
        "citations": 85,
        "references": 61,
        "year": 2022
    },
    {
        "title": "Uncover This Tech Term: Foundation Model",
        "abstract": "The foundation model (FM) is a family of machine artificial intelligence (AI) models that are generally trained by self-supervised learning using a large volume of unannotated dataset and can be adapted to various downstream tasks [1]. The most well-known examples of FMs are large language models (LLMs), such as ChatGPT [2]. Similar to ChatGPT, LLMs typically consist of billions of parameters and are designed to perform various natural language tasks. An LLM is initially pretrained to predict next words that follow a given input text (referred to as ‘pretext task’), through which the LLM learns the semantics and structure of languages. With subsequent fine-tuning by human feedback, the LLM then acquires capabilities to generate natural and plausible responses to a wide range of queries (referred to as ‘downstream tasks’). Training for the pretext task is typically achieved through self-supervised learning, using a massive unannotated Uncover This Tech Term: Foundation Model Kyu-Hwan Jung Department of Medical Device Management and Research, Samsung Advanced Institute for Health Sciences and Technology, Sungkyunkwan University, Seoul, Republic of Korea Dataset Science Research Institute, Research Institute for Future Medicine, Samsung Medical Center, Seoul, Republic of Korea",
        "authors": [
            "K. Jung"
        ],
        "citations": 18,
        "references": 31,
        "year": 2023
    },
    {
        "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
        "abstract": "Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.",
        "authors": [
            "K. Lu",
            "Hongyi Yuan",
            "Zheng Yuan",
            "Runji Lin",
            "Junyang Lin",
            "Chuanqi Tan",
            "Chang Zhou"
        ],
        "citations": 48,
        "references": 43,
        "year": 2023
    },
    {
        "title": "When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities",
        "abstract": null,
        "authors": [
            "Jin Chen",
            "Zheng Liu",
            "Xunpeng Huang",
            "Chenwang Wu",
            "Qi Liu",
            "Gangwei Jiang",
            "Yuanhao Pu",
            "Yuxuan Lei",
            "Xiaolong Chen",
            "Xingmei Wang",
            "Defu Lian",
            "Enhong Chen"
        ],
        "citations": 52,
        "references": 279,
        "year": 2023
    },
    {
        "title": "Contrastive Adapters for Foundation Model Group Robustness",
        "abstract": "While large pretrained foundation models (FMs) have shown remarkable zero-shot classification robustness to dataset-level distribution shifts, their robustness to subpopulation or group shifts is relatively underexplored. We study this problem, and find that FMs such as CLIP may not be robust to various group shifts. Across 9 robustness benchmarks, zero-shot classification with their embeddings results in gaps of up to 80.7 percentage points (pp) between average and worst-group accuracy. Unfortunately, existing methods to improve robustness require retraining, which can be prohibitively expensive on large foundation models. We also find that efficient ways to improve model inference (e.g., via adapters, lightweight networks with FM embeddings as inputs) do not consistently improve and can sometimes hurt group robustness compared to zero-shot (e.g., increasing the accuracy gap by 50.1 pp on CelebA). We thus develop an adapter training strategy to effectively and efficiently improve FM group robustness. Our motivating observation is that while poor robustness results from groups in the same class being embedded far apart in the foundation model\"embedding space,\"standard adapter training may not bring these points closer together. We thus propose contrastive adapting, which trains adapters with contrastive learning to bring sample embeddings close to both their ground-truth class embeddings and other sample embeddings in the same class. Across the 9 benchmarks, our approach consistently improves group robustness, raising worst-group accuracy by 8.5 to 56.0 pp over zero-shot. Our approach is also efficient, doing so without any FM finetuning and only a fixed set of frozen FM embeddings. On benchmarks such as Waterbirds and CelebA, this leads to worst-group accuracy comparable to state-of-the-art methods that retrain entire models, while only training $\\leq$1% of the model parameters.",
        "authors": [
            "Michael Zhang",
            "Christopher R'e"
        ],
        "citations": 49,
        "references": 91,
        "year": 2022
    },
    {
        "title": "xVal: A Continuous Numerical Tokenization for Scientific Language Models",
        "abstract": "Due in part to their discontinuous and discrete default encodings for numbers, Large Language Models (LLMs) have not yet been commonly used to process numerically-dense scientific datasets. Rendering datasets as text, however, could help aggregate diverse and multi-modal scientific data into a single training corpus, thereby potentially facilitating the development of foundation models for science. In this work, we introduce xVal, a strategy for continuously tokenizing numbers within language models that results in a more appropriate inductive bias for scientific applications. By training specially-modified language models from scratch on a variety of scientific datasets formatted as text, we find that xVal generally outperforms other common numerical tokenization strategies on metrics including out-of-distribution generalization and computational efficiency.",
        "authors": [
            "Siavash Golkar",
            "Mariel Pettee",
            "Michael Eickenberg",
            "Alberto Bietti",
            "M. Cranmer",
            "G. Krawezik",
            "François Lanusse",
            "Michael McCabe",
            "Ruben Ohana",
            "Liam Parker",
            "Bruno Régaldo-Saint Blancard",
            "Tiberiu Teşileanu",
            "Kyunghyun Cho",
            "Shirley Ho"
        ],
        "citations": 29,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Towards a foundation model for geospatial artificial intelligence (vision paper)",
        "abstract": "Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet to see an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges for developing multimodal foundation models for GeoAI. We first show the advantages of this idea by testing the performance of existing Large pre-trained Language Models (LLMs) (e.g. GPT-2 and GPT-3) on two geospatial semantics tasks. Results indicate that these task-agnostic LLMs can outperform task-specific fully-supervised models on both tasks with 2--9% improvement in a few-shot learning setting. However, we also show the limitations of these existing foundation models given the multimodality nature of GeoAI, especially when dealing with geometries in conjunction with other modalities. So we discuss the possibility of a multimodal foundation model which can reason over various types of geospatial data through geospatial alignments. We conclude this paper by discussing the unique risks and challenges to develop such model for GeoAI.",
        "authors": [
            "Gengchen Mai",
            "Chris Cundy",
            "Kristy Choi",
            "Yingjie Hu",
            "Ni Lao",
            "Stefano Ermon"
        ],
        "citations": 56,
        "references": 27,
        "year": 2022
    },
    {
        "title": "A Foundation Model for Cell Segmentation",
        "abstract": "Cells are a fundamental unit of biological organization, and identifying them in imaging data – cell segmentation – is a critical task for various cellular imaging experiments. While deep learning methods have led to substantial progress on this problem, most models in use are specialist models that work well for specific domains. Methods that have learned the general notion of “what is a cell” and can identify them across different domains of cellular imaging data have proven elusive. In this work, we present CellSAM, a foundation model for cell segmentation that generalizes across diverse cellular imaging data. CellSAM builds on top of the Segment Anything Model (SAM) by developing a prompt engineering approach for mask generation. We train an object detector, CellFinder, to automatically detect cells and prompt SAM to generate segmentations. We show that this approach allows a single model to achieve human-level performance for segmenting images of mammalian cells (in tissues and cell culture), yeast, and bacteria collected across various imaging modalities. We show that CellSAM has strong zero-shot performance and can be improved with a few examples via few-shot learning. We also show that CellSAM can unify bioimaging analysis workflows such as spatial transcriptomics and cell tracking. A deployed version of CellSAM is available at https://cellsam.deepcell.org/.",
        "authors": [
            "Uriah Israel",
            "Markus Marks",
            "Rohit Dilip",
            "Qilin Li",
            "Changhua Yu",
            "Emily Laubscher",
            "Shenyi Li",
            "Morgan Schwartz",
            "Elora Pradhan",
            "Ada Ates",
            "Martin Abt",
            "Caitlin Brown",
            "Edward Pao",
            "Alexander Pearson-Goulart",
            "Pietro Perona",
            "Georgia Gkioxari",
            "Ross Barnowski",
            "Yisong Yue",
            "D. V. Valen"
        ],
        "citations": 17,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Fine-tuning vision foundation model for crack segmentation in civil infrastructures",
        "abstract": "Large-scale foundation models have become the mainstream deep learning method, while in civil engineering, the scale of AI models is strictly limited. In this work, a vision foundation model is introduced for crack segmentation. Two parameter-efficient fine-tuning methods, adapter and low-rank adaptation, are adopted to fine-tune the foundation model in semantic segmentation: the Segment Anything Model (SAM). The fine-tuned CrackSAM shows excellent performance on different scenes and materials. To test the zero-shot performance of the proposed method, two unique datasets related to road and exterior wall cracks are collected, annotated and open-sourced, for a total of 810 images. Comparative experiments are conducted with twelve mature semantic segmentation models. On datasets with artificial noise and previously unseen datasets, the performance of CrackSAM far exceeds that of all state-of-the-art models. CrackSAM exhibits remarkable superiority, particularly under challenging conditions such as dim lighting, shadows, road markings, construction joints, and other interference factors. These cross-scenario results demonstrate the outstanding zero-shot capability of foundation models and provide new ideas for developing vision models in civil engineering.",
        "authors": [
            "Kang Ge",
            "Chen Wang",
            "Yutao Guo",
            "Yansong Tang",
            "Zhenzhong Hu"
        ],
        "citations": 8,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Foundation Model is Efficient Multimodal Multitask Model Selector",
        "abstract": "This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0\\%, 26.3\\%, 20.1\\%, 54.8\\%, 12.2\\% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.",
        "authors": [
            "Fanqing Meng",
            "Wenqi Shao",
            "Zhanglin Peng",
            "Chong Jiang",
            "Kaipeng Zhang",
            "Y. Qiao",
            "Ping Luo"
        ],
        "citations": 8,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Adapting an ASR Foundation Model for Spoken Language Assessment",
        "abstract": "A crucial part of an accurate and reliable spoken language assessment system is the underlying ASR model. Recently, large-scale pre-trained ASR foundation models such as Whisper have been made available. As the output of these models is designed to be human readable, punctuation is added, numbers are presented in Arabic numeric form and abbreviations are included. Additionally, these models have a tendency to skip disfluencies and hesitations in the output. Though useful for readability, these attributes are not helpful for assessing the ability of a candidate and providing feedback. Here a precise transcription of what a candidate said is needed. In this paper, we give a detailed analysis of Whisper outputs and propose two solutions: fine-tuning and soft prompt tuning. Experiments are conducted on both public speech corpora and an English learner dataset. Results show that we can effectively alter the decoding behaviour of Whisper to generate the exact words spoken in the response.",
        "authors": [
            "Rao Ma",
            "Mengjie Qian",
            "M. Gales",
            "K. Knill"
        ],
        "citations": 8,
        "references": 27,
        "year": 2023
    },
    {
        "title": "Towards Responsible Generative AI: A Reference Architecture for Designing Foundation Model Based Agents",
        "abstract": "Foundation models, such as large language models (LLMs), have been widely recognised as transformative AI technologies due to their capabilities to understand and generate content, including plans with reasoning capabilities. Foundation model based agents derive their autonomy from the capabilities of foundation models, which enable them to autonomously break down a given goal into a set of manageable tasks and orchestrate task execution to meet the goal. Despite the huge efforts put into building foundation model based agents, the architecture design of the agents has not yet been systematically explored. Also, while there are significant benefits of using agents for planning and execution, there are serious considerations regarding responsible AI related software quality attributes, such as security and accountability. Therefore, this paper presents a pattern-oriented reference architecture that serves as guidance when designing foundation model based agents. We evaluate the completeness and utility of the proposed reference architecture by mapping it to the architecture of two real-world agents.",
        "authors": [
            "Qinghua Lu",
            "Liming Zhu",
            "Xiwei Xu",
            "Zhenchang Xing",
            "Stefan Harrer",
            "Jon Whittle"
        ],
        "citations": 8,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations",
        "abstract": "The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date(under 12 seconds for Stable Diffusion 1.4 without INT8 quantization for a 512 × 512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.",
        "authors": [
            "Yu-Hui Chen",
            "Raman Sarokin",
            "Juhyun Lee",
            "Jiuqiang Tang",
            "Chuo-Ling Chang",
            "Andrei Kulik",
            "Matthias Grundmann"
        ],
        "citations": 28,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation",
        "abstract": "Recent advancements in large foundation models have shown promising potential in the medical industry due to their flexible prompting capability. One such model, the Segment Anything Model (SAM), a prompt-driven segmentation model, has shown remarkable performance improvements, surpassing state-of-the-art approaches in medical image segmentation. However, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. In this paper, we propose a novel perspective on self-prompting in medical vision applications. Specifically, we harness the embedding space of SAM to prompt itself through a simple yet effective linear pixel-wise classifier. By preserving the encoding capabilities of the large model, the contextual information from its decoder, and leveraging its interactive promptability, we achieve competitive results on multiple datasets (i.e. improvement of more than 15% compared to fine-tuning the mask decoder using a few images).",
        "authors": [
            "Qi Wu",
            "Yuyao Zhang",
            "Marawan Elbatel"
        ],
        "citations": 27,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Intelligent Risk Prognosis and Control of Foundation Pit Excavation Based on Digital Twin",
        "abstract": "Timely risk information acquisition and diagnosis during foundation pit excavation (FPE) processes are vital for ensuring the safe and effective construction of underground urban infrastructures. Unfortunately, diverse geological and hydrogeological conditions and complex shapes of the foundation pit create barriers for reliable FPE risk prognosis and control. Furthermore, typical support systems during FPE use temporary measures, which have limited capacity to confront excessive loads, large deformations, and seepage. This study aims to establish an intelligent risk prognosis and control framework based on digital twin (DT) for ensuring safe and effective FPE processes. Previous studies have conducted extensive experimental and numerical analyses for examining unsafe conditions during FPE. How to enable intelligent risk prognosis and control of tedious FPE processes by integrating physics-based models and sensory data collected in the field is still challenging. DT could help to establish the interaction and feedback mechanisms between the physical and virtual space. In this study, the authors have established a DT model that consists of a physical space model and a high-fidelity physics-based model of a foundation pit in virtual space. As a result, a mechanism for effective acquisition and fusion of heterogeneous information from both physical and virtual space is established. Then, the authors proposed an integrated model and data-driven approach for examining safety risks during FPE. In the end, the authors have validated the proposed method through a case study of the FPE of the Wuhan Metro Line. The results show that the proposed method could provide theoretical and practical support for future intelligent FPE.",
        "authors": [
            "Zhe Sun",
            "Haoyang Li",
            "Yan Bao",
            "Xiao-Xu Meng",
            "Dongliang Zhang"
        ],
        "citations": 14,
        "references": 26,
        "year": 2023
    },
    {
        "title": "When is a Foundation Model a Foundation Model",
        "abstract": "Recently, several studies have reported on the fine-tuning of foundation models for image-text modeling in the field of medicine, utilizing images from online data sources such as Twitter and PubMed. Foundation models are large, deep artificial neural networks capable of learning the context of a specific domain through training on exceptionally extensive datasets. Through validation, we have observed that the representations generated by such models exhibit inferior performance in retrieval tasks within digital pathology when compared to those generated by significantly smaller, conventional deep networks.",
        "authors": [
            "Saghir Alfasly",
            "Peyman Nejat",
            "S. Hemati",
            "Jibran A. Khan",
            "Isaiah Lahr",
            "Areej Alsaafin",
            "Abubakr Shafique",
            "N. Comfere",
            "Dennis Murphree",
            "Chady Meroueh",
            "Saba Yasir",
            "Aaron Mangold",
            "Lisa Boardman",
            "Vijay H. Shah",
            "Joaquin J. Garcia",
            "H. Tizhoosh"
        ],
        "citations": 5,
        "references": 13,
        "year": 2023
    },
    {
        "title": "Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation",
        "abstract": "The success of large language models has inspired the computer vision community to explore image segmentation foundation model that is able to zero/few-shot generalize through prompt engineering. Segment-Anything (SAM), among others, is the state-of-the-art image segmentation foundation model demonstrating strong zero/few-shot generalization. Despite the success, recent studies reveal the weakness of SAM under strong distribution shift. In particular, SAM performs awkwardly on corrupted natural images, camouflaged images, medical images, etc. Motivated by the observations, we aim to develop a self-training based strategy to adapt SAM to target distribution. Given the unique challenges of large source dataset, high computation cost and incorrect pseudo label, we propose a weakly supervised self-training architecture with anchor regularization and low-rank finetuning to improve the robustness and computation efficiency of adaptation. We validate the effectiveness on 5 types of downstream segmentation tasks including natural clean/corrupted images, medical images, camouflaged images and robotic images. Our proposed method is task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art domain adaptation methods on almost all downstream tasks with the same testing prompt inputs.",
        "authors": [
            "Haojie Zhang",
            "Yongyi Su",
            "Xun Xu",
            "Kui Jia"
        ],
        "citations": 13,
        "references": 78,
        "year": 2023
    },
    {
        "title": "BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages",
        "abstract": "Large language models (LLMs) demonstrate promising translation performance among various natural languages. However, many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored. In this work, we present BigTranslate which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages. BigTranslate is built upon LLaMA-13B and it is optimized in three steps. First, we continue training LLaMA with massive Chinese monolingual data. Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages. Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model. The preliminary experiments on multilingual translation show that BigTranslate performs comparably with ChatGPT and Google Translate in many languages and even outperforms ChatGPT in 8 language pairs. We release the BigTranslate model and hope it can advance the research progress.",
        "authors": [
            "Wen Yang",
            "Chong Li",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "citations": 39,
        "references": 42,
        "year": 2023
    },
    {
        "title": "PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology",
        "abstract": "As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, with significant applications in natural image interpretation. However, the field of pathology has largely remained untapped in this regard, despite the growing need for accurate, timely, and personalized diagnostics. To bridge the gap in pathology MLLMs, we present the PathAsst in this study, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. To develop PathAsst, we collect over 142K high-quality pathology image-text pairs from a variety of reliable sources, including PubMed, comprehensive pathology textbooks, reputable pathology websites, and private data annotated by pathologists. Leveraging the advanced capabilities of ChatGPT/GPT-4, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data, specifically tailored for the invocation of the pathology-specific models, allowing the PathAsst to effectively interact with these models based on the input image and user intent, consequently enhancing the model’s diagnostic capabilities. Subsequently, our PathAsst is trained based on Vicuna-13B language model in coordination with the CLIP vision encoder. The results of PathAsst show the potential of harnessing the AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid researchers in the extensive collection and preprocessing of their own datasets. Resources can be obtained at https://github.com/superjamessyx/ Generative-Foundation-AI-Assistant-for-Pathology .",
        "authors": [
            "Yuxuan Sun",
            "Chenglu Zhu",
            "Sunyi Zheng",
            "Kai Zhang",
            "Zhongyi Shui",
            "Xiaoxuan Yu",
            "Yizhi Zhao",
            "Honglin Li",
            "Yunlong Zhang",
            "Ruojia Zhao",
            "Xinheng Lyu",
            "Lin Yang"
        ],
        "citations": 12,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Maximally localized Wannier functions, interaction models, and fractional quantum anomalous Hall effect in twisted bilayer MoTe2",
        "abstract": "Significance Twisted MoTe2 provided the first realization of fractional quantum anomalous Hall effect. In this work, we construct the maximum localized Wannier functions and the minimal generalized Hubbard model for twisted MoTe2. These calculations elucidate the broad range of ferromagnetism driven by direct exchange. Notably, our two-band exact diagonalization study of ν=−2/3 state differs significantly from prior single-band approaches. It reveals pronounced band-mixing, akin to Landau-level mixing observed in the quantum Hall problem, and enlarges the optimal twist angle. Our work not only directly explains experimental findings but also establishes a foundation for future investigation of the rich topological and correlated states, as well as phase transitions at various filling factors.",
        "authors": [
            "Cheng Xu",
            "Jiangxu Li",
            "Yong Xu",
            "Zhen Bi",
            "Yang Zhang"
        ],
        "citations": 42,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Towards an astronomical foundation model for stars with a Transformer-based model",
        "abstract": "Rapid strides are currently being made in the field of artificial intelligence using Transformer-based models like Large Language Models (LLMs). The potential of these methods for creating a single, large, versatile model in astronomy has not yet been explored. In this work, we propose a framework for data-driven astronomy that uses the same core techniques and architecture as used by LLMs. Using a variety of observations and labels of stars as an example, we build a Transformer-based model and train it in a self-supervised manner with cross-survey data sets to perform a variety of inference tasks. In particular, we demonstrate that a $\\textit{single}$ model can perform both discriminative and generative tasks even if the model was not trained or fine-tuned to do any specific task. For example, on the discriminative task of deriving stellar parameters from Gaia XP spectra, we achieve an accuracy of 47 K in $T_\\mathrm{eff}$, 0.11 dex in $\\log{g}$, and 0.07 dex in $[\\mathrm{M/H}]$, outperforming an expert $\\texttt{XGBoost}$ model in the same setting. But the same model can also generate XP spectra from stellar parameters, inpaint unobserved spectral regions, extract empirical stellar loci, and even determine the interstellar extinction curve. Our framework demonstrates that building and training a $\\textit{single}$ foundation model without fine-tuning using data and parameters from multiple surveys to predict unmeasured observations and parameters is well within reach. Such\"Large Astronomy Models\"trained on large quantities of observational data will play a large role in the analysis of current and future large surveys.",
        "authors": [
            "Henry W. Leung",
            "J. Bovy"
        ],
        "citations": 13,
        "references": 9,
        "year": 2023
    },
    {
        "title": "Foundation Model Assisted Automatic Speech Emotion Recognition: Transcribing, Annotating, and Augmenting",
        "abstract": "Significant advances are being made in speech emotion recognition (SER) using deep learning models. Nonetheless, training SER systems remains challenging, requiring both time and costly resources. Like many other machine learning tasks, acquiring datasets for SER requires substantial data annotation efforts, including transcription and labeling. These annotation processes present challenges when attempting to scale up conventional SER systems. Recent developments in foundational models have had a tremendous impact, giving rise to applications such as ChatGPT. These models have enhanced human-computer interactions including bringing unique possibilities for streamlining data collection in fields like SER. In this research, we explore the use of foundational models to assist in automating SER from transcription and annotation to augmentation. Our study demonstrates that these models can generate transcriptions to enhance the performance of SER systems that rely solely on speech data. Furthermore, we note that annotating emotions from transcribed speech remains a challenging task. However, combining outputs from multiple LLMs enhances the quality of annotations. Lastly, our findings suggest the feasibility of augmenting existing speech emotion datasets by annotating unlabeled speech samples.",
        "authors": [
            "Tiantian Feng",
            "Shrikanth S. Narayanan"
        ],
        "citations": 13,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Source-Free Domain Adaptation with Frozen Multimodal Foundation Model",
        "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a target domain, with only access to unlabeled target training data and the source model pretrained on a supervised source domain. Relying on pseudo labeling and/or auxiliary supervision, conventional methods are inevitably error-prone. To mitigate this limitation, in this work we for the first time explore the potentials of off-the-shelf vision-language (ViL) multimodal models (e.g., CLIP) with rich whilst heterogeneous knowledge. We find that directly applying the ViL model to the target domain in a zero-shot fashion is unsatisfactory, as it is not specialized for this particular task but largely generic. To make it task specific, we propose a novel Distilling multImodal Foundation mOdel (DIFO) approach. Specifically, DIFO alternates between two steps during adaptation: (i) Customizing the ViL model by maximizing the mutual information with the target model in a prompt learning manner, (ii) Distilling the knowledge of this customized ViL model to the target model. For more fine-grained and reliable distillation, we further introduce two effective regularization terms, namely most-likely category encouragement and predictive consistency. Extensive experiments show that DIFO significantly outperforms the state-of-the-art alternatives. Code is here.",
        "authors": [
            "Song Tang",
            "Wenxin Su",
            "Mao Ye",
            "Xiatian Zhu"
        ],
        "citations": 10,
        "references": 52,
        "year": 2023
    },
    {
        "title": "nach0: multimodal natural and chemical languages foundation model",
        "abstract": "Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder–decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups.",
        "authors": [
            "M. Livne",
            "Z. Miftahutdinov",
            "E. Tutubalina",
            "Maksim Kuznetsov",
            "Daniil Polykovskiy",
            "Annika Brundyn",
            "Aastha Jhunjhunwala",
            "Anthony B Costa",
            "A. Aliper",
            "A. Zhavoronkov"
        ],
        "citations": 11,
        "references": 85,
        "year": 2023
    },
    {
        "title": "Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model",
        "abstract": "Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines.",
        "authors": [
            "Qi Jia",
            "Siyu Ren",
            "Yizhu Liu",
            "Kenny Q. Zhu"
        ],
        "citations": 10,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Generative AI models should include detection mechanisms as a condition for public release",
        "abstract": null,
        "authors": [
            "Alistair Knott",
            "Dino Pedreschi",
            "Raja Chatila",
            "Tapabrata Chakraborti",
            "Susan Leavy",
            "Ricardo A. Baeza-Yates",
            "D. Eyers",
            "Andrew Trotman",
            "Paul D. Teal",
            "Przemysław Biecek",
            "Stuart Russell",
            "Y. Bengio"
        ],
        "citations": 22,
        "references": 40,
        "year": 2023
    },
    {
        "title": "LAN-grasp: Using Large Language Models for Semantic Object Grasping",
        "abstract": "In this paper, we propose Lan-grasp, a novel approach towards more appropriate semantic grasping. We use foundation models to provide the robot with a deeper understanding of the objects, the right place to grasp an object, or even the parts to avoid. This allows our robot to grasp and utilize objects in a more meaningful and safe manner. We leverage the combination of a Large Language Model, a Vision Language Model, and a traditional grasp planner to generate grasps demonstrating a deeper semantic understanding of the objects. We first prompt the Large Language Model about which object part is appropriate for grasping. Next, the Vision Language Model identifies the corresponding part in the object image. Finally, we generate grasp proposals in the region proposed by the Vision Language Model. Building on foundation models provides us with a zero-shot grasp method that can handle a wide range of objects without the need for further training or fine-tuning. We evaluated our method in real-world experiments on a custom object data set. We present the results of a survey that asks the participants to choose an object part appropriate for grasping. The results show that the grasps generated by our method are consistently ranked higher by the participants than those generated by a conventional grasping planner and a recent semantic grasping approach. In addition, we propose a Visual Chain-of-Thought feedback loop to assess grasp feasibility in complex scenarios. This mechanism enables dynamic reasoning and generates alternative grasp strategies when needed, ensuring safer and more effective grasping outcomes.",
        "authors": [
            "Reihaneh Mirjalili",
            "Michael Krawez",
            "Simone Silenzi",
            "Yannik Blei",
            "Wolfram Burgard"
        ],
        "citations": 21,
        "references": 47,
        "year": 2023
    },
    {
        "title": "ChatCounselor: A Large Language Models for Mental Health Support",
        "abstract": "This paper presents ChatCounselor, a large language model (LLM) solution designed to provide mental health support. Unlike generic chatbots, ChatCounselor is distinguished by its foundation in real conversations between consulting clients and professional psychologists, enabling it to possess specialized knowledge and counseling skills in the field of psychology. The training dataset, Psych8k, was constructed from 260 in-depth interviews, each spanning an hour. To assess the quality of counseling responses, the counseling Bench was devised. Leveraging GPT-4 and meticulously crafted prompts based on seven metrics of psychological counseling assessment, the model underwent evaluation using a set of real-world counseling questions. Impressively, ChatCounselor surpasses existing open-source models in the counseling Bench and approaches the performance level of ChatGPT, showcasing the remarkable enhancement in model capability attained through high-quality domain-specific data.",
        "authors": [
            "June M. Liu",
            "Donghao Li",
            "He Cao",
            "Tianhe Ren",
            "Zeyi Liao",
            "Jiamin Wu"
        ],
        "citations": 26,
        "references": 42,
        "year": 2023
    },
    {
        "title": "MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models",
        "abstract": "As an integral part of people's daily lives, social media is becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear poor generalization ability and low interpretability, the recent large language models (LLMs) have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions in zero-shot or few-shot settings. The results show that LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner, which further significantly affects the quality of the generated explanations. Domain-specific finetuning is an effective solution, but faces two critical challenges: 1) lack of high-quality training data. 2) no open-source foundation LLMs. To alleviate these problems, we formally model interpretable mental health analysis as a text generation task, and build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset with 105K data samples to support LLM instruction tuning and evaluation. The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks. We prompt ChatGPT with expert-designed few-shot prompts to obtain explanations. To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data. Based on the IMHI dataset and LLaMA2 foundation models, we train MentaLLaMA, the first open-source instruction-following LLM series for interpretable mental health analysis on social media. We evaluate MentaLLaMA and other advanced methods on the IMHI benchmark, the first holistic evaluation benchmark for interpretable mental health analysis. The results show that MentaLLaMA approaches state-of-the-art discriminative methods in correctness and generates human-level explanations. MentaLLaMA models also show strong generalizability to unseen tasks. The project is available at https://github.com/SteveKGYang/MentaLLaMA.",
        "authors": [
            "Kailai Yang",
            "Tianlin Zhang",
            "Zi-Zhou Kuang",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "citations": 26,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Tutorial on Large Language Models for Recommendation",
        "abstract": "Foundation Models such as Large Language Models (LLMs) have significantly advanced many research areas. In particular, LLMs offer significant advantages for recommender systems, making them valuable tools for personalized recommendations. For example, by formulating various recommendation tasks such as rating prediction, sequential recommendation, straightforward recommendation, and explanation generation into language instructions, LLMs make it possible to build universal recommendation engines that can handle different recommendation tasks. Additionally, LLMs have a remarkable capacity for understanding natural language, enabling them to comprehend user preferences, item descriptions, and contextual information to generate more accurate and relevant recommendations, leading to improved user satisfaction and engagement. This tutorial introduces Foundation Models such as LLMs for recommendation. We will introduce how recommender system advanced from shallow models to deep models and to large models, how LLMs enable generative recommendation in contrast to traditional discriminative recommendation, and how to build LLM-based recommender systems. We will cover multiple perspectives of LLM-based recommendation, including data preparation, model design, model pre-training, fine-tuning and prompting, multi-modality and multi-task learning, as well as trustworthy perspectives of LLM-based recommender systems such as fairness and transparency.",
        "authors": [
            "Wenyue Hua",
            "Lei Li",
            "Shuyuan Xu",
            "L. Chen",
            "Yongfeng Zhang"
        ],
        "citations": 20,
        "references": 12,
        "year": 2023
    },
    {
        "title": "BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models",
        "abstract": "Large language models (LLMs) have demonstrated remarkable prowess in language understanding and generation. Advancing from foundation LLMs to instructionfollowing LLMs, instruction tuning plays a vital role in aligning LLMs to human preferences. However, the existing LLMs are usually focused on English, leading to inferior performance in non-English languages. In order to improve the performance for non-English languages, it is necessary to collect language-specific training data for foundation LLMs and construct language-specific instructions for instruction tuning, both of which are heavy loads. To minimize human workload, we propose to transfer the capabilities of language generation and instruction following from English to other languages through an interactive translation task. We have developed BayLing, an instruction-following LLM by utilizing LLaMA as the foundation LLM and automatically constructing interactive translation instructions for instructing tuning. Extensive assessments demonstrate that BayLing achieves comparable performance to GPT-3.5-turbo, despite utilizing a considerably smaller parameter size of only 13 billion. Experimental results on translation tasks show that BayLing achieves 95% of single-turn translation capability compared to GPT-4 with automatic evaluation and 96% of interactive translation capability compared to GPT-3.5-turbo with human evaluation. To estimate the performance on general tasks, we created a multi-turn instruction test set called BayLing-80. The experimental results on BayLing-80 indicate that BayLing achieves 89% of performance compared to GPT-3.5-turbo. BayLing also demonstrates outstanding performance on knowledge assessment of Chinese GaoKao and English SAT, second only to GPT-3.5-turbo among a multitude of instruction-following LLMs. Demo, homepage, code and models of BayLing are available.",
        "authors": [
            "Shaolei Zhang",
            "Qingkai Fang",
            "Zhuocheng Zhang",
            "Zhengrui Ma",
            "Yan Zhou",
            "Langlin Huang",
            "Mengyu Bu",
            "Shangtong Gui",
            "Yunji Chen",
            "Xilin Chen",
            "Yang Feng"
        ],
        "citations": 34,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Fusing Models with Complementary Expertise",
        "abstract": "Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the\"frugal\"setting where it is desired to reduce the number of expert model evaluations at test time. Our implementation is publicly available at https://github.com/hwang595/FoE-ICLR2024.",
        "authors": [
            "Hongyi Wang",
            "Felipe Maia Polo",
            "Yuekai Sun",
            "Souvik Kundu",
            "Eric P. Xing",
            "M. Yurochkin"
        ],
        "citations": 19,
        "references": 80,
        "year": 2023
    },
    {
        "title": "UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model in Data Science",
        "abstract": "Recent advancements in NLP have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to facilitating the prediction over tables in data science, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the establishment of a universal pretraining protocol for tables with varied structures, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a straightforward yet effective method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit. This is subsequently followed by a Transformer encoder to refine the representation. Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts. In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13B samples, meticulously gathered from the Kaggle platform. This research primarily centers on classification and regression tasks involving tabular data, and conducts rigorous experimental testing and analyses to validate the effectiveness of our methodology. The experimental results demonstrate UniTabE's superior performance against several baselines across massive benchmarks. This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride for tabular data analysis.",
        "authors": [
            "Yazheng Yang",
            "Yuqi Wang",
            "Guangyi Liu",
            "Ledell Yu Wu",
            "Qi Liu"
        ],
        "citations": 11,
        "references": 30,
        "year": 2023
    },
    {
        "title": "M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems",
        "abstract": "Industrial recommender systems have been growing increasingly complex, may involve \\emph{diverse domains} such as e-commerce products and user-generated contents, and can comprise \\emph{a myriad of tasks} such as retrieval, ranking, explanation generation, and even AI-assisted content production. The mainstream approach so far is to develop individual algorithms for each domain and each task. In this paper, we explore the possibility of developing a unified foundation model to support \\emph{open-ended domains and tasks} in an industrial recommender system, which may reduce the demand on downstream settings' data and can minimize the carbon footprint by avoiding training a separate model from scratch for every task. Deriving a unified foundation is challenging due to (i) the potentially unlimited set of downstream domains and tasks, and (ii) the real-world systems' emphasis on computational efficiency. We thus build our foundation upon M6, an existing large-scale industrial pretrained language model similar to GPT-3 and T5, and leverage M6's pretrained ability for sample-efficient downstream adaptation, by representing user behavior data as plain texts and converting the tasks to either language understanding or generation. To deal with a tight hardware budget, we propose an improved version of prompt tuning that outperforms fine-tuning with negligible 1\\% task-specific parameters, and employ techniques such as late interaction, early exiting, parameter sharing, and pruning to further reduce the inference time and the model size. We demonstrate the foundation model's versatility on a wide range of tasks such as retrieval, ranking, zero-shot recommendation, explanation generation, personalized content creation, and conversational recommendation, and manage to deploy it on both cloud servers and mobile devices.",
        "authors": [
            "Zeyu Cui",
            "Jianxin Ma",
            "Chang Zhou",
            "Jingren Zhou",
            "Hongxia Yang"
        ],
        "citations": 162,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Language Models are General-Purpose Interfaces",
        "abstract": "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",
        "authors": [
            "Y. Hao",
            "Haoyu Song",
            "Li Dong",
            "Shaohan Huang",
            "Zewen Chi",
            "Wenhui Wang",
            "Shuming Ma",
            "Furu Wei"
        ],
        "citations": 93,
        "references": 139,
        "year": 2022
    },
    {
        "title": "Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains",
        "abstract": "Multi-modal foundation models are typically trained on millions of pairs of natural images and text captions, frequently obtained through web-crawling approaches. Although such models depict excellent generative capabilities, they do not typically generalize well to specific domains such as medical images that have fundamentally shifted distributions compared to natural images. Building generative models for medical images that faithfully depict clinical context may help alleviate the paucity of healthcare datasets. Thus, in this study, we seek to research and expand the representational capabilities of large pretrained foundation models to medical concepts, specifically for leveraging the Stable Diffusion model to generate domain specific images found in medical imaging. We explore the sub-components of the Stable Diffusion pipeline (the variational autoencoder, the U-Net and the text-encoder) to fine-tune the model to generate medical images. We benchmark the efficacy of these efforts using quantitative image quality metrics and qualitative radiologist-driven evaluations that accurately represent the clinical content of conditional text prompts. Our best-performing model improves upon the stable diffusion baseline and can be conditioned to insert a realistic-looking abnormality on a synthetic radiology image, while maintaining a 95% accuracy on a classifier trained to detect the abnormality.",
        "authors": [
            "P. Chambon",
            "Christian Blüthgen",
            "C. Langlotz",
            "Akshay Chaudhari"
        ],
        "citations": 94,
        "references": 21,
        "year": 2022
    },
    {
        "title": "TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge",
        "abstract": "We introduce TeleQnA, the first benchmark dataset designed to evaluate the knowledge of Large Language Models (LLMs) in telecommunications. Comprising 10,000 questions and answers, this dataset draws from diverse sources, including standards and research articles. This paper outlines the automated question generation framework responsible for creating this dataset, along with how human input was integrated at various stages to ensure the quality of the questions. Afterwards, using the provided dataset, an evaluation is conducted to assess the capabilities of LLMs, including GPT-3.5 and GPT-4. The results highlight that these models struggle with complex standards related questions but exhibit proficiency in addressing general telecom-related inquiries. Additionally, our results showcase how incorporating telecom knowledge context significantly enhances their performance, thus shedding light on the need for a specialized telecom foundation model. Finally, the dataset is shared with active telecom professionals, whose performance is subsequently benchmarked against that of the LLMs. The findings illustrate that LLMs can rival the performance of active professionals in telecom knowledge, thanks to their capacity to process vast amounts of information, underscoring the potential of LLMs within this domain. The dataset has been made publicly accessible on GitHub.",
        "authors": [
            "Ali Maatouk",
            "Fadhel Ayed",
            "Nicola Piovesan",
            "Antonio De Domenico",
            "M. Debbah",
            "Zhi-Quan Luo"
        ],
        "citations": 28,
        "references": 13,
        "year": 2023
    },
    {
        "title": "Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models",
        "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a “chatbot”, and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Jiaying Lu",
            "Chengyuan Deng",
            "Can Zheng",
            "Junxiang Wang",
            "Tanmoy Chowdhury",
            "Yun-Qing Li",
            "Hejie Cui",
            "Tian-yu Zhao",
            "Amit Panalkar",
            "Wei Cheng",
            "Haoyu Wang",
            "Yanchi Liu",
            "Zhengzhang Chen",
            "Haifeng Chen",
            "Chris White",
            "Quanquan Gu",
            "Carl Yang",
            "Liang Zhao"
        ],
        "citations": 23,
        "references": 301,
        "year": 2023
    },
    {
        "title": "Heterogeneity within and among co-occurring foundation species increases biodiversity",
        "abstract": null,
        "authors": [
            "M. Thomsen",
            "A. Altieri",
            "C. Angelini",
            "M. Bishop",
            "F. Bulleri",
            "Roxanne Farhan",
            "Viktoria M. M. Frühling",
            "P. Gribben",
            "Seamus B. Harrison",
            "Qiang He",
            "Moritz Klinghardt",
            "Joachim Langeneck",
            "Brendan S. Lanham",
            "Luca Mondardini",
            "Yannick Mulders",
            "Semonn Oleksyn",
            "Aaron P. Ramus",
            "D. Schiel",
            "Tristan Schneider",
            "A. Siciliano",
            "B. Silliman",
            "D. Smale",
            "Paul M. South",
            "T. Wernberg",
            "Stacy Zhang",
            "G. Zotz"
        ],
        "citations": 43,
        "references": 112,
        "year": 2022
    },
    {
        "title": "Large Multimodal Models: Notes on CVPR 2023 Tutorial",
        "abstract": "This tutorial note summarizes the presentation on ``Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023 tutorial on ``Recent Advances in Vision Foundation Models''. The tutorial consists of three parts. We first introduce the background on recent GPT-like large models for vision-and-language modeling to motivate the research in instruction-tuned large multimodal models (LMMs). As a pre-requisite, we describe the basics of instruction-tuning in large language models, which is further extended to the multimodal space. Lastly, we illustrate how to build the minimum prototype of multimodal GPT-4 like models with the open-source resource, and review the recently emerged topics.",
        "authors": [
            "Chunyuan Li"
        ],
        "citations": 17,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework",
        "abstract": "Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, containing 100 million Chinese image-text pairs from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a deep benchmarking of different downstream tasks are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classiﬁcation task on 10 datasets, Wukong ViT-L achieves an average accuracy of 73.03%. For the image-text retrieval task, Wukong ViT-L achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than the result of WenLan 2.0. More information can refer to https://wukong-dataset. github.io/wukong-dataset/ .",
        "authors": [
            "Jiaxi Gu",
            "Xiaojun Meng",
            "Guansong Lu",
            "Lu Hou",
            "Minzhe Niu",
            "Hang Xu",
            "Xiaodan Liang",
            "Wei Zhang",
            "Xin Jiang",
            "Chunjing Xu"
        ],
        "citations": 41,
        "references": 69,
        "year": 2022
    },
    {
        "title": "To Transformers and Beyond: Large Language Models for the Genome",
        "abstract": "In the rapidly evolving landscape of genomics, deep learning has emerged as a useful tool for tackling complex computational challenges. This review focuses on the transformative role of Large Language Models (LLMs), which are mostly based on the transformer architecture, in genomics. Building on the foundation of traditional convolutional neural networks and recurrent neural networks, we explore both the strengths and limitations of transformers and other LLMs for genomics. Additionally, we contemplate the future of genomic modeling beyond the transformer architecture based on current trends in research. The paper aims to serve as a guide for computational biologists and computer scientists interested in LLMs for genomic data. We hope the paper can also serve as an educational introduction and discussion for biologists to a fundamental shift in how we will be analyzing genomic data in the future.",
        "authors": [
            "Micaela Elisa Consens",
            "Cameron Dufault",
            "Michael Wainberg",
            "Duncan Forster",
            "Mehran Karimzadeh",
            "Hani Goodarzi",
            "Fabian Theis",
            "Alan Moses",
            "Bo Wang"
        ],
        "citations": 22,
        "references": 129,
        "year": 2023
    },
    {
        "title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
        "abstract": "We seek to transform how new and emergent variants of pandemic-causing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pre-training on over 110 million prokaryotic gene sequences and fine-tuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole-genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.",
        "authors": [
            "Max Zvyagin",
            "Alexander Brace",
            "Kyle Hippe",
            "Yuntian Deng",
            "Bin Zhang",
            "Cindy Orozco Bohorquez",
            "Austin R. Clyde",
            "Bharati Kale",
            "Danilo Perez-Rivera",
            "Heng Ma",
            "Carla M. Mann",
            "Michael W. Irvin",
            "J. G. Pauloski",
            "Logan T. Ward",
            "Valerie Hayot",
            "M. Emani",
            "Sam Foreman",
            "Zhen Xie",
            "Diangen Lin",
            "Maulik Shukla",
            "Weili Nie",
            "Josh Romero",
            "Christian Dallago",
            "Arash Vahdat",
            "Chaowei Xiao",
            "Tom Gibbs",
            "Ian T. Foster",
            "James J. Davis",
            "M. Papka",
            "T. Brettin",
            "Rick L. Stevens",
            "Anima Anandkumar",
            "V. Vishwanath",
            "Arvind Ramanathan"
        ],
        "citations": 69,
        "references": 79,
        "year": 2022
    },
    {
        "title": "Bidirectional generation of structure and properties through a single molecular foundation model",
        "abstract": null,
        "authors": [
            "Jinho Chang",
            "Jong-Chul Ye"
        ],
        "citations": 16,
        "references": 75,
        "year": 2022
    },
    {
        "title": "Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets",
        "abstract": "Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.",
        "authors": [
            "D. Beaini",
            "Shenyang Huang",
            "Joao Alex Cunha",
            "Gabriela Moisescu-Pareja",
            "Oleksandr Dymov",
            "Sam Maddrell-Mander",
            "Callum McLean",
            "Frederik Wenkel",
            "Luis Muller",
            "Jama Hussein Mohamud",
            "Alipanah Parviz",
            "Michael Craig",
            "Michal Koziarski",
            "Jiarui Lu",
            "Zhaocheng Zhu",
            "Cristian Gabellini",
            "Kerstin Klaser",
            "Josef Dean",
            "Cas Wognum",
            "Maciej Sypetkowski",
            "Guillaume Rabusseau",
            "Reihaneh Rabbany",
            "Jian Tang",
            "Christopher Morris",
            "Ioannis Koutis",
            "M. Ravanelli",
            "Guy Wolf",
            "Prudencio Tossou",
            "Hadrien Mary",
            "Thérence Bois",
            "Andrew W. Fitzgibbon",
            "Bla.zej Banaszewski",
            "Chad Martin",
            "Dominic Masters"
        ],
        "citations": 13,
        "references": 76,
        "year": 2023
    },
    {
        "title": "ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation",
        "abstract": "—Although no speciﬁc domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for body pose estimation tasks. In this paper, we show the surprisingly good properties of plain vision transformers for body pose estimation from various aspects, namely simplicity in model structure, scalability in model size, ﬂexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model dubbed ViTPose . Speciﬁcally, ViTPose employs the plain and non-hierarchical vision transformer as an encoder to encode features and a lightweight decoder to decode body keypoints in either a top-down or a bottom-up manner. It can be scaled up from about 20M to 1B parameters by taking the advantage of the scalable model capacity and high parallelism of the vision transformer, setting a new Pareto front for throughput and performance. Besides, ViTPose is very ﬂexible regarding the attention type, input resolution, and pre-training and ﬁne-tuning strategy. Based on the ﬂexibility, a novel ViTPose+ model is proposed to deal with heterogeneous body keypoint categories in different types of body pose estimation tasks via knowledge factorization, i.e ., adopting task-agnostic and task-speciﬁc feed-forward networks in the transformer. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our ViTPose model outperforms representative methods on the challenging MS COCO Human Keypoint Detection benchmark at both top-down and bottom-up settings. Speciﬁcally, our largest single model ViTPose-G with 1B parameters sets a new record on the MS COCO test set without model ensemble. Furthermore, our ViTPose+ model achieves state-of-the-art performance simultaneously on a series of body pose estimation tasks",
        "authors": [
            "Yufei Xu",
            "Jing Zhang",
            "Qiming Zhang",
            "Dacheng Tao"
        ],
        "citations": 27,
        "references": 77,
        "year": 2022
    },
    {
        "title": "Information theory: A foundation for complexity science",
        "abstract": "Modeling and inference are central to most areas of science and especially to evolving and complex systems. Critically, the information we have is often uncertain and insufficient, resulting in an underdetermined inference problem; multiple inferences, models, and theories are consistent with available information. Information theory (in particular, the maximum information entropy formalism) provides a way to deal with such complexity. It has been applied to numerous problems, within and across many disciplines, over the last few decades. In this perspective, we review the historical development of this procedure, provide an overview of the many applications of maximum entropy and its extensions to complex systems, and discuss in more detail some recent advances in constructing comprehensive theory based on this inference procedure. We also discuss efforts at the frontier of information-theoretic inference: application to complex dynamic systems with time-varying constraints, such as highly disturbed ecosystems or rapidly changing economies.",
        "authors": [
            "Amos Golan",
            "John Harte"
        ],
        "citations": 25,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Numerical Study on the Deformation of Tunnels by Excavation of Foundation Pit Adjacent to the Subway",
        "abstract": "The excavation of the foundation pit will cause changes in the soil stress field around the foundation pit, and that may have adverse effects on the adjacent subway tunnels. In this paper, a complex deep foundation pit excavated in different sections is taken as the research object, and the support structure of the complex foundation pit project is introduced, which accumulates experience in the selection of support structure for similar projects. The finite element models are established by MIDAS/GTS software to evaluate the influence of excavation in different sections of the foundation pit on the tunnel deformation, and the accuracy of the finite element calculation results is verified by comparing the monitoring data. The results show that: The horizontal deformation of the subway tunnel is generally smaller than the vertical deformation. Tunnel monitoring should focus more on the development of the vertical deformation of the tunnel. The maximum vertical deformation and horizontal deformation of the tunnel are both smaller than the local specification limits, and the excavation of the foundation pit in this project has little influence on the deformation of the subway tunnel.",
        "authors": [
            "Xiang Zhao",
            "Han-Lin Wang",
            "Zhongwei Li",
            "Guoliang Dai",
            "Ziwei Yin",
            "Shuning Cao",
            "Junlong Zhou"
        ],
        "citations": 22,
        "references": 17,
        "year": 2022
    },
    {
        "title": "Hybrid ELM and MARS-Based Prediction Model for Bearing Capacity of Shallow Foundation",
        "abstract": "The nature of soil varies horizontally as well as vertically, owing to the process of the formation of soil. Thus, ensuring the safe design of geotechnical structures has been a major challenge. In shallow foundations, conducting field tests is expensive and time-consuming and often conducted on significantly scaled-down models. Empirical models, too, have been found to be the least reliable in the literature. The study proposes AI-based techniques to predict the bearing capacity of a shallow foundation, simulated using the datasets obtained in experiments conducted in different laboratories in the literature. The results of the ELM-EO and ELM-PSO hybrid models are compared with that of the ELM and MARS models. The performance of the models is analyzed and compared with each other using various performance parameters. The models are graded to each other using rank analysis and the visual interpretations are provided using error matrices and REC curves. ELM-EO is concluded to be the best performing model (R2 and RMSE equal to 0.995 and 0.01, respectively, in the testing phase), closely followed by ELM-PSO, MARS, and ELM. The performance of MARS is better than ELM (R2 equals 0.97 and 0.5, respectively, in the testing phase); however, hybridization greatly enhances the performance of the ELM and the hybrid models perform better than MARS. The paper concludes that AI-based models are robust and hybridization of regression models with optimization techniques should be encouraged in further research. Sensitivity analysis suggests that all the input parameters have a significant influence on the output, with friction angle being the highest.",
        "authors": [
            "Manish Kumar",
            "Vinay Kumar",
            "Rahul Biswas",
            "P. Samui",
            "Mosbeh R. Kaloop",
            "Majed Alzara",
            "A. Yosri"
        ],
        "citations": 20,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization",
        "abstract": "Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks. Empirically, it improves the state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, this work contributes to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to reliably update machine learning models. Our code is released: https://github.com/facebookresearch/ModelRatatouille.",
        "authors": [
            "Alexandre Ram'e",
            "Kartik Ahuja",
            "Jianyu Zhang",
            "M. Cord",
            "L. Bottou",
            "David Lopez-Paz"
        ],
        "citations": 63,
        "references": 117,
        "year": 2022
    },
    {
        "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
        "abstract": "Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io",
        "authors": [
            "Yi Ru Wang",
            "Jiafei Duan",
            "Dieter Fox",
            "S. Srinivasa"
        ],
        "citations": 15,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Open-vocabulary Semantic Segmentation with Frozen Vision-Language Models",
        "abstract": "When trained at a sufficient scale, self-supervised learning has exhibited a notable ability to solve a wide range of visual or language understanding tasks. In this paper, we investigate simple, yet effective approaches for adapting the pre-trained foundation models to the downstream task of interest, namely, open-vocabulary semantic segmentation. To this end, we make the following contributions: (i) we introduce Fusioner, with a lightweight, transformer-based fusion module, that pairs the frozen visual representation with language concept through a handful of image segmentation data. As a consequence, the model gains the capability of zero-shot transfer to segment novel categories; (ii) without loss of generality, we experiment on a broad range of self-supervised models that have been pre-trained with different schemes, e.g. visual-only models (MoCo v3, DINO), language-only models (BERT), visual-language model (CLIP), and show that, the proposed fusion approach is effective to any pair of visual and language models, even those pre-trained on a corpus of uni-modal data; (iii) we conduct thorough ablation studies to analyze the critical components in our proposed Fusioner, while evaluating on standard benchmarks, e.g. PASCAL-5i and COCO-20i , it surpasses existing state-of-the-art models by a large margin, despite only being trained on frozen visual and language features; (iv) to measure the model's robustness on learning visual-language correspondence, we further evaluate on synthetic dataset, named Mosaic-4, where images are constructed by mosaicking the samples from FSS-1000. Fusioner demonstrates superior performance over previous models.",
        "authors": [
            "Chaofan Ma",
            "Yu-Hao Yang",
            "Yanfeng Wang",
            "Ya Zhang",
            "Weidi Xie"
        ],
        "citations": 45,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Variational prompt tuning improves generalization of vision-language models",
        "abstract": "Prompt tuning provides an efﬁcient mechanism to adapt large vision-language models to downstream tasks by treating part of the input language prompts as learnable parameters while freezing the rest of the model. Existing works for prompt tuning are however prone to damaging the generalization capabilities of the foundation models, because the learned prompts lack the capacity of covering certain concepts within the language model. To avoid such limitation, we pro-pose a probabilistic modeling of the underlying distribution of prompts, allowing prompts within the support of an associated concept to be derived through stochastic sampling. This results in a more complete and richer transfer of the information captured by the language model, providing better generalization capabilities for downstream tasks. The resulting algorithm relies on a simple yet powerful variational framework that can be directly integrated with other developments. We show our approach is seamlessly integrated into both standard and conditional prompt learning frameworks, improving the performance on both cases considerably, especially with regards to preserving the generalization capability of the original model. Our method provides the current state-of-the-art for prompt learning, surpassing CoCoOp by 1.6% average Top-1 accuracy on the standard benchmark. Remarkably, it even surpasses the original CLIP model in terms of generalization to new classes. Implementation code will be released.",
        "authors": [
            "Mohammad Mahdi Derakhshani",
            "Enrique Sanchez",
            "Adrian Bulat",
            "Victor Costa",
            "Cees G. M. Snoek",
            "Georgios Tzimiropoulos",
            "Brais Martínez"
        ],
        "citations": 34,
        "references": 46,
        "year": 2022
    },
    {
        "title": "MolE: a molecular foundation model for drug discovery",
        "abstract": "Models that accurately predict properties based on chemical structure are valuable tools in drug discovery. However, for many properties, public and private training sets are typically small, and it is difficult for the models to generalize well outside of the training data. Recently, large language models have addressed this problem by using self-supervised pretraining on large unlabeled datasets, followed by fine-tuning on smaller, labeled datasets. In this paper, we report MolE, a molecular foundation model that adapts the DeBERTa architecture to be used on molecular graphs together with a two-step pretraining strategy. The first step of pretraining is a self-supervised approach focused on learning chemical structures, and the second step is a massive multi-task approach to learn biological information. We show that fine-tuning pretrained MolE achieves state-of-the-art results on 9 of the 22 ADMET tasks included in the Therapeutic Data Commons.",
        "authors": [
            "Oscar M'endez-Lucio",
            "C. Nicolaou",
            "Berton A. Earnshaw"
        ],
        "citations": 12,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Unified two-phase nonlocal formulation for vibration of functionally graded beams resting on nonlocal viscoelastic Winkler-Pasternak foundation",
        "abstract": null,
        "authors": [
            "Pei Zhang",
            "P. Schiavone",
            "H. Qing"
        ],
        "citations": 12,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Policy Adaptation from Foundation Model Feedback",
        "abstract": "Recent progress on vision-language foundation models have brought significant advancement to building generalpurpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unseen tasks, unseen environments, and sim-to-real transfer. We show PAFF improves baselines by a large margin in all cases.",
        "authors": [
            "Yuying Ge",
            "Annabella Macaluso",
            "Erran L. Li",
            "Ping Luo",
            "Xiaolong Wang"
        ],
        "citations": 8,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Foundational Models for Continual Learning: An Empirical Study of Latent Replay",
        "abstract": "Rapid development of large-scale pre-training has resulted in foundation models that can act as effective feature extractors on a variety of downstream tasks and domains. Motivated by this, we study the efﬁcacy of pre-trained vision models as a foundation for downstream continual learning (CL) scenarios. Our goal is twofold. First, we want to understand the compute-accuracy trade-off between CL in the raw-data space and in the latent space of pre-trained encoders. Second, we investigate how the characteristics of the encoder, the pre-training algorithm and data, as well as of the resulting latent space affect CL performance. For this, we compare the efﬁcacy of various pre-trained models in large-scale benchmarking scenarios with a vanilla replay setting applied in the latent and in the raw-data space. Notably, this study shows how transfer, forgetting, task similarity and learning are dependent on the input data characteristics and not necessarily on the CL algorithms. First, we show that under some circumstances reasonable CL performance can readily be achieved with a non-parametric classiﬁer at negligible compute. We then show how models pre-trained on broader data result in better performance for various replay sizes. We explain this with representational similarity and transfer properties of these representations. Finally, we show the effectiveness of self-supervised (SSL) pre-training for downstream domains that are out-of-distribution as compared to the pre-training domain. We point out and validate several research directions that can further increase the efﬁcacy of latent CL including representation ensembling. The diverse set of datasets used in this study can serve as a compute-efﬁcient playground for further CL research. Codebase is available under https://github.com/oleksost/latent_CL",
        "authors": [],
        "citations": 15,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Joint Communication and Sensing Toward 6G: Models and Potential of Using MIMO",
        "abstract": "The sixth-generation (6G) network is envisioned to integrate communication and sensing functions, so as to improve the spectrum efficiency and support explosive novel applications. Although the similarities of wireless communication and radio sensing lay the foundation for their combination, there is still considerable incompatible interest between them. To simultaneously guarantee the communication capacity and the sensing accuracy, the multiple-input and multiple-output (MIMO) technique plays an important role due to its unique capability of spatial beamforming and waveform shaping. However, the configuration of MIMO also brings high hardware cost, high power consumption, and high signal processing complexity. How to efficiently apply MIMO to achieve balanced communication and sensing performance is still open. In this survey, we discuss joint communication and sensing (JCAS) in the context of MIMO. We first outline the roles of MIMO in the process of wireless communication and radar sensing. Then, we present current advances in both communication and sensing coexistence and integration in detail. Three novel JCAS MIMO models are subsequently discussed by combining cutting-edge technologies, i.e., cloud radio access networks (C-RANs), unmanned aerial vehicles (UAVs), and reconfigurable intelligent surfaces (RISs). Examined from the practical perspective, the potential and challenges of MIMO in JCAS are summarized, and promising solutions are provided. Motivated by the great potential of the Internet of Things (IoT), we also specify JCAS in IoT scenarios and discuss the uniqueness of applying JCAS to IoT. In the end, open issues are outlined to envisage a ubiquitous, intelligent, and secure JCAS network in the near future.",
        "authors": [
            "Xinran Fang",
            "W. Feng",
            "Yunfei Chen",
            "Ning Ge",
            "Yan Zhang"
        ],
        "citations": 57,
        "references": 170,
        "year": 2022
    },
    {
        "title": "How Much Is Enough? A Study on Diffusion Times in Score-Based Generative Models",
        "abstract": "Score-based diffusion models are a class of generative models whose dynamics is described by stochastic differential equations that map noise into data. While recent works have started to lay down a theoretical foundation for these models, a detailed understanding of the role of the diffusion time T is still lacking. Current best practice advocates for a large T to ensure that the forward dynamics brings the diffusion sufficiently close to a known and simple noise distribution; however, a smaller value of T should be preferred for a better approximation of the score-matching objective and higher computational efficiency. Starting from a variational interpretation of diffusion models, in this work we quantify this trade-off and suggest a new method to improve quality and efficiency of both training and sampling, by adopting smaller diffusion times. Indeed, we show how an auxiliary model can be used to bridge the gap between the ideal and the simulated forward dynamics, followed by a standard reverse diffusion process. Empirical results support our analysis; for image data, our method is competitive with regard to the state of the art, according to standard sample quality metrics and log-likelihood.",
        "authors": [
            "Giulio Franzese",
            "Simone Rossi",
            "Lixuan Yang",
            "A. Finamore",
            "Dario Rossi",
            "M. Filippone",
            "Pietro Michiardi"
        ],
        "citations": 41,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Contextual Importance and Utility: aTheoretical Foundation",
        "abstract": null,
        "authors": [
            "Kary Främling"
        ],
        "citations": 8,
        "references": 20,
        "year": 2022
    },
    {
        "title": "The Foundation for Engineering a Pancreatic Islet Niche",
        "abstract": "Progress in diabetes research is hindered, in part, by deficiencies in current experimental systems to accurately model human pathophysiology and/or predict clinical outcomes. Engineering human-centric platforms that more closely mimic in vivo physiology, however, requires thoughtful and informed design. Summarizing our contemporary understanding of the unique and critical features of the pancreatic islet can inform engineering design criteria. Furthermore, a broad understanding of conventional experimental practices and their current advantages and limitations ensures that new models address key gaps. Improving beyond traditional cell culture, emerging platforms are combining diabetes-relevant cells within three-dimensional niches containing dynamic matrices and controlled fluidic flow. While highly promising, islet-on-a-chip prototypes must evolve their utility, adaptability, and adoptability to ensure broad and reproducible use. Here we propose a roadmap for engineers to craft biorelevant and accessible diabetes models. Concurrently, we seek to inspire biologists to leverage such tools to ask complex and nuanced questions. The progenies of such diabetes models should ultimately enable investigators to translate ambitious research expeditions from benchtop to the clinic.",
        "authors": [
            "Smit N. Patel",
            "C. Mathews",
            "Rachel Chandler",
            "C. Stabler"
        ],
        "citations": 9,
        "references": 240,
        "year": 2022
    },
    {
        "title": "Analytical design models for geotechnical seismic isolation systems",
        "abstract": null,
        "authors": [
            "H. Tsang"
        ],
        "citations": 28,
        "references": 52,
        "year": 2022
    },
    {
        "title": "Recycling diverse models for out-of-distribution generalization",
        "abstract": "Foundation models are redeﬁning how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: download a copy of a foundation model, and ﬁne-tune it using some in-house data about the target task of interest. Consequently, the Internet is swarmed by a handful of foundation models ﬁne-tuned on many diverse tasks. Yet, these individual ﬁne-tunings often lack strong generalization and exist in isolation without beneﬁting from each other. In our opinion, this is a missed opportunity, as these specialized models contain diverse features. Based on this insight, we propose model recycling , a simple strategy that leverages multiple ﬁne-tunings of the same foundation model on diverse auxiliary tasks, and repurposes them as rich and diverse initializations for the target task. Speciﬁcally, model recycling ﬁne-tunes in parallel each specialized model on the target task, and then averages the weights of all target ﬁne-tunings into a ﬁnal model. Empirically, we show that model recycling maximizes model diversity by beneﬁting from diverse auxiliary tasks, and achieves a new state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, model recycling is a contribution to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to incrementally and reliably update machine learning models.",
        "authors": [
            "Alexandre Ramé",
            "Kartik Ahuja",
            "Jianyu Zhang",
            "Matthieu Cord",
            "Léon Bottou",
            "David Lopez-Paz"
        ],
        "citations": 16,
        "references": 86,
        "year": 2022
    },
    {
        "title": "Large Language Models can Implement Policy Iteration",
        "abstract": "This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the\"few-shot\"quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex, a language model with no prior knowledge of the domains on which we evaluate it.",
        "authors": [
            "Ethan A. Brooks",
            "Logan Walls",
            "Richard L. Lewis",
            "Satinder Singh"
        ],
        "citations": 16,
        "references": 46,
        "year": 2022
    },
    {
        "title": "Models of Language and Multiword Expressions",
        "abstract": "Traditional accounts of language postulate two basic components: words stored in a lexicon, and rules that govern how they can be combined into meaningful sentences, a grammar. But, although this words-and-rules framework has proven itself to be useful in natural language processing and cognitive science, it has also shown important shortcomings when faced with actual language use. In this article, we review evidence from language acquisition, sentence processing, and computational modeling that shows how multiword expressions such as idioms, collocations, and other meaningful and common units that comprise more than one word play a key role in the organization of our linguistic knowledge. Importantly, multiword expressions straddle the line between lexicon and grammar, calling into question how useful this distinction is as a foundation for our understanding of language. Nonetheless, finding a replacement for the foundational role the words-and-rules approach has played in our theories is not straightforward. Thus, the second part of our article reviews and synthesizes the diverse approaches that have attempted to account for the central role of multiword expressions in language representation, acquisition, and processing.",
        "authors": [
            "Pablo Contreras Kallens",
            "Morten H. Christiansen"
        ],
        "citations": 16,
        "references": 186,
        "year": 2022
    },
    {
        "title": "A Tutorial on Fuzzy Time Series Forecasting Models: Recent Advances and Challenges",
        "abstract": "Abstract: Time series forecasting is a powerful tool in planning and decision making, from traditional statistical models to soft computing and artificial intelligence approaches several methods have been developed to generate increasingly accurate forecasts. Fuzzy Time Series (FTS) methods have been introduced in the early 1990’s to handle data uncertainty and to undercome the statistical assumptions of linearity. Many studies have been reporting their good accuracy, simplicity, potential for interpretability and reduced computational complexity. This paper presents a tutorial for FTS methods. First, a review of the relevant literature is made, offering a foundation on the main concepts and FTS-based models for different time series and different types of forecasts. Then, the current challenges and possible solutions, are discussed alongside a timeline of the research developed in this area by the authors that aims at filling some of these gaps. Finally, a tutorial on the pyFTS library is presented. PyFTS is an open and free library coded in Python programming language that was developed by the MINDS Lab (Laboratory of Machine Intelligence and Data Science) and, also provides a set of transformation functions for pre-processing time series and a set of metrics and databases for benchmarking, in addition to implementing several FTS models in the literature.",
        "authors": [
            "P. O. Lucas",
            "Omid Orang",
            "Petrônio C. L. Silva",
            "E. M. Mendes",
            "F. Guimarães"
        ],
        "citations": 11,
        "references": 172,
        "year": 2022
    },
    {
        "title": "Segment Anything",
        "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.",
        "authors": [
            "A. Kirillov",
            "Eric Mintun",
            "Nikhila Ravi",
            "Hanzi Mao",
            "Chloé Rolland",
            "Laura Gustafson",
            "Tete Xiao",
            "Spencer Whitehead",
            "A. Berg",
            "Wan-Yen Lo",
            "Piotr Dollár",
            "Ross B. Girshick"
        ],
        "citations": 1000,
        "references": 148,
        "year": 2023
    },
    {
        "title": "DINOv2: Learning Robust Visual Features without Supervision",
        "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.",
        "authors": [
            "M. Oquab",
            "Timothée Darcet",
            "Théo Moutakanni",
            "Huy Q. Vo",
            "Marc Szafraniec",
            "Vasil Khalidov",
            "Pierre Fernandez",
            "Daniel Haziza",
            "Francisco Massa",
            "Alaaeldin El-Nouby",
            "Mahmoud Assran",
            "Nicolas Ballas",
            "Wojciech Galuba",
            "Russ Howes",
            "Po-Yao (Bernie) Huang",
            "Shang-Wen Li",
            "Ishan Misra",
            "Michael G. Rabbat",
            "Vasu Sharma",
            "Gabriel Synnaeve",
            "Huijiao Xu",
            "H. Jégou",
            "J. Mairal",
            "Patrick Labatut",
            "Armand Joulin",
            "Piotr Bojanowski"
        ],
        "citations": 1000,
        "references": 140,
        "year": 2023
    },
    {
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
        "authors": [
            "Albert Gu",
            "Tri Dao"
        ],
        "citations": 1000,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
        "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{https://chat.lmsys.org}.",
        "authors": [
            "Wei-Lin Chiang",
            "Lianmin Zheng",
            "Ying Sheng",
            "Anastasios Nikolas Angelopoulos",
            "Tianle Li",
            "Dacheng Li",
            "Hao Zhang",
            "Banghua Zhu",
            "Michael Jordan",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "citations": 276,
        "references": 69,
        "year": 2024
    },
    {
        "title": "VideoChat: Chat-Centric Video Understanding",
        "abstract": "In this paper, we initiate an attempt of developing an end-to-end chat-centric video understanding system, coined as VideoChat. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we build a video-centric instruction dataset, composed of thousands of videos associated with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and captures causal relationships, providing a valuable asset for training our chat-centric video understanding system. Preliminary qualitative experiments demonstrate the potential of our system across a broad spectrum of video applications, which could serve as a simple prototype system for future research on chat-centric video understanding. Access our code and data at https://github.com/OpenGVLab/Ask-Anything",
        "authors": [
            "Kunchang Li",
            "Yinan He",
            "Yi Wang",
            "Yizhuo Li",
            "Wen Wang",
            "Ping Luo",
            "Yali Wang",
            "Limin Wang",
            "Yu Qiao"
        ],
        "citations": 392,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Segment Anything Model for Medical Image Segmentation: Current Applications and Future Directions",
        "abstract": "Due to the inherent flexibility of prompting, foundation models have emerged as the predominant force in the fields of natural language processing and computer vision. The recent introduction of the Segment Anything Model (SAM) signifies a noteworthy expansion of the prompt-driven paradigm into the domain of image segmentation, thereby introducing a plethora of previously unexplored capabilities. However, the viability of its application to medical image segmentation remains uncertain, given the substantial distinctions between natural and medical images. In this work, we provide a comprehensive overview of recent endeavors aimed at extending the efficacy of SAM to medical image segmentation tasks, encompassing both empirical benchmarking and methodological adaptations. Additionally, we explore potential avenues for future research directions in SAM's role within medical image segmentation. While direct application of SAM to medical image segmentation does not yield satisfactory performance on multi-modal and multi-target medical datasets so far, numerous insights gleaned from these efforts serve as valuable guidance for shaping the trajectory of foundational models in the realm of medical image analysis. To support ongoing research endeavors, we maintain an active repository that contains an up-to-date paper list and a succinct summary of open-source projects at https://github.com/YichiZhang98/SAM4MIS.",
        "authors": [
            "Yichi Zhang",
            "Zhenrong Shen",
            "Rushi Jiao"
        ],
        "citations": 70,
        "references": 104,
        "year": 2024
    },
    {
        "title": "Machine learning for functional protein design",
        "abstract": null,
        "authors": [
            "Pascal Notin",
            "Nathan J. Rollins",
            "Yarin Gal",
            "Chris Sander",
            "Debora Marks"
        ],
        "citations": 66,
        "references": 127,
        "year": 2024
    },
    {
        "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
        "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",
        "authors": [
            "Zhiheng Xi",
            "Wenxiang Chen",
            "Xin Guo",
            "Wei He",
            "Yiwen Ding",
            "Boyang Hong",
            "Ming Zhang",
            "Junzhe Wang",
            "Senjie Jin",
            "Enyu Zhou",
            "Rui Zheng",
            "Xiaoran Fan",
            "Xiao Wang",
            "Limao Xiong",
            "Qin Liu",
            "Yuhao Zhou",
            "Weiran Wang",
            "Changhao Jiang",
            "Yicheng Zou",
            "Xiangyang Liu",
            "Zhangyue Yin",
            "Shihan Dou",
            "Rongxiang Weng",
            "Wensen Cheng",
            "Qi Zhang",
            "Wenjuan Qin",
            "Yongyan Zheng",
            "Xipeng Qiu",
            "Xuanjing Huan",
            "Tao Gui"
        ],
        "citations": 615,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
        "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
        "authors": [
            "Alec Radford",
            "Jong Wook Kim",
            "Tao Xu",
            "Greg Brockman",
            "C. McLeavey",
            "I. Sutskever"
        ],
        "citations": 1000,
        "references": 100,
        "year": 2022
    },
    {
        "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
        "abstract": "Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-training on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner.",
        "authors": [
            "Limin Wang",
            "Bingkun Huang",
            "Zhiyu Zhao",
            "Zhan Tong",
            "Yinan He",
            "Yi Wang",
            "Yali Wang",
            "Yu Qiao"
        ],
        "citations": 260,
        "references": 122,
        "year": 2023
    },
    {
        "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
        "abstract": "Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM.",
        "authors": [
            "Wen Wang",
            "Zhe Chen",
            "Xiaokang Chen",
            "Jiannan Wu",
            "Xizhou Zhu",
            "Gang Zeng",
            "Ping Luo",
            "Tong Lu",
            "Jie Zhou",
            "Y. Qiao",
            "Jifeng Dai"
        ],
        "citations": 376,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
        "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
        "authors": [
            "Lorenz Kuhn",
            "Y. Gal",
            "Sebastian Farquhar"
        ],
        "citations": 185,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Adversarial Diffusion Distillation",
        "abstract": "We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality. We use score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps. ADD is the first method to unlock single-step, real-time image synthesis with foundation models. Code and weights available under https://github.com/Stability-AI/generative-models and https://huggingface.co/stabilityai/ .",
        "authors": [
            "Axel Sauer",
            "Dominik Lorenz",
            "A. Blattmann",
            "Robin Rombach"
        ],
        "citations": 219,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
        "abstract": "In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
        "authors": [
            "Jinze Bai",
            "Shuai Bai",
            "Shusheng Yang",
            "Shijie Wang",
            "Sinan Tan",
            "Peng Wang",
            "Junyang Lin",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "citations": 508,
        "references": 86,
        "year": 2023
    },
    {
        "title": "ORPO: Monolithic Preference Optimization without Reference Model",
        "abstract": "While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we revisit SFT in the context of preference alignment, emphasizing that a minor penalty for the disfavored style is sufficient for preference alignment. Building on this foundation, we introduce a straightforward reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the need for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models including Llama-2 Chat and Zephyr with more than 7B and 13B parameters: achieving up to 12.20% on AlpacaEval 2.0 (Figure 1), and 7.32 in MT-Bench (Table 2). We release code and model checkpoints for Mistral-ORPO-\\alpha (7B) and Mistral-ORPO-\\beta (7B).",
        "authors": [
            "Jiwoo Hong",
            "Noah Lee",
            "James Thorne"
        ],
        "citations": 119,
        "references": 67,
        "year": 2024
    },
    {
        "title": "VM-UNet: Vision Mamba UNet for Medical Image Segmentation",
        "abstract": "In the realm of medical image segmentation, both CNN-based and Transformer-based models have been extensively explored. However, CNNs exhibit limitations in long-range modeling capabilities, whereas Transformers are hampered by their quadratic computational complexity. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, leveraging state space models, we propose a U-shape architecture model for medical image segmentation, named Vision Mamba UNet (VM-UNet). Specifically, the Visual State Space (VSS) block is introduced as the foundation block to capture extensive contextual information, and an asymmetrical encoder-decoder structure is constructed with fewer convolution layers to save calculation cost. We conduct comprehensive experiments on the ISIC17, ISIC18, and Synapse datasets, and the results indicate that VM-UNet performs competitively in medical image segmentation tasks. To our best knowledge, this is the first medical image segmentation model constructed based on the pure SSM-based model. We aim to establish a baseline and provide valuable insights for the future development of more efficient and effective SSM-based segmentation systems. Our code is available at https://github.com/JCruan519/VM-UNet.",
        "authors": [
            "Jiacheng Ruan",
            "Suncheng Xiang"
        ],
        "citations": 150,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
        "abstract": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
        "authors": [
            "Subhabrata Mukherjee",
            "Arindam Mitra",
            "Ganesh Jawahar",
            "Sahaj Agarwal",
            "Hamid Palangi",
            "A. Awadallah"
        ],
        "citations": 238,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Plasma Fibrinogen as a Biomarker for Mortality and Hospitalized Exacerbations in People with COPD.",
        "abstract": "BACKGROUND\nIn 2010 the COPD Foundation established the COPD Biomarkers Qualification Consortium (CBQC) as a partnership between the Foundation, the Food and Drug Administration (FDA), and the pharmaceutical industry to pool publicly-funded and industry data to develop innovative tools to facilitate the development and approval of new therapies for COPD. We present data from the initial project seeking regulatory qualification of fibrinogen as a biomarker for the stratification of COPD patients into clinical trials.\n\n\nMETHODS\nThis analysis pooled data from 4 publicly-funded studies and 1 industry study into a common database resulting in 6376 individuals with spirometric evidence of COPD. We used a threshold of 350 mg/dL to determine high vs. low fibrinogen, and determined the subsequent risk of hospitalizations from exacerbations and death using Cox proportional hazards models.\n\n\nRESULTS\nHigh fibrinogen levels at baseline were present in 2853 (44.7%) of individuals with COPD. High fibrinogen was associated with an increased risk of hospitalized COPD exacerbations within 12 months (hazard ratio [HR]: 1.64; 95% confidence interval [CI]: 1.39-1.93) among participants in the Atherosclerosis Risk in Communities Study (ARIC), the Cardiovascular Health Study (CHS), and the Evaluation of COPD Longitudinally to Identify Predictive Surrogate Endpoints (ECLIPSE) study. High fibrinogen was associated with an increased risk of death within 36 months (HR: 1.94; 95% CI: 1.62-2.31) among all participants.\n\n\nCONCLUSIONS\nFibrinogen levels ≥ 350 mg/dL identify COPD individuals at an increased risk of exacerbations and death and could be a useful biomarker for enriching clinical trials in the COPD population.",
        "authors": [
            "jason. simeone",
            "jason. simeone"
        ],
        "citations": 99,
        "references": 43,
        "year": 2024
    },
    {
        "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
        "abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level, an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics for simple adaptation to novel tasks without updating pretrained model weights. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 17 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on all 8 datasets on average by +9 accuracy points.",
        "authors": [
            "Eric D Nguyen",
            "Michael Poli",
            "Marjan Faizi",
            "A. Thomas",
            "Callum Birch-Sykes",
            "Michael Wornow",
            "Aman Patel",
            "Clayton M. Rabideau",
            "Stefano Massaroli",
            "Y. Bengio",
            "Stefano Ermon",
            "S. Baccus",
            "Christopher Ré"
        ],
        "citations": 151,
        "references": 59,
        "year": 2023
    },
    {
        "title": "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
        "abstract": "Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific pre-defined vision tasks. Yet, existing systems can only handle videos with very few frames. For long videos, the computation complexity, memory cost, and long-term temporal connection impose additional challenges. Taking advantage of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with our specially designed memory mechanism, we propose the MovieChat to overcome these challenges. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video and 14K manual annotations for validation of the effectiveness of our method. The code, models and data can be found in https://reself.github.io/MovieChat.",
        "authors": [
            "Enxin Song",
            "Wenhao Chai",
            "Guanhong Wang",
            "Yucheng Zhang",
            "Haoyang Zhou",
            "Feiyang Wu",
            "Xun Guo",
            "Tianbo Ye",
            "Yang Lu",
            "Jenq-Neng Hwang",
            "Gaoang Wang"
        ],
        "citations": 147,
        "references": 141,
        "year": 2023
    },
    {
        "title": "Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling",
        "abstract": "Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance.",
        "authors": [
            "Yair Schiff",
            "Chia-Hsiang Kao",
            "Aaron Gokaslan",
            "Tri Dao",
            "Albert Gu",
            "Volodymyr Kuleshov"
        ],
        "citations": 42,
        "references": 63,
        "year": 2024
    },
    {
        "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
        "abstract": "We launch EVA, a vision-centric foundation model to Explore the limits of Visual representation at scAle using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVIS dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models.",
        "authors": [
            "Yuxin Fang",
            "Wen Wang",
            "Binhui Xie",
            "Quan-Sen Sun",
            "Ledell Yu Wu",
            "Xinggang Wang",
            "Tiejun Huang",
            "Xinlong Wang",
            "Yue Cao"
        ],
        "citations": 562,
        "references": 135,
        "year": 2022
    },
    {
        "title": "ConceptFusion: Open-set Multimodal 3D Mapping",
        "abstract": "Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping. For more information, visit our project page https://concept-fusion.github.io or watch our 5-minute explainer video https://www.youtube.com/watch?v=rkXgws8fiDs",
        "authors": [
            "Krishna Murthy Jatavallabhula",
            "Ali Kuwajerwala",
            "Qiao Gu",
            "Mohd. Omama",
            "Tao Chen",
            "Shuang Li",
            "Ganesh Iyer",
            "Soroush Saryazdi",
            "Nikhil Varma Keetha",
            "A. Tewari",
            "J. Tenenbaum",
            "Celso M. de Melo",
            "M. Krishna",
            "L. Paull",
            "F. Shkurti",
            "A. Torralba"
        ],
        "citations": 179,
        "references": 95,
        "year": 2023
    },
    {
        "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective",
        "abstract": "ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.",
        "authors": [
            "Jindong Wang",
            "Xixu Hu",
            "Wenxin Hou",
            "Hao Chen",
            "Runkai Zheng",
            "Yidong Wang",
            "Linyi Yang",
            "Haojun Huang",
            "Weirong Ye",
            "Xiubo Geng",
            "Binxing Jiao",
            "Yue Zhang",
            "Xingxu Xie"
        ],
        "citations": 200,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Recognize Anything: A Strong Image Tagging Model",
        "abstract": "We present the Recognize Anything Model (RAM): a strong foundation model for image tagging. RAM makes a substantial step for foundation models in computer vision, demonstrating the zero-shot ability to recognize any common category with high accuracy. By leveraging large-scale image-text pairs for training instead of manual annotations, RAM introduces a new paradigm for image tagging.The development of RAM comprises four key steps. Firstly, annotation-free image tags are obtained at scale through automatic text semantic parsing. Subsequently, a preliminary model is trained for automatic annotation by unifying the captioning and tagging tasks, supervised by the original texts and parsed tags, respectively. Thirdly, a data engine is employed to generate additional annotations and clean incorrect ones. Lastly, the model is retrained with the processed data and fine-tuned using a smaller but higher-quality dataset.We evaluate the tagging capability of RAM on numerous benchmarks and observe an impressive zero-shot performance, which significantly outperforms CLIP and BLIP. Remarkably, RAM even surpasses fully supervised models and exhibits a competitive performance compared with the Google tagging API. We have released RAM at https://recognize-anything.github.io/ to foster the advancement of foundation models in computer vision.",
        "authors": [
            "Youcai Zhang",
            "Xinyu Huang",
            "Jinyu Ma",
            "Zhaoyang Li",
            "Zhaochuan Luo",
            "Yanchun Xie",
            "Yuzhuo Qin",
            "Tong Luo",
            "Yaqian Li",
            "Siyi Liu",
            "Yandong Guo",
            "Lei Zhang"
        ],
        "citations": 161,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Three Epochs of Artificial Intelligence in Health Care.",
        "abstract": "Importance\nInterest in artificial intelligence (AI) has reached an all-time high, and health care leaders across the ecosystem are faced with questions about where, when, and how to deploy AI and how to understand its risks, problems, and possibilities.\n\n\nObservations\nWhile AI as a concept has existed since the 1950s, all AI is not the same. Capabilities and risks of various kinds of AI differ markedly, and on examination 3 epochs of AI emerge. AI 1.0 includes symbolic AI, which attempts to encode human knowledge into computational rules, as well as probabilistic models. The era of AI 2.0 began with deep learning, in which models learn from examples labeled with ground truth. This era brought about many advances both in people's daily lives and in health care. Deep learning models are task-specific, meaning they do one thing at a time, and they primarily focus on classification and prediction. AI 3.0 is the era of foundation models and generative AI. Models in AI 3.0 have fundamentally new (and potentially transformative) capabilities, as well as new kinds of risks, such as hallucinations. These models can do many different kinds of tasks without being retrained on a new dataset. For example, a simple text instruction will change the model's behavior. Prompts such as \"Write this note for a specialist consultant\" and \"Write this note for the patient's mother\" will produce markedly different content.\n\n\nConclusions and Relevance\nFoundation models and generative AI represent a major revolution in AI's capabilities, ffering tremendous potential to improve care. Health care leaders are making decisions about AI today. While any heuristic omits details and loses nuance, the framework of AI 1.0, 2.0, and 3.0 may be helpful to decision-makers because each epoch has fundamentally different capabilities and risks.",
        "authors": [
            "Michael D Howell",
            "G. Corrado",
            "Karen B. DeSalvo"
        ],
        "citations": 43,
        "references": 13,
        "year": 2024
    },
    {
        "title": "End-to-End Autonomous Driving: Challenges and Frontiers",
        "abstract": "The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework.",
        "authors": [
            "Li Chen",
            "Peng Wu",
            "Kashyap Chitta",
            "Bernhard Jaeger",
            "Andreas Geiger",
            "Hongyang Li"
        ],
        "citations": 161,
        "references": 317,
        "year": 2023
    },
    {
        "title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding",
        "abstract": "With the success of large language models (LLMs), integrating the vision model into LLMs to build vision-language foundation models has gained much more interest recently. However, existing LLM-based large multimodal models (e.g., Video-LLaMA, VideoChat) can only take in a limited number of frames for short video understanding. In this study, we mainly focus on designing an efficient and effective model for long-term video understanding. Instead of trying to process more frames simultaneously like most existing work, we propose to process videos in an online manner and store past video information in a memory bank. This allows our model to reference historical video content for long-term analysis without exceeding LLMs' context length constraints or GPU memory limits. Our memory bank can be seamlessly integrated into current multimodal LLMs in an off-the-shelf manner. We conduct extensive experiments on various video understanding tasks, such as long-video understanding, video question answering, and video captioning, and our model can achieve state-of-the-art performances across multiple datasets.",
        "authors": [
            "Bo He",
            "Hengduo Li",
            "Young Kyun Jang",
            "Menglin Jia",
            "Xuefei Cao",
            "Ashish Shah",
            "Abhinav Shrivastava",
            "Ser-Nam Lim"
        ],
        "citations": 38,
        "references": 92,
        "year": 2024
    },
    {
        "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
        "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT",
        "authors": [
            "Rongjie Huang",
            "Mingze Li",
            "Dongchao Yang",
            "Jiatong Shi",
            "Xuankai Chang",
            "Zhenhui Ye",
            "Yuning Wu",
            "Zhiqing Hong",
            "Jia-Bin Huang",
            "Jinglin Liu",
            "Yixiang Ren",
            "Zhou Zhao",
            "Shinji Watanabe"
        ],
        "citations": 156,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?",
        "abstract": "We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.",
        "authors": [
            "Arjun Majumdar",
            "Karmesh Yadav",
            "Sergio Arnaud",
            "Yecheng Jason Ma",
            "Claire Chen",
            "Sneha Silwal",
            "Aryan Jain",
            "Vincent-Pierre Berges",
            "P. Abbeel",
            "J. Malik",
            "Dhruv Batra",
            "Yixin Lin",
            "Oleksandr Maksymets",
            "A. Rajeswaran",
            "Franziska Meier"
        ],
        "citations": 133,
        "references": 83,
        "year": 2023
    },
    {
        "title": "VideoAgent: Long-form Video Understanding with Large Language Model as Agent",
        "abstract": "Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.",
        "authors": [
            "Xiaohan Wang",
            "Yuhui Zhang",
            "Orr Zohar",
            "S. Yeung-Levy"
        ],
        "citations": 35,
        "references": 70,
        "year": 2024
    },
    {
        "title": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning",
        "abstract": "For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. To explore the full scope of our experiments and results, we encourage readers to visit our project webpage.",
        "authors": [
            "Qiao Gu",
            "Ali Kuwajerwala",
            "Sacha Morin",
            "Krishna Murthy Jatavallabhula",
            "Bipasha Sen",
            "Aditya Agarwal",
            "Corban Rivera",
            "William Paul",
            "Kirsty Ellis",
            "Ramalingam Chellappa",
            "Chuang Gan",
            "C. D. Melo",
            "J. B. Tenenbaum",
            "Antonio Torralba",
            "F. Shkurti",
            "L. Paull"
        ],
        "citations": 119,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Agent AI: Surveying the Horizons of Multimodal Interaction",
        "abstract": "Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define\"Agent AI\"as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, and can produce meaningful embodied actions. In particular, we explore systems that aim to improve agents based on next-embodied action prediction by incorporating external knowledge, multi-sensory inputs, and human feedback. We argue that by developing agentic AI systems in grounded environments, one can also mitigate the hallucinations of large foundation models and their tendency to generate environmentally incorrect outputs. The emerging field of Agent AI subsumes the broader embodied and agentic aspects of multimodal interactions. Beyond agents acting and interacting in the physical world, we envision a future where people can easily create any virtual reality or simulated scene and interact with agents embodied within the virtual environment.",
        "authors": [
            "Zane Durante",
            "Qiuyuan Huang",
            "Naoki Wake",
            "Ran Gong",
            "J. Park",
            "Bidipta Sarkar",
            "Rohan Taori",
            "Yusuke Noda",
            "D. Terzopoulos",
            "Yejin Choi",
            "Katsushi Ikeuchi",
            "Hoi Vo",
            "Fei-Fei Li",
            "Jianfeng Gao"
        ],
        "citations": 36,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively",
        "abstract": null,
        "authors": [
            "Haobo Yuan",
            "Xiangtai Li",
            "Chong Zhou",
            "Yining Li",
            "Kai Chen",
            "Chen Change Loy"
        ],
        "citations": 36,
        "references": 93,
        "year": 2024
    },
    {
        "title": "Bilingual language model for protein sequence and structure",
        "abstract": "Adapting large language models (LLMs) to protein sequences spawned the development of powerful protein language models (pLMs). Concurrently, AlphaFold2 broke through in protein structure prediction. Now we can systematically and comprehensively explore the dual nature of proteins that act and exist as three-dimensional (3D) machines and evolve as linear strings of one-dimensional (1D) sequences. Here, we leverage pLMs to simultaneously model both modalities by combining 1D sequences with 3D structure in a single model. We encode protein structures as token sequences using the 3Di-alphabet introduced by the 3D-alignment method Foldseek. This new foundation pLM extracts the features and patterns of the resulting “structure-sequence” representation. Toward this end, we built a non-redundant dataset from AlphaFoldDB and fine-tuned an existing pLM (ProtT5) to translate between 3Di and amino acid sequences. As a proof-of-concept for our novel approach, dubbed Protein structure-sequence T5 (ProstT5), we showed improved performance for subsequent prediction tasks, and for “inverse folding”, namely the generation of novel protein sequences adopting a given structural scaffold (“fold”). Our work showcased the potential of pLMs to tap into the information-rich protein structure revolution fueled by AlphaFold2. ProstT5 paves the way to develop new tools integrating the vast resource of 3D predictions, and opens new research avenues in the post-AlphaFold2 era. Our model is freely available for all at https://github.com/mheinzinger/ProstT5.",
        "authors": [
            "M. Heinzinger",
            "Konstantin Weissenow",
            "Joaquin Gomez Sanchez",
            "Adrian Henkel",
            "Martin Steinegger",
            "B. Rost"
        ],
        "citations": 62,
        "references": 99,
        "year": 2024
    },
    {
        "title": "SVIT: Scaling up Visual Instruction Tuning",
        "abstract": "Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.",
        "authors": [
            "Bo Zhao",
            "Boya Wu",
            "Tiejun Huang"
        ],
        "citations": 106,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
        "abstract": "Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.",
        "authors": [
            "Alexandre Ramé",
            "Guillaume Couairon",
            "Mustafa Shukor",
            "Corentin Dancette",
            "Jean-Baptiste Gaya",
            "L. Soulier",
            "M. Cord"
        ],
        "citations": 100,
        "references": 217,
        "year": 2023
    },
    {
        "title": "Scaling Robot Learning with Semantically Imagined Experience",
        "abstract": "Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. One of the key contributing factors to this progress is the scale of robot data used to train the models. To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which are challenging to scale. To mitigate this issue, we propose an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. We term our method Robot Learning with Semantically Imagened Experience (ROSIE). Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting various unseen objects for manipulation, backgrounds, and distractors with text guidance. Through extensive real-world experiments, we show that manipulation policies trained on data augmented this way are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation. The project's website and videos can be found at diffusion-rosie.github.io",
        "authors": [
            "Tianhe Yu",
            "Ted Xiao",
            "Austin Stone",
            "Jonathan Tompson",
            "Anthony Brohan",
            "Su Wang",
            "Jaspiar Singh",
            "Clayton Tan",
            "M. Dee",
            "Jodilyn Peralta",
            "Brian Ichter",
            "Karol Hausman",
            "F. Xia"
        ],
        "citations": 117,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Language Modeling Is Compression",
        "abstract": "It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.",
        "authors": [
            "Gr'egoire Del'etang",
            "Anian Ruoss",
            "Paul-Ambroise Duquenne",
            "Elliot Catt",
            "Tim Genewein",
            "Christopher Mattern",
            "Jordi Grau-Moya",
            "Wenliang Kevin Li",
            "Matthew Aitchison",
            "Laurent Orseau",
            "Marcus Hutter",
            "J. Veness"
        ],
        "citations": 100,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
        "abstract": "We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.",
        "authors": [
            "Jinheng Xie",
            "Weijia Mao",
            "Zechen Bai",
            "David Junhao Zhang",
            "Weihao Wang",
            "Kevin Qinghong Lin",
            "Yuchao Gu",
            "Zhijie Chen",
            "Zhenheng Yang",
            "Mike Zheng Shou"
        ],
        "citations": 53,
        "references": 81,
        "year": 2024
    },
    {
        "title": "Sequence modeling and design from molecular to genome scale with Evo",
        "abstract": "The genome is a sequence that completely encodes the DNA, RNA, and proteins that orchestrate the function of a whole organism. Advances in machine learning combined with massive datasets of whole genomes could enable a biological foundation model that accelerates the mechanistic understanding and generative design of complex molecular interactions. We report Evo, a genomic foundation model that enables prediction and generation tasks from the molecular to genome scale. Using an architecture based on advances in deep signal processing, we scale Evo to 7 billion parameters with a context length of 131 kilobases (kb) at single-nucleotide, byte resolution. Trained on 2.7M prokaryotic and phage genomes, Evo can generalize across the three fundamental modalities of the central dogma of molecular biology to perform zero-shot function prediction that is competitive with, or outperforms, leading domain-specific language models. Evo also excels at multi-element generation tasks, which we demonstrate by generating synthetic CRISPR-Cas molecular complexes and entire transposable systems for the first time. Using information learned over whole genomes, Evo can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to 650 kb in length, orders of magnitude longer than previous methods. Advances in multi-modal and multiscale learning with Evo provides a promising path toward improving our understanding and control of biology across multiple levels of complexity.",
        "authors": [
            "Eric Nguyen",
            "Michael Poli",
            "Matthew G. Durrant",
            "Armin W. Thomas",
            "Brian Kang",
            "Jeremy Sullivan",
            "Madelena Y Ng",
            "Ashley Lewis",
            "Aman Patel",
            "Aaron Lou",
            "Stefano Ermon",
            "S. Baccus",
            "Tina Hernandez-Boussard",
            "Christopher Ré",
            "Patrick D. Hsu",
            "Brian L. Hie"
        ],
        "citations": 53,
        "references": 0,
        "year": 2024
    },
    {
        "title": "PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents",
        "abstract": "Foundation models trained on large-scale dataset gain a recent surge in CV and NLP. In contrast, development in biomedical domain lags far behind due to data scarcity. To address this issue, we build and release PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, which is 8 times larger than before. PMC-OA covers diverse modalities or diseases, with majority of the image-caption samples aligned at finer-grained level, i.e., subfigure and subcaption. While pretraining a CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art results on various downstream tasks, including image-text retrieval on ROCO, MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text retrieval, +3.9% accuracy on image classification.",
        "authors": [
            "Weixiong Lin",
            "Ziheng Zhao",
            "Xiaoman Zhang",
            "Chaoyi Wu",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "citations": 94,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields",
        "abstract": "3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework encounters significant challenges, notably the disparities in spatial resolution and channel consistency between RGB images and feature maps. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model. Project website at: https://feature-3dgs.github.io/.",
        "authors": [
            "Shijie Zhou",
            "Haoran Chang",
            "Sicheng Jiang",
            "Zhiwen Fan",
            "Zehao Zhu",
            "Dejia Xu",
            "Pradyumna Chari",
            "Suya You",
            "Zhangyang Wang",
            "A. Kadambi"
        ],
        "citations": 91,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Is Mamba Capable of In-Context Learning?",
        "abstract": "State of the art foundation models such as GPT-4 perform surprisingly well at in-context learning (ICL), a variant of meta-learning concerning the learned ability to solve tasks during a neural network forward pass, exploiting contextual information provided as input to the model. This useful ability emerges as a side product of the foundation model's massive pretraining. While transformer models are currently the state of the art in ICL, this work provides empirical evidence that Mamba, a newly proposed state space model which scales better than transformers w.r.t. the input sequence length, has similar ICL capabilities. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that, across both categories of tasks, Mamba closely matches the performance of transformer models for ICL. Further analysis reveals that, like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving long input sequences. This is an exciting finding in meta-learning and may enable generalizations of in-context learned AutoML algorithms (like TabPFN or Optformer) to long input sequences.",
        "authors": [
            "Riccardo Grazzi",
            "Julien N. Siems",
            "Simon Schrodi",
            "Thomas Brox",
            "Frank Hutter"
        ],
        "citations": 33,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
        "abstract": "Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.",
        "authors": [
            "Adaptive Agent Team",
            "Jakob Bauer",
            "Kate Baumli",
            "Satinder Baveja",
            "Feryal M. P. Behbahani",
            "Avishkar Bhoopchand",
            "N. Bradley-Schmieg",
            "Michael Chang",
            "Natalie Clay",
            "Adrian Collister",
            "Vibhavari Dasagi",
            "Lucy Gonzalez",
            "Karol Gregor",
            "Edward Hughes",
            "Sheleem Kashem",
            "Maria Loks-Thompson",
            "Hannah Openshaw",
            "Jack Parker-Holder",
            "Shreyaan Pathak",
            "Nicolas Perez Nieves",
            "Nemanja Rakicevic",
            "Tim Rocktäschel",
            "Yannick Schroecker",
            "Jakub Sygnowski",
            "K. Tuyls",
            "Sarah York",
            "Alexander Zacherl",
            "Lei M. Zhang"
        ],
        "citations": 95,
        "references": 133,
        "year": 2023
    },
    {
        "title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding",
        "abstract": "We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos. In particular, the proposed multimodal agent VideoAgent: 1) constructs a structured memory to store both the generic temporal event descriptions and object-centric tracking states of the video; 2) given an input task query, it employs tools including video segment localization and object memory querying along with other visual foundation models to interactively solve the task, utilizing the zero-shot tool-use ability of LLMs. VideoAgent demonstrates impressive performances on several long-horizon video understanding benchmarks, an average increase of 6.6% on NExT-QA and 26.0% on EgoSchema over baselines, closing the gap between open-sourced models and private counterparts including Gemini 1.5 Pro.",
        "authors": [
            "Yue Fan",
            "Xiaojian Ma",
            "Rujie Wu",
            "Yuntao Du",
            "Jiaqi Li",
            "Zhi Gao",
            "Qing Li"
        ],
        "citations": 29,
        "references": 50,
        "year": 2024
    },
    {
        "title": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild",
        "abstract": "Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions, we augment pretrained text-to-image diffusion models and introduce a task-aware HyperNet to modulate the diffusion models, enabling the adaptation to different C2I tasks simultaneously. Trained on nine unique C2I tasks, UniControl demonstrates impressive zero-shot generation abilities with unseen visual conditions. Experimental results show that UniControl often surpasses the performance of single-task-controlled methods of comparable model sizes. This control versatility positions UniControl as a significant advancement in the realm of controllable visual generation.",
        "authors": [
            "Can Qin",
            "Shu Zhang",
            "Ning Yu",
            "Yihao Feng",
            "Xinyi Yang",
            "Yingbo Zhou",
            "Haiquan Wang",
            "Juan Carlos Niebles",
            "Caiming Xiong",
            "S. Savarese",
            "Stefano Ermon",
            "Yun Fu",
            "Ran Xu"
        ],
        "citations": 85,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Frontier AI Regulation: Managing Emerging Risks to Public Safety",
        "abstract": "Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term\"frontier AI\"models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. We consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier AI models. Finally, we propose an initial set of safety standards. These include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. We hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of AI development.",
        "authors": [
            "Markus Anderljung",
            "Joslyn Barnhart",
            "Anton Korinek",
            "Jade Leung",
            "Cullen O'Keefe",
            "Jess Whittlestone",
            "S. Avin",
            "Miles Brundage",
            "Justin B. Bullock",
            "D. Cass-Beggs",
            "Ben Chang",
            "Tantum Collins",
            "Tim Fist",
            "Gillian K. Hadfield",
            "Alan Hayes",
            "Lewis Ho",
            "Sara Hooker",
            "Eric Horvitz",
            "Noam Kolt",
            "Jonas Schuett",
            "Yonadav Shavit",
            "Divya Siddarth",
            "Robert F. Trager",
            "Kevin J. Wolf"
        ],
        "citations": 93,
        "references": 249,
        "year": 2023
    },
    {
        "title": "SAM-Adapter: Adapting Segment Anything in Underperformed Scenes",
        "abstract": "The emergence of large models, also known as foundation models, has brought significant advancements to AI research. One such model is Segment Anything (SAM), which is designed for image segmentation tasks. However, as with other foundation models, our experimental findings suggest that SAM may fail or perform poorly in certain segmentation tasks, such as shadow detection and camouflaged object detection (concealed object detection). This study first paves the way for applying the large pre-trained image segmentation model SAM to these downstream tasks, even in situations where SAM performs poorly. Rather than fine-tuning the SAM network, we propose SAM-Adapter, which incorporates domain-specific information or visual prompts into the segmentation network by using simple yet effective adapters. By integrating task-specific knowledge with general knowledge learnt by the large model, SAM-Adapter can significantly elevate the performance of SAM in challenging tasks as shown in extensive experiments. We can even outperform task-specific network models and achieve state-of-the-art performance in the task we tested: camouflaged object detection, shadow detection. Our code of adapting SAM in downstream applications have been released publicly at https://github.com/tianrun-chen/SAM-Adapter-PyTorch/ and has benefited many researchers. We believe our work opens up opportunities for utilizing SAM in downstream tasks, with potential applications in various fields, including medical image processing, agriculture, remote sensing, and more.",
        "authors": [
            "Tianrun Chen",
            "Lanyun Zhu",
            "Chao Ding",
            "Runlong Cao",
            "Yan Wang",
            "Shangzhan Zhang",
            "Zejian Li",
            "Lingyun Sun",
            "Ying-Dong Zang",
            "Papa Mao"
        ],
        "citations": 90,
        "references": 56,
        "year": 2023
    },
    {
        "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
        "abstract": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.",
        "authors": [
            "Haoyu Zhen",
            "Xiaowen Qiu",
            "Peihao Chen",
            "Jincheng Yang",
            "Xin Yan",
            "Yilun Du",
            "Yining Hong",
            "Chuang Gan"
        ],
        "citations": 32,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Segment anything in medical images",
        "abstract": null,
        "authors": [
            "Jun Ma",
            "Yuting He",
            "Feifei Li",
            "Li-Jun Han",
            "Chenyu You",
            "Bo Wang"
        ],
        "citations": 267,
        "references": 139,
        "year": 2023
    },
    {
        "title": "SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model",
        "abstract": "Foundation models have taken over natural language processing and image generation domains due to the flexibility of prompting. With the recent introduction of the Segment Anything Model (SAM), this prompt-driven paradigm has entered image segmentation with a hitherto unexplored abundance of capabilities. The purpose of this paper is to conduct an initial evaluation of the out-of-the-box zero-shot capabilities of SAM for medical image segmentation, by evaluating its performance on an abdominal CT organ segmentation task, via point or bounding box based prompting. We show that SAM generalizes well to CT data, making it a potential catalyst for the advancement of semi-automatic segmentation tools for clinicians. We believe that this foundation model, while not reaching state-of-the-art segmentation performance in our investigations, can serve as a highly potent starting point for further adaptations of such models to the intricacies of the medical domain. Keywords: medical image segmentation, SAM, foundation models, zero-shot learning",
        "authors": [
            "Saikat Roy",
            "Tassilo Wald",
            "Gregor Koehler",
            "Maximilian R. Rokuss",
            "Nico Disch",
            "J. Holzschuh",
            "David Zimmerer",
            "K. Maier-Hein"
        ],
        "citations": 88,
        "references": 9,
        "year": 2023
    },
    {
        "title": "GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image",
        "abstract": "We introduce GeoWizard, a new generative foundation model designed for estimating geometric attributes, e.g., depth and normals, from single images. While significant research has already been conducted in this area, the progress has been substantially limited by the low diversity and poor quality of publicly available datasets. As a result, the prior works either are constrained to limited scenarios or suffer from the inability to capture geometric details. In this paper, we demonstrate that generative models, as opposed to traditional discriminative models (e.g., CNNs and Transformers), can effectively address the inherently ill-posed problem. We further show that leveraging diffusion priors can markedly improve generalization, detail preservation, and efficiency in resource usage. Specifically, we extend the original stable diffusion model to jointly predict depth and normal, allowing mutual information exchange and high consistency between the two representations. More importantly, we propose a simple yet effective strategy to segregate the complex data distribution of various scenes into distinct sub-distributions. This strategy enables our model to recognize different scene layouts, capturing 3D geometry with remarkable fidelity. GeoWizard sets new benchmarks for zero-shot depth and normal prediction, significantly enhancing many downstream applications such as 3D reconstruction, 2D content creation, and novel viewpoint synthesis.",
        "authors": [
            "Xiao Fu",
            "Wei Yin",
            "Mu Hu",
            "Kaixuan Wang",
            "Yuexin Ma",
            "Ping Tan",
            "Shaojie Shen",
            "Dahua Lin",
            "Xiaoxiao Long"
        ],
        "citations": 44,
        "references": 80,
        "year": 2024
    },
    {
        "title": "SAM struggles in concealed scenes — empirical study on “Segment Anything”",
        "abstract": null,
        "authors": [
            "Ge-Peng Ji",
            "Deng-Ping Fan",
            "Peng Xu",
            "Ming-Ming Cheng",
            "Bowen Zhou",
            "L. Gool"
        ],
        "citations": 86,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
        "abstract": "In recent years, the application of multimodal large language models (MLLM) in various fields has achieved remarkable success. However, as the foundation model for many downstream tasks, current MLLMs are composed of the well-known Transformer network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective multi-modal Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction benchmarks show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.",
        "authors": [
            "Han Zhao",
            "Min Zhang",
            "Wei Zhao",
            "Pengxiang Ding",
            "Siteng Huang",
            "Donglin Wang"
        ],
        "citations": 46,
        "references": 61,
        "year": 2024
    },
    {
        "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
        "abstract": "Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.",
        "authors": [
            "Rachit Bansal",
            "Bidisha Samanta",
            "Siddharth Dalmia",
            "Nitish Gupta",
            "Shikhar Vashishth",
            "Sriram Ganapathy",
            "Abhishek Bapna",
            "Prateek Jain",
            "P. Talukdar"
        ],
        "citations": 26,
        "references": 31,
        "year": 2024
    },
    {
        "title": "Knowledge-enhanced visual-language pre-training on chest radiology images",
        "abstract": null,
        "authors": [
            "Xiaoman Zhang",
            "Chaoyi Wu",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "citations": 83,
        "references": 74,
        "year": 2023
    },
    {
        "title": "OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection",
        "abstract": "Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and scope. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate and standardized evaluation of OOD detection methodologies at large scale. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale data sets (ImageNet) and foundation models (e.g., CLIP and DINOv2), and expands its scope to investigate full-spectrum OOD detection which considers semantic and covariate distribution shifts at the same time. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool of OOD detection methodologies. With these enhancements, OpenOOD v1.5 aims to drive advancements and offer a more robust and comprehensive evaluation benchmark for OOD detection research.",
        "authors": [
            "Jingyang Zhang",
            "Jingkang Yang",
            "Pengyun Wang",
            "Haoqi Wang",
            "Yueqian Lin",
            "H. Zhang",
            "Yiyou Sun",
            "Xuefeng Du",
            "Kaiyang Zhou",
            "Wayne Zhang",
            "Yixuan Li",
            "Ziwei Liu",
            "Yiran Chen",
            "H. Li"
        ],
        "citations": 82,
        "references": 96,
        "year": 2023
    },
    {
        "title": "ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs",
        "abstract": "With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models. Codes and data are available at https://github.com/NineAbyss/ZeroG.",
        "authors": [
            "Yuhan Li",
            "Peisong Wang",
            "Zhixun Li",
            "Jeffrey Xu Yu",
            "Jia Li"
        ],
        "citations": 24,
        "references": 70,
        "year": 2024
    },
    {
        "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
        "abstract": "The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: https://vita-home.github.io.",
        "authors": [
            "Chaoyou Fu",
            "Haojia Lin",
            "Zuwei Long",
            "Yunhang Shen",
            "Meng Zhao",
            "Yi-Fan Zhang",
            "Xiong Wang",
            "Di Yin",
            "Long Ma",
            "Xiawu Zheng",
            "Ran He",
            "Rongrong Ji",
            "Yunsheng Wu",
            "Caifeng Shan",
            "Xing Sun"
        ],
        "citations": 37,
        "references": 62,
        "year": 2024
    },
    {
        "title": "LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs",
        "abstract": "In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing on these innovations, LLM4TS has yielded state-of-the-art results in long-term forecasting. Our model has also shown exceptional capabilities as both a robust representation learner and an effective few-shot learner, thanks to the knowledge transferred from the pre-trained LLM.",
        "authors": [
            "Ching Chang",
            "Wenjie Peng",
            "Tien-Fu Chen"
        ],
        "citations": 81,
        "references": 24,
        "year": 2023
    },
    {
        "title": "CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation",
        "abstract": "Open-vocabulary semantic segmentation presents the challenge of labeling each pixel within an image based on a wide range of text descriptions. In this work, we introduce a novel cost-based approach to adapt vision-language foundation models, notably CLIP, for the intricate task of semantic segmentation. Through aggregating the cosine similarity score, i. e., the cost volume between image and text embeddings, our method potently adapts CLIP for segmenting seen and unseen classes by fine-tuning its encoders, addressing the challenges faced by existing methods in handling unseen classes. Building upon this, we explore methods to effectively aggregate the cost volume considering its multi-modal nature of being established between image and text embeddings. Furthermore, we examine various methods for efficiently fine-tuning CLIP.",
        "authors": [
            "Seokju Cho",
            "Heeseong Shin",
            "Sung‐Jin Hong",
            "Seungjun An",
            "Seungjun Lee",
            "Anurag Arnab",
            "P. H. Seo",
            "Seung Wook Kim"
        ],
        "citations": 71,
        "references": 115,
        "year": 2023
    },
    {
        "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
        "abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.",
        "authors": [
            "Chengyue Wu",
            "Yukang Gan",
            "Yixiao Ge",
            "Zeyu Lu",
            "Jiahao Wang",
            "Ye Feng",
            "Ping Luo",
            "Ying Shan"
        ],
        "citations": 39,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation",
        "abstract": "Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.",
        "authors": [
            "Bokui (William) Shen",
            "Ge Yang",
            "Alan Yu",
            "J. Wong",
            "L. Kaelbling",
            "Phillip Isola"
        ],
        "citations": 70,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model",
        "abstract": "Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.",
        "authors": [
            "Siyuan Huang",
            "Zhengkai Jiang",
            "Hao Dong",
            "Y. Qiao",
            "Peng Gao",
            "Hongsheng Li"
        ],
        "citations": 77,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Valley: Video Assistant with Large Language model Enhanced abilitY",
        "abstract": "Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced abilitY. The Valley consists of a LLM, a temporal modeling module, a visual encoder, and a simple projection module designed to bridge visual and textual modes. To empower Valley with video comprehension and instruction-following capabilities, we construct a video instruction dataset and adopt a two-stage tuning procedure to train it. Specifically, we employ ChatGPT to facilitate the construction of task-oriented conversation data encompassing various tasks, including multi-shot captions, long video descriptions, action recognition, causal relationship inference, etc. Subsequently, we adopt a pre-training-then-instructions-tuned pipeline to align visual and textual modalities and improve the instruction-following capability of Valley. Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.",
        "authors": [
            "Ruipu Luo",
            "Ziwang Zhao",
            "Min Yang",
            "Junwei Dong",
            "Ming-Hui Qiu",
            "Pengcheng Lu",
            "Tao Wang",
            "Zhongyu Wei"
        ],
        "citations": 140,
        "references": 39,
        "year": 2023
    },
    {
        "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
        "abstract": "We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/",
        "authors": [
            "D. Kondratyuk",
            "Lijun Yu",
            "Xiuye Gu",
            "José Lezama",
            "Jonathan Huang",
            "Rachel Hornung",
            "Hartwig Adam",
            "Hassan Akbari",
            "Y. Alon",
            "Vighnesh Birodkar",
            "Yong Cheng",
            "Ming-Chang Chiu",
            "Josh Dillon",
            "Irfan Essa",
            "Agrim Gupta",
            "Meera Hahn",
            "Anja Hauth",
            "David Hendon",
            "Alonso Martinez",
            "David C. Minnen",
            "David A. Ross",
            "Grant Schindler",
            "Mikhail Sirotenko",
            "Kihyuk Sohn",
            "Krishna Somandepalli",
            "Huisheng Wang",
            "Jimmy Yan",
            "Ming Yang",
            "Xuan Yang",
            "Bryan Seybold",
            "Lu Jiang"
        ],
        "citations": 153,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Advanced Mechanics of Structures",
        "abstract": "Fundamental aspects of engineering mechanics stresses in beams of uniform and variable cross sections large and small deformations of beams of uniform and variable cross section beams and plates on elastic foundation elastic and inelastic analysis of beams and plates stability and instability of structural and mechanical systems experimental investigations and preparation of test models additional popular subjects and methods of analysis. Appendices: basic rules of matrix algebra computer program student critique on history of mechanics in section 7.2 references answers to selected problems. (Part contents)",
        "authors": [
            "D. Fertis"
        ],
        "citations": 34,
        "references": 0,
        "year": 2024
    },
    {
        "title": "SpeechVerse: A Large-scale Generalizable Audio Language Model",
        "abstract": "Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation. We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training. The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions. We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks. Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks. Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks.",
        "authors": [
            "Nilaksh Das",
            "Saket Dingliwal",
            "S. Ronanki",
            "Rohit Paturi",
            "David Huang",
            "Prashant Mathur",
            "Jie Yuan",
            "Dhanush Bekal",
            "Xing Niu",
            "Sai Muralidhar Jayanthi",
            "Xilai Li",
            "Karel Mundnich",
            "Monica Sunkara",
            "S. Srinivasan",
            "Kyu J Han",
            "Katrin Kirchhoff"
        ],
        "citations": 22,
        "references": 54,
        "year": 2024
    },
    {
        "title": "A Comprehensive Survey on Segment Anything Model for Vision and Beyond",
        "abstract": "Artificial intelligence (AI) is evolving towards artificial general intelligence, which refers to the ability of an AI system to perform a wide range of tasks and exhibit a level of intelligence similar to that of a human being. This is in contrast to narrow or specialized AI, which is designed to perform specific tasks with a high degree of efficiency. Therefore, it is urgent to design a general class of models, which we term foundation models, trained on broad data that can be adapted to various downstream tasks. The recently proposed segment anything model (SAM) has made significant progress in breaking the boundaries of segmentation, greatly promoting the development of foundation models for computer vision. To fully comprehend SAM, we conduct a survey study. As the first to comprehensively review the progress of segmenting anything task for vision and beyond based on the foundation model of SAM, this work focuses on its applications to various tasks and data types by discussing its historical development, recent progress, and profound impact on broad applications. We first introduce the background and terminology for foundation models including SAM, as well as state-of-the-art methods contemporaneous with SAM that are significant for segmenting anything task. Then, we analyze and summarize the advantages and limitations of SAM across various image processing applications, including software scenes, real-world scenes, and complex scenes. Importantly, many insights are drawn to guide future research to develop more versatile foundation models and improve the architecture of SAM. We also summarize massive other amazing applications of SAM in vision and beyond. Finally, we maintain a continuously updated paper list and an open-source project summary for foundation model SAM at \\href{https://github.com/liliu-avril/Awesome-Segment-Anything}{\\color{magenta}{here}}.",
        "authors": [
            "Chunhui Zhang",
            "Li Liu",
            "Yawen Cui",
            "Guanjie Huang",
            "Weilin Lin",
            "Yiqian Yang",
            "Yuehong Hu"
        ],
        "citations": 65,
        "references": 223,
        "year": 2023
    },
    {
        "title": "Segment Any Anomaly without Training via Hybrid Prompt Regularization",
        "abstract": "We present a novel framework, i.e., Segment Any Anomaly + (SAA+), for zero-shot anomaly segmentation with hybrid prompt regularization to improve the adaptability of modern foundation models. Existing anomaly segmentation models typically rely on domain-specific fine-tuning, limiting their generalization across countless anomaly patterns. In this work, inspired by the great zero-shot generalization ability of foundation models like Segment Anything, we first explore their assembly to leverage diverse multi-modal prior knowledge for anomaly localization. For non-parameter foundation model adaptation to anomaly segmentation, we further introduce hybrid prompts derived from domain expert knowledge and target image context as regularization. Our proposed SAA+ model achieves state-of-the-art performance on several anomaly segmentation benchmarks, including VisA, MVTec-AD, MTD, and KSDD2, in the zero-shot setting. We will release the code at \\href{https://github.com/caoyunkang/Segment-Any-Anomaly}{https://github.com/caoyunkang/Segment-Any-Anomaly}.",
        "authors": [
            "Yunkang Cao",
            "Xiaohao Xu",
            "Chen Sun",
            "Y. Cheng",
            "Zongwei Du",
            "Liang Gao",
            "Weiming Shen"
        ],
        "citations": 65,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Transportation 5.0: The DAO to Safe, Secure, and Sustainable Intelligent Transportation Systems",
        "abstract": "In 2014, IEEE Intelligent Transportation Systems Society established a Technical Committee on Transportation 5.0 with the mission of promoting and transforming the deployment of advanced and innovative technologies, especially Artificial Intelligence in transportation. This paper briefly summarizes our main research and findings over the last decade. Transportation Foundation Models, Transportation Scenarios Engineering, and Transportation Operating Systems have been identified as the main directions for the research and development of next-generation intelligent transportation systems.",
        "authors": [
            "Fei Wang",
            "Yilun Lin",
            "Petros A. Ioannou",
            "L. Vlacic",
            "Xiaoming Liu",
            "A. Eskandarian",
            "Yisheng Lv",
            "X. Na",
            "D. Cebon",
            "Jiaqi Ma",
            "Lingxi Li",
            "Cristina Olaverri-Monreal"
        ],
        "citations": 65,
        "references": 139,
        "year": 2023
    },
    {
        "title": "Caption Anything: Interactive Image Description with Diverse Multimodal Controls",
        "abstract": "Controllable image captioning is an emerging multimodal topic that aims to describe the image with natural language following human purpose, $\\textit{e.g.}$, looking at the specified regions or telling in a particular text style. State-of-the-art methods are trained on annotated pairs of input controls and output captions. However, the scarcity of such well-annotated multimodal data largely limits their usability and scalability for interactive AI systems. Leveraging unimodal instruction-following foundation models is a promising alternative that benefits from broader sources of data. In this paper, we present Caption AnyThing (CAT), a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality. Powered by Segment Anything Model (SAM) and ChatGPT, we unify the visual and language prompts into a modularized framework, enabling the flexible combination between different controls. Extensive case studies demonstrate the user intention alignment capabilities of our framework, shedding light on effective user interaction modeling in vision-language applications. Our code is publicly available at https://github.com/ttengwang/Caption-Anything.",
        "authors": [
            "Teng Wang",
            "Jinrui Zhang",
            "Junjie Fei",
            "Yixiao Ge",
            "Hao Zheng",
            "Yunlong Tang",
            "Zhe Li",
            "Mingqi Gao",
            "Shanshan Zhao",
            "Ying Shan",
            "Feng Zheng"
        ],
        "citations": 67,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching",
        "abstract": "Powered by large-scale pre-training, vision foundation models exhibit significant potential in open-world image understanding. However, unlike large language models that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by fine-tuning on specific tasks. In this work, we present Matcher, a novel perception paradigm that utilizes off-the-shelf vision foundation models to address various perception tasks. Matcher can segment anything by using an in-context example without training. Additionally, we design three effective components within the Matcher framework to collaborate with these foundation models and unleash their full potential in diverse perception tasks. Matcher demonstrates impressive generalization performance across various segmentation tasks, all without training. For example, it achieves 52.7% mIoU on COCO-20$^i$ with one example, surpassing the state-of-the-art specialist model by 1.6%. In addition, Matcher achieves 33.0% mIoU on the proposed LVIS-92$^i$ for one-shot semantic segmentation, outperforming the state-of-the-art generalist model by 14.4%. Our visualization results further showcase the open-world generality and flexibility of Matcher when applied to images in the wild. Our code can be found at https://github.com/aim-uofa/Matcher.",
        "authors": [
            "Yang Liu",
            "Muzhi Zhu",
            "Hengtao Li",
            "Hao Chen",
            "Xinlong Wang",
            "Chunhua Shen"
        ],
        "citations": 60,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Language is All a Graph Needs",
        "abstract": "The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data like images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, languages, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is available at https://github.com/agiresearch/InstructGLM.",
        "authors": [
            "Ruosong Ye",
            "Caiqi Zhang",
            "Runhui Wang",
            "Shuyuan Xu",
            "Yongfeng Zhang"
        ],
        "citations": 126,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Visual Prompt Multi-Modal Tracking",
        "abstract": "Visible-modal object tracking gives rise to a series of downstream multi-modal tracking tributaries. To inherit the powerful representations of the foundation model, a natural modus operandi for multi-modal tracking is full fine-tuning on the RGB-based parameters. Albeit effective, this manner is not optimal due to the scarcity of downstream data and poor transferability, etc. In this paper, inspired by the recent success of the prompt learning in language models, we develop Visual Prompt multi-modal Tracking (ViPT), which learns the modal-relevant prompts to adapt the frozen pre-trained foundation model to various downstream multi-modal tracking tasks. ViPT finds a better way to stimulate the knowledge of the RGB-based model that is pre-trained at scale, meanwhile only introducing a few trainable parameters (less than 1% of model parameters). ViPT outperforms the full fine-tuning paradigm on multiple downstream tracking tasks including RGB+Depth, RGB+Thermal, and RGB+Event tracking. Extensive experiments show the potential of visual prompt learning for multi-modal tracking, and ViPT can achieve state-of-the-art performance while satisfying parameter efficiency. Code and models are available at https://github.com/jiawen-zhu/ViPT.",
        "authors": [
            "Jiawen Zhu",
            "Simiao Lai",
            "Xin Chen",
            "D. Wang",
            "Huchuan Lu"
        ],
        "citations": 116,
        "references": 64,
        "year": 2023
    },
    {
        "title": "BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine",
        "abstract": "Foundation models (FMs) have exhibited remarkable performance across a wide range of downstream tasks in many domains. Nevertheless, general-purpose FMs often face challenges when confronted with domain-specific problems, due to their limited access to the proprietary training data in a particular domain. In biomedicine, there are various biological modalities, such as molecules, proteins, and cells, which are encoded by the language of life and exhibit significant modality gaps with human natural language. In this paper, we introduce BioMedGPT, an open multimodal generative pre-trained transformer (GPT) for biomedicine, to bridge the gap between the language of life and human natural language. BioMedGPT allows users to easily ``communicate'' with diverse biological modalities through free text, which is the first of its kind. BioMedGPT aligns different biological modalities with natural language via a large generative language model, namely, BioMedGPT-LM. We publish BioMedGPT-10B, which unifies the feature spaces of molecules, proteins, and natural language via encoding and alignment. Through fine-tuning, BioMedGPT-10B outperforms or is on par with human and significantly larger general-purpose foundation models on the biomedical QA task. It also demonstrates promising performance in the molecule QA and protein QA tasks, which could greatly accelerate the discovery of new drugs and therapeutic targets. In addition, BioMedGPT-LM-7B is the first large generative language model based on Llama2 in the biomedical domain, therefore is commercial friendly. Both BioMedGPT-10B and BioMedGPT-LM-7B are open-sourced to the research community. In addition, we publish the datasets that are meticulously curated for the alignment of multi-modalities, i.e., PubChemQA and UniProtQA. All the models, codes, and datasets are available at \\url{https://github.com/PharMolix/OpenBioMed}.",
        "authors": [
            "Yi Luo",
            "Jiahuan Zhang",
            "Siqi Fan",
            "Kai Yang",
            "Yushuai Wu",
            "Mu Qiao",
            "Zaiqing Nie"
        ],
        "citations": 60,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Artificial intelligence in surgery.",
        "abstract": null,
        "authors": [
            "Chris Varghese",
            "Ewen M. Harrison",
            "Greg O’Grady",
            "E. Topol"
        ],
        "citations": 19,
        "references": 113,
        "year": 2024
    },
    {
        "title": "Pandora: Towards General World Model with Natural Language Actions and Video States",
        "abstract": "World models simulate future states of the world in response to different actions. They facilitate interactive content creation and provides a foundation for grounded, long-horizon reasoning. Current foundation models do not fully meet the capabilities of general world models: large language models (LLMs) are constrained by their reliance on language modality and their limited understanding of the physical world, while video models lack interactive action control over the world simulations. This paper makes a step towards building a general world model by introducing Pandora, a hybrid autoregressive-diffusion model that simulates world states by generating videos and allows real-time control with free-text actions. Pandora achieves domain generality, video consistency, and controllability through large-scale pretraining and instruction tuning. Crucially, Pandora bypasses the cost of training-from-scratch by integrating a pretrained LLM (7B) and a pretrained video model, requiring only additional lightweight finetuning. We illustrate extensive outputs by Pandora across diverse domains (indoor/outdoor, natural/urban, human/robot, 2D/3D, etc.). The results indicate great potential of building stronger general world models with larger-scale training.",
        "authors": [
            "Jiannan Xiang",
            "Guangyi Liu",
            "Yi Gu",
            "Qiyue Gao",
            "Yuting Ning",
            "Yuheng Zha",
            "Zeyu Feng",
            "Tianhua Tao",
            "Shibo Hao",
            "Yemin Shi",
            "Zhengzhong Liu",
            "Eric P. Xing",
            "Zhiting Hu"
        ],
        "citations": 17,
        "references": 81,
        "year": 2024
    },
    {
        "title": "Listen, Think, and Understand",
        "abstract": "The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.",
        "authors": [
            "Yuan Gong",
            "Hongyin Luo",
            "Alexander H. Liu",
            "Leonid Karlinsky",
            "James Glass"
        ],
        "citations": 113,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Seamless: Multilingual Expressive and Streaming Speech Translation",
        "abstract": "Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at https://github.com/facebookresearch/seamless_communication",
        "authors": [
            "Seamless Communication",
            "Loïc Barrault",
            "Yu-An Chung",
            "Mariano Coria Meglioli",
            "David Dale",
            "Ning Dong",
            "M. Duppenthaler",
            "Paul-Ambroise Duquenne",
            "Brian Ellis",
            "Hady ElSahar",
            "Justin Haaheim",
            "John Hoffman",
            "Min-Jae Hwang",
            "H. Inaguma",
            "Christopher Klaiber",
            "Ilia Kulikov",
            "Pengwei Li",
            "Daniel Licht",
            "Jean Maillard",
            "Ruslan Mavlyutov",
            "Alice Rakotoarison",
            "Kaushik Ram Sadagopan",
            "Abinesh Ramakrishnan",
            "Tuan Tran",
            "Guillaume Wenzek",
            "Yilin Yang",
            "Ethan Ye",
            "Ivan Evtimov",
            "Pierre Fernandez",
            "Cynthia Gao",
            "Prangthip Hansanti",
            "Elahe Kalbassi",
            "A. Kallet",
            "Artyom Kozhevnikov",
            "Gabriel Mejia Gonzalez",
            "Robin San Roman",
            "Christophe Touret",
            "Corinne Wong",
            "Carleigh Wood",
            "Bokai Yu",
            "Pierre Andrews",
            "Can Balioglu",
            "Peng-Jen Chen",
            "M. Costa-jussà",
            "Maha Elbayad",
            "Hongyu Gong",
            "Francisco Guzm'an",
            "Kevin Heffernan",
            "Somya Jain",
            "Justine T. Kao",
            "Ann Lee",
            "Xutai Ma",
            "Alexandre Mourachko",
            "Benjamin Peloquin",
            "Juan Pino",
            "Sravya Popuri",
            "C. Ropers",
            "Safiyyah Saleem",
            "H. Schwenk",
            "Anna Y. Sun",
            "Paden Tomasello",
            "Changhan Wang",
            "Jeff Wang",
            "Skyler Wang",
            "Mary Williamson"
        ],
        "citations": 106,
        "references": 0,
        "year": 2023
    },
    {
        "title": "The Real Dangers of Generative AI",
        "abstract": "Abstract:As perhaps the most consequential technology of our time, Generative Foundation Models (GFMs) present unprecedented challenges for democratic institutions. By allowing deception and de-contextualized information sharing at a previously unimaginable scale and pace, GFMs could undermine the foundations of democracy. At the same time, the investment scale required to develop the models and the race dynamics around that development threaten to enable concentrations of democratically unaccountable power (both public and private). This essay examines the twin threats of collapse and singularity occasioned by the rise of GFMs.",
        "authors": [
            "Danielle Allen",
            "E. G. Weyl",
            "James Bryant"
        ],
        "citations": 15,
        "references": 0,
        "year": 2024
    },
    {
        "title": "mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections",
        "abstract": "Large-scale pre-trained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from inefficiency and linguistic signal overwhelmed by long visual sequences in cross-modal alignment. To address both problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections.mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, including image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability on vision-language and video-language tasks. The code and pre-trained models are available at https://github.com/alibaba/AliceMind",
        "authors": [
            "Chenliang Li",
            "Haiyang Xu",
            "Junfeng Tian",
            "Wei Wang",
            "Ming Yan",
            "Bin Bi",
            "Jiabo Ye",
            "Hehong Chen",
            "Guohai Xu",
            "Zheng-da Cao",
            "Ji Zhang",
            "Songfang Huang",
            "Feiran Huang",
            "Jingren Zhou",
            "Luo Si"
        ],
        "citations": 185,
        "references": 85,
        "year": 2022
    },
    {
        "title": "Learning Concise and Descriptive Attributes for Visual Recognition",
        "abstract": "Recent advances in foundation models present new opportunities for interpretable visual recognition – one can first query Large Language Models (LLMs) to obtain a set of attributes that describe each class, then apply vision-language models to classify images via these attributes. Pioneering work shows that querying thousands of attributes can achieve performance competitive with image features. However, our further investigation on 8 datasets reveals that LLM-generated attributes in a large quantity perform almost the same as random words. This surprising finding suggests that significant noise may be present in these attributes. We hypothesize that there exist subsets of attributes that can maintain the classification performance with much smaller sizes, and propose a novel learning-to-search method to discover those concise sets of attributes. As a result, on the CUB dataset, our method achieves performance close to that of massive LLM-generated attributes (e.g., 10k attributes for CUB), yet using only 32 attributes in total to distinguish 200 bird species. Furthermore, our new paradigm demonstrates several additional benefits: higher interpretability and interactivity for humans, and the ability to summarize knowledge for a recognition task.",
        "authors": [
            "Andy Yan",
            "Yu Wang",
            "Yiwu Zhong",
            "Chengyu Dong",
            "Zexue He",
            "Yujie Lu",
            "William Wang",
            "Jingbo Shang",
            "Julian McAuley"
        ],
        "citations": 51,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Offsite-Tuning: Transfer Learning without Full Model",
        "abstract": "Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available at https://github.com/mit-han-lab/offsite-tuning.",
        "authors": [
            "Guangxuan Xiao",
            "Ji Lin",
            "Song Han"
        ],
        "citations": 55,
        "references": 60,
        "year": 2023
    },
    {
        "title": "VanillaNet: the Power of Minimalism in Deep Learning",
        "abstract": "At the heart of foundation models is the philosophy of\"more is different\", exemplified by the astonishing success in computer vision and natural language processing. However, the challenges of optimization and inherent complexity of transformer models call for a paradigm shift towards simplicity. In this study, we introduce VanillaNet, a neural network architecture that embraces elegance in design. By avoiding high depth, shortcuts, and intricate operations like self-attention, VanillaNet is refreshingly concise yet remarkably powerful. Each layer is carefully crafted to be compact and straightforward, with nonlinear activation functions pruned after training to restore the original architecture. VanillaNet overcomes the challenges of inherent complexity, making it ideal for resource-constrained environments. Its easy-to-understand and highly simplified architecture opens new possibilities for efficient deployment. Extensive experimentation demonstrates that VanillaNet delivers performance on par with renowned deep neural networks and vision transformers, showcasing the power of minimalism in deep learning. This visionary journey of VanillaNet has significant potential to redefine the landscape and challenge the status quo of foundation model, setting a new path for elegant and effective model design. Pre-trained models and codes are available at https://github.com/huawei-noah/VanillaNet and https://gitee.com/mindspore/models/tree/master/research/cv/vanillanet.",
        "authors": [
            "Hanting Chen",
            "Yunhe Wang",
            "Jianyuan Guo",
            "Dacheng Tao"
        ],
        "citations": 54,
        "references": 64,
        "year": 2023
    },
    {
        "title": "SGPT: GPT Sentence Embeddings for Semantic Search",
        "abstract": "Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.",
        "authors": [
            "Niklas Muennighoff"
        ],
        "citations": 153,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends",
        "abstract": "This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: ($i$) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and ($iii$) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.",
        "authors": [
            "Zhe Gan",
            "Linjie Li",
            "Chunyuan Li",
            "Lijuan Wang",
            "Zicheng Liu",
            "Jianfeng Gao"
        ],
        "citations": 141,
        "references": 0,
        "year": 2022
    },
    {
        "title": "ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation",
        "abstract": "In the realm of artificial intelligence, the emergence of foundation models, backed by high computing capabilities and extensive data, has been revolutionary. Segment Anything Model (SAM), built on the Vision Transformer (ViT) model with millions of parameters and vast training dataset SA-1B, excels in various segmentation scenarios relying on its significance of semantic information and generalization ability. Such achievement of visual foundation model stimulates continuous researches on specific downstream tasks in computer vision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the high-performing SAM for landcover classification on space-borne Synthetic Aperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's parameters and incorporates lightweight adapters for parameter efficient fine-tuning, and a classwise mask decoder is designed to achieve semantic segmentation task. This adapt-tuning method allows for efficient landcover classification of SAR images, balancing the accuracy with computational demand. In addition, the task specific input module injects low frequency information of SAR images by MLP-based layers to improve the model performance. Compared to conventional state-of-the-art semantic segmentation algorithms by extensive experiments, CWSAM showcases enhanced performance with fewer computing resources, highlighting the potential of leveraging foundational models like SAM for specific downstream tasks in the SAR domain. The source code is available at: https://github.com/xypu98/CWSAM.",
        "authors": [
            "Xinyang Pu",
            "He Jia",
            "Linghao Zheng",
            "Feng Wang",
            "Feng Xu"
        ],
        "citations": 12,
        "references": 53,
        "year": 2024
    },
    {
        "title": "Weakly Supervised 3D Open-vocabulary Segmentation",
        "abstract": "Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \\url{https://github.com/Kunhao-Liu/3D-OVS}.",
        "authors": [
            "Kunhao Liu",
            "Fangneng Zhan",
            "Jiahui Zhang",
            "Muyu Xu",
            "Yingchen Yu",
            "A. E. Saddik",
            "C. Theobalt",
            "Eric P. Xing",
            "Shijian Lu"
        ],
        "citations": 48,
        "references": 76,
        "year": 2023
    },
    {
        "title": "The effectiveness of MAE pre-pretraining for billion-scale pretraining",
        "abstract": "This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images.",
        "authors": [
            "Mannat Singh",
            "Quentin Duval",
            "Kalyan Vasudev Alwala",
            "Haoqi Fan",
            "Vaibhav Aggarwal",
            "Aaron B. Adcock",
            "Armand Joulin",
            "Piotr Doll'ar",
            "Christoph Feichtenhofer",
            "Ross B. Girshick",
            "Rohit Girdhar",
            "Ishan Misra"
        ],
        "citations": 47,
        "references": 92,
        "year": 2023
    },
    {
        "title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
        "abstract": "We propose a lightweight and scalable Regional Point-Language Contrastive learning framework, namely RegionPLC, for open-world 3D scene understanding, aiming to identify and recognize open-set objects and categories. Specifically, based on our empirical studies, we introduce a 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models, yielding high-quality, dense region-level language descriptions without human 3D annotations. Subsequently, we devise a region-aware point-discriminative contrastive learning objective to enable robust and effective 3D learning from dense regional language supervision. We carry out extensive experiments on ScanNet, ScanNet200, and nuScenes datasets, and our model outperforms prior 3D open-world scene understanding approaches by an average of 17.2% and 9.1% for semantic and instance segmentation, respectively, while maintaining greater scalability and lower resource demands. Furthermore, our method has the flexibility to be effortlessly integrated with language models to enable open-ended grounded 3D reasoning without extra task-specific training. Code will be released at github.",
        "authors": [
            "Jihan Yang",
            "Runyu Ding",
            "Zhe Wang",
            "Xiaojuan Qi"
        ],
        "citations": 44,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Data Filtering Networks",
        "abstract": "Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train state-of-the-art models for their compute budgets: among other improvements on a variety of tasks, a ViT-H trained on our dataset achieves 83.0% zero-shot transfer accuracy on ImageNet, out-performing models trained on other datasets such as LAION-2B, DataComp-1B, or OpenAI's WIT. In order to facilitate further research in dataset design, we also release a new 2 billion example dataset DFN-2B and show that high performance data filtering networks can be trained from scratch using only publicly available data.",
        "authors": [
            "Alex Fang",
            "Albin Madappally Jose",
            "Amit Jain",
            "Ludwig Schmidt",
            "Alexander Toshev",
            "Vaishaal Shankar"
        ],
        "citations": 81,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Adapting Segment Anything Model for Change Detection in VHR Remote Sensing Images",
        "abstract": "Vision foundation models (VFMs), such as the segment anything model (SAM), allow zero-shot or interactive segmentation of visual contents; thus, they are quickly applied in a variety of visual scenes. However, their direct use in many remote sensing (RS) applications is often unsatisfactory due to the special imaging properties of RS images (RSIs). In this work, we aim to utilize the strong visual recognition capabilities of VFMs to improve change detection (CD) in very high-resolution (VHR) RSIs. We employ the visual encoder of FastSAM, a variant of the SAM, to extract visual representations in RS scenes. To adapt FastSAM to focus on some specific ground objects in RS scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to SAM features, we introduce a task-agnostic semantic learning branch to model the semantic latent in bitemporal RSIs. The resulting method, SAM-based CD (SAM-CD), obtains superior accuracy compared with the state-of-the-art (SOTA) fully supervised CD methods and exhibits a sample-efficient learning ability that is comparable to semisupervised CD methods. To the best of our knowledge, this is the first work that adapts VFMs to CD in VHR RSIs.",
        "authors": [
            "Lei Ding",
            "Kun Zhu",
            "Daifeng Peng",
            "Hao Tang",
            "Kuiwu Yang",
            "Lorenzo Bruzzone"
        ],
        "citations": 49,
        "references": 53,
        "year": 2023
    },
    {
        "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World",
        "abstract": "We present the All-Seeing (AS) project: a large-scale data and model for recognizing and understanding everything in the open world. Using a scalable data engine that incorporates human feedback and efficient models in the loop, we create a new dataset (AS-1B) with over 1 billion regions annotated with semantic tags, question-answering pairs, and detailed captions. It covers a wide range of 3.5 million common and rare concepts in the real world, and has 132.2 billion tokens that describe the concepts and their attributes. Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding. The model is trained with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance, including region-text retrieval, region recognition, captioning, and question-answering. We hope that this project can serve as a foundation for vision-language artificial general intelligence research. Models and the dataset shall be released at https://github.com/OpenGVLab/All-Seeing, and demo can be seen at https://huggingface.co/spaces/OpenGVLab/all-seeing.",
        "authors": [
            "Weiyun Wang",
            "Min Shi",
            "Qingyun Li",
            "Wen Wang",
            "Zhenhang Huang",
            "Linjie Xing",
            "Zhe Chen",
            "Hao Li",
            "Xizhou Zhu",
            "Zhiguo Cao",
            "Yushi Chen",
            "Tong Lu",
            "Jifeng Dai",
            "Y. Qiao"
        ],
        "citations": 70,
        "references": 114,
        "year": 2023
    },
    {
        "title": "Protein Design with Guided Discrete Diffusion",
        "abstract": "A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to several therapeutic targets under locality and developability constraints, attaining a 99% expression rate and 40% binding rate in exploratory in vitro experiments.",
        "authors": [
            "Nate Gruver",
            "S. Stanton",
            "Nathan C Frey",
            "Tim G. J. Rudner",
            "I. Hotzel",
            "J. Lafrance-Vanasse",
            "A. Rajpal",
            "Kyunghyun Cho",
            "A. Wilson"
        ],
        "citations": 76,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization",
        "abstract": "Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language.",
        "authors": [
            "Weiyang Liu",
            "Zeju Qiu",
            "Yao Feng",
            "Yuliang Xiu",
            "Yuxuan Xue",
            "Longhui Yu",
            "Haiwen Feng",
            "Zhen Liu",
            "Juyeon Heo",
            "Songyou Peng",
            "Yandong Wen",
            "Michael J. Black",
            "Adrian Weller",
            "Bernhard Schölkopf"
        ],
        "citations": 39,
        "references": 103,
        "year": 2023
    },
    {
        "title": "FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning",
        "abstract": "Federated learning (FL) has emerged as a new paradigm for privacy-preserving computation in recent years. Unfortunately, FL faces two critical challenges that hinder its actual performance: data distribution heterogeneity and high resource costs brought by large foundation models. Specifically, the non-IID data in different clients make existing FL algorithms hard to converge while the high resource costs, including computational and communication costs that increase the deployment difficulty in real-world scenarios. In this paper, we propose an effective yet simple method, named FedCLIP, to achieve fast generalization and personalization for CLIP in federated learning. Concretely, we design an attention-based adapter for the large model, CLIP, and the rest operations merely depend on adapters. Lightweight adapters can make the most use of pretrained model information and ensure models be adaptive for clients in specific tasks. Simultaneously, small-scale operations can mitigate the computational burden and communication burden caused by large models. Extensive experiments are conducted on three datasets with distribution shifts. Qualitative and quantitative results demonstrate that FedCLIP significantly outperforms other baselines (9% overall improvements on PACS) and effectively reduces computational and communication costs (283x faster than FedAVG). Our code will be available at: https://github.com/microsoft/PersonalizedFL.",
        "authors": [
            "Wang Lu",
            "Xixu Hu",
            "Jindong Wang",
            "Xingxu Xie"
        ],
        "citations": 40,
        "references": 63,
        "year": 2023
    },
    {
        "title": "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research",
        "abstract": "Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. Code and data are available at https://github.com/for-ai/black-box-api-challenges.",
        "authors": [
            "Luiza Amador Pozzobon",
            "B. Ermiş",
            "Patrick Lewis",
            "Sara Hooker"
        ],
        "citations": 43,
        "references": 38,
        "year": 2023
    },
    {
        "title": "PLA: Language-Driven Open-Vocabulary 3D Scene Understanding",
        "abstract": "Open-vocabulary scene understanding aims to localize and recognize unseen categories beyond the annotated label space. The recent breakthrough of 2D open-vocabulary perception is largely driven by Internet-scale paired image-text data with rich vocabulary concepts. However, this success cannot be directly transferred to 3D scenarios due to the inaccessibility of large-scale 3D-text pairs. To this end, we propose to distill knowledge encoded in pretrained vision-language (VL) foundation models through captioning multi-view images from 3D, which allows explicitly associating 3D and semantic-rich captions. Further, to foster coarse-to-fine visual-semantic representation learning from captions, we design hierarchical 3D-caption pairs, leveraging geometric constraints between 3D scenes and multi-view images. Finally, by employing contrastive learning, the model learns language-aware embeddings that connect 3D and text for open-vocabulary tasks. Our method not only remarkably outperforms baseline methods by 25.8% ~ 44.7% hIoU and 14.5% ~ 50.4% hAP50 in open-vocabulary semantic and instance segmentation, but also shows robust transferability on challenging zero-shot domain transfer tasks. See the project website at https://dingry.github.io/projects/PLA.",
        "authors": [
            "Runyu Ding",
            "Jihan Yang",
            "Chuhui Xue",
            "Wenqing Zhang",
            "Song Bai",
            "Xiaojuan Qi"
        ],
        "citations": 123,
        "references": 57,
        "year": 2022
    },
    {
        "title": "ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System",
        "abstract": "Existing deep video models are limited by specific tasks, fixed input-output spaces, and poor generalization capabilities, making it difficult to deploy them in real-world scenarios. In this paper, we present our vision for multimodal and versatile video understanding and propose a prototype system, \\system. Our system is built upon a tracklet-centric paradigm, which treats tracklets as the basic video unit and employs various Video Foundation Models (ViFMs) to annotate their properties e.g., appearance, motion, \\etc. All the detected tracklets are stored in a database and interact with the user through a database manager. We have conducted extensive case studies on different types of in-the-wild videos, which demonstrates the effectiveness of our method in answering various video-related problems. Our project is available at https://www.wangjunke.info/ChatVideo/",
        "authors": [
            "Junke Wang",
            "Dongdong Chen",
            "Chong Luo",
            "Xiyang Dai",
            "Lu Yuan",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "citations": 41,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text",
        "abstract": "The recent advances in large language models (LLM) and foundation models with emergent capabilities have been shown to improve the performance of many NLP tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs can be used for KG construction or completion while existing KGs can be used for different tasks such as making LLM outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact extraction performance, ontology conformance, and hallucinations by LLMs. Furthermore, we provide results for two baseline models, Vicuna-13B and Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline results show that there is room for improvement using both Semantic Web and Natural Language Processing techniques.",
        "authors": [
            "Nandana Mihindukulasooriya",
            "S. Tiwari",
            "Carlos F. Enguix",
            "K. Lata"
        ],
        "citations": 41,
        "references": 55,
        "year": 2023
    },
    {
        "title": "TimeGPT-1",
        "abstract": "In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.",
        "authors": [
            "Azul Garza",
            "Cristian Challu",
            "Max Mergenthaler-Canseco"
        ],
        "citations": 71,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Deep Model Fusion: A Survey",
        "abstract": "Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1)\"Mode connectivity\", which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion; (2)\"Alignment\"matches units between neural networks to create better conditions for fusion; (3)\"Weight average\", a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution; (4)\"Ensemble learning\"combines the outputs of diverse models, which is a foundational technique for improving the accuracy and robustness of the final model. In addition, we analyze the challenges faced by deep model fusion and propose possible research directions for model fusion in the future. Our review is helpful in deeply understanding the correlation between different model fusion methods and practical application methods, which can enlighten the research in the field of deep model fusion.",
        "authors": [
            "Weishi Li",
            "Yong Peng",
            "Miao Zhang",
            "Liang Ding",
            "Han Hu",
            "Li Shen"
        ],
        "citations": 42,
        "references": 270,
        "year": 2023
    },
    {
        "title": "An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems",
        "abstract": "The increasing prevalence of Cyber- Physical Systems and the Internet of Things (CPS-loT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such human-in-the-Ioop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-the-Ioop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort. Our results show that LLMs are capable of simulating complex population movements within large open spaces. Besides, AitL- RLdemonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-loT applications. Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-loT to enhance system adaptability and efficiency. The project's code can be found on our GitHub repository.",
        "authors": [
            "Hanqing Yang",
            "Marie Siew",
            "Carlee Joe-Wong"
        ],
        "citations": 8,
        "references": 17,
        "year": 2024
    },
    {
        "title": "VistaRAG: Toward Safe and Trustworthy Autonomous Driving Through Retrieval-Augmented Generation",
        "abstract": "Autonomous driving based on foundation models has recently garnered widespread attention. However, the risk of hallucinations inherent in foundation models could compromise the safety and reliability of autonomous driving systems. This letter, as part of a series of reports from the Distributed/Decentralized Hybrid Workshop on Foundation/Infrastructure Intelligence (DHW-FII), aims to tackle these issues. We introduce VistaRAG, which integrates retrieval-augmented generation (RAG) technologies into autonomous driving systems based on foundation models, to address the inherent reliability challenges in decision-making. VistaRAG employs a dynamic retrieval mechanism to access highly relevant driving experience, real-time road network status, and other contextual information from external databases. This aids foundation models in informed reasoning and decision-making, thereby enhancing the safety and trustworthiness of foundation-model-based autonomous driving systems under complex traffic scenarios.",
        "authors": [
            "Xingyuan Dai",
            "Chao Guo",
            "Yun Tang",
            "Haichuan Li",
            "Yutong Wang",
            "Jun Huang",
            "Yonglin Tian",
            "Xin Xia",
            "Yisheng Lv",
            "Fei-Yue Wang"
        ],
        "citations": 8,
        "references": 27,
        "year": 2024
    },
    {
        "title": "How Segment Anything Model (SAM) Boost Medical Image Segmentation?",
        "abstract": "Due to the flexibility of prompting, foundation models have become the dominant force in the domains of natural language processing and image generation. With the recent introduction of the Segment Anything Model (SAM), the prompt-driven paradigm has entered the realm of image segmentation, bringing with a range of previously unexplored capabilities. However, it remains unclear whether it can be applicable to medical image segmentation due to the significant differences between natural images and medical images. In this work, we summarize recent efforts to extend the success of SAM to medical image segmentation tasks, including both empirical benchmarking and methodological adaptations, and discuss potential future directions for SAM in medical image segmentation. Although directly applying SAM to medical image segmentation cannot obtain satisfying performance on multi-modal and multi-target medical datasets, many insights are drawn to guide future research to develop foundation models for medical image analysis. We also set up a continuously updated paper list and open-source project summary to boost the research on this topic at https://github.com/YichiZhang98/SAM4MIS.",
        "authors": [
            "Yichi Zhang",
            "Rushi Jiao"
        ],
        "citations": 38,
        "references": 49,
        "year": 2023
    },
    {
        "title": "The Future of Management: DAO to Smart Organizations and Intelligent Operations",
        "abstract": "In the future, management in smart societies will revolve around knowledge workers and the works they produce. This article is committed to explore new management framework, model, paradigm, and solution for organizing, managing, and measuring knowledge works. First, the parallel management framework is presented that would allow for the virtual-real interactions of humans in social space, robots in physical space, and digital humans in cyberspace to realize descriptive, predictive, and prescriptive intelligence for management. Then, the management foundation models are proposed by fusing scenarios engineering with artificial intelligence foundation models and cyber–physical-social systems. Moreover, the new management paradigm driven by decentralized autonomous organizations and operations is formulated for the advancement of smart organizations and intelligent operations. On these basis, the management operating systems that highlight features of simple intelligence, provable security, flexible scalability, and ecological harmony are finally put forward as new management solution.",
        "authors": [
            "Juanjuan Li",
            "Rui Qin",
            "Fei-Yue Wang"
        ],
        "citations": 35,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs",
        "abstract": "As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently. Lastly, we further optimize these PEFT techniques by selectively choosing which parts of the model to train, and find that these techniques can be applied with significantly fewer parameters while maintaining and even improving performance.",
        "authors": [
            "George Pu",
            "Anirudh Jain",
            "Jihan Yin",
            "Russell Kaplan"
        ],
        "citations": 35,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
        "abstract": "We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for various computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform diverse tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with un-precedented zero-shot and fine-tuning capabilities.",
        "authors": [
            "Bin Xiao",
            "Haiping Wu",
            "Weijian Xu",
            "Xiyang Dai",
            "Houdong Hu",
            "Yumao Lu",
            "Michael Zeng",
            "Ce Liu",
            "Lu Yuan"
        ],
        "citations": 61,
        "references": 97,
        "year": 2023
    },
    {
        "title": "OpenAgents: An Open Platform for Language Agents in the Wild",
        "abstract": "Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.",
        "authors": [
            "Tianbao Xie",
            "Fan Zhou",
            "Zhoujun Cheng",
            "Peng Shi",
            "Luoxuan Weng",
            "Yitao Liu",
            "Toh Jing Hua",
            "Junning Zhao",
            "Qian Liu",
            "Che Liu",
            "Leo Z. Liu",
            "Yiheng Xu",
            "Hongjin Su",
            "Dongchan Shin",
            "Caiming Xiong",
            "Tao Yu"
        ],
        "citations": 61,
        "references": 75,
        "year": 2023
    },
    {
        "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
        "abstract": "We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.",
        "authors": [
            "Yufei Wang",
            "Zhou Xian",
            "Feng Chen",
            "Tsun-Hsuan Wang",
            "Yian Wang",
            "Zackory Erickson",
            "David Held",
            "Chuang Gan"
        ],
        "citations": 62,
        "references": 147,
        "year": 2023
    },
    {
        "title": "SAM Fails to Segment Anything? -- SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, Medical Image Segmentation, and More",
        "abstract": "The emergence of large models, also known as foundation models, has brought significant advancements to AI research. One such model is Segment Anything (SAM), which is designed for image segmentation tasks. However, as with other foundation models, our experimental findings suggest that SAM may fail or perform poorly in certain segmentation tasks, such as shadow detection and camouflaged object detection (concealed object detection). This study first paves the way for applying the large pre-trained image segmentation model SAM to these downstream tasks, even in situations where SAM performs poorly. Rather than fine-tuning the SAM network, we propose \\textbf{SAM-Adapter}, which incorporates domain-specific information or visual prompts into the segmentation network by using simple yet effective adapters. By integrating task-specific knowledge with general knowledge learnt by the large model, SAM-Adapter can significantly elevate the performance of SAM in challenging tasks as shown in extensive experiments. We can even outperform task-specific network models and achieve state-of-the-art performance in the task we tested: camouflaged object detection, shadow detection. We also tested polyp segmentation (medical image segmentation) and achieves better results. We believe our work opens up opportunities for utilizing SAM in downstream tasks, with potential applications in various fields, including medical image processing, agriculture, remote sensing, and more.",
        "authors": [
            "Tianrun Chen",
            "Lanyun Zhu",
            "Chao Ding",
            "Runlong Cao",
            "Yan Wang",
            "Z. Li",
            "Lingyun Sun",
            "Papa Mao",
            "Ying-Dong Zang"
        ],
        "citations": 34,
        "references": 62,
        "year": 2023
    },
    {
        "title": "SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model",
        "abstract": null,
        "authors": [
            "Dingyuan Zhang",
            "Dingkang Liang",
            "Hongcheng Yang",
            "Zhikang Zou",
            "Xiaoqing Ye",
            "Zhe Liu",
            "Xiang Bai"
        ],
        "citations": 33,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains",
        "abstract": "The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, Prompt Sapper, which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.",
        "authors": [
            "Yu Cheng",
            "Jieshan Chen",
            "Qing Huang",
            "Zhenchang Xing",
            "Xiwei Xu",
            "Qinghua Lu"
        ],
        "citations": 34,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Uni3D: Exploring Unified 3D Representation at Scale",
        "abstract": "Scaling up representations for images or text has been extensively investigated in the past few years and has led to revolutions in learning vision and language. However, scalable representation for 3D objects and scenes is relatively unexplored. In this work, we present Uni3D, a 3D foundation model to explore the unified 3D representation at scale. Uni3D uses a 2D initialized ViT end-to-end pretrained to align the 3D point cloud features with the image-text aligned features. Via the simple architecture and pretext task, Uni3D can leverage abundant 2D pretrained models as initialization and image-text aligned models as the target, unlocking the great potential of 2D models and scaling-up strategies to the 3D world. We efficiently scale up Uni3D to one billion parameters, and set new records on a broad range of 3D tasks, such as zero-shot classification, few-shot classification, open-world understanding and part segmentation. We show that the strong Uni3D representation also enables applications such as 3D painting and retrieval in the wild. We believe that Uni3D provides a new direction for exploring both scaling up and efficiency of the representation in 3D domain.",
        "authors": [
            "Junsheng Zhou",
            "Jinsheng Wang",
            "Baorui Ma",
            "Yu-Shen Liu",
            "Tiejun Huang",
            "Xinlong Wang"
        ],
        "citations": 57,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks",
        "abstract": "The rapid development of AI systems has been greatly influenced by the emergence of foundation models. A common approach for targeted problems involves fine-tuning these pre-trained foundation models for specific target tasks, resulting in a rapid spread of models fine-tuned across a diverse array of tasks. This work focuses on the problem of merging multiple fine-tunings of the same foundation model derived from a spectrum of auxiliary tasks. We introduce a new simple method, Model Breadcrumbs, which consists of a sparsely defined weight set that guides model adaptation within the weight space of a pre-trained model. These breadcrumbs are constructed by subtracting the weights from a pre-trained model before and after fine-tuning, followed by a sparsification process that eliminates weight outliers and negligible perturbations. Our experiments demonstrate the effectiveness of Model Breadcrumbs to simultaneously improve performance across multiple tasks. This contribution aligns with the evolving paradigm of updatable machine learning, reminiscent of the collaborative principles underlying open-source software development, fostering a community-driven effort to reliably update machine learning models. Our method is shown to be more efficient and unlike previous proposals does not require hyperparameter tuning for each new task added. Through extensive experimentation involving various models, tasks, and modalities we establish that integrating Model Breadcrumbs offers a simple, efficient, and highly effective approach for constructing multi-task models and facilitating updates to foundation models.",
        "authors": [
            "Mohammad-Javad Davari",
            "Eugene Belilovsky"
        ],
        "citations": 29,
        "references": 60,
        "year": 2023
    },
    {
        "title": "CNOS: A Strong Baseline for CAD-based Novel Object Segmentation",
        "abstract": "We propose a simple yet powerful method to segment novel objects in RGB images from their CAD models. Leveraging recent foundation models, Segment Anything and DINOv2, we generate segmentation proposals in the input image and match them against object templates that are pre-rendered using the CAD models. The matching is realized by comparing DINOv2 cls tokens of the proposed regions and the templates. The output of the method is a set of segmentation masks associated with per-object confidences defined by the matching scores. We experimentally demonstrate that the proposed method achieves state-of-the-art results in CAD-based novel object segmentation on the seven core datasets of the BOP challenge, surpassing the recent method of Chen et al. by absolute 19.8% AP.",
        "authors": [
            "Van Nguyen Nguyen",
            "Tomás Hodan",
            "G. Ponimatkin",
            "Thibault Groueix",
            "V. Lepetit"
        ],
        "citations": 27,
        "references": 28,
        "year": 2023
    },
    {
        "title": "HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?",
        "abstract": "There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber- Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR)? Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data into LLMs and utilizes the role-play and “think step-by-step“ strategies for prompting. We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models. Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform all the baselines on both datasets. Our findings indicate that by effective prompting, LLMs can interpret raw IMU data based on their knowledge base, possessing a promising potential to analyze raw sensor data of the physical world effectively.",
        "authors": [
            "Sijie Ji",
            "Xinzhe Zheng",
            "Chenshu Wu"
        ],
        "citations": 16,
        "references": 22,
        "year": 2024
    },
    {
        "title": "Segment Anything for Microscopy",
        "abstract": "We present Segment Anything for Microscopy, a tool for interactive and automatic segmentation and tracking of objects in multi-dimensional microscopy data. Our method is based on Segment Anything, a vision foundation model for image segmentation. We extend it by training specialized models for microscopy data that significantly improve segmentation quality for a wide range of imaging conditions. We also implement annotation tools for interactive (volumetric) segmentation and tracking, that speed up data annotation significantly compared to established tools. Our work constitutes the first application of vision foundation models to microscopy, laying the groundwork for solving image analysis problems in these domains with a small set of powerful deep learning architectures.",
        "authors": [
            "Anwai Archit",
            "Sushmita Nair",
            "Nabeel Khalid",
            "Paul Hilt",
            "Vikas Rajashekar",
            "Marei Freitag",
            "Sagnik Gupta",
            "A. Dengel",
            "Sheraz Ahmed",
            "Constantin Pape"
        ],
        "citations": 33,
        "references": 46,
        "year": 2023
    },
    {
        "title": "FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System",
        "abstract": "Federated Learning trains machine learning models on distributed devices by aggregating local model updates instead of local data. However, privacy concerns arise as the aggregated local models on the server may reveal sensitive personal information by inversion attacks. Privacy-preserving methods, such as homomorphic encryption (HE), then become necessary for FL training. Despite HE's privacy advantages, its applications suffer from impractical overheads, especially for foundation models. In this paper, we present FedML-HE, the first practical federated learning system with efficient HE-based secure model aggregation. FedML-HE proposes to selectively encrypt sensitive parameters, significantly reducing both computation and communication overheads during training while providing customizable privacy preservation. Our optimized system demonstrates considerable overhead reduction, particularly for large foundation models (e.g., ~10x reduction for ResNet-50, and up to ~40x reduction for BERT), demonstrating the potential for scalable HE-based FL deployment.",
        "authors": [
            "Weizhao Jin",
            "Yuhang Yao",
            "Shanshan Han",
            "Carlee Joe-Wong",
            "Srivatsan Ravi",
            "A. Avestimehr",
            "Chaoyang He"
        ],
        "citations": 34,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Ladder Fine-tuning approach for SAM integrating complementary network",
        "abstract": "Recently, foundation models have been introduced demonstrating various tasks in the field of computer vision. These models such as Segment Anything Model (SAM) are generalized models trained using huge datasets. Currently, ongoing research focuses on exploring the effective utilization of these generalized models for specific domains, such as medical imaging. However, in medical imaging, the lack of training samples due to privacy concerns and other factors presents a major challenge for applying these generalized models to medical image segmentation task. To address this issue, the effective fine tuning of these models is crucial to ensure their optimal utilization. In this study, we propose to combine a complementary Convolutional Neural Network (CNN) along with the standard SAM network for medical image segmentation. To reduce the burden of fine tuning large foundation model and implement cost-efficient trainnig scheme, we focus only on fine-tuning the additional CNN network and SAM decoder part. This strategy significantly reduces trainnig time and achieves competitive results on publicly available dataset. The code is available at https://github.com/11yxk/SAM-LST.",
        "authors": [
            "Shurong Chai",
            "R. Jain",
            "Shiyu Teng",
            "Jiaqing Liu",
            "Yinhao Li",
            "T. Tateyama",
            "Yen-Wei Chen"
        ],
        "citations": 26,
        "references": 30,
        "year": 2023
    },
    {
        "title": "PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting",
        "abstract": "This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios. We evaluate different SOTA numerical-based forecasting methods and language generation models. The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction. Additionally, in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.",
        "authors": [
            "Hao Xue",
            "Flora D.Salim"
        ],
        "citations": 96,
        "references": 32,
        "year": 2022
    },
    {
        "title": "Visual Tuning",
        "abstract": "Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: fine-tuning, prompt tuning, adapter tuning, parameter tuning, and remapping tuning. Meanwhile, it offers some exciting research directions for prospective pre-training and various interactions in visual tuning.",
        "authors": [
            "Bruce X. B. Yu",
            "Jianlong Chang",
            "Haixin Wang",
            "Lin Liu",
            "Shijie Wang",
            "Zhiyu Wang",
            "Junfan Lin",
            "Lingxi Xie",
            "Haojie Li",
            "Zhouchen Lin",
            "Qi Tian",
            "Chang Wen Chen"
        ],
        "citations": 28,
        "references": 315,
        "year": 2023
    },
    {
        "title": "Deep Bidirectional Language-Knowledge Graph Pretraining",
        "abstract": "Pretraining a language model (LM) on text has been shown to help various downstream NLP tasks. Recent works show that a knowledge graph (KG) can complement text data, offering structured background knowledge that provides a useful scaffold for reasoning. However, these works are not pretrained to learn a deep fusion of the two modalities at scale, limiting the potential to acquire fully joint representations of text and KG. Here we propose DRAGON (Deep Bidirectional Language-Knowledge Graph Pretraining), a self-supervised approach to pretraining a deeply joint language-knowledge foundation model from text and KG at scale. Specifically, our model takes pairs of text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. We pretrain this model by unifying two self-supervised reasoning tasks, masked language modeling and KG link prediction. DRAGON outperforms existing LM and LM+KG models on diverse downstream tasks including question answering across general and biomedical domains, with +5% absolute gain on average. In particular, DRAGON achieves notable performance on complex reasoning about language and knowledge (+10% on questions involving long contexts or multi-step reasoning) and low-resource QA (+8% on OBQA and RiddleSense), and new state-of-the-art results on various BioNLP tasks. Our code and trained models are available at https://github.com/michiyasunaga/dragon.",
        "authors": [
            "Michihiro Yasunaga",
            "Antoine Bosselut",
            "Hongyu Ren",
            "Xikun Zhang",
            "Christopher D. Manning",
            "Percy Liang",
            "J. Leskovec"
        ],
        "citations": 171,
        "references": 87,
        "year": 2022
    },
    {
        "title": "Language-Aware Domain Generalization Network for Cross-Scene Hyperspectral Image Classification",
        "abstract": "Text information including extensive prior knowledge about land cover classes has been ignored in hyperspectral image (HSI) classification tasks. It is necessary to explore the effectiveness of linguistic mode in assisting HSI classification. In addition, the large-scale pretraining image–text foundation models have demonstrated great performance in a variety of downstream applications, including zero-shot transfer. However, most domain generalization methods have never addressed mining linguistic modal knowledge to improve the generalization performance of model. To compensate for the inadequacies listed above, a language-aware domain generalization network (LDGnet) is proposed to learn cross-domain-invariant representation from cross-domain shared prior knowledge. The proposed method only trains on the source domain (SD) and then transfers the model to the target domain (TD). The dual-stream architecture including the image encoder and text encoder is used to extract visual and linguistic features, in which coarse-grained and fine-grained text representations are designed to extract two levels of linguistic features. Furthermore, linguistic features are used as cross-domain shared semantic space, and visual–linguistic alignment is completed by supervised contrastive learning in semantic space. Extensive experiments on three datasets demonstrate the superiority of the proposed method when compared with the state-of-the-art techniques. The codes will be available from the website: https://github.com/YuxiangZhang-BIT/IEEE_TGRS_LDGnet.",
        "authors": [
            "Yuxiang Zhang",
            "Mengmeng Zhang",
            "Wei Li",
            "Shuai Wang",
            "Ran Tao"
        ],
        "citations": 92,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
        "abstract": "Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.",
        "authors": [
            "Yucheng Li"
        ],
        "citations": 23,
        "references": 17,
        "year": 2023
    },
    {
        "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
        "abstract": "Instruction tuning has become the de facto method to equip large language models (LLMs) with the ability of following user instructions. Usually, hundreds of thousands or millions of instruction-following pairs are employed to fine-tune the foundation LLMs. Recently, some studies show that a small number of high-quality instruction data is enough. However, how to select appropriate instruction data for a given LLM is still an open problem. To address this problem, in this paper we present a model-oriented data selection (MoDS) approach, which selects instruction data based on a new criteria considering three aspects: quality, coverage and necessity. First, our approach utilizes a quality evaluation model to filter out the high-quality subset from the original instruction dataset, and then designs an algorithm to further select from the high-quality subset a seed instruction dataset with good coverage. The seed dataset is applied to fine-tune the foundation LLM to obtain an initial instruction-following LLM. Finally, we develop a necessity evaluation model to find out the instruction data which are performed badly in the initial instruction-following LLM and consider them necessary instructions to further improve the LLMs. In this way, we can get a small high-quality, broad-coverage and high-necessity subset from the original instruction datasets. Experimental results show that, the model fine-tuned with 4,000 instruction pairs selected by our approach could perform better than the model fine-tuned with the full original dataset which includes 214k instruction data.",
        "authors": [
            "Qianlong Du",
            "Chengqing Zong",
            "Jiajun Zhang"
        ],
        "citations": 55,
        "references": 35,
        "year": 2023
    },
    {
        "title": "An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry",
        "abstract": "Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems. In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.",
        "authors": [
            "Wenxin Jiang",
            "Nicholas Synovic",
            "Matt Hyatt",
            "Taylor R. Schorlemmer",
            "R. Sethi",
            "Yung-Hsiang Lu",
            "G. Thiruvathukal",
            "James C. Davis"
        ],
        "citations": 51,
        "references": 97,
        "year": 2023
    },
    {
        "title": "The role of complexity for digital twins of cities",
        "abstract": null,
        "authors": [
            "Guido Caldarelli",
            "E. Arcaute",
            "Marc Barthelemy",
            "Michael Batty",
            "Carlos Gershenson",
            "Dirk Helbing",
            "Stefano Mancuso",
            "Yamir Moreno",
            "J. Ramasco",
            "Céline Rozenblat",
            "Ángel Sánchez",
            "J. Fernández-Villacañas"
        ],
        "citations": 52,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Human Language Understanding & Reasoning",
        "abstract": "Abstract The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.",
        "authors": [
            "Christopher D. Manning"
        ],
        "citations": 91,
        "references": 25,
        "year": 2022
    },
    {
        "title": "Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels",
        "abstract": "Current 3D scene segmentation methods are heavily dependent on manually annotated 3D training datasets. Such manual annotations are labor-intensive, and often lack fine-grained details. Importantly, models trained on this data typically struggle to recognize object classes beyond the annotated classes, i.e., they do not generalize well to unseen domains and require additional domain-specific annotations. In contrast, 2D foundation models demonstrate strong generalization and impressive zero-shot abilities, inspiring us to incorporate these characteristics from 2D models into 3D models. Therefore, we explore the use of image segmentation foundation models to automatically generate training labels for 3D segmentation. We propose Segment3D, a method for class-agnostic 3D scene segmentation that produces high-quality 3D segmentation masks. It improves over existing 3D segmentation models (especially on fine-grained masks), and enables easily adding new training data to further boost the segmentation performance -- all without the need for manual training labels.",
        "authors": [
            "Rui Huang",
            "Songyou Peng",
            "Ayca Takmaz",
            "Federico Tombari",
            "Marc Pollefeys",
            "Shiji Song",
            "Gao Huang",
            "Francis Engelmann"
        ],
        "citations": 23,
        "references": 58,
        "year": 2023
    },
    {
        "title": "SAM-Path: A Segment Anything Model for Semantic Segmentation in Digital Pathology",
        "abstract": "Semantic segmentations of pathological entities have crucial clinical value in computational pathology workflows. Foundation models, such as the Segment Anything Model (SAM), have been recently proposed for universal use in segmentation tasks. SAM shows remarkable promise in instance segmentation on natural images. However, the applicability of SAM to computational pathology tasks is limited due to the following factors: (1) lack of comprehensive pathology datasets used in SAM training and (2) the design of SAM is not inherently optimized for semantic segmentation tasks. In this work, we adapt SAM for semantic segmentation by introducing trainable class prompts, followed by further enhancements through the incorporation of a pathology encoder, specifically a pathology foundation model. Our framework, SAM-Path enhances SAM's ability to conduct semantic segmentation in digital pathology without human input prompts. Through experiments on two public pathology datasets, the BCSS and the CRAG datasets, we demonstrate that the fine-tuning with trainable class prompts outperforms vanilla SAM with manual prompts and post-processing by 27.52% in Dice score and 71.63% in IOU. On these two datasets, the proposed additional pathology foundation model further achieves a relative improvement of 5.07% to 5.12% in Dice score and 4.50% to 8.48% in IOU.",
        "authors": [
            "Jingwei Zhang",
            "Ke Ma",
            "S. Kapse",
            "J. Saltz",
            "M. Vakalopoulou",
            "P. Prasanna",
            "D. Samaras"
        ],
        "citations": 23,
        "references": 22,
        "year": 2023
    },
    {
        "title": "Self-Supervised Representations in Speech-Based Depression Detection",
        "abstract": "This paper proposes handling training data sparsity in speech-based automatic depression detection (SDD) using foundation models pre-trained with self-supervised learning (SSL). An analysis of SSL representations derived from different layers of pre-trained foundation models is first presented for SDD, which provides insight to suitable indicator for depression detection. Knowledge transfer is then performed from automatic speech recognition (ASR) and emotion recognition to SDD by fine-tuning the foundation models. Results show that the uses of oracle and ASR transcriptions yield similar SDD performance when the hidden representations of the ASR model is incorporated along with the ASR textual information. By integrating representations from multiple foundation models, state-of-the-art SDD results based on real ASR were achieved on the DAIC-WOZ dataset.",
        "authors": [
            "Wen Wu",
            "C. Zhang",
            "P. Woodland"
        ],
        "citations": 20,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis",
        "abstract": "We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. Surprisingly, it performs well on various 3D tasks, requiring no parameters or training, and even surpasses existing fully trained models. Starting from this basic non-parametric model, we propose two extensions. First, Point-NN can serve as a base architectural framework to construct Parametric Networks by simply inserting linear layers on top. Given the superior non-parametric foundation, the derived Point-PN exhibits a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be regarded as a plug-and-play module for the already trained 3D models during inference. Point-NN captures the complementary geometric knowledge and enhances existing methods for different 3D benchmarks without re-training. We hope our work may cast a light on the community for understanding 3D point clouds with non-parametric methods. Code is available at https://github.com/ZrrSkywalker/Point-NN.",
        "authors": [
            "Renrui Zhang",
            "Liuhui Wang",
            "Yali Wang",
            "Peng Gao",
            "Hongsheng Li",
            "Jianbo Shi"
        ],
        "citations": 44,
        "references": 100,
        "year": 2023
    },
    {
        "title": "A Wide Evaluation of ChatGPT on Affective Computing Tasks",
        "abstract": "With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem. Such models have been shown to have emergent properties of solving problems that they were not initially trained on. The studies for the effectiveness of such models are still quite limited. In this work, we widely study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13 affective computing problems, namely aspect extraction, aspect polarity classification, opinion extraction, sentiment analysis, sentiment intensity ranking, emotions intensity ranking, suicide tendency detection, toxicity detection, well-being assessment, engagement measurement, personality assessment, sarcasm detection, and subjectivity detection. We introduce a framework to evaluate the ChatGPT models on regression-based problems, such as intensity ranking problems, by modelling them as pairwise ranking classification. We compare ChatGPT against more traditional NLP methods, such as end-to-end recurrent neural networks and transformers. The results demonstrate the emergent abilities of the ChatGPT models on a wide range of affective computing problems, where GPT-3.5 and especially GPT-4 have shown strong performance on many problems, particularly the ones related to sentiment, emotions, or toxicity. The ChatGPT models fell short for problems with implicit signals, such as engagement measurement and subjectivity detection.",
        "authors": [
            "Mostafa M. Amin",
            "Rui Mao",
            "E. Cambria",
            "Björn Schuller"
        ],
        "citations": 21,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Forest tree species adaptation to climate across biomes: Building on the legacy of ecological genetics to anticipate responses to climate change",
        "abstract": "Intraspecific variation plays a critical role in extant and future forest responses to climate change. Forest tree species with wide climatic niches rely on the intraspecific variation resulting from genetic adaptation and phenotypic plasticity to accommodate spatial and temporal climate variability. A centuries‐old legacy of forest ecological genetics and provenance trials has provided a strong foundation upon which to continue building on this knowledge, which is critical to maintain climate‐adapted forests. Our overall objective is to understand forest trees intraspecific responses to climate across species and biomes, while our specific objectives are to describe ecological genetics models used to build our foundational knowledge, summarize modeling approaches that have expanded the traditional toolset, and extensively review the literature from 1994 to 2021 to highlight the main contributions of this legacy and the new analyzes of provenance trials. We reviewed 103 studies comprising at least three common gardens, which covered 58 forest tree species, 28 of them with range‐wide studies. Although studies using provenance trial data cover mostly commercially important forest tree species from temperate and boreal biomes, this synthesis provides a global overview of forest tree species adaptation to climate. We found that evidence for genetic adaptation to local climate is commonly present in the species studied (79%), being more common in conifers (87.5%) than in broadleaf species (67%). In 57% of the species, clines in fitness‐related traits were associated with temperature variables, in 14% of the species with precipitation, and in 25% of the species with both. Evidence of adaptation lags was found in 50% of the species with range‐wide studies. We conclude that ecological genetics models and analysis of provenance trial data provide excellent insights on intraspecific genetic variation, whereas the role and limits of phenotypic plasticity, which will likely determine the fate of extant forests, is vastly understudied.",
        "authors": [
            "L. Leites",
            "Marta Benito Garzón"
        ],
        "citations": 43,
        "references": 184,
        "year": 2023
    },
    {
        "title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
        "abstract": "The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialized models, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant.",
        "authors": [
            "He Cao",
            "Zijing Liu",
            "Xingyu Lu",
            "Yuan Yao",
            "Yu Li"
        ],
        "citations": 41,
        "references": 100,
        "year": 2023
    },
    {
        "title": "Advancing Neural Encoding of Portuguese with Transformer Albertina PT-",
        "abstract": null,
        "authors": [
            "J. Rodrigues",
            "Luís Gomes",
            "J. Silva",
            "A. Branco",
            "Rodrigo Santos",
            "Henrique Lopes Cardoso",
            "T. Os'orio"
        ],
        "citations": 40,
        "references": 36,
        "year": 2023
    },
    {
        "title": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit",
        "abstract": "This paper introduces FunASR, an open-source speech recognition toolkit designed to bridge the gap between academic research and industrial applications. FunASR offers models trained on large-scale industrial corpora and the ability to deploy them in applications. The toolkit's flagship model, Paraformer, is a non-autoregressive end-to-end speech recognition model that has been trained on a manually annotated Mandarin speech recognition dataset that contains 60,000 hours of speech. To improve the performance of Paraformer, we have added timestamp prediction and hotword customization capabilities to the standard Paraformer backbone. In addition, to facilitate model deployment, we have open-sourced a voice activity detection model based on the Feedforward Sequential Memory Network (FSMN-VAD) and a text post-processing punctuation model based on the controllable time-delay Transformer (CT-Transformer), both of which were trained on industrial corpora. These functional modules provide a solid foundation for building high-precision long audio speech recognition services. Compared to other models trained on open datasets, Paraformer demonstrates superior performance.",
        "authors": [
            "Zhifu Gao",
            "Zerui Li",
            "Jiaming Wang",
            "Haoneng Luo",
            "Xian Shi",
            "Mengzhe Chen",
            "Yabin Li",
            "Lingyun Zuo",
            "Zhihao Du",
            "Zhangyu Xiao",
            "Shiliang Zhang"
        ],
        "citations": 38,
        "references": 31,
        "year": 2023
    },
    {
        "title": "The Wayback Machine",
        "abstract": "abstract:We suffer from a radical autonomy which too often collapses the therapeutic alliance between patient and physician into a health-care transaction between consumer and provider, a fee-for-service exchange for something far short of true health. Some ethicists and physicians are seeking a better way, by employing a virtue ethics approach in which health is seen as a distinct good and the proper end of a medical encounter. Curlin and Tollefsen's The Way of Medicine (2021) synthesizes this material into a heuristic contrasting what they characterize as the Provider Services Model and the Way of Medicine. The authors believe physicians must choose between the two models and serve, respectively, either the well-being or the health of the people they meet as patients. Between the authors' dichotomous choices, many physicians will find a middle way in virtue ethics approaches, which instead characterize health as a communal foundation to human flourishing and autonomy as serving communal as well as individual goods.",
        "authors": [
            "Abraham M. Nussbaum"
        ],
        "citations": 126,
        "references": 27,
        "year": 2022
    },
    {
        "title": "Segment Anything in Non-Euclidean Domains: Challenges and Opportunities",
        "abstract": "The recent work known as Segment Anything (SA) has made significant strides in pushing the boundaries of semantic segmentation into the era of foundation models. The impact of SA has sparked extremely active discussions and ushered in an encouraging new wave of developing foundation models for the diverse tasks in the Euclidean domain, such as object detection and image inpainting. Despite the promising advances led by SA, the concept has yet to be extended to the non-Euclidean graph domain. In this paper, we explore a novel Segment Non-Euclidean Anything (SNA) paradigm that strives to develop foundation models that can handle the diverse range of graph data within the non-Euclidean domain, seeking to expand the scope of SA and lay the groundwork for future research in this direction. To achieve this goal, we begin by discussing the recent achievements in foundation models associated with SA. We then shed light on the unique challenges that arise when applying the SA concept to graph analysis, which involves understanding the differences between the Euclidean and non-Euclidean domains from both the data and task perspectives. Motivated by these observations, we present several preliminary solutions to tackle the challenges of SNA and detail their corresponding limitations, along with several potential directions to pave the way for future SNA research. Experiments on five Open Graph Benchmark (OGB) datasets across various tasks, including graph property classification and regression, as well as multi-label prediction, demonstrate that the performance of the naive SNA solutions has considerable room for improvement, pointing towards a promising avenue for future exploration of Graph General Intelligence.",
        "authors": [
            "Yongcheng Jing",
            "Xinchao Wang",
            "Dacheng Tao"
        ],
        "citations": 19,
        "references": 67,
        "year": 2023
    },
    {
        "title": "MathChat: Converse to Tackle Challenging Math Problems with LLM Agents",
        "abstract": "Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. LLMs, with their generalized ability, are used as a foundation model to build AI agents for different tasks. In this paper, we study the effectiveness of utilizing LLM agents to solve math problems through conversations. We propose MathChat, a conversational problem-solving framework designed for math problems. MathChat consists of an LLM agent and a user proxy agent which is responsible for tool execution and additional guidance. This synergy facilitates a collaborative problem-solving process, where the agents engage in a dialogue to solve the problems. We perform evaluation on difficult high school competition problems from the MATH dataset. Utilizing Python, we show that MathChat can further improve previous tool-using prompting methods by 6%.",
        "authors": [
            "Yiran Wu",
            "Feiran Jia",
            "Shaokun Zhang",
            "Han-Tai Li",
            "Erkang Zhu",
            "Yue Wang",
            "Y. Lee",
            "Richard Peng",
            "Qingyun Wu",
            "Chi Wang"
        ],
        "citations": 41,
        "references": 50,
        "year": 2023
    },
    {
        "title": "GhostFaceNets: Lightweight Face Recognition Model From Cheap Operations",
        "abstract": "The development of deep learning-based biometric models that can be deployed on devices with constrained memory and computational resources has proven to be a significant challenge. Previous approaches to this problem have not prioritized the reduction of feature map redundancy, but the introduction of Ghost modules represents a major innovation in this area. Ghost modules use a series of inexpensive linear transformations to extract additional feature maps from a set of intrinsic features, allowing for a more comprehensive representation of the underlying information. GhostNetV1 and GhostNetV2, both of which are based on Ghost modules, serve as the foundation for a group of lightweight face recognition models called GhostFaceNets. GhostNetV2 expands upon the original GhostNetV1 by adding an attention mechanism to capture long-range dependencies. Evaluation of GhostFaceNets using various benchmarks reveals that these models offer superior performance while requiring a computational complexity of approximately 60–275 MFLOPs. This is significantly lower than that of State-Of-The-Art (SOTA) big convolutional neural network (CNN) models, which can require hundreds of millions of FLOPs. GhostFaceNets trained with the ArcFace loss on the refined MS-Celeb-1M dataset demonstrate SOTA performance on all benchmarks. In comparison to previous SOTA mobile CNNs, GhostFaceNets greatly improve efficiency for face verification tasks. The GhostFaceNets code is available at: https://github.com/HamadYA/GhostFaceNets.",
        "authors": [
            "Mohamad Alansari",
            "Oussama Abdul Hay",
            "S. Javed",
            "Abdulhadi Shoufan",
            "Y. Zweiri",
            "N. Werghi"
        ],
        "citations": 41,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Towards Segment Anything Model (SAM) for Medical Image Segmentation: A Survey",
        "abstract": "Due to the flexibility of prompting, foundation models have become the dominant force in the domains of natural language processing and image generation. With the recent introduction of the Segment Anything Model (SAM), the prompt-driven paradigm has entered the realm of image segmentation, bringing with a range of previously unexplored capabilities. However, it remains unclear whether it can be applicable to medical image segmentation due to the significant differences between natural images and medical images.In this work, we summarize recent efforts to extend the success of SAM to medical image segmentation tasks, including both empirical benchmarking and methodological adaptations, and discuss potential future directions for SAM in medical image segmentation. Although directly applying SAM to medical image segmentation cannot obtain satisfying performance on multi-modal and multi-target medical datasets, many insights are drawn to guide future research to develop foundation models for medical image analysis. To facilitate future research, we maintain an active repository that contains up-to-date paper list and open-source project summary at https://github.com/YichiZhang98/SAM4MIS.",
        "authors": [
            "Yichi Zhang",
            "Rushi Jiao"
        ],
        "citations": 17,
        "references": 70,
        "year": 2023
    },
    {
        "title": "EarthNets: Empowering AI in Earth Observation",
        "abstract": "Earth observation (EO), aiming at monitoring the state of planet Earth using remote sensing data, is critical for improving our daily lives and living environment. With a growing number of satellites in orbit, an increasing number of datasets with diverse sensors and research domains are being published to facilitate the research of the remote sensing community. This paper presents a comprehensive review of more than 500 publicly published datasets, including research domains like agriculture, land use and land cover, disaster monitoring, scene understanding, vision-language models, foundation models, climate change, and weather forecasting. We systematically analyze these EO datasets from four aspects: volume, resolution distributions, research domains, and the correlation between datasets. Based on the dataset attributes, we propose to measure, rank, and select datasets to build a new benchmark for model evaluation. Furthermore, a new platform for EO, termed EarthNets, is released to achieve a fair and consistent evaluation of deep learning methods on remote sensing data. EarthNets supports standard dataset libraries and cutting-edge deep learning models to bridge the gap between the remote sensing and machine learning communities. Based on this platform, extensive deep-learning methods are evaluated on the new benchmark. The insightful results are beneficial to future research. The platform and dataset collections are publicly available at https://earthnets.github.io.",
        "authors": [
            "Zhitong Xiong",
            "Fahong Zhang",
            "Yi Wang",
            "Yilei Shi",
            "Xiao Xiang Zhu"
        ],
        "citations": 72,
        "references": 427,
        "year": 2022
    },
    {
        "title": "An Overview of Technological Parameter Optimization in the Case of Laser Cladding",
        "abstract": "This review examines the methods used to optimize the process parameters of laser cladding, including traditional optimization algorithms such as single-factor, regression analysis, response surface, and Taguchi, as well as intelligent system optimization algorithms such as neural network models, genetic algorithms, support vector machines, the new non-dominance ranking genetic algorithm II, and particle swarm algorithms. The advantages and disadvantages of various laser cladding process optimization methods are analyzed and summarized. Finally, the development trend of optimization methods in the field of laser cladding is summarized and predicted. It is believed that the result would serve as a foundation for future studies on the preparation of high-quality laser cladding coatings.",
        "authors": [
            "Kaiming Wang",
            "Wei Liu",
            "Yuxiang Hong",
            "H. M. S. Sohan",
            "Y. Tong",
            "Yongle Hu",
            "Mingjun Zhang",
            "Jian Zhang",
            "Dingding Xiang",
            "H. Fu",
            "J. Ju"
        ],
        "citations": 37,
        "references": 152,
        "year": 2023
    },
    {
        "title": "Elements of ∞-Category Theory",
        "abstract": "The language of\n ∞-categories provides an insightful new way of expressing many\n results in higher-dimensional mathematics but can be challenging\n for the uninitiated. To explain what exactly an ∞-category is\n requires various technical models, raising the question of how\n they might be compared. To overcome this, a model-independent\n approach is desired, so that theorems proven with any model\n would apply to them all. This text develops the theory of\n ∞-categories from first principles in a model-independent\n fashion using the axiomatic framework of an ∞-cosmos, the\n universe in which ∞-categories live as objects. An ∞-cosmos is a\n fertile setting for the formal category theory of ∞-categories,\n and in this way the foundational proofs in ∞-category theory\n closely resemble the classical foundations of ordinary category\n theory. Equipped with exercises and appendices with background\n material, this first introduction is meant for students and\n researchers who have a strong foundation in classical 1-category\n theory.",
        "authors": [
            "E. Riehl",
            "Dominic R. Verity"
        ],
        "citations": 94,
        "references": 163,
        "year": 2022
    },
    {
        "title": "A Survey on Optimization Techniques for Edge Artificial Intelligence (AI)",
        "abstract": "Artificial Intelligence (Al) models are being produced and used to solve a variety of current and future business and technical problems. Therefore, AI model engineering processes, platforms, and products are acquiring special significance across industry verticals. For achieving deeper automation, the number of data features being used while generating highly promising and productive AI models is numerous, and hence the resulting AI models are bulky. Such heavyweight models consume a lot of computation, storage, networking, and energy resources. On the other side, increasingly, AI models are being deployed in IoT devices to ensure real-time knowledge discovery and dissemination. Real-time insights are of paramount importance in producing and releasing real-time and intelligent services and applications. Thus, edge intelligence through on-device data processing has laid down a stimulating foundation for real-time intelligent enterprises and environments. With these emerging requirements, the focus turned towards unearthing competent and cognitive techniques for maximally compressing huge AI models without sacrificing AI model performance. Therefore, AI researchers have come up with a number of powerful optimization techniques and tools to optimize AI models. This paper is to dig deep and describe all kinds of model optimization at different levels and layers. Having learned the optimization methods, this work has highlighted the importance of having an enabling AI model optimization framework.",
        "authors": [
            "Chellammal Surianarayanan",
            "J. Lawrence",
            "P. Chelliah",
            "E. Prakash",
            "C. Hewage"
        ],
        "citations": 35,
        "references": 106,
        "year": 2023
    },
    {
        "title": "Next Generation Mega Satellite Networks for Access Equality: Opportunities, Challenges, and Performance",
        "abstract": "Digital connectivity has become the foundation of prosperity and an essential need for functioning societies. Despite this dependence, limitation on Internet access remains a prevalent issue, largely hinging on socioeconomic and geographic factors. A promising solution to attain global access equality is based on integrated terrestrial-satellite networks that rely on low Earth orbit (LEO) mega constellations. While the benefits of LEO constellations complement the shortcomings of terrestrial networks, their incorporation impacts the network design, adding complexity and challenges. This article presents a systematic analysis of next generation LEO mega satellite constellations, outlining opportunities by virtue of the many benefits these constellations can provide and highlighting the major challenges. Furthermore, it provides a synopsis of analytic models and underscores modern simulation approaches for next generation mega satellite constellations. This article provides network designers with the necessary insights into satellite network performance.",
        "authors": [
            "Bassel Al Homssi",
            "A. Al-Hourani",
            "Ke Wang",
            "P. Conder",
            "S. Kandeepan",
            "Jinho Choi",
            "Brandon Allen",
            "Ben Moores"
        ],
        "citations": 97,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Model-as-a-Service (MaaS): A Survey",
        "abstract": "Due to the increased number of parameters and data in the pre-trained model exceeding a certain level, a foundation model (e.g., a large language model) can significantly improve downstream task performance and emerge with some novel special abilities (e.g., deep learning, complex reasoning, and human alignment) that were not present before. Foundation models are a form of generative artificial intelligence (GenAI), and Model-as-a-Service (MaaS) has emerged as a groundbreaking paradigm that revolutionizes the deployment and utilization of GenAI models. MaaS represents a paradigm shift in how we use AI technologies and provides a scalable and accessible solution for developers and users to leverage pre-trained AI models without the need for extensive infrastructure or expertise in model training. In this paper, the introduction aims to provide a comprehensive overview of MaaS, its significance, and its implications for various industries. We provide a brief review of the development history of “X-as-a-Service” based on cloud computing and present the key technologies involved in MaaS. The development of GenAI models will become more democratized and flourish. We also review recent application studies of MaaS. Finally, we highlight several challenges and future issues in this promising area. MaaS is a new deployment and service paradigm for different AI-based models. We hope this review will inspire future research in the field of MaaS.",
        "authors": [
            "Wensheng Gan",
            "Shicheng Wan",
            "Philip S. Yu"
        ],
        "citations": 14,
        "references": 79,
        "year": 2023
    },
    {
        "title": "AI safety on whose terms?",
        "abstract": "Rapid, widespread adoption of the latest large language models has sparked both excitement and concern about advanced artificial intelligence (AI). In response, many are looking to the field of AI safety for answers. Major AI companies are purportedly investing heavily in this young research program, even as they cut “trust and safety” teams addressing harms from current systems. Governments are taking notice too. The United Kingdom just invested £100 million in a new “Foundation Model Taskforce” and plans an AI safety summit this year. And yet, as research priorities are being set, it is already clear that the prevailing technical agenda for AI safety is inadequate to address critical questions. Only a sociotechnical approach can truly limit current and potential dangers of advanced AI.",
        "authors": [
            "Seth Lazar",
            "A. Nelson"
        ],
        "citations": 33,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Exploring the Boundaries of GPT-4 in Radiology",
        "abstract": "The recent success of general-domain large language models (LLMs) has significantly changed the natural language processing paradigm towards a unified foundation model across domains and applications. In this paper, we focus on assessing the performance of GPT-4, the most capable LLM so far, on the text-based applications for radiology reports, comparing against state-of-the-art (SOTA) radiology-specific models. Exploring various prompting strategies, we evaluated GPT-4 on a diverse range of common radiology tasks and we found GPT-4 either outperforms or is on par with current SOTA radiology models. With zero-shot prompting, GPT-4 already obtains substantial gains ($\\approx$ 10% absolute improvement) over radiology models in temporal sentence similarity classification (accuracy) and natural language inference ($F_1$). For tasks that require learning dataset-specific style or schema (e.g. findings summarisation), GPT-4 improves with example-based prompting and matches supervised SOTA. Our extensive error analysis with a board-certified radiologist shows GPT-4 has a sufficient level of radiology knowledge with only occasional errors in complex context that require nuanced domain knowledge. For findings summarisation, GPT-4 outputs are found to be overall comparable with existing manually-written impressions.",
        "authors": [
            "Qianchu Liu",
            "Stephanie L. Hyland",
            "Shruthi Bannur",
            "Kenza Bouzid",
            "Daniel C. Castro",
            "M. Wetscherek",
            "Robert Tinn",
            "Harshita Sharma",
            "Fernando Pérez-García",
            "Anton Schwaighofer",
            "Pranav Rajpurkar",
            "Sameer Tajdin Khanna",
            "Hoifung Poon",
            "Naoto Usuyama",
            "Anja Thieme",
            "A. Nori",
            "M. Lungren",
            "O. Oktay",
            "Javier Alvarez-Valle"
        ],
        "citations": 34,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Energy Transformer",
        "abstract": "Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion task, and obtain strong quantitative results on the graph anomaly detection and graph classification tasks.",
        "authors": [
            "Benjamin Hoover",
            "Yuchen Liang",
            "Bao Pham",
            "Rameswar Panda",
            "H. Strobelt",
            "Duen Horng Chau",
            "Mohammed J. Zaki",
            "D. Krotov"
        ],
        "citations": 28,
        "references": 71,
        "year": 2023
    },
    {
        "title": "A Generalist Dynamics Model for Control",
        "abstract": "We investigate the use of transformer sequence models as dynamics models (TDMs) for control. We find that TDMs exhibit strong generalization capabilities to unseen environments, both in a few-shot setting, where a generalist TDM is fine-tuned with small amounts of data from the target environment, and in a zero-shot setting, where a generalist TDM is applied to an unseen environment without any further training. Here, we demonstrate that generalizing system dynamics can work much better than generalizing optimal behavior directly as a policy. Additional results show that TDMs also perform well in a single-environment learning setting when compared to a number of baseline models. These properties make TDMs a promising ingredient for a foundation model of control.",
        "authors": [
            "Ingmar Schubert",
            "Jingwei Zhang",
            "Jake Bruce",
            "Sarah Bechtle",
            "Emilio Parisotto",
            "Martin A. Riedmiller",
            "Jost Tobias Springenberg",
            "Arunkumar Byravan",
            "Leonard Hasenclever",
            "N. Heess"
        ],
        "citations": 28,
        "references": 60,
        "year": 2023
    },
    {
        "title": "The Development of AgriVerse: Past, Present, and Future",
        "abstract": "Agricultural metaverse (AgriVerse) aims to optimize the production chain by saving costs, increasing efficiencies, and breaking information silos, in order to achieve sustainable agriculture. While AgriVerse is featured by the virtual-real interaction of the agriculture-related processes based on heterogeneous data, knowledge, and models, the link between AgriVerse and the intensively studied plant modeling is vague. This article presents briefly the research contents of plant modeling, analyzes the ongoing transition at the age of artificial intelligence (AI), and envisions future AgriVerse with the support of the agricultural foundation model, the decentralized agricultural organization (DAO) and the decentralized science (DeSci) of the plant model. Three AgriVerse application scenarios are presented. The opportunities and challenges of AgriVerse are discussed. This work is expected to identify the key research issues of AgriVerse and bring practitioners of diverse backgrounds together into the AgriVerse community.",
        "authors": [
            "Mengzhen Kang",
            "Xiujuan Wang",
            "Haoyu Wang",
            "Jing Hua",
            "P. Reffye",
            "Fei-Yue Wang"
        ],
        "citations": 28,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events",
        "abstract": "Generative, pre-trained transformers (GPTs, a.k.a.\"Foundation Models\") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper provides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that incorporates intra-event causal dependency structures and autoregressive generation capabilities, and (3) evaluate models via standardized processes that can assess few and even zero-shot performance of pre-trained models on user-specified fine-tuning tasks.",
        "authors": [
            "Matthew B. A. McDermott",
            "Bret A. Nestor",
            "Peniel Argaw",
            "I. Kohane"
        ],
        "citations": 12,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks",
        "abstract": "Despite the remarkable success of foundation models, their task-specific fine-tuning paradigm makes them inconsistent with the goal of general perception modeling. The key to eliminating this inconsistency is to use generalist models for general task modeling. However, existing attempts at generalist models are inadequate in both versatility and performance. In this paper, we propose Uni-Perceiver v2, which is the first generalist model capable of handling major large-scale vision and vision-language tasks with competitive performance. Specifically, images are encoded as general region proposals, while texts are encoded via a Transformer-based language model. The encoded representations are transformed by a task-agnostic decoder. Different tasks are formulated as a unified maximum likelihood estimation problem. We further propose an effective optimization technique named Task-Balanced Gradient Normalization to ensure stable multi-task learning with an unmixed sampling strategy, which is helpful for tasks requiring large batch-size training. After being jointly trained on various tasks, Uni-Perceiver v2 is capable of directly handling downstream tasks without any task-specific adaptation. Results show that Uni-Perceiver v2 outperforms all existing generalist models in both versatility and performance. Meanwhile, compared with the commonly-recognized strong baselines that require tasks-specific fine-tuning, Uni-Perceiver v2 achieves competitive performance on a broad range of vision and vision-language tasks.",
        "authors": [
            "Hao Li",
            "Jinguo Zhu",
            "Xiaohu Jiang",
            "Xizhou Zhu",
            "Hongsheng Li",
            "Chun Yuan",
            "Xiaohua Wang",
            "Y. Qiao",
            "Xiaogang Wang",
            "Wenhai Wang",
            "Jifeng Dai"
        ],
        "citations": 50,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes",
        "abstract": "Humans and animals have a rich and flexible understanding of the physical world, which enables them to infer the underlying dynamical trajectories of objects and events, plausible future states, and use that to plan and anticipate the consequences of actions. However, the neural mechanisms underlying these computations are unclear. We combine a goal-driven modeling approach with dense neurophysiological data and high-throughput human behavioral readouts that contain thousands of comparisons to directly impinge on this question. Specifically, we construct and evaluate several classes of sensory-cognitive networks to predict the future state of rich, ethologically-relevant environments, ranging from self-supervised end-to-end models with pixel-wise or object-slot objectives, to models that future predict in the latent space of purely static image-pretrained or dynamic video-pretrained foundation models. We find that “scale is not all you need”, and that many state-of-the-art machine learning models fail to perform well on our neural and behavioral benchmarks for future prediction. In fact, only one class of models matches these data well overall. We find that neural responses are currently best predicted by models trained to predict the future state of their environment in the latent space of pretrained foundation models optimized for dynamic scenes in a self-supervised manner. These models also approach the neurons’ ability to predict the environmental state variables that are visually hidden from view, despite not being explicitly trained to do so. Finally, we find that not all foundation model latents are equal. Notably, models that future predict in the latent space of video foundation models that are optimized to support a diverse range of egocentric sensorimotor tasks, reasonably match both human behavioral error patterns and neural dynamics across all environmental scenarios that we were able to test. Overall, these findings suggest that the neural mechanisms and behaviors of primate mental simulation have strong inductive biases associated with them, and are thus far most consistent with being optimized to future predict on reusable visual representations that are useful for Embodied AI more generally.",
        "authors": [
            "Aran Nayebi",
            "R. Rajalingham",
            "M. Jazayeri",
            "G. R. Yang"
        ],
        "citations": 13,
        "references": 64,
        "year": 2023
    },
    {
        "title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences",
        "abstract": "Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose ChiMed-GPT, a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate ChiMed-GPT's superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting ChiMed-GPT to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain. The code and model are released at https://github.com/synlp/ChiMed-GPT.",
        "authors": [
            "Yuanhe Tian",
            "Ruyi Gan",
            "Yan Song",
            "Jiaxing Zhang",
            "Yongdong Zhang"
        ],
        "citations": 22,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment",
        "abstract": "Urban environments, characterized by their complex, multi-layered networks encompassing physical, social, economic, and environmental dimensions, face significant challenges in the face of rapid urbanization. These challenges, ranging from traffic congestion and pollution to social inequality, call for advanced technological interventions. Recent developments in big data, artificial intelligence, urban computing, and digital twins have laid the groundwork for sophisticated city modeling and simulation. However, a gap persists between these technological capabilities and their practical implementation in addressing urban challenges in an systemic-intelligent way. This paper proposes Urban Generative Intelligence (UGI), a novel foundational platform integrating Large Language Models (LLMs) into urban systems to foster a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model trained on city-specific multi-source data, to create embodied agents for various urban tasks. These agents, operating within a textual urban environment emulated by city simulator and urban knowledge graph, interact through a natural language interface, offering an open platform for diverse intelligent and embodied agent development. This platform not only addresses specific urban issues but also simulates complex urban systems, providing a multidisciplinary approach to understand and manage urban complexity. This work signifies a transformative step in city science and urban intelligence, harnessing the power of LLMs to unravel and address the intricate dynamics of urban systems. The code repository with demonstrations will soon be released here https://github.com/tsinghua-fib-lab/UGI.",
        "authors": [
            "Fengli Xu",
            "Jun Zhang",
            "Chen Gao",
            "J. Feng",
            "Yong Li"
        ],
        "citations": 19,
        "references": 188,
        "year": 2023
    },
    {
        "title": "Confidence-Building Measures for Artificial Intelligence: Workshop Proceedings",
        "abstract": "Foundation models could eventually introduce several pathways for undermining state security: accidents, inadvertent escalation, unintentional conflict, the proliferation of weapons, and the interference with human diplomacy are just a few on a long list. The Confidence-Building Measures for Artificial Intelligence workshop hosted by the Geopolitics Team at OpenAI and the Berkeley Risk and Security Lab at the University of California brought together a multistakeholder group to think through the tools and strategies to mitigate the potential risks introduced by foundation models to international security. Originating in the Cold War, confidence-building measures (CBMs) are actions that reduce hostility, prevent conflict escalation, and improve trust between parties. The flexibility of CBMs make them a key instrument for navigating the rapid changes in the foundation model landscape. Participants identified the following CBMs that directly apply to foundation models and which are further explained in this conference proceedings: 1. crisis hotlines 2. incident sharing 3. model, transparency, and system cards 4. content provenance and watermarks 5. collaborative red teaming and table-top exercises and 6. dataset and evaluation sharing. Because most foundation model developers are non-government entities, many CBMs will need to involve a wider stakeholder community. These measures can be implemented either by AI labs or by relevant government actors.",
        "authors": [
            "Sarah Shoker",
            "A. Reddie",
            "Sarah Barrington",
            "Miles Brundage",
            "H. Chahal",
            "Michael Depp",
            "Bill Drexel",
            "Ritwik Gupta",
            "Marina Favaro",
            "J. Hecla",
            "Alan Hickey",
            "Margarita Konaev",
            "K. Kumar",
            "Nathan Lambert",
            "A. Lohn",
            "Cullen O'Keefe",
            "Nazneen Rajani",
            "M. Sellitto",
            "Robert F. Trager",
            "L. Walker",
            "Alexa Wehsener",
            "Jessica Young"
        ],
        "citations": 9,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval",
        "abstract": "Recent rapid advancements in deep pre-trained language models and the introductions of large datasets have powered research in embedding-based dense retrieval. While several good research papers have emerged, many of them come with their own software stacks. These stacks are typically optimized for some particular research goals instead of efficiency or code structure. In this paper, we present Tevatron, a dense retrieval toolkit optimized for efficiency, flexibility, and code simplicity. Tevatron provides a standardized pipeline for dense retrieval including text processing, model training, corpus/query encoding, and search. This paper presents an overview of Tevatron and demonstrates its effectiveness and efficiency across several IR and QA data sets. We also show how Tevatron's flexible design enables easy generalization across datasets, model architectures, and accelerator platforms(GPU/TPU). We believe Tevatron can serve as an effective software foundation for dense retrieval system research including design, modeling, and optimization.",
        "authors": [
            "Luyu Gao",
            "Xueguang Ma",
            "Jimmy J. Lin",
            "Jamie Callan"
        ],
        "citations": 67,
        "references": 26,
        "year": 2022
    },
    {
        "title": "Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation",
        "abstract": "Panoramic images with their 360° directional view encompass exhaustive information about the surrounding space, providing a rich foundation for scene understanding. To unfold this potential in the form of robust panoramic segmentation models, large quantities of expensive, pixel-wise annotations are crucial for success. Such annotations are available, but predominantly for narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal resources for training panoramic models. Distortions and the distinct image-feature distribution in 360° panoramas impede the transfer from the annotation-rich pinhole domain and therefore come with a big dent in performance. To get around this domain difference and bring together semantic annotations from pinhole- and 360° surround-visuals, we propose to learn object deformations and panoramic image distortions in the Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) components which blend into our Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we tie together shared semantics in pinhole- and panoramic feature embeddings by generating multi-scale prototype features and aligning them in our Mutual Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor Stanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance to fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled panoramas. On the outdoor DensePASS dataset, we break state-of-the-art by 14.39% mIoU and set the new bar at 56.38%.",
        "authors": [
            "Jiaming Zhang",
            "Kailun Yang",
            "Chaoxiang Ma",
            "Simon Reiß",
            "Kunyu Peng",
            "R. Stiefelhagen"
        ],
        "citations": 61,
        "references": 98,
        "year": 2022
    },
    {
        "title": "A consensus statement on detection of hippocampal sharp wave ripples and differentiation from other fast oscillations",
        "abstract": null,
        "authors": [
            "Anli Liu",
            "S. Henin",
            "S. Abbaspoor",
            "A. Bragin",
            "E. Buffalo",
            "Jordan S. Farrell",
            "David J. Foster",
            "L. Frank",
            "T. Gedankien",
            "J. Gotman",
            "J. Guidera",
            "K. Hoffman",
            "J. Jacobs",
            "M. Kahana",
            "Lin Li",
            "Zhenrui Liao",
            "Jack J. Lin",
            "A. Losonczy",
            "R. Malach",
            "Matthijs A. A. van der Meer",
            "Kathryn McClain",
            "B. McNaughton",
            "Yitzhak Norman",
            "A. Navas-Olive",
            "L. M. de la Prida",
            "J. W. Rueckemann",
            "John J. Sakon",
            "I. Skelin",
            "I. Soltesz",
            "B. Staresina",
            "S. Weiss",
            "M. Wilson",
            "K. Zaghloul",
            "Michaël Zugaro",
            "G. Buzsáki"
        ],
        "citations": 65,
        "references": 170,
        "year": 2022
    },
    {
        "title": "Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM)",
        "abstract": "The advent of foundation models signals a new era in artificial intelligence. The Segment Anything Model (SAM) is the first foundation model for image segmentation. In this study, we evaluate SAM's ability to segment features from eye images recorded in virtual reality setups. The increasing requirement for annotated eye-image datasets presents a significant opportunity for SAM to redefine the landscape of data annotation in gaze estimation. Our investigation centers on SAM's zero-shot learning abilities and the effectiveness of prompts like bounding boxes or point clicks. Our results are consistent with studies in other domains, demonstrating that SAM's segmentation effectiveness can be on-par with specialized models depending on the feature, with prompts improving its performance, evidenced by an IoU of 93.34% for pupil segmentation in one dataset. Foundation models like SAM could revolutionize gaze estimation by enabling quick and easy image segmentation, reducing reliance on specialized models and extensive manual annotation.",
        "authors": [
            "Virmarie Maquiling",
            "Sean Anthony Byrne",
            "D. Niehorster",
            "Marcus Nyström",
            "Enkelejda Kasneci"
        ],
        "citations": 8,
        "references": 48,
        "year": 2023
    },
    {
        "title": "MotherNet: A Foundational Hypernetwork for Tabular Classification",
        "abstract": "The advent of Foundation Models is transforming machine learning across many modalities (e.g., language, images, videos) with prompt engineering replacing training in many settings. Recent work on tabular data (e.g., TabPFN) hints at a similar opportunity to build Foundation Models for classification for numerical data. In this paper, we go one step further and propose a hypernetwork architecture that we call MotherNet, trained on millions of classification tasks, that, once prompted with a never-seen-before training set generates the weights of a trained ``child'' neural-network. Like other Foundation Models, MotherNet replaces training on specific datasets with in-context learning through a single forward pass. In contrast to existing hypernetworks that were either task-specific or trained for relatively constraint multi-task settings, MotherNet is trained to generate networks to perform multiclass classification on arbitrary tabular datasets without any dataset specific gradient descent. The child network generated by MotherNet using in-context learning outperforms neural networks trained using gradient descent on small datasets, and is competitive with predictions by TabPFN and standard ML methods like Gradient Boosting. Unlike a direct application of transformer models like TabPFN, MotherNet generated networks are highly efficient at inference time. This methodology opens up a new approach to building predictive models on tabular data that is both efficient and robust, without any dataset-specific training.",
        "authors": [
            "Andreas Müller",
            "C. Curino",
            "Raghu Ramakrishnan"
        ],
        "citations": 8,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Astronomia ex machina: a history, primer and outlook on neural networks in astronomy",
        "abstract": "In this review, we explore the historical development and future prospects of artificial intelligence (AI) and deep learning in astronomy. We trace the evolution of connectionism in astronomy through its three waves, from the early use of multilayer perceptrons, to the rise of convolutional and recurrent neural networks, and finally to the current era of unsupervised and generative deep learning methods. With the exponential growth of astronomical data, deep learning techniques offer an unprecedented opportunity to uncover valuable insights and tackle previously intractable problems. As we enter the anticipated fourth wave of astronomical connectionism, we argue for the adoption of GPT-like foundation models fine-tuned for astronomical applications. Such models could harness the wealth of high-quality, multimodal astronomical data to serve state-of-the-art downstream tasks. To keep pace with advancements driven by Big Tech, we propose a collaborative, open-source approach within the astronomy community to develop and maintain these foundation models, fostering a symbiotic relationship between AI and astronomy that capitalizes on the unique strengths of both fields.",
        "authors": [
            "Michael J. Smith",
            "J. Geach"
        ],
        "citations": 26,
        "references": 348,
        "year": 2022
    },
    {
        "title": "Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning",
        "abstract": "People say,\"A picture is worth a thousand words\". Then how can we get the rich information out of the image? We argue that by using visual clues to bridge large pretrained vision foundation models and language models, we can do so without any extra cross-modal training. Thanks to the strong zero-shot capability of foundation models, we start by constructing a rich semantic representation of the image (e.g., image tags, object attributes / locations, captions) as a structured textual prompt, called visual clues, using a vision foundation model. Based on visual clues, we use large language model to produce a series of comprehensive descriptions for the visual content, which is then verified by the vision model again to select the candidate that aligns best with the image. We evaluate the quality of generated descriptions by quantitative and qualitative measurement. The results demonstrate the effectiveness of such a structured semantic representation.",
        "authors": [
            "Yujia Xie",
            "Luowei Zhou",
            "Xiyang Dai",
            "Lu Yuan",
            "Nguyen Bach",
            "Ce Liu",
            "Michael Zeng"
        ],
        "citations": 26,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Shoring Up the Foundations: Fusing Model Embeddings and Weak Supervision",
        "abstract": "Foundation models offer an exciting new paradigm for constructing models with out-of-the-box embeddings and a few labeled examples. However, it is not clear how to best apply foundation models without labeled data. A potential approach is to fuse foundation models with weak supervision frameworks, which use weak label sources -- pre-trained models, heuristics, crowd-workers -- to construct pseudolabels. The challenge is building a combination that best exploits the signal available in both foundation models and weak sources. We propose Liger, a combination that uses foundation model embeddings to improve two crucial elements of existing weak supervision techniques. First, we produce finer estimates of weak source quality by partitioning the embedding space and learning per-part source accuracies. Second, we improve source coverage by extending source votes in embedding space. Despite the black-box nature of foundation models, we prove results characterizing how our approach improves performance and show that lift scales with the smoothness of label distributions in embedding space. On six benchmark NLP and video tasks, Liger outperforms vanilla weak supervision by 14.1 points, weakly-supervised kNN and adapters by 11.8 points, and kNN and adapters supervised by traditional hand labels by 7.2 points.",
        "authors": [
            "Mayee F. Chen",
            "Daniel Y. Fu",
            "Dyah Adila",
            "Michael Zhang",
            "Frederic Sala",
            "Kayvon Fatahalian",
            "Christopher R'e"
        ],
        "citations": 20,
        "references": 52,
        "year": 2022
    },
    {
        "title": "PTab: Using the Pre-trained Language Model for Modeling Tabular Data",
        "abstract": "Tabular data is the foundation of the information age and has been extensively studied. Recent studies show that neural-based models are effective in learning contextual representation for tabular data. The learning of an effective contextual representation requires meaningful features and a large amount of data. However, current methods often fail to properly learn a contextual representation from the features without semantic information. In addition, it's intractable to enlarge the training set through mixed tabular datasets due to the difference between datasets. To address these problems, we propose a novel framework PTab, using the Pre-trained language model to model Tabular data. PTab learns a contextual representation of tabular data through a three-stage processing: Modality Transformation(MT), Masked-Language Fine-tuning(MF), and Classification Fine-tuning(CF). We initialize our model with a pre-trained Model (PTM) which contains semantic information learned from the large-scale language data. Consequently, contextual representation can be learned effectively during the fine-tuning stages. In addition, we can naturally mix the textualized tabular data to enlarge the training set to further improve representation learning. We evaluate PTab on eight popular tabular classification datasets. Experimental results show that our method has achieved a better average AUC score in supervised settings compared to the state-of-the-art baselines(e.g. XGBoost), and outperforms counterpart methods under semi-supervised settings. We present visualization results that show PTab has well instance-based interpretability.",
        "authors": [
            "Guangyi Liu",
            "Jie Yang",
            "Ledell Yu Wu"
        ],
        "citations": 32,
        "references": 26,
        "year": 2022
    },
    {
        "title": "Spns1 is a lysophospholipid transporter mediating lysosomal phospholipid salvage",
        "abstract": "Significance Lysosomes mediate the hydrolysis of macromolecules for which the breakdown products must be transported out, failure of which could result in lysosomal storage diseases. While lysosomal transporters for amino acids, monosaccharides, and ions have been characterized, less is known about the identity of lipid transporters, particularly whether lysosomes export lysophospholipids that are breakdown products of phosphatidylcholine and phosphatidylethanolamine, the most abundant membrane phospholipids. This study combined a cell-based screen with biochemical, cell, and in vivo models to identify SPNS1, a previously orphaned transporter, as the transporter that mediates the rate-limiting lysosomal efflux of lysophospholipids for their recycling into cellular phospholipid pools. The deorphanization of SPNS1 sets a foundation for studying the role of lysolipid transport and recycling in physiology and disease.",
        "authors": [
            "Menglan He",
            "Alvin C. Y. Kuk",
            "Mei Ding",
            "C. Chin",
            "D. L. Galam",
            "J. Nah",
            "Bryan C. Tan",
            "Hui Li Yeo",
            "G. Chua",
            "P. Benke",
            "M. Wenk",
            "L. Ho",
            "F. Torta",
            "D. Silver"
        ],
        "citations": 37,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Stability Analysis of the Grid-Connected Inverter Considering the Asymmetric Positive-Feedback Loops Introduced by the PLL in Weak Grids",
        "abstract": "The stability of the grid-connected inverter (GCI) system in weak grids is deteriorated due to the asymmetric positive-feedback loops (APFLs) introduced by the phase-locked loop (PLL) structure. Under the same main circuit parameters and control loop parameters, the small-signal models of the GCI controlled in dq domain and αβ domain are constructed, respectively. Compared with the small-signal model of the GCI controlled in dq domain, the number of APFLs introduced by the PLL of GCI controlled in αβ domain is one less. Moreover, the mechanism why the stability of the GCI controlled in αβ domain is stronger than that controlled in dq domain is analyzed. Meanwhile, the influence degree and mechanism of the APFLs in different paths of the small-signal models for the stability of GCI are revealed for the first time. This lays a theoretical foundation for the analysis and design of asymmetric control strategies used to improve the stability of GCI in weak grids. Finally, simulations and experiments verify the correctness of the theoretical analysis.",
        "authors": [
            "Chunming Tu",
            "Jiayuan Gao",
            "Fan Xiao",
            "Qi Guo",
            "Fei Jiang"
        ],
        "citations": 44,
        "references": 28,
        "year": 2022
    },
    {
        "title": "Retrieval-Enhanced Machine Learning",
        "abstract": "Although information access systems have long supportedpeople in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.",
        "authors": [
            "Hamed Zamani",
            "Fernando Diaz",
            "Mostafa Dehghani",
            "Donald Metzler",
            "Michael Bendersky"
        ],
        "citations": 43,
        "references": 81,
        "year": 2022
    },
    {
        "title": "Risk Field Model of Driving and Its Application in Modeling Car-Following Behavior",
        "abstract": "Microscopic modeling of driving behavior is the basis for traffic design and traffic simulation studies and can be applied to automated driving systems to provide human-like decision making. Previous modeling methods can be mainly divided into scenario-based modeling methods and field theory-based modeling methods. Scenario-based models are based on behavior theories that can explain behavioral mechanisms and field theory-based models are convenient for application to different scenarios. Combining two behavior theories and field theory, this paper aims to present a novel method to uniformly model the driving behavior in different scenarios. Risk homeostasis theory and preview-follower theory are used as the theoretical foundation, and field theory is utilized to connect the two behavior theories. A new risk field model is constructed for better coupling these behavior theories. Integrating these theories, this study then develops a subjectively perceived risk quantification method and a trajectory and motion planning model, which are validated using naturalistic data in car-following scenarios. Results show the effectiveness of this method and this model with reference to an effective risk quantification index (safety margin) and in comparison with the classical models (desired safety margin model and intelligent driver model) using naturalistic data in car-following scenarios.",
        "authors": [
            "Haitian Tan",
            "G. Lu",
            "Miaomiao Liu"
        ],
        "citations": 38,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Supersonic jet noise from launch vehicles: 50 years since NASA SP-8072.",
        "abstract": "In 1971, the U.S. National Aeronautics and Space Administration (NASA) published a seminal report-NASA SP-8072-which compiled the results of the early supersonic jet noise studies and provided methods to calculate the noise produced from launch vehicles. Fifty years later and despite known limitations, SP-8072 remains the foundation for much of the launch vehicle noise modeling today. This article reviews what has been learned about the physics of noise generation and radiation from free and impinging rocket plumes since the completion of SP-8072. State-of-the-art methods for the mitigation of launch vehicle noise are also reviewed. A discussion of launch vehicle noise modeling, from empirical to numerical and including reduced-order models of supersonic jets, points to promising approaches that can describe rocket noise characteristics not captured by SP-8072.",
        "authors": [
            "Caroline P. Lubert",
            "K. Gee",
            "S. Tsutsumi"
        ],
        "citations": 34,
        "references": 321,
        "year": 2022
    },
    {
        "title": "LegalBench: Prototyping a Collaborative Benchmark for Legal Reasoning",
        "abstract": "Can foundation models be guided to execute tasks involving legal reasoning? We believe that building a benchmark to answer this question will require sustained collaborative efforts between the computer science and legal communities. To that end, this short paper serves three purposes. First, we describe how IRAC-a framework legal scholars use to distinguish different types of legal reasoning-can guide the construction of a Foundation Model oriented benchmark. Second, we present a seed set of 44 tasks built according to this framework. We discuss initial findings, and highlight directions for new tasks. Finally-inspired by the Open Science movement-we make a call for the legal and computer science communities to join our efforts by contributing new tasks. This work is ongoing, and our progress can be tracked here: https://github.com/HazyResearch/legalbench.",
        "authors": [
            "Neel Guha",
            "Daniel E. Ho",
            "Julian Nyarko",
            "Christopher R'e"
        ],
        "citations": 16,
        "references": 31,
        "year": 2022
    },
    {
        "title": "On Some Properties of the Beta Inverse Rayleigh Distribution",
        "abstract": "We study with some details a lifetime model of the class of beta generalized models, called the beta inverse Rayleigh distribution, which is a special case of the Beta Fr\\'echet distribution. We provide a better foundation for some properties including quantile function, moments, mean deviations, Bonferroni and Lorenz curves, R\\'enyi and Shannon entropies and order statistics. We fit the proposed model using maximum likelihood estimation to a real data set to illustrate its flexibility and potentiality.",
        "authors": [
            "Jeremias Leão",
            "H. Saulo",
            "M. Bourguignon",
            "R. Cintra",
            "L. Rêgo",
            "G. Cordeiro"
        ],
        "citations": 26,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Towards a general-purpose foundation model for computational pathology.",
        "abstract": null,
        "authors": [
            "Richard J. Chen",
            "Tong Ding",
            "Ming Y. Lu",
            "Drew F. K. Williamson",
            "Guillaume Jaume",
            "Andrew H. Song",
            "Bowen Chen",
            "Andrew Zhang",
            "Daniel Shao",
            "Muhammad Shaban",
            "Mane Williams",
            "Lukas Oldenburg",
            "Luca L Weishaupt",
            "Judy J. Wang",
            "Anurag Vaidya",
            "L. Le",
            "Georg K. Gerber",
            "S. Sahai",
            "Walt Williams",
            "Faisal Mahmood"
        ],
        "citations": 211,
        "references": 80,
        "year": 2024
    },
    {
        "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
        "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .",
        "authors": [
            "Shengding Hu",
            "Yuge Tu",
            "Xu Han",
            "Chaoqun He",
            "Ganqu Cui",
            "Xiang Long",
            "Zhi Zheng",
            "Yewei Fang",
            "Yuxiang Huang",
            "Weilin Zhao",
            "Xinrong Zhang",
            "Zhen Leng Thai",
            "Kaihuo Zhang",
            "Chongyi Wang",
            "Yuan Yao",
            "Chenyang Zhao",
            "Jie Zhou",
            "Jie Cai",
            "Zhongwu Zhai",
            "Ning Ding",
            "Chaochao Jia",
            "Guoyang Zeng",
            "Dahai Li",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "citations": 175,
        "references": 73,
        "year": 2024
    },
    {
        "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
        "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.",
        "authors": [
            "S. Tonmoy",
            "S. M. M. Zaman",
            "Vinija Jain",
            "Anku Rani",
            "Vipula Rawte",
            "Aman Chadha",
            "Amitava Das"
        ],
        "citations": 130,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Leveraging large language models for predictive chemistry",
        "abstract": null,
        "authors": [
            "K. Jablonka",
            "P. Schwaller",
            "Andres Ortega‐Guerrero",
            "Berend Smit"
        ],
        "citations": 104,
        "references": 49,
        "year": 2024
    },
    {
        "title": "PMC-LLaMA: toward building open-source language models for medicine",
        "abstract": "OBJECTIVE\nRecently, large language models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering (QA) situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this article, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA.\n\n\nMATERIALS AND METHODS\nWe adapt a general-purpose LLM toward the medical domain, involving data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive domain-specific instruction fine-tuning, encompassing medical QA, rationale for reasoning, and conversational dialogues with 202M tokens.\n\n\nRESULTS\nWhile evaluating various public medical QA benchmarks and manual rating, our lightweight PMC-LLaMA, which consists of only 13B parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, and datasets for instruction tuning will be released to the research community.\n\n\nDISCUSSION\nOur contributions are 3-fold: (1) we build up an open-source LLM toward the medical domain. We believe the proposed PMC-LLaMA model can promote further development of foundation models in medicine, serving as a medical trainable basic generative language backbone; (2) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component, demonstrating how different training data and model scales affect medical LLMs; (3) we contribute a large-scale, comprehensive dataset for instruction tuning.\n\n\nCONCLUSION\nIn this article, we systematically investigate the process of building up an open-source medical-specific LLM, PMC-LLaMA.",
        "authors": [
            "Chaoyi Wu",
            "Weixiong Lin",
            "Xiaoman Zhang",
            "Ya Zhang",
            "Weidi Xie",
            "Yanfeng Wang"
        ],
        "citations": 96,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Universal Cell Embeddings: A Foundation Model for Cell Biology",
        "abstract": "Developing a universal representation of cells which encompasses the tremendous molecular diversity of cell types within the human body and more generally, across species, would be transformative for cell biology. Recent work using single-cell transcriptomic approaches to create molecular definitions of cell types in the form of cell atlases has provided the necessary data for such an endeavor. Here, we present the Universal Cell Embedding (UCE) foundation model. UCE was trained on a corpus of cell atlas data from human and other species in a completely self-supervised way without any data annotations. UCE offers a unified biological latent space that can represent any cell, regardless of tissue or species. This universal cell embedding captures important biological variation despite the presence of experimental noise across diverse datasets. An important aspect of UCE’s universality is that any new cell from any organism can be mapped to this embedding space with no additional data labeling, model training or fine-tuning. We applied UCE to create the Integrated Mega-scale Atlas, embedding 36 million cells, with more than 1,000 uniquely named cell types, from hundreds of experiments, dozens of tissues and eight species. We uncovered new insights about the organization of cell types and tissues within this universal cell embedding space, and leveraged it to infer function of newly discovered cell types. UCE’s embedding space exhibits emergent behavior, uncovering new biology that it was never explicitly trained for, such as identifying developmental lineages and embedding data from novel species not included in the training set. Overall, by enabling a universal representation for every cell state and type, UCE provides a valuable tool for analysis, annotation and hypothesis generation as the scale and diversity of single cell datasets continues to grow.",
        "authors": [
            "Yanay Rosen",
            "Yusuf Roohani",
            "Ayush Agrawal",
            "Leon Samotorčan",
            "S. Quake",
            "J. Leskovec"
        ],
        "citations": 27,
        "references": 70,
        "year": 2024
    },
    {
        "title": "What matters when building vision-language models?",
        "abstract": "The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.",
        "authors": [
            "Hugo Laurençon",
            "Léo Tronchon",
            "Matthieu Cord",
            "Victor Sanh"
        ],
        "citations": 95,
        "references": 144,
        "year": 2024
    },
    {
        "title": "Moshi: a speech-text foundation model for real-time dialogue",
        "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this\"Inner Monologue\"method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
        "authors": [
            "Alexandre D'efossez",
            "Laurent Mazar'e",
            "Manu Orsini",
            "Am'elie Royer",
            "Patrick P'erez",
            "Herv'e J'egou",
            "Edouard Grave",
            "Neil Zeghidour"
        ],
        "citations": 24,
        "references": 0,
        "year": 2024
    },
    {
        "title": "A Foundation Model for the Earth System",
        "abstract": "Reliable forecasts of the Earth system are crucial for human progress and safety from natural disasters. Artificial intelligence offers substantial potential to improve prediction accuracy and computational efficiency in this field, however this remains underexplored in many domains. Here we introduce Aurora, a large-scale foundation model for the Earth system trained on over a million hours of diverse data. Aurora outperforms operational forecasts for air quality, ocean waves, tropical cyclone tracks, and high-resolution weather forecasting at orders of magnitude smaller computational expense than dedicated existing systems. With the ability to fine-tune Aurora to diverse application domains at only modest computational cost, Aurora represents significant progress in making actionable Earth system predictions accessible to anyone.",
        "authors": [
            "Cristian Bodnar",
            "W. Bruinsma",
            "Ana Lucic",
            "Megan Stanley",
            "Anna Vaughan",
            "Johannes Brandstetter",
            "P. Garvan",
            "Maik Riechert",
            "Jonathan A. Weyn",
            "Haiyu Dong",
            "Jayesh K. Gupta",
            "Kit Thambiratnam",
            "Alexander T. Archibald",
            "Chun-Chieh Wu",
            "E. Heider",
            "Max Welling",
            "Richard E. Turner",
            "P. Perdikaris"
        ],
        "citations": 39,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models",
        "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.",
        "authors": [
            "Usman Anwar",
            "Abulhair Saparov",
            "Javier Rando",
            "Daniel Paleka",
            "Miles Turpin",
            "Peter Hase",
            "Ekdeep Singh Lubana",
            "Erik Jenner",
            "Stephen Casper",
            "Oliver Sourbut",
            "Benjamin L. Edelman",
            "Zhaowei Zhang",
            "Mario Gunther",
            "Anton Korinek",
            "J. Hernández-Orallo",
            "Lewis Hammond",
            "Eric J. Bigelow",
            "Alexander Pan",
            "L. Langosco",
            "Tomasz Korbak",
            "Heidi Zhang",
            "Ruiqi Zhong",
            "Se'an 'O h'Eigeartaigh",
            "Gabriel Recchia",
            "Giulio Corsi",
            "Alan Chan",
            "Markus Anderljung",
            "Lilian Edwards",
            "Y. Bengio",
            "Danqi Chen",
            "Samuel Albanie",
            "Tegan Maharaj",
            "Jakob N. Foerster",
            "Florian Tramèr",
            "He He",
            "Atoosa Kasirzadeh",
            "Yejin Choi",
            "David Krueger"
        ],
        "citations": 73,
        "references": 0,
        "year": 2024
    },
    {
        "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
        "abstract": "Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/",
        "authors": [
            "Christoph Schuhmann",
            "Romain Beaumont",
            "Richard Vencu",
            "Cade Gordon",
            "Ross Wightman",
            "Mehdi Cherti",
            "Theo Coombes",
            "Aarush Katta",
            "Clayton Mullis",
            "Mitchell Wortsman",
            "P. Schramowski",
            "Srivatsa Kundurthy",
            "Katherine Crowson",
            "Ludwig Schmidt",
            "R. Kaczmarczyk",
            "J. Jitsev"
        ],
        "citations": 1000,
        "references": 109,
        "year": 2022
    },
    {
        "title": "A visual–language foundation model for pathology image analysis using medical Twitter",
        "abstract": null,
        "authors": [
            "Zhi Huang",
            "Federico Bianchi",
            "Mert Yuksekgonul",
            "T. Montine",
            "J. Zou"
        ],
        "citations": 249,
        "references": 48,
        "year": 2023
    },
    {
        "title": "BrainLM: A foundation model for brain activity recordings",
        "abstract": "We introduce the Brain Language Model (BrainLM), a foundation model for brain activity dynamics trained on 6,700 hours of fMRI recordings. Utilizing self-supervised masked-prediction training, BrainLM demonstrates proficiency in both fine-tuning and zero-shot inference tasks. Fine-tuning allows for the accurate prediction of clinical variables like age, anxiety, and PTSD as well as forecasting of future brain states. Critically, the model generalizes well to entirely new external cohorts not seen during training. In zero-shot inference mode, BrainLM can identify intrinsic functional networks directly from raw fMRI data without any network-based supervision during training. The model also generates interpretable latent representations that reveal relationships between brain activity patterns and cognitive states. Overall, BrainLM offers a versatile and interpretable framework for elucidating the complex spatiotemporal dynamics of human brain activity. It serves as a powerful “lens” through which massive repositories of fMRI data can be analyzed in new ways, enabling more effective interpretation and utilization at scale. The work demonstrates the potential of foundation models to advance computational neuroscience research.",
        "authors": [
            "J. O. Caro",
            "Antonio H. O. Fonseca",
            "Christopher Averill",
            "S. Rizvi",
            "Matteo Rosati",
            "James L. Cross",
            "Prateek Mittal",
            "E. Zappala",
            "Daniel Levine",
            "Rahul M. Dhodapkar",
            "Insu Han",
            "Amin Karbasi",
            "C. Abdallah",
            "David van Dijk"
        ],
        "citations": 26,
        "references": 34,
        "year": 2024
    },
    {
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
        "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality \\textit{AI feedback} automatically for a scalable alternative. Specifically, we identify \\textbf{scale and diversity} as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present \\textsc{UltraFeedback}, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon \\textsc{UltraFeedback}, we align a LLaMA-based model by best-of-$n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research. Our data and models are available at https://github.com/thunlp/UltraFeedback.",
        "authors": [
            "Ganqu Cui",
            "Lifan Yuan",
            "Ning Ding",
            "Guanming Yao",
            "Wei Zhu",
            "Yuan Ni",
            "Guotong Xie",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "citations": 285,
        "references": 81,
        "year": 2023
    },
    {
        "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
        "abstract": "Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.",
        "authors": [
            "Kaining Ying",
            "Fanqing Meng",
            "Jin Wang",
            "Zhiqiang Li",
            "Han Lin",
            "Yue Yang",
            "Hao Zhang",
            "Wenbo Zhang",
            "Yuqi Lin",
            "Shuo Liu",
            "Jiayi Lei",
            "Quanfeng Lu",
            "Runjian Chen",
            "Peng Xu",
            "Renrui Zhang",
            "Haozhe Zhang",
            "Peng Gao",
            "Yali Wang",
            "Yuning Qiao",
            "Ping Luo",
            "Kaipeng Zhang",
            "Wenqi Shao"
        ],
        "citations": 51,
        "references": 0,
        "year": 2024
    },
    {
        "title": "General Flow as Foundation Affordance for Scalable Robot Learning",
        "abstract": "We address the challenge of acquiring real-world manipulation skills with a scalable framework. We hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning. Therefore, we propose to utilize 3D flow, which represents the future trajectories of 3D points on objects of interest, as an ideal prediction target. To exploit scalable data resources, we turn our attention to human videos. We develop, for the first time, a language-conditioned 3D flow prediction model directly from large-scale RGBD human video datasets. Our predicted flow offers actionable guidance, thus facilitating zero-shot skill transfer in real-world scenarios. We deploy our method with a policy based on closed-loop flow prediction. Remarkably, without any in-domain finetuning, our method achieves an impressive 81\\% success rate in zero-shot human-to-robot skill transfer, covering 18 tasks in 6 scenes. Our framework features the following benefits: (1) scalability: leveraging cross-embodiment data resources; (2) wide application: multiple object categories, including rigid, articulated, and soft bodies; (3) stable skill transfer: providing actionable guidance with a small inference domain-gap. Code, data, and supplementary materials are available https://general-flow.github.io",
        "authors": [
            "Chengbo Yuan",
            "Chuan Wen",
            "Tong Zhang",
            "Yang Gao"
        ],
        "citations": 15,
        "references": 106,
        "year": 2024
    },
    {
        "title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
        "abstract": "Biomedical data is inherently multimodal, comprising physical measurements and natural language narratives. A generalist biomedical AI model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image-text pairs. Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image-text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision-language processing. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of standard datasets, substantially outperforming prior approaches. Intriguingly, by large-scale pretraining on diverse biomedical image types, BiomedCLIP even outperforms state-of-the-art radiology-specific models such as BioViL in radiology-specific tasks such as RSNA pneumonia detection. In summary, BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications. We release our models at https://aka.ms/biomedclip to facilitate future research in multimodal biomedical AI.",
        "authors": [
            "Sheng Zhang",
            "Yanbo Xu",
            "Naoto Usuyama",
            "J. Bagga",
            "Robert Tinn",
            "Sam Preston",
            "Rajesh N. Rao",
            "Mu-Hsin Wei",
            "Naveen Valluri",
            "Cliff Wong",
            "M. Lungren",
            "Tristan Naumann",
            "Hoifung Poon"
        ],
        "citations": 137,
        "references": 58,
        "year": 2023
    },
    {
        "title": "A Survey on Knowledge Distillation of Large Language Models",
        "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of KD's role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement. Our survey is meticulously structured around three foundational pillars: \\textit{algorithm}, \\textit{skill}, and \\textit{verticalization} -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in KD and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.",
        "authors": [
            "Xiaohan Xu",
            "Ming Li",
            "Chongyang Tao",
            "Tao Shen",
            "Reynold Cheng",
            "Jinyang Li",
            "Can Xu",
            "Dacheng Tao",
            "Tianyi Zhou"
        ],
        "citations": 50,
        "references": 388,
        "year": 2024
    },
    {
        "title": "Pretraining a foundation model for generalizable fluorescence microscopy-based image restoration.",
        "abstract": null,
        "authors": [
            "Chenxi Ma",
            "Weimin Tan",
            "Ruian He",
            "Bo Yan"
        ],
        "citations": 13,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "abstract": "This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "authors": [
            "Yang Liu",
            "Jiahuan Cao",
            "Chongyu Liu",
            "Kai Ding",
            "Lianwen Jin"
        ],
        "citations": 35,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
        "abstract": "As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for fine-tuning, as shown in our reward model evaluation case study (FLAMe-RM). On Reward-Bench, FLAMe-RM-24B achieves 87.8% accuracy, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch fine-tuning approach that offers competitive RewardBench performance using 25×fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark.",
        "authors": [
            "Tu Vu",
            "Kalpesh Krishna",
            "Salaheddin Alzubi",
            "Chris Tar",
            "Manaal Faruqui",
            "Yun-Hsuan Sung"
        ],
        "citations": 19,
        "references": 136,
        "year": 2024
    },
    {
        "title": "RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation Based on Visual Foundation Model",
        "abstract": "Leveraging the extensive training data from SA-1B, the segment anything model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this article, we aim to develop an automated instance segmentation approach for remote sensing images based on the foundational SAM model and incorporating semantic category information. Drawing inspiration from prompt learning, we propose a method to learn the generation of appropriate prompts for SAM. This enables SAM to produce semantically discernible segmentation results for remote sensing images, a concept that we have termed RSPrompter. We also propose several ongoing derivatives for instance segmentation tasks, drawing on recent advancements within the SAM community, and compare their performance with RSPrompter. Extensive experimental results, derived from the WHU building dataset, the NWPU VHR-10 dataset, and the SAR Ship Detection Dataset (SSDD) dataset, validate the effectiveness of our proposed method. The code for our method is publicly available at https://kychen.me/RSPrompter.",
        "authors": [
            "Keyan Chen",
            "Chenyang Liu",
            "Hao Chen",
            "Haotian Zhang",
            "Wenyuan Li",
            "Zhengxia Zou",
            "Z. Shi"
        ],
        "citations": 131,
        "references": 111,
        "year": 2023
    },
    {
        "title": "mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
        "abstract": "Recent years have witnessed a big convergence of language, vision, and multi-modal pretraining. In this work, we present mPLUG-2, a new unified paradigm with modularized design for multi-modal pretraining, which can benefit from modality collaboration while addressing the problem of modality entanglement. In contrast to predominant paradigms of solely relying on sequence-to-sequence generation or encoder-based instance discrimination, mPLUG-2 introduces a multi-module composition network by sharing common universal modules for modality collaboration and disentangling different modality modules to deal with modality entanglement. It is flexible to select different modules for different understanding and generation tasks across all modalities including text, image, and video. Empirical study shows that mPLUG-2 achieves state-of-the-art or competitive results on a broad range of over 30 downstream tasks, spanning multi-modal tasks of image-text and video-text understanding and generation, and uni-modal tasks of text-only, image-only, and video-only understanding. Notably, mPLUG-2 shows new state-of-the-art results of 48.0 top-1 accuracy and 80.3 CIDEr on the challenging MSRVTT video QA and video caption tasks with a far smaller model size and data scale. It also demonstrates strong zero-shot transferability on vision-language and video-language tasks. Code and models will be released in https://github.com/alibaba/AliceMind.",
        "authors": [
            "Haiyang Xu",
            "Qinghao Ye",
            "Mingshi Yan",
            "Yaya Shi",
            "Jiabo Ye",
            "Yuanhong Xu",
            "Chenliang Li",
            "Bin Bi",
            "Qiuchen Qian",
            "Wei Wang",
            "Guohai Xu",
            "Ji Zhang",
            "Songfang Huang",
            "Feiran Huang",
            "Jingren Zhou"
        ],
        "citations": 127,
        "references": 126,
        "year": 2023
    },
    {
        "title": "Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions",
        "abstract": "The integration of large language models (LLMs), such as those in the Generative Pre-trained Transformers (GPT) series, into medical education has the potential to transform learning experiences for students and elevate their knowledge, skills, and competence. Drawing on a wealth of professional and academic experience, we propose that LLMs hold promise for revolutionizing medical curriculum development, teaching methodologies, personalized study plans and learning materials, student assessments, and more. However, we also critically examine the challenges that such integration might pose by addressing issues of algorithmic bias, overreliance, plagiarism, misinformation, inequity, privacy, and copyright concerns in medical education. As we navigate the shift from an information-driven educational paradigm to an artificial intelligence (AI)–driven educational paradigm, we argue that it is paramount to understand both the potential and the pitfalls of LLMs in medical education. This paper thus offers our perspective on the opportunities and challenges of using LLMs in this context. We believe that the insights gleaned from this analysis will serve as a foundation for future recommendations and best practices in the field, fostering the responsible and effective use of AI technologies in medical education.",
        "authors": [
            "Alaa A. Abd-alrazaq",
            "Rawan AlSaad",
            "Dari Alhuwail",
            "Arfan Ahmed",
            "P. Healy",
            "Syed Latifi",
            "S. Aziz",
            "R. Damseh",
            "Sadam Alabed Alrazak",
            "Javaid Sheikh"
        ],
        "citations": 196,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
        "abstract": "Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of\"jailbreaking\", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.",
        "authors": [
            "Sibo Yi",
            "Yule Liu",
            "Zhen Sun",
            "Tianshuo Cong",
            "Xinlei He",
            "Jiaxing Song",
            "Ke Xu",
            "Qi Li"
        ],
        "citations": 31,
        "references": 122,
        "year": 2024
    },
    {
        "title": "On the Hidden Mystery of OCR in Large Multimodal Models",
        "abstract": "Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efﬁcacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction. Our ﬁndings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting ﬁne-grained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-speciﬁc methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study could provide a foundational framework for the conception and assessment of innovative strategies targeted at enhancing zero-shot multimodal techniques. Evaluation pipeline will be available at https://github.com/Yuliang-Liu/MultimodalOCR .",
        "authors": [
            "Yuliang Liu",
            "Zhang Li",
            "Hongliang Li",
            "Wenwen Yu",
            "Mingxin Huang",
            "Dezhi Peng",
            "Mingyu Liu",
            "Mingrui Chen",
            "Chunyuan Li",
            "Lianwen Jin",
            "Xiang Bai"
        ],
        "citations": 164,
        "references": 106,
        "year": 2023
    },
    {
        "title": "A foundation model for atomistic materials chemistry",
        "abstract": "Machine-learned force fields have transformed the atomistic modelling of materials by enabling simulations of ab initio quality on unprecedented time and length scales. However, they are currently limited by: (i) the significant computational and human effort that must go into development and validation of potentials for each particular system of interest; and (ii) a general lack of transferability from one chemical system to the next. Here, using the state-of-the-art MACE architecture we introduce a single general-purpose ML model, trained on a public database of 150k inorganic crystals, that is capable of running stable molecular dynamics on molecules and materials. We demonstrate the power of the MACE-MP-0 model - and its qualitative and at times quantitative accuracy - on a diverse set problems in the physical sciences, including the properties of solids, liquids, gases, chemical reactions, interfaces and even the dynamics of a small protein. The model can be applied out of the box and as a starting or\"foundation model\"for any atomistic system of interest and is thus a step towards democratising the revolution of ML force fields by lowering the barriers to entry.",
        "authors": [
            "Ilyes Batatia",
            "Philipp Benner",
            "Chiang Yuan",
            "A. Elena",
            "D. Kov'acs",
            "Janosh Riebesell",
            "Xavier R Advincula",
            "M. Asta",
            "William J. Baldwin",
            "Noam Bernstein",
            "Arghya Bhowmik",
            "Samuel M. Blau",
            "Vlad Cuarare",
            "James P Darby",
            "Sandip De",
            "Flaviano Della Pia",
            "Volker L. Deringer",
            "Rokas Elijovsius",
            "Zakariya El-Machachi",
            "Edvin Fako",
            "A. C. Ferrari",
            "A. Genreith‐Schriever",
            "Janine George",
            "Rhys E. A. Goodall",
            "Clare P. Grey",
            "Shuang Han",
            "Will Handley",
            "H. H. Heenen",
            "K. Hermansson",
            "Christian Holm",
            "Jad Jaafar",
            "Stephan Hofmann",
            "Konstantin S. Jakob",
            "Hyunwook Jung",
            "V. Kapil",
            "Aaron D. Kaplan",
            "Nima Karimitari",
            "Namu Kroupa",
            "J. Kullgren",
            "Matthew C Kuner",
            "Domantas Kuryla",
            "Guoda Liepuoniute",
            "Johannes T. Margraf",
            "Ioan B Magduau",
            "A. Michaelides",
            "J. H. Moore",
            "A. Naik",
            "Samuel P Niblett",
            "Sam Walton Norwood",
            "Niamh O'Neill",
            "Christoph Ortner",
            "Kristin A. Persson",
            "Karsten Reuter",
            "Andrew S. Rosen",
            "Lars L. Schaaf",
            "Christoph Schran",
            "Eric Sivonxay",
            "Tamás K Stenczel",
            "Viktor Svahn",
            "Christopher Sutton",
            "Cas van der Oord",
            "Eszter Varga-Umbrich",
            "T. Vegge",
            "Martin Vondr'ak",
            "Yangshuai Wang",
            "William C. Witt",
            "Fabian Zills",
            "G'abor Cs'anyi"
        ],
        "citations": 84,
        "references": 295,
        "year": 2023
    },
    {
        "title": "When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions",
        "abstract": "The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations, challenges, and future directions. Through an exploration of the challenges faced by FL and FM individually and their interconnections, we aim to inspire future research directions that can further enhance both fields, driving advancements and propelling the development of privacy-preserving and scalable AI systems.",
        "authors": [
            "Weiming Zhuang",
            "Chen Chen",
            "Lingjuan Lyu"
        ],
        "citations": 71,
        "references": 145,
        "year": 2023
    },
    {
        "title": "FinGPT: Open-Source Financial Large Language Models",
        "abstract": "Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data. In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT} and \\url{https://github.com/AI4Finance-Foundation/FinNLP}",
        "authors": [
            "Hongyang Yang",
            "Xiao-Yang Liu",
            "Chris Wang"
        ],
        "citations": 154,
        "references": 19,
        "year": 2023
    },
    {
        "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
        "abstract": "Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM",
        "authors": [
            "Wei He",
            "Kai Han",
            "Yehui Tang",
            "Chengcheng Wang",
            "Yujie Yang",
            "Tianyu Guo",
            "Yunhe Wang"
        ],
        "citations": 20,
        "references": 38,
        "year": 2024
    },
    {
        "title": "OmniGlue: Generalizable Feature Matching with Foundation Model Guidance",
        "abstract": "The image matching field has been witnessing a contin-uous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that de-spite these gains, their potential for real-world applications is restricted by their limited generalization capabili-ties to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is de-signed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting general-ization to domains not seen at training time. Addition-ally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appear-ance information, leading to enhanced matching descrip-tors. We perform comprehensive experiments on a suite of 7 datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue's novel components lead to relative gains on unseen domains of 20.9% with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by 9.5% relatively. Code and model can be found at https://hwjiang151o.github.io/OmniGlue.",
        "authors": [
            "Hanwen Jiang",
            "Hanwen Jiang",
            "Arjun Karpur",
            "Bingyi Cao",
            "Qixing Huang",
            "Qi-Xing Huang"
        ],
        "citations": 10,
        "references": 59,
        "year": 2024
    },
    {
        "title": "uniGradICON: A Foundation Model for Medical Image Registration",
        "abstract": "Conventional medical image registration approaches directly optimize over the parameters of a transformation model. These approaches have been highly successful and are used generically for registrations of different anatomical regions. Recent deep registration networks are incredibly fast and accurate but are only trained for specific tasks. Hence, they are no longer generic registration approaches. We therefore propose uniGradICON, a first step toward a foundation model for registration providing 1) great performance \\emph{across} multiple datasets which is not feasible for current learning-based registration methods, 2) zero-shot capabilities for new registration tasks suitable for different acquisitions, anatomical regions, and modalities compared to the training dataset, and 3) a strong initialization for finetuning on out-of-distribution registration tasks. UniGradICON unifies the speed and accuracy benefits of learning-based registration algorithms with the generic applicability of conventional non-deep-learning approaches. We extensively trained and evaluated uniGradICON on twelve different public datasets. Our code and the uniGradICON model are available at https://github.com/uncbiag/uniGradICON.",
        "authors": [
            "Lin Tian",
            "Hastings Greer",
            "R. Kwitt",
            "François-Xavier Vialard",
            "R. Estépar",
            "Sylvain Bouix",
            "R. Rushmore",
            "Marc Niethammer"
        ],
        "citations": 11,
        "references": 34,
        "year": 2024
    },
    {
        "title": "BiomedParse: a biomedical foundation model for image parsing of everything everywhere all at once",
        "abstract": null,
        "authors": [
            "Theodore Zhao",
            "Yu Gu",
            "Jianwei Yang",
            "Naoto Usuyama",
            "Ho Hin Lee",
            "Tristan Naumann",
            "Jianfeng Gao",
            "Angela Crabtree",
            "B. Piening",
            "Carlo Bifulco",
            "Mu-Hsin Wei",
            "Hoifung Poon",
            "Sheng Wang"
        ],
        "citations": 10,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
        "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
        "authors": [
            "Hoagy Cunningham",
            "Aidan Ewart",
            "Logan Riggs",
            "R. Huben",
            "Lee Sharkey"
        ],
        "citations": 182,
        "references": 38,
        "year": 2023
    },
    {
        "title": "SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery",
        "abstract": "Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense potential towards a generic model for Earth Observation. Nevertheless, these works primar-ily focus on a single modality without temporal and geo-context modeling, hampering their capabilities for diverse tasks. In this study, we present SkySense, a generic billion-scale model, pretrained on a curated multimodal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal sequences. SkySense incorporates a factorized multimodal spatiotemporal encoder taking temporal sequences of opti-cal and Synthetic Aperture Radar (SAR) data as input. This encoder is pretrained by our proposed Multi-Granularity Contrastive Learning to learn representations across different modal and spatial granularities. To further enhance the RSI representations by the geo-context clue, we introduce Geo-Context Prototype Learning to learn region-aware prototypes upon RSI's multimodal spatiotemporal features. To our best knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules can be flexibly combined or used individually to accommodate various tasks. It demonstrates remarkable generalization capabilities on a thor-ough evaluation encompassing 16 datasets over 7 tasks, from single- to multimodal, static to temporal, and classification to localization. SkySense surpasses 18 recent RSFMs in all test scenarios. Specifically, it outperforms the latest models such as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and 3.61% on average respectively. We will release the pretrained weights to facilitate future research and Earth Observation applications.",
        "authors": [
            "Xin Guo",
            "Jiangwei Lao",
            "Bo Dang",
            "Yingying Zhang",
            "Lei Yu",
            "Lixiang Ru",
            "Liheng Zhong",
            "Ziyuan Huang",
            "Kang Wu",
            "Dingxiang Hu",
            "Huimei He",
            "Jian Wang",
            "Jingdong Chen",
            "Ming Yang",
            "Yongjun Zhang",
            "Yansheng Li"
        ],
        "citations": 68,
        "references": 95,
        "year": 2023
    },
    {
        "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
        "abstract": "There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost. The ideas and findings presented here lay a foundation for using LLMs sustainably and efficiently.",
        "authors": [
            "Lingjiao Chen",
            "M. Zaharia",
            "James Y. Zou"
        ],
        "citations": 152,
        "references": 42,
        "year": 2023
    },
    {
        "title": "EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model",
        "abstract": "Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance on various downstream tasks but also provide interpretable outcomes of the useful patterns within the data. To validate the effectiveness of our model, we extensively evaluate it on various downstream tasks and assess the performance under different transfer settings. Furthermore, we demonstrate how the learned model exhibits transferable anomaly detection performance and provides valuable interpretability of the acquired patterns via self-supervised learning.",
        "authors": [
            "Yuqi Chen",
            "Kan Ren",
            "Kaitao Song",
            "Yansen Wang",
            "Yifan Wang",
            "Dongsheng Li",
            "Lili Qiu"
        ],
        "citations": 9,
        "references": 30,
        "year": 2024
    },
    {
        "title": "BioCLIP: A Vision Foundation Model for the Tree of Life",
        "abstract": "Images of the natural world, collected by a variety of cameras, from drones to individual phones, are increasingly abundant sources of biological information. There is an ex-plosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. Yet most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. A vision model for general or-ganismal biology questions on images is of timely need. To approach this, we curate and release Tree Of Life-10m, the largest and most diverse ML-ready dataset of biology images. We then develop Bioclip, a foundation model for the tree of life, leveraging the unique properties of bi-ology captured by Treeoflife-10m, namely the abun-dance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. We rigorously benchmark our approach on di-verse fine-grained biology classification tasks and find that BloCLIP consistently and substantially outperforms existing baselines (by 16% to 17% absolute). Intrinsic evaluation reveals that BloCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability.11imageomics.github.io/bioclip has models, data and code.",
        "authors": [
            "Samuel Stevens",
            "Jiaman Wu",
            "Matthew J. Thompson",
            "Elizabeth G. Campolongo",
            "Chan Hee Song",
            "David Carlyn",
            "Li Dong",
            "W. Dahdul",
            "Charles V. Stewart",
            "Tanya Y. Berger-Wolf",
            "Wei-Lun Chao",
            "Yu Su"
        ],
        "citations": 32,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review",
        "abstract": "This comprehensive review delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). The development of Artificial Intelligence (AI), from its inception in the 1950s to the emergence of advanced neural networks and deep learning architectures, has made a breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in Vision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt engineering is the process of structuring inputs, which has emerged as a crucial technique to maximize the utility and accuracy of these models. This paper explores both foundational and advanced methodologies of prompt engineering, including techniques such as self-consistency, chain-of-thought, and generated knowledge, which significantly enhance model performance. Additionally, it examines the prompt method of VLMs through innovative approaches such as Context Optimization (CoOp), Conditional Context Optimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this discussion is the aspect of AI security, particularly adversarial attacks that exploit vulnerabilities in prompt engineering. Strategies to mitigate these risks and enhance model robustness are thoroughly reviewed. The evaluation of prompt methods is also addressed, through both subjective and objective metrics, ensuring a robust analysis of their efficacy. This review also reflects the essential role of prompt engineering in advancing AI capabilities, providing a structured framework for future research and application.",
        "authors": [
            "Banghao Chen",
            "Zhaofeng Zhang",
            "Nicolas Langren'e",
            "Shengxin Zhu"
        ],
        "citations": 136,
        "references": 129,
        "year": 2023
    },
    {
        "title": "Lens: A Foundation Model for Network Traffic",
        "abstract": "Network traffic refers to the amount of data being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic is challenging due to the diverse nature of data packets, which often feature heterogeneous headers and encrypted payloads lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from massive traffic data. However, these methods typically excel in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundation model for network traffic that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from raw data. To further enhance pre-training effectiveness, we design a novel loss that combines three distinct tasks: Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation results across various benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and generation. Notably, it also requires much less labeled data for fine-tuning compared to current methods.",
        "authors": [
            "Qineng Wang",
            "Chen Qian",
            "Xiaochang Li",
            "Ziyu Yao",
            "Gang Zhou",
            "Huajie Shao"
        ],
        "citations": 7,
        "references": 45,
        "year": 2024
    },
    {
        "title": "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models",
        "abstract": "Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena",
        "authors": [
            "Peng Xu",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Peng Gao",
            "Shuo Liu",
            "Meng Lei",
            "Fanqing Meng",
            "Siyuan Huang",
            "Y. Qiao",
            "Ping Luo"
        ],
        "citations": 130,
        "references": 84,
        "year": 2023
    },
    {
        "title": "EgoVideo: Exploring Egocentric Foundation Model and Downstream Adaptation",
        "abstract": "In this report, we present our solutions to the EgoVis Challenges in CVPR 2024, including five tracks in the Ego4D challenge and three tracks in the EPIC-Kitchens challenge. Building upon the video-language two-tower model and leveraging our meticulously organized egocentric video data, we introduce a novel foundation model called EgoVideo. This model is specifically designed to cater to the unique characteristics of egocentric videos and provides strong support for our competition submissions. In the Ego4D challenges, we tackle various tasks including Natural Language Queries, Step Grounding, Moment Queries, Short-term Object Interaction Anticipation, and Long-term Action Anticipation. In addition, we also participate in the EPIC-Kitchens challenge, where we engage in the Action Recognition, Multiple Instance Retrieval, and Domain Adaptation for Action Recognition tracks. By adapting EgoVideo to these diverse tasks, we showcase its versatility and effectiveness in different egocentric video analysis scenarios, demonstrating the powerful representation ability of EgoVideo as an egocentric foundation model. Our codebase and pretrained models are publicly available at https://github.com/OpenGVLab/EgoVideo.",
        "authors": [
            "Baoqi Pei",
            "Guo Chen",
            "Jilan Xu",
            "Yuping He",
            "Yicheng Liu",
            "Kanghua Pan",
            "Yifei Huang",
            "Yali Wang",
            "Tong Lu",
            "Limin Wang",
            "Yu Qiao"
        ],
        "citations": 6,
        "references": 43,
        "year": 2024
    },
    {
        "title": "AIM: Adapting Image Models for Efficient Video Action Recognition",
        "abstract": "Recent vision transformer based video models mostly follow the ``image pre-training then finetuning\"paradigm and have achieved great success on multiple video benchmarks. However, full finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding. By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability. We show that our proposed AIM can achieve competitive or even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks. Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is \\url{https://adapt-image-models.github.io/}.",
        "authors": [
            "Taojiannan Yang",
            "Yi Zhu",
            "Yusheng Xie",
            "Aston Zhang",
            "Chen Chen",
            "Mu Li"
        ],
        "citations": 122,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
        "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
        "authors": [
            "Xiangyu Qi",
            "Kaixuan Huang",
            "Ashwinee Panda",
            "Mengdi Wang",
            "Prateek Mittal"
        ],
        "citations": 98,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Geometric Latent Diffusion Models for 3D Molecule Generation",
        "abstract": "Generative models, especially diffusion models (DMs), have achieved promising results for generating feature-rich geometries and advancing foundational science problems such as molecule design. Inspired by the recent huge success of Stable (latent) Diffusion models, we propose a novel and principled method for 3D molecule generation named Geometric Latent Diffusion Models (GeoLDM). GeoLDM is the first latent DM model for the molecular geometry domain, composed of autoencoders encoding structures into continuous latent codes and DMs operating in the latent space. Our key innovation is that for modeling the 3D molecular geometries, we capture its critical roto-translational equivariance constraints by building a point-structured latent space with both invariant scalars and equivariant tensors. Extensive experiments demonstrate that GeoLDM can consistently achieve better performance on multiple molecule generation benchmarks, with up to 7\\% improvement for the valid percentage of large biomolecules. Results also demonstrate GeoLDM's higher capacity for controllable generation thanks to the latent modeling. Code is provided at \\url{https://github.com/MinkaiXu/GeoLDM}.",
        "authors": [
            "Minkai Xu",
            "Alexander Powers",
            "R. Dror",
            "Stefano Ermon",
            "J. Leskovec"
        ],
        "citations": 99,
        "references": 62,
        "year": 2023
    },
    {
        "title": "RingMo-SAM: A Foundation Model for Segment Anything in Multimodal Remote-Sensing Images",
        "abstract": "The proposal of the segment anything model (SAM) has created a new paradigm for the deep-learning-based semantic segmentation field and has shown amazing generalization performance. However, we find it may fail or perform poorly on multimodal remote-sensing scenarios, especially synthetic aperture radar (SAR) images. Besides, SAM does not provide category information for objects. In this article, we propose a foundation model for multimodal remote-sensing image segmentation called RingMo-SAM, which can not only segment anything in optical and SAR remote-sensing data, but also identify object categories. First, a large-scale dataset containing millions of segmentation instances is constructed by collecting multiple open-source datasets in this field to train the model. Then, by constructing an instance-type and terrain-type category-decoupling mask decoder (CDMDecoder), the categorywise segmentation of various objects is achieved. In addition, a prompt encoder embedded with the characteristics of multimodal remote-sensing data is designed. It not only supports multibox prompts to improve the segmentation accuracy of multiobjects in complicated remote-sensing scenes, but also supports SAR characteristics prompts to improve the segmentation performance on SAR images. Extensive experimental results on several datasets including iSAID, ISPRS Vaihingen, ISPRS Potsdam, AIR-PolSAR-Seg, and so on have demonstrated the effectiveness of our method.",
        "authors": [
            "Zhiyuan Yan",
            "Junxi Li",
            "Xuexue Li",
            "Ruixue Zhou",
            "Wenkai Zhang",
            "Yingchao Feng",
            "W. Diao",
            "Kun Fu",
            "Xian Sun"
        ],
        "citations": 31,
        "references": 78,
        "year": 2023
    },
    {
        "title": "A Foundation Model for Error Correction Codes",
        "abstract": null,
        "authors": [
            "Yoni Choukroun",
            "Lior Wolf"
        ],
        "citations": 7,
        "references": 0,
        "year": 2024
    },
    {
        "title": "An analytical solution for the horizontal vibration behavior of a cylindrical rigid foundation in poroelastic soil layer",
        "abstract": "The large size embedded foundation is widely used in the engineering, but the finite thickness of soil layer underlying this foundation is usually neglected in design, which leads to the non‐negligible error of calculation. By virtue of Biot's elastodynamic theory, this paper proposes a simple method to discuss the horizontal dynamic response of the cylindrical rigid foundation partially embedded in a poroelastic soil layer. First, based on the Novak plane strain model, the shaft resistance from the surrounding soil is simulated and solved. Second, the foundation end soil is assumed as a continuous medium of finite thickness, whose initial mechanism is derived from the dynamic interaction between the rigid disk and soil. Finally, the horizontal dynamic response factor is calculated by adopting newton's second law. Several cases are set to verify the rationality of the presented solution and to develop the analysis of key parameters. The numerical results suggest that the soil layer thickness has a significant influence on the dynamic vibration of the embedded foundation, and its effect is consistent with that of poroelastic half‐space when the thickness exceeds certain value.",
        "authors": [
            "Zijian Yang",
            "X. Zou"
        ],
        "citations": 30,
        "references": 24,
        "year": 2023
    },
    {
        "title": "General Object Foundation Model for Images and Videos at Scale",
        "abstract": "We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos. Through a unified framework, GLEE accomplishes detection, segmentation, tracking, grounding, and identification of arbitrary objects in the open world scenario for various object perception tasks. Adopting a cohesive learning strategy, GLEE acquires knowledge from diverse data sources with varying supervision levels to formu-late general object representations, excelling in zero-shot transfer to new data and tasks. Specifically, we employ an image encoder, text encoder, and visual prompter to handle multimodal inputs, enabling to simultaneously solve various object-centric downstream tasks while maintaining state-of-the-art performance. Demonstrated through extensive training on over five million images from diverse benchmarks, GLEE exhibits remarkable versatility and improved generalization performance, efficiently tack-ling downstream tasks without the need for task-specific adaptation. By integrating large volumes of automatically labeled data, we further enhance its zero-shot generalization capabilities. Additionally, GLEE is capable of being integrated into Large Language Models, serving as a foun-dational model to provide universal object-level information for multimodal tasks. We hope that the versatility and universality of our method will mark a significant step in the development of efficient visual foundation models for AGI systems. The models and code are released at https://github.com/FoundationVision/GLEE.",
        "authors": [
            "Junfeng Wu",
            "Yi Jiang",
            "Qihao Liu",
            "Zehuan Yuan",
            "Xiang Bai",
            "Song Bai"
        ],
        "citations": 25,
        "references": 137,
        "year": 2023
    },
    {
        "title": "RingMo-Sense: Remote Sensing Foundation Model for Spatiotemporal Prediction via Spatiotemporal Evolution Disentangling",
        "abstract": "Remote sensing (RS) spatiotemporal prediction aims to infer future trends from historical spatiotemporal data, e.g., videos and time-series images, which has a broad application prospect in many fields. The foundation model is a promising research direction for spatiotemporal information mining because of its robust feature extraction capability and has made rapid progress in natural scenes. Nevertheless, due to the spatially multiscale and temporally multiscale properties in RS data, these methods still encounter bottlenecks when applied to RS. Therefore, we propose a remote sensing foundation model for spatiotemporal prediction via spatiotemporal evolution disentangling, abbreviated as RingMo-Sense. Considering spatial affinity, temporal continuity, and spatiotemporal interaction, we construct spatial, temporal, and spatiotemporal triple-branch prediction networks. Specifically, we use parameter-sharing and progressive joint training strategies to achieve stable long-range prediction and parameter reduction simultaneously. In addition, we build an RS spatiotemporal dataset by collecting various RS videos and time-series images. The experimental results on six downstream spatiotemporal tasks demonstrate that the proposed model yields competitive performance.",
        "authors": [
            "Fanglong Yao",
            "Wanxuan Lu",
            "Heming Yang",
            "Liangyu Xu",
            "Chenglong Liu",
            "Leiyi Hu",
            "Hongfeng Yu",
            "Nayu Liu",
            "Chubo Deng",
            "Deke Tang",
            "Changshuo Chen",
            "Jiaqi Yu",
            "Xian Sun",
            "Kun Fu"
        ],
        "citations": 23,
        "references": 100,
        "year": 2023
    },
    {
        "title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook",
        "abstract": "Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. These models are referred to as foundational models. The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. In this survey, we provide a comprehensive review of such emerging foundational models, including typical architecture designs to combine different modalities (vision, text, audio, etc), training objectives (contrastive, generative), pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous. We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding, biases, vulnerability to adversarial attacks, and interpretability issues. We review recent developments in this field, covering a wide range of applications of foundation models systematically and comprehensively. A comprehensive list of foundational models studied in this work is available at \\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.",
        "authors": [
            "Muhammad Awais",
            "Muzammal Naseer",
            "Salman Siddique Khan",
            "R. Anwer",
            "Hisham Cholakkal",
            "M. Shah",
            "Ming Yang",
            "F. Khan"
        ],
        "citations": 92,
        "references": 365,
        "year": 2023
    },
    {
        "title": "Representation Learning with Large Language Models for Recommendation",
        "abstract": "Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, employs LLMs for user/item profiling, and aligns the semantic space of LLMs with collaborative relational signals through cross-view alignment. This work further demonstrates the theoretical foundation of incorporating textual signals through mutual information maximization, which improves the quality of representations. Our evaluation integrates RLMRec with state-of-the-art recommender models, while also analyzing its efficiency and robustness to noise data. Implementation codes are available at https://github.com/HKUDS/RLMRec.",
        "authors": [
            "Xubin Ren",
            "Wei Wei",
            "Lianghao Xia",
            "Lixin Su",
            "Suqi Cheng",
            "Junfeng Wang",
            "Dawei Yin",
            "Chao Huang"
        ],
        "citations": 89,
        "references": 48,
        "year": 2023
    },
    {
        "title": "A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision",
        "abstract": null,
        "authors": [
            "Julio Silva-Rodríguez",
            "H. Chakor",
            "Riadh Kobbi",
            "J. Dolz",
            "Ismail Ben Ayed"
        ],
        "citations": 23,
        "references": 112,
        "year": 2023
    },
    {
        "title": "Leveraging medical Twitter to build a visual–language foundation model for pathology AI",
        "abstract": "The lack of annotated publicly available medical images is a major barrier for innovations. At the same time, many de-identified images and much knowledge are shared by clinicians on public forums such as medical Twitter. Here we harness these crowd platforms to curate OpenPath, a large dataset of 208,414 pathology images paired with natural language descriptions. This is the largest public dataset for pathology images annotated with natural text. We demonstrate the value of this resource by developing PLIP, a multimodal AI with both image and text understanding, which is trained on OpenPath. PLIP achieves state-of-the-art zero-shot and transfer learning performances for classifying new pathology images across diverse tasks. Moreover, PLIP enables users to retrieve similar cases by either image or natural language search, greatly facilitating knowledge sharing. Our approach demonstrates that publicly shared medical information is a tremendous resource that can be harnessed to advance biomedical AI.",
        "authors": [
            "Zhi Huang",
            "Federico Bianchi",
            "Mert Yuksekgonul",
            "T. Montine",
            "J. Zou"
        ],
        "citations": 26,
        "references": 58,
        "year": 2023
    },
    {
        "title": "A foundation model for clinician-centered drug repurposing",
        "abstract": "Of the several thousand diseases that affect humans, only about 500 have treatments approved by the U.S. Food and Drug Administration. Even for those with approved treatments, discovering new drugs can offer alternative options that cause fewer side effects and replace drugs that are ineffective for certain patient groups. However, identifying new therapeutic opportunities for diseases with limited treatment options remains a challenge, as existing algorithms often perform poorly. Here, we leverage recent advances in geometric deep learning and human-centered AI to introduce TxGNN, a model for identifying therapeutic opportunities for diseases with limited treatment options and minimal molecular understanding. TxGNN is a graph neural network pre-trained on a comprehensive knowledge graph of 17,080 clinically-recognized diseases and 7,957 therapeutic candidates. The model can process various therapeutic tasks, such as indication and contraindication prediction, in a unified formulation. Once trained, we show that TxGNN can perform zero-shot inference on new diseases without additional parameters or fine-tuning on ground truth labels. Evaluation of TxGNN shows significant improvements over existing methods, with up to 49.2% higher accuracy in indication tasks and 35.1% higher accuracy in contraindication tasks. TxGNN can also predict therapeutic use for new drugs developed since June 2021. To facilitate interpretation and analysis of the model's predictions by clinicians, we develop a human-AI explorer for TxGNN and evaluate its usability with medical experts. Finally, we demonstrate that TxGNN's novel predictions are consistent with off-label prescription decisions made by clinicians in a large healthcare system. These label-efficient and clinician-centered learning systems pave the way for improvements for many therapeutic tasks.",
        "authors": [
            "K. Huang",
            "P. Chandak",
            "Q. Wang",
            "Shreya Havaldar",
            "A. Vaid",
            "J. Leskovec",
            "G. Nadkarni",
            "B. Glicksberg",
            "Nils Gehlenborg",
            "M. Zitnik"
        ],
        "citations": 26,
        "references": 121,
        "year": 2023
    },
    {
        "title": "The Segment Anything foundation model achieves favorable brain tumor autosegmentation accuracy on MRI to support radiotherapy treatment planning",
        "abstract": "Background: Tumor segmentation in MRI is crucial in radiotherapy (RT) treatment planning for brain tumor patients. Segment anything (SA), a novel promptable foundation model for autosegmentation, has shown high accuracy for multiple segmentation tasks but was not evaluated on medical datasets yet. Methods: SA was evaluated in a point-to-mask task for glioma brain tumor autosegmentation on 16744 transversal slices from 369 MRI datasets (BraTS 2020). Up to 9 point prompts were placed per slice. Tumor core (enhancing tumor + necrotic core) was segmented on contrast-enhanced T1w sequences. Out of the 3 masks predicted by SA, accuracy was evaluated for the mask with the highest calculated IoU (oracle mask) and with highest model predicted IoU (suggested mask). In addition to assessing SA on whole MRI slices, SA was also evaluated on images cropped to the tumor (max. 3D extent + 2 cm). Results: Mean best IoU (mbIoU) using oracle mask on full MRI slices was 0.762 (IQR 0.713-0.917). Best 2D mask was achieved after a mean of 6.6 point prompts (IQR 5-9). Segmentation accuracy was significantly better for high- compared to low-grade glioma cases (mbIoU 0.789 vs. 0.668). Accuracy was worse using MRI slices cropped to the tumor (mbIoU 0.759) and was much worse using suggested mask (full slices 0.572). For all experiments, accuracy was low on peripheral slices with few tumor voxels (mbIoU,<300: 0.537 vs.>=300: 0.841). Stacking best oracle segmentations from full axial MRI slices, mean 3D DSC for tumor core was 0.872, which was improved to 0.919 by combining axial, sagittal and coronal masks. Conclusions: The Segment Anything foundation model, while trained on photos, can achieve high zero-shot accuracy for glioma brain tumor segmentation on MRI slices. The results suggest that Segment Anything can accelerate and facilitate RT treatment planning, when properly integrated in a clinical application.",
        "authors": [
            "F. Putz",
            "J. Grigo",
            "T. Weissmann",
            "P. Schubert",
            "D. Hoefler",
            "A. Gomaa",
            "Hassen Ben Tkhayat",
            "Amr Hagag",
            "S. Lettmaier",
            "B. Frey",
            "U. Gaipl",
            "L. Distel",
            "S. Semrau",
            "C. Bert",
            "R. Fietkau",
            "Yixing Huang"
        ],
        "citations": 18,
        "references": 22,
        "year": 2023
    },
    {
        "title": "GeneCompass: deciphering universal gene regulatory mechanisms with a knowledge-informed cross-species foundation model",
        "abstract": "Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, the traditional research paradigm primarily focuses on individual model organisms, resulting in limited collection and integration of complex features on various cell types across species. Recent breakthroughs in single-cell sequencing and advancements in deep learning techniques present an unprecedented opportunity to tackle this challenge. In this study, we developed GeneCompass, the first knowledge-informed, cross-species foundation model pre-trained on an extensive dataset of over 120 million single-cell transcriptomes from human and mouse. During pre-training, GeneCompass effectively integrates four types of biological prior knowledge to enhance the understanding of gene regulatory mechanisms in a self-supervised manner. Fine-tuning towards multiple downstream tasks, GeneCompass outperforms competing state-of-the-art models in multiple tasks on single species and unlocks new realms of cross-species biological investigation. Overall, GeneCompass marks a milestone in advancing knowledge of universal gene regulatory mechanisms and accelerating the discovery of key cell fate regulators and candidate targets for drug development.",
        "authors": [
            "Xiaodong Yang",
            "Guole Liu",
            "Guihai Feng",
            "Dechao Bu",
            "Pengfei Wang",
            "Jie Jiang",
            "Shubai Chen",
            "Qinmeng Yang",
            "Hefan Miao",
            "Yiyang Zhang",
            "Zhenpeng Man",
            "Zhongming Liang",
            "Zichen Wang",
            "Yaning Li",
            "Zheng Li",
            "Yana Liu",
            "Yao Tian",
            "Wenhao Liu",
            "Cong Li",
            "Ao Li",
            "Jingxi Dong",
            "Zhilong Hu",
            "Chen Fang",
            "Lina Cui",
            "Zixu Deng",
            "Haiping Jiang",
            "Wentao Cui",
            "Jiahao Zhang",
            "Zhaohui Yang",
            "Handong Li",
            "Xingjian He",
            "Liqun Zhong",
            "Jiaheng Zhou",
            "Zijian Wang",
            "Qingqing Long",
            "Ping Xu",
            "Hongmei Wang",
            "Z. Meng",
            "Xuezhi Wang",
            "Yangang Wang",
            "Yong Wang",
            "Shihua Zhang",
            "Jingtao Guo",
            "Yi Zhao",
            "Yuanchun Zhou",
            "Fei Li",
            "Jing Liu",
            "Yiqiang Chen",
            "Ge Yang",
            "Xin Li"
        ],
        "citations": 20,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook",
        "abstract": "Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to equip practitioners with the knowledge to develop applications and further research in this underexplored domain. We primarily categorize the existing literature into two major clusters: large models for time series analysis (LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further classify research based on model scopes (i.e., general vs. domain-specific) and application areas/tasks. We also provide a comprehensive collection of pertinent resources, including datasets, model assets, and useful tools, categorized by mainstream applications. This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.",
        "authors": [
            "Ming Jin",
            "Qingsong Wen",
            "Yuxuan Liang",
            "Chaoli Zhang",
            "Siqiao Xue",
            "Xue Wang",
            "James Y. Zhang",
            "Yi Wang",
            "Haifeng Chen",
            "Xiaoli Li",
            "Shirui Pan",
            "Vincent S. Tseng",
            "Yu Zheng",
            "Lei Chen",
            "Hui Xiong"
        ],
        "citations": 89,
        "references": 342,
        "year": 2023
    },
    {
        "title": "Graph foundation model",
        "abstract": null,
        "authors": [
            "Chuan Shi",
            "Junze Chen",
            "Jiawei Liu",
            "Cheng Yang"
        ],
        "citations": 3,
        "references": 4,
        "year": 2024
    },
    {
        "title": "Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
        "abstract": null,
        "authors": [
            "Cheng Deng",
            "Tianhang Zhang",
            "Zhongmou He",
            "Yi Xu",
            "Qiyuan Chen",
            "Yuanyuan Shi",
            "Le Zhou",
            "Luoyi Fu",
            "Weinan Zhang",
            "Xinbing Wang",
            "Cheng Zhou",
            "Zhouhan Lin",
            "Junxian He"
        ],
        "citations": 23,
        "references": 0,
        "year": 2023
    },
    {
        "title": "OmniVL: One Foundation Model for Image-Language and Video-Language Tasks",
        "abstract": "This paper presents OmniVL, a new foundation model to support both image-language and video-language tasks using one universal architecture. It adopts a unified transformer-based visual encoder for both image and video inputs, and thus can perform joint image-language and video-language pretraining. We demonstrate, for the first time, such a paradigm benefits both image and video tasks, as opposed to the conventional one-directional transfer (e.g., use image-language to help video-language). To this end, we propose a decoupled joint pretraining of image-language and video-language to effectively decompose the vision-language modeling into spatial and temporal dimensions and obtain performance boost on both image and video tasks. Moreover, we introduce a novel unified vision-language contrastive (UniVLC) loss to leverage image-text, video-text, image-label (e.g., image classification), video-label (e.g., video action recognition) data together, so that both supervised and noisily supervised pretraining data are utilized as much as possible. Without incurring extra task-specific adaptors, OmniVL can simultaneously support visual only tasks (e.g., image classification, video action recognition), cross-modal alignment tasks (e.g., image/video-text retrieval), and multi-modal understanding and generation tasks (e.g., image/video question answering, captioning). We evaluate OmniVL on a wide range of downstream tasks and achieve state-of-the-art or competitive results with similar model size and data scale.",
        "authors": [
            "Junke Wang",
            "Dongdong Chen",
            "Zuxuan Wu",
            "Chong Luo",
            "Luowei Zhou",
            "Yucheng Zhao",
            "Yujia Xie",
            "Ce Liu",
            "Yu-Gang Jiang",
            "Lu Yuan"
        ],
        "citations": 136,
        "references": 91,
        "year": 2022
    },
    {
        "title": "Are Emergent Abilities in Large Language Models just In-Context Learning?",
        "abstract": "Large language models, comprising billions of parameters and pre-trained on extensive web-scale corpora, have been claimed to acquire certain capabilities without having been specifically trained on them. These capabilities, referred to as\"emergent abilities,\"have been a driving force in discussions regarding the potentials and risks of language models. A key challenge in evaluating emergent abilities is that they are confounded by model competencies that arise through alternative prompting techniques, including in-context learning, which is the ability of models to complete a task based on a few examples. We present a novel theory that explains emergent abilities, taking into account their potential confounding factors, and rigorously substantiate this theory through over 1000 experiments. Our findings suggest that purported emergent abilities are not truly emergent, but result from a combination of in-context learning, model memory, and linguistic knowledge. Our work is a foundational step in explaining language model performance, providing a template for their efficient use and clarifying the paradox of their ability to excel in some instances while faltering in others. Thus, we demonstrate that their capabilities should not be overestimated.",
        "authors": [
            "Sheng Lu",
            "Irina Bigoulaeva",
            "Rachneet Sachdeva",
            "Harish Tayyar Madabushi",
            "Iryna Gurevych"
        ],
        "citations": 75,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Metamaterial foundation for seismic wave attenuation for low and wide frequency band",
        "abstract": null,
        "authors": [
            "Arpan Gupta",
            "Rishabh Sharma",
            "Aman Thakur",
            "P. Gulia"
        ],
        "citations": 19,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Heave Behavior of Granular Pile Anchor-Foundation System (GPA-Foundation System) in Expansive Soil",
        "abstract": "Granular  Pile  Anchor  (GPA)  is  one  of  the  innovative  foundation  techniques,  devised  for mitigating heave of footing resulting from the expansive soils. This research attempts to study the heave behavior of (GPA-Foundation System) in expansive soil. Laboratory tests have been conducted on an experimental model in addition to a series of numerical modeling and analysis using the finite element package PLAXIS software. The effects of different parameters, such as (GPA) length (L) and diameter (D), footing diameter (B), expansive clay layer thickness (H) and presence of non-expansive clay are studied. The results proved the efficiency of (GPA) in reducing the heave of expansive soil and showed that the heave can be reduced with increasing length and diameter of (GPA). The heave of (GPA-Foundation System) is controlled by three independent variables these are (L/D) ratio, (L/H) ratio and (B/D) ratio. The heave can be reduced by up to (38 %) when (GPA) is embedded in expansive soil layer at (L/H=1) and reduced by about (90 %) when (GPA) is embedded in expansive soil and extended to non- expansive clay (stable zone) at (L/H=2) at the same diameter of (GPA) and footing. An equation (mathematical mode1) was obtained by using the computer package (SPSS 17.0) for statistical analysis based on the results of finite element analysis relating the maximum heave of (GPA-Foundation System) as a function of the above mentioned three independent variables with coefficient of regression of (R2 = 92.3 %).",
        "authors": [
            "Ala Nasir Aljorany",
            "S. Ibrahim",
            "Ahmed Ibrahim Fadhil Al-Adly"
        ],
        "citations": 19,
        "references": 2,
        "year": 2023
    },
    {
        "title": "The Role of Large Language Models in Medical Education: Applications and Implications",
        "abstract": "Large language models (LLMs) such as ChatGPT have sparked extensive discourse within the medical education community, spurring both excitement and apprehension. Written from the perspective of medical students, this editorial offers insights gleaned through immersive interactions with ChatGPT, contextualized by ongoing research into the imminent role of LLMs in health care. Three distinct positive use cases for ChatGPT were identified: facilitating differential diagnosis brainstorming, providing interactive practice cases, and aiding in multiple-choice question review. These use cases can effectively help students learn foundational medical knowledge during the preclinical curriculum while reinforcing the learning of core Entrustable Professional Activities. Simultaneously, we highlight key limitations of LLMs in medical education, including their insufficient ability to teach the integration of contextual and external information, comprehend sensory and nonverbal cues, cultivate rapport and interpersonal interaction, and align with overarching medical education and patient care goals. Through interacting with LLMs to augment learning during medical school, students can gain an understanding of their strengths and weaknesses. This understanding will be pivotal as we navigate a health care landscape increasingly intertwined with LLMs and artificial intelligence.",
        "authors": [
            "Conrad W Safranek",
            "A. Sidamon-Eristoff",
            "Aidan Gilson",
            "David Chartash"
        ],
        "citations": 81,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study",
        "abstract": "Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT in both fine-tuning and zero-shot evaluation settings. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our code and model at: https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md",
        "authors": [
            "Boxin Wang",
            "Wei Ping",
            "P. Xu",
            "Lawrence C. McAfee",
            "Zihan Liu",
            "Mohammad Shoeybi",
            "Yi Dong",
            "Oleksii Kuchaiev",
            "Bo Li",
            "Chaowei Xiao",
            "Anima Anandkumar",
            "Bryan Catanzaro"
        ],
        "citations": 52,
        "references": 70,
        "year": 2023
    },
    {
        "title": "LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation",
        "abstract": "Layout-to-image generation refers to the task of synthesizing photo-realistic images based on semantic layouts. In this paper, we propose LayoutDiffuse that adapts a foundational diffusion model pretrained on large-scale image or text-image datasets for layout-to-image generation. By adopting a novel neural adaptor based on layout attention and task-aware prompts, our method trains efficiently, generates images with both high perceptual quality and layout alignment, and needs less data. Experiments on three datasets show that our method significantly outperforms other 10 generative models based on GANs, VQ-VAE, and diffusion models.",
        "authors": [
            "Jiaxin Cheng",
            "Xiao Liang",
            "Xingjian Shi",
            "Tong He",
            "Tianjun Xiao",
            "Mu Li"
        ],
        "citations": 54,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Multiple Physics Pretraining for Physical Surrogate Models",
        "abstract": "We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling of spatiotemporal systems with transformers. In MPP, rather than training one model on a specific physical system, we train a backbone model to predict the dynamics of multiple heterogeneous physical systems simultaneously in order to learn features that are broadly useful across systems and facilitate transfer. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on systems with previously unseen physical components or higher dimensional systems compared to training from scratch or finetuning pretrained video foundation models. We open-source our code and model weights trained at multiple scales for reproducibility.",
        "authors": [
            "Michael McCabe",
            "Bruno Régaldo-Saint Blancard",
            "Liam Parker",
            "Ruben Ohana",
            "M. Cranmer",
            "Alberto Bietti",
            "Michael Eickenberg",
            "Siavash Golkar",
            "G. Krawezik",
            "François Lanusse",
            "Mariel Pettee",
            "Tiberiu Teşileanu",
            "Kyunghyun Cho",
            "Shirley Ho"
        ],
        "citations": 42,
        "references": 100,
        "year": 2023
    },
    {
        "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
        "abstract": "In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",
        "authors": [
            "Neng Wang",
            "Hongyang Yang",
            "Chris Wang"
        ],
        "citations": 39,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model",
        "abstract": "Driven by the progress of large-scale pre-training, parameter-efficient transfer learning has gained immense popularity across different subfields of Artificial Intelligence. The core is to adapt the model to downstream tasks with only a small set of parameters. Recently, researchers have leveraged such proven techniques in multimodal tasks and achieve promising results. However, two critical issues remain unresolved: how to further reduce the complexity with lightweight design and how to boost alignment between modalities under extremely low parameters. In this paper, we propose A graceful prompt framework for cross-modal transfer (Aurora) to overcome these challenges. Considering the redundancy in existing architectures, we first utilize the mode approximation to generate 0.1M trainable parameters to implement the multimodal prompt tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pre-trained model. Then, for better modality alignment, we propose the Informative Context Enhancement and Gated Query Transformation module under extremely few parameters scenes. A thorough evaluation on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even outperforms the full fine-tuning approach. Our code is available at: https://github.com/WillDreamer/Aurora.",
        "authors": [
            "Haixin Wang",
            "Xinlong Yang",
            "Jianlong Chang",
            "Di Jin",
            "Jinan Sun",
            "Shikun Zhang",
            "Xiao Luo",
            "Qi Tian"
        ],
        "citations": 18,
        "references": 78,
        "year": 2023
    },
    {
        "title": "On the Foundation of Distributionally Robust Reinforcement Learning",
        "abstract": "Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the existence or absence of the dynamic programming principle (DPP). From an algorithmic standpoint, the existence of DPP holds significant implications, as the vast majority of existing data and computationally efficiency RL algorithms are reliant on the DPP. To study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. We also offer counterexamples for settings in which a DPP with full generality is absent.",
        "authors": [
            "Shengbo Wang",
            "Nian Si",
            "Jose H. Blanchet",
            "Zhengyuan Zhou"
        ],
        "citations": 14,
        "references": 68,
        "year": 2023
    },
    {
        "title": "VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for Generalist Ophthalmic Artificial Intelligence",
        "abstract": "We present VisionFM, a foundation model pre-trained with 3.4 million ophthalmic images from 560,457 individuals, covering a broad range of ophthalmic diseases, modalities, imaging devices, and demography. After pre-training, VisionFM provides a foundation to foster multiple ophthalmic artificial intelligence (AI) applications, such as disease screening and diagnosis, disease prognosis, subclassification of disease phenotype, and systemic biomarker and disease prediction, with each application enhanced with expert-level intelligence and accuracy. The generalist intelligence of VisionFM outperformed ophthalmologists with basic and intermediate levels in jointly diagnosing 12 common ophthalmic diseases. Evaluated on a new large-scale ophthalmic disease diagnosis benchmark database, as well as a new large-scale segmentation and detection benchmark database, VisionFM outperformed strong baseline deep neural networks. The ophthalmic image representations learned by VisionFM exhibited noteworthy explainability, and demonstrated strong generalizability to new ophthalmic modalities, disease spectrum, and imaging devices. As a foundation model, VisionFM has a large capacity to learn from diverse ophthalmic imaging data and disparate datasets. To be commensurate with this capacity, in addition to the real data used for pre-training, we also generated and leveraged synthetic ophthalmic imaging data. Experimental results revealed that synthetic data that passed visual Turing tests, can also enhance the representation learning capability of VisionFM, leading to substantial performance gains on downstream ophthalmic AI tasks. Beyond the ophthalmic AI applications developed, validated, and demonstrated in this work, substantial further applications can be achieved in an efficient and cost-effective manner using VisionFM as the foundation.",
        "authors": [
            "Jianing Qiu",
            "Jian Wu",
            "Hao Wei",
            "Peilun Shi",
            "Minqing Zhang",
            "Yunyun Sun",
            "Lin Li",
            "Hanruo Liu",
            "Hongyi Liu",
            "Simeng Hou",
            "Yuyang Zhao",
            "Xuehui Shi",
            "Junfang Xian",
            "Xiaoxia Qu",
            "Sirui Zhu",
            "Lijie Pan",
            "Xiaoniao Chen",
            "Xiaojia Zhang",
            "Shuai Jiang",
            "Kebing Wang",
            "Chenlong Yang",
            "Mingqiang Chen",
            "Sujie Fan",
            "Jianhua Hu",
            "Aiguo Lv",
            "Hui Miao",
            "Li Guo",
            "Shujun Zhang",
            "Cheng Pei",
            "Xiaojuan Fan",
            "Jia-Li Lei",
            "Ting Wei",
            "Junguo Duan",
            "Chun Liu",
            "Xiaobo Xia",
            "Siqi Xiong",
            "Junhong Li",
            "Benny Lo",
            "Y. Tham",
            "T. Wong",
            "Ningli Wang",
            "Wu Yuan"
        ],
        "citations": 14,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Accelerating drug target inhibitor discovery with a deep generative foundation model",
        "abstract": "Inhibitor discovery for emerging drug-target proteins is challenging, especially when target structure or active molecules are unknown. Here, we experimentally validate the broad utility of a deep generative framework trained at-scale on protein sequences, small molecules, and their mutual interactions—unbiased toward any specific target. We performed a protein sequence-conditioned sampling on the generative foundation model to design small-molecule inhibitors for two dissimilar targets: the spike protein receptor-binding domain (RBD) and the main protease from SARS-CoV-2. Despite using only the target sequence information during the model inference, micromolar-level inhibition was observed in vitro for two candidates out of four synthesized for each target. The most potent spike RBD inhibitor exhibited activity against several variants in live virus neutralization assays. These results establish that a single, broadly deployable generative foundation model for accelerated inhibitor discovery is effective and efficient, even in the absence of target structure or binder information.",
        "authors": [
            "V. Chenthamarakshan",
            "Samuel C. Hoffman",
            "C. Owen",
            "P. Lukacik",
            "C. Strain-Damerell",
            "D. Fearon",
            "T. R. Malla",
            "A. Tumber",
            "C. Schofield",
            "Helen M. E. Duyvesteyn",
            "W. Dejnirattisai",
            "L. Carrique",
            "T. Walter",
            "G. Screaton",
            "T. Matviiuk",
            "A. Mojsilovic",
            "Jason Crain",
            "Martin A. Walsh",
            "David I. Stuart",
            "Payel Das"
        ],
        "citations": 13,
        "references": 102,
        "year": 2023
    },
    {
        "title": "Semantic anomaly detection with large language models",
        "abstract": null,
        "authors": [
            "Amine Elhafsi",
            "Rohan Sinha",
            "Christopher Agia",
            "E. Schmerling",
            "I. Nesnas",
            "M. Pavone"
        ],
        "citations": 51,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Brant: Foundation Model for Intracranial Neural Signal",
        "abstract": "We propose a foundation model named Brant for modeling intracranial recordings, which learns powerful representations of intracranial neural signals by pre-training, providing a large-scale, off-the-shelf model for medicine. Brant is the largest model in the field of brain signals and is pre-trained on a large corpus of intracranial data collected by us. The design of Brant is to capture long-term temporal dependency and spatial correlation from neural signals, combining the information in both time and frequency domains. As a foundation model, Brant achieves SOTA performance on various downstream tasks (i.e. neural signal forecasting, frequency-phase forecasting, imputation and seizure detection), showing the generalization ability to a broad range of tasks. The low-resource label analysis and representation visualization further illustrate the effectiveness of our pre-training strategy. In addition, we explore the effect of model size to show that a larger model with a higher capacity can lead to performance improvements on our dataset. The source code and pre-trained weights are available at: https://zju-brainnet.github. io/Brant.github.io/ .",
        "authors": [
            "Daoze Zhang",
            "Zhizhang Yuan",
            "Yang Yang",
            "Junru Chen",
            "Jingjing Wang",
            "Yafeng Li"
        ],
        "citations": 12,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding",
        "abstract": "Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks. Additionally, our in-depth error analysis on the challenging relation extraction task offers valuable insights into error distribution and potential avenues for improvement using SQP. Our study sheds light on the practical implications of employing LLMs in the specialized domain of healthcare, serving as a foundation for future research and the development of potential applications in healthcare settings.",
        "authors": [
            "Yuqing Wang",
            "Yun Zhao",
            "Linda Petzold"
        ],
        "citations": 42,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Test of Time: Instilling Video-Language Models with a Sense of Time",
        "abstract": "Modelling and understanding time remains a challenge in contemporary video understanding models. With language emerging as a key driver towards powerful generalization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We establish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adaptation recipe on top of one such model, VideoCLIp, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness. We observe encouraging performance gains especially when the task needs higher time awareness. Our work serves as a first step towards probing and instilling a sense of time in existing video-language models without the need for data and compute-intense training from scratch.",
        "authors": [
            "Piyush Bagad",
            "Makarand Tapaswi",
            "Cees G. M. Snoek"
        ],
        "citations": 30,
        "references": 137,
        "year": 2023
    },
    {
        "title": "TransWorldNG: Traffic Simulation via Foundation Model",
        "abstract": "Traffic simulation is a crucial tool for transportation decision-making and policy development. However, achieving realistic simulations in the face of the high dimensionality and heterogeneity of traffic environments is a longstanding challenge. In this paper, we present TransWordNG, a traffic simulator that uses Data-driven algorithms and Graph Computing techniques to learn traffic dynamics from real data. The functionality and structure of TransWorldNG are introduced, which utilize a foundation model for transportation management and control. The results demonstrate that TransWorldNG can generate more realistic traffic patterns compared to traditional simulators. Additionally, TransWorldNG exhibits better scalability, as it shows linear growth in computation time as the scenario scale increases. To the best of our knowledge, this is the first traffic simulator that can automatically learn traffic patterns from real-world data and efficiently generate accurate and realistic traffic environments.",
        "authors": [
            "Dingsu Wang",
            "Xuhong Wang",
            "Liang Chen",
            "Shengyue Yao",
            "Mi Jing",
            "Honghai Li",
            "Li Li",
            "Shiqiang Bao",
            "Feiyue Wang",
            "Yilun Lin"
        ],
        "citations": 11,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation",
        "abstract": "With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledge, Open-TransMind is the first open-source transportation foundation model with multitask and multi-modal capabilities. Simultaneously, Open-TransMind can achieve state-of-the-art performance on detection, classification, and segmentation datasets of traffic scenarios. Our source code is available at https://github.com/Traffic-X/Open-TransMind.",
        "authors": [
            "Yifeng Shi",
            "Feng Lv",
            "Xinliang Wang",
            "Chunlong Xia",
            "Shaojie Li",
            "Shu-Zhen Yang",
            "Teng Xi",
            "Gang Zhang"
        ],
        "citations": 11,
        "references": 24,
        "year": 2023
    },
    {
        "title": "Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions",
        "abstract": "Non-coding RNA structure and function are essential to understanding various biological processes, such as cell signaling, gene expression, and post-transcriptional regulations. These are all among the core problems in the RNA field. With the rapid growth of sequencing technology, we have accumulated a massive amount of unannotated RNA sequences. On the other hand, expensive experimental observatory results in only limited numbers of annotated data and 3D structures. Hence, it is still challenging to design computational methods for predicting their structures and functions. The lack of annotated data and systematic study causes inferior performance. To resolve the issue, we propose a novel RNA foundation model (RNA-FM) to take advantage of all the 23 million non-coding RNA sequences through self-supervised learning. Within this approach, we discover that the pre-trained RNA-FM could infer sequential and evolutionary information of non-coding RNAs without using any labels. Furthermore, we demonstrate RNA-FM’s effectiveness by applying it to the downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and evolution prediction, protein-RNA binding preference modeling, and gene expression regulation modeling. The comprehensive experiments show that the proposed method improves the RNA structural and functional modelling results significantly and consistently. Despite only being trained with unlabelled data, RNA-FM can serve as the foundational model for the field.",
        "authors": [
            "Jiayang Chen",
            "Zhihang Hu",
            "Siqi Sun",
            "Qingxiong Tan",
            "Yixuan Wang",
            "Qinze Yu",
            "Licheng Zong",
            "Liang Hong",
            "Jin Xiao",
            "Irwin King",
            "Yu Li"
        ],
        "citations": 90,
        "references": 70,
        "year": 2022
    },
    {
        "title": "FP8-LM: Training FP8 Large Language Models",
        "abstract": "In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 39% reduction in real memory usage but also ran 75% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 37%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.",
        "authors": [
            "Houwen Peng",
            "Kan Wu",
            "Yixuan Wei",
            "Guoshuai Zhao",
            "Yuxiang Yang",
            "Ze Liu",
            "Yifan Xiong",
            "Ziyue Yang",
            "Bolin Ni",
            "Jingcheng Hu",
            "Ruihang Li",
            "Miaosen Zhang",
            "Chen Li",
            "Jia Ning",
            "Ruizhe Wang",
            "Zheng Zhang",
            "Shuguang Liu",
            "Joe Chau",
            "Han Hu",
            "Peng Cheng"
        ],
        "citations": 27,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Towards a Foundation Model of the Mouse Visual Cortex",
        "abstract": ",",
        "authors": [
            "Eric Y. Wang",
            "Paul G. Fahey",
            "Kayla Ponder",
            "Zhuokun Ding",
            "Andersen Chang",
            "Taliah Muhammad",
            "Saumil S. Patel",
            "Zhiwei Ding",
            "Dat Tran",
            "Jiakun Fu",
            "S. Papadopoulos",
            "K. Franke",
            "Alexander S. Ecker",
            "Jacob Reimer",
            "X. Pitkow",
            "Fabian H Sinz",
            "A. Tolias"
        ],
        "citations": 18,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Sparks of Large Audio Models: A Survey and Outlook",
        "abstract": "This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \\textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \\textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \\textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.",
        "authors": [
            "S. Latif",
            "Moazzam Shoukat",
            "Fahad Shamshad",
            "Muhammad Usama",
            "Heriberto Cuay'ahuitl",
            "Björn Schuller"
        ],
        "citations": 24,
        "references": 339,
        "year": 2023
    },
    {
        "title": "Using Large Language Models for Hyperparameter Optimization",
        "abstract": "This paper explores the use of foundational large language models (LLMs) in hyperparameter optimization (HPO). Hyperparameters are critical in determining the effectiveness of machine learning models, yet their optimization often relies on manual approaches in limited-budget settings. By prompting LLMs with dataset and model descriptions, we develop a methodology where LLMs suggest hyperparameter configurations, which are iteratively refined based on model performance. Our empirical evaluations on standard benchmarks reveal that within constrained search budgets, LLMs can match or outperform traditional HPO methods like Bayesian optimization across different models on standard benchmarks. Furthermore, we propose to treat the code specifying our model as a hyperparameter, which the LLM outputs and affords greater flexibility than existing HPO approaches.",
        "authors": [
            "Michael R. Zhang",
            "Nishkrit Desai",
            "Juhan Bae",
            "Jonathan Lorraine",
            "Jimmy Ba"
        ],
        "citations": 31,
        "references": 83,
        "year": 2023
    },
    {
        "title": "Automatic Evaluation of Attribution by Large Language Models",
        "abstract": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.",
        "authors": [
            "Xiang Yue",
            "Boshi Wang",
            "Kai Zhang",
            "Ziru Chen",
            "Yu Su",
            "Huan Sun"
        ],
        "citations": 47,
        "references": 92,
        "year": 2023
    },
    {
        "title": "CCEdit: Creative and Controllable Video Editing via Diffusion Models",
        "abstract": "In this paper, we present CCEdit, a versatile generative video editing framework based on diffusion models. Our approach employs a novel trident network structure that separates structure and appearance control, ensuring precise and creative editing capabilities. Utilizing the foundational ControlNet architecture, we maintain the structural integrity of the video during editing. The incorporation of an additional appearance branch enables users to exert fine-grained control over the edited key frame. These two side branches seamlessly integrate into the main branch, which is constructed upon existing text-to-image (T2I) generation models, through learnable temporal layers. The versatility of our framework is demonstrated through a diverse range of choices in both structure representations and personalized T2I models, as well as the option to provide the edited key frame. To facilitate comprehensive evaluation, we introduce the BalanceCC benchmark dataset, comprising 100 videos and 4 target prompts for each video. Our extensive user studies compare CCEdit with eight state-of-the-art video editing methods. The outcomes demonstrate CCEdit's substantial superiority over all other methods.",
        "authors": [
            "Ruoyu Feng",
            "Wenming Weng",
            "Yanhui Wang",
            "Yuhui Yuan",
            "Jianmin Bao",
            "Chong Luo",
            "Zhibo Chen",
            "Baining Guo"
        ],
        "citations": 30,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Effects of Jet-Grouting Piles on Loess Tunnel Foundation with Centrifugal Model Tests",
        "abstract": null,
        "authors": [
            "H. Fan",
            "Tong Liu",
            "S. Zhang",
            "Haijun He",
            "Zhen Zhu",
            "Yongquan Zhu",
            "Xinqiang Gao"
        ],
        "citations": 14,
        "references": 31,
        "year": 2023
    },
    {
        "title": "netFound: Foundation Model for Network Security",
        "abstract": "Developing generalizable ML-based solutions for disparate learning problems in network security is highly desired. However, despite a rich history of applying ML to network security, most existing solutions lack generalizability. This lack of progress can be attributed to an overreliance on supervised learning techniques and the associated challenges of curating well-specified labeled training data. This paper addresses a fundamental gap by introducing a novel transformer-based network foundation model, netFound. We employ self-supervised learning techniques on abundant, unlabeled network telemetry data for pre-training. This pretrained model can subsequently be fine-tuned to create generalizable learning artifacts for disparate learning tasks, even when using commonly available but challenging labeled datasets that are sparse, noisy, and skewed. To realize this goal, netFound leverages various domain-specific attributes and constraints unique to network data (packet traces) by developing multi-modal embeddings, protocol-aware tokenization, data-driven token composition, and hierarchical transformers. Our results demonstrate that netFound's domain-specific design choices ensure that it (1) effectively captures the hidden networking context in production settings, (2) outperforms four different SOTA methods on five different learning tasks, and (3) is robust to both noisy labels and learning shortcuts -- critical for developing generalizable ML models in practical settings.",
        "authors": [
            "Satyandra Guthula",
            "Navya Battula",
            "Roman Beltiukov",
            "Wenbo Guo",
            "Arpit Gupta"
        ],
        "citations": 6,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Neuro-GPT: Towards A Foundation Model For EEG",
        "abstract": "To handle the scarcity and heterogeneity of electroencephalography (EEG) data for Brain-Computer Interface (BCI) tasks, and to harness the power of large publicly available data sets, we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model. The foundation model is pre-trained on a large-scale data set using a self-supervised task that learns how to reconstruct masked EEG segments. We then fine-tune the model on a motor imagery classification task to validate its performance in a low-data regime (9 subjects). Our experiments demonstrate that applying a foundation model can significantly improve classification performance compared to a model trained from scratch, which provides evidence for the generalizability of the foundation model and its ability to address challenges of data scarcity and heterogeneity in EEG. The code is publicly available at https://github.com/wenhui0206/NeuroGPT.",
        "authors": [
            "Wenhui Cui",
            "Woojae Jeong",
            "Philipp Tholke",
            "T. Medani",
            "Karim Jerbi",
            "Anand A. Joshi",
            "R. Leahy"
        ],
        "citations": 6,
        "references": 25,
        "year": 2023
    },
    {
        "title": "OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer Thin Film Structures",
        "abstract": "Optical multilayer thin film structures have been widely used in numerous photonic applications. However, existing inverse design methods have many drawbacks because they either fail to quickly adapt to different design targets, or are difficult to suit for different types of structures, e.g., designing for different materials at each layer. These methods also cannot accommodate versatile design situations under different angles and polarizations. In addition, how to benefit practical fabrications and manufacturing has not been extensively considered yet. In this work, we introduce OptoGPT (Opto Generative Pretrained Transformer), a decoder-only transformer, to solve all these drawbacks and issues simultaneously.",
        "authors": [
            "Taigao Ma",
            "Haozhu Wang",
            "L. J. Guo"
        ],
        "citations": 11,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Three Pillars Improving Vision Foundation Model Distillation for Lidar",
        "abstract": "Self-supervised image backbones can be used to address complex 2D tasks (e.g., semantic segmentation, object discovery) very efficiently and with little or no downstream supervision. Ideally, 3D backbones for lidar should be able to inherit these properties after distillation of these powerful 2D features. The most recent methods for image-to-lidar distillation on autonomous driving data show promising results, obtained thanks to distillation methods that keep improving. Yet, we still notice a large performance gap when measuring by linear probing the quality of distilled vs fully supervised features. In this work, instead of focusing only on the distillation method, we study the effect of three pillars for distillation: the 3D backbone, the pretrained 2D backbone, and the pretraining 2D+3D dataset. In particular, thanks to our scalable distillation method named ScaLR, we show that scaling the 2D and 3D backbones and pretraining on diverse datasets leads to a substantial improvement of the feature quality. This allows us to significantly reduce the gap between the quality of distilled and fully-supervised 3D features, and to improve the robustness of the pretrained backbones to domain gaps and perturbations. The code is available at https://github.com/valeoai/ScaLR.",
        "authors": [
            "Gilles Puy",
            "Spyros Gidaris",
            "Alexandre Boulch",
            "Oriane Sim'eoni",
            "Corentin Sautier",
            "Patrick P'erez",
            "Andrei Bursuc",
            "Renaud Marlet"
        ],
        "citations": 11,
        "references": 85,
        "year": 2023
    },
    {
        "title": "Visually sensitive seizures: An updated review by the Epilepsy Foundation",
        "abstract": "Light flashes, patterns, or color changes can provoke seizures in up to 1 in 4000 persons. Prevalence may be higher because of selection bias. The Epilepsy Foundation reviewed light‐induced seizures in 2005. Since then, images on social media, virtual reality, three‐dimensional (3D) movies, and the Internet have proliferated. Hundreds of studies have explored the mechanisms and presentations of photosensitive seizures, justifying an updated review. This literature summary derives from a nonsystematic literature review via PubMed using the terms “photosensitive” and “epilepsy.” The photoparoxysmal response (PPR) is an electroencephalography (EEG) phenomenon, and photosensitive seizures (PS) are seizures provoked by visual stimulation. Photosensitivity is more common in the young and in specific forms of generalized epilepsy. PS can coexist with spontaneous seizures. PS are hereditable and linked to recently identified genes. Brain imaging usually is normal, but special studies imaging white matter tracts demonstrate abnormal connectivity. Occipital cortex and connected regions are hyperexcitable in subjects with light‐provoked seizures. Mechanisms remain unclear. Video games, social media clips, occasional movies, and natural stimuli can provoke PS. Virtual reality and 3D images so far appear benign unless they contain specific provocative content, for example, flashes. Images with flashes brighter than 20 candelas/m2 at 3‐60 (particularly 15‐20) Hz occupying at least 10 to 25% of the visual field are a risk, as are red color flashes or oscillating stripes. Equipment to assay for these characteristics is probably underutilized. Prevention of seizures includes avoiding provocative stimuli, covering one eye, wearing dark glasses, sitting at least two meters from screens, reducing contrast, and taking certain antiseizure drugs. Measurement of PPR suppression in a photosensitivity model can screen putative antiseizure drugs. Some countries regulate media to reduce risk. Visually‐induced seizures remain significant public health hazards so they warrant ongoing scientific and regulatory efforts and public education.",
        "authors": [
            "R. Fisher",
            "J. Acharya",
            "Fiona M. Baumer",
            "J. French",
            "P. Parisi",
            "Jessica Solodar",
            "J. Szaflarski",
            "L. Thio",
            "B. Tolchin",
            "A. Wilkins",
            "D. Kasteleijn-Nolst Trenité"
        ],
        "citations": 46,
        "references": 291,
        "year": 2022
    },
    {
        "title": "Sustainability in the Circular Economy: Insights and Dynamics of Designing Circular Business Models",
        "abstract": "The integration of sustainability in the circular economy is an emerging paradigm that can offer a long term vision to achieve environmental and social sustainability targets in line with the United Nation’s Sustainable Development Goals. Developing scalable and sustainable impacts in circular economy business models (CEBMs) has many challenges. While many advanced technology manufacturing firms start as small enterprises, remarkably little is known about how material reuse firms in sociotechnical systems transition towards circular business models. Research into CEBMs integrating sustainability and environmental conservation is still in its early stages. There has been increased interest in sustainability and circular economy research, but current research is fragmented. The innovation surrounding CEBMs eludes some firms with relatively limited evidence of the transitional perspective necessary to integrate aspects of sustainability. This lack of evidence is especially applicable to the context of circular economy practices in small and medium enterprises in the United States regarding capabilities, operations obstacles, and elements of success in designing circular business models. Based on a qualitative, interview-based inductive study of a material reuse firm, our research develops a conceptual model of the critical success factors and obstacles that are part of implementing circular economy practices. Firms must first manage strategic enablers and monitor tactical enablers to achieve sustainability goals. In this study, we identify the underlying enablers of how these capabilities affect the transition to a CEBM that integrates sustainability. The framework emerging from our findings highlights the interplay of CEBM, innovation success factors, and obstacles at a micro-level. The investigation of a material reuse firm serves as the foundation for developing a framework for how managers can alter a company and revise the business model to transition towards a more innovative circular economy.",
        "authors": [
            "Usama Awan",
            "Robert Sroufe"
        ],
        "citations": 182,
        "references": 139,
        "year": 2022
    },
    {
        "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models",
        "abstract": "We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to human intelligence. However, even the most advanced LLMs currently struggle with this form of reasoning. We examine this problem within the framework of in-context learning and find that demonstrating both foundational skills and compositional examples grounded in these skills within the same prompt context is crucial. We refer to this prompt structure as skills-in-context (SKiC). With as few as two exemplars, this in-context learning structure enables LLMs to tackle more challenging problems requiring innovative skill combinations, achieving near-perfect systematic generalization across a broad range of tasks. Intriguingly, SKiC also unlocks the latent potential of LLMs, allowing them to more actively utilize pre-existing internal skills acquired during earlier pretraining stages to solve complex reasoning problems. The SKiC structure is robust across different skill constructions and exemplar choices and demonstrates strong transferability to new tasks. Finally, inspired by our in-context learning study, we show that fine-tuning LLMs with SKiC-style data can elicit zero-shot weak-to-strong generalization, enabling the models to solve much harder problems directly with standard prompting.",
        "authors": [
            "Jiaao Chen",
            "Xiaoman Pan",
            "Dian Yu",
            "Kaiqiang Song",
            "Xiaoyang Wang",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "citations": 22,
        "references": 91,
        "year": 2023
    },
    {
        "title": "Evaluation of the contact problem of functionally graded layer resting on rigid foundation pressed via rigid punch by analytical and numerical (FEM and MLP) methods",
        "abstract": null,
        "authors": [
            "Murat Yaylacı",
            "Merve Abanoz",
            "E. U. Yaylacı",
            "H. Ölmez",
            "D. M. Sekban",
            "A. Birinci"
        ],
        "citations": 40,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Cascaded Latent Diffusion Models for High-Resolution Chest X-ray Synthesis",
        "abstract": "While recent advances in large-scale foundational models show promising results, their application to the medical domain has not yet been explored in detail. In this paper, we progress into the realms of large-scale modeling in medical synthesis by proposing Cheff - a foundational cascaded latent diffusion model, which generates highly-realistic chest radiographs providing state-of-the-art quality on a 1-megapixel scale. We further propose MaCheX, which is a unified interface for public chest datasets and forms the largest open collection of chest X-rays up to date. With Cheff conditioned on radiological reports, we further guide the synthesis process over text prompts and unveil the research area of report-to-chest-X-ray generation.",
        "authors": [
            "Tobias Weber",
            "M. Ingrisch",
            "Bernd Bischl",
            "David Rügamer"
        ],
        "citations": 18,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models",
        "abstract": "Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models. The project page is https://xufangzhi.github.io/symbol-llm-page/.",
        "authors": [
            "Fangzhi Xu",
            "Zhiyong Wu",
            "Qiushi Sun",
            "Siyu Ren",
            "Fei Yuan",
            "Shuai Yuan",
            "Qika Lin",
            "Yu Qiao",
            "Jun Liu"
        ],
        "citations": 21,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Inferring Neural Activity Before Plasticity: A Foundation for Learning Beyond Backpropagation",
        "abstract": "For both humans and machines, the essence of learning is to pinpoint which components in its information processing pipeline are responsible for an error in its output — a challenge that is known as credit assignment1. How the brain solves credit assignment is a key question in neuroscience, and also of significant importance for artificial intelligence. Many recent studies1–12 presuppose that it is solved by backpropagation13–16, which is also the foundation of modern machine learning17–22. However, it has been questioned whether it is possible for the brain to implement backpropagation23, 24, and learning in the brain may actually be more efficient and effective than backpropagation25. Here, we set out a fundamentally different principle on credit assignment, called prospective configuration. In prospective configuration, the network first infers the pattern of neural activity that should result from learning, and then the synaptic weights are modified to consolidate the change in neural activity. We demonstrate that this distinct mechanism, in contrast to backpropagation, (1) underlies learning in a well-established family of models of cortical circuits, (2) enables learning that is more efficient and effective in many contexts faced by biological organisms, and (3) reproduces surprising patterns of neural activity and behaviour observed in diverse human and animal learning experiments. Our findings establish a new foundation for learning beyond backpropagation, for both understanding biological learning and building artificial intelligence.",
        "authors": [
            "Yuhang Song",
            "Beren Millidge",
            "Tommaso Salvatori",
            "Thomas Lukasiewicz",
            "Zhenghua Xu",
            "R. Bogacz"
        ],
        "citations": 35,
        "references": 132,
        "year": 2022
    },
    {
        "title": "Permeability Prediction Model Modified on Kozeny-Carman for Building Foundation of Clay Soil",
        "abstract": "Clay soil is a common building foundation material, and its permeability is very important for the safety of foundation pits and the later settlement of buildings. However, the traditional Kozeny-Carman (K-C) equation shows serious discrepancies when predicting the permeability of clay in building foundation treatment. Therefore, solving the application of K-C equation in clay is a problem faced by the engineers and scholars. In this paper, the influence of clay mineralogy on pore structure and permeability is analyzed, and then the effective e (eeff) and effective SSA (Seff) are proposed. Based on the eeff and Seff, the permeability prediction model modified on Kozeny-Carman is built. Then, seepage experiments are conducted on two types of clay samples to test this prediction model; at the same time, the MIP combining freeze-drying methods are used to obtain the Seff and eeff. Through the discussion of the test results, three main conclusions are obtained: (1) there are invalid pores in clay due to the influence of clay mineral, this is the reason for which K-C equation is unsuitable for clay; (2) the eeff and Seff can reflect the structural state of clay during seepage; (3) the results of the permeability prediction model in this paper agree well with the test results, which indicates that this prediction model is applicable to clay. The research results of this paper are significant to solve the academic problem that K-C equation is not applicable to clay and significant to ensure the safety of building foundation pits in clay areas.",
        "authors": [
            "Jian Chen",
            "Huawei Tong",
            "Jie Yuan",
            "Y. Fang",
            "R. Gu"
        ],
        "citations": 32,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Towards a Foundation Model for Neural Network Wavefunctions",
        "abstract": "Deep neural networks have become a highly accurate and powerful wavefunction ansatz in combination with variational Monte Carlo methods for solving the electronic Schr\\\"odinger equation. However, despite their success and favorable scaling, these methods are still computationally too costly for wide adoption. A significant obstacle is the requirement to optimize the wavefunction from scratch for each new system, thus requiring long optimization. In this work, we propose a novel neural network ansatz, which effectively maps uncorrelated, computationally cheap Hartree-Fock orbitals, to correlated, high-accuracy neural network orbitals. This ansatz is inherently capable of learning a single wavefunction across multiple compounds and geometries, as we demonstrate by successfully transferring a wavefunction model pre-trained on smaller fragments to larger compounds. Furthermore, we provide ample experimental evidence to support the idea that extensive pre-training of a such a generalized wavefunction model across different compounds and geometries could lead to a foundation wavefunction model. Such a model could yield high-accuracy ab-initio energies using only minimal computational effort for fine-tuning and evaluation of observables.",
        "authors": [
            "Michael Scherbela",
            "Leon Gerard",
            "P. Grohs"
        ],
        "citations": 8,
        "references": 38,
        "year": 2023
    },
    {
        "title": "OTFS—A Mathematical Foundation for Communication and Radar Sensing in the Delay-Doppler Domain",
        "abstract": "Orthogonal time frequency space (OTFS) is a framework for communication and active sensing that processes signals in the delay-Doppler (DD) domain. This article explores three key features of the OTFS framework, and explains their value to applications. The first feature is a compact and sparse DD domain parameterization of the wireless channel, where the parameters map directly to physical attributes of the reflectors that comprise the scattering environment, and as a consequence these parameters evolve predictably. The second feature is a novel waveform/modulation technique, matched to the DD channel model, that embeds information symbols in the DD domain. The relation between channel inputs and outputs is localized, non-fading, and predictable, even in the presence of significant delay and Doppler spread, and as a consequence the channel can be efficiently acquired and equalized. By avoiding fading, the post equalization signal to noise ratio remains constant across all information symbols in a packet, so that bit error performance is superior to contemporary multicarrier waveforms. Further, the OTFS carrier waveform is a localized pulse in the DD domain, making it possible to separate reflectors along both delay and Doppler simultaneously, and to achieve a high-resolution DD radar image of the environment. In other words, the DD parameterization provides a common mathematical framework for communication and radar. This is the third feature of the OTFS framework, and it is ideally suited to intelligent transportation systems involving self-driving cars and unmanned ground/aerial vehicles, which are self/network controlled. The OTFS waveform is able to support stable and superior performance over a wide range of user speeds. In the emerging 6G systems and standards, it is ideally suited to support mobility-on-demand envisaged in next generation cellular and WiFi systems, as well as high-mobility use cases. Finally, the compactness and predictability of the OTFS input–output relation makes it a natural fit for machine learning and AI algorithms designed for the intelligent nonmyopic management of control plane resources in future mobile networks.",
        "authors": [
            "S. K. Mohammed",
            "R. Hadani",
            "A. Chockalingam",
            "Robert Calderbank"
        ],
        "citations": 62,
        "references": 36,
        "year": 2022
    },
    {
        "title": "Effect of grid-form deep soil mixing on the liquefaction-induced foundation settlement, using numerical approach",
        "abstract": null,
        "authors": [
            "F. Rahmani",
            "S. M. Hosseini",
            "Ali Khezri",
            "M. Maleki"
        ],
        "citations": 28,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Assessing effects of parameters of viscoelastic foundation on the dynamic response of functionally graded plates using a novel HSDT theory",
        "abstract": "Abstract In this study, vibration response of functionally graded plate resting on a viscoelastic foundation is studied. An analytical solution is proposed using a high-order shear deformation theory with only four unknowns, which means that the current theory has few unknowns compared to the first and other high shear deformation theories. The present theory uses a displacement field with integer terms instead of derivative terms by also including the shear deformation effect without introducing the shear correction factors. The mathematical model of the foundation used followed the Winkler–Pasternak two-coefficient model, with an additional term added to represent the damping effect. The governing equations were generated using the principle of virtual works. Subsequently, the analytical solution is based on Navier's principle to solve the vibration problem of a simply supported FG plate resting on a viscoelastic foundation. Some numerical results are presented to demonstrate the impact of the material index, the type of elastic foundation and the damping coefficient of the foundation on the dynamic response of FG plates resting on viscoelastic foundations. In the end, it is concluded that the current results with the proposed theory are found to be in good agreement with the other available results and this theory can easily be used to solve the free vibration problems of FGM plates resting on visco-Pasternak medium.",
        "authors": [
            "Hamid Frahlia",
            "Riadh Bennai",
            "Mokhtar Nebab",
            "Hassen Ait Atmane",
            "A. Tounsi"
        ],
        "citations": 26,
        "references": 93,
        "year": 2022
    },
    {
        "title": "Using Identity-Based Cryptography as a Foundation for an Effective and Secure Cloud Model for E-Health",
        "abstract": "Nowadays, one of the most popular applications is cloud computing for storing data and information through World Wide Web. Since cloud computing has become available, users are rapidly increasing. Cloud computing enables users to obtain a better and more effective application at a lower cost in a more satisfactory way. Health services data must therefore be kept as safe and secure as possible because the release of this data could have serious consequences for patients. A framework for security and privacy must be employed to store and manage extremely sensitive data. Patients' confidential health records have been encrypted and saved in the cloud using cypher text so far. To ensure privacy and security in a cloud computing environment is a big issue. The medical system has been designed as a standard, access of records, and effective use by medical practitioners as required. In this paper, we propose a novel algorithm along with implementation details as an effective and secure E-health cloud model using identity-based cryptography. The comparison of the proposed and existing techniques has been carried out in terms of time taken for encryption and decryption, energy, and power. Decryption time has been decreased up to 50% with the proposed method of cryptography. As it will take less time for decryption, less power is consumed for doing the cryptography operations.",
        "authors": [
            "Shikha Mittal",
            "Ankit Bansal",
            "D. Gupta",
            "Sapna Juneja",
            "H. Turabieh",
            "Mahmoud M Elarabawy",
            "Ashish Sharma",
            "Zelalem Kiros Bitsue"
        ],
        "citations": 63,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Foundation Transformers",
        "abstract": "A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name\"Transformers\", the above areas use different implementations for better performance, e.g., Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).",
        "authors": [
            "Hongyu Wang",
            "Shuming Ma",
            "Shaohan Huang",
            "Li Dong",
            "Wenhui Wang",
            "Zhiliang Peng",
            "Yu Wu",
            "Payal Bajaj",
            "Saksham Singhal",
            "Alon Benhaim",
            "Barun Patra",
            "Zhun Liu",
            "Vishrav Chaudhary",
            "Xia Song",
            "Furu Wei"
        ],
        "citations": 24,
        "references": 38,
        "year": 2022
    },
    {
        "title": "Model and Predictive Uncertainty: A Foundation for Smooth Ambiguity Preferences",
        "abstract": "Smooth ambiguity preferences (Klibanoff, Marinacci, and Mukerji (2005)) describe a decision maker who evaluates each act \n f according to the twofold expectation\n \n \n \n V\n (\n f\n )\n =\n \n \n ∫\n \n \n P\n \n \n ϕ\n (\n \n \n ∫\n \n \n Ω\n \n \n u\n (\n f\n )\n \n d\n p\n )\n \n d\n μ\n (\n p\n )\n \n \n defined by a utility function \n u, an ambiguity index \n ϕ, and a belief \n μ over a set \n \n P\n of probabilities. We provide an axiomatic foundation for the representation, taking as a primitive a preference over Anscombe–Aumann acts. We study a special case where \n \n P\n is a subjective statistical model that is point identified, that is, the decision maker believes that the true law \n \n p\n ∈\n P\n can be recovered empirically. Our main axiom is a joint weakening of Savage's sure‐thing principle and Anscombe–Aumann's mixture independence. In addition, we show that the parameters of the representation can be uniquely recovered from preferences, thereby making operational the separation between ambiguity attitude and perception, a hallmark feature of the smooth ambiguity representation.\n",
        "authors": [
            "Tommaso Denti",
            "L. Pomatto"
        ],
        "citations": 23,
        "references": 64,
        "year": 2022
    },
    {
        "title": "Construction Risk Assessment of Deep Foundation Pit Projects Based on the Projection Pursuit Method and Improved Set Pair Analysis",
        "abstract": "Accurately evaluating the construction risk of deep foundation pit projects is crucial to formulate science-based risk response measures. Here, we propose a novel construction risk assessment method for deep foundation pit projects. A construction risk evaluation index system based on a work breakdown structure-risk breakdown structure matrix was established to deal with the complex risks of deep foundation pit construction. The projection pursuit method optimized by particle swarm optimization was used to extract the structural features from the evaluation data to obtain objective index weights. The calculation method of the five-element connection number in the set pair analysis was improved to evaluate the static construction risk. The partial derivatives of the five-element connection number were utilized to assess the dynamic construction risk. The Qi ‘an Fu deep foundation pit project in China was selected as a case study. The results show that the construction risk was acceptable and decreased during the construction period, which was consistent with actual conditions, demonstrating the effectiveness of this novel method. The proposed model showed better performance than classical methods (analytic hierarchy process, entropy weight method, classical set pair analysis, fuzzy comprehensive evaluation, gray clustering method, backpropagation neural network, and support vector machine).",
        "authors": [
            "Long Zhang",
            "Hongbing Li"
        ],
        "citations": 22,
        "references": 25,
        "year": 2022
    },
    {
        "title": "Interactions among safety risks in metro deep foundation pit projects: An association rule mining-based modeling framework",
        "abstract": null,
        "authors": [
            "Lipeng Fu",
            "Xueqing Wang",
            "Heng Zhao",
            "Mengnan Li"
        ],
        "citations": 46,
        "references": 32,
        "year": 2022
    },
    {
        "title": "An Integrated Intelligent Approach for Monitoring and Management of a Deep Foundation Pit in a Subway Station",
        "abstract": "As the scale of foundation pit projects of subway stations in Shenzhen becomes larger, and the construction constraints become more and more complex, there is an urgent need for intelligent monitoring and safety management of foundation pits. In this study, an integrated intelligent approach for monitoring and management of a deep foundation pit in a subway station was proposed and a case study based on the Waterlands Resort East Station Project of Shenzhen Metro Line 12 was used for validation. The present study first proposed the path of intelligent foundation pit engineering. Based on geotechnical survey and building information modeling, a three-dimensional transparent geological model of foundation pit was constructed. Multi-source sensing technologies were integrated, including micro electromechanical system sensing technology, Brillouin optical frequency domain analysis sensing technology, an unmanned aerial vehicle and machine vision for real-time high-precision wireless monitoring of the foundation pit. Moreover, machine learning models were developed for predicting key parameters of foundation pits. Finally, a digital twin integrated platform was developed for the management of the subway foundation pit in both construction and maintenance phases. This typical case study is expected to improve the construction, maintenance and management level of foundation pits in subway stations.",
        "authors": [
            "Chengyu Hong",
            "Jinyang Zhang",
            "Weibin Chen"
        ],
        "citations": 16,
        "references": 63,
        "year": 2022
    },
    {
        "title": "Geometrically Nonlinear Response of FGM Beams on Elastic Foundation Subjected to Thermal Shock",
        "abstract": null,
        "authors": [
            "H. Bagheri",
            "Y. Kiani",
            "M. Eslami"
        ],
        "citations": 17,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Research on the Settlement Prediction Model of Foundation Pit Based on the Improved PSO-SVM Model",
        "abstract": "This paper presents a settlement prediction method based on PSO optimized SVM for improving the accuracy of foundation pit settlement prediction. Firstly, the method uses the SA algorithm to improve the traditional PSO algorithm, and thus, the overall optimization-seeking ability of the PSO algorithm is improved. Secondly, the improved PSO algorithm is used to train the SVM algorithm. Finally, the optimal SVM model is obtained, and the trained model is used in foundation pit settlement prediction. The results suggest that the settling results obtained from the optimized model are closer to the actual values and also more advantageous in indicators such as RMSE. The fitting value R2 = 0.9641, which is greater, indicates a better fitting effect. Thus, it is indicated that the improvement method is feasible.",
        "authors": [
            "Zhibin Song",
            "Shurong Liu",
            "Mingyue Jiang",
            "Suling Yao"
        ],
        "citations": 17,
        "references": 28,
        "year": 2022
    },
    {
        "title": "Static analysis of functionally graded plate structures resting on variable elastic foundation under various boundary conditions",
        "abstract": null,
        "authors": [
            "A. Daikh",
            "M. Belarbi",
            "D. Ahmed",
            "M. Houari",
            "M. Avcar",
            "A. Tounsi",
            "M. A. Eltaher"
        ],
        "citations": 45,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Physics-Informed Multifidelity Residual Neural Networks for Hydromechanical Modeling of Granular Soils and Foundation Considering Internal Erosion",
        "abstract": null,
        "authors": [
            "Pin Zhang",
            "Z. Yin",
            "Yin‐Fu Jin",
            "Jie Yang",
            "B. Sheil"
        ],
        "citations": 40,
        "references": 38,
        "year": 2022
    },
    {
        "title": "Application of regularized ELM optimized by sine algorithm in prediction of ground settlement around foundation pit",
        "abstract": null,
        "authors": [
            "Yalu Han",
            "Yong Wang",
            "Chenyang Liu",
            "X. Hu",
            "L. Du"
        ],
        "citations": 13,
        "references": 63,
        "year": 2022
    },
    {
        "title": "RustHornBelt: a semantic foundation for functional verification of Rust programs with unsafe code",
        "abstract": "Rust is a systems programming language that offers both low-level memory operations and high-level safety guarantees, via a strong ownership type system that prohibits mutation of aliased state. In prior work, Matsushita et al. developed RustHorn, a promising technique for functional verification of Rust code: it leverages the strong invariants of Rust types to express the behavior of stateful Rust code with first-order logic (FOL) formulas, whose verification is amenable to off-the-shelf automated techniques. RustHorn’s key idea is to use prophecies to describe the behavior of mutable borrows. However, the soundness of RustHorn was only established for a safe subset of Rust, and it has remained unclear how to extend it to support various safe APIs that encapsulate unsafe code (i.e., code where Rust’s aliasing discipline is relaxed). In this paper, we present RustHornBelt, the first machine-checked proof of soundness for RustHorn-style verification which supports giving FOL specs to safe APIs implemented with unsafe code. RustHornBelt employs the approach of semantic typing used in Jung et al.’s RustBelt framework, but it extends RustBelt’s model to reason not only about safety but also functional correctness. The key challenge in RustHornBelt is to develop a semantic model of RustHorn-style prophecies, which we achieve via a new separation-logic mechanism we call parametric prophecies.",
        "authors": [
            "Yusuke Matsushita",
            "Xavier Denis",
            "Jacques-Henri Jourdan",
            "Derek Dreyer"
        ],
        "citations": 36,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Dynamic modeling of the gear-rotor systems with spatial propagation crack and complicated foundation structure",
        "abstract": null,
        "authors": [
            "Kangkang Chen",
            "Y. Huangfu",
            "Zhifang Zhao",
            "Hui Ma",
            "Xingjian Dong"
        ],
        "citations": 30,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Large Language Models and the Reverse Turing Test",
        "abstract": "Abstract Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.",
        "authors": [
            "T. Sejnowski"
        ],
        "citations": 84,
        "references": 84,
        "year": 2022
    },
    {
        "title": "Construction safety ontology development and alignment with industry foundation classes (IFC)",
        "abstract": "A pronounced gap often exists between expected and actual safety performance in the construction industry. The multifaceted causes of this performance gap are resulting from the misalignment between design assumptions and actual construction processes that take place on-site. In general, critical factors are rooted in the lack of interoperability around the building and work-environment information due to its heterogeneous nature. To overcome the interoperability challenge in safety management, this paper represents the development of an ontological model consisting of terms and relationships between these terms, creating a conceptual information model for construction safety management and linking that ontology to IfcOWL. The developed ontology, named Safety and Health Exchange (SHE), comprises eight concepts and their relationships required to identify and manage safety risks in the design and planning stages. The main concepts of the developed ontology are identified based on reviewing accident cases from 165 Reporting of Injuries, Diseases and Dangerous Occurrences Regulations (RIDDOR) and 31 Press Releases from the database of the Health and Safety Executive (HSE) in the United Kingdom. Consequently, a semantic mapping between the developed ontology and IfcOWL (the most popular ontology and schema for interoperability in the AEC sector) is proposed. Then several SPARQL queries were developed and implemented to evaluate the semantic consistency of the developed ontology and the cross-mapping. The proposed ontology and cross-mapping gained recognition for its innovation in utilising OpenBIM and won the BuildingSMART professional research award 2020. This work could facilitate developing a knowledge-based system in the BIM environment to assist designers in addressing health and safety issues during the design and planning phases in the construction sector.",
        "authors": [
            "Karim Farghaly",
            "Ranjith K. Soman",
            "William H. Collinge",
            "M. H. Mosleh",
            "P. Manu",
            "C. Cheung"
        ],
        "citations": 21,
        "references": 53,
        "year": 2022
    },
    {
        "title": "Hydro-mechanical analysis of a braced foundation pit affected by rainfall and excavation in unsaturated soils",
        "abstract": null,
        "authors": [
            "Xiao-Qian Zhang",
            "Ming‐Guang Li",
            "Jin-jian Chen"
        ],
        "citations": 12,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Numerical Analysis of the Carrying Capacity of a Piled Raft Foundation in Soft Clayey Soils",
        "abstract": "Piled raft foundations are a common type of foundation for high-rise buildings. Unlike shallow foundations, deep foundations (piles) pass through weak or soft soil deposits and can reach stiff soil or bedrock to support the weight of the structure. In this paper, the performance of a medium embedment depth piled raft foundation in soft soil is presented. A numerical model was developed and a parametric study was conducted in order to simulate the case of such a foundation system and to investigate its performance in soft clay. This parametric study investigated the effect of the geometry of a piled raft foundation and the stiffness ratio between the pile material and clay on the performance of the foundation system in soft soil. Additionally, the failure mechanism of such a foundation system under load was examined. An analytical model was developed to predict the ultimate carrying capacity based on the observed failure mechanism. A semi-empirical model is proposed for determining the Improvement Factor (IF) of a given soil, pile, and geometric condition. Findings of the study indicate that the performance of piled raft foundations on soft soils is significantly affected by the piles’ spacing. As the ratio S/D increases, the ultimate carrying capacity of a piled raft foundation decreases. However, when this ratio exceeds 10 (S/D> 10), piles have little or no effect on the ultimate carrying capacity of this foundation system. A piled raft foundation system fails by bearing at the base of the piles and also by shear at the side of the pile group on hyperbolic plans. Doi: 10.28991/CEJ-2022-08-04-01 Full Text: PDF",
        "authors": [
            "D. Ahmed",
            "Siti Noor Linda Bt Taib",
            "T. Ayadat",
            "A. Hasan"
        ],
        "citations": 12,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Seismic Fragility Assessment of a Novel Suction Bucket Foundation for Offshore Wind Turbine under Scour Condition",
        "abstract": "This study proposed a new suction bucket (SB) foundation model for offshore wind turbines (OWT) suitable for a shallow muddy seabed, using more than three single buckets through kinetic derivation. The performance of new optimal foundation was evaluated by its horizontal displacement capacity and compared with a conventional SB composed of three buckets. Under external loads such as earthquakes, wind, and the combination of the both, the stability of this novel SB foundation was verified. The seismic fragility curve was also evaluated at some scour depths. These results were compared with the response of a tripod suction bucket (TSB) foundation, which was also designed for a shallow muddy seabed. The results indicated that scour significantly changed the dynamic response of this novel SB foundation but it had a better bearing capacity than the TSB foundation, despite its smaller size and weight. The fragility of TSB is always higher than the developed foundation in the same environmental condition. With reasonable volume and size, this novel SB foundation has great potential for future industrialization and commercialization.",
        "authors": [
            "Duc-Vu Ngo",
            "Young-Jin Kim",
            "Dong-Hyawn Kim"
        ],
        "citations": 12,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Ground vibration induced by high speed trains on an embankment with pile-board foundation: modelling and validation with in situ tests",
        "abstract": null,
        "authors": [
            "Li Wang",
            "Ping Wang",
            "Kai Wei",
            "R. Dollevoet",
            "Zili Li"
        ],
        "citations": 20,
        "references": 50,
        "year": 2022
    },
    {
        "title": "Adversarial Attacks on Foundational Vision Models",
        "abstract": "Rapid progress is being made in developing large, pretrained, task-agnostic foundational vision models such as CLIP, ALIGN, DINOv2, etc. In fact, we are approaching the point where these models do not have to be finetuned downstream, and can simply be used in zero-shot or with a lightweight probing head. Critically, given the complexity of working at this scale, there is a bottleneck where relatively few organizations in the world are executing the training then sharing the models on centralized platforms such as HuggingFace and torch.hub. The goal of this work is to identify several key adversarial vulnerabilities of these models in an effort to make future designs more robust. Intuitively, our attacks manipulate deep feature representations to fool an out-of-distribution (OOD) detector which will be required when using these open-world-aware models to solve closed-set downstream tasks. Our methods reliably make in-distribution (ID) images (w.r.t. a downstream task) be predicted as OOD and vice versa while existing in extremely low-knowledge-assumption threat models. We show our attacks to be potent in whitebox and blackbox settings, as well as when transferred across foundational model types (e.g., attack DINOv2 with CLIP)! This work is only just the beginning of a long journey towards adversarially robust foundational vision models.",
        "authors": [
            "Nathan Inkawhich",
            "Gwendolyn McDonald",
            "R. Luley"
        ],
        "citations": 8,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Research and Application of Deformation Prediction Model for Deep Foundation Pit Based on LSTM",
        "abstract": "Deep foundation pit is a door with a long history, but it has new disciplines; in this paper, firstly, the modeling method and process of LSTM (long short-term memory) network are discussed in detail, then the optimization algorithm used in the model is described in detail, and the parameter selection methods such as initial learning rate, activation function, and iteration number related to LSTM network training are introduced in detail. LSTM network is used to process the deformation data of deep foundation pit, and random gradient descent, momentum, Nesterov, RMSProp, AdaGmd, and Adam algorithms are selected in the same example for modeling prediction and comparison. Two examples of horizontal displacement prediction of pile and vertical displacement prediction of column in deep foundation pit show that the LSTM network model established by different optimization algorithms has different prediction accuracy, and the LSTM network model established by Adam optimization algorithm has the highest accuracy, which proves that the selection of optimization algorithm plays an important role in LSTM and also verifies the feasibility of LSTM network in the data processing and prediction of deep foundation pit deformation.",
        "authors": [
            "Haizhou Li",
            "Zhizhou Zhao",
            "Xue Du"
        ],
        "citations": 10,
        "references": 37,
        "year": 2022
    },
    {
        "title": "Numerical modelling of the effects of foundation scour on the response of a bridge pier",
        "abstract": null,
        "authors": [
            "A. Ciancimino",
            "I. Anastasopoulos",
            "S. Foti",
            "A. Gajo"
        ],
        "citations": 10,
        "references": 70,
        "year": 2022
    },
    {
        "title": "The failure mode of transmission tower foundation on the landslide under heavy rainfall: a case study on a 500-kV transmission tower foundation on the Yanzi landslide in Badong, China",
        "abstract": null,
        "authors": [
            "Yongqiang Zhou",
            "Q. Sheng",
            "Jian Chen",
            "Nana Li",
            "X. Fu",
            "Ying Zhou"
        ],
        "citations": 9,
        "references": 43,
        "year": 2022
    },
    {
        "title": "Soil-foundation interaction model for the assessment of tunnelling-induced damage to masonry buildings",
        "abstract": null,
        "authors": [
            "H. Burd",
            "W. N. Yiu",
            "S. Acikgoz",
            "C. Martin"
        ],
        "citations": 15,
        "references": 19,
        "year": 2022
    },
    {
        "title": "Fretting wear modeling of 3D and 2D Hertzian contacts with a third-body layer using a Winkler elastic foundation model",
        "abstract": null,
        "authors": [
            "Simon Garcin",
            "S. Baydoun",
            "P. Arnaud",
            "S. Fouvry"
        ],
        "citations": 14,
        "references": 29,
        "year": 2022
    },
    {
        "title": "The transition towards circular economy and waste within accounting and accountability models: a systematic literature review and conceptual framework",
        "abstract": null,
        "authors": [
            "Assunta Di Vaio",
            "Sohail Hasan",
            "Rosa Palladino",
            "Rohail Hassan"
        ],
        "citations": 89,
        "references": 86,
        "year": 2022
    },
    {
        "title": "Toward Foundational Deep Learning Models for Medical Imaging in the New Era of Transformer Networks.",
        "abstract": "Deep learning models are currently the cornerstone of artificial intelligence in medical imaging. While progress is still being made, the generic technological core of convolutional neural networks (CNNs) has had only modest innovations over the last several years, if at all. There is thus a need for improvement. More recently, transformer networks have emerged that replace convolutions with a complex attention mechanism, and they have already matched or exceeded the performance of CNNs in many tasks. Transformers need very large amounts of training data, even more than CNNs, but obtaining well-curated labeled data is expensive and difficult. A possible solution to this issue would be transfer learning with pretraining on a self-supervised task using very large amounts of unlabeled medical data. This pretrained network could then be fine-tuned on specific medical imaging tasks with relatively modest data requirements. The authors believe that the availability of a large-scale, three-dimension-capable, and extensively pretrained transformer model would be highly beneficial to the medical imaging and research community. In this article, authors discuss the challenges and obstacles of training a very large medical imaging transformer, including data needs, biases, training tasks, network architecture, privacy concerns, and computational requirements. The obstacles are substantial but not insurmountable for resourceful collaborative teams that may include academia and information technology industry partners. © RSNA, 2022 Keywords: Computer-aided Diagnosis (CAD), Informatics, Transfer Learning, Convolutional Neural Network (CNN).",
        "authors": [
            "M. Willemink",
            "H. Roth",
            "V. Sandfort"
        ],
        "citations": 34,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Can Language Models Handle Recursively Nested Grammatical Structures? A Case Study on Comparing Models and Humans",
        "abstract": "Abstract How should we compare the capabilities of language models (LMs) and humans? In this article, I draw inspiration from comparative psychology to highlight challenges in these comparisons. I focus on a case study: processing of recursively nested grammatical structures. Prior work suggests that LMs cannot process these structures as reliably as humans can. However, the humans were provided with instructions and substantial training, while the LMs were evaluated zero-shot. I therefore match the evaluation more closely. Providing large LMs with a simple prompt—with substantially less content than the human training—allows the LMs to consistently outperform the human results, even in more deeply nested conditions than were tested with humans. Furthermore, the effects of prompting are robust to the particular structures and vocabulary used in the prompt. Finally, reanalyzing the existing human data suggests that the humans may not perform above chance at the difficult structures initially. Thus, large LMs may indeed process recursively nested grammatical structures as reliably as humans, when evaluated comparably. This case study highlights how discrepancies in the evaluation methods can confound comparisons of language models and humans. I conclude by reflecting on the broader challenge of comparing human and model capabilities, and highlight an important difference between evaluating cognitive models and foundation models.",
        "authors": [
            "Andrew Kyle Lampinen"
        ],
        "citations": 33,
        "references": 102,
        "year": 2022
    },
    {
        "title": "Centrifuge Model Investigation of Interaction between Successively Constructed Foundation Pits",
        "abstract": "A series of centrifuge model tests were conducted to study the interaction between successively constructed adjacent foundation pits. The stress, deformation, and earth pressure on retaining structures and the settlement of the soil between the two adjacent foundation pits during successive construction were investigated by a comprehensive instrumentation program. To reveal the effect of the construction sequence, both the stress and deformation of successively constructed foundation pits were compared. The results showed that the stress and deformation of the retaining structure in the foundation pit constructed first were larger than those in the foundation pit constructed later. Due to the inward displacement of the soil around the foundation pits excavated first, the first strut of the foundation pit constructed later underwent high tension during the construction of the first foundation pit. The lateral deformation of the retaining structure of the foundation pit excavated first increased with the increase of the excavation depth. However, the excavation of the second foundation pit reduced the earth pressure on the retaining wall between the two excavations, thus leading to the recovery of the inward deformation in the first excavation. However, the top of the retaining wall deformed into the first foundation pit during the whole construction. The settlement of the soil between the two foundation pits showed a superposition effect. During the construction of the two foundation pits, the settlement of the soil between them kept increasing. The active earth pressure on the middle wall of the foundation pit constructed later was lower than that on the middle wall of the first foundation pit. The excavation of the foundation pit constructed later had no significant effect on the passive earth pressure of the first foundation pit.",
        "authors": [
            "Shang-Xiong Chen",
            "Ji-fei Cui",
            "F. Liang"
        ],
        "citations": 7,
        "references": 20,
        "year": 2022
    },
    {
        "title": "Diffusion Generative Models in Infinite Dimensions",
        "abstract": "Diffusion generative models have recently been applied to domains where the available data can be seen as a discretization of an underlying function, such as audio signals or time series. However, these models operate directly on the discretized data, and there are no semantics in the modeling process that relate the observed data to the underlying functional forms. We generalize diffusion models to operate directly in function space by developing the foundational theory for such models in terms of Gaussian measures on Hilbert spaces. A significant benefit of our function space point of view is that it allows us to explicitly specify the space of functions we are working in, leading us to develop methods for diffusion generative modeling in Sobolev spaces. Our approach allows us to perform both unconditional and conditional generation of function-valued data. We demonstrate our methods on several synthetic and real-world benchmarks.",
        "authors": [
            "Gavin Kerrigan",
            "Justin Ley",
            "Padhraic Smyth"
        ],
        "citations": 25,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Consent as a Foundation for Responsible Autonomy",
        "abstract": "This paper focuses on a dynamic aspect of responsible autonomy, namely, to make intelligent agents be responsible at run time. That is, it considers settings where decision making by agents impinges upon the outcomes perceived by other agents. For an agent to act responsibly, it must accommodate the desires and other attitudes of its users and, through other agents, of their users.\n\nThe contribution of this paper is twofold. First, it provides a conceptual analysis of consent, its benefits and misuses, and how understanding consent can help achieve responsible autonomy. Second, it outlines challenges for AI (in particular, for agents and multiagent systems) that merit investigation to form as a basis for modeling consent in multiagent systems and applying consent to achieve responsible autonomy.",
        "authors": [
            "Munindar P. Singh"
        ],
        "citations": 10,
        "references": 53,
        "year": 2022
    },
    {
        "title": "Holistic Adversarial Robustness of Deep Learning Models",
        "abstract": "Adversarial robustness studies the worst-case performance of a machine learning model to ensure safety and reliability. With the proliferation of deep-learning-based technology, the potential risks associated with model development and deployment can be amplified and become dreadful vulnerabilities. This paper provides a comprehensive overview of research topics and foundational principles of research methods for adversarial robustness of deep learning models, including attacks, defenses, verification, and novel applications.",
        "authors": [
            "Pin-Yu Chen",
            "Sijia Liu"
        ],
        "citations": 16,
        "references": 132,
        "year": 2022
    },
    {
        "title": "Parallel Factories for Smart Industrial Operations: From Big AI Models to Field Foundational Models and Scenarios Engineering",
        "abstract": null,
        "authors": [
            "Jingwei Lu",
            "Xingxia Wang",
            "Xiang Cheng",
            "J. Yang",
            "Oliver Kwan",
            "Xiao Wang"
        ],
        "citations": 26,
        "references": 15,
        "year": 2022
    },
    {
        "title": "Advanced Modelling of Soil Organic Carbon Content in Coal Mining Areas Using Integrated Spectral Analysis: A Dengcao Coal Mine Case Study",
        "abstract": "Effective modelling and integrated spectral analysis approaches can advance modelling precision. To develop an integrated spectral forecast modelling of soil organic carbon (SOC), this research investigated a mining coal in Dengcao Coal Mine Area, Zhengzhou. The study utilizes the Lasso and Ranger algorithms were utilized in spectral band analysis. Four primary models employed during this process include Artificial Neural Network (ANN), Support Vector Machine, Random Forest (RF), and Partial Least Squares Regression (PLSR). The ideal model was chosen. The results showed that, in contrast to when band collection was based on Lasso algorithm modelling, model precision was higher when it was based on the Ranger algorithm. ANN model had an ideal goodness acceptance, and the modelling developed by RF showed the steadiest modelling consequences. Based on the results, a distinct method is proposed in this study for band assortment at the earlier stage of integrated spectral modelling of SOC. The Ranger method can be used to check the spectral particles, and RF or ANN can be chosen to develop the prediction modelling based on different statistics sets, which is appropriate to create the prediction modelling of SOC content in Dengcao Coal Mine Area. This research avails a position for the integrated spectral of Analysis for Advanced Modelling of Soil Organic Carbon Content in Coal Sources alongside a theoretical foundation for innovating portable device for the integrated spectral assessment of SOC content in coal mining habitats. This study might be significant for the changing modelling and monitoring of SOC in mining and environmental areas.",
        "authors": [
            "Gill Ammara",
            "Xiaojun Nie",
            "Chang -hua LIU"
        ],
        "citations": 928,
        "references": 34,
        "year": 2024
    },
    {
        "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
        "abstract": "This work presents Depth Anything11While the grammatical soundness of this name may be questionable, we treat it as a whole and pay homage to Segment Anything [26]., a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability (Figure 1). Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released here.",
        "authors": [
            "Lihe Yang",
            "Bingyi Kang",
            "Zilong Huang",
            "Xiaogang Xu",
            "Jiashi Feng",
            "Hengshuang Zhao"
        ],
        "citations": 404,
        "references": 89,
        "year": 2024
    },
    {
        "title": "SAM 2: Segment Anything in Images and Videos",
        "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",
        "authors": [
            "Nikhila Ravi",
            "Valentin Gabeur",
            "Yuan-Ting Hu",
            "Ronghang Hu",
            "Chaitanya K. Ryali",
            "Tengyu Ma",
            "Haitham Khedr",
            "Roman Rädle",
            "Chloé Rolland",
            "Laura Gustafson",
            "Eric Mintun",
            "Junting Pan",
            "Kalyan Vasudev Alwala",
            "Nicolas Carion",
            "Chao-Yuan Wu",
            "Ross B. Girshick",
            "Piotr Doll'ar",
            "Christoph Feichtenhofer"
        ],
        "citations": 240,
        "references": 126,
        "year": 2024
    },
    {
        "title": "Qwen2 Technical Report",
        "abstract": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",
        "authors": [
            "An Yang",
            "Baosong Yang",
            "Binyuan Hui",
            "Bo Zheng",
            "Bowen Yu",
            "Chang Zhou",
            "Chengpeng Li",
            "Chengyuan Li",
            "Dayiheng Liu",
            "Fei Huang",
            "Guanting Dong",
            "Haoran Wei",
            "Huan Lin",
            "Jialong Tang",
            "Jialin Wang",
            "Jian Yang",
            "Jianhong Tu",
            "Jianwei Zhang",
            "Jianxin Ma",
            "Jin Xu",
            "Jingren Zhou",
            "Jinze Bai",
            "Jinzheng He",
            "Junyang Lin",
            "Kai Dang",
            "Keming Lu",
            "Ke-Yang Chen",
            "Kexin Yang",
            "Mei Li",
            "Min Xue",
            "Na Ni",
            "Pei Zhang",
            "Peng Wang",
            "Ru Peng",
            "Rui Men",
            "Ruize Gao",
            "Runji Lin",
            "Shijie Wang",
            "Shuai Bai",
            "Sinan Tan",
            "Tianhang Zhu",
            "Tianhao Li",
            "Tianyu Liu",
            "Wenbin Ge",
            "Xiaodong Deng",
            "Xiaohuan Zhou",
            "Xingzhang Ren",
            "Xinyu Zhang",
            "Xipin Wei",
            "Xuancheng Ren",
            "Yang Fan",
            "Yang Yao",
            "Yichang Zhang",
            "Yunyang Wan",
            "Yunfei Chu",
            "Zeyu Cui",
            "Zhenru Zhang",
            "Zhi-Wei Fan"
        ],
        "citations": 457,
        "references": 77,
        "year": 2024
    },
    {
        "title": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
        "abstract": "In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding.",
        "authors": [
            "Jianlv Chen",
            "Shitao Xiao",
            "Peitian Zhang",
            "Kun Luo",
            "Defu Lian",
            "Zheng Liu"
        ],
        "citations": 174,
        "references": 60,
        "year": 2024
    },
    {
        "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
        "abstract": "Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.",
        "authors": [
            "Lianghui Zhu",
            "Bencheng Liao",
            "Qian Zhang",
            "Xinlong Wang",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "citations": 449,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
        "abstract": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
        "authors": [
            "Penghao Zhao",
            "Hailin Zhang",
            "Qinhan Yu",
            "Zhengren Wang",
            "Yunteng Geng",
            "Fangcheng Fu",
            "Ling Yang",
            "Wentao Zhang",
            "Bin Cui"
        ],
        "citations": 120,
        "references": 400,
        "year": 2024
    },
    {
        "title": "Genie: Generative Interactive Environments",
        "abstract": "We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",
        "authors": [
            "Jake Bruce",
            "Michael D. Dennis",
            "Ashley Edwards",
            "Jack Parker-Holder",
            "Yuge Shi",
            "Edward Hughes",
            "Matthew Lai",
            "Aditi Mavalankar",
            "Richie Steigerwald",
            "Chris Apps",
            "Y. Aytar",
            "Sarah Bechtle",
            "Feryal M. P. Behbahani",
            "Stephanie Chan",
            "N. Heess",
            "Lucy Gonzalez",
            "Simon Osindero",
            "Sherjil Ozair",
            "Scott Reed",
            "Jingwei Zhang",
            "Konrad Zolna",
            "Jeff Clune",
            "Nando de Freitas",
            "Satinder Singh",
            "Tim Rocktaschel"
        ],
        "citations": 80,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation",
        "abstract": "We introduce LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models.",
        "authors": [
            "Peize Sun",
            "Yi Jiang",
            "Shoufa Chen",
            "Shilong Zhang",
            "Bingyue Peng",
            "Ping Luo",
            "Zehuan Yuan"
        ],
        "citations": 84,
        "references": 98,
        "year": 2024
    },
    {
        "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
        "abstract": "Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as a valuable framework for future research collaborations that aim to bridge gaps in resources.",
        "authors": [
            "Shivalika Singh",
            "Freddie Vargus",
            "Daniel Dsouza",
            "Börje F. Karlsson",
            "Abinaya Mahendiran",
            "Wei-Yin Ko",
            "Herumb Shandilya",
            "Jay Patel",
            "Deividas Mataciunas",
            "Laura OMahony",
            "Mike Zhang",
            "Ramith Hettiarachchi",
            "Joseph Wilson",
            "Marina Machado",
            "Luisa Souza Moura",
            "Dominik Krzemi'nski",
            "Hakimeh Fadaei",
            "Irem Ergun",
            "Ifeoma Okoh",
            "Aisha Alaagib",
            "Oshan Mudannayake",
            "Zaid Alyafeai",
            "Minh Chien Vu",
            "Sebastian Ruder",
            "Surya Guthikonda",
            "Emad A. Alghamdi",
            "Sebastian Gehrmann",
            "Niklas Muennighoff",
            "Max Bartolo",
            "Julia Kreutzer",
            "A. Ustun",
            "Marzieh Fadaee",
            "Sara Hooker"
        ],
        "citations": 87,
        "references": 215,
        "year": 2024
    },
    {
        "title": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein",
        "abstract": "Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to an advanced 3D structural prediction model that surpasses existing language model-based tools. 2) xTrimoPGLM not only can generate de novo protein sequences following the principles of natural ones, but also can perform programmable generation after supervised fine-tuning (SFT) on curated sequences. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences, contributing to the evolving landscape of foundation models in protein science. Trained weight for the xTrimoPGLM model, and downstream datasets are available at https://huggingface.co/proteinglm.",
        "authors": [
            "Bo Chen",
            "Xingyi Cheng",
            "Yangli-ao Geng",
            "Shengyin Li",
            "Xin Zeng",
            "Bo Wang",
            "Jing Gong",
            "Chiming Liu",
            "Aohan Zeng",
            "Yuxiao Dong",
            "Jie Tang",
            "Leo T. Song"
        ],
        "citations": 75,
        "references": 111,
        "year": 2024
    },
    {
        "title": "RSMamba: Remote Sensing Image Classification With State Space Model",
        "abstract": "Remote sensing image classification forms the foundation of various understanding tasks, serving a crucial function in remote sensing image interpretation. The recent advancements of convolutional neural networks (CNNs) and transformers have markedly enhanced classification accuracy. Nonetheless, remote sensing scene classification remains a significant challenge, especially given the complexity and diversity of remote sensing scenarios and the variability of spatiotemporal resolutions. The capacity for whole-image understanding can provide more precise semantic cues for scene discrimination. In this letter, we introduce RSMamba, a novel architecture for remote sensing image classification. RSMamba is based on the state space model (SSM) and incorporates an efficient, hardware-aware design known as the Mamba. It integrates the advantages of both a global receptive field and linear modeling complexity. To overcome the limitation of the vanilla Mamba, which can only model causal sequences and is not adaptable to 2-D image data, we propose a dynamic multipath activation mechanism to augment Mamba’s capacity to model noncausal data. Notably, RSMamba maintains the inherent modeling mechanism of the vanilla Mamba, yet exhibits superior performance across multiple remote sensing image classification datasets, e.g., F1 scores of 95.25, 92.63, and 95.18 on the UC Merced, AID, and RESISC45 classification datasets, respectively, exceeding those of concurrent Vim and VMamba. This indicates that RSMamba holds significant potential to function as the backbone of future visual foundation models. The code is available at https://github.com/KyanChen/RSMamba.",
        "authors": [
            "Keyan Chen",
            "Bo-Ying Chen",
            "Chenyang Liu",
            "Wenyuan Li",
            "Zhengxia Zou",
            "Z. Shi"
        ],
        "citations": 60,
        "references": 19,
        "year": 2024
    },
    {
        "title": "Principles of Power Electronics",
        "abstract": "Substantially\n expanded and updated, the new edition of this classic textbook\n provides unrivalled coverage of the fundamentals of power\n electronics. Comprehensive coverage of foundational concepts in\n circuits, magnetics, devices, dynamic models, and control\n establishes a strong conceptual framework for further study.\n Extensive discussion of contemporary practical considerations,\n enhanced by real-world examples, prepares readers for design\n scenarios ranging from low-power dc/dc converters to\n multi-megawatt ac machine drives. New topics include SiC and GaN\n wide-bandgap materials, superjunction MOSFET and IGBT devices,\n advanced magnetics design, multi-level and switched-capacitor\n converters, RF converter circuits, and EMI. Over 300 new and\n revised end-of-chapter problems enhance and expand understanding\n of the material, with solutions for instructors. Unique in its\n breadth and depth, and providing a range of flexible teaching\n pathways at multiple levels, this is the definitive guide to\n power electronics for graduate and senior undergraduate students\n in electrical engineering, and practicing electrical\n engineers.",
        "authors": [
            "J. Kassakian",
            "M. Schlecht",
            "G. Verghese"
        ],
        "citations": 1000,
        "references": 4,
        "year": 2023
    },
    {
        "title": "Evolutionary Optimization of Model Merging Recipes",
        "abstract": null,
        "authors": [
            "Takuya Akiba",
            "Makoto Shing",
            "Yujin Tang",
            "Qi Sun",
            "David Ha"
        ],
        "citations": 61,
        "references": 43,
        "year": 2024
    },
    {
        "title": "SaulLM-7B: A pioneering Large Language Model for Law",
        "abstract": "In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the MIT License.",
        "authors": [
            "Pierre Colombo",
            "T. Pires",
            "Malik Boudiaf",
            "Dominic Culver",
            "Rui Melo",
            "Caio Corro",
            "André Martins",
            "Fabrizio Esposito",
            "Vera L'ucia Raposo",
            "Sofia Morgado",
            "Michael Desa"
        ],
        "citations": 43,
        "references": 75,
        "year": 2024
    },
    {
        "title": "Mechanistic Interpretability for AI Safety - A Review",
        "abstract": "Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.",
        "authors": [
            "Leonard Bereska",
            "E. Gavves"
        ],
        "citations": 53,
        "references": 287,
        "year": 2024
    },
    {
        "title": "Scientific discovery in the age of artificial intelligence",
        "abstract": null,
        "authors": [
            "Hanchen Wang",
            "Tianfan Fu",
            "Yuanqi Du",
            "Wenhao Gao",
            "Kexin Huang",
            "Ziming Liu",
            "P. Chandak",
            "Shengchao Liu",
            "Peter Van Katwyk",
            "Andreea Deac",
            "Anima Anandkumar",
            "K. Bergen",
            "Carla P. Gomes",
            "Shirley Ho",
            "Pushmeet Kohli",
            "Joan Lasenby",
            "J. Leskovec",
            "Tie-Yan Liu",
            "A. Manrai",
            "Debora S. Marks",
            "Bharath Ramsundar",
            "Le Song",
            "Jimeng Sun",
            "Jian Tang",
            "Petar Velickovic",
            "Max Welling",
            "Linfeng Zhang",
            "Connor W. Coley",
            "Y. Bengio",
            "M. Zitnik"
        ],
        "citations": 587,
        "references": 269,
        "year": 2023
    },
    {
        "title": "MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
        "abstract": "We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and text-books, covering six core disciplines: Art & Design, Busi-ness, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly het-erogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 28 open-source LMMs as well as the propri-etary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.",
        "authors": [
            "Xiang Yue",
            "Yuansheng Ni",
            "Kai Zhang",
            "Tianyu Zheng",
            "Ruoqi Liu",
            "Ge Zhang",
            "Samuel Stevens",
            "Dongfu Jiang",
            "Weiming Ren",
            "Yuxuan Sun",
            "Cong Wei",
            "Botao Yu",
            "Ruibin Yuan",
            "Renliang Sun",
            "Ming Yin",
            "Boyuan Zheng",
            "Zhenzhu Yang",
            "Yibo Liu",
            "Wenhao Huang",
            "Huan Sun",
            "Yu Su",
            "Wenhu Chen"
        ],
        "citations": 462,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",
        "abstract": "Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them. Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities. This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.",
        "authors": [
            "Yuheng Cheng",
            "Ceyao Zhang",
            "Zhengwen Zhang",
            "Xiangrui Meng",
            "Sirui Hong",
            "Wenhao Li",
            "Zihao Wang",
            "Zekai Wang",
            "Feng Yin",
            "Junhua Zhao",
            "Xiuqiang He"
        ],
        "citations": 46,
        "references": 0,
        "year": 2024
    },
    {
        "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)",
        "abstract": "Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: https://cdn.openai.com/contributions/gpt-4v.pdf",
        "authors": [
            "Zhengyuan Yang",
            "Linjie Li",
            "Kevin Lin",
            "Jianfeng Wang",
            "Chung-Ching Lin",
            "Zicheng Liu",
            "Lijuan Wang"
        ],
        "citations": 478,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks",
        "abstract": "A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEIT-3, which achieves excellent transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We use Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked “language” modeling on images (Imglish), texts (English), and image-text pairs (“parallel sentences”) in a unified manner. Experimental results show that BEIT-3 obtains remarkable performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).",
        "authors": [
            "Wen Wang",
            "Hangbo Bao",
            "Li Dong",
            "Johan Bjorck",
            "Zhiliang Peng",
            "Qiangbo Liu",
            "Kriti Aggarwal",
            "O. Mohammed",
            "Saksham Singhal",
            "Subhojit Som",
            "Furu Wei"
        ],
        "citations": 223,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Fast Segment Anything",
        "abstract": "The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.",
        "authors": [
            "Xu Zhao",
            "Wen-Yan Ding",
            "Yongqi An",
            "Yinglong Du",
            "Tao Yu",
            "Min Li",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "citations": 195,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "abstract": "Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.",
        "authors": [
            "Bin Lin",
            "Bin Zhu",
            "Yang Ye",
            "Munan Ning",
            "Peng Jin",
            "Li Yuan"
        ],
        "citations": 347,
        "references": 57,
        "year": 2023
    },
    {
        "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought",
        "abstract": "Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the\"Chain of Thoughts\"mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control. Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering. Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.",
        "authors": [
            "Yao Mu",
            "Qinglong Zhang",
            "Mengkang Hu",
            "Wen Wang",
            "Mingyu Ding",
            "Jun Jin",
            "Bin Wang",
            "Jifeng Dai",
            "Y. Qiao",
            "Ping Luo"
        ],
        "citations": 162,
        "references": 73,
        "year": 2023
    },
    {
        "title": "mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
        "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily fo-cus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collab-oration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modal-ity collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experi-ments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.",
        "authors": [
            "Qinghao Ye",
            "Haiyang Xu",
            "Jiabo Ye",
            "Mingshi Yan",
            "Anwen Hu",
            "Haowei Liu",
            "Qi Qian",
            "Ji Zhang",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "citations": 292,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging",
        "abstract": "The segment anything model (SAM) was released as a foundation model for image segmentation. The promptable segmentation model was trained by over 1 billion masks on 11M licensed and privacy-respecting images. The model supports zero-shot image segmentation with various segmentation prompts (e.g., points, boxes, masks). It makes the SAM attractive for medical image analysis, especially for digital pathology where the training data are rare. In this study, we evaluate the zero-shot segmentation performance of SAM model on representative segmentation tasks on whole slide imaging (WSI), including (1) tumor segmentation, (2) non-tumor tissue segmentation, (3) cell nuclei segmentation. Core Results: The results suggest that the zero-shot SAM model achieves remarkable segmentation performance for large connected objects. However, it does not consistently achieve satisfying performance for dense instance object segmentation, even with 20 prompts (clicks/boxes) on each image. We also summarized the identified limitations for digital pathology: (1) image resolution, (2) multiple scales, (3) prompt selection, and (4) model fine-tuning. In the future, the few-shot fine-tuning with images from downstream pathological segmentation tasks might help the model to achieve better performance in dense object segmentation.",
        "authors": [
            "Ruining Deng",
            "C. Cui",
            "Quan Liu",
            "Tianyuan Yao",
            "Lucas W. Remedios",
            "Shunxing Bao",
            "Bennett A. Landman",
            "L. Wheless",
            "Lori A. Coburn",
            "K. Wilson",
            "Yaohong Wang",
            "Shilin Zhao",
            "A. Fogo",
            "Haichun Yang",
            "Yucheng Tang",
            "Yuankai Huo"
        ],
        "citations": 170,
        "references": 14,
        "year": 2023
    },
    {
        "title": "ACCESS: Advancing Innovation: NSF’s Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support",
        "abstract": "As the National Science Foundation evolves its investments in cyberinfrastructure, it has made a significant investment in the ACCESS (Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support) program instantiating a novel set of services along with a novel governance and management model. Research cyberinfrastructure (CI) is a key catalyst for discovery and innovation and plays a critical role in ensuring U.S. leadership in science and engineering, economic competitiveness, and national security, consistent with NSF’s mission. Funding of a set of awards through the ACCESS program has established a suite of CI coordination services targeted at supporting a broad and diverse set of requirements, researchers, and usage modalities spanning all areas of science and engineering research and education complemented by support for the collective and coordinated operation of the overall ACCESS program.",
        "authors": [
            "Timothy J. Boerner",
            "Stephen Deems",
            "T. Furlani",
            "Shelley Knuth",
            "John Towns"
        ],
        "citations": 168,
        "references": 12,
        "year": 2023
    },
    {
        "title": "UV-SAM: Adapting Segment Anything Model for Urban Village Identification",
        "abstract": "Urban villages, defined as informal residential areas in or around urban centers, are characterized by inadequate infrastructures and poor living conditions, closely related to the Sustainable Development Goals (SDGs) on poverty, adequate housing, and sustainable cities. Traditionally, governments heavily depend on field survey methods to monitor the urban villages, which however are time-consuming, labor-intensive, and possibly delayed. Thanks to widely available and timely updated satellite images, recent studies develop computer vision techniques to detect urban villages efficiently. However, existing studies either focus on simple urban village image classification or fail to provide accurate boundary information. To accurately identify urban village boundaries from satellite images, we harness the power of the vision foundation model and adapt the Segment Anything Model (SAM) to urban village segmentation, named UV-SAM. Specifically, UV-SAM first leverages a small-sized semantic segmentation model to produce mixed prompts for urban villages, including mask, bounding box, and image representations, which are then fed into SAM for fine-grained boundary identification. Extensive experimental results on two datasets in China demonstrate that UV-SAM outperforms existing baselines, and identification results over multiple years show that both the number and area of urban villages are decreasing over time, providing deeper insights into the development trends of urban villages and sheds light on the vision foundation models for sustainable cities. The dataset and codes of this study are available at https://github.com/tsinghua-fib-lab/UV-SAM.",
        "authors": [
            "Xin Zhang",
            "Yu Liu",
            "Yuming Lin",
            "Qingmin Liao",
            "Yong Li"
        ],
        "citations": 22,
        "references": 31,
        "year": 2024
    },
    {
        "title": "High-Fidelity Audio Compression with Improved RVQGAN",
        "abstract": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.",
        "authors": [
            "Rithesh Kumar",
            "Prem Seetharaman",
            "Alejandro Luebs",
            "I. Kumar",
            "Kundan Kumar"
        ],
        "citations": 210,
        "references": 47,
        "year": 2023
    },
    {
        "title": "EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters",
        "abstract": "Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models. We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin. Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the potential of EVA-style weak-to-strong visual model scaling. With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models.",
        "authors": [
            "Quan Sun",
            "Jinsheng Wang",
            "Qiying Yu",
            "Yufeng Cui",
            "Fan Zhang",
            "Xiaosong Zhang",
            "Xinlong Wang"
        ],
        "citations": 27,
        "references": 76,
        "year": 2024
    },
    {
        "title": "A Survey on Visual Mamba",
        "abstract": "State space models (SSM) with selection mechanisms and hardware-aware architectures, namely Mamba, have recently shown significant potential in long-sequence modeling. Since the complexity of transformers’ self-attention mechanism is quadratic with image size, as well as increasing computational demands, researchers are currently exploring how to adapt Mamba for computer vision tasks. This paper is the first comprehensive survey that aims to provide an in-depth analysis of Mamba models within the domain of computer vision. It begins by exploring the foundational concepts contributing to Mamba’s success, including the SSM framework, selection mechanisms, and hardware-aware design. Then, we review these vision Mamba models by categorizing them into foundational models and those enhanced with techniques including convolution, recurrence, and attention to improve their sophistication. Furthermore, we investigate the widespread applications of Mamba in vision tasks, which include their use as a backbone in various levels of vision processing. This encompasses general visual tasks, medical visual tasks (e.g., 2D/3D segmentation, classification, image registration, etc.), and remote sensing visual tasks. In particular, we introduce general visual tasks from two levels: high/mid-level vision (e.g., object detection, segmentation, video classification, etc.) and low-level vision (e.g., image super-resolution, image restoration, visual generation, etc.). We hope this endeavor will spark additional interest within the community to address current challenges and further apply Mamba models in computer vision.",
        "authors": [
            "Hanwei Zhang",
            "Ying Zhu",
            "Dan Wang",
            "Lijun Zhang",
            "Tianxiang Chen",
            "Zi Ye"
        ],
        "citations": 29,
        "references": 101,
        "year": 2024
    },
    {
        "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners",
        "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io",
        "authors": [
            "Allen Z. Ren",
            "Anushri Dixit",
            "Alexandra Bodrova",
            "Sumeet Singh",
            "Stephen Tu",
            "Noah Brown",
            "Peng Xu",
            "L. Takayama",
            "F. Xia",
            "Jacob Varley",
            "Zhenjia Xu",
            "Dorsa Sadigh",
            "Andy Zeng",
            "Anirudha Majumdar"
        ],
        "citations": 175,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
        "abstract": "A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked\"language\"modeling on images (Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).",
        "authors": [
            "Wenhui Wang",
            "Hangbo Bao",
            "Li Dong",
            "Johan Bjorck",
            "Zhiliang Peng",
            "Qiang Liu",
            "Kriti Aggarwal",
            "O. Mohammed",
            "Saksham Singhal",
            "S. Som",
            "Furu Wei"
        ],
        "citations": 584,
        "references": 75,
        "year": 2022
    },
    {
        "title": "Bayesian Optimization",
        "abstract": "Bayesian optimization is a methodology for optimizing expensive objective functions that has proven success in the sciences, engineering, and beyond. This timely text provides a self-contained and comprehensive introduction to the subject, starting from scratch and carefully developing all the key ideas along the way. This bottom-up approach illuminates unifying themes in the design of Bayesian optimization algorithms and builds a solid theoretical foundation for approaching novel situations. The core of the book is divided into three main parts, covering theoretical and practical aspects of Gaussian process modeling, the Bayesian approach to sequential decision making, and the realization and computation of practical and effective optimization policies. Following this foundational material, the book provides an overview of theoretical convergence results, a survey of notable extensions, a comprehensive history of Bayesian optimization, and an extensive annotated bibliography of applications.",
        "authors": [
            "R. Garnett"
        ],
        "citations": 123,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Generative Pretraining in Multimodality",
        "abstract": "We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.",
        "authors": [
            "Quan Sun",
            "Qiying Yu",
            "Yufeng Cui",
            "Fan Zhang",
            "Xiaosong Zhang",
            "Yueze Wang",
            "Hongcheng Gao",
            "Jingjing Liu",
            "Tiejun Huang",
            "Xinlong Wang"
        ],
        "citations": 110,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Accuracy of Segment-Anything Model (SAM) in medical image segmentation tasks",
        "abstract": "— The segment-anything model (SAM), was introduced as a fundamental model for segmenting images. It was trained using over 1 billion masks from 11 million natural images. The model can perform zero-shot segmentation of images by using various prompts such as masks, boxes, and points. In this report, we explored (1) the accuracy of SAM on 12 public medical image segmentation datasets which cover various organs (brain, breast, chest, lung, skin, liver, bowel, pancreas, and prostate), image modalities (2D X-ray, histology, endoscropy, and 3D MRI and CT), and health conditions (normal, lesioned). (2) if the computer vision foundational segmentation model SAM can provide promising research directions for medical image segmentation. We found that SAM without re-training on medical images do not perform as accurately as U-Net or other deep learning models trained on medical images.",
        "authors": [
            "Sheng He",
            "Rina Bao",
            "Jingpeng Li",
            "P. Grant",
            "Yangming Ou"
        ],
        "citations": 133,
        "references": 27,
        "year": 2023
    },
    {
        "title": "AlignSAM: Aligning Segment Anything Model to Open Context via Reinforcement Learning",
        "abstract": "Powered by massive curated training data, Segment Any-thing Model (SAM) has demonstrated its impressive generalization capabilities in open-world scenarios with the guidance of prompts. However, the vanilla SAM is class-agnostic and heavily relies on user-provided prompts to segment objects of interest. Adapting this method to diverse tasks is crucial for accurate target identification and to avoid suboptimal segmentation results. In this paper, we propose a novel framework, termed AlignSAM, designed for automatic prompting for aligning SAM to an open context through reinforcement learning. Anchored by an agent, AlignSAM enables the generality of the SAM model across diverse downstream tasks while keeping its parameters frozen. Specifically, AlignSAM initiates a prompting agent to iteratively refine segmentation predictions by interacting with the foundational model. It integrates a reinforcement learning policy network to provide informative prompts to the foundational models. Additionally, a semantic recal-ibration module is introduced to provide fine-grained labels of prompts, enhancing the model's proficiency in handling tasks encompassing explicit and implicit semantics. Experiments conducted on various challenging segmentation tasks among existing foundation models demonstrate the superiority of the proposed AlignSAM over state-of-the-art approaches. Project page: https://github.com/Duojun-Huang/AIignSAM-CVPR2024.",
        "authors": [
            "Duojun Huang",
            "Xinyu Xiong",
            "Jie Ma",
            "Jichang Li",
            "Zequn Jie",
            "Lin Ma",
            "Guanbin Li"
        ],
        "citations": 10,
        "references": 71,
        "year": 2024
    },
    {
        "title": "Self-regulating Prompts: Foundational Model Adaptation without Forgetting",
        "abstract": "Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks. Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP. This leads to the loss of the model’s original generalization capability. To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating prompted representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch. To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity. PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization. We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods. Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC.",
        "authors": [
            "Muhammad Uzair Khattak",
            "Syed Talal Wasim",
            "Muzammal Naseer",
            "Salman Siddique Khan",
            "Ming Yang",
            "F. Khan"
        ],
        "citations": 104,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions",
        "abstract": "Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks. While there has been a growing interest in Auto-GPT stypled agents, questions remain regarding the effectiveness and flexibility of Auto-GPT in solving real-world decision-making tasks. Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties. In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate real-world scenarios. Our aim is to gain deeper insights into this problem and understand the adaptability of GPT-based agents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme. This approach enables lightweight supervised learning without requiring fine-tuning of the foundational LLMs. We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.",
        "authors": [
            "Hui Yang",
            "Sifu Yue",
            "Yunzhong He"
        ],
        "citations": 101,
        "references": 30,
        "year": 2023
    },
    {
        "title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects",
        "abstract": "We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without finetuning, as long as its CAD model is given, or a small number of reference images are captured. Thanks to the unified framework, the downstream pose estimation modules are the same in both setups, with a neural implicit representation used for efficient novel view synthesis when no CAD model is available. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions. Project page: https://nvlabs.github.io/FoundationPose/",
        "authors": [
            "Bowen Wen",
            "Wei Yang",
            "Jan Kautz",
            "Stanley T. Birchfield"
        ],
        "citations": 85,
        "references": 79,
        "year": 2023
    },
    {
        "title": "SAM on Medical Images: A Comprehensive Study on Three Prompt Modes",
        "abstract": "The Segment Anything Model (SAM) made an eye-catching debut recently and inspired many researchers to explore its potential and limitation in terms of zero-shot generalization capability. As the first promptable foundation model for segmentation tasks, it was trained on a large dataset with an unprecedented number of images and annotations. This large-scale dataset and its promptable nature endow the model with strong zero-shot generalization. Although the SAM has shown competitive performance on several datasets, we still want to investigate its zero-shot generalization on medical images. As we know, the acquisition of medical image annotation usually requires a lot of effort from professional practitioners. Therefore, if there exists a foundation model that can give high-quality mask prediction simply based on a few point prompts, this model will undoubtedly become the game changer for medical image analysis. To evaluate whether SAM has the potential to become the foundation model for medical image segmentation tasks, we collected more than 12 public medical image datasets that cover various organs and modalities. We also explore what kind of prompt can lead to the best zero-shot performance with different modalities. Furthermore, we find that a pattern shows that the perturbation of the box size will significantly change the prediction accuracy. Finally, Extensive experiments show that the predicted mask quality varied a lot among different datasets. And providing proper prompts, such as bounding boxes, to the SAM will significantly increase its performance.",
        "authors": [
            "D. Cheng",
            "Ziyuan Qin",
            "Zekun Jiang",
            "Shaoting Zhang",
            "Qicheng Lao",
            "Kang Li"
        ],
        "citations": 90,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping",
        "abstract": "Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, Segment Anything Model (SAM), and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks.",
        "authors": [
            "Chunming He",
            "Kai Li",
            "Yachao Zhang",
            "Guoxia Xu",
            "Longxiang Tang",
            "Yulun Zhang",
            "Z. Guo",
            "Xiu Li"
        ],
        "citations": 71,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
        "abstract": "Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1. See \\url{https://github.com/baoguangsheng/fast-detect-gpt} for code, data, and results.",
        "authors": [
            "Guangsheng Bao",
            "Yanbin Zhao",
            "Zhiyang Teng",
            "Linyi Yang",
            "Yue Zhang"
        ],
        "citations": 92,
        "references": 60,
        "year": 2023
    },
    {
        "title": "TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting",
        "abstract": "Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their memory and compute-intensive requirements pose a critical bottleneck for long-term forecasting, despite numerous advancements in compute-aware self-attention modules. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).",
        "authors": [
            "Vijayabharathi Ekambaram",
            "Arindam Jati",
            "Nam H. Nguyen",
            "Phanwadee Sinthong",
            "J. Kalagnanam"
        ],
        "citations": 88,
        "references": 43,
        "year": 2023
    },
    {
        "title": "AnyLoc: Towards Universal Visual Place Recognition",
        "abstract": "Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are <italic>environment- and task-specific:</italic> while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a <italic>universal</italic> solution to VPR – a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or finetuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models <italic>with no VPR-specific training</italic> are the right substrate upon which to build such a universal VPR solution. Combining these derived features with <italic>unsupervised feature aggregation</italic> enables our suite of methods, <italic>AnyLoc</italic>, to achieve up to <inline-formula><tex-math notation=\"LaTeX\">$4\\times$</tex-math></inline-formula> significantly higher performance than existing approaches. We further obtain a 6% improvement in performance by characterizing the semantic properties of these features, uncovering unique <italic>domains</italic> which encapsulate datasets from similar environments. Our detailed experiments and analysis lay a foundation for building VPR solutions that may be deployed <italic>anywhere</italic>, <italic>anytime</italic>, and across <italic>anyview</italic>.",
        "authors": [
            "Nikhil Varma Keetha",
            "Avneesh Mishra",
            "Jay Karhade",
            "Krishna Murthy Jatavallabhula",
            "S. Scherer",
            "M. Krishna",
            "Sourav Garg"
        ],
        "citations": 79,
        "references": 61,
        "year": 2023
    },
    {
        "title": "GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields",
        "abstract": "It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\\textbf{G}$eneralizable $\\textbf{N}$eural feature $\\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of demonstrations. We observe a substantial improvement of GNFactor over current state-of-the-art methods in seen and unseen tasks, demonstrating the strong generalization ability of GNFactor. Our project website is https://yanjieze.com/GNFactor/ .",
        "authors": [
            "Yanjie Ze",
            "Ge Yan",
            "Yueh-Hua Wu",
            "Annabella Macaluso",
            "Yuying Ge",
            "Jianglong Ye",
            "Nicklas Hansen",
            "Li Erran Li",
            "X. Wang"
        ],
        "citations": 63,
        "references": 56,
        "year": 2023
    },
    {
        "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting",
        "abstract": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.",
        "authors": [
            "Defu Cao",
            "Furong Jia",
            "Sercan Ö. Arik",
            "Tomas Pfister",
            "Yixiang Zheng",
            "Wen Ye",
            "Yan Liu"
        ],
        "citations": 73,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Distance-based support vector machine to predict DNA N6-methyladenine modification",
        "abstract": "\n\nDNA N6-methyladenine plays an important role in the restriction-modification system to isolate invasion from adventive DNA. The shortcomings of the high time-consumption and high costs of experimental methods have been exposed, and some computational methods have emerged. The support vector machine theory has received extensive attention in the bioinformatics field due to its solid theoretical foundation and many good characteristics.\n\n\n\nGeneral machine learning methods include an important step of extracting features. The research has omitted this step and replaced with easy-to-obtain sequence distances matrix to obtain better results\n\n\n\nFirst sequence alignment technology was used to achieve the similarity matrix. Then a novel transformation turned the similarity matrix into a distance matrix. Next, the similarity-distance matrix is made positive semi-definite so that it can be used in the kernel matrix. Finally, the LIBSVM software was applied to solve the support vector machine.\n\n\n\nThe five-fold cross-validation of this model on rice and mouse data has achieved excellent accuracy rates of 92.04% and 96.51%, respectively. This shows that the DB-SVM method has obvious advantages compared with traditional machine learning methods. Meanwhile this model achieved 0.943,0.982 and 0.818 accuracy,0.944, 0.982, and 0.838 Matthews correlation coefficient and 0.942, 0.982 and 0.840 F1 scores for the rice, M. musculus and cross-species genome datasets, respectively.\n\n\n\nThese outcomes show that this model outperforms the iIM-CNN and csDMA in the prediction of DNA 6mA modification, which are the lastest research on DNA 6mA.\n",
        "authors": [
            "Haoyu Zhang",
            "Q. Zou",
            "Y. Ju",
            "Chenggang Song",
            "Dong Chen"
        ],
        "citations": 275,
        "references": 0,
        "year": 2022
    },
    {
        "title": "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation",
        "abstract": "Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first generalist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. 1) For the data scaling, we perform a systematic investigation on 32 EHPS datasets, including a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. 2) For the model scaling, we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into specialist models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF (62.3 mm PVE without finetuning). Homepage: https://caizhongang.github.io/projects/SMPLer-X/",
        "authors": [
            "Zhongang Cai",
            "Wanqi Yin",
            "Ailing Zeng",
            "Chen Wei",
            "Qingping Sun",
            "Yanjun Wang",
            "Hui En Pang",
            "Haiyi Mei",
            "Mingyuan Zhang",
            "Lei Zhang",
            "Chen Change Loy",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "citations": 52,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Tag2Text: Guiding Vision-Language Model via Image Tagging",
        "abstract": "This paper presents Tag2Text, a vision language pre-training (VLP) framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. In contrast to prior works which utilize object tags either manually labeled or automatically detected with an off-the-shelf detector with limited performance, our approach explicitly learns an image tagger using tags parsed from image-paired text and thus provides a strong semantic guidance to vision-language models. In this way, Tag2Text can utilize large-scale annotation-free image tags in accordance with image-text pairs, and provides more diverse tag categories beyond objects. As a result, Tag2Text demonstrates the ability of a foundational image tagging model, with superior zero-shot performance even comparable to fully supervised models. Moreover, by leveraging the tagging guidance, Tag2Text effectively enhances the performance of vision-language models on both generation-based and alignment-based tasks. Across a wide range of downstream benchmarks, Tag2Text achieves state-of-the-art results with similar model sizes and data scales, demonstrating the efficacy of the proposed tagging guidance. Code, demo and pre-trained models are available at https://github.com/xinyu1205/recognize-anything.",
        "authors": [
            "Xinyu Huang",
            "Youcai Zhang",
            "Jinyu Ma",
            "Weiwei Tian",
            "Rui Feng",
            "Yuejie Zhang",
            "Yaqian Li",
            "Yandong Guo",
            "Lei Zhang"
        ],
        "citations": 60,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Performance of ChatGPT on the US Fundamentals of Engineering Exam: Comprehensive Assessment of Proficiency and Potential Implications for Professional Environmental Engineering Practice",
        "abstract": "In recent years, advancements in artificial intelligence (AI) have led to the development of large language models like GPT-4, demonstrating potential applications in various fields, including education. This study investigates the feasibility and effectiveness of using ChatGPT, a GPT-4 based model, in achieving satisfactory performance on the Fundamentals of Engineering (FE) Environmental Exam. This study further shows a significant improvement in the model's accuracy when answering FE exam questions through noninvasive prompt modifications, substantiating the utility of prompt modification as a viable approach to enhance AI performance in educational contexts. Furthermore, the findings reflect remarkable improvements in mathematical capabilities across successive iterations of ChatGPT models, showcasing their potential in solving complex engineering problems. Our paper also explores future research directions, emphasizing the importance of addressing AI challenges in education, enhancing accessibility and inclusion for diverse student populations, and developing AI-resistant exam questions to maintain examination integrity. By evaluating the performance of ChatGPT in the context of the FE Environmental Exam, this study contributes valuable insights into the potential applications and limitations of large language models in educational settings. As AI continues to evolve, these findings offer a foundation for further research into the responsible and effective integration of AI models across various disciplines, ultimately optimizing the learning experience and improving student outcomes.",
        "authors": [
            "Vinay Pursnani",
            "Y. Sermet",
            "I. Demir"
        ],
        "citations": 60,
        "references": 35,
        "year": 2023
    },
    {
        "title": "How to Efficiently Adapt Large Segmentation Model(SAM) to Medical Images",
        "abstract": "The emerging scale segmentation model, Segment Anything (SAM), exhibits impressive capabilities in zero-shot segmentation for natural images. However, when applied to medical images, SAM suffers from noticeable performance drop. To make SAM a real ``foundation model\"for the computer vision community, it is critical to find an efficient way to customize SAM for medical image dataset. In this work, we propose to freeze SAM encoder and finetune a lightweight task-specific prediction head, as most of weights in SAM are contributed by the encoder. In addition, SAM is a promptable model, while prompt is not necessarily available in all application cases, and precise prompts for multiple class segmentation are also time-consuming. Therefore, we explore three types of prompt-free prediction heads in this work, include ViT, CNN, and linear layers. For ViT head, we remove the prompt tokens in the mask decoder of SAM, which is named AutoSAM. AutoSAM can also generate masks for different classes with one single inference after modification. To evaluate the label-efficiency of our finetuning method, we compare the results of these three prediction heads on a public medical image segmentation dataset with limited labeled data. Experiments demonstrate that finetuning SAM significantly improves its performance on medical image dataset, even with just one labeled volume. Moreover, AutoSAM and CNN prediction head also has better segmentation accuracy than training from scratch and self-supervised learning approaches when there is a shortage of annotations.",
        "authors": [
            "Xinrong Hu",
            "Xiaowei Xu",
            "Yi Shi"
        ],
        "citations": 49,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Kosmos-2.5: A Multimodal Literate Model",
        "abstract": "The automatic reading of text-intensive images represents a significant advancement toward achieving Artificial General Intelligence (AGI). In this paper we present KOSMOS-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on a large-scale corpus of text-intensive images, KOSMOS-2.5 excels in two distinct yet complementary transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned spatial coordinates within the image, and (2) producing structured text output that captures both style and structure in markdown format. This unified multimodal literate capability is achieved through a shared decoder-only autoregressive Transformer architecture and task-specific prompts. Building on this foundation, we fine-tune KOSMOS-2.5 for document understanding tasks, resulting in a document understanding generalist named KOSMOS-2.5-CHAT. Additionally, a large corpus of 357.4 million document pages spanning diverse domains was curated for pre-training. We evaluate KOSMOS-2.5 on two newly proposed benchmarks, OCREval and MarkdownEval, for document-level text recognition and image-to-markdown generation, demonstrating impressive literate capabilities comparable to GPT-4o. KOSMOS-2.5-CHAT achieves performance comparable to other state-of-the-art generalists that are five times larger (1.3B vs. 7B) across nine text-rich visual question answering benchmarks. Models and code have been available at \\url{https://aka.ms/kosmos25}.",
        "authors": [
            "Tengchao Lv",
            "Yupan Huang",
            "Jingye Chen",
            "Lei Cui",
            "Shuming Ma",
            "Ya-Chi Chang",
            "Shaohan Huang",
            "Wenhui Wang",
            "Li Dong",
            "Weiyao Luo",
            "Shaoxiang Wu",
            "Guoxin Wang",
            "Cha Zhang",
            "Furu Wei"
        ],
        "citations": 49,
        "references": 128,
        "year": 2023
    },
    {
        "title": "Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation",
        "abstract": "Weakly supervised semantic segmentation (WSSS) aims to bypass the need for laborious pixel-level annotation by using only image-level annotation. Most existing methods rely on Class Activation Maps (CAM) to derive pixel-level pseudo-labels and use them to train a fully supervised semantic segmentation model. Although these pseudo-labels are class-aware, indicating the coarse regions for particular classes, they are not object-aware and fail to delineate accurate object boundaries. To address this, we introduce a simple yet effective method harnessing the Segment Anything Model (SAM), a class-agnostic foundation model capable of producing fine-grained instance masks of objects, parts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM masks, resulting in high-quality pseudo-labels that are both class-aware and object-aware. Our approach is highly versatile and can be easily integrated into existing WSSS methods without any modification. Despite its simplicity, our approach shows consistent gain over the state-of-the-art WSSS methods on both PASCAL VOC and MS-COCO datasets.",
        "authors": [
            "Tianle Chen",
            "Zheda Mai",
            "Ruiwen Li",
            "Wei-Lun Chao"
        ],
        "citations": 43,
        "references": 52,
        "year": 2023
    },
    {
        "title": "The role of artificial intelligence in consumers’ brand preference for retail banks in Hong Kong",
        "abstract": null,
        "authors": [
            "Shirie Pui Shan Ho",
            "Matthew Yau Choi Chow"
        ],
        "citations": 36,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected",
        "abstract": "Meta AI Research has recently released SAM (Segment Anything Model) which is trained on a large segmentation dataset of over 1 billion masks. As a foundation model in the field of computer vision, SAM (Segment Anything Model) has gained attention for its impressive performance in generic object segmentation. Despite its strong capability in a wide range of zero-shot transfer tasks, it remains unknown whether SAM can detect things in challenging setups like transparent objects. In this work, we perform an empirical evaluation of two glass-related challenging scenarios: mirror and transparent objects. We found that SAM often fails to detect the glass in both scenarios, which raises concern for deploying the SAM in safety-critical situations that have various forms of glass.",
        "authors": [
            "Dongsheng Han",
            "Chaoning Zhang",
            "Yu Qiao",
            "Maryam Qamar",
            "Yuna Jung",
            "Seungkyu Lee",
            "S. Bae",
            "Choong-Seon Hong"
        ],
        "citations": 32,
        "references": 33,
        "year": 2023
    },
    {
        "title": "Ambient Diffusion: Learning Clean Distributions from Corrupted Data",
        "abstract": "We present the first diffusion-based framework that can learn an unknown distribution using only highly-corrupted samples. This problem arises in scientific applications where access to uncorrupted samples is impossible or expensive to acquire. Another benefit of our approach is the ability to train generative models that are less likely to memorize individual training samples since they never observe clean training data. Our main idea is to introduce additional measurement distortion during the diffusion process and require the model to predict the original corrupted image from the further corrupted image. We prove that our method leads to models that learn the conditional expectation of the full uncorrupted image given this additional measurement corruption. This holds for any corruption process that satisfies some technical conditions (and in particular includes inpainting and compressed sensing). We train models on standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn the distribution even when all the training samples have $90\\%$ of their pixels missing. We also show that we can finetune foundation models on small corrupted datasets (e.g. MRI scans with block corruptions) and learn the clean distribution without memorizing the training set.",
        "authors": [
            "Giannis Daras",
            "Kulin Shah",
            "Y. Dagan",
            "Aravind Gollakota",
            "A. Dimakis",
            "Adam R. Klivans"
        ],
        "citations": 45,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Denoising Diffusion Autoencoders are Unified Self-supervised Learners",
        "abstract": "Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations within its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for generative-and-discriminative dual learning. To validate this, we conduct linear probe and finetuning evaluations. Our diffusion-based approach achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to contrastive learning and masked autoencoders for the first time. Transfer learning from ImageNet also confirms the suitability of DDAE for Vision Transformers, suggesting the potential to scale DDAEs as unified foundation models. Code is available at github.com/FutureXiang/ddae.",
        "authors": [
            "Weilai Xiang",
            "Hongyu Yang",
            "Di Huang",
            "Yunhong Wang"
        ],
        "citations": 44,
        "references": 75,
        "year": 2023
    },
    {
        "title": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation",
        "abstract": "Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that SegVol outperforms the competitors in 19 tasks, with improvements up to 37.24% compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at: https://github.com/BAAI-DCAI/SegVol.",
        "authors": [
            "Yuxin Du",
            "Fan Bai",
            "Tiejun Huang",
            "Bo Zhao"
        ],
        "citations": 27,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Taken out of context: On measuring situational awareness in LLMs",
        "abstract": "We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: https://github.com/AsaCooperStickland/situational-awareness-evals.",
        "authors": [
            "Lukas Berglund",
            "Asa Cooper Stickland",
            "Mikita Balesni",
            "Max Kaufmann",
            "Meg Tong",
            "Tomasz Korbak",
            "Daniel Kokotajlo",
            "Owain Evans"
        ],
        "citations": 46,
        "references": 63,
        "year": 2023
    },
    {
        "title": "4M: Massively Multimodal Masked Modeling",
        "abstract": "Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens. 4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility. Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.",
        "authors": [
            "David Mizrahi",
            "Roman Bachmann",
            "Ouguzhan Fatih Kar",
            "Teresa Yeo",
            "Mingfei Gao",
            "Afshin Dehghan",
            "Amir Zamir"
        ],
        "citations": 38,
        "references": 133,
        "year": 2023
    },
    {
        "title": "Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare",
        "abstract": "Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.",
        "authors": [
            "Junling Liu",
            "Ziming Wang",
            "Qichen Ye",
            "Dading Chong",
            "Peilin Zhou",
            "Y. Hua"
        ],
        "citations": 36,
        "references": 31,
        "year": 2023
    },
    {
        "title": "GenRec: Large Language Model for Generative Recommendation",
        "abstract": "In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recommendation tasks. Subsequently, we use these prompts to fine-tune the LLaMA backbone LLM on a dataset of user-item interactions, represented by textual data, to capture user preferences and item characteristics. Our research underscores the potential of LLM-based generative recommendation in revolutionizing the domain of recommendation systems and offers a foundational framework for future explorations in this field. We conduct extensive experiments on benchmark datasets, and the experiments shows that our GenRec has significant better results on large dataset.",
        "authors": [
            "Jianchao Ji",
            "Zelong Li",
            "Shuyuan Xu",
            "Wenyue Hua",
            "Yingqiang Ge",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "citations": 36,
        "references": 19,
        "year": 2023
    },
    {
        "title": "DRG-LLaMA : tuning LLaMA model to predict diagnosis-related group for hospitalized patients",
        "abstract": "In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is pivotal, but its assignment process is inefficient. The study introduces DRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes to enhance DRGs assignment. Utilizing LLaMA as the foundational model and optimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries, our DRG-LLaMA -7B model exhibited a noteworthy macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0%, and a macro-averaged Area Under the Curve (AUC) of 0.986, with a maximum input token length of 512. This model surpassed the performance of prior leading models in DRG prediction, showing a relative improvement of 40.3% and 35.7% in macro-averaged F1 score compared to ClinicalBERT and CAML, respectively. Applied to base DRG and complication or comorbidity (CC)/major complication or comorbidity (MCC) prediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8% and 67.5%, respectively. Additionally, our findings indicate that DRG-LLaMA ’s performance correlates with increased model parameters and input context lengths.",
        "authors": [
            "Hanyin Wang",
            "Chufan Gao",
            "Christopher Dantona",
            "Bryan Hull",
            "Jimeng Sun"
        ],
        "citations": 35,
        "references": 54,
        "year": 2023
    },
    {
        "title": "A foundational vision transformer improves diagnostic performance for electrocardiograms",
        "abstract": null,
        "authors": [
            "A. Vaid",
            "Joy Jiang",
            "Ashwin S. Sawant",
            "S. Lerakis",
            "E. Argulian",
            "Yuri Ahuja",
            "J. Lampert",
            "A. Charney",
            "H. Greenspan",
            "J. Narula",
            "Benjamin S. Glicksberg",
            "G. Nadkarni"
        ],
        "citations": 32,
        "references": 14,
        "year": 2023
    },
    {
        "title": "Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation",
        "abstract": "Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personalized prompt generation globally. The former aims to train visual prompts that adapt foundation models to each client, while the latter observes local optimization directions to generate personalized prompts for all clients. Through extensive experiments on benchmark datasets, we show that our pFedPG is favorable against state-of-the-art personalized FL methods under various types of data heterogeneity, allowing computation and communication efficient model personalization.",
        "authors": [
            "Fu-En Yang",
            "Chien-Yi Wang",
            "Yu-Chiang Frank Wang"
        ],
        "citations": 32,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Customer Relationship Management in an Era of Digital Disruption and Business Model Evolution",
        "abstract": "This research is grounded on the conceptualization of CRM as a collection of technology solutions that play a pivotal role in effective company management. The current work aims to elucidate and establish the significance of the advantages associated with CRM, as previously emphasized in relevant literature, in relation to achieving business success. Existing sustainable studies are of special importance in this setting because they provide a model of practical inquiry into the positive effects of CRM practices across the 3D of sustainability (environmental, social, and economic). Once our predictions are confirmed, the following model`s validation will enhance our comprehension of how CRM-related advantages might amplify the beneficial influence of its conditions on each sustainability dimension. CRM may be seen as a kind of Green IT, focused on facilitating modern transformation and promoting the development of sustainable business models. This research model has the potential to serve as a foundational framework for a more targeted approach aimed at quantifying the effects and advantages of using CRM. We argue that this encompasses models of sustainable innovation and business.",
        "authors": [
            "Kazuo Adachi",
            "Ryo Sato"
        ],
        "citations": 27,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding",
        "abstract": "Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human and computer vision through the Brain-Computer Interface. However, reconstructing high-quality images with correct semantics from brain recordings is a challenging problem due to the complex underlying representations of brain signals and the scarcity of data annotations. In this work, we present MinD-Vis: Sparse Masked Brain Modeling with Double-Conditioned Latent Diffusion Model for Human Vision Decoding. Firstly, we learn an effective self-supervised representation of fMRI data using mask modeling in a large latent space inspired by the sparse coding of information in the primary visual cortex. Then by augmenting a latent diffusion model with double-conditioning, we show that MinD-Vis can reconstruct highly plausible images with semantically matching details from brain recordings using very few paired annotations. We benchmarked our model qualitatively and quantitatively; the experimental results indicate that our method outperformed state-of-the-art in both semantic mapping (100-way semantic classification) and generation quality (FID) by 66% and 41% respectively. An exhaustive ablation study was also conducted to analyze our framework.",
        "authors": [
            "Zijiao Chen",
            "Jiaxin Qing",
            "Tiange Xiang",
            "Wan Lin Yue",
            "J. Zhou"
        ],
        "citations": 124,
        "references": 72,
        "year": 2022
    },
    {
        "title": "An Empirical Study on the Robustness of the Segment Anything Model (SAM)",
        "abstract": "The Segment Anything Model (SAM) is a foundation model for general image segmentation. Although it exhibits impressive performance predominantly on natural images, understanding its robustness against various image perturbations and domains is critical for real-world applications where such challenges frequently arise. In this study we conduct a comprehensive robustness investigation of SAM under diverse real-world conditions. Our experiments encompass a wide range of image perturbations. Our experimental results demonstrate that SAM's performance generally declines under perturbed images, with varying degrees of vulnerability across different perturbations. By customizing prompting techniques and leveraging domain knowledge based on the unique characteristics of each dataset, the model's resilience to these perturbations can be enhanced, addressing dataset-specific challenges. This work sheds light on the limitations and strengths of SAM in real-world applications, promoting the development of more robust and versatile image segmentation solutions.",
        "authors": [
            "Yuqing Wang",
            "Yun Zhao",
            "Linda Petzold"
        ],
        "citations": 19,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Noninvasive proteomic biomarkers for alcohol-related liver disease",
        "abstract": null,
        "authors": [
            "L. Niu",
            "M. Thiele",
            "Philipp E. Geyer",
            "D. Rasmussen",
            "H. Webel",
            "Alberto Santos",
            "Rajat Gupta",
            "F. Meier",
            "Maximilian T. Strauss",
            "M. Kjaergaard",
            "K. Lindvig",
            "Suganya Jacobsen",
            "Simon Rasmussen",
            "T. Hansen",
            "A. Krag",
            "Matthias Mann"
        ],
        "citations": 137,
        "references": 65,
        "year": 2022
    },
    {
        "title": "Scalable querying of human cell atlases via a foundational model reveals commonalities across fibrosis-associated macrophages",
        "abstract": "Single-cell RNA-seq (scRNA-seq) studies have profiled over 100 million human cells across diseases, developmental stages, and perturbations to date. A singular view of this vast and growing expression landscape could help reveal novel associations between cell states and diseases, discover cell states in unexpected tissue contexts, and relate in vivo cells to in vitro models. However, these require a common, scalable representation of cell profiles from across the body, a general measure of their similarity, and an efficient way to query these data. Here, we present SCimilarity, a metric learning framework to learn and search a unified and interpretable representation that annotates cell types and instantaneously queries for a cell state across tens of millions of profiles. We demonstrate SCimilarity on a 22.7 million cell corpus assembled across 399 published scRNA-seq studies, showing accurate integration, annotation and querying. We experimentally validated SCimilarity by querying across tissues for a macrophage subset originally identified in interstitial lung disease, and showing that cells with similar profiles are found in other fibrotic diseases, tissues, and a 3D hydrogel system, which we then repurposed to yield this cell state in vitro. SCimilarity serves as a foundational model for single cell gene expression data and enables researchers to query for similar cellular states across the entire human body, providing a powerful tool for generating novel biological insights from the growing Human Cell Atlas.",
        "authors": [
            "Graham S. Heimberg",
            "Tony Kuo",
            "D. DePianto",
            "Tobias Heigl",
            "N. Diamant",
            "Omar Salem",
            "Gabriele Scalia",
            "Tommaso Biancalani",
            "S. Turley",
            "Jason Rock",
            "H. C. Bravo",
            "J. Kaminker",
            "J. V. Heiden",
            "A. Regev"
        ],
        "citations": 21,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Prediction of Pile Bearing Capacity Using XGBoost Algorithm: Modeling and Performance Evaluation",
        "abstract": "The major criteria that control pile foundation design is pile bearing capacity (Pu). The load bearing capacity of piles is affected by the various characteristics of soils and the involvement of multiple parameters related to both soil and foundation. In this study, a new model for predicting bearing capacity is developed using an extreme gradient boosting (XGBoost) algorithm. A total of 200 driven piles static load test-based case histories were used to construct and verify the model. The developed XGBoost model results were compared to a number of commonly used algorithms—Adaptive Boosting (AdaBoost), Random Forest (RF), Decision Tree (DT) and Support Vector Machine (SVM) using various performance measure metrics such as coefficient of determination, mean absolute error, root mean square error, mean absolute relative error, Nash–Sutcliffe model efficiency coefficient and relative strength ratio. Furthermore, sensitivity analysis was performed to determine the effect of input parameters on Pu. The results show that all of the developed models were capable of making accurate predictions however the XGBoost algorithm surpasses others, followed by AdaBoost, RF, DT, and SVM. The sensitivity analysis result shows that the SPT blow count along the pile shaft has the greatest effect on the Pu.",
        "authors": [
            "Maaz Amjad",
            "I. Ahmad",
            "Mahmood Ahmad",
            "P. Wróblewski",
            "P. Kamiński",
            "Uzair Amjad"
        ],
        "citations": 112,
        "references": 76,
        "year": 2022
    },
    {
        "title": "PLACES: Local Data for Better Health",
        "abstract": "Local-level data on the health of populations are important to inform and drive effective and efficient actions to improve health, but such data are often expensive to collect and thus rare. Population Level Analysis and Community EStimates (PLACES) (www.cdc.gov/places/), a collaboration between the Centers for Disease Control and Prevention (CDC), the Robert Wood Johnson Foundation, and the CDC Foundation, provides model-based estimates for 29 measures among all counties and most incorporated and census-designated places, census tracts, and ZIP Code tabulation areas across the US. PLACES allows local health departments and others to better understand the burden and geographic distribution of chronic disease–related outcomes in their areas regardless of population size and urban–rural status and assists them in planning public health interventions. Online resources allow users to visually explore health estimates geographically, compare estimates, and download data for further use and exploration. By understanding the PLACES overall approach and using the easy-to-use PLACES applications, practitioners, policy makers, and others can enhance their efforts to improve public health, including informing prevention activities, programs, and policies; identifying priority health risk behaviors for action; prioritizing investments to areas with the biggest gaps or inequities; and establishing key health objectives to achieve community health and health equity.",
        "authors": [
            "K. Greenlund",
            "Hua Lu",
            "Yan Wang",
            "Kevin A. Matthews",
            "Jennifer LeClercq",
            "Benjamin Lee",
            "S. Carlson"
        ],
        "citations": 104,
        "references": 34,
        "year": 2022
    },
    {
        "title": "Digital business model innovation: toward construct clarity and future research directions",
        "abstract": null,
        "authors": [
            "M. Trischler",
            "J. Li‐Ying"
        ],
        "citations": 100,
        "references": 136,
        "year": 2022
    },
    {
        "title": "A Foundational Multimodal Vision Language AI Assistant for Human Pathology",
        "abstract": "The field of computational pathology has witnessed remarkable progress in the development of both task-specific predictive models and task-agnostic self-supervised vision encoders. However, despite the explosive growth of generative artificial intelligence (AI), there has been limited study on building general purpose, multimodal AI assistants tailored to pathology. Here we present PathChat, a vision-language generalist AI assistant for human pathology using an in-house developed foundational vision encoder pretrained on 100 million histology images from over 100,000 patient cases and 1.18 million pathology image-caption pairs. The vision encoder is then combined with a pretrained large language model and the whole system is finetuned on over 250,000 diverse disease agnostic visual language instructions. We compare PathChat against several multimodal vision language AI assistants as well as GPT4V, which powers the commercially available multimodal general purpose AI assistant ChatGPT-4. When relevant clinical context is provided with the histology image, PathChat achieved a diagnostic accuracy of 87% on multiple-choice questions based on publicly available cases of diverse tissue origins and disease models. Additionally, using open-ended questions and human expert evaluation, we found that overall PathChat produced more accurate and pathologist-preferable responses to diverse queries related to pathology. As an interactive and general vision language AI assistant that can flexibly handle both visual and natural language inputs, PathChat can potentially find impactful applications in pathology education, research, and human-in-the-loop clinical decision making.",
        "authors": [
            "Ming Y. Lu",
            "Bowen Chen",
            "Drew F. K. Williamson",
            "Richard J. Chen",
            "Kenji Ikamura",
            "Georg K. Gerber",
            "Ivy Liang",
            "L. Le",
            "Tong Ding",
            "Anil V. Parwani",
            "Faisal Mahmood"
        ],
        "citations": 16,
        "references": 126,
        "year": 2023
    },
    {
        "title": "SimpleClick: Interactive Image Segmentation with Simple Vision Transformers",
        "abstract": "Click-based interactive image segmentation aims at extracting objects with a limited user clicking. A hierarchical backbone is the de-facto architecture for current methods. Recently, the plain, non-hierarchical Vision Transformer (ViT) has emerged as a competitive backbone for dense prediction tasks. This design allows the original ViT to be a foundation model that can be finetuned for downstream tasks without redesigning a hierarchical backbone for pretraining. Although this design is simple and has been proven effective, it has not yet been explored for interactive image segmentation. To fill this gap, we propose SimpleClick, the first interactive segmentation method that leverages a plain backbone. Based on the plain backbone, we introduce a symmetric patch embedding layer that encodes clicks into the backbone with minor modifications to the backbone itself. With the plain backbone pretrained as a masked autoencoder (MAE), SimpleClick achieves state-of-the-art performance. Remarkably, our method achieves 4.15 NoC@90 on SBD, improving 21.8% over the previous best result. Extensive evaluation on medical images demonstrates the generalizability of our method. We provide a detailed computational analysis, highlighting the suitability of our method as a practical annotation tool.",
        "authors": [
            "Qin Liu",
            "Zhenlin Xu",
            "Gedas Bertasius",
            "M. Niethammer"
        ],
        "citations": 88,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Unifying Vision, Text, and Layout for Universal Document Processing",
        "abstract": "We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA, across diverse data domains like finance reports, academic papers, and web-sites. UDOP ranks first on the leaderboard of the Document Understanding Benchmark.11 Code and models: https://github.com/microsoft/i-Code/tree/main/i-Code-Doc",
        "authors": [
            "Zineng Tang",
            "Ziyi Yang",
            "Guoxin Wang",
            "Yuwei Fang",
            "Yang Liu",
            "Chenguang Zhu",
            "Michael Zeng",
            "Chao-Yue Zhang",
            "Mohit Bansal"
        ],
        "citations": 88,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Thoughts on how to think (and talk) about RNA structure",
        "abstract": "Recent events have pushed RNA research into the spotlight. Continued discoveries of RNA with unexpected diverse functions in healthy and diseased cells, such as the role of RNA as both the source and countermeasure to a severe acute respiratory syndrome coronavirus 2 infection, are igniting a new passion for understanding this functionally and structurally versatile molecule. Although RNA structure is key to function, many foundational characteristics of RNA structure are misunderstood, and the default state of RNA is often thought of and depicted as a single floppy strand. The purpose of this perspective is to help adjust mental models, equipping the community to better use the fundamental aspects of RNA structural information in new mechanistic models, enhance experimental design to test these models, and refine data interpretation. We discuss six core observations focused on the inherent nature of RNA structure and how to incorporate these characteristics to better understand RNA structure. We also offer some ideas for future efforts to make validated RNA structural information available and readily used by all researchers.",
        "authors": [
            "Q. Vicens",
            "J. Kieft"
        ],
        "citations": 97,
        "references": 109,
        "year": 2022
    },
    {
        "title": "Periodic formation of epithelial somites from human pluripotent stem cells",
        "abstract": null,
        "authors": [
            "Marina Sanaki-Matsumiya",
            "Mitsuhiro Matsuda",
            "N. Gritti",
            "Fumio Nakaki",
            "James Sharpe",
            "V. Trivedi",
            "Miki Ebisuya"
        ],
        "citations": 73,
        "references": 80,
        "year": 2022
    },
    {
        "title": "A Modern Approach towards an Industry 4.0 Model: From Driving Technologies to Management",
        "abstract": "Every so often, a confluence of novel technologies emerges that radically transforms every aspect of the industry, the global economy, and finally, the way we live. These sharp leaps of human ingenuity are known as industrial revolutions, and we are currently in the midst of the fourth such revolution, coined Industry 4.0 by the World Economic Forum. Building on their guideline set of technologies that encompass Industry 4.0, we present a full set of pillar technologies on which Industry 4.0 project portfolio management rests as well as the foundation technologies that support these pillars. A complete model of an Industry 4.0 factory which relies on these pillar technologies is presented. The full set of pillars encompasses cyberphysical systems and Internet of Things (IoT), artificial intelligence (AI), machine learning (ML) and big data, robots and drones, cloud computing, 5G and 6G networks, 3D printing, virtual and augmented reality, and blockchain technology. These technologies are based on a set of foundation technologies which include advances in computing, nanotechnology, biotechnology, materials, energy, and finally cube satellites. We illustrate the confluence of all these technologies in a single model factory. This new factory model succinctly demonstrates the advancements in manufacturing introduced by these modern technologies, which qualifies this as a seminal industrial revolutionary event in human history.",
        "authors": [
            "Georgios Tsaramirsis",
            "Antreas Kantaros",
            "Izzat Al-Darraji",
            "D. Piromalis",
            "Charalampos Apostolopoulos",
            "Athanasia Pavlopoulou",
            "M. Alrammal",
            "Z. Ismail",
            "S. Buhari",
            "M. Stojmenovic",
            "Hatem Tamimi",
            "Princy Randhawa",
            "Akshet Patel",
            "F. Khan"
        ],
        "citations": 68,
        "references": 132,
        "year": 2022
    },
    {
        "title": "Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?",
        "abstract": "The success of deep learning heavily relies on large-scale data with comprehensive labels, which is more expensive and time-consuming to fetch in 3D compared to 2D images or natural languages. This promotes the potential of utilizing models pretrained with data more than 3D as teachers for cross-modal knowledge transferring. In this paper, we revisit masked modeling in a unified fashion of knowledge distillation, and we show that foundational Transformers pretrained with 2D images or natural languages can help self-supervised 3D representation learning through training Autoencoders as Cross-Modal Teachers (ACT). The pretrained Transformers are transferred as cross-modal 3D teachers using discrete variational autoencoding self-supervision, during which the Transformers are frozen with prompt tuning for better knowledge inheritance. The latent features encoded by the 3D teachers are used as the target of masked point modeling, wherein the dark knowledge is distilled to the 3D Transformer students as foundational geometry understanding. Our ACT pretrained 3D learner achieves state-of-the-art generalization capacity across various downstream benchmarks, e.g., 88.21% overall accuracy on ScanObjectNN. Codes have been released at https://github.com/RunpeiDong/ACT.",
        "authors": [
            "Runpei Dong",
            "Zekun Qi",
            "Linfeng Zhang",
            "Junbo Zhang",
            "Jian‐Yuan Sun",
            "Zheng Ge",
            "Li Yi",
            "Kaisheng Ma"
        ],
        "citations": 73,
        "references": 122,
        "year": 2022
    },
    {
        "title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
        "abstract": null,
        "authors": [
            "Jannis Born",
            "M. Manica"
        ],
        "citations": 70,
        "references": 97,
        "year": 2022
    },
    {
        "title": "Finding the right thermal limit: a framework to reconcile ecological, physiological and methodological aspects of CTmax in ectotherms.",
        "abstract": "Upper thermal limits (CTmax) are frequently used to parameterize the fundamental niche of ectothermic animals and to infer biogeographical distribution limits under current and future climate scenarios. However, there is considerable debate associated with the methodological, ecological and physiological definitions of CTmax. The recent (re)introduction of the thermal death time (TDT) model has reconciled some of these issues and now offers a solid mathematical foundation to model CTmax by considering both intensity and duration of thermal stress. Nevertheless, the physiological origin and boundaries of this temperature-duration model remain unexplored. Supported by empirical data, we here outline a reconciling framework that integrates the TDT model, which operates at stressful temperatures, with the classic thermal performance curve (TPC) that typically describes biological functions at permissive temperatures. Further, we discuss how the TDT model is founded on a balance between disruptive and regenerative biological processes that ultimately defines a critical boundary temperature (Tc) separating the TDT and TPC models. Collectively, this framework allows inclusion of both repair and accumulation of heat stress, and therefore also offers a consistent conceptual approach to understand the impact of high temperature under fluctuating thermal conditions. Further, this reconciling framework allows improved experimental designs to understand the physiological underpinnings and ecological consequences of ectotherm heat tolerance.",
        "authors": [
            "Michael Ørsted",
            "L. B. Jørgensen",
            "J. Overgaard"
        ],
        "citations": 59,
        "references": 134,
        "year": 2022
    },
    {
        "title": "Supporting Artificial Social Intelligence With Theory of Mind",
        "abstract": "In this paper, we discuss the development of artificial theory of mind as foundational to an agent's ability to collaborate with human team members. Agents imbued with artificial social intelligence will require various capabilities to gather the social data needed to inform an artificial theory of mind of their human counterparts. We draw from social signals theorizing and discuss a framework to guide consideration of core features of artificial social intelligence. We discuss how human social intelligence, and the development of theory of mind, can contribute to the development of artificial social intelligence by forming a foundation on which to help agents model, interpret and predict the behaviors and mental states of humans to support human-agent interaction. Artificial social intelligence will need the processing capabilities to perceive, interpret, and generate combinations of social cues to operate within a human-agent team. Artificial Theory of Mind affords a structure by which a socially intelligent agent could be imbued with the ability to model their human counterparts and engage in effective human-agent interaction. Further, modeling Artificial Theory of Mind can be used by an ASI to support transparent communication with humans, improving trust in agents, so that they may better predict future system behavior based on their understanding of and support trust in artificial socially intelligent agents.",
        "authors": [
            "Jessica Williams",
            "S. Fiore",
            "F. Jentsch"
        ],
        "citations": 54,
        "references": 80,
        "year": 2022
    },
    {
        "title": "DeCAST in TransVerse for Parallel Intelligent Transportation Systems and Smart Cities: Three Decades and Beyond",
        "abstract": "Parallel transportation management and control was proposed three decades ago as a new paradigm for conducting complex transportation operations and has led to today’s DeCAST in TransVerse platform designed and constructed according to the principle of decentralized/distributed autonomous operations and organizations. This article presents an overview of its architectures, processes, operating procedures, and major applications. The developments and applications have demonstrated clearly that parallel transportation systems are effective for networked traffic control and distributed logistical operations. The existing challenges and emerging opportunities are also addressed. A transportation foundation model based on parallel learning and federated intelligence is proposed as a potential path to the next-generation parallel intelligent transportation systems.",
        "authors": [
            "Chen Zhao",
            "Yisheng Lv",
            "Junchen Jin",
            "Yonglin Tian",
            "Jiangong Wang",
            "Fei-Yue Wang"
        ],
        "citations": 48,
        "references": 39,
        "year": 2022
    },
    {
        "title": "InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges",
        "abstract": "In this report, we present our champion solutions to five tracks at Ego4D challenge. We leverage our developed InternVideo, a video foundation model, for five Ego4D tasks, including Moment Queries, Natural Language Queries, Future Hand Prediction, State Change Object Detection, and Short-term Object Interaction Anticipation. InternVideo-Ego4D is an effective paradigm to adapt the strong foundation model to the downstream ego-centric video understanding tasks with simple head designs. In these five tasks, the performance of InternVideo-Ego4D comprehensively surpasses the baseline methods and the champions of CVPR2022, demonstrating the powerful representation ability of InternVideo as a video foundation model. Our code will be released at https://github.com/OpenGVLab/ego4d-eccv2022-solutions",
        "authors": [
            "Guo Chen",
            "Sen Xing",
            "Zhe Chen",
            "Yi Wang",
            "Kunchang Li",
            "Yizhuo Li",
            "Yi Liu",
            "Jiahao Wang",
            "Yin-Dong Zheng",
            "Bingkun Huang",
            "Zhiyu Zhao",
            "Junting Pan",
            "Yifei Huang",
            "Zun Wang",
            "Jiashuo Yu",
            "Yinan He",
            "Hongjie Zhang",
            "Tong Lu",
            "Yali Wang",
            "Liming Wang",
            "Yu Qiao"
        ],
        "citations": 43,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Autonomous Cooperative Search Model for Multi-UAV With Limited Communication Network",
        "abstract": "With the rapid development of artificial intelligence technology, the multi-UAV cooperative search has wide applications in the field of Internet of Things, such as resource exploration, emergency rescue, intelligent transportation, etc. However, the communication network in an unknown environment may be inaccessible, and the real-time information sharing among UAVs cannot be guaranteed, resulting in the failure of cooperative search. Aiming at this issue, this article is devoted to the design of the multi-UAV flight strategy to improve the cooperative search capability in an uncertain communication environment. Specifically, a new cooperative architecture oriented to a local communication network is devised to control the observation locations of multiple UAVs in the search process, and some local communication networks are established based on the distance among UAVs to meet the requirements of the search task. On this foundation, we develop a multi-UAV cooperative search model (MCSM) with communication cost and formation benefit as an optimization function to ensure the effectiveness of multi-UAV search. Moreover, in the process of model solving, an improved sparrow search algorithm (ISSA) is presented with some different search strategies to enhance the optimization capability. To verify the superiority of the proposed method, we designed several groups of simulation experiments to analyze the performance of MCSM. Experimental results illustrate that our method can not only maintain high cooperative search accuracy but also has high stability and convergence speed.",
        "authors": [
            "Bowen Fei",
            "Weidong Bao",
            "Xiaomin Zhu",
            "Daqian Liu",
            "Tong Men",
            "Zhenliang Xiao"
        ],
        "citations": 39,
        "references": 40,
        "year": 2022
    },
    {
        "title": "VL-BEiT: Generative Vision-Language Pretraining",
        "abstract": "We introduce a vision-language foundation model called VL-BEiT, which is a bidirectional multimodal Transformer learned by generative pretraining. Our minimalist solution conducts masked prediction on both monomodal and multimodal data with a shared Transformer. Specifically, we perform masked vision-language modeling on image-text pairs, masked language modeling on texts, and masked image modeling on images. VL-BEiT is learned from scratch with one unified pretraining task, one shared backbone, and one-stage training. Our method is conceptually simple and empirically effective. Experimental results show that VL-BEiT obtains strong results on various vision-language benchmarks, such as visual question answering, visual reasoning, and image-text retrieval. Moreover, our method learns transferable visual features, achieving competitive performance on image classification, and semantic segmentation.",
        "authors": [
            "Hangbo Bao",
            "Wenhui Wang",
            "Li Dong",
            "Furu Wei"
        ],
        "citations": 43,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Overcoming data scarcity in biomedical imaging with a foundational multi-task model",
        "abstract": "Foundational models, pretrained on a large scale, have demonstrated substantial success across non-medical domains. However, training these models typically requires large, comprehensive datasets, which contrasts with the smaller and more specialized datasets common in biomedical imaging. Here we propose a multi-task learning strategy that decouples the number of training tasks from memory requirements. We trained a universal biomedical pretrained model (UMedPT) on a multi-task database including tomographic, microscopic and X-ray images, with various labeling strategies such as classification, segmentation and object detection. The UMedPT foundational model outperformed ImageNet pretraining and previous state-of-the-art models. For classification tasks related to the pretraining database, it maintained its performance with only 1% of the original training data and without fine-tuning. For out-of-domain tasks it required only 50% of the original training data. In an external independent validation, imaging features extracted using UMedPT proved to set a new standard for cross-center transferability.",
        "authors": [
            "Raphael Schäfer",
            "Till Nicke",
            "Henning Höfener",
            "Annkristin Lange",
            "D. Merhof",
            "Friedrich Feuerhake",
            "Volkmar Schulz",
            "Johannes Lotz",
            "F. Kiessling"
        ],
        "citations": 9,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Modal analysis of offshore monopile wind turbine: An analytical solution",
        "abstract": "\n An analytical solution of the dynamic response of offshore wind turbines under wave load with nonlinear Stokes's wave theory and wave-structure and soil-foundation interactions is developed. Natural frequencies and the corresponding modes are obtained. The effect of the wave-structure interaction, the added mass, the foundation stiffness, and the nacelle translational and rotational inertia on the motion of the structure is investigated. The nonlinear loading provided by the drag term of Morison's equation is successfully handled. A parametric study to examine the effect of the structural parameters on the dynamic response is conducted and the results of the proposed analytical solution are compared to numerical ones. The proposed method has the following advantages: a) it is accurate and straightforward because of its analytical nature, b) it does not ignore the drag term in the wave loading by keeping its nonlinearity nature, c) the structure of the wind turbine is modeled as a continuous system, d) it takes into account the effect of the rotational and translational inertia of the nacelle on the dynamic response, e) it provides an interpretation of the effect of the sea level variation in changing the natural frequencies.",
        "authors": [
            "Hadi Pezeshki",
            "D. Pavlou",
            "H. Adeli",
            "S. Siriwardane"
        ],
        "citations": 44,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?",
        "abstract": "As the scope of machine learning broadens, we observe a recurring theme of algorithmic monoculture: the same systems, or systems that share components (e.g. training data), are deployed by multiple decision-makers. While sharing offers clear advantages (e.g. amortizing costs), does it bear risks? We introduce and formalize one such risk, outcome homogenization: the extent to which particular individuals or groups experience negative outcomes from all decision-makers. If the same individuals or groups exclusively experience undesirable outcomes, this may institutionalize systemic exclusion and reinscribe social hierarchy. To relate algorithmic monoculture and outcome homogenization, we propose the component-sharing hypothesis: if decision-makers share components like training data or specific models, then they will produce more homogeneous outcomes. We test this hypothesis on algorithmic fairness benchmarks, demonstrating that sharing training data reliably exacerbates homogenization, with individual-level effects generally exceeding group-level effects. Further, given the dominant paradigm in AI of foundation models, i.e. models that can be adapted for myriad downstream tasks, we test whether model sharing homogenizes outcomes across tasks. We observe mixed results: we find that for both vision and language settings, the specific methods for adapting a foundation model significantly influence the degree of outcome homogenization. We conclude with philosophical analyses of and societal challenges for outcome homogenization, with an eye towards implications for deployed machine learning systems.",
        "authors": [
            "Rishi Bommasani",
            "Kathleen A. Creel",
            "Ananya Kumar",
            "Dan Jurafsky",
            "Percy Liang"
        ],
        "citations": 63,
        "references": 86,
        "year": 2022
    },
    {
        "title": "Mechanical Behavior and Design Properties of Ultra-High-Performance Concrete",
        "abstract": "The appropriate and efficient design of structural components made with ultra-high-performance concrete (UHPC) requires the establishment of key design properties and material models that engage UHPC’s distinct mechanical properties, as compared to conventional concrete. This paper presents the results of an extensive program of compression and tension property assessment executed according to existing testing methods to assess the mechanical characteristics of several commercially available UHPC products. The experimental results are then used to propose suitable mechanical models and design parameters that are foundational for the structural-level application of UHPC. The models rely on a set of experimentally identified mechanical performance properties that distinguish UHPC from conventional concrete and establish the basis of the material qualification for use in structural design. As such, this work constitutes a fundamental step in ongoing efforts to develop UHPC structural design guidance in the United States.",
        "authors": [
            "Rafic G. El-Helou",
            "Z. Haber",
            "B. Graybeal"
        ],
        "citations": 54,
        "references": 27,
        "year": 2022
    },
    {
        "title": "Altered excitatory and inhibitory neuronal subpopulation parameters are distinctly associated with tau and amyloid in Alzheimer’s disease",
        "abstract": "Background Neuronal and circuit level abnormalities of excitation and inhibition are shown to be associated with tau and amyloid-beta (Aβ) in preclinical models of Alzheimer’s disease (AD). These relationships remain poorly understood in patients with AD. Methods Using empirical spectra from magnetoencephalography (MEG) and computational modeling (neural mass model; NMM) we examined excitatory and inhibitory parameters of neuronal subpopulations and investigated their specific associations to regional tau and Aβ, measured by positron emission tomography (PET), in patients with AD. Results Patients with AD showed abnormal excitatory and inhibitory time-constants and neural gains compared to age-matched controls. Increased excitatory time-constants distinctly correlated with higher tau depositions while increased inhibitory time-constants distinctly correlated with higher Aβ depositions. Conclusions Our results provide critical insights about potential mechanistic links between abnormal neural oscillations and cellular correlates of impaired excitatory and inhibitory synaptic functions associated with tau and Aβ in patients with AD. Funding This study was supported by the National Institutes of Health grants: K08AG058749 (KGR), F32AG050434-01A1 (KGR), K23 AG038357 (KAV), P50 AG023501, P01 AG19724 (BLM), P50-AG023501 (BLM & GDR), R01 AG045611 (GDR); AG034570, AG062542 (WJ); NS100440 (SSN), DC176960 (SSN), DC017091 (SSN), AG062196 (SSN); a grant from John Douglas French Alzheimer’s Foundation (KAV); grants from Larry L. Hillblom Foundation: 2015-A-034-FEL and (KGR); 2019-A-013-SUP (KGR); a grant from the Alzheimer’s Association: (PCTRB-13-288476) (KAV), and made possible by Part the CloudTM, (ETAC-09-133596); a grant from Tau Consortium (GDR & WJJ), and a gift from the S. D. Bechtel Jr. Foundation.",
        "authors": [
            "K. Ranasinghe",
            "Parul Verma",
            "Chang Cai",
            "Xihe Xie",
            "Kiwamu Kudo",
            "Xiao Gao",
            "Hannah Lerner",
            "D. Mizuiri",
            "A. Strom",
            "L. Iaccarino",
            "R. La Joie",
            "B. Miller",
            "M. Gorno-Tempini",
            "K. Rankin",
            "W. Jagust",
            "K. Vossel",
            "G. Rabinovici",
            "A. Raj",
            "S. Nagarajan"
        ],
        "citations": 53,
        "references": 82,
        "year": 2022
    },
    {
        "title": "Bearing Capacity of Ring Foundations on Anisotropic and Heterogenous Clays: FEA, NGI-ADP, and MARS",
        "abstract": null,
        "authors": [
            "Van Qui Lai",
            "J. Shiau",
            "S. Keawsawasvong",
            "Duy Tan Tran"
        ],
        "citations": 38,
        "references": 89,
        "year": 2022
    },
    {
        "title": "Indian Sign Language Recognition System for Dynamic Signs",
        "abstract": "Sign language is a means of communication utilising manual gestures (movement of hands and wrists) and non-manual gestures (expressions of face and body language). There are many different sign languages in the world, each with its own collection of words and signs. This study focuses on the implementation of Indian Sign Language Recognition System (ISLRS) which help deaf people to communicate with other persons. In this paper, the model based on Sign Language Recognition (SLR) of dynamic signs using Convolutional Neural Network (CNN) is proposed. The proposed model has been trained and tested on video clips of dynamic signs and achieved the training accuracy of 70%. This study should serve as a road map for users to use in deciding which model to implement and laying a foundation for future research and enhancing model accuracy, allowing the sign language community to communicate and share their ideas more effectively. This work is also used to overcome the educational gap between hearing impaired persons.",
        "authors": [
            "Arun Singh",
            "Ankita Wadhawan",
            "Manik Rakhra",
            "Usha Mittal",
            "Ahmed Al Ahdal",
            "S. K. Jha"
        ],
        "citations": 36,
        "references": 15,
        "year": 2022
    },
    {
        "title": "Numerical Analysis of Passive Piles under Surcharge Load in Extensively Deep Soft Soil",
        "abstract": "The three-dimensional finite difference method was used in this study to analyze the deformation and stresses of a passive pile under surcharge load in extensively deep soft soil. A three-dimensional numerical model was proposed and verified by a field test. The horizontal displacements of the pile agreed well with the field results. This study investigated the pile-foundation soil interaction, the load transfer mechanism, the excess pore water pressure (EPWP), and the horizontal resistance of the foundation soil. The results show that the soil in the corner of the loading area developed a large uplift deformation, while the center of the loading area developed a large settlement. The lateral displacement of the pile decreased sharply with the increase of the depth and increased with the surcharge load. The lateral displacement of the soil was negligible when the depth exceeded 30 m. The EPWP increased in a nonlinear way with the increase of the surcharge load and accumulated with the placement of the new lift. The distribution of the lateral earth pressure in the shallow soil layer was complex, and the negative value was observed under a high surcharge load due to the suction effect. The proportion coefficient of the horizontal resistance coefficient showed much smaller value in the situation of large lateral deformation and high surcharge load. The design code overestimated the horizontal resistance of the shallow foundation soil, which should be given attention for the design and analysis of the laterally loaded structures in extensively soft soil.",
        "authors": [
            "M. Gu",
            "Xiao-Hua Cai",
            "Q. Fu",
            "Haibo Li",
            "Xi Wang",
            "B. Mao"
        ],
        "citations": 36,
        "references": 32,
        "year": 2022
    },
    {
        "title": "The genome of homosporous maidenhair fern sheds light on the euphyllophyte evolution and defences",
        "abstract": null,
        "authors": [
            "Y. Fang",
            "Xing Qin",
            "Qinggang Liao",
            "Ran Du",
            "Xizhi Luo",
            "Qian Zhou",
            "Zhen Li",
            "Hen-Huang Chen",
            "Wanting Jin",
            "Yaning Yuan",
            "Pengbo Sun",
            "Rui Zhang",
            "Jiao Zhang",
            "Li Wang",
            "Shifeng Cheng",
            "Xueyong Yang",
            "Yuehong Yan",
            "Xingtan Zhang",
            "Zhonghua Zhang",
            "Shunong Bai",
            "Y. Van de Peer",
            "W. J. Lucas",
            "Sanwen Huang",
            "Jianbin Yan"
        ],
        "citations": 34,
        "references": 142,
        "year": 2022
    },
    {
        "title": "Characteristics of ground settlement due to combined actions of groundwater drawdown and enclosure wall movement",
        "abstract": null,
        "authors": [
            "Chaofeng Zeng",
            "Shuo Wang",
            "Xiu-li Xue",
            "G. Zheng",
            "Guo Mei"
        ],
        "citations": 34,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Interlinking Structural Racism and Heteropatriarchy: Rethinking Family Structure's Effects on Child Outcomes in a Racialized, Unequal Society.",
        "abstract": "In the field of family science and in the broader family policy discourse, debate is ongoing about the importance of family structure for child outcomes. Missing from this debate is a full integration of how the foundational pillars of White supremacy, namely structural racism and heteropatriarchy, impact both family formation and child outcomes, especially among diversely configured Black families. From a critical intersectional lens, we argue that conceptual models used to explain racialized child outcomes based on family structure effects are problematic because they compare family structure statuses without accounting for structural racism and interlinked heteropatriarchal conditions. We present a new conceptual model that integrates structural racism and heteropatriarchy to examine the salience of family structure statuses for child outcomes and discuss approaches to research design, empirical measurement, and interpretation in order to bring this new model into practice.",
        "authors": [
            "Christina J. Cross",
            "Paula Fomby",
            "Bethany L. Letiecq"
        ],
        "citations": 41,
        "references": 90,
        "year": 2022
    },
    {
        "title": "VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners",
        "abstract": "We explore an efficient approach to establish a foundational video-text model. We present VideoCoCa that maximally reuses a pretrained image-text contrastive captioner (CoCa) model and adapt it to video-text tasks with minimal extra training. While previous works adapt image-text models with various cross-frame fusion modules, we find that the generative attentional pooling and contrastive attentional pooling layers in CoCa are instantly adaptable to flattened frame embeddings, yielding state-of-the-art results on zero-shot video classification and zero-shot text-to-video retrieval. Furthermore, we explore lightweight finetuning on top of VideoCoCa, and achieve strong results on video question-answering and video captioning.",
        "authors": [
            "Shen Yan",
            "Tao Zhu",
            "Zirui Wang",
            "Yuan Cao",
            "Mi Zhang",
            "Soham Ghosh",
            "Yonghui Wu",
            "Jiahui Yu"
        ],
        "citations": 36,
        "references": 76,
        "year": 2022
    },
    {
        "title": "A guide to area‐restricted search: a foundational foraging behaviour",
        "abstract": "Area‐restricted search is the capacity to change search effort adaptively in response to resource encounters or expectations, from directional exploration (global, extensive search) to focused exploitation (local, intensive search). This search pattern is used by numerous organisms, from worms and insects to humans, to find various targets, such as food, mates, nests, and other resources. Area‐restricted search has been studied for at least 80 years by ecologists, and more recently in the neurological and psychological literature. In general, the conditions promoting this search pattern are: (1) clustered resources; (2) active search (e.g. not a sit‐and‐wait predator); (3) searcher memory for recent target encounters or expectations; and (4) searcher ignorance about the exact location of targets. Because area‐restricted search adapts to resource encounters, the search can be performed at multiple spatial scales. Models and experiments have demonstrated that area‐restricted search is superior to alternative search patterns that do not involve a memory of the exact location of the target, such as correlated random walks or Lévy walks/flights. Area‐restricted search is triggered by sensory cues whereas concentrated search in the absence of sensory cues is associated with other forms of foraging. Some neural underpinnings of area‐restricted search are probably shared across metazoans, suggesting a shared ancestry and a shared solution to a common ecological problem of finding clustered resources. Area‐restricted search is also apparent in other domains, such as memory and visual search in humans, which may indicate an exaptation from spatial search to other forms of search. Here, we review these various aspects of area‐restricted search, as well as how to identify it, and point to open questions.",
        "authors": [
            "A. Dorfman",
            "Thomas T. Hills",
            "I. Scharf"
        ],
        "citations": 35,
        "references": 160,
        "year": 2022
    },
    {
        "title": "A negative stiffness dynamic base absorber for seismic retrofitting of residential buildings",
        "abstract": "In this study, a negative stiffness‐based passive vibration absorber is developed and implemented as a seismic retrofitting measure for typical reinforced concrete (RC) residential buildings. The device, namely, the extended KDamper for retrofitting (EKD‐R), is introduced at the base of the structure, between the foundation level and the first story of the building. The design of the EKD‐R device and the selection of its properties are undertaken by incorporating a harmony search (HS) algorithm that provides optimized parameters for the mechanism, following constraints and limitations imposed by the examined structural system. Nonlinearities due to the plastic behavior of the structural members and soil–structure interaction (SSI) effects are modeled and taken into consideration during the process. Subsequently, a realistic case study of a benchmark three‐story RC building is examined, and the performance of the EKD‐R system is assessed. The building superstructure is designed according to Eurocodes. The structure–foundation system, along with the EKD‐R, is explicitly modeled using finite elements (FE) that may realistically capture structural nonlinearities and SSI effects. The HS algorithm is employed, and optimized EKD‐R components are obtained and implemented in the benchmark structure. Finally, a series of recorded real ground motions are selected, and nonlinear time‐history dynamic analyses are conducted aiming to assess the behavior of the controlled system. Results indicate the beneficial role of the novel dynamic absorber, hence rendering the concept a compelling seismic retrofitting technology.",
        "authors": [
            "A. Mantakas",
            "K. Kapasakalis",
            "Antonios Alvertos",
            "I. Antoniadis",
            "E. Sapountzakis"
        ],
        "citations": 29,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Nonlinear static bending analysis of microplates resting on imperfect two-parameter elastic foundations using modified couple stress theory",
        "abstract": ". The nonlinear static bending analysis of microplates resting on imperfect Pasternak elastic foundations is carried out in this paper. The ﬁnite element method based on the modiﬁed couple stress theory is used to derive the nonlinear ﬁnite element formulations. The present theory and mathematical model are validated by comparisons of this work’s results with those of other reputable publications, which show a very good agreement. The inﬂuences of length-scale parameter, nonlinearity, elastic foundation parameters, imperfect foundations, and boundary conditions on the nonlinear static bending response of microplates are then explored. The computed data of this study is very intriguing, particularly the interaction of the microplate with the imperfect elastic foundation, and this helps us better understand the mechanical behavior of this structure.",
        "authors": [
            "Nguyen Thai Dung",
            "Le Minh Thai",
            "Tran Van Ke",
            "Truong Thi Huong Huyen",
            "Phung Van Minh"
        ],
        "citations": 25,
        "references": 50,
        "year": 2022
    },
    {
        "title": "Determination of 3D near fault seismic behaviour of Oroville earth fill dam using burger material model and free field-quiet boundary conditions",
        "abstract": "ABSTRACT In this study, the three-dimensional (3D) near-fault earthquake performance of the Oroville dam is examined considering a special material model and various seismic boundary conditions. The 3D finite-difference model of the Oroville EF dam is modeled using the finite difference method. Burger Creep (BC) material model is utilized for the foundation and dam body materials. Special interface elements are taken into account between the dam body and foundation. Fix, free field, and quiet seismic boundary conditions are considered for 3D nonlinear earthquake analyses. Total six various strong near-fault earthquakes are used in the 3D analyses. According to the non-linear earthquake analyses, principal stresses, horizontal and vertical displacements for three nodal points are assessed in detail and numerical results are compared for reflecting and non-reflecting seismic boundary conditions. It is clearly understood that seismic boundary conditions should not be utilized randomly for 3D modeling and analysis of EF dams.",
        "authors": [
            "M. Karalar",
            "Murat Çavuşlu"
        ],
        "citations": 22,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Cohesive zone modelling of hydrogen assisted fatigue crack growth: the role of trapping",
        "abstract": "We investigate the influence of microstructural traps in hydrogen-assisted fatigue crack growth. To this end, a new formulation combining multi-trap stress-assisted diffusion, mechanism-based strain gradient plasticity and a hydrogen- and fatigue-dependent cohesive zone model is presented and numerically implemented. The results show that the ratio of loading frequency to effective diffusivity governs fatigue crack growth behaviour. Increasing the density of \\emph{beneficial} traps, not involved in the fracture process, results in lower fatigue crack growth rates. The combinations of loading frequency and carbide trap densities that minimise embrittlement susceptibility are identified, providing the foundation for a rational design of hydrogen-resistant alloys.",
        "authors": [
            "R. Fern'andez-Sousa",
            "C. Beteg'on",
            "E. Mart'inez-Paneda"
        ],
        "citations": 22,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Simplified method for evaluating the response of existing tunnel induced by adjacent excavation",
        "abstract": "Adjacent excavation may have a negative influence on the existing tunnel underneath. Thus, it is important to evaluate the response of the tunnel due to adjacent excavation. However, there is little report about using the Kerr foundation model to simulate the tunnel‐soil interaction. Meanwhile, the Timoshenko beam, which can take the tunnel shearing effect into consideration, is more suitable to estimate the behavior of the tunnel. To simulate the interaction between soil and tunnel, the existing tunnel is simplified as a Timoshenko beam lying on the Kerr foundation model, and a simplified theoretical method is proposed to calculate the response of the existing tunnel induced by adjacent excavation. The proposed method is validated by two field case studies. Results indicate that the predictions given by the proposed method show great agreement with field measurements and it is more accurate to evaluate the tunnel‐soil interaction compared with the previous method. The further parametric study shows that the relative position between excavation and tunnel, the ground Young's modulus, the depth of existing tunnel centerline, and length and width of excavation are both significant factors governing the tunnel response induced by adjacent excavation, while the influence of tunnel shear stiffness and skew between tunnel and excavation are slight. The proposed method can be applied to predict the potential risk of existing tunnels induced by adjacent excavation in relevant engineering projects.",
        "authors": [
            "Guohui Feng",
            "Changjie Xu",
            "Luju Liang",
            "Ming‐wang Tey",
            "M. Chi",
            "S. Ge"
        ],
        "citations": 21,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Nonlinear Static Stability of Imperfect Bio-Inspired Helicoidal Composite Beams",
        "abstract": "The objective of this manuscript is to develop, for the first time, a mathematical model for the prediction of buckling, postbuckling, and nonlinear bending of imperfect bio-inspired helicoidal composite beams with nonlinear rotation angle. The equilibrium nonlinear integrodifferential equations of imperfect (curved) helicoidal composite beams are derived from the Euler–Bernoulli kinematic assumption. The differential integral quadrature method (DIQM) and Newton-iterative method are employed to evaluate the response of imperfect helicoidal composite beams. Following the validation of the proposed model, numerical studies are performed to quantify the effect of rotation angle, imperfection amplitude, and foundation stiffness on postbuckling and bending behaviors of helicoidal composite beams. The perfect beam buckles through a pitchfork bifurcation. However, the imperfect beam snaps through the buckling type. The critical buckling load increases with the increasing value of elastic foundation constants. However, the nonlinear foundation constant has no effect in the case of perfect beams. The present model can be exploited in the analysis of bio-inspired structure, which has a failure similar to a metal and low interlaminar shear stress, and is used extensively in numerous engineering applications.",
        "authors": [
            "N. Mohamed",
            "S. Mohamed",
            "M. A. Eltaher"
        ],
        "citations": 20,
        "references": 41,
        "year": 2022
    },
    {
        "title": "Buckling analysis and dynamic response of FGM sandwich cylindrical panels in thermal environments using nonlocal strain gradient theory",
        "abstract": null,
        "authors": [
            "Do Quang Chan",
            "T. Q. Quan",
            "B. Phi",
            "Dang Van Hieu",
            "N. D. Duc"
        ],
        "citations": 19,
        "references": 73,
        "year": 2022
    },
    {
        "title": "Transformation of Health and Social Care Systems—An Interdisciplinary Approach Toward a Foundational Architecture",
        "abstract": "Objective For realizing pervasive and ubiquitous health and social care services in a safe and high quality as well as efficient and effective way, health and social care systems have to meet new organizational, methodological, and technological paradigms. The resulting ecosystems are highly complex, highly distributed, and highly dynamic, following inter-organizational and even international approaches. Even though based on international, but domain-specific models and standards, achieving interoperability between such systems integrating multiple domains managed by multiple disciplines and their individually skilled actors is cumbersome. Methods Using the abstract presentation of any system by the universal type theory as well as universal logics and combining the resulting Barendregt Cube with parameters and the engineering approach of cognitive theories, systems theory, and good modeling best practices, this study argues for a generic reference architecture model moderating between the different perspectives and disciplines involved provide on that system. To represent architectural elements consistently, an aligned system of ontologies is used. Results The system-oriented, architecture-centric, and ontology-based generic reference model allows for re-engineering the existing and emerging knowledge representations, models, and standards, also considering the real-world business processes and the related development process of supporting IT systems for the sake of comprehensive systems integration and interoperability. The solution enables the analysis, design, and implementation of dynamic, interoperable multi-domain systems without requesting continuous revision of existing specifications.",
        "authors": [
            "B. Blobel",
            "F. Oemig",
            "P. Ruotsalainen",
            "D. López"
        ],
        "citations": 21,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Partitioned analysis of soil‐structure interaction for Nuclear Island Buildings",
        "abstract": "The present methods for three‐dimensional time‐domain soil‐structure interaction (SSI) analysis are often uneconomical because they are performed with a single time integration scheme and a single time step, which prohibits their application to large‐scale SSI problems. In this study, a partitioned analysis of SSI (PASSI) is proposed for enhancing SSI computational efficiency. This is accomplished by partitioning the soil‐foundation‐structure system into the soil (foundation) and structure subsystems and implementing the continuity conditions of the displacements and reaction forces at the soil (foundation)‐structure interface in a primal way. A lumped‐mass explicit finite element method and a transmitting artificial boundary are used to model the unbounded soil, the structure is analyzed via the implicit finite element method, and the response of the rigid foundation is calculated through an explicit time integration scheme. The solution is separately advanced over time for each subsystem. Different time steps can be chosen for the explicit and implicit integration schemes, which can greatly improve efficiency. Interaction effects are accounted for by the transmission and synchronization of the coupled state variables. In addition, intrafield and interfield parallel procedures for PASSI are developed, and their theoretical efficiencies are analyzed. A simple example is provided to verify the performance of the partitioned approach with an explicit‐implicit co‐computation and to compare it to the fully explicit approach. Finally, the seismic response analysis of a nuclear power plant is presented to validate the feasibility and efficiency of the intrafield and interfield parallel procedures.",
        "authors": [
            "Shaolin Chen",
            "Hao Lv",
            "G. Zhou"
        ],
        "citations": 16,
        "references": 48,
        "year": 2022
    },
    {
        "title": "concepts in",
        "abstract": "Extending inner-ear anatomical the Foundational Model of Anatomy (FMA) ontology Abstract —The inner ear is physically inaccessible in living humans, which leads to unique difﬁculties in studying its normal function and pathology as in other human organs. Recently, biosimulation model has gained a signiﬁcant attention to understand the exact causative factors that give rise to im- pairment in human organs. However, to build a biosimulation model for human organ concepts and their topological rela- tionships from multiple and semantically overlapping domains such as biology, anatomy, geometrical, mathematical, physical models are required. In this paper, we focus on modelling the inner-ear macro anatomical concepts and their topological relationships. We extended the Foundational Model of Anatomy (FMA) ontology to cover micro-level version of human inner- ear anatomy where connection between simulating tissues, liquids, soft tissues and connecting adjacent (e.g. hair cells, perilymph) parts studied in detail, included and implemented.",
        "authors": [
            "F. Bagarello",
            "F. Bagarello"
        ],
        "citations": 19,
        "references": 29,
        "year": 2022
    },
    {
        "title": "Foundational ontologies in action",
        "abstract": "The idea of proposing a special issue on the use of foundational ontologies for modelling simple everyday, or commonsense, situations came after several discussions with people in different domains which pointed to difficulties in understanding the philosophical and general presentations in the documentation accompanying foundational systems. This problem, we noticed, is worsened by the lack of step-by-step guidance to model a real situation from the point of view of a specific ontology. The result was that different knowledge engineers could generate mutually inconsistent models starting from the very same foundational ontology even when working on the same topic. Given that one of the main roles of ontologies is to serve as interoperability drivers, this conclusion was puzzling if not discouraging. We observed that it would not be possible to provide solutions to these issues without involving the very teams that developed foundational ontologies. On the other hand, if we could collect a series of presentations centred around the presentation of modelling cases, we would also have fostered a more consistent use of foundational ontologies, and set the bases for practical and comparative evaluations of the consequences in adopting one ontology rather than another. Another contribution that a special issue on “Foundational Ontologies in Action” could provide, and whose consideration eventually led to the particular selection of use cases discussed in this special issue, is to indicate how these ontologies differ in modelling core aspects in knowledge engineering practices. Such core aspects include the description of artefacts and their components, the modelling of changes",
        "authors": [
            "S. Borgo",
            "Antony Galton",
            "O. Kutz"
        ],
        "citations": 15,
        "references": 58,
        "year": 2022
    },
    {
        "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
        "abstract": "We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions: We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model's user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model's ability to capture critical semantic and detailed information across various visual tasks. We posit that a proficient Vision-Language Model should, foremost, possess strong language abilities. To ensure the preservation of LLM capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating LLM training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks. We have made both 1.3B and 7B models publicly accessible to foster innovations based on this foundation model.",
        "authors": [
            "Haoyu Lu",
            "Wen Liu",
            "Bo Zhang",
            "Bing-Li Wang",
            "Kai Dong",
            "Bo Liu (Benjamin Liu)",
            "Jingxiang Sun",
            "Tongzheng Ren",
            "Zhuoshu Li",
            "Hao Yang",
            "Yaofeng Sun",
            "C. Deng",
            "Hanwei Xu",
            "Zhenda Xie",
            "C. Ruan"
        ],
        "citations": 153,
        "references": 82,
        "year": 2024
    },
    {
        "title": "PointMamba: A Simple State Space Model for Point Cloud Analysis",
        "abstract": "Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity, making the design of a linear complexity method with global modeling appealing. In this paper, we propose PointMamba, transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks. Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs. Specifically, our method leverages space-filling curves for effective point tokenization and adopts an extremely simple, non-hierarchical Mamba encoder as the backbone. Comprehensive evaluations demonstrate that PointMamba achieves superior performance across multiple datasets while significantly reducing GPU memory usage and FLOPs. This work underscores the potential of SSMs in 3D vision-related tasks and presents a simple yet effective Mamba-based baseline for future research. The code will be made available at \\url{https://github.com/LMD0311/PointMamba}.",
        "authors": [
            "Dingkang Liang",
            "Xin Zhou",
            "Wei Xu",
            "Xingkui Zhu",
            "Zhikang Zou",
            "Xiaoqing Ye",
            "Xiao Tan",
            "Xiang Bai"
        ],
        "citations": 60,
        "references": 92,
        "year": 2024
    },
    {
        "title": "Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model",
        "abstract": "The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks.",
        "authors": [
            "Zihan Zhong",
            "Zhiqiang Tang",
            "Tong He",
            "Haoyang Fang",
            "Chun Yuan"
        ],
        "citations": 30,
        "references": 75,
        "year": 2024
    },
    {
        "title": "Codon affinity in mitochondrial DNA shapes evolutionary and somatic fitness",
        "abstract": "Somatic variation contributes to biological heterogeneity by modulating cellular proclivity to differentiate, expand, adapt, or die. While large-scale sequencing efforts have revealed the foundational role of somatic variants to drive human tumor evolution, our understanding of the contribution of mutations to modulate cellular fitness in non-malignant contexts remains understudied. Here, we identify a mosaic synonymous variant (m.7076A>G) in the mitochondrial DNA (mtDNA) encoded cytochrome c-oxidase subunit 1 gene (MT-CO1, p.Gly391=), which was present at homoplasmy in 47% of immune cells from a healthy donor. Using single-cell multi-omics, we discover highly specific selection against the m.7076G mutant allele in the CD8+ effector memory T cell compartment in vivo, reminiscent of selection observed for pathogenic mtDNA alleles1, 2 and indicative of lineage-specific metabolic requirements. While the wildtype m.7076A allele is translated via Watson-Crick-Franklin base-pairing, the anticodon diversity of the mitochondrial transfer RNA pool is limited, requiring wobble-dependent translation of the m.7076G mutant allele. Notably, mitochondrial ribosome profiling revealed altered codon-anticodon affinity at the wobble position as evidenced by stalled translation of the synonymous m.7076G mutant allele encoding for glycine. Generalizing this observation, we provide a new ontogeny of the 8,482 synonymous variants in the human mitochondrial genome that enables interpretation of functional mtDNA variation. Specifically, via inter- and intra-species evolutionary analyses, population-level complex trait associations, and the occurrence of germline and somatic mtDNA mutations from large-scale sequencing studies, we demonstrate that synonymous variation impacting codon:anticodon affinity is actively evolving across the entire mitochondrial genome and has broad functional and phenotypic effects. In summary, our results introduce a new ontogeny for mitochondrial genetic variation and support a model where organismal principles can be discerned from somatic evolution via single-cell genomics.",
        "authors": [
            "Caleb A. Lareau",
            "Yajie Yin",
            "Jacob C. Gutierrez",
            "R. Dhindsa",
            "Anne-Sophie Gribling-Burrer",
            "Yu-Hsin Hsieh",
            "Lena Nitsch",
            "Frank A. Buquicchio",
            "Tsion Abay",
            "Sebastian Zielinski",
            "Robert R. Stickels",
            "Jacob C. Ulirsch",
            "Patrick K. Yan",
            "Fangyi Wang",
            "Zhuang Miao",
            "Katalin D. Sandor",
            "Bence Daniel",
            "Vincent Liu",
            "Quanli Wang",
            "Fengyuan Hu",
            "Katherine R Smith",
            "S. V. Deevi",
            "P. Maschmeyer",
            "S. Petrovski",
            "R. Smyth",
            "William J. Greenleaf",
            "A. Kundaje",
            "Mathias Munschauer",
            "Leif S. Ludwig",
            "Ansuman T. Satpathy"
        ],
        "citations": 331,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Robust Drone Delivery with Weather Information",
        "abstract": "Problem definition: Drone delivery has recently garnered significant attention due to its potential for faster delivery at a lower cost than other delivery options. When scheduling drones from a depot for delivery to various destinations, the dispatcher must take into account the uncertain wind conditions, which affect the delivery times of drones to their destinations, leading to late deliveries. Methodology/results: To mitigate the risk of delivery delays caused by wind uncertainty, we propose a two-period drone scheduling model to robustly optimize the delivery schedule. In this framework, the scheduling decisions are made in the morning, with the provision for different delivery schedules in the afternoon that adapt to updated weather information available by midday. Our approach minimizes the essential riskiness index, which can simultaneously account for the probability of tardy delivery and the magnitude of lateness. Using wind observation data, we characterize the uncertain flight times via a cluster-wise ambiguity set, which has the benefit of tractability while avoiding overfitting the empirical distribution. A branch-and-cut (B&C) algorithm is developed for this adaptive distributionally framework to improve its scalability. Our adaptive distributionally robust model can effectively reduce lateness in out-of-sample tests compared with other classical models. The proposed B&C algorithm can solve instances to optimality within a shorter time frame than a general modeling toolbox. Managerial implications: Decision makers can use the adaptive robust model together with the cluster-wise ambiguity set to effectively reduce service lateness at customers for drone delivery systems. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72101049 and 72232001], the Natural Science Foundation of Liaoning Province [Grant 2023-BS-091], the Fundamental Research Funds for the Central Universities [Grant DUT23RC(3)045], and the Major Project of the National Social Science Foundation [Grant 22&ZD151]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2022.0339 .",
        "authors": [
            "Chun Cheng",
            "Y. Adulyasak",
            "Louis-Martin Rousseau"
        ],
        "citations": 21,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Safe and just Earth system boundaries",
        "abstract": null,
        "authors": [
            "J. Rockström",
            "J. Gupta",
            "Dahe Qin",
            "S. Lade",
            "J. Abrams",
            "L. Andersen",
            "D. A. Armstrong McKay",
            "Xuemei Bai",
            "G. Bala",
            "S. Bunn",
            "Daniel Ciobanu",
            "F. DeClerck",
            "K. Ebi",
            "L. Gifford",
            "C. Gordon",
            "Syezlin Hasan",
            "N. Kanie",
            "T. Lenton",
            "S. Loriani",
            "D. Liverman",
            "Awaz Mohamed",
            "N. Nakicenovic",
            "D. Obura",
            "D. Ospina",
            "K. Prodani",
            "C. Rammelt",
            "B. Sakschewski",
            "J. Scholtens",
            "B. Stewart‐Koster",
            "Thejna Tharammal",
            "D. V. van Vuuren",
            "P. Verburg",
            "R. Winkelmann",
            "C. Zimm",
            "E. Bennett",
            "S. Bringezu",
            "Wendy Broadgate",
            "P. Green",
            "Lei Huang",
            "L. Jacobson",
            "C. Ndehedehe",
            "Simona Pedde",
            "J. Rocha",
            "M. Scheffer",
            "L. Schulte-Uebbing",
            "W. de Vries",
            "C. Xiao",
            "Chi Xu",
            "Xinwu Xu",
            "Noelia Zafra‐Calvo",
            "Xin Zhang"
        ],
        "citations": 411,
        "references": 380,
        "year": 2023
    },
    {
        "title": "VideoPrism: A Foundational Visual Encoder for Video Understanding",
        "abstract": "We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 31 out of 33 video understanding benchmarks.",
        "authors": [
            "Long Zhao",
            "N. B. Gundavarapu",
            "Liangzhe Yuan",
            "Hao Zhou",
            "Shen Yan",
            "Jennifer J. Sun",
            "Luke Friedman",
            "Rui Qian",
            "Tobias Weyand",
            "Yue Zhao",
            "Rachel Hornung",
            "Florian Schroff",
            "Ming Yang",
            "David A. Ross",
            "Huisheng Wang",
            "Hartwig Adam",
            "Mikhail Sirotenko",
            "Ting Liu",
            "Boqing Gong"
        ],
        "citations": 18,
        "references": 144,
        "year": 2024
    },
    {
        "title": "Inflammation and aging: signaling pathways and intervention therapies",
        "abstract": null,
        "authors": [
            "Xia Li",
            "Chentao Li",
            "Wanying Zhang",
            "Yanan Wang",
            "P. Qian",
            "He Huang"
        ],
        "citations": 333,
        "references": 478,
        "year": 2023
    },
    {
        "title": "A foundational large language model for edible plant genomes",
        "abstract": "Significant progress has been made in the field of plant genomics, as demonstrated by the increased use of high-throughput methodologies that enable the characterization of multiple genome-wide molecular phenotypes. These findings have provided valuable insights into plant traits and their underlying genetic mechanisms, particularly in model plant species. Nonetheless, effectively leveraging them to make accurate predictions represents a critical step in crop genomic improvement. We present AgroNT, a foundational large language model trained on genomes from 48 plant species with a predominant focus on crop species. We show that AgroNT can obtain state-of-the-art predictions for regulatory annotations, promoter/terminator strength, tissue-specific gene expression, and prioritize functional variants. We conduct a large-scale in silico saturation mutagenesis analysis on cassava to evaluate the regulatory impact of over 10 million mutations and provide their predicted effects as a resource for variant characterization. Finally, we propose the use of the diverse datasets compiled here as the Plants Genomic Benchmark (PGB), providing a comprehensive bench-mark for deep learning-based methods in plant genomic research. The pre-trained AgroNT model is publicly available on HuggingFace at https://huggingface.co/InstaDeepAI/agro-nucleotide-transformer-1b for future research purposes.",
        "authors": [
            "Javier Mendoza-Revilla",
            "Evan Trop",
            "Liam Gonzalez",
            "Maša Roller",
            "Hugo Dalla-torre",
            "B. P. de Almeida",
            "Guillaume Richard",
            "Jonathan Caton",
            "Nicolás López Carranza",
            "Marcin Skwark",
            "Alexandre Laterre",
            "Karim Beguir",
            "Thomas Pierrot",
            "Marie Lopez"
        ],
        "citations": 17,
        "references": 109,
        "year": 2024
    },
    {
        "title": "InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition",
        "abstract": "We propose InternLM-XComposer, a vision-language large model that enables advanced image-text comprehension and composition. The innovative nature of our model is highlighted by three appealing properties: 1) Interleaved Text-Image Composition: InternLM-XComposer can effortlessly generate coherent and contextual articles that seamlessly integrate images, providing a more engaging and immersive reading experience. Simply provide a writing instruction, and our system will generate the corresponding manuscript. It can intelligently identify the areas in the text where images would enhance the content and automatically insert the most appropriate visual candidates. 2) Comprehension with Rich Multilingual Knowledge: The text-image comprehension is empowered by training on an extensive multi-modal multilingual database with carefully crafted strategies, resulting in a deep understanding of visual content. 3) State-of-the-art Performance: Our model consistently achieves state-of-the-art results across various mainstream benchmarks for vision-language foundational models, including MME Benchmark, MMBench, MMBench-CN, Seed-Bench, CCBench (Chinese Cultural Benchmark), QBench and Tiny LVLM. Owing to the absence of established metrics for quantitatively assessing text-image composition, we have devised a robust evaluation procedure that comprises both human and GPT4-Vision (GPT4-V) to ensure reliability. Notably, our InternLM-XComposer achieves competitive text-image composition scores compared to public solutions, including GPT4-V and GPT3.5. Collectively, InternLM-XComposer seamlessly blends advanced text-image comprehension and composition, revolutionizing vision-language interaction and offering new insights and opportunities. The InternLM-XComposer model series are publicly available at https://github.com/InternLM/InternLM-XComposer.",
        "authors": [
            "Pan Zhang",
            "Xiaoyi Wang",
            "Yuhang Cao",
            "Chao Xu",
            "Linke Ouyang",
            "Zhiyuan Zhao",
            "Shuangrui Ding",
            "Songyang Zhang",
            "Haodong Duan",
            "Hang Yan",
            "Xinyu Zhang",
            "Wei Li",
            "Jingwen Li",
            "Kai Chen",
            "Conghui He",
            "Xingcheng Zhang",
            "Y. Qiao",
            "Da Lin",
            "Jiaqi Wang"
        ],
        "citations": 177,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Transformer-Based Visual Segmentation: A Survey",
        "abstract": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research.",
        "authors": [
            "Xiangtai Li",
            "Henghui Ding",
            "Wenwei Zhang",
            "Haobo Yuan",
            "Jiangmiao Pang",
            "Guangliang Cheng",
            "Kai Chen",
            "Ziwei Liu",
            "Chen Change Loy"
        ],
        "citations": 95,
        "references": 419,
        "year": 2023
    },
    {
        "title": "Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems",
        "abstract": "Advances in artificial intelligence (AI) are fueling a new paradigm of discoveries in natural sciences. Today, AI has started to advance natural sciences by improving, accelerating, and enabling our understanding of natural phenomena at a wide range of spatial and temporal scales, giving rise to a new area of research known as AI for science (AI4Science). Being an emerging research paradigm, AI4Science is unique in that it is an enormous and highly interdisciplinary area. Thus, a unified and technical treatment of this field is needed yet challenging. This work aims to provide a technically thorough account of a subarea of AI4Science; namely, AI for quantum, atomistic, and continuum systems. These areas aim at understanding the physical world from the subatomic (wavefunctions and electron density), atomic (molecules, proteins, materials, and interactions), to macro (fluids, climate, and subsurface) scales and form an important subarea of AI4Science. A unique advantage of focusing on these areas is that they largely share a common set of challenges, thereby allowing a unified and foundational treatment. A key common challenge is how to capture physics first principles, especially symmetries, in natural systems by deep learning methods. We provide an in-depth yet intuitive account of techniques to achieve equivariance to symmetry transformations. We also discuss other common technical challenges, including explainability, out-of-distribution generalization, knowledge transfer with foundation and large language models, and uncertainty quantification. To facilitate learning and education, we provide categorized lists of resources that we found to be useful. We strive to be thorough and unified and hope this initial effort may trigger more community interests and efforts to further advance AI4Science.",
        "authors": [
            "Xuan Zhang",
            "Limei Wang",
            "Jacob Helwig",
            "Youzhi Luo",
            "Cong Fu",
            "Yaochen Xie",
            "Meng Liu",
            "Yu-Ching Lin",
            "Zhao Xu",
            "Keqiang Yan",
            "Keir Adams",
            "Maurice Weiler",
            "Xiner Li",
            "Tianfan Fu",
            "Yucheng Wang",
            "Haiyang Yu",
            "Yuqing Xie",
            "Xiang Fu",
            "A. Strasser",
            "Shenglong Xu",
            "Yi Liu",
            "Yuanqi Du",
            "Alexandra Saxton",
            "Hongyi Ling",
            "Hannah Lawrence",
            "Hannes Stärk",
            "Shurui Gui",
            "Carl N. Edwards",
            "Nicholas Gao",
            "A. Ladera",
            "Tailin Wu",
            "E. Hofgard",
            "A. M. Tehrani",
            "Rui Wang",
            "Ameya Daigavane",
            "Montgomery Bohde",
            "Jerry Kurtin",
            "Qiang Huang",
            "Tuong Phung",
            "Minkai Xu",
            "Chaitanya K. Joshi",
            "Simon V. Mathis",
            "K. Azizzadenesheli",
            "Ada Fang",
            "A. Aspuru‐Guzik",
            "E. Bekkers",
            "Michael M. Bronstein",
            "M. Zitnik",
            "Anima Anandkumar",
            "Stefano Ermon",
            "Pietro Lio'",
            "Rose Yu",
            "Stephan Gunnemann",
            "J. Leskovec",
            "Heng Ji",
            "Jimeng Sun",
            "R. Barzilay",
            "T. Jaakkola",
            "Connor W. Coley",
            "Xiaoning Qian",
            "Xiaofeng Qian",
            "T. Smidt",
            "Shuiwang Ji"
        ],
        "citations": 83,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)",
        "abstract": "For a long time, different recommendation tasks require designing task-specific architectures and training objectives. As a result, it is hard to transfer the knowledge and representations from one task to another, thus restricting the generalization ability of existing recommendation approaches. To deal with such issues, considering that language can describe almost anything and language grounding is a powerful medium to represent various problems or tasks, we present a flexible and unified text-to-text paradigm called “Pretrain, Personalized Prompt, and Predict Paradigm” (P5) for recommendation, which unifies various recommendation tasks in a shared framework. In P5, all data such as user-item interactions, user descriptions, item metadata, and user reviews are converted to a common format — natural language sequences. The rich information from natural language assists P5 to capture deeper semantics for personalization and recommendation. Specifically, P5 learns different tasks with the same language modeling objective during pretraining. Thus, it serves as the foundation model for various downstream recommendation tasks, allows easy integration with other modalities, and enables instruction-based recommendation. P5 advances recommender systems from shallow model to deep model to big model, and will revolutionize the technical form of recommender systems towards universal recommendation engine. With adaptive personalized prompt for different users, P5 is able to make predictions in a zero-shot or few-shot manner and largely reduces the necessity for extensive fine-tuning. On several benchmarks, we conduct experiments to show the effectiveness of P5. To help advance future research on Recommendation as Language Processing (RLP), Personalized Foundation Models (PFM), and Universal Recommendation Engine (URE), we release the source code, dataset, prompts, and pretrained P5 model at https://github.com/jeykigung/P5.",
        "authors": [
            "Shijie Geng",
            "Shuchang Liu",
            "Zuohui Fu",
            "Yingqiang Ge",
            "Yongfeng Zhang"
        ],
        "citations": 362,
        "references": 87,
        "year": 2022
    },
    {
        "title": "Advanced Mango Leaf Disease Detection and Severity Analysis with Federated Learning and CNN",
        "abstract": "Worldwide mango production is threatened by mango leaf diseases, resulting in considerable financial losses. Effective illness management and reduction depend on early identification and severity level categorization. This research provides a brand-new FL-CNN model for categorizing mango leaf diseases into four severity categories. The FL-CNN model allows training to be distributed across several clients without compromising data privacy. The suggested model consistently outperforms all clients in identifying the four severity levels of mango leaf diseases. The model's efficiency across customers with various data distributions is shown using macro, weighted, and micro-averaging techniques. Client 4 had the greatest macro average (96.11), weighted average (96.09), and micro average (96.08) values, while Client 1 had the lowest macro average (93.92), weighted average (93.96), and micro average (93.96) values. These findings show that the FL-CNN model successfully recognizes and categorizes various degrees of mango leaf disease severity, making it an invaluable tool for practical agricultural applications. Utilizing federated learning provides data privacy while offering a scalable and safe method for managing plant diseases. Additionally, federated learning preserves data privacy and lessens the need for data centralization, offering a solid foundation for cooperation between many stakeholders. Future studies should focus on enhancing the model's effectiveness and accuracy and investigating whether it can predict additional plant diseases and agricultural situations.",
        "authors": [
            "Shiva Mehta",
            "Vinay Kukreja",
            "Rishika Yadav"
        ],
        "citations": 69,
        "references": 16,
        "year": 2023
    },
    {
        "title": "Decomposing NeRF for Editing via Feature Field Distillation",
        "abstract": "Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations. However, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional. In particular, it has been difficult to selectively edit specific regions or objects. In this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes. We propose to distill the knowledge of off-the-shelf, self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field. Given a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training and enable us to semantically select and edit regions in the radiance field. Our experiments validate that the distilled feature fields (DFFs) can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations.",
        "authors": [
            "Sosuke Kobayashi",
            "Eiichi Matsumoto",
            "Vincent Sitzmann"
        ],
        "citations": 289,
        "references": 132,
        "year": 2022
    },
    {
        "title": "Three types of incremental learning",
        "abstract": null,
        "authors": [
            "Gido M. van de Ven",
            "T. Tuytelaars",
            "A. Tolias"
        ],
        "citations": 322,
        "references": 97,
        "year": 2022
    },
    {
        "title": "Unraveling the nature of nano-diamonds and silica in a catheterized tapered artery: highlights into hydrophilic traits",
        "abstract": null,
        "authors": [
            "S. Abdelsalam",
            "M. M. Bhatti"
        ],
        "citations": 54,
        "references": 31,
        "year": 2023
    },
    {
        "title": "SAM3D: Segment Anything Model in Volumetric Medical Images",
        "abstract": "Image segmentation remains a pivotal component in medical image analysis, aiding in the extraction of critical information for precise diagnostic practices. With the advent of deep learning, automated image segmentation methods have risen to prominence, showcasing exceptional proficiency in processing medical imagery. Motivated by the Segment Anything Model (SAM)—a foundational model renowned for its remarkable precision and robust generalization capabilities in segmenting 2D natural images—we introduce SAM3D, an innovative adaptation tailored for 3D volumetric medical image analysis. Unlike current SAM-based methods that segment volumetric data by converting the volume into separate 2D slices for individual analysis, our SAM3D model processes the entire 3D volume image in a unified approach. Extensive experiments are conducted on multiple medical image datasets to demonstrate that our network attains competitive results compared with other state-of-the-art methods in 3D medical segmentation tasks while being significantly efficient in terms of parameters. Code and checkpoints are available at https://github.com/UARK-AICV/SAM3D.",
        "authors": [
            "Nhat-Tan Bui",
            "Dinh-Hieu Hoang",
            "Minh-Triet Tran",
            "Ngan Le"
        ],
        "citations": 25,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Programmable probiotics modulate inflammation and gut microbiota for inflammatory bowel disease treatment after effective oral delivery",
        "abstract": null,
        "authors": [
            "Jun Zhou",
            "Maoyi Li",
            "Qiufang Chen",
            "Xinjie Li",
            "Linfu Chen",
            "Z. Dong",
            "Wenjun Zhu",
            "Yang Yang",
            "Zhuang Liu",
            "Qian Chen"
        ],
        "citations": 267,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Information criteria for model selection",
        "abstract": "The rapid development of modeling techniques has brought many opportunities for data‐driven discovery and prediction. However, this also leads to the challenge of selecting the most appropriate model for any particular data task. Information criteria, such as the Akaike information criterion (AIC) and Bayesian information criterion (BIC), have been developed as a general class of model selection methods with profound connections with foundational thoughts in statistics and information theory. Many perspectives and theoretical justifications have been developed to understand when and how to use information criteria, which often depend on particular data circumstances. This review article will revisit information criteria by summarizing their key concepts, evaluation metrics, fundamental properties, interconnections, recent advancements, and common misconceptions to enrich the understanding of model selection in general.",
        "authors": [
            "Jiawei Zhang",
            "Yuhong Yang",
            "Jie Ding"
        ],
        "citations": 24,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Overview of the MOSAiC expedition—Atmosphere",
        "abstract": "With the Arctic rapidly changing, the needs to observe, understand, and model the changes are essential. To support these needs, an annual cycle of observations of atmospheric properties, processes, and interactions were made while drifting with the sea ice across the central Arctic during the Multidisciplinary drifting Observatory for the Study of Arctic Climate (MOSAiC) expedition from October 2019 to September 2020. An international team designed and implemented the comprehensive program to document and characterize all aspects of the Arctic atmospheric system in unprecedented detail, using a variety of approaches, and across multiple scales. These measurements were coordinated with other observational teams to explore cross-cutting and coupled interactions with the Arctic Ocean, sea ice, and ecosystem through a variety of physical and biogeochemical processes. This overview outlines the breadth and complexity of the atmospheric research program, which was organized into 4 subgroups: atmospheric state, clouds and precipitation, gases and aerosols, and energy budgets. Atmospheric variability over the annual cycle revealed important influences from a persistent large-scale winter circulation pattern, leading to some storms with pressure and winds that were outside the interquartile range of past conditions suggested by long-term reanalysis. Similarly, the MOSAiC location was warmer and wetter in summer than the reanalysis climatology, in part due to its close proximity to the sea ice edge. The comprehensiveness of the observational program for characterizing and analyzing atmospheric phenomena is demonstrated via a winter case study examining air mass transitions and a summer case study examining vertical atmospheric evolution. Overall, the MOSAiC atmospheric program successfully met its objectives and was the most comprehensive atmospheric measurement program to date conducted over the Arctic sea ice. The obtained data will support a broad range of coupled-system scientific research and provide an important foundation for advancing multiscale modeling capabilities in the Arctic.",
        "authors": [
            "M. Shupe",
            "M. Rex",
            "B. Blomquist",
            "P. Persson",
            "J. Schmale",
            "T. Uttal",
            "D. Althausen",
            "H. Angot",
            "S. Archer",
            "L. Bariteau",
            "Ivo Beck",
            "John Bilberry",
            "S. Bucci",
            "C. Buck",
            "M. Boyer",
            "Zoé Brasseur",
            "I. Brooks",
            "R. Calmer",
            "J. Cassano",
            "Vagner Castro",
            "David Chu",
            "D. Costa",
            "C. Cox",
            "J. Creamean",
            "S. Crewell",
            "S. Dahlke",
            "E. Damm",
            "G. de Boer",
            "H. Deckelmann",
            "K. Dethloff",
            "M. Dütsch",
            "K. Ebell",
            "A. Ehrlich",
            "Jody Ellis",
            "R. Engelmann",
            "A. Fong",
            "M. Frey",
            "Michael R. Gallagher",
            "L. Ganzeveld",
            "R. Gradinger",
            "Jürgen Graeser",
            "Vernon Greenamyer",
            "H. Griesche",
            "Steele Griffiths",
            "Jonathan Hamilton",
            "G. Heinemann",
            "D. Helmig",
            "A. Herber",
            "C. Heuzé",
            "J. Hofer",
            "Todd Houchens",
            "D. Howard",
            "J. Inoue",
            "H. Jacobi",
            "Ralf Jaiser",
            "T. Jokinen",
            "O. Jourdan",
            "Gina C. Jozef",
            "Wessley King",
            "A. Kirchgaessner",
            "M. Klingebiel",
            "M. Krassovski",
            "T. Krumpen",
            "A. Lampert",
            "W. Landing",
            "T. Laurila",
            "D. Lawrence",
            "M. Lonardi",
            "B. Loose",
            "C. Lüpkes",
            "Maximilian Maahn",
            "A. Macke",
            "W. Maslowski",
            "C. Marsay",
            "M. Maturilli",
            "M. Mech",
            "Sara M. Morris",
            "M. Moser",
            "M. Nicolaus",
            "Paul Ortega",
            "J. Osborn",
            "F. Pätzold",
            "D. Perovich",
            "T. Petäjä",
            "C. Pilz",
            "R. Pirazzini",
            "Kevin M. Posman",
            "H. Powers",
            "K. Pratt",
            "A. Preußer",
            "L. Quéléver",
            "M. Radenz",
            "B. Rabe",
            "A. Rinke",
            "T. Sachs",
            "A. Schulz",
            "H. Siebert",
            "Tercio Silva",
            "A. Solomon",
            "Anja Sommerfeld",
            "G. Spreen",
            "M. Stephens",
            "A. Stohl",
            "G. Svensson",
            "J. Uin",
            "J. Viegas",
            "C. Voigt",
            "P. von der Gathen",
            "B. Wehner",
            "J. Welker",
            "M. Wendisch",
            "M. Werner",
            "Z. Xie",
            "Fange Yue"
        ],
        "citations": 196,
        "references": 125,
        "year": 2022
    },
    {
        "title": "A Survey on Active Simultaneous Localization and Mapping: State of the Art and New Frontiers",
        "abstract": "Active simultaneous localization and mapping (SLAM) is the problem of planning and controlling the motion of a robot to build the most accurate and complete model of the surrounding environment. Since the first foundational work in active perception appeared, more than three decades ago, this field has received increasing attention across different scientific communities. This has brought about many different approaches and formulations, and makes a review of the current trends necessary and extremely valuable for both new and experienced researchers. In this article, we survey the state of the art in active SLAM and take an in-depth look at the open challenges that still require attention to meet the needs of modern applications. After providing a historical perspective, we present a unified problem formulation and review the well-established modular solution scheme, which decouples the problem into three stages that identify, select, and execute potential navigation actions. We then analyze alternative approaches, including belief-space planning and deep reinforcement learning techniques, and review related work on multirobot coordination. This article concludes with a discussion of new research directions, addressing reproducible research, active spatial perception, and practical applications, among other topics.",
        "authors": [
            "Julio A. Placed",
            "Jared Strader",
            "Henry Carrillo",
            "Nikolay A. Atanasov",
            "V. Indelman",
            "L. Carlone",
            "J. A. Castellanos"
        ],
        "citations": 143,
        "references": 281,
        "year": 2022
    },
    {
        "title": "UniMath: A Foundational and Multimodal Mathematical Reasoner",
        "abstract": "While significant progress has been made in natural language processing (NLP), existing methods exhibit limitations in effectively interpreting and processing diverse mathematical modalities. Therefore, we introduce UniMath, a versatile and unified system designed for multimodal mathematical reasoning tasks. Tack-ling complex problem-solving in arithmetic, geometry, and table-based math, UniMath utilizes a fine-tuned T5 model augmented with a variational autoencoder (VAE)-based image tokenizer. By jointly training and evaluating the model on three diverse datasets - SVAMP, GeoQA, and TableMWP, UniMath achieves state-of-the-art performance. The model’s generalization ability is further demonstrated via fine-tuning on two additional datasets, MathQA and Geo-Proving. Through comprehensive evaluations, we show that joint training across diverse math tasks improves overall model performance and enhances its ability to generalize across different mathematical reasoning tasks. This pioneering approach provides a blueprint and inspires further efforts on unified mathematical reasoning with deep learning systems.",
        "authors": [
            "Zhenwen Liang",
            "Tianyu Yang",
            "Jipeng Zhang",
            "Xiangliang Zhang"
        ],
        "citations": 16,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Foundational Competencies in Educational Measurement",
        "abstract": "This article presents the consensus of an National Council on Measurement in Education Presidential Task Force on Foundational Competencies in Educational Measurement. Foundational competencies are those that support future development of additional professional and disciplinary competencies. The authors develop a framework for foundational competencies in educational measurement, illustrate how educational measurement programs can help learners develop these competencies, and demonstrate how foundational competencies continue to develop in educational measurement professions. The framework introduces three foundational competency domains: Communication and Collaboration Competencies; Technical, Statistical, and Computational Competencies; and Educational Measurement Competencies. Within the Educational Measurement Competency domain, the authors identify five subdomains: Social, Cultural, Historical, and Political Context; Validity, Validation, and Fairness; Theory and Instrumentation; Precision and Generalization; and Psychometric Modeling.",
        "authors": [
            "Terry A. Ackerman",
            "D. Bandalos",
            "Derek C. Briggs",
            "Howard T. Everson",
            "Andrew D. Ho",
            "Susan M. Lottridge",
            "Matthew J. Madison",
            "S. Sinharay",
            "Michael C. Rodriguez",
            "Michaeline Russell",
            "Alina A. von Davier",
            "Stefanie A. Wind"
        ],
        "citations": 14,
        "references": 16,
        "year": 2023
    },
    {
        "title": "Building bridges to advance the Community of Inquiry framework for online learning",
        "abstract": "Abstract The COVID-19 pandemic forced institutions of higher education around the world to quickly transition to forms of distance education, including synchronous and asynchronous online learning. Often lacking conceptual, empirical, and practical understanding of online pedagogy, many institutions have met this endeavor with mixed success. It seems inevitable that online learning will continue to play a key role in all sectors of education and, accordingly, that online pedagogy deserves a more mainstream focus. To help build a joint understanding of foundational knowledge between the online learning, educational technology, and educational psychology communities, in this article, we summarize the most frequently cited conceptual model that shapes research and practice in the field of higher education online learning: the Community of Inquiry (CoI) framework. We describe the original CoI model and its foundational components (i.e., cognitive, social, and teaching presence) and highlight opportunities for improvement of the model by incorporating the educational psychology and learning sciences research base to inform: (1) conceptualizations of the social dimensions of collaborative learning and (2) understanding of learner contributions to online collaborative education including self-, co-, and shared regulation of learning. We propose that a new, more comprehensive conceptualization of the regulation of collaborative online learning be integrated into the existing CoI framework and that a new “presence” be referenced going forward—“Learning Presence.” Through this work, we strive to develop a more nuanced, generative, and informed vision of the future of online learning informed by relevant contemporary conceptualizations in educational psychology.",
        "authors": [
            "P. Shea",
            "Jennifer C. Richardson",
            "Karen Swan"
        ],
        "citations": 70,
        "references": 135,
        "year": 2022
    },
    {
        "title": "Bayesian Model Selection, the Marginal Likelihood, and Generalization",
        "abstract": "How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We also re-examine the connection between the marginal likelihood and PAC-Bayes bounds and use this connection to further elucidate the shortcomings of the marginal likelihood for model selection. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.",
        "authors": [
            "Sanae Lotfi",
            "Pavel Izmailov",
            "Gregory W. Benton",
            "Micah Goldblum",
            "A. Wilson"
        ],
        "citations": 50,
        "references": 101,
        "year": 2022
    },
    {
        "title": "Don't Stop Learning: Towards Continual Learning for the CLIP Model",
        "abstract": "The Contrastive Language-Image Pre-training (CLIP) Model is a recently proposed large-scale pre-train model which attracts increasing attention in the computer vision community. Benefiting from its gigantic image-text training set, the CLIP model has learned outstanding capabilities in zero-shot learning and image-text matching. To boost the recognition performance of CLIP on some target visual concepts, it is often desirable to further update the CLIP model by fine-tuning some classes-of-interest on extra training data. This operation, however, raises an important concern: will the update hurt the zero-shot learning or image-text matching capability of the CLIP, i.e., the catastrophic forgetting issue? If yes, could existing continual learning algorithms be adapted to alleviate the risk of catastrophic forgetting? To answer these questions, this work conducts a systemic study on the continual learning issue of the CLIP model. We construct evaluation protocols to measure the impact of fine-tuning updates and explore different ways to upgrade existing continual learning methods to mitigate the forgetting issue of the CLIP model. Our study reveals the particular challenges of CLIP continual learning problem and lays a foundation for further researches. Moreover, we propose a new algorithm, dubbed Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact effectiveness for alleviating the forgetting issue of the CLIP model.",
        "authors": [
            "Yuxuan Ding",
            "Lingqiao Liu",
            "Chunna Tian",
            "Jingyuan Yang",
            "Haoxuan Ding"
        ],
        "citations": 44,
        "references": 57,
        "year": 2022
    }
]