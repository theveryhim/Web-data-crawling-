[
    {
        "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
        "abstract": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
        "authors": [
            "Chitwan Saharia",
            "William Chan",
            "Saurabh Saxena",
            "Lala Li",
            "Jay Whang",
            "Emily L. Denton",
            "Seyed Kamyar Seyed Ghasemipour",
            "Burcu Karagol Ayan",
            "S. S. Mahdavi",
            "Raphael Gontijo Lopes",
            "Tim Salimans",
            "Jonathan Ho",
            "David J. Fleet",
            "Mohammad Norouzi"
        ],
        "citations": 1000,
        "references": 108,
        "year": 2022
    },
    {
        "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",
        "authors": [
            "Lvmin Zhang",
            "Anyi Rao",
            "Maneesh Agrawala"
        ],
        "citations": 1000,
        "references": 120,
        "year": 2023
    },
    {
        "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
        "abstract": "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for “personalization” of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/",
        "authors": [
            "Nataniel Ruiz",
            "Yuanzhen Li",
            "Varun Jampani",
            "Y. Pritch",
            "Michael Rubinstein",
            "Kfir Aberman"
        ],
        "citations": 1000,
        "references": 78,
        "year": 2022
    },
    {
        "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
        "abstract": "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models",
        "authors": [
            "Dustin Podell",
            "Zion English",
            "Kyle Lacey",
            "A. Blattmann",
            "Tim Dockhorn",
            "Jonas Muller",
            "Joe Penna",
            "Robin Rombach"
        ],
        "citations": 1000,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Scalable Diffusion Models with Transformers",
        "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
        "authors": [
            "William S. Peebles",
            "Saining Xie"
        ],
        "citations": 1000,
        "references": 68,
        "year": 2022
    },
    {
        "title": "Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets",
        "abstract": "We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .",
        "authors": [
            "A. Blattmann",
            "Tim Dockhorn",
            "Sumith Kulal",
            "Daniel Mendelevitch",
            "Maciej Kilian",
            "Dominik Lorenz"
        ],
        "citations": 616,
        "references": 104,
        "year": 2023
    },
    {
        "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
        "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.",
        "authors": [
            "Zhuoyi Yang",
            "Jiayan Teng",
            "Wendi Zheng",
            "Ming Ding",
            "Shiyu Huang",
            "Jiazheng Xu",
            "Yuanming Yang",
            "Wenyi Hong",
            "Xiaohan Zhang",
            "Guanyu Feng",
            "Da Yin",
            "Xiaotao Gu",
            "Yuxuan Zhang",
            "Weihan Wang",
            "Yean Cheng",
            "Ting Liu",
            "Bin Xu",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "citations": 111,
        "references": 44,
        "year": 2024
    },
    {
        "title": "Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models",
        "abstract": "Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and finetuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution $512 \\times 1024$, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pretrained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to $1280 \\times 2048$. We show that the temporal layers trained in this way generalize to different finetuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/",
        "authors": [
            "A. Blattmann",
            "Robin Rombach",
            "Huan Ling",
            "Tim Dockhorn",
            "Seung Wook Kim",
            "S. Fidler",
            "Karsten Kreis"
        ],
        "citations": 781,
        "references": 121,
        "year": 2023
    },
    {
        "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning",
        "abstract": "With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fine-tuning technique for AnimateDiff that enables a pre-trained motion module to adapt to new motion patterns, such as different shot types, at a low training and data collection cost. We evaluate AnimateDiff and MotionLoRA on several public representative personalized T2I models collected from the community. The results demonstrate that our approaches help these models generate temporally smooth animation clips while preserving the visual quality and motion diversity. Codes and pre-trained weights are available at https://github.com/guoyww/AnimateDiff.",
        "authors": [
            "Yuwei Guo",
            "Ceyuan Yang",
            "Anyi Rao",
            "Yaohui Wang",
            "Y. Qiao",
            "Dahua Lin",
            "Bo Dai"
        ],
        "citations": 529,
        "references": 59,
        "year": 2023
    },
    {
        "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
        "abstract": "Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes:\"an image is worth a thousand words\". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at \\url{https://ip-adapter.github.io}.",
        "authors": [
            "Hu Ye",
            "Jun Zhang",
            "Siyi Liu",
            "Xiao Han",
            "Wei Yang"
        ],
        "citations": 463,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Imagen Video: High Definition Video Generation with Diffusion Models",
        "abstract": "We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.",
        "authors": [
            "Jonathan Ho",
            "William Chan",
            "Chitwan Saharia",
            "Jay Whang",
            "Ruiqi Gao",
            "A. Gritsenko",
            "Diederik P. Kingma",
            "Ben Poole",
            "Mohammad Norouzi",
            "David J. Fleet",
            "Tim Salimans"
        ],
        "citations": 1000,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models",
        "abstract": "Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen --- or excite --- their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts. Code is available at our project page: https://attendandexcite.github.io/Attend-and-Excite/.",
        "authors": [
            "Hila Chefer",
            "Yuval Alaluf",
            "Yael Vinker",
            "Lior Wolf",
            "D. Cohen-Or"
        ],
        "citations": 410,
        "references": 62,
        "year": 2023
    },
    {
        "title": "T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models",
        "abstract": "The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., structure and color) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn low-cost T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications. Our code is available at https://github.com/TencentARC/T2I-Adapter.",
        "authors": [
            "Chong Mou",
            "Xintao Wang",
            "Liangbin Xie",
            "Jing Zhang",
            "Zhongang Qi",
            "Ying Shan",
            "Xiaohu Qie"
        ],
        "citations": 769,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Supplementary Materials for: NULL-text Inversion for Editing Real Images using Guided Diffusion Models",
        "abstract": "Most of the presented results consist of applying our method with the editing technique of Prompt-to-Prompt [4]. However, we demonstrate that our method is not confined to a specific editing approach, by showing it improves the results of the SDEdit [7] editing technique. In Fig. 1 (top), we measure the fidelity to the original image using LPIPS perceptual distance [13] (lower is better), and the fidelity to the target text using CLIP similarity [8] (higher is better) over 100 examples. We use different values of the SDEdit parameter t0 (marked on the curve), i.e., we start the diffusion process from different t = t0 · T using a correspondingly noised input image. This parameter controls the trade-off between fidelity to the input image (low t0) and alignment to the text (high t0). We compare the standard SDEdit to first applying our inversion and then performing SDEdit while replacing the null-text embedding with our optimized embeddings. As shown, our inversion significantly improves the fidelity to the input image. This is visually demonstrated in Fig. 1 (bottom). Since the parameter t0 controls a reconstruction-editability tradeoff, we have used a different parameter for each method (SDEdit with and without our inversion) such that both achieve the same CLIP score. As can be seen, when using our method, the true identity of the baby is well preserved.",
        "authors": [
            "Ron Mokady",
            "Amir Hertz",
            "Kfir Aberman",
            "Y. Pritch",
            "D. Cohen-Or"
        ],
        "citations": 661,
        "references": 14,
        "year": 2023
    },
    {
        "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
        "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.",
        "authors": [
            "Zeqian Ju",
            "Yuancheng Wang",
            "Kai Shen",
            "Xu Tan",
            "Detai Xin",
            "Dongchao Yang",
            "Yanqing Liu",
            "Yichong Leng",
            "Kaitao Song",
            "Siliang Tang",
            "Zhizheng Wu",
            "Tao Qin",
            "Xiang-Yang Li",
            "Wei Ye",
            "Shikun Zhang",
            "Jiang Bian",
            "Lei He",
            "Jinyu Li",
            "Sheng Zhao"
        ],
        "citations": 103,
        "references": 75,
        "year": 2024
    },
    {
        "title": "Erasing Concepts from Diffusion Models",
        "abstract": "Motivated by concerns that large-scale diffusion models can produce undesirable output such as sexually explicit content or copyrighted artistic styles, we study erasure of specific concepts from diffusion model weights. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at erasing.baulab.info.",
        "authors": [
            "Rohit Gandikota",
            "Joanna Materzynska",
            "Jaden Fiotto-Kaufman",
            "David Bau"
        ],
        "citations": 211,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Extracting Training Data from Diffusion Models",
        "abstract": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
        "authors": [
            "Nicholas Carlini",
            "Jamie Hayes",
            "Milad Nasr",
            "Matthew Jagielski",
            "Vikash Sehwag",
            "Florian Tramèr",
            "Borja Balle",
            "Daphne Ippolito",
            "Eric Wallace"
        ],
        "citations": 481,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models",
        "abstract": "We present ODISE: Open-vocabulary DIffusion-based panoptic SEgmentation, which unifies pre-trained text-image diffusion and discriminative models to perform open-vocabulary panoptic segmentation. Text-to-image diffusion models have the remarkable ability to generate high-quality images with diverse open-vocabulary language descriptions. This demonstrates that their internal representation space is highly correlated with open concepts in the real world. Text-image discriminative models like CLIP, on the other hand, are good at classifying images into open-vocabulary labels. We leverage the frozen internal representations of both these models to perform panoptic segmentation of any category in the wild. Our approach outperforms the previous state of the art by significant margins on both open-vocabulary panoptic and semantic segmentation tasks. In particular, with COCO training only, our method achieves 23.4 PQ and 30.0 mIoU on the ADE20K dataset, with 8.3 PQ and 7.9 mIoU absolute improvement over the previous state of the art. We open-source our code and models at https://github.com/NVlabs/ODISE.",
        "authors": [
            "Jiarui Xu",
            "Sifei Liu",
            "Arash Vahdat",
            "Wonmin Byeon",
            "Xiaolong Wang",
            "Shalini De Mello"
        ],
        "citations": 287,
        "references": 105,
        "year": 2023
    },
    {
        "title": "Training Diffusion Models with Reinforcement Learning",
        "abstract": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io .",
        "authors": [
            "Kevin Black",
            "Michael Janner",
            "Yilun Du",
            "Ilya Kostrikov",
            "S. Levine"
        ],
        "citations": 207,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Video Diffusion Models",
        "abstract": "Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/",
        "authors": [
            "Jonathan Ho",
            "Tim Salimans",
            "Alexey Gritsenko",
            "William Chan",
            "Mohammad Norouzi",
            "David J. Fleet"
        ],
        "citations": 1000,
        "references": 81,
        "year": 2022
    },
    {
        "title": "Structure and Content-Guided Video Synthesis with Diffusion Models",
        "abstract": "Text-guided generative diffusion models unlock powerful image creation and editing tools. Recent approaches that edit the content of footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames.In this work, we present a structure and content-guided video diffusion model that edits videos based on descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. A novel guidance method, enabled by joint video and image training, exposes explicit control over temporal consistency. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.",
        "authors": [
            "Patrick Esser",
            "Johnathan Chiu",
            "Parmida Atighehchian",
            "Jonathan Granskog",
            "Anastasis Germanidis"
        ],
        "citations": 414,
        "references": 69,
        "year": 2023
    },
    {
        "title": "VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models",
        "abstract": "Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with mini-mal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.",
        "authors": [
            "Haoxin Chen",
            "Yong Zhang",
            "Xiaodong Cun",
            "Menghan Xia",
            "Xintao Wang",
            "Chao-Liang Weng",
            "Ying Shan"
        ],
        "citations": 158,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators",
        "abstract": "Recent text-to-video generation approaches rely on computationally heavy training and require large-scale video datasets. In this paper, we introduce a new task, zero-shot text-to-video generation, and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g. Stable Diffusion), making them suitable for the video domain. Our key modifications include (i) enriching the latent codes of the generated frames with motion dynamics to keep the global scene and the background time consistent; and (ii) reprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object. Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video generation, and Video Instruct-Pix2Pix, i.e., instruction-guided video editing. As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data. Our code is publicly available at: https://github.com/Picsart-AI-Research/Text2Video-Zero.",
        "authors": [
            "Levon Khachatryan",
            "A. Movsisyan",
            "Vahram Tadevosyan",
            "Roberto Henschel",
            "Zhangyang Wang",
            "Shant Navasardyan",
            "Humphrey Shi"
        ],
        "citations": 416,
        "references": 52,
        "year": 2023
    },
    {
        "title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models",
        "abstract": "Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.",
        "authors": [
            "Haohe Liu",
            "Zehua Chen",
            "Yiitan Yuan",
            "Xinhao Mei",
            "Xubo Liu",
            "Danilo P. Mandic",
            "Wenwu Wang",
            "Mark D. Plumbley"
        ],
        "citations": 417,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Text2Tex: Text-driven Texture Synthesis via Diffusion Models",
        "abstract": "We present Text2Tex, a novel method for generating high-quality textures for 3D meshes from the given text prompts. Our method incorporates inpainting into a pre-trained depth-aware image diffusion model to progressively synthesize high resolution partial textures from multiple viewpoints. To avoid accumulating inconsistent and stretched artifacts across views, we dynamically segment the rendered view into a generation mask, which represents the generation status of each visible texel. This partitioned view representation guides the depth-aware inpainting model to generate and update partial textures for the corresponding regions. Furthermore, we propose an automatic view sequence generation scheme to determine the next best view for updating the partial texture. Extensive experiments demonstrate that our method significantly outperforms the existing text-driven approaches and GAN-based methods.",
        "authors": [
            "Dave Zhenyu Chen",
            "Yawar Siddiqui",
            "Hsin-Ying Lee",
            "S. Tulyakov",
            "M. Nießner"
        ],
        "citations": 156,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Pseudoinverse-Guided Diffusion Models for Inverse Problems",
        "abstract": null,
        "authors": [
            "Jiaming Song",
            "Arash Vahdat",
            "M. Mardani",
            "J. Kautz"
        ],
        "citations": 231,
        "references": 0,
        "year": 2023
    },
    {
        "title": "V3D: Video Diffusion Models are Effective 3D Generators",
        "abstract": "Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at https://github.com/heheyas/V3D",
        "authors": [
            "Zilong Chen",
            "Yikai Wang",
            "Feng Wang",
            "Zhengyi Wang",
            "Huaping Liu"
        ],
        "citations": 42,
        "references": 115,
        "year": 2024
    },
    {
        "title": "MACE: Mass Concept Erasure in Diffusion Models",
        "abstract": "The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of MAss Concept Erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE integrates multiple LoRAs without mutual interference. We conduct extensive evaluations of MACE against prior methods across four different tasks: object erasure, celebrity erasure, explicit content erasure, and artistic style erasure. Our results reveal that MACE surpasses prior methods in all evaluated tasks. Code is available at https://github.com/Shilin-LU/MACE.",
        "authors": [
            "Shilin Lu",
            "Zilan Wang",
            "Leyang Li",
            "Yanzhu Liu",
            "A. Kong"
        ],
        "citations": 34,
        "references": 78,
        "year": 2024
    },
    {
        "title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models",
        "abstract": "Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene. We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model. Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene. These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time. CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation. See our project page for results and interactive demos at https://cat3d.github.io .",
        "authors": [
            "Ruiqi Gao",
            "Aleksander Holynski",
            "Philipp Henzler",
            "Arthur Brussee",
            "Ricardo Martin-Brualla",
            "Pratul P. Srinivasan",
            "Jonathan T. Barron",
            "Ben Poole"
        ],
        "citations": 69,
        "references": 75,
        "year": 2024
    },
    {
        "title": "Detecting, Explaining, and Mitigating Memorization in Diffusion Models",
        "abstract": "Recent breakthroughs in diffusion models have exhibited exceptional image-generation capabilities. However, studies show that some outputs are merely replications of training data. Such replications present potential legal challenges for model owners, especially when the generated content contains proprietary information. In this work, we introduce a straightforward yet effective method for detecting memorized prompts by inspecting the magnitude of text-conditional predictions. Our proposed method seamlessly integrates without disrupting sampling algorithms, and delivers high accuracy even at the first generation step, with a single generation per prompt. Building on our detection strategy, we unveil an explainable approach that shows the contribution of individual words or tokens to memorization. This offers an interactive medium for users to adjust their prompts. Moreover, we propose two strategies i.e., to mitigate memorization by leveraging the magnitude of text-conditional predictions, either through minimization during inference or filtering during training. These proposed strategies effectively counteract memorization while maintaining high-generation quality. Code is available at https://github.com/YuxinWenRick/diffusion_memorization.",
        "authors": [
            "Yuxin Wen",
            "Yuchen Liu",
            "Chen Chen",
            "Lingjuan Lyu"
        ],
        "citations": 30,
        "references": 30,
        "year": 2024
    },
    {
        "title": "Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation",
        "abstract": "We introduce Score identity Distillation (SiD), an innovative data-free method that distills the generative capabilities of pretrained diffusion models into a single-step generator. SiD not only facilitates an exponentially fast reduction in Fr\\'echet inception distance (FID) during distillation but also approaches or even exceeds the FID performance of the original teacher diffusion models. By reformulating forward diffusion processes as semi-implicit distributions, we leverage three score-related identities to create an innovative loss mechanism. This mechanism achieves rapid FID reduction by training the generator using its own synthesized images, eliminating the need for real data or reverse-diffusion-based generation, all accomplished within significantly shortened generation time. Upon evaluation across four benchmark datasets, the SiD algorithm demonstrates high iteration efficiency during distillation and surpasses competing distillation approaches, whether they are one-step or few-step, data-free, or dependent on training data, in terms of generation quality. This achievement not only redefines the benchmarks for efficiency and effectiveness in diffusion distillation but also in the broader field of diffusion-based generation. The PyTorch implementation is available at https://github.com/mingyuanzhou/SiD",
        "authors": [
            "Mingyuan Zhou",
            "Huangjie Zheng",
            "Zhendong Wang",
            "Mingzhang Yin",
            "Hai Huang"
        ],
        "citations": 30,
        "references": 88,
        "year": 2024
    },
    {
        "title": "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models",
        "abstract": "Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own COrrelation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also achieves SOTA video generation quality on the small-scale UCF-101 benchmark with a 10× smaller model using significantly less computation than the prior art. The project page is available at https://research.nvidia.com/labs/dir/pyoco/.",
        "authors": [
            "Songwei Ge",
            "Seungjun Nah",
            "Guilin Liu",
            "Tyler Poon",
            "Andrew Tao",
            "Bryan Catanzaro",
            "David Jacobs",
            "Jia-Bin Huang",
            "Ming-Yu Liu",
            "Y. Balaji"
        ],
        "citations": 193,
        "references": 73,
        "year": 2023
    },
    {
        "title": "VideoCrafter1: Open Diffusion Models for High-Quality Video Generation",
        "abstract": "Video generation has increasingly gained interest in both academia and industry. Although commercial tools can generate plausible videos, there is a limited number of open-source models available for researchers and engineers. In this work, we introduce two diffusion models for high-quality video generation, namely text-to-video (T2V) and image-to-video (I2V) models. T2V models synthesize a video based on a given text input, while I2V models incorporate an additional image input. Our proposed T2V model can generate realistic and cinematic-quality videos with a resolution of $1024 \\times 576$, outperforming other open-source T2V models in terms of quality. The I2V model is designed to produce videos that strictly adhere to the content of the provided reference image, preserving its content, structure, and style. This model is the first open-source I2V foundation model capable of transforming a given image into a video clip while maintaining content preservation constraints. We believe that these open-source video generation models will contribute significantly to the technological advancements within the community.",
        "authors": [
            "Haoxin Chen",
            "Menghan Xia",
            "Yin-Yin He",
            "Yong Zhang",
            "Xiaodong Cun",
            "Shaoshu Yang",
            "Jinbo Xing",
            "Yaofang Liu",
            "Qifeng Chen",
            "Xintao Wang",
            "Chao-Liang Weng",
            "Ying Shan"
        ],
        "citations": 193,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models",
        "abstract": "Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance.",
        "authors": [
            "T. Kynkäänniemi",
            "M. Aittala",
            "Tero Karras",
            "S. Laine",
            "Timo Aila",
            "J. Lehtinen"
        ],
        "citations": 24,
        "references": 42,
        "year": 2024
    },
    {
        "title": "DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models",
        "abstract": "Diffusion models have achieved great success in synthesizing high-quality images. However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a pro-hibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However,naïvely implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous commu-nication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a 6.1 ×speedup on eight A100 GPUs compared to one.",
        "authors": [
            "Muyang Li",
            "Tianle Cai",
            "Jiaxin Cao",
            "Qinsheng Zhang",
            "Han Cai",
            "Junjie Bai",
            "Yangqing Jia",
            "Ming-Yu Liu",
            "Kai Li",
            "Song Han"
        ],
        "citations": 27,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Dynamical regimes of diffusion models",
        "abstract": null,
        "authors": [
            "Giulio Biroli",
            "Tony Bonnaire",
            "Valentin De Bortoli",
            "Marc M'ezard"
        ],
        "citations": 23,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Erasing Undesirable Influence in Diffusion Models",
        "abstract": "Diffusion models are highly effective at generating high-quality images but pose risks, such as the unintentional generation of NSFW (not safe for work) content. Although various techniques have been proposed to mitigate unwanted influences in diffusion models while preserving overall performance, achieving a balance between these goals remains challenging. In this work, we introduce EraseDiff, an algorithm designed to preserve the utility of the diffusion model on retained data while removing the unwanted information associated with the data to be forgotten. Our approach formulates this task as a constrained optimization problem using the value function, resulting in a natural first-order algorithm for solving the optimization problem. By altering the generative process to deviate away from the ground-truth denoising trajectory, we update parameters for preservation while controlling constraint reduction to ensure effective erasure, striking an optimal trade-off. Extensive experiments and thorough comparisons with state-of-the-art algorithms demonstrate that EraseDiff effectively preserves the model's utility, efficacy, and efficiency.",
        "authors": [
            "Jing Wu",
            "Trung Le",
            "Munawar Hayat",
            "Mehrtash Harandi"
        ],
        "citations": 18,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Better Diffusion Models Further Improve Adversarial Training",
        "abstract": "It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\\ell_\\infty$-norm threat model with $\\epsilon=8/255$, our models achieve $70.69\\%$ and $42.67\\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\\%$ and $+8.03\\%$. Under the $\\ell_2$-norm threat model with $\\epsilon=128/255$, our models achieve $84.86\\%$ on CIFAR-10 ($+4.44\\%$). These results also beat previous works that use external data. We also provide compelling results on the SVHN and TinyImageNet datasets. Our code is available at https://github.com/wzekai99/DM-Improves-AT.",
        "authors": [
            "Zekai Wang",
            "Tianyu Pang",
            "Chao Du",
            "Min Lin",
            "Weiwei Liu",
            "Shuicheng Yan"
        ],
        "citations": 169,
        "references": 112,
        "year": 2023
    },
    {
        "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications",
        "abstract": "Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy",
        "authors": [
            "Ling Yang",
            "Zhilong Zhang",
            "Shenda Hong",
            "Runsheng Xu",
            "Yue Zhao",
            "Yingxia Shao",
            "Wentao Zhang",
            "Ming-Hsuan Yang",
            "Bin Cui"
        ],
        "citations": 1000,
        "references": 394,
        "year": 2022
    },
    {
        "title": "DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design",
        "abstract": "Designing 3D ligands within a target binding site is a fundamental task in drug discovery. Existing structured-based drug design methods treat all ligand atoms equally, which ignores different roles of atoms in the ligand for drug design and can be less efficient for exploring the large drug-like molecule space. In this paper, inspired by the convention in pharmaceutical practice, we decompose the ligand molecule into two parts, namely arms and scaffold, and propose a new diffusion model, DecompDiff, with decomposed priors over arms and scaffold. In order to facilitate the decomposed generation and improve the properties of the generated molecules, we incorporate both bond diffusion in the model and additional validity guidance in the sampling phase. Extensive experiments on CrossDocked2020 show that our approach achieves state-of-the-art performance in generating high-affinity molecules while maintaining proper molecular properties and conformational stability, with up to -8.39 Avg. Vina Dock score and 24.5 Success Rate. The code is provided at https://github.com/bytedance/DecompDiff",
        "authors": [
            "Jiaqi Guan",
            "Xiangxin Zhou",
            "Yuwei Yang",
            "Yu Bao",
            "Jian-wei Peng",
            "Jianzhu Ma",
            "Q. Liu",
            "Liang Wang",
            "Quanquan Gu"
        ],
        "citations": 44,
        "references": 68,
        "year": 2024
    },
    {
        "title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
        "abstract": "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's\"paint-with-words\"capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/",
        "authors": [
            "Y. Balaji",
            "Seungjun Nah",
            "Xun Huang",
            "Arash Vahdat",
            "Jiaming Song",
            "Qinsheng Zhang",
            "Karsten Kreis",
            "M. Aittala",
            "Timo Aila",
            "S. Laine",
            "Bryan Catanzaro",
            "Tero Karras",
            "Ming-Yu Liu"
        ],
        "citations": 708,
        "references": 90,
        "year": 2022
    },
    {
        "title": "Unleashing Text-to-Image Diffusion Models for Visual Perception",
        "abstract": "Diffusion models (DMs) have become the new trend of generative models and have demonstrated a powerful ability of conditional synthesis. Among those, text-to-image diffusion models pre-trained on large-scale image-text pairs are highly controllable by customizable prompts. Unlike the unconditional generative models that focus on low-level attributes and details, text-to-image diffusion models contain more high-level knowledge thanks to the vision-language pre-training. In this paper, we propose VPD (Visual Perception with pre-trained Diffusion models), a new framework that exploits the semantic information of a pre-trained text-to-image diffusion model in visual perception tasks. Instead of using the pre-trained denoising autoencoder in a diffusion-based pipeline, we simply use it as a backbone and aim to study how to take full advantage of the learned knowledge. Specifically, we prompt the denoising decoder with proper textual inputs and refine the text features with an adapter, leading to a better alignment to the pre-trained stage and making the visual contents interact with the text prompts. We also propose to utilize the cross-attention maps between the visual features and the text features to provide explicit guidance. Compared with other pre-training methods, we show that vision-language pre-trained diffusion models can be faster adapted to downstream visual perception tasks using the proposed VPD. Extensive experiments on semantic segmentation, referring image segmentation, and depth estimation demonstrate the effectiveness of our method. Notably, VPD attains 0.254 RMSE on NYUv2 depth estimation and 73.3% oIoU on RefCOCO-val referring image segmentation, establishing new records on these two benchmarks. Code is available at https://github.com/wl-zhao/VPD.",
        "authors": [
            "Wenliang Zhao",
            "Yongming Rao",
            "Zuyan Liu",
            "Benlin Liu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "citations": 165,
        "references": 60,
        "year": 2023
    },
    {
        "title": "An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization",
        "abstract": "Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models.",
        "authors": [
            "Minshuo Chen",
            "Song Mei",
            "Jianqing Fan",
            "Mengdi Wang"
        ],
        "citations": 31,
        "references": 211,
        "year": 2024
    },
    {
        "title": "VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation",
        "abstract": "A diffusion probabilistic model (DPM), which constructs a forward diffusion process by gradually adding noise to data points and learns the reverse denoising process to generate new samples, has been shown to handle complex data distribution. Despite its recent success in image synthesis, applying DPMs to video generation is still challenging due to high-dimensional data spaces. Previous methods usually adopt a standard diffusion process, where frames in the same video clip are destroyed with independent noises, ignoring the content redundancy and temporal correlation. This work presents a decomposed diffusion process via resolving the per-frame noise into a base noise that is shared among all frames and a residual noise that varies along the time axis. The denoising pipeline employs two jointly-learned networks to match the noise decomposition accordingly. Experiments on various datasets confirm that our approach, termed as VideoFusion, surpasses both GAN-based and diffusion-based alternatives in high-quality video generation. We further show that our decomposed formulation can benefit from pre-trained image diffusion models and well-support text-conditioned video creation.",
        "authors": [
            "Zhengxiong Luo",
            "Dayou Chen",
            "Yingya Zhang",
            "Yan Huang",
            "Liangsheng Wang",
            "Yujun Shen",
            "Deli Zhao",
            "Jinren Zhou",
            "Tien-Ping Tan"
        ],
        "citations": 242,
        "references": 53,
        "year": 2023
    },
    {
        "title": "LatentPaint: Image Inpainting in Latent Space with Diffusion Models",
        "abstract": "Image inpainting using diffusion models is generally done using either preconditioned models, i.e. image conditioned models fine-tuned for the painting task, or postconditioned models, i.e. unconditioned models repurposed for the painting task at inference time. Preconditioned models are fast at inference time but extremely costly to train. Postconditioned models do not require any training but are slow during inference, requiring multiple forward and backward passes to converge to a desirable solution. Here, we derive an approach that does not require expensive training, yet is fast at inference time. To solve the costly inference computational time, we perform the forward-backward fusion step on a latent space rather than the image space. This is solved with a newly proposed propagation module in the diffusion process. Experiments on a number of domains demonstrate our approach attains or improves state-of-the-art results with the advantages of preconditioned and postconditioned models and none of their disadvantages.",
        "authors": [
            "C. Corneanu",
            "Raghudeep Gadde",
            "Aleix M. Martínez"
        ],
        "citations": 29,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Scalable Diffusion Models with State Space Backbone",
        "abstract": "This paper presents a new exploration into a category of diffusion models built upon state space architecture. We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space. Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens. Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to CNN-based or Transformer-based U-Net architectures of commensurate size. Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops. DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID. In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet benchmarks at the resolution of 256$\\times$256 and 512$\\times$512, while significantly reducing the computational burden. The code and models are available at: https://github.com/feizc/DiS.",
        "authors": [
            "Zhengcong Fei",
            "Mingyuan Fan",
            "Changqian Yu",
            "Junshi Huang"
        ],
        "citations": 30,
        "references": 73,
        "year": 2024
    },
    {
        "title": "Imagic: Text-Based Real Image Editing with Diffusion Models",
        "abstract": "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. – each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench.",
        "authors": [
            "Bahjat Kawar",
            "Shiran Zada",
            "Oran Lang",
            "Omer Tov",
            "Hui-Tang Chang",
            "Tali Dekel",
            "Inbar Mosseri",
            "M. Irani"
        ],
        "citations": 883,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Measuring Style Similarity in Diffusion Models",
        "abstract": "Generative models are now widely used by graphic designers and artists. Prior works have shown that these models remember and often replicate content from their training data during generation. Hence as their proliferation increases, it has become important to perform a database search to determine whether the properties of the image are attributable to specific training data, every time before a generated image is used for professional purposes. Existing tools for this purpose focus on retrieving images of similar semantic content. Meanwhile, many artists are concerned with style replication in text-to-image models. We present a framework for understanding and extracting style descriptors from images. Our framework comprises a new dataset curated using the insight that style is a subjective property of an image that captures complex yet meaningful interactions of factors including but not limited to colors, textures, shapes, etc. We also propose a method to extract style descriptors that can be used to attribute style of a generated image to the images used in the training dataset of a text-to-image model. We showcase promising results in various style retrieval tasks. We also quantitatively and qualitatively analyze style attribution and matching in the Stable Diffusion model. Code and artifacts are available at https://github.com/learn2phoenix/CSD.",
        "authors": [
            "Gowthami Somepalli",
            "Anubhav Gupta",
            "Kamal K. Gupta",
            "Shramay Palta",
            "Micah Goldblum",
            "Jonas Geiping",
            "Abhinav Shrivastava",
            "Tom Goldstein"
        ],
        "citations": 23,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation",
        "abstract": "To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting—One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.",
        "authors": [
            "Jay Zhangjie Wu",
            "Yixiao Ge",
            "Xintao Wang",
            "Weixian Lei",
            "Yuchao Gu",
            "W. Hsu",
            "Ying Shan",
            "Xiaohu Qie",
            "Mike Zheng Shou"
        ],
        "citations": 555,
        "references": 71,
        "year": 2022
    },
    {
        "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
        "abstract": "This paper presents a novel method for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 90% of the time.",
        "authors": [
            "Junlin Han",
            "Filippos Kokkinos",
            "Philip Torr"
        ],
        "citations": 27,
        "references": 73,
        "year": 2024
    },
    {
        "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning",
        "abstract": "In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.",
        "authors": [
            "Xinlei Chen",
            "Zhuang Liu",
            "Saining Xie",
            "Kaiming He"
        ],
        "citations": 28,
        "references": 43,
        "year": 2024
    },
    {
        "title": "Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control",
        "abstract": "Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth\"genuine\"reward, as is the case in many practical applications. These challenges, collectively termed\"reward collapse,\"pose a substantial obstacle. To address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural SDEs. We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.",
        "authors": [
            "Masatoshi Uehara",
            "Yulai Zhao",
            "Kevin Black",
            "Ehsan Hajiramezanali",
            "Gabriele Scalia",
            "N. Diamant",
            "Alex Tseng",
            "Tommaso Biancalani",
            "Sergey Levine"
        ],
        "citations": 26,
        "references": 68,
        "year": 2024
    },
    {
        "title": "Video Interpolation with Diffusion Models",
        "abstract": "We present VIDIM, a generative model for video inter-polation, which creates short videos given a start and end frame. In order to achieve high fidelity and generate motions unseen in the input data, VIDIM uses cascaded diffusion models to first generate the target video at low resolution, and then generate the high-resolution video conditioned on the low-resolution generated video. We compare VIDIM to previous state-of-the-art methods on video interpolation, and demonstrate how such works fail in most settings where the underlying motion is complex, nonlinear, or ambiguous while VIDIM can easily handle such cases. We additionally demonstrate how classifier-free guidance on the start and end frame and conditioning the super-resolution model on the original high-resolution frames without additional parameters unlocks high-fidelity results. VIDIM is fast to sample from as it jointly denoises all the frames to be generated, requires less than a billion parameters per diffusion model to produce compelling results, and still enjoys scalability and improved quality at larger parameter counts. Please see our project page at vidim-interpolation.github.io.",
        "authors": [
            "Siddhant Jain",
            "Daniel Watson",
            "Eric Tabellion",
            "Aleksander Holynski",
            "Ben Poole",
            "Janne Kontkanen"
        ],
        "citations": 22,
        "references": 63,
        "year": 2024
    },
    {
        "title": "Tutorial on Diffusion Models for Imaging and Vision",
        "abstract": "The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.",
        "authors": [
            "Stanley H. Chan"
        ],
        "citations": 15,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Pseudo Numerical Methods for Diffusion Models on Manifolds",
        "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce final samples. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at a high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a fresh perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that the pseudo linear multi-step method is the best in most situations. According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules. Our implementation is available at https://github.com/luping-liu/PNDM.",
        "authors": [
            "Luping Liu",
            "Yi Ren",
            "Zhijie Lin",
            "Zhou Zhao"
        ],
        "citations": 528,
        "references": 35,
        "year": 2022
    },
    {
        "title": "Generative emulation of weather forecast ensembles with diffusion models",
        "abstract": "Uncertainty quantification is crucial to decision-making. A prominent example is probabilistic forecasting in numerical weather prediction. The dominant approach to representing uncertainty in weather forecasting is to generate an ensemble of forecasts by running physics-based simulations under different conditions, which is a computationally costly process. We propose to amortize the computational cost by emulating these forecasts with deep generative diffusion models learned from historical data. The learned models are highly scalable with respect to high-performance computing accelerators and can sample thousands of realistic weather forecasts at low cost. When designed to emulate operational ensemble forecasts, the generated ones are similar to physics-based ensembles in statistical properties and predictive skill. When designed to correct biases present in the operational forecasting system, the generated ensembles show improved probabilistic forecast metrics. They are more reliable and forecast probabilities of extreme weather events more accurately. While we focus on weather forecasting, this methodology may enable creating large climate projection ensembles for climate risk assessment.",
        "authors": [
            "Lizao Li",
            "Rob Carver",
            "I. Lopez‐Gomez",
            "Fei Sha",
            "John Anderson"
        ],
        "citations": 25,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Diffusion Models for Adversarial Purification",
        "abstract": "Adversarial purification refers to a class of defense methods that remove adversarial perturbations using a generative model. These methods do not make assumptions on the form of attack and the classification model, and thus can defend pre-existing classifiers against unseen threats. However, their performance currently falls behind adversarial training methods. In this work, we propose DiffPure that uses diffusion models for adversarial purification: Given an adversarial example, we first diffuse it with a small amount of noise following a forward diffusion process, and then recover the clean image through a reverse generative process. To evaluate our method against strong adaptive attacks in an efficient and scalable way, we propose to use the adjoint method to compute full gradients of the reverse generative process. Extensive experiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQ with three classifier architectures including ResNet, WideResNet and ViT demonstrate that our method achieves the state-of-the-art results, outperforming current adversarial training and adversarial purification methods, often by a large margin. Project page: https://diffpure.github.io.",
        "authors": [
            "Weili Nie",
            "Brandon Guo",
            "Yujia Huang",
            "Chaowei Xiao",
            "Arash Vahdat",
            "Anima Anandkumar"
        ],
        "citations": 342,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Large-scale Reinforcement Learning for Diffusion Models",
        "abstract": "Text-to-image diffusion models are a class of deep generative models that have demonstrated an impressive capacity for high-quality image generation. However, these models are susceptible to implicit biases that arise from web-scale text-image training pairs and may inaccurately model aspects of images we care about. This can result in suboptimal samples, model bias, and images that do not align with human ethics and preferences. In this paper, we present an effective scalable algorithm to improve diffusion models using Reinforcement Learning (RL) across a diverse set of reward functions, such as human preference, compositionality, and fairness over millions of images. We illustrate how our approach substantially outperforms existing methods for aligning diffusion models with human preferences. We further illustrate how this substantially improves pretrained Stable Diffusion (SD) models, generating samples that are preferred by humans 80.3% of the time over those from the base SD model while simultaneously improving both the composition and diversity of generated samples.",
        "authors": [
            "Yinan Zhang",
            "Eric Tzeng",
            "Yilun Du",
            "Dmitry Kislyuk"
        ],
        "citations": 21,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
        "abstract": "In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \\url{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}.",
        "authors": [
            "Pu Cao",
            "Feng Zhou",
            "Qing Song",
            "Lu Yang"
        ],
        "citations": 20,
        "references": 249,
        "year": 2024
    },
    {
        "title": "Ablating Concepts in Text-to-Image Diffusion Models",
        "abstract": "Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model.",
        "authors": [
            "Nupur Kumari",
            "Bin Zhang",
            "Sheng-Yu Wang",
            "Eli Shechtman",
            "Richard Zhang",
            "Jun-Yan Zhu"
        ],
        "citations": 133,
        "references": 84,
        "year": 2023
    },
    {
        "title": "The Stable Signature: Rooting Watermarks in Latent Diffusion Models",
        "abstract": "Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. We introduce an active content tracing method combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that the Stable Signature is robust to image modifications. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep 10% of the content, with 90+% accuracy at a false positive rate below 10−6.",
        "authors": [
            "Pierre Fernandez",
            "Guillaume Couairon",
            "Herv'e J'egou",
            "Matthijs Douze",
            "T. Furon"
        ],
        "citations": 126,
        "references": 106,
        "year": 2023
    },
    {
        "title": "Compositional Visual Generation with Composable Diffusion Models",
        "abstract": "Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/",
        "authors": [
            "Nan Liu",
            "Shuang Li",
            "Yilun Du",
            "A. Torralba",
            "J. Tenenbaum"
        ],
        "citations": 429,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Unified Concept Editing in Diffusion Models",
        "abstract": "Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models.We present scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and perform extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at unified.baulab.info.",
        "authors": [
            "Rohit Gandikota",
            "Hadas Orgad",
            "Yonatan Belinkov",
            "Joanna Materzy'nska",
            "David Bau"
        ],
        "citations": 111,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models",
        "abstract": "Public large-scale text-to-image diffusion models, such as Stable Diffusion, have gained significant attention from the community. These models can be easily customized for new concepts using low-rank adaptations (LoRAs). However, the utilization of multiple concept LoRAs to jointly support multiple customized concepts presents a challenge. We refer to this scenario as decentralized multi-concept customization, which involves single-client concept tuning and center-node concept fusion. In this paper, we propose a new framework called Mix-of-Show that addresses the challenges of decentralized multi-concept customization, including concept conflicts resulting from existing single-client LoRA tuning and identity loss during model fusion. Mix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client tuning and gradient fusion for the center node to preserve the in-domain essence of single concepts and support theoretically limitless concept fusion. Additionally, we introduce regionally controllable sampling, which extends spatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address attribute binding and missing object problems in multi-concept sampling. Extensive experiments demonstrate that Mix-of-Show is capable of composing multiple customized concepts with high fidelity, including characters, objects, and scenes.",
        "authors": [
            "Yuchao Gu",
            "Xintao Wang",
            "Jay Zhangjie Wu",
            "Yujun Shi",
            "Yunpeng Chen",
            "Zihan Fan",
            "Wuyou Xiao",
            "Rui Zhao",
            "Shuning Chang",
            "Wei Wu",
            "Yixiao Ge",
            "Ying Shan",
            "Mike Zheng Shou"
        ],
        "citations": 120,
        "references": 51,
        "year": 2023
    },
    {
        "title": "LION: Latent Point Diffusion Models for 3D Shape Generation",
        "abstract": "Denoising diffusion models (DDMs) have shown promising results in 3D point cloud synthesis. To advance 3D DDMs and make them useful for digital artists, we require (i) high generation quality, (ii) flexibility for manipulation and applications such as conditional synthesis and shape interpolation, and (iii) the ability to output smooth surfaces or meshes. To this end, we introduce the hierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION is set up as a variational autoencoder (VAE) with a hierarchical latent space that combines a global shape latent representation with a point-structured latent space. For generation, we train two hierarchical DDMs in these latent spaces. The hierarchical VAE approach boosts performance compared to DDMs that operate on point clouds directly, while the point-structured latents are still ideally suited for DDM-based modeling. Experimentally, LION achieves state-of-the-art generation performance on multiple ShapeNet benchmarks. Furthermore, our VAE framework allows us to easily use LION for different relevant tasks: LION excels at multimodal shape denoising and voxel-conditioned synthesis, and it can be adapted for text- and image-driven 3D generation. We also demonstrate shape autoencoding and latent shape interpolation, and we augment LION with modern surface reconstruction techniques to generate smooth 3D meshes. We hope that LION provides a powerful tool for artists working with 3D shapes due to its high-quality generation, flexibility, and surface reconstruction. Project page and code: https://nv-tlabs.github.io/LION.",
        "authors": [
            "Xiaohui Zeng",
            "Arash Vahdat",
            "Francis Williams",
            "Zan Gojcic",
            "O. Litany",
            "S. Fidler",
            "Karsten Kreis"
        ],
        "citations": 407,
        "references": 147,
        "year": 2022
    },
    {
        "title": "Fast Sampling of Diffusion Models with Exponential Integrator",
        "abstract": "The past few years have witnessed the great success of Diffusion models~(DMs) in generating high-fidelity samples in generative modeling tasks. A major limitation of the DM is its notoriously slow sampling procedure which normally requires hundreds to thousands of time discretization steps of the learned diffusion process to reach the desired accuracy. Our goal is to develop a fast sampling method for DMs with a much less number of steps while retaining high sample quality. To this end, we systematically analyze the sampling procedure in DMs and identify key factors that affect the sample quality, among which the method of discretization is most crucial. By carefully examining the learned diffusion process, we propose Diffusion Exponential Integrator Sampler~(DEIS). It is based on the Exponential Integrator designed for discretizing ordinary differential equations (ODEs) and leverages a semilinear structure of the learned diffusion process to reduce the discretization error. The proposed method can be applied to any DMs and can generate high-fidelity samples in as few as 10 steps. In our experiments, it takes about 3 minutes on one A6000 GPU to generate $50k$ images from CIFAR10. Moreover, by directly using pre-trained DMs, we achieve the state-of-art sampling performance when the number of score function evaluation~(NFE) is limited, e.g., 4.17 FID with 10 NFEs, 3.37 FID, and 9.74 IS with only 15 NFEs on CIFAR10. Code is available at https://github.com/qsh-zh/deis",
        "authors": [
            "Qinsheng Zhang",
            "Yongxin Chen"
        ],
        "citations": 345,
        "references": 61,
        "year": 2022
    },
    {
        "title": "Improving Diffusion Models for Inverse Problems using Manifold Constraints",
        "abstract": "Recently, diffusion models have been used to solve various inverse problems in an unsupervised manner with appropriate modifications to the sampling process. However, the current solvers, which recursively apply a reverse diffusion step followed by a projection-based measurement consistency step, often produce suboptimal results. By studying the generative sampling path, here we show that current solvers throw the sample path off the data manifold, and hence the error accumulates. To address this, we propose an additional correction term inspired by the manifold constraint, which can be used synergistically with the previous solvers to make the iterations close to the manifold. The proposed manifold constraint is straightforward to implement within a few lines of code, yet boosts the performance by a surprisingly large margin. With extensive experiments, we show that our method is superior to the previous methods both theoretically and empirically, producing promising results in many applications such as image inpainting, colorization, and sparse-view computed tomography. Code available https://github.com/HJ-harry/MCG_diffusion",
        "authors": [
            "Hyungjin Chung",
            "Byeongsu Sim",
            "Dohoon Ryu",
            "J. C. Ye"
        ],
        "citations": 342,
        "references": 62,
        "year": 2022
    },
    {
        "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
        "abstract": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \\url{https://github.com/Shark-NLP/DiffuSeq}",
        "authors": [
            "Shansan Gong",
            "Mukai Li",
            "Jiangtao Feng",
            "Zhiyong Wu",
            "Lingpeng Kong"
        ],
        "citations": 264,
        "references": 53,
        "year": 2022
    },
    {
        "title": "DragDiffusion: Harnessing Diffusion Models for Interactive Point-Based Image Editing",
        "abstract": "Accurate and controllable image editing is a challenging task that has attracted significant attention recently. Notably, DRAGGAN developed by Pan et al. (2023) [33] is an interactive point-based image editing framework that achieves impressive editing results with pixel-level precision. However, due to its reliance on generative adversarial networks (GANs), its generality is limited by the capacity of pretrained GAN models. In this work, we extend this editing framework to diffusion models and propose a novel approach Dragdiffusion. By harnessing large-scale pretrained diffusion models, we greatly enhance the applicability of interactive point-based editing on both real and diffusion-generated images. Unlike other diffusion-based editing methods that provide guidance on diffusion latents of multiple time steps, our approach achieves efficient yet accurate spatial control by optimizing the latent of only one time step. This novel design is motivated by our observations that UNet features at a specific time step provides sufficient semantic and geometric information to support the drag-based editing. Moreover, we introduce two additional techniques, namely identity-preserving fine-tuning and reference-latent-control, to further preserve the identity of the original image. Lastly, we present a challenging benchmark dataset called DRAGBENCH─ the first benchmark to evaluate the performance of interactive point-based image editing methods. Experiments across a wide range of challenging cases (e.g., images with multiple objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of Dragdiffusion. Code and the Dragbench dataset: https://github.com/Yujun-Shi/DragDiffusion.",
        "authors": [
            "Yujun Shi",
            "Chuhui Xue",
            "Jiachun Pan",
            "Wenqing Zhang",
            "Vincent Y. F. Tan",
            "Song Bai"
        ],
        "citations": 136,
        "references": 62,
        "year": 2023
    },
    {
        "title": "I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models",
        "abstract": "Video synthesis has recently made remarkable strides benefiting from the rapid development of diffusion models. However, it still encounters challenges in terms of semantic accuracy, clarity and spatio-temporal continuity. They primarily arise from the scarcity of well-aligned text-video data and the complex inherent structure of videos, making it difficult for the model to simultaneously ensure semantic and qualitative excellence. In this report, we propose a cascaded I2VGen-XL approach that enhances model performance by decoupling these two factors and ensures the alignment of the input data by utilizing static images as a form of crucial guidance. I2VGen-XL consists of two stages: i) the base stage guarantees coherent semantics and preserves content from input images by using two hierarchical encoders, and ii) the refinement stage enhances the video's details by incorporating an additional brief text and improves the resolution to 1280$\\times$720. To improve the diversity, we collect around 35 million single-shot text-video pairs and 6 billion text-image pairs to optimize the model. By this means, I2VGen-XL can simultaneously enhance the semantic accuracy, continuity of details and clarity of generated videos. Through extensive experiments, we have investigated the underlying principles of I2VGen-XL and compared it with current top methods, which can demonstrate its effectiveness on diverse data. The source code and models will be publicly available at \\url{https://i2vgen-xl.github.io}.",
        "authors": [
            "Shiwei Zhang",
            "Jiayu Wang",
            "Yingya Zhang",
            "Kang Zhao",
            "Hangjie Yuan",
            "Z. Qin",
            "Xiang Wang",
            "Deli Zhao",
            "Jingren Zhou"
        ],
        "citations": 142,
        "references": 60,
        "year": 2023
    },
    {
        "title": "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models",
        "abstract": "This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously a) accomplish the synthesis of visually realistic and temporally coherent videos while b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: 1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. 2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications.",
        "authors": [
            "Yaohui Wang",
            "Xinyuan Chen",
            "Xin Ma",
            "Shangchen Zhou",
            "Ziqi Huang",
            "Yi Wang",
            "Ceyuan Yang",
            "Yinan He",
            "Jiashuo Yu",
            "Pe-der Yang",
            "Yuwei Guo",
            "Tianxing Wu",
            "Chenyang Si",
            "Yuming Jiang",
            "Cunjian Chen",
            "Chen Change Loy",
            "Bo Dai",
            "Dahua Lin",
            "Y. Qiao",
            "Ziwei Liu"
        ],
        "citations": 186,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Synthetic Data from Diffusion Models Improves ImageNet Classification",
        "abstract": "Deep generative models are becoming increasingly powerful, now generating diverse high fidelity photo-realistic samples given text prompts. Have they reached the point where models of natural images can be used for generative data augmentation, helping to improve challenging discriminative tasks? We show that large-scale text-to image diffusion models can be fine-tuned to produce class conditional models with SOTA FID (1.76 at 256x256 resolution) and Inception Score (239 at 256x256). The model also yields a new SOTA in Classification Accuracy Scores (64.96 for 256x256 generative samples, improving to 69.24 for 1024x1024 samples). Augmenting the ImageNet training set with samples from the resulting models yields significant improvements in ImageNet classification accuracy over strong ResNet and Vision Transformer baselines.",
        "authors": [
            "Shekoofeh Azizi",
            "Simon Kornblith",
            "Chitwan Saharia",
            "Mohammad Norouzi",
            "David J. Fleet"
        ],
        "citations": 240,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Universal Guidance for Diffusion Models",
        "abstract": "Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, and classifier signals. Code is available at github.com/arpitbansal297/Universal-Guided-Diffusion.",
        "authors": [
            "Arpit Bansal",
            "Hong-Min Chu",
            "Avi Schwarzschild",
            "Soumyadip Sengupta",
            "Micah Goldblum",
            "Jonas Geiping",
            "T. Goldstein"
        ],
        "citations": 182,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
        "abstract": "This survey reviews the progress of diffusion models in generating images from text, ~\\textit{i.e.} text-to-image diffusion models. As a self-contained work, this survey starts with a brief introduction of how diffusion models work for image synthesis, followed by the background for text-conditioned image synthesis. Based on that, we present an organized review of pioneering methods and their improvements on text-to-image generation. We further summarize applications beyond image generation, such as text-guided generation for various modalities like videos, and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.",
        "authors": [
            "Chenshuang Zhang",
            "Chaoning Zhang",
            "Mengchun Zhang",
            "In-So Kweon"
        ],
        "citations": 214,
        "references": 151,
        "year": 2023
    },
    {
        "title": "Paint by Example: Exemplar-based Image Editing with Diffusion Models",
        "abstract": "Language-guided image editing has achieved great success recently. In this paper, we investigate exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose a content bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity. The code and pretrained models are available at https://github.com/Fantasy-Studio/Paint-by-Example.",
        "authors": [
            "Binxin Yang",
            "Shuyang Gu",
            "Bo Zhang",
            "Ting Zhang",
            "Xuejin Chen",
            "Xiaoyan Sun",
            "Dong Chen",
            "Fang Wen"
        ],
        "citations": 332,
        "references": 80,
        "year": 2022
    },
    {
        "title": "Distilling Diffusion Models into Conditional GANs",
        "abstract": "We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality. Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory. For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations. Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation. E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs. We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models -- DMD, SDXL-Turbo, and SDXL-Lightning -- on the zero-shot COCO benchmark.",
        "authors": [
            "Minguk Kang",
            "Richard Zhang",
            "Connelly Barnes",
            "Sylvain Paris",
            "Suha Kwak",
            "Jaesik Park",
            "Eli Shechtman",
            "Jun-Yan Zhu",
            "Taesung Park"
        ],
        "citations": 15,
        "references": 107,
        "year": 2024
    },
    {
        "title": "Effective Data Augmentation With Diffusion Models",
        "abstract": "Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains.",
        "authors": [
            "Brandon Trabucco",
            "Kyle Doherty",
            "Max Gurinas",
            "R. Salakhutdinov"
        ],
        "citations": 171,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models",
        "abstract": "Text-to-Image diffusion models have made tremendous progress over the past two years, enabling the generation of highly realistic images based on open-domain text descriptions. However, despite their success, text descriptions often struggle to adequately convey detailed controls, even when composed of long and complex texts. Moreover, recent studies have also shown that these models face challenges in understanding such complex texts and generating the corresponding images. Therefore, there is a growing need to enable more control modes beyond text description. In this paper, we introduce Uni-ControlNet, a unified framework that allows for the simultaneous utilization of different local controls (e.g., edge maps, depth map, segmentation masks) and global controls (e.g., CLIP image embeddings) in a flexible and composable manner within one single model. Unlike existing methods, Uni-ControlNet only requires the fine-tuning of two additional adapters upon frozen pre-trained text-to-image diffusion models, eliminating the huge cost of training from scratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNet only necessitates a constant number (i.e., 2) of adapters, regardless of the number of local or global controls used. This not only reduces the fine-tuning costs and model size, making it more suitable for real-world deployment, but also facilitate composability of different conditions. Through both quantitative and qualitative comparisons, Uni-ControlNet demonstrates its superiority over existing methods in terms of controllability, generation quality and composability. Code is available at \\url{https://github.com/ShihaoZhaoZSH/Uni-ControlNet}.",
        "authors": [
            "Shihao Zhao",
            "Dongdong Chen",
            "Yen-Chun Chen",
            "Jianmin Bao",
            "Shaozhe Hao",
            "Lu Yuan",
            "Kwan-Yee K. Wong"
        ],
        "citations": 168,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Noise2Music: Text-conditioned Music Generation with Diffusion Models",
        "abstract": "We introduce Noise2Music, where a series of diffusion models is trained to generate high-quality 30-second music clips from text prompts. Two types of diffusion models, a generator model, which generates an intermediate representation conditioned on text, and a cascader model, which generates high-fidelity audio conditioned on the intermediate representation and possibly the text, are trained and utilized in succession to generate high-fidelity music. We explore two options for the intermediate representation, one using a spectrogram and the other using audio with lower fidelity. We find that the generated audio is not only able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood, and era, but goes beyond to ground fine-grained semantics of the prompt. Pretrained large language models play a key role in this story -- they are used to generate paired text for the audio of the training set and to extract embeddings of the text prompts ingested by the diffusion models. Generated examples: https://google-research.github.io/noise2music",
        "authors": [
            "Qingqing Huang",
            "Daniel S. Park",
            "Tao Wang",
            "Timo I. Denk",
            "Andy Ly",
            "Nanxin Chen",
            "Zhengdong Zhang",
            "Zhishuai Zhang",
            "Jiahui Yu",
            "C. Frank",
            "Jesse Engel",
            "Quoc V. Le",
            "William Chan",
            "Wei Han"
        ],
        "citations": 160,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Dreamix: Video Diffusion Models are General Video Editors",
        "abstract": "Text-driven image and video diffusion models have recently achieved unprecedented generation realism. While diffusion models have been successfully applied for image editing, very few works have done so for video editing. We present the first diffusion-based method that is able to perform text-based motion and appearance editing of general videos. Our approach uses a video diffusion model to combine, at inference time, the low-resolution spatio-temporal information from the original video with new, high resolution information that it synthesized to align with the guiding text prompt. As obtaining high-fidelity to the original video requires retaining some of its high-resolution information, we add a preliminary stage of finetuning the model on the original video, significantly boosting fidelity. We propose to improve motion editability by a new, mixed objective that jointly finetunes with full temporal attention and with temporal attention masking. We further introduce a new framework for image animation. We first transform the image into a coarse video by simple image processing operations such as replication and perspective geometric projections, and then use our general video editor to animate it. As a further application, we can use our method for subject-driven video generation. Extensive qualitative and numerical experiments showcase the remarkable editing ability of our method and establish its superior performance compared to baseline methods.",
        "authors": [
            "Eyal Molad",
            "Eliahu Horwitz",
            "Dani Valevski",
            "Alex Rav Acha",
            "Yossi Matias",
            "Y. Pritch",
            "Yaniv Leviathan",
            "Yedid Hoshen"
        ],
        "citations": 162,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation",
        "abstract": "A diffusion model learns to predict a vector field of gradients. We propose to apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instantiate to be a voxel radiance field. This setup aggregates 2D scores at multiple camera viewpoints into a 3D score, and re-purposes a pretrained 2D model for 3D data generation. We identify a technical challenge of distribution mismatch that arises in this application, and propose a novel estimation mechanism to resolve it. We run our algorithm on several off-the-shelf diffusion image generative models, including the recently released Stable Diffusion trained on the large-scale LAION 5B dataset.",
        "authors": [
            "Haochen Wang",
            "Xiaodan Du",
            "Jiahao Li",
            "Raymond A. Yeh",
            "Gregory Shakhnarovich"
        ],
        "citations": 455,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Imitating Human Behaviour with Diffusion Models",
        "abstract": "Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment.",
        "authors": [
            "Tim Pearce",
            "Tabish Rashid",
            "A. Kanervisto",
            "David Bignell",
            "Mingfei Sun",
            "Raluca Georgescu",
            "Sergio Valcarcel Macua",
            "Shan Zheng Tan",
            "I. Momennejad",
            "Katja Hofmann",
            "Sam Devlin"
        ],
        "citations": 155,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Rolling Diffusion Models",
        "abstract": "Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores Rolling Diffusion: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.",
        "authors": [
            "David Ruhe",
            "Jonathan Heek",
            "Tim Salimans",
            "Emiel Hoogeboom"
        ],
        "citations": 13,
        "references": 56,
        "year": 2024
    },
    {
        "title": "Elucidating the Design Space of Diffusion-Based Generative Models",
        "abstract": "We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.",
        "authors": [
            "Tero Karras",
            "M. Aittala",
            "Timo Aila",
            "S. Laine"
        ],
        "citations": 1000,
        "references": 65,
        "year": 2022
    },
    {
        "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
        "abstract": "Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.11Code available at https://huggingface.co/docs/diffusers/api/pipelines/stable.diffusion.safe",
        "authors": [
            "P. Schramowski",
            "Manuel Brack",
            "Bjorn Deiseroth",
            "K. Kersting"
        ],
        "citations": 200,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation",
        "abstract": "Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at https://github.com/showlab/Show-1.",
        "authors": [
            "David Junhao Zhang",
            "Jay Zhangjie Wu",
            "Jia-Wei Liu",
            "Rui Zhao",
            "L. Ran",
            "Yuchao Gu",
            "Difei Gao",
            "Mike Zheng Shou"
        ],
        "citations": 153,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Video Probabilistic Diffusion Models in Projected Latent Space",
        "abstract": "Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their high-dimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computation and memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion model (PVDM), a probabilistic diffusion model which learns a video distribution in a low-dimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized latent space and the training/sampling procedure to synthesize videos of arbitrary length with a single model. Experiments on popular video generation datasets demonstrate the superiority of PVDM compared with previous video synthesis methods; e.g., PVDM obtains the FVD score of 639.7 on the UCF-101 long video (128 frames) generation benchmark, which improves 1773.4 of the prior state-of-the-art.",
        "authors": [
            "Sihyun Yu",
            "Kihyuk Sohn",
            "Subin Kim",
            "Jinwoo Shin"
        ],
        "citations": 146,
        "references": 85,
        "year": 2023
    },
    {
        "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
        "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
        "authors": [
            "Kai Shen",
            "Zeqian Ju",
            "Xu Tan",
            "Yanqing Liu",
            "Yichong Leng",
            "Lei He",
            "Tao Qin",
            "Sheng Zhao",
            "Jiang Bian"
        ],
        "citations": 185,
        "references": 78,
        "year": 2023
    },
    {
        "title": "High-resolution image reconstruction with latent diffusion models from human brain activity",
        "abstract": "Reconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straight-forward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs. Please check out our webpage at https://sites.google.com/view/stablediffusion-with-brain/.",
        "authors": [
            "Yu Takagi",
            "Shinji Nishimoto"
        ],
        "citations": 195,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions",
        "abstract": "We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does not reduce the complexity of SGMs.",
        "authors": [
            "Sitan Chen",
            "Sinho Chewi",
            "Jungshian Li",
            "Yuanzhi Li",
            "A. Salim",
            "Anru R. Zhang"
        ],
        "citations": 200,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models",
        "abstract": "Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with\"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io",
        "authors": [
            "Rongjie Huang",
            "Jia-Bin Huang",
            "Dongchao Yang",
            "Yi Ren",
            "Luping Liu",
            "Mingze Li",
            "Zhenhui Ye",
            "Jinglin Liu",
            "Xiaoyue Yin",
            "Zhou Zhao"
        ],
        "citations": 255,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Generative Novel View Synthesis with 3D-Aware Diffusion Models",
        "abstract": "We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method’s ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.",
        "authors": [
            "Eric Chan",
            "Koki Nagano",
            "Matthew Chan",
            "Alexander W. Bergman",
            "Jeong Joon Park",
            "A. Levy",
            "M. Aittala",
            "Shalini De Mello",
            "Tero Karras",
            "Gordon Wetzstein"
        ],
        "citations": 198,
        "references": 119,
        "year": 2023
    },
    {
        "title": "Implicit Diffusion Models for Continuous Super-Resolution",
        "abstract": "Image super-resolution (SR) has attracted increasing attention due to its widespread applications. However, current SR methods generally suffer from over-smoothing and artifacts, and most work only with fixed magnifications. This paper introduces an Implicit Diffusion Model (IDM) for high-fidelity continuous image super-resolution. IDM integrates an implicit neural representation and a denoising diffusion model in a unified end-to-end framework, where the implicit neural representation is adopted in the decoding process to learn continuous-resolution representation. Furthermore, we design a scale-adaptive conditioning mechanism that consists of a low-resolution (LR) conditioning network and a scaling factor. The scaling factor regulates the resolution and accordingly modulates the proportion of the LR information and generated features in the final output, which enables the model to accommodate the continuous-resolution requirement. Extensive experiments validate the effectiveness of our IDM and demonstrate its superior performance over prior arts. The source code will be available at https://github.com/Ree1s/IDM.",
        "authors": [
            "Sicheng Gao",
            "Xuhui Liu",
            "Bo-Wen Zeng",
            "Sheng Xu",
            "Yanjing Li",
            "Xiaonan Luo",
            "Jianzhuang Liu",
            "Xiantong Zhen",
            "Baochang Zhang"
        ],
        "citations": 170,
        "references": 55,
        "year": 2023
    },
    {
        "title": "TabDDPM: Modelling Tabular Data with Diffusion Models",
        "abstract": "Denoising diffusion probabilistic models are currently becoming the leading paradigm of generative modeling for many important data modalities. Being the most prevalent in the computer vision community, diffusion models have also recently gained some attention in other domains, including speech, NLP, and graph-like data. In this work, we investigate if the framework of diffusion models can be advantageous for general tabular problems, where datapoints are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data makes it quite challenging for accurate modeling, since the individual features can be of completely different nature, i.e., some of them can be continuous and some of them can be discrete. To address such data types, we introduce TabDDPM -- a diffusion model that can be universally applied to any tabular dataset and handles any type of feature. We extensively evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority over existing GAN/VAE alternatives, which is consistent with the advantage of diffusion models in other fields. Additionally, we show that TabDDPM is eligible for privacy-oriented setups, where the original datapoints cannot be publicly shared.",
        "authors": [
            "Akim Kotelnikov",
            "Dmitry Baranchuk",
            "Ivan Rubachev",
            "Artem Babenko"
        ],
        "citations": 175,
        "references": 57,
        "year": 2022
    },
    {
        "title": "3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models",
        "abstract": "We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation, and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation. Code: https://1zb.github.io/3DShape2VecSet/.",
        "authors": [
            "Biao Zhang",
            "Jiapeng Tang",
            "M. Nießner",
            "Peter Wonka"
        ],
        "citations": 130,
        "references": 94,
        "year": 2023
    },
    {
        "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration",
        "abstract": "Plug-and-play Image Restoration (IR) has been widely recognized as a flexible and interpretable method for solving various inverse problems by utilizing any off-the-shelf denoiser as the implicit image prior. However, most existing methods focus on discriminative Gaussian denoisers. Although diffusion models have shown impressive performance for high-quality image synthesis, their potential to serve as a generative denoiser prior to the plug-and-play IR methods remains to be further explored. While several other attempts have been made to adopt diffusion models for image restoration, they either fail to achieve satisfactory results or typically require an unacceptable number of Neural Function Evaluations (NFEs) during inference. This paper proposes DiffPIR, which integrates the traditional plug-and-play method into the diffusion sampling framework. Compared to plug-and-play IR methods that rely on discriminative Gaussian denoisers, DiffPIR is expected to inherit the generative ability of diffusion models. Experimental results on three representative IR tasks, including super-resolution, image deblurring, and inpainting, demonstrate that DiffPIR achieves state-of-the-art performance on both the FFHQ and ImageNet datasets in terms of reconstruction faithfulness and perceptual quality with no more than 100 NFEs. The source code is available at https://github.com/yuanzhi-zhu/DiffPIR",
        "authors": [
            "Yuanzhi Zhu",
            "K. Zhang",
            "Jingyun Liang",
            "Jiezhang Cao",
            "B. Wen",
            "R. Timofte",
            "L. Gool"
        ],
        "citations": 134,
        "references": 62,
        "year": 2023
    },
    {
        "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
        "abstract": "Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",
        "authors": [
            "Long Lian",
            "Boyi Li",
            "Adam Yala",
            "Trevor Darrell"
        ],
        "citations": 114,
        "references": 84,
        "year": 2023
    },
    {
        "title": "On the Importance of Noise Scheduling for Diffusion Models",
        "abstract": "We empirically study the effect of noise scheduling strategies for denoising diffusion generative models. There are three findings: (1) the noise scheduling is crucial for the performance, and the optimal one depends on the task (e.g., image sizes), (2) when increasing the image size, the optimal noise scheduling shifts towards a noisier one (due to increased redundancy in pixels), and (3) simply scaling the input data by a factor of $b$ while keeping the noise schedule function fixed (equivalent to shifting the logSNR by $\\log b$) is a good strategy across image sizes. This simple recipe, when combined with recently proposed Recurrent Interface Network (RIN), yields state-of-the-art pixel-based diffusion models for high-resolution images on ImageNet, enabling single-stage, end-to-end generation of diverse and high-fidelity images at 1024$\\times$1024 resolution (without upsampling/cascades).",
        "authors": [
            "Ting Chen"
        ],
        "citations": 118,
        "references": 21,
        "year": 2023
    },
    {
        "title": "Conditional Image-to-Video Generation with Latent Flow Diffusion Models",
        "abstract": "Conditional image-to-video (cI2V) generation aims to synthesize a new plausible video starting from an image (e.g., a person's face) and a condition (e.g., an action class label like smile). The key challenge of the cI2V task lies in the simultaneous generation of realistic spatial appearance and temporal dynamics corresponding to the given image and condition. In this paper, we propose an approach for cI2V using novel latent flow diffusion models (LFDM) that synthesize an optical flow sequence in the latent space based on the given condition to warp the given image. Compared to previous direct-synthesis-based works, our proposed LFDM can better synthesize spatial details and temporal motion by fully utilizing the spatial content of the given image and warping it in the latent space according to the generated temporally-coherent flow. The training of LFDM consists of two separate stages: (1) an unsupervised learning stage to train a latent flow auto-encoder for spatial content generation, including a flow predictor to estimate latent flow between pairs of video frames, and (2) a conditional learning stage to train a 3D-UNet-based diffusion model (DM) for temporal latent flow generation. Unlike previous DMs operating in pixel space or latent feature space that couples spatial and temporal information, the DM in our LFDM only needs to learn a low-dimensional latent flow space for motion generation, thus being more computationally efficient. We conduct comprehensive experiments on multiple datasets, where LFDM consistently outperforms prior arts. Furthermore, we show that LFDM can be easily adapted to new domains by simply finetuning the image decoder. Our code is available at https://github.com/nihaomiao/CVPR23_LFDM.",
        "authors": [
            "Haomiao Ni",
            "Changhao Shi",
            "Kaican Li",
            "Sharon X. Huang",
            "Martin Renqiang Min"
        ],
        "citations": 120,
        "references": 93,
        "year": 2023
    },
    {
        "title": "DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models",
        "abstract": "We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body's shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a train-able NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the generation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deformation field. This facilitates the generation of more complete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face “Janus” problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing meth-ods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation.",
        "authors": [
            "Yukang Cao",
            "Yan-Pei Cao",
            "Kai Han",
            "Ying Shan",
            "Kwan-Yee Kenneth Wong"
        ],
        "citations": 113,
        "references": 72,
        "year": 2023
    },
    {
        "title": "On the Importance of Noise Scheduling for Diffusion Models",
        "abstract": "We empirically study the effect of noise scheduling strategies for denoising diffusion generative models. There are three findings: (1) the noise scheduling is crucial for the performance, and the optimal one depends on the task (e.g., image sizes), (2) when increasing the image size, the optimal noise scheduling shifts towards a noisier one (due to increased redundancy in pixels), and (3) simply scaling the input data by a factor of $b$ while keeping the noise schedule function fixed (equivalent to shifting the logSNR by $\\log b$) is a good strategy across image sizes. This simple recipe, when combined with recently proposed Recurrent Interface Network (RIN), yields state-of-the-art pixel-based diffusion models for high-resolution images on ImageNet, enabling single-stage, end-to-end generation of diverse and high-fidelity images at 1024$\\times$1024 resolution (without upsampling/cascades).",
        "authors": [
            "Ting Chen"
        ],
        "citations": 118,
        "references": 21,
        "year": 2023
    },
    {
        "title": "Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models",
        "abstract": "Recent advancements in diffusion models have unlocked unprecedented abilities in visual creation. However, current text-to-video generation models struggle with the trade-off among movement range, action coherence and object consistency. To mitigate this issue, we present a controllable text-to-video (T2V) diffusion model, called Control-A-Video, capable of maintaining consistency while customizable video synthesis. Based on a pre-trained conditional text-to-image (T2I) diffusion model, our model aims to generate videos conditioned on a sequence of control signals, such as edge or depth maps. For the purpose of improving object consistency, Control-A-Video integrates motion priors and content priors into video generation. We propose two motion-adaptive noise initialization strategies, which",
        "authors": [
            "Weifeng Chen",
            "Jie Wu",
            "Pan Xie",
            "Hefeng Wu",
            "Jiashi Li",
            "Xin Xia",
            "Xuefeng Xiao",
            "Liang-Jin Lin"
        ],
        "citations": 118,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Geometric Latent Diffusion Models for 3D Molecule Generation",
        "abstract": "Generative models, especially diffusion models (DMs), have achieved promising results for generating feature-rich geometries and advancing foundational science problems such as molecule design. Inspired by the recent huge success of Stable (latent) Diffusion models, we propose a novel and principled method for 3D molecule generation named Geometric Latent Diffusion Models (GeoLDM). GeoLDM is the first latent DM model for the molecular geometry domain, composed of autoencoders encoding structures into continuous latent codes and DMs operating in the latent space. Our key innovation is that for modeling the 3D molecular geometries, we capture its critical roto-translational equivariance constraints by building a point-structured latent space with both invariant scalars and equivariant tensors. Extensive experiments demonstrate that GeoLDM can consistently achieve better performance on multiple molecule generation benchmarks, with up to 7\\% improvement for the valid percentage of large biomolecules. Results also demonstrate GeoLDM's higher capacity for controllable generation thanks to the latent modeling. Code is provided at \\url{https://github.com/MinkaiXu/GeoLDM}.",
        "authors": [
            "Minkai Xu",
            "Alexander Powers",
            "R. Dror",
            "Stefano Ermon",
            "J. Leskovec"
        ],
        "citations": 99,
        "references": 62,
        "year": 2023
    },
    {
        "title": "A Variational Perspective on Solving Inverse Problems with Diffusion Models",
        "abstract": "Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-Diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-to-noise-ratio (SNR). Our approach provides a new variational perspective for solving inverse problems with diffusion models, allowing us to formulate sampling as stochastic optimization, where one can simply apply off-the-shelf solvers with lightweight iterates. Our experiments for image restoration tasks such as inpainting and superresolution demonstrate the strengths of our method compared with state-of-the-art sampling-based diffusion models.",
        "authors": [
            "M. Mardani",
            "Jiaming Song",
            "J. Kautz",
            "Arash Vahdat"
        ],
        "citations": 92,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Q-Diffusion: Quantizing Diffusion Models",
        "abstract": "Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut quantization in this work. Experimental results show that our proposed method is able to quantize full-precision unconditional diffusion models into 4-bit while maintaining comparable performance (small FID change of at most 2.34 compared to >100 for traditional PTQ) in a training-free manner. Our approach can also be applied to text-guided image generation, where we can run stable diffusion in 4-bit weights with high generation quality for the first time.",
        "authors": [
            "Xiuyu Li",
            "Long Lian",
            "Yijia Liu",
            "Hua Yang",
            "Zhen Dong",
            "Daniel Kang",
            "Shanghang Zhang",
            "Kurt Keutzer"
        ],
        "citations": 100,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Understanding and Mitigating Copying in Diffusion Models",
        "abstract": "Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at both training and inference time by randomizing and augmenting image captions in the training set.",
        "authors": [
            "Gowthami Somepalli",
            "Vasu Singla",
            "Micah Goldblum",
            "Jonas Geiping",
            "T. Goldstein"
        ],
        "citations": 99,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Photorealistic Video Generation with Diffusion Models",
        "abstract": "We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of $512 \\times 896$ resolution at $8$ frames per second.",
        "authors": [
            "Agrim Gupta",
            "Lijun Yu",
            "Kihyuk Sohn",
            "Xiuye Gu",
            "Meera Hahn",
            "Fei-Fei Li",
            "Irfan Essa",
            "Lu Jiang",
            "José Lezama"
        ],
        "citations": 108,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Directly Fine-Tuning Diffusion Models on Differentiable Rewards",
        "abstract": "We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of rewards, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms.",
        "authors": [
            "Kevin Clark",
            "Paul Vicol",
            "Kevin Swersky",
            "David J. Fleet"
        ],
        "citations": 92,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",
        "abstract": "Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers. Intriguingly we find these samplers lead to notable improvements in compositional generation across a wide set of problems such as classifier-guided ImageNet modeling and compositional text-to-image generation.",
        "authors": [
            "Yilun Du",
            "Conor Durkan",
            "Robin Strudel",
            "J. Tenenbaum",
            "S. Dieleman",
            "R. Fergus",
            "Jascha Narain Sohl-Dickstein",
            "A. Doucet",
            "Will Grathwohl"
        ],
        "citations": 105,
        "references": 52,
        "year": 2023
    },
    {
        "title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models",
        "abstract": "Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning with respect to both image-text alignment and image quality. Our code is available at https://github.com/google-research/google-research/tree/master/dpok.",
        "authors": [
            "Ying Fan",
            "Olivia Watkins",
            "Yuqing Du",
            "Hao Liu",
            "M. Ryu",
            "Craig Boutilier",
            "P. Abbeel",
            "M. Ghavamzadeh",
            "Kangwook Lee",
            "Kimin Lee"
        ],
        "citations": 101,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Low-Light Image Enhancement with Wavelet-Based Diffusion Models",
        "abstract": "Diffusion models have achieved promising results in image restoration tasks, yet suffer from time-consuming, excessive computational resource consumption, and unstable restoration. To address these issues, we propose a robust and efficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL. Specifically, we present a wavelet-based conditional diffusion model (WCDM) that leverages the generative power of diffusion models to produce results with satisfactory perceptual fidelity. Additionally, it also takes advantage of the strengths of wavelet transformation to greatly accelerate inference and reduce computational resource usage without sacrificing information. To avoid chaotic content and diversity, we perform both forward diffusion and denoising in the training phase of WCDM, enabling the model to achieve stable denoising and reduce randomness during inference. Moreover, we further design a high-frequency restoration module (HFRM) that utilizes the vertical and horizontal details of the image to complement the diagonal information for better fine-grained restoration. Extensive experiments on publicly available real-world benchmarks demonstrate that our method outperforms the existing state-of-the-art methods both quantitatively and visually, and it achieves remarkable improvements in efficiency compared to previous diffusion-based methods. In addition, we empirically show that the application for low-light face detection also reveals the latent practical values of our method. Code is available at https://github.com/JianghaiSCU/Diffusion-Low-Light.",
        "authors": [
            "Hailin Jiang",
            "Ao Luo",
            "Songchen Han",
            "Haoqiang Fan",
            "Shuaicheng Liu"
        ],
        "citations": 92,
        "references": 81,
        "year": 2023
    },
    {
        "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models",
        "abstract": "Despite the ability of existing large-scale text-to-image (T2I) models to generate high-quality images from detailed textual descriptions, they often lack the ability to precisely edit the generated or real images. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models. Specifically, we construct classifier guidance based on the strong correspondence of intermediate features in the diffusion model. It can transform the editing signals into gradients via feature correspondence loss to modify the intermediate representation of the diffusion model. Based on this guidance strategy, we also build a multi-scale guidance to consider both semantic and geometric alignment. Moreover, a cross-branch self-attention is added to maintain the consistency between the original image and the editing result. Our method, through an efficient design, achieves various editing modes for the generated or real images, such as object moving, object resizing, object appearance replacement, and content dragging. It is worth noting that all editing and content preservation signals come from the image itself, and the model does not require fine-tuning or additional modules. Our source code will be available at https://github.com/MC-E/DragonDiffusion.",
        "authors": [
            "Chong Mou",
            "Xintao Wang",
            "Jie Song",
            "Ying Shan",
            "Jian Zhang"
        ],
        "citations": 105,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models",
        "abstract": "Large-scale text-to-image diffusion models achieve unprecedented success in image generation and editing. However, how to extend such success to video editing is unclear. Recent initial attempts at video editing require significant text-to-video data and computation resources for training, which is often not accessible. In this work, we propose vid2vid-zero, a simple yet effective method for zero-shot video editing. Our vid2vid-zero leverages off-the-shelf image diffusion models, and doesn't require training on any video. At the core of our method is a null-text inversion module for text-to-video alignment, a cross-frame modeling module for temporal consistency, and a spatial regularization module for fidelity to the original video. Without any training, we leverage the dynamic nature of the attention mechanism to enable bi-directional temporal modeling at test time. Experiments and analyses show promising results in editing attributes, subjects, places, etc., in real-world videos. Code is made available at \\url{https://github.com/baaivision/vid2vid-zero}.",
        "authors": [
            "Wen Wang",
            "K. Xie",
            "Zide Liu",
            "Hao Chen",
            "Yue Cao",
            "Xinlong Wang",
            "Chunhua Shen"
        ],
        "citations": 102,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Are Diffusion Models Vulnerable to Membership Inference Attacks?",
        "abstract": "Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e.g., DDPM, and the text-to-image diffusion models, e.g., Latent Diffusion Models and Stable Diffusion. Experimental results demonstrate that our methods precisely infer the membership with high confidence on both of the two scenarios across multiple different datasets. Code is available at https://github.com/jinhaoduan/SecMI.",
        "authors": [
            "Jinhao Duan",
            "Fei Kong",
            "Shiqi Wang",
            "Xiaoshuang Shi",
            "Kaidi Xu"
        ],
        "citations": 90,
        "references": 57,
        "year": 2023
    },
    {
        "title": "A Recipe for Watermarking Diffusion Models",
        "abstract": "Diffusion models (DMs) have demonstrated advantageous potential on generative tasks. Widespread interest exists in incorporating DMs into downstream applications, such as producing or editing photorealistic images. However, practical deployment and unprecedented power of DMs raise legal issues, including copyright protection and monitoring of generated content. In this regard, watermarking has been a proven solution for copyright protection and content monitoring, but it is underexplored in the DMs literature. Specifically, DMs generate samples from longer tracks and may have newly designed multimodal structures, necessitating the modification of conventional watermarking pipelines. To this end, we conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe is straightforward but involves empirically ablated implementation details, providing a foundation for future research on watermarking DMs. The code is available at https://github.com/yunqing-me/WatermarkDM.",
        "authors": [
            "Yunqing Zhao",
            "Tianyu Pang",
            "Chao Du",
            "Xiao Yang",
            "Ngai-Man Cheung",
            "Min Lin"
        ],
        "citations": 86,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples",
        "abstract": "Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-Art applications. The code of our method is available on GitHub: https://github.com/mist-project/mist.git.",
        "authors": [
            "Chumeng Liang",
            "Xiaoyu Wu",
            "Yang Hua",
            "Jiaru Zhang",
            "Yiming Xue",
            "Tao Song",
            "Zhengui Xue",
            "Ruhui Ma",
            "Haibing Guan"
        ],
        "citations": 86,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Structural Pruning for Diffusion Models",
        "abstract": "Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across several datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50\\% reduction in FLOPs at a mere 10\\% to 20\\% of the original training expenditure; 2) Consistency: the pruned diffusion models inherently preserve generative behavior congruent with their pre-trained models. Code is available at \\url{https://github.com/VainF/Diff-Pruning}.",
        "authors": [
            "Gongfan Fang",
            "Xinyin Ma",
            "Xinchao Wang"
        ],
        "citations": 85,
        "references": 60,
        "year": 2023
    },
    {
        "title": "MagicVideo: Efficient Video Generation With Latent Diffusion Models",
        "abstract": "We present an efficient text-to-video generation framework based on latent diffusion models, termed MagicVideo. MagicVideo can generate smooth video clips that are concordant with the given text descriptions. Due to a novel and efficient 3D U-Net design and modeling video distributions in a low-dimensional space, MagicVideo can synthesize video clips with 256x256 spatial resolution on a single GPU card, which takes around 64x fewer computations than the Video Diffusion Models (VDM) in terms of FLOPs. In specific, unlike existing works that directly train video models in the RGB space, we use a pre-trained VAE to map video clips into a low-dimensional latent space and learn the distribution of videos' latent codes via a diffusion model. Besides, we introduce two new designs to adapt the U-Net denoiser trained on image tasks to video data: a frame-wise lightweight adaptor for the image-to-video distribution adjustment and a directed temporal attention module to capture temporal dependencies across frames. Thus, we can exploit the informative weights of convolution operators from a text-to-image model for accelerating video training. To ameliorate the pixel dithering in the generated videos, we also propose a novel VideoVAE auto-encoder for better RGB reconstruction. We conduct extensive experiments and demonstrate that MagicVideo can generate high-quality video clips with either realistic or imaginary content. Refer to \\url{https://magicvideo.github.io/#} for more examples.",
        "authors": [
            "Daquan Zhou",
            "Weimin Wang",
            "Hanshu Yan",
            "Weiwei Lv",
            "Yizhe Zhu",
            "Jiashi Feng"
        ],
        "citations": 324,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Analyzing and Improving the Training Dynamics of Diffusion Models",
        "abstract": "Diffusion models currently dominate the field of data- driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high- level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on ex- pectation. We find that systematic application of this philoso- phy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational com- plexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling. As an independent contribution, we present a method for setting the exponential moving average (EMA) parameters post-hoc, i.e., after completing the training run. This allows precise tuning of EMA length without the cost of performing several training runs, and reveals its surprising interactions with network architecture, training time, and guidance.",
        "authors": [
            "Tero Karras",
            "M. Aittala",
            "J. Lehtinen",
            "Janne Hellsten",
            "Timo Aila",
            "S. Laine"
        ],
        "citations": 84,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models",
        "abstract": "Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we in-stead focus on the underexplored text-to-4D setting and syn-thesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby si-multaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (A YG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D represen-tation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby sta-bilize the optimization and induce motion. We also pro-pose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, out-perform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.",
        "authors": [
            "Huan Ling",
            "Seung Wook Kim",
            "Antonio Torralba",
            "Sanja Fidler",
            "Karsten Kreis"
        ],
        "citations": 84,
        "references": 115,
        "year": 2023
    },
    {
        "title": "On Distillation of Guided Diffusion Models",
        "abstract": "Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALL.E 2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet $64\\times 64$ and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet $256\\times 256$ and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2–4 denoising steps.",
        "authors": [
            "Chenlin Meng",
            "Ruiqi Gao",
            "Diederik P. Kingma",
            "Stefano Ermon",
            "Jonathan Ho",
            "Tim Salimans"
        ],
        "citations": 396,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models",
        "abstract": "Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffusion process, which we show to be more robust in comparing distributions with misaligned supports. We also reveal non-trivial connections of our method to existing works such as DreamFusion, and generative adversarial training. To demonstrate the effectiveness and universality of Diff-Instruct, we consider two scenarios: distilling pre-trained diffusion models and refining existing GAN models. The experiments on distilling pre-trained diffusion models show that Diff-Instruct results in state-of-the-art single-step diffusion-based models. The experiments on refining GAN models show that the Diff-Instruct can consistently improve the pre-trained generators of GAN models across various settings.",
        "authors": [
            "Weijian Luo",
            "Tianyang Hu",
            "Shifeng Zhang",
            "Jiacheng Sun",
            "Zhenguo Li",
            "Zhihua Zhang"
        ],
        "citations": 79,
        "references": 82,
        "year": 2023
    },
    {
        "title": "TextDiffuser: Diffusion Models as Text Painters",
        "abstract": "Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout. Additionally, we contribute the first large-scale text images dataset with OCR annotations, MARIO-10M, containing 10 million image-text pairs with text recognition, detection, and character-level segmentation annotations. We further collect the MARIO-Eval benchmark to serve as a comprehensive tool for evaluating text rendering quality. Through experiments and user studies, we show that TextDiffuser is flexible and controllable to create high-quality text images using text prompts alone or together with text template images, and conduct text inpainting to reconstruct incomplete images with text. The code, model, and dataset will be available at \\url{https://aka.ms/textdiffuser}.",
        "authors": [
            "Jingye Chen",
            "Yupan Huang",
            "Tengchao Lv",
            "Lei Cui",
            "Qifeng Chen",
            "Furu Wei"
        ],
        "citations": 82,
        "references": 107,
        "year": 2023
    },
    {
        "title": "Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation",
        "abstract": "We consider guiding denoising diffusion models with general differentiable loss functions in a plug-and-play fashion, enabling controllable generation without additional training. This paradigm, termed Loss-Guided Diffusion (LGD), can easily be integrated into all diffusion models and leverage various efficient samplers. Despite the benefits, the resulting guidance term is, unfortunately, an intractable integral and needs to be approximated. Existing methods compute the guidance term based on a point estimate. However, we show that such approaches have significant errors over the scale of the approximations. To address this issue, we propose a Monte Carlo method that uses multiple samples from a suitable distribution to reduce bias. Our method is effective in various synthetic and real-world settings, including image super-resolution, text or label-conditional image generation, and controllable motion synthesis. Notably, we show how our method can be applied to control a pretrained motion diffusion model to follow certain paths and avoid obstacles that are proven challenging to prior methods.",
        "authors": [
            "Jiaming Song",
            "Qinsheng Zhang",
            "Hongxu Yin",
            "M. Mardani",
            "Ming-Yu Liu",
            "J. Kautz",
            "Yongxin Chen",
            "Arash Vahdat"
        ],
        "citations": 80,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models",
        "abstract": "In image editing employing diffusion models, it is crucial to preserve the reconstruction fidelity to the original image while changing its style. Although existing methods ensure reconstruction fidelity through optimization, a drawback of these is the significant amount of time required for optimization. In this paper, we propose negative-prompt inversion, a method capable of achieving equivalent reconstruction solely through forward propagation without optimization, thereby enabling ultrafast editing processes. We experimentally demonstrate that the reconstruction fidelity of our method is comparable to that of existing methods, allowing for inversion at a resolution of 512 pixels and with 50 sampling steps within approximately 5 seconds, which is more than 30 times faster than null-text inversion. Reduction of the computation time by the proposed method further allows us to use a larger number of sampling steps in diffusion models to improve the reconstruction fidelity with a moderate increase in computation time.",
        "authors": [
            "Daiki Miyake",
            "Akihiro Iohara",
            "Yuriko Saito",
            "Toshiyuki TANAKA"
        ],
        "citations": 83,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Text-to-Image Diffusion Models are Zero-Shot Classifiers",
        "abstract": "The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data. However, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks. We investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers. The key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood. We apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image classification datasets. Additionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot. Although generative pre-training is prevalent in NLP, visual foundation models often use other methods such as contrastive learning. Based on our findings, we argue that generative pre-training should be explored as a compelling alternative for vision-language tasks.",
        "authors": [
            "Kevin Clark",
            "P. Jaini"
        ],
        "citations": 81,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning",
        "abstract": "We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.",
        "authors": [
            "Ting Chen",
            "Ruixiang Zhang",
            "Geoffrey E. Hinton"
        ],
        "citations": 241,
        "references": 68,
        "year": 2022
    },
    {
        "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models",
        "abstract": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html",
        "authors": [
            "Gowthami Somepalli",
            "Vasu Singla",
            "Micah Goldblum",
            "Jonas Geiping",
            "T. Goldstein"
        ],
        "citations": 248,
        "references": 76,
        "year": 2022
    },
    {
        "title": "DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models",
        "abstract": "Current deep networks are very data-hungry and benefit from training on largescale datasets, which are often time-consuming to collect and annotate. By contrast, synthetic data can be generated infinitely using generative models such as DALL-E and diffusion models, with minimal effort and cost. In this paper, we present DatasetDM, a generic dataset generation model that can produce diverse synthetic images and the corresponding high-quality perception annotations (e.g., segmentation masks, and depth). Our method builds upon the pre-trained diffusion model and extends text-guided image synthesis to perception data generation. We show that the rich latent code of the diffusion model can be effectively decoded as accurate perception annotations using a decoder module. Training the decoder only needs less than 1% (around 100 images) manually labeled images, enabling the generation of an infinitely large annotated dataset. Then these synthetic data can be used for training various perception models for downstream tasks. To showcase the power of the proposed approach, we generate datasets with rich dense pixel-wise labels for a wide range of downstream tasks, including semantic segmentation, instance segmentation, and depth estimation. Notably, it achieves 1) state-of-the-art results on semantic segmentation and instance segmentation; 2) significantly more robust on domain generalization than using the real data alone; and state-of-the-art results in zero-shot segmentation setting; and 3) flexibility for efficient application and novel task composition (e.g., image editing). The project website and code can be found at https://weijiawu.github.io/DatasetDM_page/ and https://github.com/showlab/DatasetDM, respectively",
        "authors": [
            "Wei Wu",
            "Yuzhong Zhao",
            "Hao Chen",
            "Yuchao Gu",
            "Rui Zhao",
            "Yefei He",
            "Hong Zhou",
            "Mike Zheng Shou",
            "Chunhua Shen"
        ],
        "citations": 71,
        "references": 69,
        "year": 2023
    },
    {
        "title": "DeepCache: Accelerating Diffusion Models for Free",
        "abstract": "Diffusion models have recently gained unprecedented attention in the field of image synthesis due to their re-markable generative capabilities. Notwithstanding their prowess, these models often incur substantial computational costs, primarily attributed to the sequential denoising process and cumbersome model size. Traditional methods for compressing diffusion models typically involve extensive retraining, presenting cost and feasibility challenges. In this paper, we introduce DeepCache, a novel training-free paradigm that accelerates diffusion models from the per-spective of model architecture. DeepCache capitalizes on the inherent temporal redundancy observed in the sequential denoising steps of diffusion models, which caches and retrieves features across adjacent denoising stages, thereby curtailing redundant computations. Utilizing the property of the U-Net, we reuse the high-level features while updating the low-level features in a very cheap way. This innovative strategy, in turn, enables a speedup factor of 2.3× for Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1 x for LDM-4-G with a slight de-crease of 0.22 in FID on ImageNet. Our experiments also demonstrate DeepCache's superiority over existing pruning and distillation methods that necessitate retraining and its compatibility with current sampling techniques. Furthermore, we find that under the same throughput, Deep-Cache effectively achieves comparable or even marginally improved results with DDIM or PLMS. Code is available at https://github.com/horseee/DeepCache.",
        "authors": [
            "Xinyin Ma",
            "Gongfan Fang",
            "Xinchao Wang"
        ],
        "citations": 76,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models",
        "abstract": "This work aims to improve the applicability of diffusion models in realistic image restoration. Specifically, we enhance the diffusion model in several aspects such as network architecture, noise level, denoising steps, training image size, and optimizer/scheduler. We show that tuning these hyperparameters allows us to achieve better performance on both distortion and perceptual scores. We also propose a U-Net based latent diffusion model which performs diffusion in a low-resolution latent space while preserving high-resolution information from the original input for the decoding process. Compared to the previous latent-diffusion model which trains a VAE-GAN to compress the image, our proposed U-Net compression strategy is significantly more stable and can recover highly accurate images without relying on adversarial optimization. Importantly, these modifications allow us to apply diffusion models to various image restoration tasks, including real-world shadow removal, HR non-homogeneous dehazing, stereo super-resolution, and bokeh effect transformation. By simply replacing the datasets and slightly changing the noise network, our model, named Refusion, is able to deal with large-size images (e.g., 6000 × 4000 × 3 in HR dehazing) and produces good results on all the above restoration problems. Our Refusion achieves the best perceptual performance in the NTIRE 2023 Image Shadow Removal Challenge and wins 2nd place overall.",
        "authors": [
            "Ziwei Luo",
            "Fredrik K. Gustafsson",
            "Zhengli Zhao",
            "Jens Sjolund",
            "T. Schon"
        ],
        "citations": 78,
        "references": 83,
        "year": 2023
    },
    {
        "title": "Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models",
        "abstract": "Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\\mathbf{\\ge 2\\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\\times$64, 1.93 on AFHQv2-Wild-64$\\times$64, and 2.72 on ImageNet-256$\\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.",
        "authors": [
            "Zhendong Wang",
            "Yifan Jiang",
            "Huangjie Zheng",
            "Peihao Wang",
            "Pengcheng He",
            "Zhangyang Wang",
            "Weizhu Chen",
            "Mingyuan Zhou"
        ],
        "citations": 71,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
        "abstract": "Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets. Due to their unsupervised training, controlling their behavior in downstream tasks, such as maximizing human-perceived image quality, image-text alignment, or ethical image generation, is difficult. Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators. In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end-to-end backpropagation of the reward gradient through the denoising process. While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable. We test AlignProp in finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations. We show AlignProp achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest. Code and Visualization results are available at https://align-prop.github.io/.",
        "authors": [
            "Mihir Prabhudesai",
            "Anirudh Goyal",
            "Deepak Pathak",
            "Katerina Fragkiadaki"
        ],
        "citations": 70,
        "references": 54,
        "year": 2023
    },
    {
        "title": "AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners",
        "abstract": "Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For example, AdaptDiffuser not only outperforms the previous art Diffuser by 20.8% on Maze2D and 7.5% on MuJoCo locomotion, but also adapts better to new tasks, e.g., KUKA pick-and-place, by 27.9% without requiring additional expert data. More visualization results and demo videos could be found on our project page.",
        "authors": [
            "Zhixuan Liang",
            "Yao Mu",
            "Mingyu Ding",
            "Fei Ni",
            "M. Tomizuka",
            "Ping Luo"
        ],
        "citations": 73,
        "references": 69,
        "year": 2023
    },
    {
        "title": "A Survey on Video Diffusion Models",
        "abstract": "The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.",
        "authors": [
            "Zhen Xing",
            "Qijun Feng",
            "Haoran Chen",
            "Qi Dai",
            "Hang-Rui Hu",
            "Hang Xu",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "citations": 75,
        "references": 320,
        "year": 2023
    },
    {
        "title": "Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models",
        "abstract": "We present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.",
        "authors": [
            "Litu Rout",
            "Negin Raoof",
            "Giannis Daras",
            "C. Caramanis",
            "A. Dimakis",
            "S. Shakkottai"
        ],
        "citations": 71,
        "references": 57,
        "year": 2023
    },
    {
        "title": "DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation",
        "abstract": "Talking head synthesis is a promising approach for the video production industry. Recently, a lot of effort has been devoted in this research area to improve the generation quality or enhance the model generalization. However, there are few works able to address both issues simultaneously, which is essential for practical applications. To this end, in this paper, we turn attention to the emerging powerful Latent Diffusion Models, and model the Talking head generation as an audio-driven temporally coherent denoising process (DiffTalk). More specifically, instead of employing audio signals as the single driving factor, we investigate the control mechanism of the talking face, and incorporate reference face images and landmarks as conditions for personality-aware generalized synthesis. In this way, the proposed DiffTalk is capable of producing high-quality talking head videos in synchronization with the source audio, and more importantly, it can be naturally generalized across different identities without further finetuning. Additionally, our DiffTalk can be gracefully tailored for higher-resolution synthesis with negligible extra computational cost. Extensive experiments show that the proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking head videos for generalized novel identities. For more video results, please refer to https://sstzal.github.io/DiffTalk/.",
        "authors": [
            "Shuai Shen",
            "Wenliang Zhao",
            "Zibin Meng",
            "Wanhua Li",
            "Zhengbiao Zhu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "citations": 76,
        "references": 57,
        "year": 2023
    },
    {
        "title": "GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models",
        "abstract": "In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object gener-ation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initial-ization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.",
        "authors": [
            "Taoran Yi",
            "Jiemin Fang",
            "Junjie Wang",
            "Guanjun Wu",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wenyu Liu",
            "Qi Tian",
            "Xinggang Wang"
        ],
        "citations": 75,
        "references": 102,
        "year": 2023
    },
    {
        "title": "Nearly d-Linear Convergence Bounds for Diffusion Models via Stochastic Localization",
        "abstract": "Denoising diffusions are a powerful method to generate approximate samples from high-dimensional data distributions. Recent results provide polynomial bounds on their convergence rate, assuming $L^2$-accurate scores. Until now, the tightest bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\\tilde O(\\frac{d \\log^2(1/\\delta)}{\\varepsilon^2})$ steps to approximate an arbitrary distribution on $\\mathbb{R}^d$ corrupted with Gaussian noise of variance $\\delta$ to within $\\varepsilon^2$ in KL divergence. Our proof extends the Girsanov-based methods of previous works. We introduce a refined treatment of the error from discretizing the reverse SDE inspired by stochastic localization.",
        "authors": [
            "Joe Benton",
            "Valentin De Bortoli",
            "A. Doucet",
            "George Deligiannidis"
        ],
        "citations": 75,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Score-Based Diffusion Models as Principled Priors for Inverse Imaging",
        "abstract": "Priors are essential for reconstructing images from noisy and/or incomplete measurements. The choice of the prior determines both the quality and uncertainty of recovered images. We propose turning score-based diffusion models into principled image priors (\"score-based priors\") for analyzing a posterior of images given measurements. Previously, probabilistic priors were limited to handcrafted regularizers and simple distributions. In this work, we empirically validate the theoretically-proven probability function of a score-based diffusion model. We show how to sample from resulting posteriors by using this probability function for variational inference. Our results, including experiments on denoising, deblurring, and interferometric imaging, suggest that score-based priors enable principled inference with a sophisticated, data-driven image prior.",
        "authors": [
            "Berthy T. Feng",
            "Jamie Smith",
            "Michael Rubinstein",
            "Huiwen Chang",
            "K. Bouman",
            "W. T. Freeman"
        ],
        "citations": 71,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data",
        "abstract": "Diffusion models achieve state-of-the-art performance in various generation tasks. However, their theoretical foundations fall far behind. This paper studies score approximation, estimation, and distribution recovery of diffusion models, when data are supported on an unknown low-dimensional linear subspace. Our result provides sample complexity bounds for distribution estimation using diffusion models. We show that with a properly chosen neural network architecture, the score function can be both accurately approximated and efficiently estimated. Furthermore, the generated distribution based on the estimated score function captures the data geometric structures and converges to a close vicinity of the data distribution. The convergence rate depends on the subspace dimension, indicating that diffusion models can circumvent the curse of data ambient dimensionality.",
        "authors": [
            "Minshuo Chen",
            "Kaixuan Huang",
            "Tuo Zhao",
            "Mengdi Wang"
        ],
        "citations": 69,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Diffusion Models are Minimax Optimal Distribution Estimators",
        "abstract": "While efficient distribution learning is no doubt behind the groundbreaking success of diffusion modeling, its theoretical guarantees are quite limited. In this paper, we provide the first rigorous analysis on approximation and generalization abilities of diffusion modeling for well-known function spaces. The highlight of this paper is that when the true density function belongs to the Besov space and the empirical score matching loss is properly minimized, the generated data distribution achieves the nearly minimax optimal estimation rates in the total variation distance and in the Wasserstein distance of order one. Furthermore, we extend our theory to demonstrate how diffusion models adapt to low-dimensional data distributions. We expect these results advance theoretical understandings of diffusion modeling and its ability to generate verisimilar outputs.",
        "authors": [
            "Kazusato Oko",
            "Shunta Akiyama",
            "Taiji Suzuki"
        ],
        "citations": 67,
        "references": 0,
        "year": 2023
    },
    {
        "title": "PTQD: Accurate Post-Training Quantization for Diffusion Models",
        "abstract": "Diffusion models have recently dominated image synthesis tasks. However, the iterative denoising process is expensive in computations at inference time, making diffusion models less practical for low-latency and scalable real-world applications. Post-training quantization (PTQ) of diffusion models can significantly reduce the model size and accelerate the sampling process without re-training. Nonetheless, applying existing PTQ methods directly to low-bit diffusion models can significantly impair the quality of generated samples. Specifically, for each denoising step, quantization noise leads to deviations in the estimated mean and mismatches with the predetermined variance schedule. As the sampling process proceeds, the quantization noise may accumulate, resulting in a low signal-to-noise ratio (SNR) during the later denoising steps. To address these challenges, we propose a unified formulation for the quantization noise and diffusion perturbed noise in the quantized denoising process. Specifically, we first disentangle the quantization noise into its correlated and residual uncorrelated parts regarding its full-precision counterpart. The correlated part can be easily corrected by estimating the correlation coefficient. For the uncorrelated part, we subtract the bias from the quantized results to correct the mean deviation and calibrate the denoising variance schedule to absorb the excess variance resulting from quantization. Moreover, we introduce a mixed-precision scheme for selecting the optimal bitwidth for each denoising step. Extensive experiments demonstrate that our method outperforms previous post-training quantized diffusion models, with only a 0.06 increase in FID score compared to full-precision LDM-4 on ImageNet 256x256, while saving 19.9x bit operations. Code is available at https://github.com/ziplab/PTQD.",
        "authors": [
            "Yefei He",
            "Luping Liu",
            "Jing Liu",
            "Weijia Wu",
            "Hong Zhou",
            "Bohan Zhuang"
        ],
        "citations": 67,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Monocular Depth Estimation using Diffusion Models",
        "abstract": "We formulate monocular depth estimation using denoising diffusion models, inspired by their recent successes in high fidelity image generation. To that end, we introduce innovations to address problems arising due to noisy, incomplete depth maps in training data, including step-unrolled denoising diffusion, an $L_1$ loss, and depth infilling during training. To cope with the limited availability of data for supervised training, we leverage pre-training on self-supervised image-to-image translation tasks. Despite the simplicity of the approach, with a generic loss and architecture, our DepthGen model achieves SOTA performance on the indoor NYU dataset, and near SOTA results on the outdoor KITTI dataset. Further, with a multimodal posterior, DepthGen naturally represents depth ambiguity (e.g., from transparent surfaces), and its zero-shot performance combined with depth imputation, enable a simple but effective text-to-3D pipeline. Project page: https://depth-gen.github.io",
        "authors": [
            "Saurabh Saxena",
            "Abhishek Kar",
            "Mohammad Norouzi",
            "David J. Fleet"
        ],
        "citations": 66,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency",
        "abstract": "Diffusion models have recently emerged as powerful generative priors for solving inverse problems. However, training diffusion models in the pixel space are both data-intensive and computationally demanding, which restricts their applicability as priors for high-dimensional real-world data such as medical images. Latent diffusion models, which operate in a much lower-dimensional space, offer a solution to these challenges. However, incorporating latent diffusion models to solve inverse problems remains a challenging problem due to the nonlinearity of the encoder and decoder. To address these issues, we propose \\textit{ReSample}, an algorithm that can solve general inverse problems with pre-trained latent diffusion models. Our algorithm incorporates data consistency by solving an optimization problem during the reverse sampling process, a concept that we term as hard data consistency. Upon solving this optimization problem, we propose a novel resampling scheme to map the measurement-consistent sample back onto the noisy data manifold and theoretically demonstrate its benefits. Lastly, we apply our algorithm to solve a wide range of linear and nonlinear inverse problems in both natural and medical images, demonstrating that our approach outperforms existing state-of-the-art approaches, including those based on pixel-space diffusion models.",
        "authors": [
            "Bowen Song",
            "Soo Min Kwon",
            "Zecheng Zhang",
            "Xinyu Hu",
            "Qing Qu",
            "Liyue Shen"
        ],
        "citations": 66,
        "references": 62,
        "year": 2023
    },
    {
        "title": "All are Worth Words: A ViT Backbone for Diffusion Models",
        "abstract": "Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and classconditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256×256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-based image modeling, the long skip connection is crucial while the down-sampling and upsampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can provide insights for future research on backbones in diffusion models and benefit generative modeling on large scale cross-modality datasets.",
        "authors": [
            "Fan Bao",
            "Shen Nie",
            "Kaiwen Xue",
            "Yue Cao",
            "Chongxuan Li",
            "Hang Su",
            "Jun Zhu"
        ],
        "citations": 218,
        "references": 89,
        "year": 2022
    },
    {
        "title": "Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models",
        "abstract": "Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multi-modality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several baselines in simulated planar robot and 7-dof robot arm manipulator environments. To assess the generalization capabilities of our method, we test it in environments with previously unseen obstacles. Our experiments show that diffusion models are strong priors to encode high-dimensional trajectory distributions of robot motions. https://sites.google.com/view/mp-diffusion",
        "authors": [
            "João Carvalho",
            "An T. Le",
            "Mark Baierl",
            "Dorothea Koert",
            "Jan Peters"
        ],
        "citations": 65,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models",
        "abstract": "Detecting fake images is becoming a major goal of computer vision. This need is becoming more and more pressing with the continuous improvement of synthesis methods based on Generative Adversarial Networks (GAN), and even more with the appearance of powerful methods based on Diffusion Models (DM). Towards this end, it is important to gain insight into which image features better discriminate fake images from real ones. In this paper we report on our systematic study of a large number of image generators of different families, aimed at discovering the most forensically relevant characteristics of real and generated images. Our experiments provide a number of interesting observations and shed light on some intriguing properties of synthetic images: (1) not only the GAN models but also the DM and VQ-GAN (Vector Quantized Generative Adversarial Networks) models give rise to visible artifacts in the Fourier domain and exhibit anomalous regular patterns in the autocorrelation; (2) when the dataset used to train the model lacks sufficient variety, its biases can be transferred to the generated images; (3) synthetic and real images exhibit significant differences in the mid-high frequency signal content, observable in their radial and angular spectral power distributions.",
        "authors": [
            "Riccardo Corvi",
            "D. Cozzolino",
            "G. Poggi",
            "Koki Nagano",
            "L. Verdoliva"
        ],
        "citations": 62,
        "references": 66,
        "year": 2023
    },
    {
        "title": "DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning",
        "abstract": "Diffusion models have proven to be highly effective in generating high-quality images. However, adapting large pre-trained diffusion models to new domains remains an open challenge, which is critical for real-world applications. This paper proposes DiffFit, a parameter-efficient strategy to fine-tune large pre-trained diffusion models that enable fast adaptation to new domains. DiffFit is embarrassingly simple that only fine-tunes the bias term and newly-added scaling factors in specific layers, yet resulting in significant training speed-up and reduced model storage costs. Compared with full fine-tuning, DiffFit achieves 2× training speed-up and only needs to store approximately 0.12% of the total model parameters. Intuitive theoretical analysis has been provided to justify the efficacy of scaling factors on fast adaptation. On 8 downstream datasets, DiffFit achieves superior or competitive performances compared to the full fine-tuning while being more efficient. Remarkably, we show that DiffFit can adapt a pre-trained low-resolution generative model to a high-resolution one by adding minimal cost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of 3.02 on ImageNet 512×512 benchmark by fine-tuning only 25 epochs from a public pre-trained ImageNet 256×256 checkpoint while being 30× more training efficient than the closest competitor.",
        "authors": [
            "Enze Xie",
            "Lewei Yao",
            "Han Shi",
            "Zhili Liu",
            "Daquan Zhou",
            "Zhaoqiang Liu",
            "Jiawei Li",
            "Zhenguo Li"
        ],
        "citations": 64,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Editing Implicit Assumptions in Text-to-Image Diffusion Models",
        "abstract": "Text-to-image diffusion models often make implicit assumptions about the world when generating images. While some assumptions are useful (e.g., the sky is blue), they can also be outdated, incorrect, or reflective of social biases present in the training data. Thus, there is a need to control these assumptions without requiring explicit user input or costly re-training. In this work, we aim to edit a given implicit assumption in a pre-trained diffusion model. Our Text-to-Image Model Editing method, TIME for short, receives a pair of inputs: a \"source\" under-specified prompt for which the model makes an implicit assumption (e.g., \"a pack of roses\"), and a \"destination\" prompt that describes the same setting, but with a specified desired attribute (e.g., \"a pack of blue roses\"). TIME then updates the model’s cross-attention layers, as these layers assign visual meaning to textual tokens. We edit the projection matrices in these layers such that the source prompt is projected close to the destination prompt. Our method is highly efficient, as it modifies a mere 2.2% of the model’s parameters in under one second. To evaluate model editing approaches, we introduce TIMED (TIME Dataset), containing 147 source and destination prompt pairs from various domains. Our experiments (using Stable Diffusion) show that TIME is successful in model editing, generalizes well for related prompts unseen during editing, and imposes minimal effect on unrelated generations.1",
        "authors": [
            "Hadas Orgad",
            "Bahjat Kawar",
            "Yonatan Belinkov"
        ],
        "citations": 64,
        "references": 80,
        "year": 2023
    },
    {
        "title": "MotionDirector: Motion Customization of Text-to-Video Diffusion Models",
        "abstract": "Large-scale pre-trained diffusion models have exhibited remarkable capabilities in diverse video generations. Given a set of video clips of the same motion concept, the task of Motion Customization is to adapt existing text-to-video diffusion models to generate videos with this motion. For example, generating a video with a car moving in a prescribed manner under specific camera movements to make a movie, or a video illustrating how a bear would lift weights to inspire creators. Adaptation methods have been developed for customizing appearance like subject or style, yet unexplored for motion. It is straightforward to extend mainstream adaption methods for motion customization, including full model tuning, parameter-efficient tuning of additional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept learned by these methods is often coupled with the limited appearances in the training videos, making it difficult to generalize the customized motion to other appearances. To overcome this challenge, we propose MotionDirector, with a dual-path LoRAs architecture to decouple the learning of appearance and motion. Further, we design a novel appearance-debiased temporal loss to mitigate the influence of appearance on the temporal training objective. Experimental results show the proposed method can generate videos of diverse appearances for the customized motions. Our method also supports various downstream applications, such as the mixing of different videos with their appearance and motion respectively, and animating a single image with customized motions. Our code and model weights will be released.",
        "authors": [
            "Rui Zhao",
            "Yuchao Gu",
            "Jay Zhangjie Wu",
            "David Junhao Zhang",
            "Jia-Wei Liu",
            "Weijia Wu",
            "J. Keppo",
            "Mike Zheng Shou"
        ],
        "citations": 64,
        "references": 53,
        "year": 2023
    },
    {
        "title": "DiffCollage: Parallel Generation of Large Content with Diffusion Models",
        "abstract": "We present DiffCollage, a compositional diffusion model that can generate large content by leveraging diffusion models trained on generating pieces of the large content. Our approach is based on a factor graph representation where each factor node represents a portion of the content and a variable node represents their overlap. This representation allows us to aggregate intermediate outputs from diffusion models defined on individual nodes to generate content of arbitrary size and shape in parallel without resorting to an autoregressive generation procedure. We apply DiffCollage to various tasks, including infinite image generation, panorama image generation, and long-duration text-guided motion generation. Extensive experimental results with a comparison to strong autoregressive baselines verify the effectiveness of our approach.",
        "authors": [
            "Qinsheng Zhang",
            "Jiaming Song",
            "Xun Huang",
            "Yongxin Chen",
            "Ming-Yu Liu"
        ],
        "citations": 64,
        "references": 75,
        "year": 2023
    },
    {
        "title": "UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models",
        "abstract": "Diffusion probabilistic models (DPMs) have demonstrated a very promising ability in high-resolution image synthesis. However, sampling from a pre-trained DPM is time-consuming due to the multiple evaluations of the denoising network, making it more and more important to accelerate the sampling of DPMs. Despite recent progress in designing fast samplers, existing methods still cannot generate satisfying images in many applications where fewer steps (e.g., $<$10) are favored. In this paper, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Combining UniP and UniC, we propose a unified predictor-corrector framework called UniPC for the fast sampling of DPMs, which has a unified analytical form for any order and can significantly improve the sampling quality over previous methods, especially in extremely few steps. We evaluate our methods through extensive experiments including both unconditional and conditional sampling using pixel-space and latent-space DPMs. Our UniPC can achieve 3.87 FID on CIFAR10 (unconditional) and 7.51 FID on ImageNet 256$\\times$256 (conditional) with only 10 function evaluations. Code is available at https://github.com/wl-zhao/UniPC.",
        "authors": [
            "Wenliang Zhao",
            "Lujia Bai",
            "Yongming Rao",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "citations": 145,
        "references": 47,
        "year": 2023
    },
    {
        "title": "TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation",
        "abstract": "Denoising Diffusion models have demonstrated their proficiency for generative sampling. However, generating good samples often requires many iterations. Consequently, techniques such as binary time-distillation (BTD) have been proposed to reduce the number of network calls for a fixed architecture. In this paper, we introduce TRAnsitive Closure Time-distillation (TRACT), a new method that extends BTD. For single step diffusion,TRACT improves FID by up to 2.4x on the same architecture, and achieves new single-step Denoising Diffusion Implicit Models (DDIM) state-of-the-art FID (7.4 for ImageNet64, 3.8 for CIFAR10). Finally we tease apart the method through extended ablations. The PyTorch implementation will be released soon.",
        "authors": [
            "David Berthelot",
            "Arnaud Autef",
            "Jierui Lin",
            "Dian Ang Yap",
            "Shuangfei Zhai",
            "Siyuan Hu",
            "Daniel Zheng",
            "Walter Talbot",
            "Eric Gu"
        ],
        "citations": 60,
        "references": 51,
        "year": 2023
    },
    {
        "title": "A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance",
        "abstract": "Diffusion models generate images by iterative denoising. Recent work has shown that by making the denoising process deterministic, one can encode real images into latent codes of the same size, which can be used for image editing. This paper explores the possibility of defining a latent space even when the denoising process remains stochastic. Recall that, in stochastic diffusion models, Gaussian noises are added in each denoising step, and we can concatenate all the noises to form a latent code. This results in a latent space of much higher dimensionality than the original image. We demonstrate that this latent space of stochastic diffusion models can be used in the same way as that of deterministic diffusion models in two applications. First, we propose CycleDiffusion, a method for zero-shot and unpaired image editing using stochastic diffusion models, which improves the performance over its deterministic counterpart. Second, we demonstrate unified, plug-and-play guidance in the latent spaces of deterministic and stochastic diffusion models.1",
        "authors": [
            "Chen Henry Wu",
            "Fernando De la Torre"
        ],
        "citations": 61,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Pyramid Diffusion Models For Low-light Image Enhancement",
        "abstract": "Recovering noise-covered details from low-light images is challenging, and the results given by previous methods leave room for improvement. Recent diffusion models show realistic and detailed image generation through a sequence of denoising refinements and motivate us to introduce them to low-light image enhancement for recovering realistic details. However, we found two problems when doing this, i.e., 1) diffusion models keep constant resolution in one reverse process, which limits the speed; 2) diffusion models sometimes result in global degradation (e.g., RGB shift). To address the above problems, this paper proposes a Pyramid Diffusion model (PyDiff) for low-light image enhancement. PyDiff uses a novel pyramid diffusion method to perform sampling in a pyramid resolution style (i.e., progressively increasing resolution in one reverse process). Pyramid diffusion makes PyDiff much faster than vanilla diffusion models and introduces no performance degradation. Furthermore, PyDiff uses a global corrector to alleviate the global degradation that may occur in the reverse process, significantly improving the performance and making the training of diffusion models easier with little additional computational consumption. Extensive experiments on popular benchmarks show that PyDiff achieves superior performance and efficiency. Moreover, PyDiff can generalize well to unseen noise and illumination distributions. Code and supplementary materials are available at https://github.com/limuloo/PyDIff.git.",
        "authors": [
            "Dewei Zhou",
            "Zongxin Yang",
            "Yi Yang"
        ],
        "citations": 59,
        "references": 44,
        "year": 2023
    },
    {
        "title": "3D-aware Image Generation using 2D Diffusion Models",
        "abstract": "In this paper, we introduce a novel 3D-aware image generation method that leverages 2D diffusion models. We formulate the 3D-aware image generation task as multiview 2D image set generation, and further to a sequential unconditional–conditional multiview image generation process. This allows us to utilize 2D diffusion models to boost the generative modeling power of the method. Additionally, we incorporate depth information from monocular depth estimators to construct the training data for the conditional diffusion model using only still images.We train our method on a large-scale unstructured 2D image dataset, i.e., ImageNet, which is not addressed by previous methods. It produces high-quality images that significantly outperform prior methods. Furthermore, our approach showcases its capability to generate instances with large view angles, even though the training images are diverse and unaligned, gathered from \"in-the-wild\" realworld environments.1",
        "authors": [
            "Jianfeng Xiang",
            "Jiaolong Yang",
            "Binbin Huang",
            "Xin Tong"
        ],
        "citations": 53,
        "references": 61,
        "year": 2023
    },
    {
        "title": "TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models",
        "abstract": "We present TexFusion (Texture Diffusion), a new method to synthesize textures for given 3D geometries, using large-scale text-guided image diffusion models. In contrast to recent works that leverage 2D text-to-image diffusion models to distill 3D objects using a slow and fragile optimization process, TexFusion introduces a new 3D-consistent generation technique specifically designed for texture synthesis that employs regular diffusion model sampling on different 2D rendered views. Specifically, we leverage latent diffusion models, apply the diffusion model’s denoiser on a set of 2D renders of the 3D object, and aggregate the different denoising predictions on a shared latent texture map. Final output RGB textures are produced by optimizing an intermediate neural color field on the decodings of 2D renders of the latent texture. We thoroughly validate TexFusion and show that we can efficiently generate diverse, high quality and globally coherent textures. We achieve state-of-the-art text-guided texture synthesis performance using only image diffusion models, while avoiding the pitfalls of previous distillation-based methods. The text-conditioning offers detailed control and we also do not rely on any ground truth 3D textures for training. This makes our method versatile and applicable to a broad range of geometry and texture types. We hope that TexFusion will advance AI-based texturing of 3D assets for applications in virtual reality, game design, simulation, and more.",
        "authors": [
            "Tianshi Cao",
            "Karsten Kreis",
            "Sanja Fidler",
            "Nicholas Sharp",
            "Kangxue Yin"
        ],
        "citations": 59,
        "references": 101,
        "year": 2023
    },
    {
        "title": "DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models",
        "abstract": "Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic uncertainties within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on probabilistic STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex ST dependencies. In this study, we present the first attempt to generalize the popular de-noising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7% over existing methods on three real-world datasets.",
        "authors": [
            "Haomin Wen",
            "Youfang Lin",
            "Yutong Xia",
            "Huaiyu Wan",
            "Roger Zimmermann",
            "Yuxuan Liang"
        ],
        "citations": 55,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Enhancing Deep Reinforcement Learning: A Tutorial on Generative Diffusion Models in Network Optimization",
        "abstract": "Generative Diffusion Models (GDMs) have emerged as a transformative force in the realm of Generative Artificial Intelligence (GenAI), demonstrating their versatility and efficacy across various applications. The ability to model complex data distributions and generate high-quality samples has made GDMs particularly effective in tasks such as image generation and reinforcement learning. Furthermore, their iterative nature, which involves a series of noise addition and denoising steps, is a powerful and unique approach to learning and generating data. This paper serves as a comprehensive tutorial on applying GDMs in network optimization tasks. We delve into the strengths of GDMs, emphasizing their wide applicability across various domains, such as vision, text, and audio generation. We detail how GDMs can be effectively harnessed to solve complex optimization problems inherent in networks. The paper first provides a basic background of GDMs and their applications in network optimization. This is followed by a series of case studies, showcasing the integration of GDMs with Deep Reinforcement Learning (DRL), incentive mechanism design, Semantic Communications (SemCom), Internet of Vehicles (IoV) networks, etc. These case studies underscore the practicality and efficacy of GDMs in real-world scenarios, offering insights into network design. We conclude with a discussion on potential future directions for GDM research and applications, providing major insights into how they can continue to shape the future of network optimization.",
        "authors": [
            "Hongyang Du",
            "Ruichen Zhang",
            "Yinqiu Liu",
            "Jiacheng Wang",
            "Yi-Lan Lin",
            "Zonghang Li",
            "Dusist Niyato",
            "Jiawen Kang",
            "Zehui Xiong",
            "Shuguang Cui",
            "Bo Ai",
            "Haibo Zhou",
            "Dong In Kim"
        ],
        "citations": 52,
        "references": 238,
        "year": 2023
    },
    {
        "title": "Differentially Private Diffusion Models Generate Useful Synthetic Images",
        "abstract": "The ability to generate privacy-preserving synthetic versions of sensitive image datasets could unlock numerous ML applications currently constrained by data availability. Due to their astonishing image generation quality, diffusion models are a prime candidate for generating high-quality synthetic data. However, recent studies have found that, by default, the outputs of some diffusion models do not preserve training data privacy. By privately fine-tuning ImageNet pre-trained diffusion models with more than 80M parameters, we obtain SOTA results on CIFAR-10 and Camelyon17 in terms of both FID and the accuracy of downstream classifiers trained on synthetic data. We decrease the SOTA FID on CIFAR-10 from 26.2 to 9.8, and increase the accuracy from 51.0% to 88.0%. On synthetic data from Camelyon17, we achieve a downstream accuracy of 91.1% which is close to the SOTA of 96.5% when training on the real data. We leverage the ability of generative models to create infinite amounts of data to maximise the downstream prediction performance, and further show how to use synthetic data for hyperparameter tuning. Our results demonstrate that diffusion models fine-tuned with differential privacy can produce useful and provably private synthetic data, even in applications with significant distribution shift between the pre-training and fine-tuning distributions.",
        "authors": [
            "Sahra Ghalebikesabi",
            "Leonard Berrada",
            "Sven Gowal",
            "Ira Ktena",
            "Robert Stanforth",
            "Jamie Hayes",
            "Soham De",
            "Samuel L. Smith",
            "Olivia Wiles",
            "Borja Balle"
        ],
        "citations": 55,
        "references": 93,
        "year": 2023
    },
    {
        "title": "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets",
        "abstract": "Diffusion models have achieved great success in a range of tasks, such as image synthesis and molecule design. As such successes hinge on large-scale training data collected from diverse sources, the trustworthiness of these collected data is hard to control or audit. In this work, we aim to explore the vulnerabilities of diffusion models under potential training data manipulations and try to answer: How hard is it to perform Trojan attacks on well-trained diffusion models? What are the adversarial targets that such Trojan attacks can achieve? To answer these questions, we propose an effective Trojan attack against diffusion models, TrojDiff, which optimizes the Trojan diffusion and generative processes during training. In particular, we design novel transitions during the Trojan diffusion process to diffuse adversarial targets into a biased Gaussian distribution and propose a new parameterization of the Trojan generative process that leads to an effective training objective for the attack. In addition, we consider three types of adversarial targets: the Trojaned diffusion models will always output instances belonging to a certain class from the in-domain distribution (In-D2D attack), out-of-domain distribution (Out-D2D-attack), and one specific instance (D2I attack). We evaluate TrojDiff on CIFAR-10 and CelebA datasets against both DDPM and DDIM diffusion models. We show that TrojDiff always achieves high attack performance under different adversarial targets using different types of triggers, while the performance in benign environments is preserved. The code is available at https://github.com/chenweixin107/TrojDiff.",
        "authors": [
            "Weixin Chen",
            "D. Song",
            "Bo Li"
        ],
        "citations": 54,
        "references": 57,
        "year": 2023
    },
    {
        "title": "BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping",
        "abstract": "Diffusion models have demonstrated excellent potential for generating diverse images. However, their performance often suffers from slow generation due to iterative denoising. Knowledge distillation has been recently proposed as a remedy that can reduce the number of inference steps to one or a few without significant quality degradation. However, existing distillation methods either require significant amounts of offline computation for generating synthetic training data from the teacher model or need to perform expensive online learning with the help of real data. In this work, we present a novel technique called BOOT, that overcomes these limitations with an efficient data-free distillation algorithm. The core idea is to learn a time-conditioned model that predicts the output of a pre-trained diffusion model teacher given any time step. Such a model can be efficiently trained based on bootstrapping from two consecutive sampled steps. Furthermore, our method can be easily adapted to large-scale text-to-image diffusion models, which are challenging for conventional methods given the fact that the training sets are often large and difficult to access. We demonstrate the effectiveness of our approach on several benchmark datasets in the DDIM setting, achieving comparable generation quality while being orders of magnitude faster than the diffusion teacher. The text-to-image results show that the proposed approach is able to handle highly complex distributions, shedding light on more efficient generative modeling.",
        "authors": [
            "Jiatao Gu",
            "Shuangfei Zhai",
            "Yizhe Zhang",
            "Lingjie Liu",
            "J. Susskind"
        ],
        "citations": 57,
        "references": 63,
        "year": 2023
    },
    {
        "title": "ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models",
        "abstract": "Personalizing generative models offers a way to guide image generation with user-provided references. Current personalization methods can invert an object or concept into the textual conditioning space and compose new natural sentences for text-to-image diffusion models. However, representing and editing specific visual attributes such as material, style, and layout remains a challenge, leading to a lack of disentanglement and editability. To address this problem, we propose a novel approach that leverages the step-by-step generation process of diffusion models, which generate images from low to high frequency information, providing a new perspective on representing, generating, and editing images. We develop the Prompt Spectrum Space P*, an expanded textual conditioning space, and a new image representation method called ProSpect. ProSpect represents an image as a collection of inverted textual token embeddings encoded from per-stage prompts, where each prompt corresponds to a specific generation stage (i.e., a group of consecutive steps) of the diffusion model. Experimental results demonstrate that P* and ProSpect offer better disentanglement and controllability compared to existing methods. We apply ProSpect in various personalized attribute-aware image generation applications, such as image-guided or text-driven manipulations of materials, style, and layout, achieving previously unattainable results from a single image input without fine-tuning the diffusion models. Our source code is available at https://github.com/zyxElsa/ProSpect.",
        "authors": [
            "Yu-xin Zhang",
            "Weiming Dong",
            "Fan Tang",
            "Nisha Huang",
            "Haibin Huang",
            "Chongyang Ma",
            "Tong-Yee Lee",
            "O. Deussen",
            "Changsheng Xu"
        ],
        "citations": 55,
        "references": 102,
        "year": 2023
    },
    {
        "title": "Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models",
        "abstract": "We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at https://github.com/layer6ai-labs/dgm-eval.",
        "authors": [
            "G. Stein",
            "Jesse C. Cresswell",
            "Rasa Hosseinzadeh",
            "Yi Sui",
            "Brendan Leigh Ross",
            "Valentin Villecroze",
            "Zhaoyan Liu",
            "Anthony L. Caterini",
            "J. E. T. Taylor",
            "G. Loaiza-Ganem"
        ],
        "citations": 58,
        "references": 112,
        "year": 2023
    },
    {
        "title": "The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation",
        "abstract": "Denoising diffusion probabilistic models have transformed image generation with their impressive fidelity and diversity. We show that they also excel in estimating optical flow and monocular depth, surprisingly, without task-specific architectures and loss functions that are predominant for these tasks. Compared to the point estimates of conventional regression-based methods, diffusion models also enable Monte Carlo inference, e.g., capturing uncertainty and ambiguity in flow and depth. With self-supervised pre-training, the combined use of synthetic and real data for supervised training, and technical innovations (infilling and step-unrolled denoising diffusion training) to handle noisy-incomplete training data, and a simple form of coarse-to-fine refinement, one can train state-of-the-art diffusion models for depth and optical flow estimation. Extensive experiments focus on quantitative performance against benchmarks, ablations, and the model's ability to capture uncertainty and multimodality, and impute missing values. Our model, DDVM (Denoising Diffusion Vision Model), obtains a state-of-the-art relative depth error of 0.074 on the indoor NYU benchmark and an Fl-all outlier rate of 3.26\\% on the KITTI optical flow benchmark, about 25\\% better than the best published method. For an overview see https://diffusion-vision.github.io.",
        "authors": [
            "Saurabh Saxena",
            "Charles Herrmann",
            "Junhwa Hur",
            "Abhishek Kar",
            "Mohammad Norouzi",
            "Deqing Sun",
            "David J. Fleet"
        ],
        "citations": 53,
        "references": 88,
        "year": 2023
    },
    {
        "title": "LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation",
        "abstract": "Layout-to-image generation refers to the task of synthesizing photo-realistic images based on semantic layouts. In this paper, we propose LayoutDiffuse that adapts a foundational diffusion model pretrained on large-scale image or text-image datasets for layout-to-image generation. By adopting a novel neural adaptor based on layout attention and task-aware prompts, our method trains efficiently, generates images with both high perceptual quality and layout alignment, and needs less data. Experiments on three datasets show that our method significantly outperforms other 10 generative models based on GANs, VQ-VAE, and diffusion models.",
        "authors": [
            "Jiaxin Cheng",
            "Xiao Liang",
            "Xingjian Shi",
            "Tong He",
            "Tianjun Xiao",
            "Mu Li"
        ],
        "citations": 54,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Diffusion Models in Vision: A Survey",
        "abstract": "Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.",
        "authors": [
            "Florinel-Alin Croitoru",
            "Vlad Hondru",
            "Radu Tudor Ionescu",
            "M. Shah"
        ],
        "citations": 894,
        "references": 171,
        "year": 2022
    },
    {
        "title": "Practical and Asymptotically Exact Conditional Sampling in Diffusion Models",
        "abstract": "Diffusion models have been successful on a range of conditional generation tasks including molecular design and text-to-image generation. However, these achievements have primarily depended on task-specific conditional training or error-prone heuristic approximations. Ideally, a conditional generation method should provide exact samples for a broad range of conditional distributions without requiring task-specific training. To this end, we introduce the Twisted Diffusion Sampler, or TDS. TDS is a sequential Monte Carlo (SMC) algorithm that targets the conditional distributions of diffusion models through simulating a set of weighted particles. The main idea is to use twisting, an SMC technique that enjoys good computational efficiency, to incorporate heuristic approximations without compromising asymptotic exactness. We first find in simulation and in conditional image generation tasks that TDS provides a computational statistical trade-off, yielding more accurate approximations with many particles but with empirical improvements over heuristics with as few as two particles. We then turn to motif-scaffolding, a core task in protein design, using a TDS extension to Riemannian diffusion models. On benchmark test cases, TDS allows flexible conditioning criteria and often outperforms the state of the art.",
        "authors": [
            "Luhuan Wu",
            "Brian L. Trippe",
            "C. A. Naesseth",
            "D. Blei",
            "J. Cunningham"
        ],
        "citations": 50,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Diffusion Models Without Attention",
        "abstract": "In recent advancements in high-fidelity image generation, Denoising Diffusion Probabilistic Models (DDPMs) have emerged as a key player. However, their application at high resolutions presents significant computational challenges. Current methods, such as patchifying, expedite processes in UNet and Transformer architectures but at the expense of rep-resentational capacity. Addressing this, we introduce the Dif-fusion State Space Model (DIFFUSSM), an architecture that supplants attention mechanisms with a more scalable state space model backbone. This approach effectively handles higher resolutions without resorting to global compression, thus preserving detailed image representation throughout the diffusion process. Our focus on FLOP-efficient architectures in diffusion training marks a significant step forward. Comprehensive evaluations on both ImageNet and LSUN datasets at two resolutions demonstrate that DiffuSSMs are on par or even outperform existing diffusion models with attention modules in FID and Inception Score metrics while significantly reducing total FLOP usage.",
        "authors": [
            "Jing Nathan Yan",
            "Jiatao Gu",
            "Alexander M. Rush"
        ],
        "citations": 50,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Reflected Diffusion Models",
        "abstract": "Score-based diffusion models learn to reverse a stochastic differential equation that maps data to noise. However, for complex tasks, numerical error can compound and result in highly unnatural samples. Previous work mitigates this drift with thresholding, which projects to the natural data domain (such as pixel space for images) after each diffusion step, but this leads to a mismatch between the training and generative processes. To incorporate data constraints in a principled manner, we present Reflected Diffusion Models, which instead reverse a reflected stochastic differential equation evolving on the support of the data. Our approach learns the perturbed score function through a generalized score matching loss and extends key components of standard diffusion models including diffusion guidance, likelihood-based training, and ODE sampling. We also bridge the theoretical gap with thresholding: such schemes are just discretizations of reflected SDEs. On standard image benchmarks, our method is competitive with or surpasses the state of the art without architectural modifications and, for classifier-free guidance, our approach enables fast exact sampling with ODEs and produces more faithful samples under high guidance weight.",
        "authors": [
            "Aaron Lou",
            "Stefano Ermon"
        ],
        "citations": 44,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?",
        "abstract": "Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online services such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents. Our codes are available at https://github.com/chiayi-hsu/Ring-A-Bell.",
        "authors": [
            "Yu-Lin Tsai",
            "Chia-Yi Hsu",
            "Chulin Xie",
            "Chih-Hsun Lin",
            "Jia-You Chen",
            "Bo Li",
            "Pin-Yu Chen",
            "Chia-Mu Yu",
            "Chun-ying Huang"
        ],
        "citations": 49,
        "references": 40,
        "year": 2023
    },
    {
        "title": "To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images ... For Now",
        "abstract": "The recent advances in diffusion models (DMs) have revolutionized the generation of realistic and complex images. However, these models also introduce potential safety hazards, such as producing harmful content and infringing data copyrights. Despite the development of safety-driven unlearning techniques to counteract these challenges, doubts about their efficacy persist. To tackle this issue, we introduce an evaluation framework that leverages adversarial prompts to discern the trustworthiness of these safety-driven DMs after they have undergone the process of unlearning harmful concepts. Specifically, we investigated the adversarial robustness of DMs, assessed by adversarial prompts, when eliminating unwanted concepts, styles, and objects. We develop an effective and efficient adversarial prompt generation approach for DMs, termed UnlearnDiffAtk. This method capitalizes on the intrinsic classification abilities of DMs to simplify the creation of adversarial prompts, thereby eliminating the need for auxiliary classification or diffusion models. Through extensive benchmarking, we evaluate the robustness of widely-used safety-driven unlearned DMs (i.e., DMs after unlearning undesirable concepts, styles, or objects) across a variety of tasks. Our results demonstrate the effectiveness and efficiency merits of UnlearnDiffAtk over the state-of-the-art adversarial prompt generation method and reveal the lack of robustness of current safetydriven unlearning techniques when applied to DMs. Codes are available at https://github.com/OPTML-Group/Diffusion-MU-Attack. WARNING: There exist AI generations that may be offensive in nature.",
        "authors": [
            "Yimeng Zhang",
            "Jinghan Jia",
            "Xin Chen",
            "Aochuan Chen",
            "Yihua Zhang",
            "Jiancheng Liu",
            "Ke Ding",
            "Sijia Liu"
        ],
        "citations": 48,
        "references": 94,
        "year": 2023
    },
    {
        "title": "AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models",
        "abstract": "Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edited) audio as conditions and generating output (edited) audio; 2) it can automatically learn to only modify segments that need to be edited by comparing the difference between the input and output audio; 3) it only needs edit instructions instead of full target audio descriptions as text input. AUDIT achieves state-of-the-art results in both objective and subjective metrics for several audio editing tasks (e.g., adding, dropping, replacement, inpainting, super-resolution). Demo samples are available at https://audit-demo.github.io/.",
        "authors": [
            "Yuancheng Wang",
            "Zeqian Ju",
            "Xuejiao Tan",
            "Lei He",
            "Zhizheng Wu",
            "Jiang Bian",
            "Sheng Zhao"
        ],
        "citations": 44,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts",
        "abstract": "Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered\"safe\"can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models.",
        "authors": [
            "Zhi-Yi Chin",
            "Chieh-Ming Jiang",
            "Ching-Chun Huang",
            "Pin-Yu Chen",
            "Wei-Chen Chiu"
        ],
        "citations": 48,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Open-Vocabulary Segmentation",
        "abstract": null,
        "authors": [
            "Laurynas Karazija",
            "Iro Laina",
            "A. Vedaldi",
            "C. Rupprecht"
        ],
        "citations": 47,
        "references": 85,
        "year": 2023
    },
    {
        "title": "Fast Training of Diffusion Models with Masked Transformers",
        "abstract": "We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance.",
        "authors": [
            "Hongkai Zheng",
            "Weili Nie",
            "Arash Vahdat",
            "Anima Anandkumar"
        ],
        "citations": 48,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning",
        "abstract": "With the help of conditioning mechanisms, the state-of-the-art diffusion models have achieved tremendous success in guided image generation, particularly in text-to-image synthesis. To gain a better understanding of the training process and potential risks of text-to-image synthesis, we perform a systematic investigation of backdoor attack on text-to-image diffusion models and propose BadT2I, a general multimodal backdoor attack framework that tampers with image synthesis in diverse semantic levels. Specifically, we perform backdoor attacks on three levels of the vision semantics: Pixel-Backdoor, Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, our methods efficiently inject backdoors into a large-scale text-to-image diffusion model while preserving its utility with benign inputs. We conduct empirical experiments on Stable Diffusion, the widely-used text-to-image diffusion model, demonstrating that the large-scale diffusion model can be easily backdoored within a few fine-tuning steps. We conduct additional experiments to explore the impact of different types of textual triggers, as well as the backdoor persistence during further training, providing insights for the development of backdoor defense methods. Besides, our investigation may contribute to the copyright protection of text-to-image models in the future. Our Code: https://github.com/sf-zhai/BadT2I.",
        "authors": [
            "Shengfang Zhai",
            "Yinpeng Dong",
            "Qingni Shen",
            "Shih-Chieh Pu",
            "Yuejian Fang",
            "Hang Su"
        ],
        "citations": 48,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model",
        "abstract": "Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for De-noising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal re-ward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. Our code is publicly available at https://github.com/yk7333/D3PO.",
        "authors": [
            "Kai Yang",
            "Jian Tao",
            "Jiafei Lyu",
            "Chunjiang Ge",
            "Jiaxin Chen",
            "Qimai Li",
            "Weihan Shen",
            "Xiaolong Zhu",
            "Xiu Li"
        ],
        "citations": 44,
        "references": 65,
        "year": 2023
    },
    {
        "title": "ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models",
        "abstract": "In this work, we investigate the capability of generating images from pre-trained diffusion models at much higher resolutions than the training image sizes. In addition, the generated images should have arbitrary image aspect ratios. When generating images directly at a higher resolution, 1024 x 1024, with the pre-trained Stable Diffusion using training images of resolution 512 x 512, we observe persistent problems of object repetition and unreasonable object structures. Existing works for higher-resolution generation, such as attention-based and joint-diffusion approaches, cannot well address these issues. As a new perspective, we examine the structural components of the U-Net in diffusion models and identify the crucial cause as the limited perception field of convolutional kernels. Based on this key observation, we propose a simple yet effective re-dilation that can dynamically adjust the convolutional perception field during inference. We further propose the dispersed convolution and noise-damped classifier-free guidance, which can enable ultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our approach does not require any training or optimization. Extensive experiments demonstrate that our approach can address the repetition issue well and achieve state-of-the-art performance on higher-resolution image synthesis, especially in texture details. Our work also suggests that a pre-trained diffusion model trained on low-resolution images can be directly used for high-resolution visual generation without further tuning, which may provide insights for future research on ultra-high-resolution image and video synthesis.",
        "authors": [
            "Yin-Yin He",
            "Shaoshu Yang",
            "Haoxin Chen",
            "Xiaodong Cun",
            "Menghan Xia",
            "Yong Zhang",
            "Xintao Wang",
            "Ran He",
            "Qifeng Chen",
            "Ying Shan"
        ],
        "citations": 48,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation",
        "abstract": "Talking face generation has historically struggled to produce head movements and natural facial expressions without guidance from additional reference videos. Recent developments in diffusion-based generative models allow for more realistic and stable data synthesis and their performance on image and video generation has surpassed that of other generative models. In this work, we present an autoregressive diffusion model that requires only one identity image and audio sequence to generate a video of a realistic talking head. Our solution is capable of hallucinating head movements, facial expressions, such as blinks, and preserving a given background. We evaluate our model on two different datasets, achieving state-of-the-art results in expressiveness and smoothness on both of them.1",
        "authors": [
            "Michal Stypulkowski",
            "Konstantinos Vougioukas",
            "Sen He",
            "Maciej Ziȩba",
            "Stavros Petridis",
            "M. Pantic"
        ],
        "citations": 101,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Stable Bias: Analyzing Societal Representations in Diffusion Models",
        "abstract": "As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems' outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Our approach relies on characterizing the variation in generated images triggered by enumerating gender and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning different professions. This allows us to (1) identify specific bias trends, (2) provide targeted scores to directly compare models in terms of diversity and representation, and (3) jointly model interdependent social variables to support a multidimensional analysis. We leverage this method to analyze images generated by 3 popular TTI systems (Dall-E 2, Stable Diffusion v 1.4 and 2) and find that while all of their outputs show correlations with US labor demographics, they also consistently under-represent marginalized identities to different extents. We also release the datasets and low-code interactive bias exploration platforms developed for this work, as well as the necessary tools to similarly evaluate additional TTI systems.",
        "authors": [
            "A. Luccioni",
            "Christopher Akiki",
            "Margaret Mitchell",
            "Yacine Jernite"
        ],
        "citations": 139,
        "references": 100,
        "year": 2023
    },
    {
        "title": "Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models",
        "abstract": "The significant advances in applications of text-to-image generation models have prompted the demand of a post-hoc adaptation algorithms that can efficiently remove unwanted concepts (e.g. privacy, copyright, and safety) from a pretrained models with minimal influence on the existing knowledge system learned from pretraining. Existing methods mainly resort to explicitly finetuning unwanted concepts to be some alternatives such as their hypernyms or antonyms. Essentially, they are modifying the knowledge system of pretrained models by replacing unwanted to be something arbitrarily defined by user. Furthermore, these methods require hundreds of optimization steps, as they solely rely on denoising loss used for pretraining. To address above challenges, we propose Forget-Me-Not, a model-centric and efficient solution designed to remove identities, objects, or styles from a well-configured text-to-image model in as little as 30 seconds, without significantly impairing its ability to generate other content. In contrast to existing methods, we introduce attention re-steering loss to redirect model’s generation from unwanted concepts to those are learned during pretraining, rather than being user-defined. Furthermore, our method offers two practical extensions: a) removal of potentially harmful or NSFW content, and b) enhancement of model accuracy, inclusion and diversity through concept correction and disentanglement.",
        "authors": [
            "Eric Zhang",
            "Kai Wang",
            "Xingqian Xu",
            "Zhangyang Wang",
            "Humphrey Shi"
        ],
        "citations": 127,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Ground states for aggregation-diffusion models on Cartan-Hadamard manifolds",
        "abstract": "We consider a free energy functional on Cartan-Hadamard manifolds, and investigate the existence of its global minimizers. The energy functional consists of two components: an entropy (or internal energy) and an interaction energy modelled by an attractive potential. The two components have competing effects, as they favour spreading by linear diffusion and blow-up by nonlocal attractive interactions, respectively. We find necessary and sufficient conditions for existence of ground states for manifolds with sectional curvatures bounded above and below, respectively. In particular, for general Cartan-Hadamard manifolds, superlinear growth at infinity of the attractive potential prevents the spreading. The behaviour can be relaxed for homogeneous manifolds, for which only linear growth of the potential is sufficient for this purpose.",
        "authors": [
            "R. Fetecau",
            "Hansol Park"
        ],
        "citations": 101,
        "references": 48,
        "year": 2023
    },
    {
        "title": "DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models",
        "abstract": "Collecting and annotating images with pixel-wise labels is time-consuming and laborious. In contrast, synthetic data can be freely available using a generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that it is possible to automatically obtain accurate semantic masks of synthetic images generated by the Off-the-shelf Stable Diffusion model, which uses only text-image pairs during training. Our approach, termed DiffuMask, exploits the potential of the cross-attention map between text and image, which is natural and seamless to extend the text-driven image synthesis to semantic mask generation. DiffuMask uses text-guided cross-attention information to localize class/word-specific regions, which are combined with practical techniques to create a novel high-resolution and class-discriminative pixel-wise mask. The methods help to significantly reduce data collection and annotation costs. Experiments demonstrate that the existing segmentation methods trained on synthetic data of DiffuMask can achieve a competitive performance over the counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird), DiffuMask presents promising performance, close to the state-of-the-art result of real data (within 3% mIoU gap). Moreover, in the open-vocabulary segmentation (zero-shot) setting, DiffuMask achieves new state-of-the-art results on the Unseen classes of VOC 2012. The project website can be found at ${\\color{red}{\\text{DiffuMask}}}$.",
        "authors": [
            "Weijia Wu",
            "Yuzhong Zhao",
            "Mike Zheng Shou",
            "Hong Zhou",
            "Chunhua Shen"
        ],
        "citations": 101,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Cones: Concept Neurons in Diffusion Models for Customized Generation",
        "abstract": "Human brains respond to semantic features of presented stimuli with different neurons. It is then curious whether modern deep neural networks admit a similar behavior pattern. Specifically, this paper finds a small cluster of neurons in a diffusion model corresponding to a particular subject. We call those neurons the concept neurons. They can be identified by statistics of network gradients to a stimulation connected with the given subject. The concept neurons demonstrate magnetic properties in interpreting and manipulating generation results. Shutting them can directly yield the related subject contextualized in different scenes. Concatenating multiple clusters of concept neurons can vividly generate all related concepts in a single image. A few steps of further fine-tuning can enhance the multi-concept capability, which may be the first to manage to generate up to four different subjects in a single image. For large-scale applications, the concept neurons are environmentally friendly as we only need to store a sparse cluster of int index instead of dense float32 values of the parameters, which reduces storage consumption by 90\\% compared with previous subject-driven generation methods. Extensive qualitative and quantitative studies on diverse scenarios show the superiority of our method in interpreting and manipulating diffusion models.",
        "authors": [
            "Zhiheng Liu",
            "Ruili Feng",
            "Kai Zhu",
            "Yifei Zhang",
            "Kecheng Zheng",
            "Yu Liu",
            "Deli Zhao",
            "Jingren Zhou",
            "Yang Cao"
        ],
        "citations": 96,
        "references": 33,
        "year": 2023
    },
    {
        "title": "State of the Art on Diffusion Models for Visual Computing",
        "abstract": "The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion‐based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state‐of‐the‐art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion‐based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.",
        "authors": [
            "Ryan Po",
            "Wang Yifan",
            "Vladislav Golyanik",
            "Kfir Aberman",
            "J. Barron",
            "Amit H. Bermano",
            "Eric R. Chan",
            "Tali Dekel",
            "Aleksander Holynski",
            "Angjoo Kanazawa",
            "C. K. Liu",
            "Lingjie Liu",
            "B. Mildenhall",
            "M. Nießner",
            "Bjorn Ommer",
            "C. Theobalt",
            "Peter Wonka",
            "Gordon Wetzstein"
        ],
        "citations": 78,
        "references": 340,
        "year": 2023
    },
    {
        "title": "StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models",
        "abstract": "Content and style (C-S) disentanglement is a fundamental problem and critical challenge of style transfer. Existing approaches based on explicit definitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a new C-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the complementary style information, yielding interpretable and controllable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coordinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further leveraging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and flexible C-S disentanglement and trade-off control. Our work provides new insights into the C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled C-S characteristics.",
        "authors": [
            "Zhizhong Wang",
            "Lei Zhao",
            "Wei Xing"
        ],
        "citations": 79,
        "references": 106,
        "year": 2023
    },
    {
        "title": "Generative Semantic Communication: Diffusion Models Beyond Bit Recovery",
        "abstract": "Semantic communication is expected to be one of the cores of next-generation AI-based communications. One of the possibilities offered by semantic communication is the capability to regenerate, at the destination side, images or videos semantically equivalent to the transmitted ones, without necessarily recovering the transmitted sequence of bits. The current solutions still lack the ability to build complex scenes from the received partial information. Clearly, there is an unmet need to balance the effectiveness of generation methods and the complexity of the transmitted information, possibly taking into account the goal of communication. In this paper, we aim to bridge this gap by proposing a novel generative diffusion-guided framework for semantic communication that leverages the strong abilities of diffusion models in synthesizing multimedia content while preserving semantic features. We reduce bandwidth usage by sending highly-compressed semantic information only. Then, the diffusion model learns to synthesize semantic-consistent scenes through spatially-adaptive normalizations from such denoised semantic information. We prove, through an in-depth assessment of multiple scenarios, that our method outperforms existing solutions in generating high-quality images with preserved semantic information even in cases where the received content is significantly degraded. More specifically, our results show that objects, locations, and depths are still recognizable even in the presence of extremely noisy conditions of the communication channel. The code is available at https://github.com/ispamm/GESCO.",
        "authors": [
            "Eleonora Grassucci",
            "S. Barbarossa",
            "D. Comminiello"
        ],
        "citations": 42,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models",
        "abstract": "We present a method to create interpretable concept sliders that enable precise control over attributes in image generations from diffusion models. Our approach identifies a low-rank parameter direction corresponding to one concept while minimizing interference with other attributes. A slider is created using a small set of prompts or sample images; thus slider directions can be created for either textual or visual concepts. Concept Sliders are plug-and-play: they can be composed efficiently and continuously modulated, enabling precise control over image generation. In quantitative experiments comparing to previous editing techniques, our sliders exhibit stronger targeted edits with lower interference. We showcase sliders for weather, age, styles, and expressions, as well as slider compositions. We show how sliders can transfer latents from StyleGAN for intuitive editing of visual concepts for which textual description is difficult. We also find that our method can help address persistent quality issues in Stable Diffusion XL including repair of object deformations and fixing distorted hands. Our code, data, and trained sliders are available at https://sliders.baulab.info/",
        "authors": [
            "Rohit Gandikota",
            "Joanna Materzynska",
            "Tingrui Zhou",
            "Antonio Torralba",
            "David Bau"
        ],
        "citations": 42,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification",
        "abstract": "Diffusion models have been widely adopted in image generation, producing higher-quality and more diverse samples than generative adversarial networks (GANs). We introduce a latent diffusion model (LDM) for precipitation nowcasting - short-term forecasting based on the latest observational data. The LDM is more stable and requires less computation to train than GANs, albeit with more computationally expensive generation. We benchmark it against the GAN-based Deep Generative Models of Rainfall (DGMR) and a statistical model, PySTEPS. The LDM produces more accurate precipitation predictions, while the comparisons are more mixed when predicting whether the precipitation exceeds predefined thresholds. The clearest advantage of the LDM is that it generates more diverse predictions than DGMR or PySTEPS. Rank distribution tests indicate that the distribution of samples from the LDM accurately reflects the uncertainty of the predictions. Thus, LDMs are promising for any applications where uncertainty quantification is important, such as weather and climate.",
        "authors": [
            "J. Leinonen",
            "U. Hamann",
            "D. Nerini",
            "U. Germann",
            "Gabriele Franch"
        ],
        "citations": 42,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Membership Inference Attacks against Diffusion Models",
        "abstract": "Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., timesteps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then show that the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of timesteps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify that DDIM is vulnerable to the attack for small sample sizes instead of achieving a lower FID. Second, sampling steps in hyperparameters are important for resistance to the attack, whereas the impact of sampling variances is quite limited.",
        "authors": [
            "Tomoya Matsumoto",
            "Takayuki Miura",
            "Naoto Yanai"
        ],
        "citations": 42,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Perception Prioritized Training of Diffusion Models",
        "abstract": "Diffusion models learn to restore noisy data, which is corrupted with different levels of noise, by optimizing the weighted sum of the corresponding loss terms, i.e., denoising score matching loss. In this paper, we show that restoring data corrupted with certain noise levels offers a proper pretext task for the model to learn rich visual concepts. We propose to prioritize such noise levels over other levels during training, by redesigning the weighting scheme of the objective function. We show that our simple redesign of the weighting scheme significantly improves the performance of diffusion models regardless of the datasets, architectures, and sampling strategies.",
        "authors": [
            "Jooyoung Choi",
            "Jungbeom Lee",
            "Chaehun Shin",
            "Sungwon Kim",
            "Hyunwoo J. Kim",
            "Sung-Hoon Yoon"
        ],
        "citations": 197,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Style Injection in Diffusion: A Training-Free Approach for Adapting Large-Scale Diffusion Models for Style Transfer",
        "abstract": "Despite the impressive generative capabilities of diffusion models, existing diffusion model-based style transfer methods require inference-stage optimization (e.g. fine-tuning or textual inversion of style) which is time-consuming, or fails to leverage the generative ability of large-scale diffusion models. To address these issues, we introduce a novel artistic style transfer method based on a pre-trained large-scale diffusion model without any optimization. Specifically, we manipulate the features of self-attention layers as the way the cross-attention mechanism works; in the generation process, substituting the key and value of content with those of style image. This approach provides several desirable characteristics for style transfer including 1) preservation of content by transferring similar styles into similar image patches and 2) transfer of style based on similarity of local texture (e.g. edge) between content and style images. Furthermore, we introduce query preservation and attention temperature scaling to mitigate the issue of disruption of original content, and initial latent Adaptive Instance Normalization (AdaIN) to deal with the disharmonious color (failure to transfer the colors of style). Our experimental results demonstrate that our proposed method surpasses state-of-the-art methods in both conventional and diffusion-based style transfer baselines. Codes are available at github.com/jiwoogit/StyleID.",
        "authors": [
            "Jiwoo Chung",
            "Sangeek Hyun",
            "Jae-Pil Heo"
        ],
        "citations": 36,
        "references": 55,
        "year": 2023
    },
    {
        "title": "SpectralDiff: A Generative Framework for Hyperspectral Image Classification With Diffusion Models",
        "abstract": "Hyperspectral image (HSI) classification is an important issue in remote sensing field with extensive applications in Earth science. In recent years, a large number of deep learning-based HSI classification methods have been proposed. However, the existing methods have limited ability to handle high-dimensional, highly redundant, and complex data, making it challenging to capture the spectral–spatial distributions of data and relationships between samples. To address this issue, we propose a generative framework for HSI classification with diffusion models (SpectralDiff) that effectively mines the distribution information of high-dimensional and highly redundant data by iteratively denoising and explicitly constructing the data generation process, thus better reflecting the relationships between samples. The framework consists of a spectral–spatial diffusion module and an attention-based classification module. The spectral–spatial diffusion module adopts forward and reverse spectral–spatial diffusion processes to achieve adaptive construction of sample relationships without requiring prior knowledge of graphical structure or neighborhood information. It captures spectral–spatial distribution and contextual information of objects in HSI and mines unsupervised spectral–spatial diffusion features within the reverse diffusion process. Finally, these features are fed into the attention-based classification module for per-pixel classification. The diffusion features can facilitate cross-sample perception via reconstruction distribution, leading to improved classification performance. Experiments on three public HSI datasets demonstrate that the proposed method can achieve better performance than state-of-the-art methods. For the sake of reproducibility, the source code of SpectralDiff will be publicly available at https://github.com/chenning0115/SpectralDiff.",
        "authors": [
            "Ning Chen",
            "Jun Yue",
            "Leyuan Fang",
            "Shaobo Xia"
        ],
        "citations": 41,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Parallel Sampling of Diffusion Models",
        "abstract": "Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we improve sampling speed by 2-4x across a range of robotics and image generation models, giving state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable degradation of task reward, FID score, or CLIP score.",
        "authors": [
            "Andy Shih",
            "Suneel Belkhale",
            "Stefano Ermon",
            "Dorsa Sadigh",
            "Nima Anari"
        ],
        "citations": 37,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry",
        "abstract": "Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\\mathbf{x}_t \\in \\mathcal{X}$, we analyze them from a geometrical perspective. Our approach involves deriving the local latent basis within $\\mathcal{X}$ by leveraging the pullback metric associated with their encoding feature maps. Remarkably, our discovered local latent basis enables image editing capabilities by moving $\\mathbf{x}_t$, the latent space of DMs, along the basis vector at specific timesteps. We further analyze how the geometric structure of DMs evolves over diffusion timesteps and differs across different text conditions. This confirms the known phenomenon of coarse-to-fine generation, as well as reveals novel insights such as the discrepancy between $\\mathbf{x}_t$ across timesteps, the effect of dataset complexity, and the time-varying influence of text prompts. To the best of our knowledge, this paper is the first to present image editing through $\\mathbf{x}$-space traversal, editing only once at specific timestep $t$ without any additional training, and providing thorough analyses of the latent structure of DMs. The code to reproduce our experiments can be found at https://github.com/enkeejunior1/Diffusion-Pullback.",
        "authors": [
            "Yong-Hyun Park",
            "Mingi Kwon",
            "J. Choi",
            "Junghyo Jo",
            "Youngjung Uh"
        ],
        "citations": 37,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models",
        "abstract": "Diffusion models have become a popular approach for image generation and reconstruction due to their numerous advantages. However, most diffusion-based inverse problem-solving methods only deal with 2D images, and even recently published 3D methods do not fully exploit the 3D distribution prior. To address this, we propose a novel approach using two perpendicular pre-trained 2D diffusion models to solve the 3D inverse problem. By modeling the 3D data distribution as a product of 2D distributions sliced in different directions, our method effectively addresses the curse of dimensionality. Our experimental results demonstrate that our method is highly effective for 3D medical image reconstruction tasks, including MRI Z-axis super-resolution, compressed sensing MRI, and sparse-view CT. Our method can generate high-quality voxel volumes suitable for medical applications. The code is available at https://github.com/hyn2028/tpdm",
        "authors": [
            "Suhyeon Lee",
            "Hyungjin Chung",
            "Minyoung Park",
            "Jonghyuk Park",
            "Wi-Sun Ryu",
            "J. C. Ye"
        ],
        "citations": 38,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models",
        "abstract": "Recently large-scale language-image models (e.g., text-guided diffusion models) have considerably improved the image generation capabilities to generate photorealistic images in various domains. Based on this success, current image editing methods use texts to achieve intuitive and versatile modification of images. To edit a real image using diffusion models, one must first invert the image to a noisy latent from which an edited image is sampled with a target text prompt. However, most methods lack one of the following: user-friendliness (e.g., additional masks or precise descriptions of the input image are required), generalization to larger domains, or high fidelity to the input image. In this paper, we design an accurate and quick inversion technique, Prompt Tuning Inversion, for text-driven image editing. Specifically, our proposed editing method consists of a reconstruction stage and an editing stage. In the first stage, we encode the information of the input image into a learnable conditional embedding via Prompt Tuning Inversion. In the second stage, we apply classifier-free guidance to sample the edited image, where the conditional embedding is calculated by linearly interpolating between the target embedding and the optimized one obtained in the first stage. This technique ensures a superior trade-off between editability and high fidelity to the input image of our method. For example, we can change the color of a specific object while preserving its original shape and background under the guidance of only a target text prompt. Extensive experiments on ImageNet demonstrate the superior editing performance of our method compared to the state-of-the-art baselines.",
        "authors": [
            "Wenkai Dong",
            "Song Xue",
            "Xiaoyue Duan",
            "Shumin Han"
        ],
        "citations": 41,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Paint3D: Paint Anything 3D With Lighting-Less Texture Diffusion Models",
        "abstract": "This paper presents Paint3D, a novel coarse-to-fine generative framework that is capable of producing high-resolution, lighting-less, and diverse 2K UV texture maps for untextured 3D meshes conditioned on text or image inputs. The key challenge addressed is generating high-quality textures without embedded illumination information, which allows the textures to be re-lighted or re-edited within modern graphics pipelines. To achieve this, our method first leverages a pre-trained depth-aware 2D diffusion model to generate view-conditional images and per-form multi-view texture fusion, producing an initial coarse texture map. However, as 2D models cannot fully repre-sent 3D shapes and disable lighting effects, the coarse texture map exhibits incomplete areas and illumination artifacts. To resolve this, we train separate UV Inpainting and UVHD diffusion models specialized for the shape-aware re-finement of incomplete areas and the removal of illumination artifacts. Through this coarse-to-fine process, Paint3D can produce high-quality 2K UV textures that maintain se-mantic consistency while being lighting-less, significantly advancing the state-of-the-art in texturing 3D objects.",
        "authors": [
            "Xianfang Zeng",
            "Xin Chen",
            "Zhongqi Qi",
            "Wen Liu",
            "Zibo Zhao",
            "Zhibin Wang",
            "Bin Fu",
            "Yong Liu",
            "Gang Yu"
        ],
        "citations": 40,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Parallel Sampling of Diffusion Models",
        "abstract": "Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we improve sampling speed by 2-4x across a range of robotics and image generation models, giving state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable degradation of task reward, FID score, or CLIP score.",
        "authors": [
            "Andy Shih",
            "Suneel Belkhale",
            "Stefano Ermon",
            "Dorsa Sadigh",
            "Nima Anari"
        ],
        "citations": 37,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Score-based Diffusion Models in Function Space",
        "abstract": "Diffusion models have recently emerged as a powerful framework for generative modeling. They consist of a forward process that perturbs input data with Gaussian white noise and a reverse process that learns a score function to generate samples by denoising. Despite their tremendous success, they are mostly formulated on finite-dimensional spaces, e.g., Euclidean, limiting their applications to many domains where the data has a functional form, such as in scientific computing and 3D geometric data analysis. This work introduces a mathematically rigorous framework called Denoising Diffusion Operators (DDOs) for training diffusion models in function space. In DDOs, the forward process perturbs input functions gradually using a Gaussian process. The generative process is formulated by a function-valued annealed Langevin dynamic. Our approach requires an appropriate notion of the score for the perturbed data distribution, which we obtain by generalizing denoising score matching to function spaces that can be infinite-dimensional. We show that the corresponding discretized algorithm generates accurate samples at a fixed cost independent of the data resolution. We theoretically and numerically verify the applicability of our approach on a set of function-valued problems, including generating solutions to the Navier-Stokes equation viewed as the push-forward distribution of forcings from a Gaussian Random Field (GRF), as well as volcano InSAR and MNIST-SDF.",
        "authors": [
            "Jae Hyun Lim",
            "Nikola B. Kovachki",
            "R. Baptista",
            "Christopher Beckham",
            "K. Azizzadenesheli",
            "Jean Kossaifi",
            "Vikram S. Voleti",
            "Jiaming Song",
            "Karsten Kreis",
            "J. Kautz",
            "C. Pal",
            "Arash Vahdat",
            "Anima Anandkumar"
        ],
        "citations": 35,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Dream the Impossible: Outlier Imagination with Diffusion Models",
        "abstract": "Utilizing auxiliary outlier datasets to regularize the machine learning model has demonstrated promise for out-of-distribution (OOD) detection and safe prediction. Due to the labor intensity in data collection and cleaning, automating outlier data generation has been a long-desired alternative. Despite the appeal, generating photo-realistic outliers in the high dimensional pixel space has been an open challenge for the field. To tackle the problem, this paper proposes a new framework DREAM-OOD, which enables imagining photo-realistic outliers by way of diffusion models, provided with only the in-distribution (ID) data and classes. Specifically, DREAM-OOD learns a text-conditioned latent space based on ID data, and then samples outliers in the low-likelihood region via the latent, which can be decoded into images by the diffusion model. Different from prior works, DREAM-OOD enables visualizing and understanding the imagined outliers, directly in the pixel space. We conduct comprehensive quantitative and qualitative studies to understand the efficacy of DREAM-OOD, and show that training with the samples generated by DREAM-OOD can benefit OOD detection performance. Code is publicly available at https://github.com/deeplearning-wisc/dream-ood.",
        "authors": [
            "Xuefeng Du",
            "Yiyou Sun",
            "Xiaojin Zhu",
            "Yixuan Li"
        ],
        "citations": 35,
        "references": 134,
        "year": 2023
    },
    {
        "title": "Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models",
        "abstract": "We introduce W\\\"urstchen, a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models. A key contribution of our work is to develop a latent diffusion technique in which we learn a detailed but extremely compact semantic image representation used to guide the diffusion process. This highly compressed representation of an image provides much more detailed guidance compared to latent representations of language and this significantly reduces the computational requirements to achieve state-of-the-art results. Our approach also improves the quality of text-conditioned image generation based on our user preference study. The training requirements of our approach consists of 24,602 A100-GPU hours - compared to Stable Diffusion 2.1's 200,000 GPU hours. Our approach also requires less training data to achieve these results. Furthermore, our compact latent representations allows us to perform inference over twice as fast, slashing the usual costs and carbon footprint of a state-of-the-art (SOTA) diffusion model significantly, without compromising the end performance. In a broader comparison against SOTA models our approach is substantially more efficient and compares favorably in terms of image quality. We believe that this work motivates more emphasis on the prioritization of both performance and computational accessibility.",
        "authors": [
            "Pablo Pernias",
            "Dominic Rampas",
            "Mats L. Richter",
            "Christopher Pal",
            "Marc Aubreville"
        ],
        "citations": 35,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Membership Inference of Diffusion Models",
        "abstract": ". Recent years have witnessed the tremendous success of diffusion models in data synthesis. However, when diﬀusion models are applied to sensitive data, they also give rise to severe privacy concerns. In this paper, we systematically present the ﬁrst study about membership inference attacks against diﬀusion models, which aims to infer whether a sample was used to train the model. Two attack methods are proposed, namely loss-based and likelihood-based attacks. Our attack methods are evaluated on several state-of-the-art diﬀusion models, over diﬀerent datasets in relation to privacy-sensitive data. Extensive experimental evaluations show that our attacks can achieve remarkable performance. Furthermore, we exhaustively investigate various factors which can af-fect attack performance. Finally, we also evaluate the performance of our attack methods on diﬀusion models trained with diﬀerential privacy.",
        "authors": [
            "Hailong Hu",
            "Jun Pang"
        ],
        "citations": 35,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Patched Diffusion Models for Unsupervised Anomaly Detection in Brain MRI",
        "abstract": "The use of supervised deep learning techniques to detect pathologies in brain MRI scans can be challenging due to the diversity of brain anatomy and the need for annotated data sets. An alternative approach is to use unsupervised anomaly detection, which only requires sample-level labels of healthy brains to create a reference representation. This reference representation can then be compared to unhealthy brain anatomy in a pixel-wise manner to identify abnormalities. To accomplish this, generative models are needed to create anatomically consistent MRI scans of healthy brains. While recent diffusion models have shown promise in this task, accurately generating the complex structure of the human brain remains a challenge. In this paper, we propose a method that reformulates the generation task of diffusion models as a patch-based estimation of healthy brain anatomy, using spatial context to guide and improve reconstruction. We evaluate our approach on data of tumors and multiple sclerosis lesions and demonstrate a relative improvement of 25.1% compared to existing baselines.",
        "authors": [
            "Finn Behrendt",
            "Debayan Bhattacharya",
            "Julia Kruger",
            "R. Opfer",
            "A. Schlaefer"
        ],
        "citations": 33,
        "references": 41,
        "year": 2023
    },
    {
        "title": "On Memorization in Diffusion Models",
        "abstract": "Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuration, and training procedure. Besides comprehensive empirical results identifying the influential factors, we surprisingly find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models. Our study holds practical significance for diffusion model users and offers clues to theoretical research in deep generative models. Code is available at https://github.com/sail-sg/DiffMemorize.",
        "authors": [
            "Xiangming Gu",
            "Chao Du",
            "Tianyu Pang",
            "Chongxuan Li",
            "Min Lin",
            "Ye Wang"
        ],
        "citations": 31,
        "references": 53,
        "year": 2023
    },
    {
        "title": "I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors",
        "abstract": "Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic metaphors and their associated visual elaborations. Evaluation by professional illustrators shows the promise of LLM-Diffusion Model collaboration for this task . To evaluate the utility of our Human-AI collaboration framework and the quality of our dataset, we perform both an intrinsic human-based evaluation and an extrinsic evaluation using visual entailment as a downstream task.",
        "authors": [
            "Tuhin Chakrabarty",
            "Arkadiy Saakyan",
            "Olivia Winn",
            "Artemis Panagopoulou",
            "Yue Yang",
            "Marianna Apidianaki",
            "S. Muresan"
        ],
        "citations": 33,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Imperceptible and Transferable Adversarial Attack",
        "abstract": "Many existing adversarial attacks generate <inline-formula><tex-math notation=\"LaTeX\">$L_{p}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"shi-ieq1-3480519.gif\"/></alternatives></inline-formula>-norm perturbations on image RGB space. Despite some achievements in transferability and attack success rate, the crafted adversarial examples are easily perceived by human eyes. Towards visual imperceptibility, some recent works explore unrestricted attacks without <inline-formula><tex-math notation=\"LaTeX\">$L_{p}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"shi-ieq2-3480519.gif\"/></alternatives></inline-formula>-norm constraints, yet lacking transferability of attacking black-box models. In this work, we propose a novel imperceptible and transferable attack by leveraging both the generative and discriminative power of diffusion models. Specifically, instead of direct manipulation in pixel space, we craft perturbations in the latent space of diffusion models. Combined with well-designed content-preserving structures, we can generate human-insensitive perturbations embedded with semantic clues. For better transferability, we further “deceive” the diffusion model which can be viewed as an implicit recognition surrogate, by distracting its attention away from the target regions. To our knowledge, our proposed method, <italic>DiffAttack</italic>, is the first that introduces diffusion models into the adversarial attack field. Extensive experiments conducted across diverse model architectures (CNNs, Transformers, and MLPs), datasets (ImageNet, CUB-200, and Standford Cars), and defense mechanisms underscore the superiority of our attack over existing methods such as iterative attacks, GAN-based attacks, and ensemble attacks. Furthermore, we provide a comprehensive discussion on future research avenues in diffusion-based adversarial attacks, aiming to chart a course for this burgeoning field.",
        "authors": [
            "Jianqi Chen",
            "H. Chen",
            "Keyan Chen",
            "Yilan Zhang",
            "Zhengxia Zou",
            "Z. Shi"
        ],
        "citations": 32,
        "references": 100,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Black-Box Optimization",
        "abstract": "The goal of offline black-box optimization (BBO) is to optimize an expensive black-box function using a fixed dataset of function evaluations. Prior works consider forward approaches that learn surrogates to the black-box function and inverse approaches that directly map function values to corresponding points in the input domain of the black-box function. These approaches are limited by the quality of the offline dataset and the difficulty in learning one-to-many mappings in high dimensions, respectively. We propose Denoising Diffusion Optimization Models (DDOM), a new inverse approach for offline black-box optimization based on diffusion models. Given an offline dataset, DDOM learns a conditional generative model over the domain of the black-box function conditioned on the function values. We investigate several design choices in DDOM, such as re-weighting the dataset to focus on high function values and the use of classifier-free guidance at test-time to enable generalization to function values that can even exceed the dataset maxima. Empirically, we conduct experiments on the Design-Bench benchmark and show that DDOM achieves results competitive with state-of-the-art baselines.",
        "authors": [
            "S. Krishnamoorthy",
            "Satvik Mashkaria",
            "Aditya Grover"
        ],
        "citations": 32,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
        "abstract": "The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a $75\\%$ young and $25\\%$ old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We share code and various fair diffusion model adaptors at https://sail-sg.github.io/finetune-fair-diffusion/.",
        "authors": [
            "Xudong Shen",
            "Chao Du",
            "Tianyu Pang",
            "Min Lin",
            "Yongkang Wong",
            "Mohan S. Kankanhalli"
        ],
        "citations": 33,
        "references": 52,
        "year": 2023
    },
    {
        "title": "VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models",
        "abstract": "Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.",
        "authors": [
            "Sheng-Yen Chou",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "citations": 34,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models",
        "abstract": "The introduction of diffusion models in anomaly detection has paved the way for more effective and accurate image reconstruction in pathologies. However, the current limitations in controlling noise granularity hinder diffusion models' ability to generalize across diverse anomaly types and compromise the restoration of healthy tissues. To overcome these challenges, we propose AutoDDPM, a novel approach that enhances the robustness of diffusion models. AutoDDPM utilizes diffusion models to generate initial likelihood maps of potential anomalies and seamlessly integrates them with the original image. Through joint noised distribution re-sampling, AutoDDPM achieves harmonization and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in replacing anomalous regions while preserving healthy tissues, considerably surpassing diffusion models' limitations. It also contributes valuable insights and analysis on the limitations of current diffusion models, promoting robust and interpretable anomaly detection in medical imaging - an essential aspect of building autonomous clinical decision systems with higher interpretability.",
        "authors": [
            "Cosmin I. Bercea",
            "Michaela Neumayr",
            "D. Rueckert",
            "J. Schnabel"
        ],
        "citations": 32,
        "references": 24,
        "year": 2023
    },
    {
        "title": "Cache Me if You Can: Accelerating Diffusion Models through Block Caching",
        "abstract": "Diffusion models have recently revolutionized the field of image synthesis due to their ability to generate photorealistic images. However, one of the major drawbacks of diffusion models is that the image generation process is costly. A large image-to-image network has to be applied many times to iteratively refine an image from random noise. While many recent works propose techniques to reduce the number of required steps, they generally treat the underlying denoising network as a black box. In this work, we investigate the behavior of the layers within the network and find that 1) the layers' output changes smoothly over time, 2) the layers show distinct patterns of change, and 3) the change from step to step is often very small. We hypothesize that many layer computations in the denoising network are redundant. Leveraging this, we introduce block caching, in which we reuse outputs from layer blocks of previous steps to speed up inference. Furthermore, we propose a technique to automatically determine caching schedules based on each block's changes over timesteps. In our experiments, we show through FID, human evaluation and qualitative analysis that Block Caching allows to generate images with higher visual quality at the same computational cost. We demonstrate this for different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM). Project page: fwmb.github.io/blockcaching",
        "authors": [
            "Felix Wimbauer",
            "Bichen Wu",
            "Edgar Schoenfeld",
            "Xiaoliang Dai",
            "Ji Hou",
            "Zijian He",
            "A. Sanakoyeu",
            "Peizhao Zhang",
            "Sam S. Tsai",
            "Jonas Kohler",
            "Christian Rupprecht",
            "Daniel Cremers",
            "Péter Vajda",
            "Jialiang Wang"
        ],
        "citations": 34,
        "references": 60,
        "year": 2023
    },
    {
        "title": "LLM-grounded Video Diffusion Models",
        "abstract": "Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion. To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free and can be integrated into any video diffusion model that admits classifier guidance. Our results demonstrate that LVD significantly outperforms its base video diffusion model and several strong baseline methods in faithfully generating videos with the desired attributes and motion patterns.",
        "authors": [
            "Long Lian",
            "Baifeng Shi",
            "Adam Yala",
            "Trevor Darrell",
            "Boyi Li"
        ],
        "citations": 33,
        "references": 57,
        "year": 2023
    },
    {
        "title": "FreeInit: Bridging Initialization Gap in Video Diffusion Models",
        "abstract": "Though diffusion-based video generation has witnessed rapid progress, the inference results of existing models still exhibit unsatisfactory temporal consistency and unnatural dynamics. In this paper, we delve deep into the noise initialization of video diffusion models, and discover an implicit training-inference gap that attributes to the unsatisfactory inference quality.Our key findings are: 1) the spatial-temporal frequency distribution of the initial noise at inference is intrinsically different from that for training, and 2) the denoising process is significantly influenced by the low-frequency components of the initial noise. Motivated by these observations, we propose a concise yet effective inference sampling strategy, FreeInit, which significantly improves temporal consistency of videos generated by diffusion models. Through iteratively refining the spatial-temporal low-frequency components of the initial latent during inference, FreeInit is able to compensate the initialization gap between training and inference, thus effectively improving the subject appearance and temporal consistency of generation results. Extensive experiments demonstrate that FreeInit consistently enhances the generation quality of various text-to-video diffusion models without additional training or fine-tuning.",
        "authors": [
            "Tianxing Wu",
            "Chenyang Si",
            "Yuming Jiang",
            "Ziqi Huang",
            "Ziwei Liu"
        ],
        "citations": 34,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Mist: Towards Improved Adversarial Examples for Diffusion Models",
        "abstract": "Diffusion Models (DMs) have empowered great success in artificial-intelligence-generated content, especially in artwork creation, yet raising new concerns in intellectual properties and copyright. For example, infringers can make profits by imitating non-authorized human-created paintings with DMs. Recent researches suggest that various adversarial examples for diffusion models can be effective tools against these copyright infringements. However, current adversarial examples show weakness in transferability over different painting-imitating methods and robustness under straightforward adversarial defense, for example, noise purification. We surprisingly find that the transferability of adversarial examples can be significantly enhanced by exploiting a fused and modified adversarial loss term under consistent parameters. In this work, we comprehensively evaluate the cross-method transferability of adversarial examples. The experimental observation shows that our method generates more transferable adversarial examples with even stronger robustness against the simple adversarial defense.",
        "authors": [
            "Chumeng Liang",
            "Xiaoyu Wu"
        ],
        "citations": 31,
        "references": 13,
        "year": 2023
    },
    {
        "title": "Consistent View Synthesis with Pose-Guided Diffusion Models",
        "abstract": "Novel view synthesis from a single image has been a cornerstone problem for many Virtual Reality applications that provide immersive experiences. However, most existing techniques can only synthesize novel views within a limited range of camera motion or fail to generate consistent and high-quality novel views under significant camera movement. In this work, we propose a pose-guided diffusion model to generate a consistent long-term video of novel views from a single image. We design an attention layer that uses epipolar lines as constraints to facilitate the association between different viewpoints. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed diffusion model against state-of-the-art transformer-based and GAN-based approaches. More qualitative results are available at https://poseguided-diffusion.github.io/.",
        "authors": [
            "Hung-Yu Tseng",
            "Qinbo Li",
            "Changil Kim",
            "Suhib Alsisan",
            "Jia-Bin Huang",
            "J. Kopf"
        ],
        "citations": 85,
        "references": 88,
        "year": 2023
    },
    {
        "title": "DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models",
        "abstract": "Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive results on novel view synthesis tasks. NeRFs learn a scene's color and density fields by minimizing the photometric discrepancy between training views and differentiable renderings of the scene. Once trained from a sufficient set of views, NeRFs can generate novel views from arbitrary camera positions. However, the scene geometry and color fields are severely under-constrained, which can lead to artifacts, especially when trained with few input views. To alleviate this problem we learn a prior over scene geometry and color, using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of the synthetic Hypersim dataset and can be used to predict the gradient of the logarithm of a joint probability distribution of color and depth patches. We show that, these gradients of logarithms of RGBD patch priors serve to regularize geom-etry and color of a scene. During NeRF training, random RGBD patches are rendered and the estimated gradient of the log-likelihood is backpropagated to the color and density fields. Evaluations on LLFF, the most relevant dataset, show that our learned prior achieves improved quality in the reconstructed geometry and improved generalization to novel views. Evaluations on DTU show improved Reconstruction quality among NeRF methods.",
        "authors": [
            "Jamie M. Wynn",
            "Daniyar Turmukhambetov"
        ],
        "citations": 87,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation",
        "abstract": "Animating virtual avatars to make co-speech gestures facilitates various applications in human-machine interaction. The existing methods mainly rely on generative adversarial networks (GANs), which typically suffer from notorious mode collapse and unstable training, thus making it difficult to learn accurate audio-gesture joint distributions. In this work, we propose a novel diffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to effectively capture the cross-modal audio-to-gesture associations and preserve temporal coherence for high-fidelity audio-driven co-speech gesture generation. Specifically, we first establish the diffusion-conditional generation process on clips of skeleton sequences and audio to enable the whole framework. Then, a novel Diffusion Audio-Gesture Transformer is devised to better attend to the information from multiple modalities and model the long-term temporal dependency. Moreover, to eliminate temporal inconsistency, we propose an effective Diffusion Gesture Stabilizer with an annealed noise sampling strategy. Benefiting from the architectural advantages of diffusion models, we further incorporate implicit classifier-free guidance to trade off between diversity and gesture quality. Extensive experiments demonstrate that DiffGesture achieves state-of-the-art performance, which renders coherent gestures with better mode coverage and stronger audio correlations. Code is available at https://github.com/Advocate99/DiffGesture.",
        "authors": [
            "Lingting Zhu",
            "Xian Liu",
            "Xuan Liu",
            "Rui Qian",
            "Ziwei Liu",
            "Lequan Yu"
        ],
        "citations": 84,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Ambiguous Medical Image Segmentation Using Diffusion Models",
        "abstract": "Collective insights from a group of experts have always proven to outperform an individual's best diagnostic for clinical tasks. For the task of medical image segmentation, existing research on AI-based alternatives focuses more on developing models that can imitate the best individual rather than harnessing the power of expert groups. In this paper, we introduce a single diffusion model-based approach that produces multiple plausible outputs by learning a distribution over group insights. Our proposed model generates a distribution of segmentation masks by leveraging the inherent stochastic sampling process of diffusion using only minimal additional learning. We demonstrate on three different medical image modalities- CT, ultrasound, and MRI that our model is capable of producing several possible variants while capturing the frequencies of their occurrences. Comprehensive results show that our proposed approach outperforms existing state-of-the-art ambiguous segmentation networks in terms of accuracy while preserving naturally occurring variation. We also propose a new metric to evaluate the diversity as well as the accuracy of segmentation predictions that aligns with the interest of clinical practice of collective insights. Implementation code: https://github.com/aimansnigdha/Ambiguous-Medical-Image-Segmentation-using-Diffusion-Models.",
        "authors": [
            "Aimon Rahman",
            "Jeya Maria Jose Valanarasu",
            "I. Hacihaliloglu",
            "V. Patel"
        ],
        "citations": 74,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Dif-Fusion: Toward High Color Fidelity in Infrared and Visible Image Fusion With Diffusion Models",
        "abstract": "Color plays an important role in human visual perception, reflecting the spectrum of objects. However, the existing infrared and visible image fusion methods rarely explore how to handle multi-spectral/channel data directly and achieve high color fidelity. This paper addresses the above issue by proposing a novel method with diffusion models, termed as Dif-Fusion, to generate the distribution of the multi-channel input data, which increases the ability of multi-source information aggregation and the fidelity of colors. In specific, instead of converting multi-channel images into single-channel data in existing fusion methods, we create the multi-channel data distribution with a denoising network in a latent space with forward and reverse diffusion process. Then, we use the the denoising network to extract the multi-channel diffusion features with both visible and infrared information. Finally, we feed the multi-channel diffusion features to the multi-channel fusion module to directly generate the three-channel fused image. To retain the texture and intensity information, we propose multi-channel gradient loss and intensity loss. Along with the current evaluation metrics for measuring texture and intensity fidelity, we introduce Delta E as a new evaluation metric to quantify color fidelity. Extensive experiments indicate that our method is more effective than other state-of-the-art image fusion methods, especially in color fidelity. The source code is available at https://github.com/GeoVectorMatrix/Dif-Fusion.",
        "authors": [
            "Jun Yue",
            "Leyuan Fang",
            "Shaobo Xia",
            "Yue Deng",
            "Jiayi Ma"
        ],
        "citations": 61,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models",
        "abstract": "If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot's own training data. We propose SuSIE, a method that leverages an image-editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller can accomplish. Specifically, we finetune InstructPix2Pix on video data, consisting of both human videos and robot rollouts, such that it outputs hypothetical future\"subgoal\"observations given the robot's current observation and a language command. We also use the robot data to train a low-level goal-conditioned policy to act as the aforementioned low-level controller. We find that the high-level subgoal predictions can utilize Internet-scale pretraining and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization and precision than conventional language-conditioned policies. We achieve state-of-the-art results on the CALVIN benchmark, and also demonstrate robust generalization on real-world manipulation tasks, beating strong baselines that have access to privileged information or that utilize orders of magnitude more compute and training data. The project website can be found at http://rail-berkeley.github.io/susie .",
        "authors": [
            "Kevin Black",
            "Mitsuhiko Nakamoto",
            "P. Atreya",
            "Homer Walke",
            "Chelsea Finn",
            "Aviral Kumar",
            "Sergey Levine"
        ],
        "citations": 82,
        "references": 68,
        "year": 2023
    },
    {
        "title": "NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models",
        "abstract": "Automatically generating high-quality real world 3D scenes is of enormous interest for applications such as virtual reality and robotics simulation. Towards this goal, we introduce NeuralField-LDM, a generative model capable of synthesizing complex 3D environments. We leverage Latent Diffusion Models that have been successfully utilized for efficient high-quality 2D content creation. We first train a scene auto-encoder to express a set of image and pose pairs as a neural field, represented as density and feature voxel grids that can be projected to produce novel views of the scene. To further compress this representation, we train a latent-autoencoder that maps the voxel grids to a set of latent representations. A hierarchical diffusion model is then fit to the latents to complete the scene generation pipeline. We achieve a substantial improvement over existing state-of-the-art scene generation models. Additionally, we show how NeuralField-LDM can be used for a variety of 3D content creation applications, including conditional scene generation, scene inpainting and scene style manipulation.",
        "authors": [
            "Seung Wook Kim",
            "B. Brown",
            "K. Yin",
            "Karsten Kreis",
            "Katja Schwarz",
            "Daiqing Li",
            "Robin Rombach",
            "A. Torralba",
            "S. Fidler"
        ],
        "citations": 53,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Image Restoration and Enhancement - A Comprehensive Survey",
        "abstract": "Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question,\"whether diffusion model can boost image restoration\". To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods. Despite that, a comprehensive and enlightening survey on diffusion model-based image restoration remains scarce. In this paper, we are the first to present a comprehensive review of recent diffusion model-based methods on image restoration, encompassing the learning paradigm, conditional strategy, framework design, modeling strategy, and evaluation. Concretely, we first introduce the background of the diffusion model briefly and then present two prevalent workflows that exploit diffusion models in image restoration. Subsequently, we classify and emphasize the innovative designs using diffusion models for both IR and blind/real-world IR, intending to inspire future development. To evaluate existing methods thoroughly, we summarize the commonly-used dataset, implementation details, and evaluation metrics. Additionally, we present the objective comparison for open-sourced methods across three tasks, including image super-resolution, deblurring, and inpainting. Ultimately, informed by the limitations in existing works, we propose five potential and challenging directions for the future research of diffusion model-based IR, including sampling efficiency, model compression, distortion simulation and estimation, distortion invariant learning, and framework design.",
        "authors": [
            "Xin Li",
            "Yulin Ren",
            "Xin Jin",
            "Cuiling Lan",
            "X. Wang",
            "Wenjun Zeng",
            "Xinchao Wang",
            "Zhibo Chen"
        ],
        "citations": 57,
        "references": 333,
        "year": 2023
    },
    {
        "title": "Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models",
        "abstract": "Recent text-to-image (T2I) diffusion models show outstanding performance in generating high-quality images conditioned on textual prompts. However, they fail to semantically align the generated images with the prompts due to their limited compositional capabilities, leading to attribute leakage, entity leakage, and missing entities. In this paper, we propose a novel attention mask control strategy based on predicted object boxes to address these issues. In particular, we first train a BoxNet to predict a box for each entity that possesses the attribute specified in the prompt. Then, depending on the predicted boxes, a unique mask control is applied to the cross- and self-attention maps. Our approach produces a more semantically accurate synthesis by constraining the attention regions of each token in the prompt to the image. In addition, the proposed method is straightforward and effective and can be readily integrated into existing cross-attention-based T2I generators. We compare our approach to competing methods and demonstrate that it can faithfully convey the semantics of the original text to the generated content and achieve high availability as a ready-to-use plugin. Please refer to https://github.com/OPPO-Mente-Lab/attention-mask-control.",
        "authors": [
            "Ruichen Wang",
            "Zekang Chen",
            "Chen Chen",
            "Jiancang Ma",
            "H. Lu",
            "Xiaodong Lin"
        ],
        "citations": 54,
        "references": 35,
        "year": 2023
    },
    {
        "title": "PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models",
        "abstract": "Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack ﬁne-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each deﬁned by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model ( PAIR-Diffusion ), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image’s appearance into the input im*",
        "authors": [
            "Vidit Goel",
            "E. Peruzzo",
            "Yifan Jiang",
            "Dejia Xu",
            "N. Sebe",
            "Trevor Darrell",
            "Zhangyang Wang",
            "Humphrey Shi"
        ],
        "citations": 56,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Diffusion models in bioinformatics and computational biology.",
        "abstract": null,
        "authors": [
            "Zhiye Guo",
            "Jian Liu",
            "Yanli Wang",
            "Mengrui Chen",
            "Duolin Wang",
            "Dong Xu",
            "Jianlin Cheng"
        ],
        "citations": 53,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Fast point cloud generation with diffusion models in high energy physics",
        "abstract": "Many particle physics datasets like those generated at colliders are described by continuous coordinates (in contrast to grid points like in an image), respect a number of symmetries (like permutation invariance), and have a stochastic dimensionality. For this reason, standard deep generative models that produce images or at least a fixed set of features are limiting. We introduce a new neural network simulation based on a diffusion model that addresses these limitations named Fast Point Cloud Diffusion (FPCD). We show that our approach can reproduce the complex properties of hadronic jets from proton-proton collisions with competitive precision to other recently proposed models. Additionally, we use a procedure called progressive distillation to accelerate the generation time of our method, which is typically a significant challenge for diffusion models despite their state-of-the-art precision.",
        "authors": [
            "V. Mikuni",
            "B. Nachman",
            "M. Pettee"
        ],
        "citations": 53,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Localizing Object-level Shape Variations with Text-to-Image Diffusion Models",
        "abstract": "Text-to-image models give rise to workflows which often begin with an exploration step, where users sift through a large collection of generated images. The global nature of the text-to-image generation process prevents users from narrowing their exploration to a particular object in the image. In this paper, we present a technique to generate a collection of images that depicts variations in the shape of a specific object, enabling an object-level shape exploration process. Creating plausible variations is challenging as it requires control over the shape of the generated object while respecting its semantics. A particular challenge when generating object variations is accurately localizing the manipulation applied over the object’s shape. We introduce a prompt-mixing technique that switches between prompts along the denoising process to attain a variety of shape choices. To localize the image-space operation, we present two techniques that use the self-attention layers in conjunction with the cross-attention layers. Moreover, we show that these localization techniques are general and effective beyond the scope of generating object variations. Extensive results and comparisons demonstrate the effectiveness of our method in generating object variations, and the competence of our localization techniques.",
        "authors": [
            "Or Patashnik",
            "Daniel Garibi",
            "Idan Azuri",
            "Hadar Averbuch-Elor",
            "D. Cohen-Or"
        ],
        "citations": 84,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow",
        "abstract": "Virtual try-on is a critical image synthesis task that aims to transfer clothes from one image to another while preserving the details of both humans and clothes. While many existing methods rely on Generative Adversarial Networks (GANs) to achieve this, flaws can still occur, particularly at high resolutions. Recently, the diffusion model has emerged as a promising alternative for generating high-quality images in various applications. However, simply using clothes as a condition for guiding the diffusion model to inpaint is insufficient to maintain the details of the clothes. To overcome this challenge, we propose an exemplar-based inpainting approach that leverages a warping module to guide the diffusion model's generation effectively. The warping module performs initial processing on the clothes, which helps to preserve the local details of the clothes. We then combine the warped clothes with clothes-agnostic person image and add noise as the input of diffusion model. Additionally, the warped clothes is used as local conditions for each denoising process to ensure that the resulting output retains as much detail as possible. Our approach, namely Diffusion-based Conditional Inpainting for Virtual Try-ON(DCI-VTON), effectively utilizes the power of the diffusion model, and the incorporation of the warping module helps to produce high-quality and realistic virtual try-on results. Experimental results on VITON-HD demonstrate the effectiveness and superiority of our method. Source code and trained models will be publicly released at: https://github.com/bcmi/DCI-VTON-Virtual-Try-On.",
        "authors": [
            "Junhong Gou",
            "Siyu Sun",
            "Jianfu Zhang",
            "Jianlou Si",
            "Chen Qian",
            "Liqing Zhang"
        ],
        "citations": 67,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Zero-shot spatial layout conditioning for text-to-image diffusion models",
        "abstract": "Large-scale text-to-image diffusion models have significantly improved the state of the art in generative image modeling and allow for an intuitive and powerful user interface to drive the image generation process. Expressing spatial constraints, e.g. to position specific objects in particular locations, is cumbersome using text; and current text-based image generation models are not able to accurately follow such instructions. In this paper we consider image generation from text associated with segments on the image canvas, which combines an intuitive natural language interface with precise spatial control over the generated content. We propose ZestGuide, a \"zero-shot\" segmentation guidance approach that can be plugged into pre-trained text-to-image diffusion models, and does not require any additional training. It leverages implicit segmentation maps that can be extracted from cross-attention layers, and uses them to align the generation with input masks. Our experimental results combine high image quality with accurate alignment of generated content with input segmentations, and improve over prior work both quantitatively and qualitatively, including methods that require training on images with corresponding segmentations. Compared to Paint with Words, the previous state-of-the art in image generation with zero-shot segmentation conditioning, we improve by 5 to 10 mIoU points on the COCO dataset with similar FID scores.",
        "authors": [
            "Guillaume Couairon",
            "Marlene Careil",
            "M. Cord",
            "Stéphane Lathuilière",
            "Jakob Verbeek"
        ],
        "citations": 50,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Prompt-Free Diffusion: Taking “Text” Out of Text-to-Image Diffusion Models",
        "abstract": "Text-to-image (T2I) research has grown explosively in the past year, owing to the large-scale pre-trained diffusion models and many emerging personalization and editing approaches. Yet, one pain point persists: the text prompt engineering, and searching high-quality text prompts for customized results is more art than science. Moreover, as commonly argued: “an image is worth a thousand words” - the attempt to describe a desired image with texts often ends up being ambiguous and cannot comprehensively cover delicate visual details, hence necessitating more additional controls from the visual domain. In this paper, we take a bold step forward: taking “Text” out of a pretrained T2I diffusion model, to reduce the burdensome prompt engineering efforts for users. Our proposed frame-work, Prompt-Free Diffusion, relies on only visual inputs to generate new images: it takes a reference image as “context”, an optional image structural conditioning, and an initial noise, with absolutely no text prompt. The core architecture behind the scene is Semantic Context Encoder (SeeCoder), substituting the commonly used CLIP-based or LLM-based text encoder. The reusability of SeeCoder also makes it a convenient drop-in component: one can also pre-train a SeeCoder in one T2I model and reuse it for another. Through extensive experiments, Prompt-Free Diffusion is experimentally found to (i) outperform prior exemplar-based image synthesis approaches; (ii) perform on par with state-of-the-art T2I models using prompts following the best practice; and (iii) be naturally extensible to other downstream applications such as anime figure generation and virtual try-on, with promising quality. Our code and models will be open-sourced.",
        "authors": [
            "Xingqian Xu",
            "Jiayi Guo",
            "Zhangyang Wang",
            "Gao Huang",
            "Irfan Essa",
            "Humphrey Shi"
        ],
        "citations": 50,
        "references": 76,
        "year": 2023
    },
    {
        "title": "On the Design Fundamentals of Diffusion Models: A Survey",
        "abstract": "Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.",
        "authors": [
            "Ziyi Chang",
            "G. Koulieris",
            "Hubert P. H. Shum"
        ],
        "citations": 44,
        "references": 306,
        "year": 2023
    },
    {
        "title": "Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models",
        "abstract": "Text-to-image diffusion models can generate diverse, high-fidelity images based on user-provided text prompts. Recent research has extended these models to support text-guided image editing. While text guidance is an intuitive editing interface for users, it often fails to ensure the precise concept conveyed by users. To address this issue, we propose Custom-Edit, in which we (i) customize a diffusion model with a few reference images and then (ii) perform text-guided editing. Our key discovery is that customizing only language-relevant parameters with augmented prompts improves reference similarity significantly while maintaining source similarity. Moreover, we provide our recipe for each customization and editing process. We compare popular customization methods and validate our findings on two editing methods using various datasets.",
        "authors": [
            "Jooyoung Choi",
            "Yunjey Choi",
            "Yunji Kim",
            "Junho Kim",
            "Sung-Hoon Yoon"
        ],
        "citations": 43,
        "references": 28,
        "year": 2023
    },
    {
        "title": "Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models",
        "abstract": "Long-horizon tasks, usually characterized by complex subtask dependencies, present a significant challenge in manipulation planning. Skill chaining is a practical approach to solving unseen tasks by combining learned skill priors. However, such methods are myopic if sequenced greedily and face scalability issues with search-based planning strategy. To address these challenges, we introduce Generative Skill Chaining~(GSC), a probabilistic framework that learns skill-centric diffusion models and composes their learned distributions to generate long-horizon plans during inference. GSC samples from all skill models in parallel to efficiently solve unseen tasks while enforcing geometric constraints. We evaluate the method on various long-horizon tasks and demonstrate its capability in reasoning about action dependencies, constraint handling, and generalization, along with its ability to replan in the face of perturbations. We show results in simulation and on real robot to validate the efficiency and scalability of GSC, highlighting its potential for advancing long-horizon task planning. More details are available at: https://generative-skill-chaining.github.io/",
        "authors": [
            "Utkarsh Aashu Mishra",
            "Shangjie Xue",
            "Yongxin Chen",
            "Danfei Xu"
        ],
        "citations": 48,
        "references": 65,
        "year": 2023
    },
    {
        "title": "In-Context Learning Unlocked for Diffusion Models",
        "abstract": "We present Prompt Diffusion, a framework for enabling in-context learning in diffusion-based generative models. Given a pair of task-specific example images, such as depth from/to image and scribble from/to image, and a text guidance, our model automatically understands the underlying task and performs the same task on a new query image following the text guidance. To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input. The diffusion model is trained jointly over six different tasks using these prompts. The resulting Prompt Diffusion model is the first diffusion-based vision-language foundation model capable of in-context learning. It demonstrates high-quality in-context generation on the trained tasks and generalizes effectively to new, unseen vision tasks with their respective prompts. Our model also shows compelling text-guided image editing results. Our framework aims to facilitate research into in-context learning for computer vision. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Prompt-Diffusion.",
        "authors": [
            "Zhendong Wang",
            "Yifan Jiang",
            "Yadong Lu",
            "Yelong Shen",
            "Pengcheng He",
            "Weizhu Chen",
            "Zhangyang Wang",
            "Mingyuan Zhou"
        ],
        "citations": 63,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Diffusion models for time-series applications: a survey",
        "abstract": "Diffusion models, a family of generative models based on deep learning, have become increasingly prominent in cutting-edge machine learning research. With distinguished performance in generating samples that resemble the observed data, diffusion models are widely used in image, video, and text synthesis nowadays. In recent years, the concept of diffusion has been extended to time-series applications, and many powerful models have been developed. Considering the deficiency of a methodical summary and discourse on these models, we provide this survey as an elementary resource for new researchers in this area and to provide inspiration to motivate future research. For better understanding, we include an introduction about the basics of diffusion models. Except for this, we primarily focus on diffusion-based methods for time-series forecasting, imputation, and generation, and present them, separately, in three individual sections. We also compare different methods for the same application and highlight their connections if applicable. Finally, we conclude with the common limitation of diffusion-based methods and highlight potential future research directions. 扩散模型,一类基于深度学习的生成模型家族,在前沿机器学习研究中变得日益重要。扩散模型以在生成与观察数据相似样本方面的卓越性能而著称,如今广泛用于图像、视频和文本合成。近年来,扩散的概念已扩展到时间序列应用领域,涌现出许多强大的模型。鉴于这些模型缺乏系统性总结和讨论,我们提供此综述作为此领域新研究人员的基础资源,并为激发未来研究提供灵感。为更好理解,引入了有关扩散模型基础知识的介绍。除此之外,主要关注基于扩散的时间序列预测、插补和生成方法,并将它们分别在三个独立章节中呈现。还比较了同一应用的不同方法,并强调它们之间的关联(若适用)。最后,总结了扩散方法的共同局限性,并突出强调潜在的未来研究方向。",
        "authors": [
            "Lequan Lin",
            "Zhengkun Li",
            "Ruikun Li",
            "Xuliang Li",
            "Junbin Gao"
        ],
        "citations": 50,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Constrained Domains",
        "abstract": "Denoising diffusion models are a novel class of generative algorithms that achieve state-of-the-art performance across a range of domains, including image generation and text-to-image tasks. Building on this success, diffusion models have recently been extended to the Riemannian manifold setting, broadening their applicability to a range of problems from the natural and engineering sciences. However, these Riemannian diffusion models are built on the assumption that their forward and backward processes are well-defined for all times, preventing them from being applied to an important set of tasks that consider manifolds defined via a set of inequality constraints. In this work, we introduce a principled framework to bridge this gap. We present two distinct noising processes based on (i) the logarithmic barrier metric and (ii) the reflected Brownian motion induced by the constraints. As existing diffusion model techniques cannot be applied in this setting, we derive new tools to define such models in our framework. We then demonstrate the practical utility of our methods on a number of synthetic and real-world tasks, including applications from robotics and protein design.",
        "authors": [
            "N. Fishman",
            "Leo Klarner",
            "Valentin De Bortoli",
            "Emile Mathieu",
            "M. Hutchinson"
        ],
        "citations": 30,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Conditional Text Image Generation with Diffusion Models",
        "abstract": "Current text recognition systems, including those for handwritten scripts and scene text, have relied heavily on image synthesis and augmentation, since it is difficult to realize real-world complexity and diversity through collecting and annotating enough real text images. In this paper, we explore the problem of text image generation, by taking advantage of the powerful abilities of Diffusion Models in generating photo-realistic and diverse image samples with given conditions, and propose a method called Conditional Text Image Generation with Diffusion Models (CTIG-DM for short). To conform to the characteristics of text images, we devise three conditions: image condition, text condition, and style condition, which can be used to control the attributes, contents, and styles of the samples in the image generation process. Specifically, four text image generation modes, namely: (1) synthesis mode, (2) augmentation mode, (3) recovery mode, and (4) imitation mode, can be derived by combining and configuring these three conditions. Extensive experiments on both handwritten and scene text demonstrate that the proposed CTIG-DM is able to produce image samples that simulate real-world complexity and diversity, and thus can boost the performance of existing text recognizers. Besides, CTIG-DM shows its appealing potential in domain adaptation and generating images containing Out-Of-Vocabulary (OOV) words.",
        "authors": [
            "Yuanzhi Zhu",
            "Zhaohai Li",
            "Tianwei Wang",
            "Mengchao He",
            "C. Yao"
        ],
        "citations": 30,
        "references": 77,
        "year": 2023
    },
    {
        "title": "DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models",
        "abstract": "Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates \\textit{vectorized} free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of B\\'ezier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than prior work. The code and demo of DiffSketcher can be found at https://ximinng.github.io/DiffSketcher-project/.",
        "authors": [
            "Ximing Xing",
            "Chuan Wang",
            "Haitao Zhou",
            "Jing Zhang",
            "Qian Yu",
            "Dong Xu"
        ],
        "citations": 30,
        "references": 47,
        "year": 2023
    },
    {
        "title": "DocDiff: Document Enhancement via Residual Diffusion Models",
        "abstract": "Removing degradation from document images not only improves their visual quality and readability, but also enhances the performance of numerous automated document analysis and recognition tasks. However, existing regression-based methods optimized for pixel-level distortion reduction tend to suffer from significant loss of high-frequency information, leading to distorted and blurred text edges. To compensate for this major deficiency, we propose DocDiff, the first diffusion-based framework specifically designed for diverse challenging document enhancement problems, including document deblurring, denoising, and removal of watermarks and seals. DocDiff consists of two modules: the Coarse Predictor (CP), which is responsible for recovering the primary low-frequency content, and the High-Frequency Residual Refinement (HRR) module, which adopts the diffusion models to predict the residual (high-frequency information, including text edges), between the ground-truth and the CP-predicted image. DocDiff is a compact and computationally efficient model that benefits from a well-designed network architecture, an optimized training loss objective, and a deterministic sampling process with short time steps. Extensive experiments demonstrate that DocDiff achieves state-of-the-art (SOTA) performance on multiple benchmark datasets, and can significantly enhance the readability and recognizability of degraded document images. Furthermore, our proposed HRR module in pre-trained DocDiff is plug-and-play and ready-to-use, with only 4.17M parameters. It greatly sharpens the text edges generated by SOTA deblurring methods without additional joint training. Available codes: https://github.com/Royalvice/DocDiff https://github.com/Royalvice/DocDiff.",
        "authors": [
            "Zongyuan Yang",
            "Baolin Liu",
            "Yongping Xiong",
            "Lan Yi",
            "Guibin Wu",
            "Xiaojun Tang",
            "Ziqi Liu",
            "Junjie Zhou",
            "Xing Zhang"
        ],
        "citations": 30,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Generative Diffusion Models on Graphs: Methods and Applications",
        "abstract": "Diffusion models, as a novel generative paradigm, have achieved remarkable success in various image generation tasks such as image inpainting, image-to-text translation, and video generation. Graph generation is a crucial computational task on graphs with numerous real-world applications. It aims to learn the distribution of given graphs and then generate new graphs. Given the great success of diffusion models in image generation, increasing efforts have been made to leverage these techniques to advance graph generation in recent years. In this paper, we first provide a comprehensive overview of generative diffusion models on graphs, In particular, we review representative algorithms for three variants of graph diffusion models, i.e., Score Matching with Langevin Dynamics (SMLD), Denoising Diffusion Probabilistic Model (DDPM), and Score-based Generative Model (SGM). Then, we summarize the major applications of generative diffusion models on graphs with a specific focus on molecule and protein modeling. Finally, we discuss promising directions in generative diffusion models on graph-structured data.",
        "authors": [
            "Wenqi Fan",
            "C. Liu",
            "Yunqing Liu",
            "Jiatong Li",
            "Hang Li",
            "Hui Liu",
            "Jiliang Tang",
            "Qing Li"
        ],
        "citations": 49,
        "references": 122,
        "year": 2023
    },
    {
        "title": "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models",
        "abstract": "The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonstrate Diff-Foley practical applicability and generalization capabilities via downstream finetuning. Project Page: see https://diff-foley.github.io/",
        "authors": [
            "Simian Luo",
            "Chuanhao Yan",
            "Chenxu Hu",
            "Hang Zhao"
        ],
        "citations": 64,
        "references": 52,
        "year": 2023
    },
    {
        "title": "EigenFold: Generative Protein Structure Prediction with Diffusion Models",
        "abstract": "Protein structure prediction has reached revolutionary levels of accuracy on single structures, yet distributional modeling paradigms are needed to capture the conformational ensembles and flexibility that underlie biological function. Towards this goal, we develop EigenFold, a diffusion generative modeling framework for sampling a distribution of structures from a given protein sequence. We define a diffusion process that models the structure as a system of harmonic oscillators and which naturally induces a cascading-resolution generative process along the eigenmodes of the system. On recent CAMEO targets, EigenFold achieves a median TMScore of 0.84, while providing a more comprehensive picture of model uncertainty via the ensemble of sampled structures relative to existing methods. We then assess EigenFold's ability to model and predict conformational heterogeneity for fold-switching proteins and ligand-induced conformational change. Code is available at https://github.com/bjing2016/EigenFold.",
        "authors": [
            "Bowen Jing",
            "Ezra Erives",
            "Peter Pao-Huang",
            "Gabriele Corso",
            "B. Berger",
            "T. Jaakkola"
        ],
        "citations": 51,
        "references": 32,
        "year": 2023
    },
    {
        "title": "SODA: Bottleneck Diffusion Models for Representation Learning",
        "abstract": "We introduce SODA, a self-supervised diffusion model, designed for representation learning. The model incorpo-rates an image encoder, which distills a source view into a compact representation, that, in turn, guides the generation of related novel views. We show that by imposing a tight bottleneck between the encoder and a denoising decoder, and leveraging novel view synthesis as a self-supervised ob-jective, we can turn diffusion models into strong represen-tation learners, capable of capturing visual semantics in an unsupervised manner. To the best of our knowledge, SODA is the first diffusion model to succeed at ImageNet linear-probe classification, and, at the same time, it accomplishes reconstruction, editing and synthesis tasks across a wide range of datasets. Further investigation reveals the disentangled nature of its emergent latent space, that serves as an effective interface to control and manipulate the produced images. All in all, we aim to shed light on the exciting and promising potential of diffusion models, not only for image generation, but also for learning rich and robust represen-tations. See our website at soda-diffusion.github.io.",
        "authors": [
            "Drew A. Hudson",
            "Daniel Zoran",
            "Mateusz Malinowski",
            "Andrew Kyle Lampinen",
            "Andrew Jaegle",
            "James L. McClelland",
            "L. Matthey",
            "Felix Hill",
            "Alexander Lerchner"
        ],
        "citations": 24,
        "references": 131,
        "year": 2023
    },
    {
        "title": "Relightify: Relightable 3D Faces from a Single Image via Diffusion Models",
        "abstract": "Following the remarkable success of diffusion models on image generation, recent works have also demonstrated their impressive ability to address a number of inverse problems in an unsupervised way, by properly constraining the sampling process based on a conditioning input. Motivated by this, in this paper, we present the first approach to use diffusion models as a prior for highly accurate 3D facial BRDF reconstruction from a single image. We start by leveraging a high-quality UV dataset of facial reflectance (diffuse and specular albedo and normals), which we render under varying illumination settings to simulate natural RGB textures and, then, train an unconditional diffusion model on concatenated pairs of rendered textures and reflectance components. At test time, we fit a 3D morphable model to the given image and unwrap the face in a partial UV texture. By sampling from the diffusion model, while retaining the observed texture part intact, the model inpaints not only the self-occluded areas but also the unknown reflectance components, in a single sequence of denoising steps. In contrast to existing methods, we directly acquire the observed texture from the input image, thus, resulting in more faithful and consistent reflectance estimation. Through a series of qualitative and quantitative comparisons, we demonstrate superior performance in both texture completion as well as reflectance reconstruction tasks.",
        "authors": [
            "Foivos Paraperas Papantoniou",
            "Alexandros Lattas",
            "Stylianos Moschoglou",
            "S. Zafeiriou"
        ],
        "citations": 24,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality",
        "abstract": "Diffusion models recently have been successfully applied for the visual synthesis of strikingly realistic appearing images. This raises strong concerns about their potential for malicious purposes. In this paper, we propose using the lightweight multi Local Intrinsic Dimensionality (multiLID), which has been originally developed in context of the detection of adversarial examples, for the automatic detection of synthetic images and the identification of the according generator networks. In contrast to many existing detection approaches, which often only work for GAN-generated images, the proposed method provides close to perfect detection results in many realistic use cases. Extensive experiments on known and newly created datasets demonstrate that the proposed multiLID approach exhibits superiority in diffusion detection and model identification.Since the empirical evaluations of recent publications on the detection of generated images are often mainly focused on the \"LSUN-Bedroom\" dataset, we further establish a comprehensive benchmark for the detection of diffusion-generated images, including samples from several diffusion models with different image sizes.The code for our experiments is provided at https://github.com/deepfake-study/deepfake-multiLID.",
        "authors": [
            "P. Lorenz",
            "Ricard Durall",
            "J. Keuper"
        ],
        "citations": 27,
        "references": 113,
        "year": 2023
    },
    {
        "title": "Non-autoregressive Conditional Diffusion Models for Time Series Prediction",
        "abstract": "Recently, denoising diffusion models have led to significant breakthroughs in the generation of images, audio and text. However, it is still an open question on how to adapt their strong modeling ability to model time series. In this paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves high-quality time series prediction with the introduction of two novel conditioning mechanisms: future mixup and autoregressive initialization. Similar to teacher forcing, future mixup allows parts of the ground-truth future predictions for conditioning, while autoregressive initialization helps better initialize the model with basic time series patterns such as short-term trends. Extensive experiments are performed on nine real-world datasets. Results show that TimeDiff consistently outperforms existing time series diffusion models, and also achieves the best overall performance across a variety of the existing strong baselines (including transformers and FiLM).",
        "authors": [
            "Lifeng Shen",
            "James Kwok"
        ],
        "citations": 26,
        "references": 50,
        "year": 2023
    },
    {
        "title": "InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models",
        "abstract": "While diffusion models excel at generating high-quality samples, their latent variables typically lack semantic meaning and are not suitable for representation learning. Here, we propose InfoDiffusion, an algorithm that augments diffusion models with low-dimensional latent variables that capture high-level factors of variation in the data. InfoDiffusion relies on a learning objective regularized with the mutual information between observed and hidden variables, which improves latent space quality and prevents the latents from being ignored by expressive diffusion-based decoders. Empirically, we find that InfoDiffusion learns disentangled and human-interpretable latent representations that are competitive with state-of-the-art generative and contrastive methods, while retaining the high sample quality of diffusion models. Our method enables manipulating the attributes of generated images and has the potential to assist tasks that require exploring a learned latent space to generate quality samples, e.g., generative design.",
        "authors": [
            "Yingheng Wang",
            "Yair Schiff",
            "Aaron Gokaslan",
            "Weishen Pan",
            "Fei Wang",
            "Chris De Sa",
            "Volodymyr Kuleshov"
        ],
        "citations": 25,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models",
        "abstract": "One of the key components within diffusion models is the UNet for noise prediction. While several works have explored basic properties of the UNet decoder, its encoder largely remains unexplored. In this work, we conduct the first comprehensive study of the UNet encoder. We empirically analyze the encoder features and provide insights to important questions regarding their changes at the inference process. In particular, we find that encoder features change gently, whereas the decoder features exhibit substantial variations across different time-steps. This finding inspired us",
        "authors": [
            "Senmao Li",
            "Taihang Hu",
            "Fahad Shahbaz Khan",
            "Linxuan Li",
            "Shiqi Yang",
            "Yaxing Wang",
            "Ming-Ming Cheng",
            "Jian Yang"
        ],
        "citations": 23,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Class-Balancing Diffusion Models",
        "abstract": "Diffusion-based models have shown the merits of generating high-quality visual data while preserving better diversity in recent studies. However, such observation is only justified with curated data distribution, where the data samples are nicely pre-processed to be uniformly distributed in terms of their labels. In practice, a long-tailed data distribution appears more common and how diffusion models perform on such classimbalanced data remains unknown. In this work, we first investigate this problem and observe significant degradation in both diversity and fidelity when the diffusion model is trained on datasets with classimbalanced distributions. Especially in tail classes, the generations largely lose diversity and we observe severe mode-collapse issues. To tackle this problem, we set from the hypothesis that the data distribution is not class-balanced, and propose Class-Balancing Diffusion Models (CBDM) that are trained with a distribution adjustment regularizer as a solution. Experiments show that images generated by CBDM exhibit higher diversity and quality in both quantitative and qualitative ways. Our method benchmarked the generation results on CIFAR100/CIFAR100LT dataset and shows out-standing performance on the downstream recognition task.",
        "authors": [
            "Yiming Qin",
            "Huangjie Zheng",
            "Jiangchao Yao",
            "Mingyuan Zhou",
            "Ya Zhang"
        ],
        "citations": 26,
        "references": 55,
        "year": 2023
    },
    {
        "title": "CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling",
        "abstract": "While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We attribute this problem to the role of the conditioning signal in inference and offer an improved sampling strategy for diffusion models that can increase generation diversity, especially at high guidance scales, with minimal loss of sample quality. Our sampling strategy anneals the conditioning signal by adding scheduled, monotonically decreasing Gaussian noise to the conditioning vector during inference to balance diversity and condition alignment. Our Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained model and sampling algorithm, and we show that it boosts the diversity of diffusion models in various conditional generation tasks. Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256$\\times$256 and 512$\\times$512 respectively.",
        "authors": [
            "Seyedmorteza Sadat",
            "Jakob Buhmann",
            "Derek Bradley",
            "Otmar Hilliges",
            "Romann M. Weber"
        ],
        "citations": 26,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Denoising diffusion models with geometry adaptation for high fidelity calorimeter simulation",
        "abstract": "Simulation is crucial for all aspects of collider data analysis, but the available computing budget in the High Luminosity LHC era will be severely constrained. Generative machine learning models may act as surrogates to replace physics-based full simulation of particle detectors, and diffusion models have recently emerged as the state of the art for other generative tasks. We introduce CaloDiffusion, a denoising diffusion model trained on the public CaloChallenge datasets to generate calorimeter showers. Our algorithm employs 3D cylindrical convolutions, which take advantage of symmetries of the underlying data representation. To handle irregular detector geometries, we augment the diffusion model with a new geometry latent mapping (GLaM) layer to learn forward and reverse transformations to a regular geometry that is suitable for cylindrical convolutions. The showers generated by our approach are nearly indistinguishable from the full simulation, as measured by several different metrics.",
        "authors": [
            "O. Amram",
            "K. Pedro"
        ],
        "citations": 23,
        "references": 57,
        "year": 2023
    },
    {
        "title": "PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance",
        "abstract": "Exploiting pre-trained diffusion models for restoration has recently become a favored alternative to the traditional task-specific training approach. Previous works have achieved noteworthy success by limiting the solution space using explicit degradation models. However, these methods often fall short when faced with complex degradations as they generally cannot be precisely modeled. In this paper, we propose PGDiff by introducing partial guidance, a fresh perspective that is more adaptable to real-world degradations compared to existing works. Rather than specifically defining the degradation process, our approach models the desired properties, such as image structure and color statistics of high-quality images, and applies this guidance during the reverse diffusion process. These properties are readily available and make no assumptions about the degradation process. When combined with a diffusion prior, this partial guidance can deliver appealing results across a range of restoration tasks. Additionally, PGDiff can be extended to handle composite tasks by consolidating multiple high-quality image properties, achieved by integrating the guidance from respective tasks. Experimental results demonstrate that our method not only outperforms existing diffusion-prior-based approaches but also competes favorably with task-specific models.",
        "authors": [
            "Peiqing Yang",
            "Shangchen Zhou",
            "Qingyi Tao",
            "Chen Change Loy"
        ],
        "citations": 24,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting",
        "abstract": "Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally-trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (refine). Notably, the generative performance of the model remains intact -- downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (synthesize).",
        "authors": [
            "Marcel Kollovieh",
            "Abdul Fatir Ansari",
            "Michael Bohlke-Schneider",
            "Jasper Zschiegner",
            "Hao Wang",
            "Yuyang Wang"
        ],
        "citations": 26,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Input Perturbation Reduces Exposure Bias in Diffusion Models",
        "abstract": "Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64$\\times$64, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at https://github.com/forever208/DDPM-IP",
        "authors": [
            "Mang Ning",
            "E. Sangineto",
            "Angelo Porrello",
            "S. Calderara",
            "R. Cucchiara"
        ],
        "citations": 45,
        "references": 58,
        "year": 2023
    },
    {
        "title": "SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models",
        "abstract": "The development of text-to-video (T2V), i.e., generating videos with a given text prompt, has been significantly advanced in recent years. However, relying solely on text prompts often results in ambiguous frame composition due to spatial uncertainty. The research community thus leverages the dense structure signals, e.g., per-frame depth/edge sequences, to enhance controllability, whose collection accordingly increases the burden of inference. In this work, we present SparseCtrl to enable flexible structure control with temporally sparse signals, requiring only one or a few inputs, as shown in Figure 1. It incorporates an additional condition encoder to process these sparse signals while leaving the pre-trained T2V model untouched. The proposed approach is compatible with various modalities, including sketches, depth maps, and RGB images, providing more practical control for video generation and promoting applications such as storyboarding, depth rendering, keyframe animation, and interpolation. Extensive experiments demonstrate the generalization of SparseCtrl on both original and personalized T2V generators. Codes and models will be publicly available at https://guoyww.github.io/projects/SparseCtrl .",
        "authors": [
            "Yuwei Guo",
            "Ceyuan Yang",
            "Anyi Rao",
            "Maneesh Agrawala",
            "Dahua Lin",
            "Bo Dai"
        ],
        "citations": 67,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis",
        "abstract": "Diffusion-based models have achieved state-of-the-art performance on text-to-image synthesis tasks. However, one critical limitation of these models is the low fidelity of generated images with respect to the text description, such as missing objects, mismatched attributes, and mislocated objects. One key reason for such inconsistencies is the inaccurate cross-attention to text in both the spatial dimension, which controls at what pixel region an object should appear, and the temporal dimension, which controls how different levels of details are added through the denoising steps. In this paper, we propose a new text-to-image algorithm that adds explicit control over spatial-temporal cross-attention in diffusion models. We first utilize a layout predictor to predict the pixel regions for objects mentioned in the text. We then impose spatial attention control by combining the attention over the entire text description and that over the local description of the particular object in the corresponding pixel region of that object. The temporal attention control is further added by allowing the combination weights to change at each denoising step, and the combination weights are optimized to ensure high fidelity between the image and the text. Experiments show that our method generates images with higher fidelity compared to diffusion-model-based baselines without fine-tuning the diffusion model. Our code is publicly available. 1",
        "authors": [
            "Qiucheng Wu",
            "Yujian Liu",
            "Handong Zhao",
            "T. Bui",
            "Zhe Lin",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "citations": 38,
        "references": 60,
        "year": 2023
    },
    {
        "title": "DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models",
        "abstract": "Magnetic resonance imaging (MRI) is a common and life-saving medical imaging technique. However, acquiring high signal-to-noise ratio MRI scans requires long scan times, resulting in increased costs and patient discomfort, and decreased throughput. Thus, there is great interest in denoising MRI scans, especially for the subtype of diffusion MRI scans that are severely SNR-limited. While most prior MRI denoising methods are supervised in nature, acquiring supervised training datasets for the multitude of anatomies, MRI scanners, and scan parameters proves impractical. Here, we propose Denoising Diffusion Models for Denoising Diffusion MRI (DDM$^2$), a self-supervised denoising method for MRI denoising using diffusion denoising generative models. Our three-stage framework integrates statistic-based denoising theory into diffusion models and performs denoising through conditional generation. During inference, we represent input noisy measurements as a sample from an intermediate posterior distribution within the diffusion Markov chain. We conduct experiments on 4 real-world in-vivo diffusion MRI datasets and show that our DDM$^2$ demonstrates superior denoising performances ascertained with clinically-relevant visual qualitative and quantitative metrics.",
        "authors": [
            "Tiange Xiang",
            "Mahmut Yurt",
            "Ali B. Syed",
            "K. Setsompop",
            "A. Chaudhari"
        ],
        "citations": 36,
        "references": 56,
        "year": 2023
    },
    {
        "title": "DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models",
        "abstract": "Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, DiffusionShield ensures low distortion of the original image, high watermark detection performance, and the ability to embed lengthy messages. We conduct rigorous and comprehensive experiments to show the effectiveness of DiffusionShield in defending against infringement by GDMs and its superiority over traditional watermarking methods. The code for DiffusionShield is accessible in https://github.com/Yingqiancui/DiffusionShield.",
        "authors": [
            "Yingqian Cui",
            "J. Ren",
            "Han Xu",
            "Pengfei He",
            "Hui Liu",
            "Lichao Sun",
            "Jiliang Tang"
        ],
        "citations": 41,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style",
        "abstract": "Diffusion models have demonstrated impressive capability of text-conditioned image synthesis, and broader application horizons are emerging by personalizing those pretrained diffusion models toward generating some specialized target object or style. In this paper, we aim to learn an unseen style by simply fine-tuning a pre-trained diffusion model with a handful of images (e.g., less than 10), so that the fine-tuned model can generate high-quality images of arbitrary objects in this style. Such extremely lowshot fine-tuning is accomplished by a novel toolkit of finetuning techniques, including text-to-image customized data augmentations, a content loss to facilitate content-style disentanglement, and sparse updating that focuses on only a few time steps. Our framework, dubbed Specialist Diffusion, is plug-and-play to existing diffusion model backbones and other personalization techniques. We demonstrate it to outperform the latest few-shot personalization alternatives of diffusion models such as Textual Inversion [7] and DreamBooth [24], in terms of learning highly sophisticated styles with ultra-sample-efficient tuning. We further show that Specialist Diffusion can be integrated on top of textual inversion to boost performance further, even on highly unusual styles. Our codes are available at: https://github.com/Picsart-AI-Research/Specialist-Diffusion.",
        "authors": [
            "Haoming Lu",
            "Hazarapet Tunanyan",
            "Kai Wang",
            "Shant Navasardyan",
            "Zhangyang Wang",
            "Humphrey Shi"
        ],
        "citations": 38,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Diffusion Models as Masked Autoencoders",
        "abstract": "There has been a longstanding belief that generation can facilitate a true understanding of visual data. In line with this, we revisit generatively pre-training visual representations in light of recent interest in denoising diffusion models. While directly pre-training with diffusion models does not produce strong representations, we condition diffusion models on masked input and formulate diffusion models as masked autoencoders (DiffMAE). Our approach is capable of (i) serving as a strong initialization for downstream recognition tasks, (ii) conducting high-quality image inpainting, and (iii) being effortlessly extended to video where it produces state-of-the-art classification accuracy. We further perform a comprehensive study on the pros and cons of design choices and build connections between diffusion models and masked autoencoders. Project page.",
        "authors": [
            "Chen Wei",
            "K. Mangalam",
            "Po-Yao (Bernie) Huang",
            "Yanghao Li",
            "Haoqi Fan",
            "Hu Xu",
            "Huiyu Wang",
            "Cihang Xie",
            "A. Yuille",
            "Christoph Feichtenhofer"
        ],
        "citations": 36,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Reinforcement Learning: A Survey",
        "abstract": "Diffusion models surpass previous generative models in sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions. This survey aims to provide an overview of this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by RL algorithms. Then, we present a taxonomy of existing methods based on the roles of diffusion models in RL and explore how the preceding challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks. Finally, we conclude the survey and offer insights into future research directions. We are actively maintaining a GitHub repository for papers and other related resources in utilizing diffusion models in RL: https://github.com/apexrl/Diff4RLSurvey.",
        "authors": [
            "Zhengbang Zhu",
            "Hanye Zhao",
            "Haoran He",
            "Yichao Zhong",
            "Shenyu Zhang",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "citations": 39,
        "references": 138,
        "year": 2023
    },
    {
        "title": "On The Detection of Synthetic Images Generated by Diffusion Models",
        "abstract": "Over the past decade, there has been tremendous progress in creating synthetic media, mainly thanks to the development of powerful methods based on generative adversarial networks (GAN). Very recently, methods based on diffusion models (DM) have been gaining the spotlight. In addition to providing an impressive level of photorealism, they enable the creation of text-based visual content, opening up new and exciting opportunities in many different application fields, from arts to video games. On the other hand, this property is an additional asset in the hands of malicious users, who can generate and distribute fake media perfectly adapted to their attacks, posing new challenges to the media forensic community. With this work, we seek to understand how difficult it is to distinguish synthetic images generated by diffusion models from pristine ones and whether current state-of-the-art detectors are suitable for the task. To this end, first we expose the forensics traces left by diffusion models, then study how current detectors, developed for GAN-generated images, perform on these new synthetic images, especially in challenging social-network scenarios involving image compression and resizing. Datasets and code are available at https:github.com/grip-unina/DMimageDetection.",
        "authors": [
            "Riccardo Corvi",
            "D. Cozzolino",
            "Giada Zingarini",
            "G. Poggi",
            "Koki Nagano",
            "L. Verdoliva"
        ],
        "citations": 176,
        "references": 34,
        "year": 2022
    },
    {
        "title": "Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths",
        "abstract": "AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models (DMs) are another class of deep generative models and have recently achieved remarkable performance on various image synthesis tasks. However, training image diffusion models usually requires substantial computational resources to achieve a high performance, which makes expanding diffusion models to high-dimensional video synthesis tasks more computationally expensive. To ease this problem while leveraging its advantages, we introduce lightweight video diffusion models that synthesize high-ﬁdelity and arbitrary-long videos from pure noise. Speciﬁcally, we propose to perform diffusion and de-noising in a low-dimensional 3D latent space, which sig-niﬁcantly outperforms previous methods on 3D pixel space when under a limited computational budget. In addition, though trained on tens of frames, our models is generate videos with arbitrary lengths, i.e., thousands of frames, in an autoregressive way. Finally, we further introduce conditional latent perturbation to reduce performance degradation during generating long-duration videos. Extensive experiments on various datasets and generated lengths suggest that our framework is able to sample much more realistic and longer videos than previous approaches, including GAN-based, autoregressive-based, and diffusion-based methods.",
        "authors": [
            "Yin-Yin He",
            "Tianyu Yang",
            "Yong Zhang",
            "Ying Shan",
            "Qifeng Chen"
        ],
        "citations": 180,
        "references": 44,
        "year": 2022
    },
    {
        "title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI",
        "abstract": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
        "authors": [
            "Chenshuang Zhang",
            "Chaoning Zhang",
            "Sheng Zheng",
            "Mengchun Zhang",
            "Maryam Qamar",
            "S. Bae",
            "In-So Kweon"
        ],
        "citations": 59,
        "references": 141,
        "year": 2023
    },
    {
        "title": "DiffuseStyleGesture: Stylized Audio-Driven Co-Speech Gesture Generation with Diffusion Models",
        "abstract": "The art of communication beyond speech there are gestures. The automatic co-speech gesture generation draws much attention in computer animation. It is a challenging task due to the diversity of gestures and the difficulty of matching the rhythm and semantics of the gesture to the corresponding speech. To address these problems, we present DiffuseStyleGesture, a diffusion model based speech-driven gesture generation approach. It generates high-quality, speech-matched, stylized, and diverse co-speech gestures based on given speeches of arbitrary length. Specifically, we introduce cross-local attention and self-attention to the gesture diffusion pipeline to generate better speech matched and realistic gestures. We then train our model with classifier-free guidance to control the gesture style by interpolation or extrapolation. Additionally, we improve the diversity of generated gestures with different initial gestures and noise. Extensive experiments show that our method outperforms recent approaches on speech-driven gesture generation. Our code, pre-trained models, and demos are available at https://github.com/YoungSeng/DiffuseStyleGesture.",
        "authors": [
            "Sicheng Yang",
            "Zhiyong Wu",
            "Minglei Li",
            "Zhensong Zhang",
            "Lei Hao",
            "Weihong Bao",
            "Ming Cheng",
            "Long Xiao"
        ],
        "citations": 50,
        "references": 56,
        "year": 2023
    },
    {
        "title": "A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material",
        "abstract": "Diffusion models have become a new SOTA generative modeling method in various fields, for which there are multiple survey works that provide an overall survey. With the number of articles on diffusion models increasing exponentially in the past few years, there is an increasing need for surveys of diffusion models on specific fields. In this work, we are committed to conducting a survey on the graph diffusion models. Even though our focus is to cover the progress of diffusion models in graphs, we first briefly summarize how other generative modeling methods are used for graphs. After that, we introduce the mechanism of diffusion models in various forms, which facilitates the discussion on the graph diffusion models. The applications of graph diffusion models mainly fall into the category of AI-generated content (AIGC) in science, for which we mainly focus on how graph diffusion models are utilized for generating molecules and proteins but also cover other cases, including materials design. Moreover, we discuss the issue of evaluating diffusion models in the graph domain and the existing challenges.",
        "authors": [
            "Mengchun Zhang",
            "Maryam Qamar",
            "Taegoo Kang",
            "Yuna Jung",
            "Chenshuang Zhang",
            "S. Bae",
            "Chaoning Zhang"
        ],
        "citations": 37,
        "references": 151,
        "year": 2023
    },
    {
        "title": "Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models",
        "abstract": "Recent work has showcased the significant potential of diffusion models in pose-guided person image synthesis. However, owing to the inconsistency in pose between the source and target images, synthesizing an image with a distinct pose, relying exclusively on the source image and target pose information, remains a formidable challenge. This paper presents Progressive Conditional Diffusion Models (PCDMs) that incrementally bridge the gap between person images under the target and source poses through three stages. Specifically, in the first stage, we design a simple prior conditional diffusion model that predicts the global features of the target image by mining the global alignment relationship between pose coordinates and image appearance. Then, the second stage establishes a dense correspondence between the source and target images using the global features from the previous stage, and an inpainting conditional diffusion model is proposed to further align and enhance the contextual features, generating a coarse-grained person image. In the third stage, we propose a refining conditional diffusion model to utilize the coarsely generated image from the previous stage as a condition, achieving texture restoration and enhancing fine-detail consistency. The three-stage PCDMs work progressively to generate the final high-quality and high-fidelity synthesized image. Both qualitative and quantitative results demonstrate the consistency and photorealism of our proposed PCDMs under challenging scenarios.The code and model will be available at https://github.com/tencent-ailab/PCDMs.",
        "authors": [
            "Fei Shen",
            "Hu Ye",
            "Jun Zhang",
            "Cong Wang",
            "Xiao Han",
            "Wei Yang"
        ],
        "citations": 33,
        "references": 47,
        "year": 2023
    },
    {
        "title": "ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection",
        "abstract": "Anomaly detection in multivariate time series data is of paramount importance for large-scale systems. However, accurately detecting anomalies in such data poses significant challenges due to the need for precise data modeling capability. Existing forecasting and reconstruction-based methods struggle to address these challenges effectively. To overcome these limitations, we propose a novel anomaly detection framework named ImDiffusion, which combines time series imputation and diffusion models to achieve accurate and robust anomaly detection. The imputation-based approach employed by ImDiffusion leverages the information from neighboring values in the time series, enabling precise modeling of temporal and inter-correlated dependencies, reducing uncertainty in the data, thereby enhancing the robustness of the anomaly detection process. ImDiffusion further leverages diffusion models as time series imputers to accurately capture complex dependencies. We leverage the step-by-step denoised outputs generated during the inference process to serve as valuable signals for anomaly prediction, resulting in improved accuracy and robustness of the detection process.\n We evaluate the performance of ImDiffusion via extensive experiments on benchmark datasets. The results demonstrate that our proposed framework significantly outperforms state-of-the-art approaches in terms of detection accuracy and timeliness. ImDiffusion is further integrated into the real production system in Microsoft and observes a remarkable 11.4% increase in detection F1 score compared to the legacy approach. To the best of our knowledge, ImDiffusion represents a pioneering approach that combines imputation-based techniques with time series anomaly detection, while introducing the novel use of diffusion models to the field.",
        "authors": [
            "Yuhang Chen",
            "C. Zhang",
            "Minghua Ma",
            "Yudong Liu",
            "Ruomeng Ding",
            "Bo Li",
            "Shilin He",
            "S. Rajmohan",
            "Qingwei Lin",
            "Dongmei Zhang"
        ],
        "citations": 32,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Matryoshka Diffusion Models",
        "abstract": "Diffusion models are the de facto approach for generating high-quality images and videos, but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space or using a downsampled latent space of a separately trained auto-encoder. In this paper, we introduce Matryoshka Diffusion Models(MDM), an end-to-end framework for high-resolution image and video synthesis. We propose a diffusion process that denoises inputs at multiple resolutions jointly and uses a NestedUNet architecture where features and parameters for small-scale inputs are nested within those of large scales. In addition, MDM enables a progressive training schedule from lower to higher resolutions, which leads to significant improvements in optimization for high-resolution generation. We demonstrate the effectiveness of our approach on various benchmarks, including class-conditioned image generation, high-resolution text-to-image, and text-to-video applications. Remarkably, we can train a single pixel-space model at resolutions of up to 1024x1024 pixels, demonstrating strong zero-shot generalization using the CC12M dataset, which contains only 12 million images. Our code is released at https://github.com/apple/ml-mdm",
        "authors": [
            "Jiatao Gu",
            "Shuangfei Zhai",
            "Yizhen Zhang",
            "Joshua M. Susskind",
            "Navdeep Jaitly"
        ],
        "citations": 34,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Stable Bias: Evaluating Societal Representations in Diffusion Models",
        "abstract": null,
        "authors": [
            "Sasha Luccioni",
            "Christopher Akiki",
            "Margaret Mitchell",
            "Yacine Jernite"
        ],
        "citations": 62,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Toward High-Quality HDR Deghosting With Conditional Diffusion Models",
        "abstract": "High Dynamic Range (HDR) images can be recovered from several Low Dynamic Range (LDR) images by existing Deep Neural Networks (DNNs) techniques. Despite the remarkable progress, DNN-based methods still generate ghosting artifacts when LDR images have saturation and large motion, which hinders potential applications in real-world scenarios. To address this challenge, we formulate the HDR deghosting problem as an image generation that leverages LDR features as the diffusion model’s condition, consisting of the feature condition generator and the noise predictor. Feature condition generator employs attention and Domain Feature Alignment (DFA) layer to transform the intermediate features to avoid ghosting artifacts. With the learned features as conditions, the noise predictor leverages a stochastic iterative denoising process for diffusion models to generate an HDR image by steering the sampling process. Furthermore, to mitigate semantic confusion caused by the saturation problem of LDR images, we design a sliding window noise estimator to sample smooth noise in a patch-based manner. In addition, an image space loss is proposed to avoid the color distortion of the estimated HDR results. We empirically evaluate our model on benchmark datasets for HDR imaging. The results demonstrate that our approach achieves state-of-the-art performances and well generalization to real-world images.",
        "authors": [
            "Qingsen Yan",
            "Tao Hu",
            "Yuan Sun",
            "Hao Tang",
            "Yu Zhu",
            "Wei Dong",
            "Luc Van Gool",
            "Yanning Zhang"
        ],
        "citations": 34,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics",
        "abstract": "Coarse-grained (CG) molecular dynamics enables the study of biological processes at temporal and spatial scales that would be intractable at an atomistic resolution. However, accurately learning a CG force field remains a challenge. In this work, we leverage connections between score-based generative models, force fields, and molecular dynamics to learn a CG force field without requiring any force inputs during training. Specifically, we train a diffusion generative model on protein structures from molecular dynamics simulations, and we show that its score function approximates a force field that can directly be used to simulate CG molecular dynamics. While having a vastly simplified training setup compared to previous work, we demonstrate that our approach leads to improved performance across several protein simulations for systems up to 56 amino acids, reproducing the CG equilibrium distribution and preserving the dynamics of all-atom simulations such as protein folding events.",
        "authors": [
            "Marloes Arts",
            "Victor Garcia Satorras",
            "Chin-Wei Huang",
            "Daniel Zuegner",
            "M. Federici",
            "C. Clementi",
            "Frank No'e",
            "Robert Pinsler",
            "Rianne van den Berg"
        ],
        "citations": 66,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Inserting Anybody in Diffusion Models via Celeb Basis",
        "abstract": "Exquisite demand exists for customizing the pretrained large text-to-image model, $\\textit{e.g.}$, Stable Diffusion, to generate innovative concepts, such as the users themselves. However, the newly-added concept from previous customization methods often shows weaker combination abilities than the original ones even given several images during training. We thus propose a new personalization method that allows for the seamless integration of a unique individual into the pre-trained diffusion model using just $\\textbf{one facial photograph}$ and only $\\textbf{1024 learnable parameters}$ under $\\textbf{3 minutes}$. So as we can effortlessly generate stunning images of this person in any pose or position, interacting with anyone and doing anything imaginable from text prompts. To achieve this, we first analyze and build a well-defined celeb basis from the embedding space of the pre-trained large text encoder. Then, given one facial photo as the target identity, we generate its own embedding by optimizing the weight of this basis and locking all other parameters. Empowered by the proposed celeb basis, the new identity in our customized model showcases a better concept combination ability than previous personalization methods. Besides, our model can also learn several new identities at once and interact with each other where the previous customization model fails to. The code will be released.",
        "authors": [
            "Genlan Yuan",
            "Xiaodong Cun",
            "Yong Zhang",
            "Maomao Li",
            "Chenyang Qi",
            "Xintao Wang",
            "Ying Shan",
            "Huicheng Zheng"
        ],
        "citations": 42,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Inst-Inpaint: Instructing to Remove Objects with Diffusion Models",
        "abstract": "Image inpainting task refers to erasing unwanted pixels from images and filling them in a semantically consistent and realistic way. Traditionally, the pixels that are wished to be erased are defined with binary masks. From the application point of view, a user needs to generate the masks for the objects they would like to remove which can be time-consuming and prone to errors. In this work, we are interested in an image inpainting algorithm that estimates which object to be removed based on natural language input and removes it, simultaneously. For this purpose, first, we construct a dataset named GQA-Inpaint for this task. Second, we present a novel inpainting framework, Inst-Inpaint, that can remove objects from images based on the instructions given as text prompts. We set various GAN and diffusion-based baselines and run experiments on synthetic and real image datasets. We compare methods with different evaluation metrics that measure the quality and accuracy of the models and show significant quantitative and qualitative improvements.",
        "authors": [
            "Ahmet Burak Yildirim",
            "Vedat Baday",
            "Erkut Erdem",
            "Aykut Erdem",
            "A. Dundar"
        ],
        "citations": 41,
        "references": 57,
        "year": 2023
    },
    {
        "title": "PreDiff: Precipitation Nowcasting with Latent Diffusion Models",
        "abstract": "Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge alignment mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly. We conduct empirical studies on two datasets: N-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in N-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.",
        "authors": [
            "Zhihan Gao",
            "Xingjian Shi",
            "Boran Han",
            "Hongya Wang",
            "Xiaoyong Jin",
            "Danielle C. Maddix",
            "Yi Zhu",
            "Mu Li",
            "Bernie Wang"
        ],
        "citations": 39,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Spontaneous symmetry breaking in generative diffusion models",
        "abstract": "\n Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking phenomenon that divides the generative dynamics into two distinct phases: (1) a linear steady-state dynamics around a central fixed-point and, (2) an attractor dynamics directed towards the data manifold. These two ‘phases’ are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3× Fréchet inception distance improvements on fast samplers, while also increasing sample diversity (e.g. racial composition of generated CelebA images). Our work offers a new way to understand the generative dynamics of diffusion models that has the potential to bring about higher performance and less biased fast-samplers.",
        "authors": [
            "G. Raya",
            "L. Ambrogioni"
        ],
        "citations": 22,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Multi-Architecture Multi-Expert Diffusion Models",
        "abstract": "In this paper, we address the performance degradation of efficient diffusion models by introducing Multi-architecturE Multi-Expert diffusion models (MEME). We identify the need for tailored operations at different time-steps in diffusion processes and leverage this insight to create compact yet high-performing models. MEME assigns distinct architectures to different time-step intervals, balancing convolution and self-attention operations based on observed frequency characteristics. We also introduce a soft interval assignment strategy for comprehensive training. Empirically, MEME operates 3.3 times faster than baselines while improving image generation quality (FID scores) by 0.62 (FFHQ) and 0.37 (CelebA). Though we validate the effectiveness of assigning more optimal architecture per time-step, where efficient models outperform the larger models, we argue that MEME opens a new design choice for diffusion models that can be easily applied in other scenarios, such as large multi-expert models.",
        "authors": [
            "Yunsung Lee",
            "Jin-Young Kim",
            "Hyojun Go",
            "Myeongho Jeong",
            "Shinhyeok Oh",
            "Seungtaek Choi"
        ],
        "citations": 22,
        "references": 74,
        "year": 2023
    },
    {
        "title": "CDDM: Channel Denoising Diffusion Models for Wireless Communications",
        "abstract": "Diffusion models (DM) can gradually learn to re-move noise, which have been widely used in artificial intelligence generated content (AIGC) in recent years. The property of DM for removing noise leads us to wonder whether DM can be applied to wireless communications to help the receiver eliminate the channel noise. To address this, we propose channel denoising diffusion models (CDDM) for wireless communications in this paper. CDDM can be applied as a new physical layer module after the channel equalization to learn the distribution of the channel input signal, and then utilizes this learned knowledge to remove the channel noise. We design corresponding training and sampling algorithms for the forward diffusion process and the reverse sampling process of CDDM. Moreover, we apply CDDM to a semantic communications system based on joint source-channel coding (JSCC). Experimental results demonstrate that CDDM can further reduce the mean square error (MSE) after minimum mean square error (MMSE) equalizer, and the joint CDDM and JSCC system achieves better performance than the JSCC system and the traditional JPEG2000 with low-density parity-check (LDPC) code approach.",
        "authors": [
            "Tong Wu",
            "Zhiyong Chen",
            "Dazhi He",
            "Liang Qian",
            "Yin Xu",
            "M. Tao",
            "Wenjun Zhang"
        ],
        "citations": 22,
        "references": 21,
        "year": 2023
    },
    {
        "title": "Diffusion Models Beat GANs on Image Classification",
        "abstract": "While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which uses a single pre-training stage to address both families of tasks simultaneously. We identify diffusion models as a prime candidate. Diffusion models have risen to prominence as a state-of-the-art method for image generation, denoising, inpainting, super-resolution, manipulation, etc. Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high fidelity, diverse, novel images. The U-Net architecture, as a convolution-based architecture, generates a diverse set of feature representations in the form of intermediate feature maps. We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification. We explore optimal methods for extracting and using these embeddings for classification tasks, demonstrating promising results on the ImageNet classification task. We find that with careful feature selection and pooling, diffusion models outperform comparable generative-discriminative methods such as BigBiGAN for classification tasks. We investigate diffusion models in the transfer learning regime, examining their performance on several fine-grained visual classification datasets. We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.",
        "authors": [
            "Soumik Mukhopadhyay",
            "M. Gwilliam",
            "Vatsal Agarwal",
            "Namitha Padmanabhan",
            "A. Swaminathan",
            "Srinidhi Hegde",
            "Tianyi Zhou",
            "Abhinav Shrivastava"
        ],
        "citations": 30,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt",
        "abstract": "Diffusion models have",
        "authors": [
            "Kai Chen",
            "Enze Xie",
            "Zhe Chen",
            "Lanqing Hong",
            "Zhenguo Li",
            "Dit-Yan Yeung"
        ],
        "citations": 29,
        "references": 58,
        "year": 2023
    },
    {
        "title": "DensePure: Understanding Diffusion Models for Adversarial Robustness",
        "abstract": "Diffusion models have been recently employed to improve certified robustness through the process of denoising. However, the theoretical understanding of why diffusion models are able to improve the certified robustness is still lacking, preventing from further improvement. In this study, we close this gap by analyzing the fundamental properties of diffusion models and establishing the conditions under which they can enhance certified robustness. This deeper understanding allows us to propose a new method DensePure , designed to improve the certified robustness of a pretrained model (i.e. classifier). Given an (adversarial) input, DensePure consists of multiple runs of denoising via the reverse process of the diffusion model (with different random seeds) to get multiple reversed samples, which are then passed through the classifier, followed by majority voting of inferred labels to make the final prediction. This design of using multiple runs of denoising is informed by our theoretical analysis of the conditional distribution of the reversed sample. Specifically, when the data density of a clean sample is high, its conditional density under the reverse process in a diffusion model is also high; thus sampling from the latter conditional distribution can purify the adversarial example and return the corresponding clean sample with a high probability. By using the highest density point in the conditional distribution as the reversed sample, we identify the robust region of a given instance under the diffusion model’s reverse process. We show that this robust region is a union of multiple convex sets, and is potentially much larger than the robust regions identified in previous works. In practice, DensePure can approximate the label of the high density region in the conditional distribution so that it can enhance certified robustness. We conduct extensive experiments to demonstrate the effectiveness of DensePure by evaluating its certified robustness given a standard model via randomized smoothing. We show that DensePure is consistently better than existing methods on ImageNet, with 7% improvement on average. Project",
        "authors": [
            "Chaowei Xiao",
            "Zhongzhu Chen",
            "Kun Jin",
            "Jiong Wang",
            "Weili Nie",
            "Mingyan Liu",
            "Anima Anandkumar",
            "Bo Li",
            "D. Song"
        ],
        "citations": 29,
        "references": 38,
        "year": 2023
    },
    {
        "title": "SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models",
        "abstract": "Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization. Leveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery. In addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing. However, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities. In this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation. We introduce SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for both image and video data. Thanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation across six datasets. Furthermore, our learned object features can be utilized by existing object-centric dynamics models, improving video prediction quality and downstream temporal reasoning tasks. Finally, we demonstrate the scalability of SlotDiffusion to unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated with self-supervised pre-trained image encoders.",
        "authors": [
            "Ziyi Wu",
            "Jingyu Hu",
            "Wuyue Lu",
            "Igor Gilitschenski",
            "Animesh Garg"
        ],
        "citations": 35,
        "references": 118,
        "year": 2023
    },
    {
        "title": "Zero-Shot Medical Image Translation via Frequency-Guided Diffusion Models",
        "abstract": "Recently, the diffusion model has emerged as a superior generative model that can produce high quality and realistic images. However, for medical image translation, the existing diffusion models are deficient in accurately retaining structural information since the structure details of source domain images are lost during the forward diffusion process and cannot be fully recovered through learned reverse diffusion, while the integrity of anatomical structures is extremely important in medical images. For instance, errors in image translation may distort, shift, or even remove structures and tumors, leading to incorrect diagnosis and inadequate treatments. Training and conditioning diffusion models using paired source and target images with matching anatomy can help. However, such paired data are very difficult and costly to obtain, and may also reduce the robustness of the developed model to out-of-distribution testing data. We propose a frequency-guided diffusion model (FGDM) that employs frequency-domain filters to guide the diffusion model for structure-preserving image translation. Based on its design, FGDM allows zero-shot learning, as it can be trained solely on the data from the target domain, and used directly for source-to-target domain translation without any exposure to the source-domain data during training. We evaluated it on three cone-beam CT (CBCT)-to-CT translation tasks for different anatomical sites, and a cross-institutional MR imaging translation task. FGDM outperformed the state-of-the-art methods (GAN-based, VAE-based, and diffusion-based) in metrics of Fréchet Inception Distance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM), showing its significant advantages in zero-shot medical image translation.",
        "authors": [
            "Yunxiang Li",
            "H. Shao",
            "Xiao Liang",
            "Liyuan Chen",
            "Ruiqi Li",
            "Steve B. Jiang",
            "Jing Wang",
            "You Zhang"
        ],
        "citations": 30,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Classification Diffusion Models",
        "abstract": "A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\\textit{classify}$ between data samples and samples from some reference distribution. DRE-based models can directly output the likelihood for any given input, a highly desired property that is lacking in most generative techniques. Nevertheless, to date, DRE methods have failed in accurately capturing the distributions of complex high-dimensional data, like images, and have thus been drawing reduced research attention in recent years. In this work we present $\\textit{classification diffusion models}$ (CDMs), a DRE-based generative method that adopts the formalism of denoising diffusion models (DDMs) while making use of a classifier that predicts the level of noise added to a clean signal. Our method is based on an analytical connection that we derive between the MSE-optimal denoiser for removing white Gaussian noise and the cross-entropy-optimal classifier for predicting the noise level. Our method is the first DRE-based technique that can successfully generate images beyond the MNIST dataset. Furthermore, it can output the likelihood of any input in a single forward pass, achieving state-of-the-art negative log likelihood (NLL) among methods with this property. Code is available on the project's webpage in https://shaharYadin.github.io/CDM/ .",
        "authors": [
            "Shahar Yadin",
            "Noam Elata",
            "T. Michaeli"
        ],
        "citations": 1,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Temporal Dynamic Quantization for Diffusion Models",
        "abstract": "The diffusion model has gained popularity in vision applications due to its remarkable generative performance and versatility. However, high storage and computation demands, resulting from the model size and iterative generation, hinder its use on mobile devices. Existing quantization techniques struggle to maintain performance even in 8-bit precision due to the diffusion model's unique property of temporal variation in activation. We introduce a novel quantization method that dynamically adjusts the quantization interval based on time step information, significantly improving output quality. Unlike conventional dynamic quantization techniques, our approach has no computational overhead during inference and is compatible with both post-training quantization (PTQ) and quantization-aware training (QAT). Our extensive experiments demonstrate substantial improvements in output quality with the quantized diffusion model across various datasets.",
        "authors": [
            "Junhyuk So",
            "Jungwon Lee",
            "Daehyun Ahn",
            "Hyungjun Kim",
            "Eunhyeok Park"
        ],
        "citations": 32,
        "references": 49,
        "year": 2023
    },
    {
        "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models",
        "abstract": "Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter via knowledge distillation so that it can acquire the powerful semantic understanding and reasoning capabilities to build a high-quality textual semantic representation for text-to-image generation. We conduct experiments by integrating multiple LLMs and popular pre-trained diffusion models to show the effectiveness of our approach in enabling diffusion models to understand and reason concise natural language without image quality degradation. Our approach can make text-to-image diffusion models easier to use with better user experience, which demonstrates our approach has the potential for further advancing the development of user-friendly text-to-image generation models by bridging the semantic gap between simple narrative prompts and complex keyword-based prompts. The code is released at https://github.com/Qrange-group/SUR-adapter.",
        "authors": [
            "Shan Zhong",
            "Zhongzhan Huang",
            "Wushao Wen",
            "Jinghui Qin",
            "Liang Lin"
        ],
        "citations": 28,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations",
        "abstract": "The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date(under 12 seconds for Stable Diffusion 1.4 without INT8 quantization for a 512 × 512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.",
        "authors": [
            "Yu-Hui Chen",
            "Raman Sarokin",
            "Juhyun Lee",
            "Jiuqiang Tang",
            "Chuo-Ling Chang",
            "Andrei Kulik",
            "Matthias Grundmann"
        ],
        "citations": 28,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Control3Diff: Learning Controllable 3D Diffusion Models from Single-view Images",
        "abstract": "Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging, due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any types of controlling inputs, allowing us to train it with the same diffusion objective without any auxiliary supervision. We validate the efficacy of Control3Diff on standard image generation benchmarks including FFHQ, AFHQ, and ShapeNet, using various conditioning inputs such as images, sketches, and text prompts.",
        "authors": [
            "Jiatao Gu",
            "Qingzhe Gao",
            "Shuangfei Zhai",
            "Baoquan Chen",
            "Lingjie Liu",
            "J. Susskind"
        ],
        "citations": 28,
        "references": 98,
        "year": 2023
    },
    {
        "title": "Inverse-design of nonlinear mechanical metamaterials via video denoising diffusion models",
        "abstract": null,
        "authors": [
            "Jan-Hendrik Bastek",
            "D. Kochmann"
        ],
        "citations": 51,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Modulating Pretrained Diffusion Models for Multimodal Image Synthesis",
        "abstract": "We present multimodal conditioning modules (MCM) for enabling conditional image synthesis using pretrained diffusion models. Previous multimodal synthesis works rely on training networks from scratch or fine-tuning pretrained networks, both of which are computationally expensive for large, state-of-the-art diffusion models. Our method uses pretrained networks but does not require any updates to the diffusion network’s parameters. MCM is a small module trained to modulate the diffusion network’s predictions during sampling using 2D modalities (e.g., semantic segmentation maps, sketches) that were unseen during the original training of the diffusion model. We show that MCM enables user control over the spatial layout of the image and leads to increased control over the image generation process. Training MCM is cheap as it does not require gradients from the original diffusion net, consists of only ∼ 1% of the number of parameters of the base diffusion model, and is trained using only a limited number of training examples. We evaluate our method on unconditional and text-conditional models to demonstrate the improved control over the generated images and their alignment with respect to the conditioning inputs.",
        "authors": [
            "Cusuh Ham",
            "James Hays",
            "Jingwan Lu",
            "Krishna Kumar Singh",
            "Zhifei Zhang",
            "T. Hinz"
        ],
        "citations": 23,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Exploring Diffusion Models for Unsupervised Video Anomaly Detection",
        "abstract": "This paper investigates the performance of diffusion models for video anomaly detection (VAD) within the most challenging but also the most operational scenario in which the data annotations are not used. As being sparse, diverse, contextual, and often ambiguous, detecting abnormal events precisely is a very ambitious task. To this end, we rely only on the information-rich spatio-temporal data, and the reconstruction power of the diffusion models such that a high reconstruction error is utilized to decide the abnormality. Experiments performed on two large-scale video anomaly detection datasets demonstrate the consistent improvement of the proposed method over the state-of-the-art generative models while in some cases our method achieves better scores than the more complex models. This is the first study using a diffusion model and examining its parameters’ influence to present guidance for VAD in surveillance scenarios.",
        "authors": [
            "Anil Osman Tur",
            "Nicola Dall'Asen",
            "Cigdem Beyan",
            "E. Ricci"
        ],
        "citations": 27,
        "references": 34,
        "year": 2023
    },
    {
        "title": "Generating Images of Rare Concepts Using Pre-trained Diffusion Models",
        "abstract": "Text-to-image diffusion models can synthesize high quality images, but they have various limitations. Here we highlight a common failure mode of these models, namely, generating uncommon concepts and structured concepts like hand palms. We show that their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. We characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, using a small reference set of images, a technique that we call SeedSelect. SeedSelect does not require retraining or finetuning the diffusion model. We assess the faithfulness, quality and diversity of SeedSelect in creating rare objects and generating complex formations like hand images, and find it consistently achieves superior performance. We further show the advantage of SeedSelect in semantic data augmentation. Generating semantically appropriate images can successfully improve performance in few-shot recognition benchmarks, for classes from the head and from the tail of the training data of diffusion models.",
        "authors": [
            "Dvir Samuel",
            "Rami Ben-Ari",
            "Simon Raviv",
            "N. Darshan",
            "Gal Chechik"
        ],
        "citations": 27,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Self-Correcting LLM-Controlled Diffusion Models",
        "abstract": "Text-to-image generation has witnessed significant progress with the advent of diffusion models. Despite the ability to generate photorealistic images, current text-to-image diffusion models still often struggle to accurately interpret and follow complex input text prompts. In contrast to existing models that aim to generate images only with their best effort, we introduce Self-correcting LLM-controlled Diffusion (SLD). SLD is a framework that generates an image from the input prompt, assesses its alignment with the prompt, and performs self-corrections on the inaccuracies in the generated image. Steered by an LLM controller, SLD turns text-to-image generation into an iterative closed-loop process, ensuring correctness in the resulting image. SLD is not only training-free but can also be seamlessly integrated with diffusion models behind API access, such as DALL-E 3, to further boost the performance of state-of-the-art diffusion models. Experimental results show that our approach can rectify a majority of incorrect generations, particularly in generative numeracy, attribute binding, and spatial relationships. Furthermore, by simply adjusting the instructions to the LLM, SLD can perform image editing tasks, bridging the gap between text-to-image generation and image editing pipelines. Our code is available at: https://self-correcting-llm-diffusion.github.io.",
        "authors": [
            "Tsung-Han Wu",
            "Long Lian",
            "Joseph Gonzalez",
            "Boyi Li",
            "Trevor Darrell"
        ],
        "citations": 25,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Prompt-tuning latent diffusion models for inverse problems",
        "abstract": "We propose a new method for solving imaging inverse problems using text-to-image latent diffusion models as general priors. Existing methods using latent diffusion models for inverse problems typically rely on simple null text prompts, which can lead to suboptimal performance. To address this limitation, we introduce a method for prompt tuning, which jointly optimizes the text embedding on-the-fly while running the reverse diffusion process. This allows us to generate images that are more faithful to the diffusion prior. In addition, we propose a method to keep the evolution of latent variables within the range space of the encoder, by projection. This helps to reduce image artifacts, a major problem when using latent diffusion models instead of pixel-based diffusion models. Our combined method, called P2L, outperforms both image- and latent-diffusion model-based inverse problem solvers on a variety of tasks, such as super-resolution, deblurring, and inpainting.",
        "authors": [
            "Hyungjin Chung",
            "Jong Chul Ye",
            "P. Milanfar",
            "M. Delbracio"
        ],
        "citations": 24,
        "references": 75,
        "year": 2023
    },
    {
        "title": "AdvDiffuser: Natural Adversarial Example Synthesis with Diffusion Models",
        "abstract": "Previous work on adversarial examples typically involves a fixed norm perturbation budget, which fails to capture the way humans perceive perturbations. Recent work has shifted towards natural unrestricted adversarial examples (UAEs) that breaks ℓp perturbation bounds but nonetheless remain semantically plausible. Current methods use GAN or VAE to generate UAEs by perturbing latent codes. However, this leads to loss of high-level information, resulting in low-quality and unnatural UAEs. In light of this, we propose AdvDiffuser, a new method for synthesizing natural UAEs using diffusion models. It can generate UAEs from scratch or conditionally based on reference images. To generate natural UAEs, we perturb predicted images to steer their latent code towards the adversarial sample space of a particular classifier. We also propose adversarial inpainting based on class activation mapping to retain the salient regions of the image while perturbing less important areas. On CIFAR-10, CelebA and ImageNet, we demonstrate that it can defeat the most robust models on the RobustBench leaderboard with near 100% success rates. Furthermore, The synthesized UAEs are not only more natural but also stronger compared to the current state-of-the-art attacks. Specifically, compared with GA-attack, the UAEs generated with AdvDiffuser exhibit 6× smaller LPIPS perturbations, 2 ~ 3× smaller FID scores and 0.28 higher in SSIM metrics, making them perceptually stealthier. Finally, adversarial training with AdvDiffuser further improves the model robustness against attacks with unseen threat models.1",
        "authors": [
            "Xinquan Chen",
            "Xitong Gao",
            "Juanjuan Zhao",
            "Kejiang Ye",
            "Chengjie Xu"
        ],
        "citations": 25,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Open-vocabulary Object Segmentation with Diffusion Models",
        "abstract": "The goal of this paper is to extract the visual-language correspondence from a pre-trained text-to-image diffusion model, in the form of segmentation map, i.e., simultaneously generating images and segmentation masks for the corresponding visual entities described in the text prompt. We make the following contributions: (i) we pair the existing Stable Diffusion model with a novel grounding module, that can be trained to align the visual and textual embedding space of the diffusion model with only a small number of object categories; (ii) we establish an automatic pipeline for constructing a dataset, that consists of {image, segmentation mask, text prompt} triplets, to train the proposed grounding module; (iii) we evaluate the performance of open-vocabulary grounding on images generated from the text-to-image diffusion model and show that the module can well segment the objects of categories beyond seen ones at training time, as shown in Fig. 1; (iv) we adopt the augmented diffusion model to build a synthetic semantic segmentation dataset, and show that, training a standard segmentation model on such dataset demonstrates competitive performance on the zero-shot segmentation (ZS3) benchmark, which opens up new opportunities for adopting the powerful diffusion model for discriminative tasks.",
        "authors": [
            "Ziyi Li",
            "Qinye Zhou",
            "Xiaoyun Zhang",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "citations": 53,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
        "abstract": "Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process. Moreover, we discuss the utilization of pre-trained language models (PLMs) for text diffusion models and introduce optimization techniques for text data. Finally, we discuss several promising directions and conclude this paper. Our survey aims to provide researchers with a systematic reference of related research on text diffusion models for NAR generation. We also demonstrate our collection of text diffusion models at https://github.com/RUCAIBox/Awesome-Text-Diffusion-Models.",
        "authors": [
            "Yifan Li",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Ji-rong Wen"
        ],
        "citations": 26,
        "references": 63,
        "year": 2023
    },
    {
        "title": "FinDiff: Diffusion Models for Financial Tabular Data Generation",
        "abstract": "The sharing of microdata, such as fund holdings and derivative instruments, by regulatory institutions presents a unique challenge due to strict data confidentiality and privacy regulations. These challenges often hinder the ability of both academics and practitioners to conduct collaborative research effectively. The emergence of generative models, particularly diffusion models, capable of synthesizing data mimicking the underlying distributions of real-world data presents a compelling solution. This work introduces Financial Tabular Diffusion (FinDiff), a diffusion model designed to generate real-world mixed-type financial tabular data for a variety of downstream tasks, for example, economic scenario modeling, stress tests, and fraud detection. The model uses embedding encodings to model mixed modality financial data, comprising both categorical and numeric attributes. The performance of FinDiff in generating synthetic tabular financial data is evaluated against state-of-the-art baseline models using three real-world financial datasets (including two publicly available datasets and one proprietary dataset). Empirical results demonstrate that FinDiff excels in generating synthetic tabular financial data with high fidelity, privacy, and utility.",
        "authors": [
            "Timur Sattarov",
            "Marco Schreyer",
            "Damian Borth"
        ],
        "citations": 24,
        "references": 42,
        "year": 2023
    },
    {
        "title": "StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation",
        "abstract": "The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the diversity of the generated avatars. Additionally, we develop a latent diffusion model within the style space of StyleGAN to enable the generation of avatars based on image inputs. Our approach demonstrates superior performance over current state-of-the-art methods in terms of visual quality and diversity of the produced avatars.",
        "authors": [
            "Chi Zhang",
            "Yiwen Chen",
            "Yijun Fu",
            "Zheng-Yang Zhou",
            "YU Gang",
            "Billzb Wang",
            "Bin Fu",
            "Tao Chen",
            "Guosheng Lin",
            "Chunhua Shen"
        ],
        "citations": 24,
        "references": 55,
        "year": 2023
    },
    {
        "title": "A Comprehensive Survey on Knowledge Distillation of Diffusion Models",
        "abstract": "Diffusion Models (DMs), also referred to as score-based diffusion models, utilize neural networks to specify score functions. Unlike most other probabilistic models, DMs directly model the score functions, which makes them more flexible to parametrize and potentially highly expressive for probabilistic modeling. DMs can learn fine-grained knowledge, i.e., marginal score functions, of the underlying distribution. Therefore, a crucial research direction is to explore how to distill the knowledge of DMs and fully utilize their potential. Our objective is to provide a comprehensible overview of the modern approaches for distilling DMs, starting with an introduction to DMs and a discussion of the challenges involved in distilling them into neural vector fields. We also provide an overview of the existing works on distilling DMs into both stochastic and deterministic implicit generators. Finally, we review the accelerated diffusion sampling algorithms as a training-free method for distillation. Our tutorial is intended for individuals with a basic understanding of generative models who wish to apply DM's distillation or embark on a research project in this field.",
        "authors": [
            "Weijian Luo"
        ],
        "citations": 26,
        "references": 74,
        "year": 2023
    },
    {
        "title": "DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing",
        "abstract": "Diffusion models have been leveraged to perform adversarial purification and thus provide both empirical and certified robustness for a standard model. On the other hand, different robustly trained smoothed models have been studied to improve the certified robustness. Thus, it raises a natural question: Can diffusion model be used to achieve improved certified robustness on those robustly trained smoothed models? In this work, we first theoretically show that recovered instances by diffusion models are in the bounded neighborhood of the original instance with high probability; and the\"one-shot\"denoising diffusion probabilistic models (DDPM) can approximate the mean of the generated distribution of a continuous-time diffusion model, which approximates the original instance under mild conditions. Inspired by our analysis, we propose a certifiably robust pipeline DiffSmooth, which first performs adversarial purification via diffusion models and then maps the purified instances to a common region via a simple yet effective local smoothing strategy. We conduct extensive experiments on different datasets and show that DiffSmooth achieves SOTA-certified robustness compared with eight baselines. For instance, DiffSmooth improves the SOTA-certified accuracy from $36.0\\%$ to $53.0\\%$ under $\\ell_2$ radius $1.5$ on ImageNet. The code is available at [https://github.com/javyduck/DiffSmooth].",
        "authors": [
            "Jiawei Zhang",
            "Zhongzhu Chen",
            "Huan Zhang",
            "Chaowei Xiao",
            "Bo Li"
        ],
        "citations": 19,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Restoring Vision in Adverse Weather Conditions With Patch-Based Denoising Diffusion Models",
        "abstract": "Image restoration under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image restoration algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image restoration by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image restoration, and experimentally show strong generalization to real-world test images.",
        "authors": [
            "Ozan Özdenizci",
            "R. Legenstein"
        ],
        "citations": 172,
        "references": 77,
        "year": 2022
    },
    {
        "title": "Understanding Diffusion Models: A Unified Perspective",
        "abstract": "Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.",
        "authors": [
            "Calvin Luo"
        ],
        "citations": 259,
        "references": 27,
        "year": 2022
    },
    {
        "title": "MMA-Diffusion: MultiModal Attack on Diffusion Models",
        "abstract": "In recent years, Text-to-Image (T2I) models have seen remarkable advancements, gaining widespread adoption. However, this progress has inadvertently opened avenues for potential misuse, particularly in generating inappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces MMA-Diffusion, a framework that presents a significant and realistic threat to the security of T2I models by effectively circumventing current defensive measures in both open-source models and commercial online services. Unlike previous approaches, MMA-Diffusion leverages both textual and visual modalities to bypass safeguards like prompt filters and post-hoc safety checkers, thus exposing and highlighting the vulnerabilities in existing defense mechanisms. Our codes are available at https://github.com/cure-lab/MMA-Diffusion.",
        "authors": [
            "Yijun Yang",
            "Ruiyuan Gao",
            "Xiaosen Wang",
            "Nan Xu",
            "Qiang Xu"
        ],
        "citations": 30,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Brain Imaging Generation with Latent Diffusion Models",
        "abstract": "Deep neural networks have brought remarkable breakthroughs in medical image analysis. However, due to their data-hungry nature, the modest dataset sizes in medical imaging projects might be hindering their full potential. Generating synthetic data provides a promising alternative, allowing to complement training datasets and conducting medical image research at a larger scale. Diffusion models recently have caught the attention of the computer vision community by producing photorealistic synthetic images. In this study, we explore using Latent Diffusion Models to generate synthetic images from high-resolution 3D brain images. We used T1w MRI images from the UK Biobank dataset (N=31,740) to train our models to learn about the probabilistic distribution of brain images, conditioned on covariables, such as age, sex, and brain structure volumes. We found that our models created realistic data, and we could use the conditioning variables to control the data generation effectively. Besides that, we created a synthetic dataset with 100,000 brain images and made it openly available to the scientific community.",
        "authors": [
            "W. H. Pinaya",
            "Petru-Daniel Tudosiu",
            "J. Dafflon",
            "P. F. D. Costa",
            "Virginia Fernandez",
            "P. Nachev",
            "S. Ourselin",
            "M. Cardoso"
        ],
        "citations": 229,
        "references": 35,
        "year": 2022
    },
    {
        "title": "Anomaly Detection with Conditioned Denoising Diffusion Models",
        "abstract": "Traditional reconstruction-based methods have struggled to achieve competitive performance in anomaly detection. In this paper, we introduce Denoising Diffusion Anomaly Detection (DDAD), a novel denoising process for image reconstruction conditioned on a target image. This ensures a coherent restoration that closely resembles the target image. Our anomaly detection framework employs the conditioning mechanism, where the target image is set as the input image to guide the denoising process, leading to a defectless reconstruction while maintaining nominal patterns. Anomalies are then localised via a pixel-wise and feature-wise comparison of the input and reconstructed image. Finally, to enhance the effectiveness of the feature-wise comparison, we introduce a domain adaptation method that utilises nearly identical generated examples from our conditioned denoising process to fine-tune the pretrained feature extractor. The veracity of DDAD is demonstrated on various datasets including MVTec and VisA benchmarks, achieving state-of-the-art results of \\(99.8 \\%\\) and \\(98.9 \\%\\) image-level AUROC respectively.",
        "authors": [
            "Arian Mousakhan",
            "T. Brox",
            "Jawad Tayyub"
        ],
        "citations": 29,
        "references": 55,
        "year": 2023
    },
    {
        "title": "ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models",
        "abstract": "Generating novel views of an object from a single image is a challenging task. It requires an understanding of the underlying 3D structure of the object from an image and ren-dering high-quality, spatially consistent new views. While recent methods for view synthesis based on diffusion have shown great progress, achieving consistency among various view estimates and at the same time abiding by the desired camera pose remains a critical problem yet to be solved. In this work, we demonstrate a strikingly simple method, where we utilize a pre-trained video diffusion model to solve this problem. Our key idea is that synthesizing a novel view could be reformulated as synthesizing a video of a cam-era going around the object of interest-a scanning video-which then allows us to leverage the powerful priors that a video diffusion model would have learned. Thus, to perform novel-view synthesis, we create a smooth camera trajectory to the target view that we wish to render, and denoise using both a view-conditioned diffusion model and a video diffusion model. By doing so, we obtain a highly consistent novel view synthesis, outperforming the state of the art.",
        "authors": [
            "Jeong-gi Kwak",
            "Erqun Dong",
            "Yuhe Jin",
            "Hanseok Ko",
            "Shweta Mahajan",
            "Kwang Moo Yi"
        ],
        "citations": 29,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Bellman Diffusion Models",
        "abstract": "Diffusion models have seen tremendous success as generative architectures. Recently, they have been shown to be effective at modelling policies for offline reinforcement learning and imitation learning. We explore using diffusion as a model class for the successor state measure (SSM) of a policy. We find that enforcing the Bellman flow constraints leads to a simple Bellman update on the diffusion step distribution.",
        "authors": [
            "Liam Schramm",
            "Abdeslam Boularias"
        ],
        "citations": 0,
        "references": 7,
        "year": 2024
    },
    {
        "title": "MADiff: Offline Multi-agent Learning with Diffusion Models",
        "abstract": "Offline reinforcement learning (RL) aims to learn policies from pre-existing datasets without further interactions, making it a challenging task. Q-learning algorithms struggle with extrapolation errors in offline settings, while supervised learning methods are constrained by model expressiveness. Recently, diffusion models (DMs) have shown promise in overcoming these limitations in single-agent learning, but their application in multi-agent scenarios remains unclear. Generating trajectories for each agent with independent DMs may impede coordination, while concatenating all agents' information can lead to low sample efficiency. Accordingly, we propose MADiff, which is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple agents. To our knowledge, MADiff is the first diffusion-based multi-agent learning framework, functioning as both a decentralized policy and a centralized controller. During decentralized executions, MADiff simultaneously performs teammate modeling, and the centralized controller can also be applied in multi-agent trajectory predictions. Our experiments demonstrate that MADiff outperforms baseline algorithms across various multi-agent learning tasks, highlighting its effectiveness in modeling complex multi-agent interactions. Our code is available at https://github.com/zbzhu99/madiff.",
        "authors": [
            "Zhengbang Zhu",
            "Minghuan Liu",
            "Liyuan Mao",
            "Bingyi Kang",
            "Minkai Xu",
            "Yong Yu",
            "Stefano Ermon",
            "Weinan Zhang"
        ],
        "citations": 22,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Unlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation",
        "abstract": "Diffusion models have demonstrated remarkable performance in image generation tasks, paving the way for powerful AIGC applications. However, these widely-used generative models can also raise security and privacy concerns, such as copyright infringement, and sensitive data leakage. To tackle these issues, we propose a method, Unlearnable Diffusion Perturbation, to safeguard images from unauthorized exploitation. Our approach involves designing an algorithm to generate sample-wise perturbation noise for each image to be protected. This imperceptible protective noise makes the data almost unlearnable for diffusion models, i.e., diffusion models trained or fine-tuned on the protected data cannot generate high-quality and diverse images related to the protected training data. Theoretically, we frame this as a max-min optimization problem and introduce EUDP, a noise scheduler-based method to enhance the effectiveness of the protective noise. We evaluate our methods on both Denoising Diffusion Probabilistic Model and Latent Diffusion Models, demonstrating that training diffusion models on the protected data lead to a significant reduction in the quality of the generated images. Especially, the experimental results on Stable Diffusion demonstrate that our method effectively safeguards images from being used to train Diffusion Models in various tasks, such as training specific objects and styles. This achievement holds significant importance in real-world scenarios, as it contributes to the protection of privacy and copyright against AI-generated content.",
        "authors": [
            "Zhengyue Zhao",
            "Jinhao Duan",
            "Xingui Hu",
            "Kaidi Xu",
            "Chenan Wang",
            "Rui Zhang",
            "Zidong Du",
            "Qi Guo",
            "Yunji Chen"
        ],
        "citations": 22,
        "references": 45,
        "year": 2023
    },
    {
        "title": "On the Generalization Properties of Diffusion Models",
        "abstract": "Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progressively increasing distances between modes. This precisely elucidates the adverse effect of\"modes shift\"in ground truths on the model generalization. Moreover, these estimates are not solely theoretical constructs but have also been confirmed through numerical simulations. Our findings contribute to the rigorous understanding of diffusion models' generalization properties and provide insights that may guide practical applications.",
        "authors": [
            "Puheng Li",
            "Zhong Li",
            "Huishuai Zhang",
            "Jiang Bian"
        ],
        "citations": 22,
        "references": 71,
        "year": 2023
    },
    {
        "title": "A Reproducible Extraction of Training Images from Diffusion Models",
        "abstract": "Recently, Carlini et al. demonstrated the widely used model Stable Diffusion can regurgitate real training samples, which is troublesome from a copyright perspective. In this work, we provide an efficient extraction attack on par with the recent attack, with several order of magnitudes less network evaluations. In the process, we expose a new phenomena, which we dub template verbatims, wherein a diffusion model will regurgitate a training sample largely in tact. Template verbatims are harder to detect as they require retrieval and masking to correctly label. Furthermore, they are still generated by newer systems, even those which de-duplicate their training set, and we give insight into why they still appear during generation. We extract training images from several state of the art systems, including Stable Diffusion 2.0, Deep Image Floyd, and finally Midjourney v4. We release code to verify our extraction attack, perform the attack, as well as all extracted prompts at \\url{https://github.com/ryanwebster90/onestep-extraction}.",
        "authors": [
            "Ryan Webster"
        ],
        "citations": 25,
        "references": 25,
        "year": 2023
    },
    {
        "title": "PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models",
        "abstract": "We introduce PolyDiff, the first diffusion-based approach capable of directly generating realistic and diverse 3D polygonal meshes. In contrast to methods that use alternate 3D shape representations (e.g. implicit representations), our approach is a discrete denoising diffusion probabilistic model that operates natively on the polygonal mesh data structure. This enables learning of both the geometric properties of vertices and the topological characteristics of faces. Specifically, we treat meshes as quantized triangle soups, progressively corrupted with categorical noise in the forward diffusion phase. In the reverse diffusion phase, a transformer-based denoising network is trained to revert the noising process, restoring the original mesh structure. At inference, new meshes can be generated by applying this denoising network iteratively, starting with a completely noisy triangle soup. Consequently, our model is capable of producing high-quality 3D polygonal meshes, ready for integration into downstream 3D workflows. Our extensive experimental analysis shows that PolyDiff achieves a significant advantage (avg. FID and JSD improvement of 18.2 and 5.8 respectively) over current state-of-the-art methods.",
        "authors": [
            "A. Alliegro",
            "Yawar Siddiqui",
            "Tatiana Tommasi",
            "Matthias Nießner"
        ],
        "citations": 24,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Differentially Private Latent Diffusion Models",
        "abstract": "Diffusion models (DMs) are one of the most widely used generative models for producing high quality images. However, a flurry of recent papers points out that DMs are least private forms of image generators, by extracting a significant number of near-identical replicas of training images from DMs. Existing privacy-enhancing techniques for DMs, unfortunately, do not provide a good privacy-utility tradeoff. In this paper, we aim to improve the current state of DMs with differential privacy (DP) by adopting the \\textit{Latent} Diffusion Models (LDMs). LDMs are equipped with powerful pre-trained autoencoders that map the high-dimensional pixels into lower-dimensional latent representations, in which DMs are trained, yielding a more efficient and fast training of DMs. Rather than fine-tuning the entire LDMs, we fine-tune only the $\\textit{attention}$ modules of LDMs with DP-SGD, reducing the number of trainable parameters by roughly $90\\%$ and achieving a better privacy-accuracy trade-off. Our approach allows us to generate realistic, high-dimensional images (256x256) conditioned on text prompts with DP guarantees, which, to the best of our knowledge, has not been attempted before. Our approach provides a promising direction for training more powerful, yet training-efficient differentially private DMs, producing high-quality DP images. Our code is available at https://anonymous.4open.science/r/DP-LDM-4525.",
        "authors": [
            "Saiyue Lyu",
            "Margarita Vinaroz",
            "Michael F. Liu",
            "Mijung Park"
        ],
        "citations": 18,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Addressing Negative Transfer in Diffusion Models",
        "abstract": "Diffusion-based generative models have achieved remarkable success in various domains. It trains a shared model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of negative transfer, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we first aim to analyze diffusion training from an MTL standpoint, presenting two key observations: (O1) the task affinity between denoising tasks diminishes as the gap between noise levels widens, and (O2) negative transfer can arise even in diffusion training. Building upon these observations, we aim to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL methods, but the presence of a huge number of denoising tasks makes this computationally expensive to calculate the necessary per-task loss or gradient. To address this challenge, we propose clustering the denoising tasks into small task clusters and applying MTL methods to them. Specifically, based on (O2), we employ interval clustering to enforce temporal proximity among denoising tasks within clusters. We show that interval clustering can be solved using dynamic programming, utilizing signal-to-noise ratio, timestep, and task affinity for clustering objectives. Through this, our approach addresses the issue of negative transfer in diffusion models by allowing for efficient computation of MTL methods. We validate the efficacy of proposed clustering and its integration with MTL methods through various experiments, demonstrating 1) improved generation quality and 2) faster training convergence of diffusion models.",
        "authors": [
            "Hyojun Go",
            "Jinyoung Kim",
            "Yunsung Lee",
            "Seunghyun Lee",
            "Shinhyeok Oh",
            "Hyeongdon Moon",
            "Seungtaek Choi"
        ],
        "citations": 17,
        "references": 84,
        "year": 2023
    },
    {
        "title": "The Journey, Not the Destination: How Data Guides Diffusion Models",
        "abstract": "Diffusion models trained on large datasets can synthesize photo-realistic images of remarkable quality and diversity. However, attributing these images back to the training data-that is, identifying specific training examples which caused an image to be generated-remains a challenge. In this paper, we propose a framework that: (i) provides a formal notion of data attribution in the context of diffusion models, and (ii) allows us to counterfactually validate such attributions. Then, we provide a method for computing these attributions efficiently. Finally, we apply our method to find (and evaluate) such attributions for denoising diffusion probabilistic models trained on CIFAR-10 and latent diffusion models trained on MS COCO. We provide code at https://github.com/MadryLab/journey-TRAK .",
        "authors": [
            "Kristian Georgiev",
            "Joshua Vendrow",
            "Hadi Salman",
            "Sung Min Park",
            "Aleksander Mądry"
        ],
        "citations": 17,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Learning to Read Braille: Bridging the Tactile Reality Gap with Diffusion Models",
        "abstract": "Simulating vision-based tactile sensors enables learning models for contact-rich tasks when collecting real world data at scale can be prohibitive. However, modeling the optical response of the gel deformation as well as incorporating the dynamics of the contact makes sim2real challenging. Prior works have explored data augmentation, fine-tuning, or learning generative models to reduce the sim2real gap. In this work, we present the first method to leverage probabilistic diffusion models for capturing complex illumination changes from gel deformations. Our tactile diffusion model is able to generate realistic tactile images from simulated contact depth bridging the reality gap for vision-based tactile sensing. On real braille reading task with a DIGIT sensor, a classifier trained with our diffusion model achieves 75.74% accuracy outperforming classifiers trained with simulation and other approaches. Project page: https://github.com/carolinahiguera/Tactile-Diffusion",
        "authors": [
            "C. Higuera",
            "Byron Boots",
            "Mustafa Mukadam"
        ],
        "citations": 21,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation",
        "abstract": "Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting using the notion of trace class operators. Second, we illustrate that approximating the score function with an operator network, in our case Fourier neural operators (FNOs), is beneficial for multilevel training. After deriving the forward process in the infinite-dimensional setting and reverse processes for finite approximations, we show their well-posedness, derive adequate discretizations, and investigate the role of the latent distributions. We provide first promising numerical results on two datasets, MNIST and material structures. In particular, we show that multilevel training is feasible within this framework.",
        "authors": [
            "Paul Hagemann",
            "Lars Ruthotto",
            "G. Steidl",
            "Ni Yang"
        ],
        "citations": 21,
        "references": 85,
        "year": 2023
    },
    {
        "title": "SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis",
        "abstract": "Text-conditioned image generation has made significant progress in recent years with generative adversarial networks and more recently, diffusion models. While diffusion models conditioned on text prompts have produced impressive and high-quality images, accurately representing complex text prompts such as the number of instances of a specific object remains challenging.To address this limitation, we propose a novel guidance approach for the sampling process in the diffusion model that leverages bounding box and segmentation map information at inference time without additional training data. Through a novel loss in the sampling process, our approach guides the model with semantic features from CLIP embeddings and enforces geometric constraints, leading to high-resolution images that accurately represent the scene. To obtain bounding box and segmentation map information, we structure the text prompt as a scene graph and enrich the nodes with CLIP embeddings. Our proposed model achieves state-of-the-art performance on two public benchmarks for image generation from scene graphs, surpassing both scene graph to image and text-based diffusion models in various metrics. Our results demonstrate the effectiveness of incorporating bounding box and segmentation map guidance in the diffusion model sampling process for more accurate text-to-image generation. Project Page: scenegenie.github.io/SceneGenie/",
        "authors": [
            "Azade Farshad",
            "Yousef Yeganeh",
            "Yucong Chi",
            "Cheng-nan Shen",
            "B. Ommer",
            "N. Navab"
        ],
        "citations": 20,
        "references": 77,
        "year": 2023
    },
    {
        "title": "The Hidden Language of Diffusion Models",
        "abstract": "Text-to-image diffusion models have demonstrated an unparalleled ability to generate high-quality, diverse images from a textual prompt. However, the internal representations learned by these models remain an enigma. In this work, we present Conceptor, a novel method to interpret the internal representation of a textual concept by a diffusion model. This interpretation is obtained by decomposing the concept into a small set of human-interpretable textual elements. Applied over the state-of-the-art Stable Diffusion model, Conceptor reveals non-trivial structures in the representations of concepts. For example, we find surprising visual connections between concepts, that transcend their textual semantics. We additionally discover concepts that rely on mixtures of exemplars, biases, renowned artistic styles, or a simultaneous fusion of multiple meanings of the concept. Through a large battery of experiments, we demonstrate Conceptor's ability to provide meaningful, robust, and faithful decompositions for a wide variety of abstract, concrete, and complex textual concepts, while allowing to naturally connect each decomposition element to its corresponding visual impact on the generated images. Our code will be available at: https://hila-chefer.github.io/Conceptor/",
        "authors": [
            "Hila Chefer",
            "Oran Lang",
            "Mor Geva",
            "Volodymyr Polosukhin",
            "Assaf Shocher",
            "M. Irani",
            "Inbar Mosseri",
            "Lior Wolf"
        ],
        "citations": 22,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Diffusion Models in NLP: A Survey",
        "abstract": "Diffusion models have become a powerful family of deep generative models, with record-breaking performance in many applications. This paper first gives an overview and derivation of the basic theory of diffusion models, then reviews the research results of diffusion models in the field of natural language processing, from text generation, text-driven image generation and other four aspects, and analyzes and summarizes the relevant literature materials sorted out, and finally records the experience and feelings of this topic literature review research.",
        "authors": [
            "Yuansong Zhu",
            "Yu Zhao"
        ],
        "citations": 22,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models",
        "abstract": "Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a new dataset to establish a fair evaluation setup and apply it to Stable Diffusion, also applicable to other generative models. With the proposed dataset, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future.",
        "authors": [
            "Jan Dubi'nski",
            "A. Kowalczuk",
            "Stanislaw Pawlak",
            "Przemyslaw Rokita",
            "Tomasz Trzci'nski",
            "P. Morawiecki"
        ],
        "citations": 22,
        "references": 45,
        "year": 2023
    },
    {
        "title": "AutoDecoding Latent 3D Diffusion Models",
        "abstract": "We present a novel approach to the generation of static and articulated 3D assets that has a 3D autodecoder at its core. The 3D autodecoder framework embeds properties learned from the target dataset in the latent space, which can then be decoded into a volumetric representation for rendering view-consistent appearance and geometry. We then identify the appropriate intermediate volumetric latent space, and introduce robust normalization and de-normalization operations to learn a 3D diffusion from 2D images or monocular videos of rigid or articulated objects. Our approach is flexible enough to use either existing camera supervision or no camera information at all -- instead efficiently learning it during training. Our evaluations demonstrate that our generation results outperform state-of-the-art alternatives on various benchmark datasets and metrics, including multi-view image datasets of synthetic objects, real in-the-wild videos of moving people, and a large-scale, real video dataset of static objects.",
        "authors": [
            "Evangelos Ntavelis",
            "Aliaksandr Siarohin",
            "Kyle Olszewski",
            "Chao-Yuan Wang",
            "Luc Van Gool",
            "Sergey Tulyakov"
        ],
        "citations": 36,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Self-Supervised MRI Reconstruction with Unrolled Diffusion Models",
        "abstract": "Magnetic Resonance Imaging (MRI) produces excellent soft tissue contrast, albeit it is an inherently slow imaging modality. Promising deep learning methods have recently been proposed to reconstruct accelerated MRI scans. However, existing methods still suffer from various limitations regarding image fidelity, contextual sensitivity, and reliance on fully-sampled acquisitions for model training. To comprehensively address these limitations, we propose a novel self-supervised deep reconstruction model, named Self-Supervised Diffusion Reconstruction (SSDiffRecon). SSDiffRecon expresses a conditional diffusion process as an unrolled architecture that interleaves cross-attention transformers for reverse diffusion steps with data-consistency blocks for physics-driven processing. Unlike recent diffusion methods for MRI reconstruction, a self-supervision strategy is adopted to train SSDiffRecon using only undersampled k-space data. Comprehensive experiments on public brain MR datasets demonstrates the superiority of SSDiffRecon against state-of-the-art supervised, and self-supervised baselines in terms of reconstruction speed and quality. Implementation will be available at https://github.com/yilmazkorkmaz1/SSDiffRecon.",
        "authors": [
            "Yilmaz Korkmaz",
            "Tolga Cukur",
            "V. Patel"
        ],
        "citations": 35,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Diffusion Models in Bioinformatics: A New Wave of Deep Learning Revolution in Action",
        "abstract": "Denoising diffusion models have emerged as one of the most powerful generative models in recent years. They have achieved remarkable success in many fields, such as computer vision, natural language processing (NLP), and bioinformatics. Although there are a few excellent reviews on diffusion models and their applications in computer vision and NLP, there is a lack of an overview of their applications in bioinformatics. This review aims to provide a rather thorough overview of the applications of diffusion models in bioinformatics to aid their further development in bioinformatics and computational biology. We start with an introduction of the key concepts and theoretical foundations of three cornerstone diffusion modeling frameworks (denoising diffusion probabilistic models, noise-conditioned scoring networks, and stochastic differential equations), followed by a comprehensive description of diffusion models employed in the different domains of bioinformatics, including cryo-EM data enhancement, single-cell data analysis, protein design and generation, drug and small molecule design, and protein-ligand interaction. The review is concluded with a summary of the potential new development and applications of diffusion models in bioinformatics.",
        "authors": [
            "Zhiye Guo",
            "Jian Liu",
            "Yanli Wang",
            "Mengrui Chen",
            "Duolin Wang",
            "Dong Xu",
            "Jianlin Cheng"
        ],
        "citations": 20,
        "references": 97,
        "year": 2023
    },
    {
        "title": "SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models",
        "abstract": "Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores on substantial benchmark datasets under a suitable number of function evaluations (NFEs).",
        "authors": [
            "Shuchen Xue",
            "Mingyang Yi",
            "Weijian Luo",
            "Shifeng Zhang",
            "Jiacheng Sun",
            "Z. Li",
            "Zhi-Ming Ma"
        ],
        "citations": 30,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Diffusion Models already have a Semantic Latent Space",
        "abstract": "Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/",
        "authors": [
            "Mingi Kwon",
            "Jaeseok Jeong",
            "Youngjung Uh"
        ],
        "citations": 203,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Diffusion Handles Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D",
        "abstract": "Diffusion Handles is a novel approach to enable 3D object edits on diffusion images, requiring only existing pre-trained diffusion models depth estimation, without any fine-tuning or 3D object retrieval. The edited results remain plausible, photo-real, and preserve object identity. Diffusion Handles address a critically missing facet of generative image-based creative design. Our key insight is to lift diffusion activations for a selected object to 3D using a proxy depth, 3D-transform the depth and associated activations, and project them back to image space. The diffusion process guided by the manipulated activations produces plausible edited images showing complex 3D occlusion and lighting effects. We evaluate Diffusion Handles: quantitatively, on a large synthetic data benchmark; and qualitatively by a user study, showing our output to be more plausible, and better than prior art at both, 3D editing and identity control.",
        "authors": [
            "Karran Pandey",
            "Paul Guerrero",
            "Matheus Gadelha",
            "Yannick Hold-Geoffroy",
            "Karan Singh",
            "Niloy J. Mitra"
        ],
        "citations": 19,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Diffusion Models for Medical Anomaly Detection",
        "abstract": "In medical applications, weakly supervised anomaly detection methods are of great interest, as only image-level annotations are required for training. Current anomaly detection methods mainly rely on generative adversarial networks or autoencoder models. Those models are often complicated to train or have difficulties to preserve fine details in the image. We present a novel weakly supervised anomaly detection method based on denoising diffusion implicit models. We combine the deterministic iterative noising and denoising scheme with classifier guidance for image-to-image translation between diseased and healthy subjects. Our method generates very detailed anomaly maps without the need for a complex training procedure. We evaluate our method on the BRATS2020 dataset for brain tumor detection and the CheXpert dataset for detecting pleural effusions.",
        "authors": [
            "Julia Wolleb",
            "Florentin Bieder",
            "Robin Sandkühler",
            "P. Cattin"
        ],
        "citations": 221,
        "references": 31,
        "year": 2022
    },
    {
        "title": "Novel View Synthesis with Diffusion Models",
        "abstract": "We present 3DiM, a diffusion model for 3D novel view synthesis, which is able to translate a single input view into consistent and sharp completions across many views. The core component of 3DiM is a pose-conditional image-to-image diffusion model, which takes a source view and its pose as inputs, and generates a novel view for a target pose as output. 3DiM can generate multiple views that are 3D consistent using a novel technique called stochastic conditioning. The output views are generated autoregressively, and during the generation of each novel view, one selects a random conditioning view from the set of available views at each denoising step. We demonstrate that stochastic conditioning significantly improves the 3D consistency of a naive sampler for an image-to-image diffusion model, which involves conditioning on a single fixed view. We compare 3DiM to prior work on the SRN ShapeNet dataset, demonstrating that 3DiM's generated completions from a single view achieve much higher fidelity, while being approximately 3D consistent. We also introduce a new evaluation methodology, 3D consistency scoring, to measure the 3D consistency of a generated object by training a neural field on the model's output views. 3DiM is geometry free, does not rely on hyper-networks or test-time optimization for novel view synthesis, and allows a single model to easily scale to a large number of scenes.",
        "authors": [
            "Daniel Watson",
            "William Chan",
            "Ricardo Martin-Brualla",
            "Jonathan Ho",
            "A. Tagliasacchi",
            "Mohammad Norouzi"
        ],
        "citations": 236,
        "references": 57,
        "year": 2022
    },
    {
        "title": "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation",
        "abstract": "We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchmarks demonstrate that MosaicFusion can significantly improve the performance of existing instance segmentation models, especially for rare and novel categories. Code: https://github.com/Jiahao000/MosaicFusion.",
        "authors": [
            "Jiahao Xie",
            "Wei Li",
            "Xiangtai Li",
            "Ziwei Liu",
            "Y. Ong",
            "Chen Change Loy"
        ],
        "citations": 29,
        "references": 103,
        "year": 2023
    },
    {
        "title": "CaloScore v2: single-shot calorimeter shower simulation with diffusion models",
        "abstract": "Diffusion generative models are promising alternatives for fast surrogate models, producing high-fidelity physics simulations. However, the generation time often requires an expensive denoising process with hundreds of function evaluations, restricting the current applicability of these models in a realistic setting. In this work, we report updates on the CaloScore architecture, detailing the changes in the diffusion process, which produces higher quality samples, and the use of progressive distillation, resulting in a diffusion model capable of generating new samples with a single function evaluation. We demonstrate these improvements using the Calorimeter Simulation Challenge 2022 dataset.",
        "authors": [
            "V. Mikuni",
            "B. Nachman"
        ],
        "citations": 28,
        "references": 65,
        "year": 2023
    },
    {
        "title": "LayerDiffusion: Layered Controlled Image Editing with Diffusion Models",
        "abstract": "Text-guided image editing has recently experienced rapid development. However, simultaneously performing multiple editing actions on a single image, such as background replacement and specific subject attribute changes, while maintaining consistency between the subject and the background remains challenging. In this paper, we propose LayerDiffusion, a semantic-based layered controlled image editing method. Our method enables non-rigid editing and attribute modification of specific subjects while preserving their unique characteristics and seamlessly integrating them into new backgrounds. We leverage a large-scale text-to-image model and employ a layered controlled optimization strategy combined with layered diffusion training. During the diffusion process, an iterative guidance strategy is used to generate a final image that aligns with the textual description. Experimental results demonstrate the effectiveness of our method in generating highly coherent images that closely align with the given textual description. The edited images maintain a high similarity to the features of the input image and surpass the performance of current leading image editing methods. LayerDiffusion opens up new possibilities for controllable image editing.",
        "authors": [
            "Pengzhi Li",
            "Qinxuan Huang",
            "Yikang Ding",
            "Zhiheng Li"
        ],
        "citations": 30,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Stochastic Segmentation with Conditional Categorical Diffusion Models",
        "abstract": "Semantic segmentation has made significant progress in recent years thanks to deep neural networks, but the common objective of generating a single segmentation output that accurately matches the image's content may not be suitable for safety-critical domains such as medical diagnostics and autonomous driving. Instead, multiple possible correct segmentation maps may be required to reflect the true distribution of annotation maps. In this context, stochastic semantic segmentation methods must learn to predict conditional distributions of labels given the image, but this is challenging due to the typically multimodal distributions, high-dimensional output spaces, and limited annotation data. To address these challenges, we propose a conditional categorical diffusion model (CCDM) for semantic segmentation based on Denoising Diffusion Probabilistic Models. Our model is conditioned to the input image, enabling it to generate multiple segmentation label maps that account for the aleatoric uncertainty arising from divergent ground truth annotations. Our experimental results show that CCDM achieves state-of-the-art performance on LIDC, a stochastic semantic segmentation dataset, and outperforms established baselines on the classical segmentation dataset Cityscapes.",
        "authors": [
            "L. Zbinden",
            "Lars Doorenbos",
            "Theodoros Pissas",
            "R. Sznitman",
            "Pablo M'arquez-Neila"
        ],
        "citations": 28,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Unsupervised Medical Image Translation With Adversarial Diffusion Models",
        "abstract": "Imputation of missing images via source-to-target modality translation can improve diversity in medical imaging protocols. A pervasive approach for synthesizing target images involves one-shot mapping through generative adversarial networks (GAN). Yet, GAN models that implicitly characterize the image distribution can suffer from limited sample fidelity. Here, we propose a novel method based on adversarial diffusion modeling, SynDiff, for improved performance in medical image translation. To capture a direct correlate of the image distribution, SynDiff leverages a conditional diffusion process that progressively maps noise and source images onto the target image. For fast and accurate image sampling during inference, large diffusion steps are taken with adversarial projections in the reverse diffusion direction. To enable training on unpaired datasets, a cycle-consistent architecture is devised with coupled diffusive and non-diffusive modules that bilaterally translate between two modalities. Extensive assessments are reported on the utility of SynDiff against competing GAN and diffusion models in multi-contrast MRI and MRI-CT translation. Our demonstrations indicate that SynDiff offers quantitatively and qualitatively superior performance against competing baselines.",
        "authors": [
            "Muzaffer Ozbey",
            "S. Dar",
            "H. Bedel",
            "Onat Dalmaz",
            "cSaban Ozturk",
            "Alper Gungor",
            "Tolga cCukur"
        ],
        "citations": 211,
        "references": 86,
        "year": 2022
    },
    {
        "title": "WaveDM: Wavelet-Based Diffusion Models for Image Restoration",
        "abstract": "Latest diffusion-based methods for many image restoration tasks outperform traditional models, but they encounter the long-time inference problem. To tackle it, this paper proposes a Wavelet-Based Diffusion Model (WaveDM). WaveDM learns the distribution of clean images in the wavelet domain conditioned on the wavelet spectrum of degraded images after wavelet transform, which is more time-saving in each step of sampling than modeling in the spatial domain. To ensure restoration performance, a unique training strategy is proposed where the low-frequency and high-frequency spectrums are learned using distinct modules. In addition, an Efficient Conditional Sampling (ECS) strategy is developed from experiments, which reduces the number of total sampling steps to around 5. Evaluations on twelve benchmark datasets including image raindrop removal, rain steaks removal, dehazing, defocus deblurring, demoiréing, and denoising demonstrate that WaveDM achieves state-of-the-art performance with the efficiency that is comparable to traditional one-pass methods and over 100× faster than existing image restoration methods using vanilla diffusion models.",
        "authors": [
            "Yi Huang",
            "Jiancheng Huang",
            "Jianzhuang Liu",
            "Mingfu Yan",
            "Yu Dong",
            "Jiaxi Lv",
            "Shifeng Chen"
        ],
        "citations": 26,
        "references": 132,
        "year": 2023
    },
    {
        "title": "Training Data Attribution for Diffusion Models",
        "abstract": "Diffusion models have become increasingly popular for synthesizing high-quality samples based on training datasets. However, given the oftentimes enormous sizes of the training datasets, it is difficult to assess how training data impact the samples produced by a trained diffusion model. The difficulty of relating diffusion model inputs and outputs poses significant challenges to model explainability and training data attribution. Here we propose a novel solution that reveals how training data influence the output of diffusion models through the use of ensembles. In our approach individual models in an encoded ensemble are trained on carefully engineered splits of the overall training data to permit the identification of influential training examples. The resulting model ensembles enable efficient ablation of training data influence, allowing us to assess the impact of training data on model outputs. We demonstrate the viability of these ensembles as generative models and the validity of our approach to assessing influence.",
        "authors": [
            "Zheng Dai",
            "D. Gifford"
        ],
        "citations": 17,
        "references": 26,
        "year": 2023
    },
    {
        "title": "On the Robustness of Latent Diffusion Models",
        "abstract": "Latent diffusion models achieve state-of-the-art performance on a variety of generative tasks, such as image synthesis and image editing. However, the robustness of latent diffusion models is not well studied. Previous works only focus on the adversarial attacks against the encoder or the output image under white-box settings, regardless of the denoising process. Therefore, in this paper, we aim to analyze the robustness of latent diffusion models more thoroughly. We first study the influence of the components inside latent diffusion models on their white-box robustness. In addition to white-box scenarios, we evaluate the black-box robustness of latent diffusion models via transfer attacks, where we consider both prompt-transfer and model-transfer settings and possible defense mechanisms. However, all these explorations need a comprehensive benchmark dataset, which is missing in the literature. Therefore, to facilitate the research of the robustness of latent diffusion models, we propose two automatic dataset construction pipelines for two kinds of image editing models and release the whole dataset. Our code and dataset are available at \\url{https://github.com/jpzhang1810/LDM-Robustness}.",
        "authors": [
            "Jianping Zhang",
            "Zhuoer Xu",
            "Shiwen Cui",
            "Changhua Meng",
            "Weibin Wu",
            "Michael R. Lyu"
        ],
        "citations": 15,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Protecting the Intellectual Property of Diffusion Models by the Watermark Diffusion Process",
        "abstract": "Diffusion models have emerged as state-of-the-art deep generative architectures with the increasing demands for generation tasks. Training large diffusion models for good performance requires high resource costs, making them valuable intellectual properties to protect. While most of the existing ownership solutions, including watermarking, mainly focus on discriminative models. This paper proposes WDM , a novel watermarking method for diffusion models, including watermark embedding, extraction, and verification. WDM embeds the watermark data through training or fine-tuning the diffusion model to learn a Watermark Diffusion Process (WDP) , different from the standard diffusion process for the task data. The embedded watermark can be extracted by sampling using the shared reverse noise from the learned WDP without degrading performance on the original task. We also provide theoretical foundations and analysis of the proposed method by connecting the WDP to the diffusion process with a modified Gaussian kernel. Extensive experiments are conducted to demonstrate its effectiveness and robustness against various attacks.",
        "authors": [
            "Sen Peng",
            "Yufei Chen",
            "Cong Wang",
            "Xiaohua Jia"
        ],
        "citations": 16,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
        "abstract": "Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with highfidelity and diverse contents. Despite this advancement, latent space smoothness within diffusion models remains largely unexplored. Smooth latent spaces en-sure that a perturbation on an input latent corresponds to a steady change in the output image. This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing. In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations. To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth. Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step. In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model. Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks. Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models. Code is available at https://github.com/SHI-Labs/Smooth-Diffusion.",
        "authors": [
            "Jiayi Guo",
            "Xingqian Xu",
            "Yifan Pu",
            "Zanlin Ni",
            "Chaofei Wang",
            "Manushree Vasu",
            "Shiji Song",
            "Gao Huang",
            "Humphrey Shi"
        ],
        "citations": 16,
        "references": 97,
        "year": 2023
    },
    {
        "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers",
        "abstract": "We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: learning in discrete or continuous time, the objective function, the interpolant that connects the distributions, and deterministic or stochastic sampling. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 and 512x512 benchmark using the exact same model structure, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06 and 2.62, respectively.",
        "authors": [
            "Nanye Ma",
            "Mark Goldstein",
            "M. S. Albergo",
            "Nicholas M. Boffi",
            "Eric Vanden-Eijnden",
            "Saining Xie"
        ],
        "citations": 86,
        "references": 73,
        "year": 2024
    },
    {
        "title": "A Geometric Perspective on Diffusion Models",
        "abstract": "Recent years have witnessed significant progress in developing effective training and fast sampling techniques for diffusion models. A remarkable advancement is the use of stochastic differential equations (SDEs) and their marginal-preserving ordinary differential equations (ODEs) to describe data perturbation and generative modeling in a unified framework. In this paper, we carefully inspect the ODE-based sampling of a popular variance-exploding SDE and reveal several intriguing structures of its sampling dynamics. We discover that the data distribution and the noise distribution are smoothly connected with a quasi-linear sampling trajectory and another implicit denoising trajectory that even converges faster. Meanwhile, the denoising trajectory governs the curvature of the corresponding sampling trajectory and its finite differences yield various second-order samplers used in practice. Furthermore, we establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with which we can characterize the asymptotic behavior of diffusion models and identify the empirical score deviation. Code is available at \\url{https://github.com/zju-pi/diff-sampler}.",
        "authors": [
            "Defang Chen",
            "Zhenyu Zhou",
            "Jianhan Mei",
            "Chunhua Shen",
            "Chun Chen",
            "C. Wang"
        ],
        "citations": 15,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Residual Denoising Diffusion Models",
        "abstract": "We propose residual denoising diffusion models (RDDM), a novel dual diffusion process that decouples the traditional single denoising diffusion process into residual diffusion and noise diffusion. This dual diffusion framework expands the denoising-based diffusion models, initially uninterpretable for image restoration, into a unified and interpretable model for both image generation and restoration by introducing residuals. Specifically, our residual diffusion represents directional diffusion from the target image to the degraded input image and explicitly guides the reverse generation process for image restoration, while noise diffusion represents random perturbations in the diffusion process. The residual prioritizes certainty, while the noise emphasizes diversity, enabling RDDM to effectively unify tasks with varying certainty or diversity requirements, such as image generation and restoration. We demonstrate that our sampling process is consistent with that of DDPM and DDIM through coefficient transformation, and propose a partially path-independent generation process to better understand the reverse process. Notably, our RDDM enables a generic UNet, trained with only an L1 loss and a batch size of 1, to compete with state-of-the-art image restoration methods. We provide code and pre-trained models to encourage further exploration, application, and development of our innovative framework (https://github.com/nachifurlRDDM).",
        "authors": [
            "Jiawei Liu",
            "Qiang Wang",
            "Huijie Fan",
            "Yinong Wang",
            "Yandong Tang",
            "Liangqiong Qu"
        ],
        "citations": 14,
        "references": 115,
        "year": 2023
    },
    {
        "title": "Diffusion models in medical imaging: A comprehensive survey",
        "abstract": null,
        "authors": [
            "A. Kazerouni",
            "Ehsan Khodapanah Aghdam",
            "Moein Heidari",
            "Reza Azad",
            "Mohsen Fayyaz",
            "I. Hacihaliloglu",
            "D. Merhof"
        ],
        "citations": 256,
        "references": 191,
        "year": 2022
    },
    {
        "title": "Diffusion models as plug-and-play priors",
        "abstract": "We consider the problem of inferring high-dimensional data $\\mathbf{x}$ in a model that consists of a prior $p(\\mathbf{x})$ and an auxiliary differentiable constraint $c(\\mathbf{x},\\mathbf{y})$ on $x$ given some additional information $\\mathbf{y}$. In this paper, the prior is an independently trained denoising diffusion generative model. The auxiliary constraint is expected to have a differentiable form, but can come from diverse sources. The possibility of such inference turns diffusion models into plug-and-play modules, thereby allowing a range of potential applications in adapting models to new domains and tasks, such as conditional generation or image segmentation. The structure of diffusion models allows us to perform approximate inference by iterating differentiation through the fixed denoising network enriched with different amounts of noise at each step. Considering many noised versions of $\\mathbf{x}$ in evaluation of its fitness is a novel search mechanism that may lead to new algorithms for solving combinatorial optimization problems.",
        "authors": [
            "Alexandros Graikos",
            "Nikolay Malkin",
            "N. Jojic",
            "D. Samaras"
        ],
        "citations": 183,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality",
        "abstract": "Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We also present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required.",
        "authors": [
            "Daniel Watson",
            "William Chan",
            "Jonathan Ho",
            "Mohammad Norouzi"
        ],
        "citations": 160,
        "references": 41,
        "year": 2022
    },
    {
        "title": "MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation",
        "abstract": "We propose the first joint audio-video generation framework that brings engaging watching and listening experiences simultaneously, towards high-quality realistic videos. To generate joint audio-video pairs, we propose a novel Multi-Modal Diffusion model (i.e., MM-Diffusion), with two-coupled denoising autoencoders. In contrast to existing single-modal diffusion models, MM-Diffusion consists of a sequential multi-modal U-Net for a joint denoising process by design. Two subnets for audio and video learn to gradually generate aligned audio-video pairs from Gaussian noises. To ensure semantic consistency across modalities, we propose a novel random-shift based attention block bridging over the two subnets, which enables efficient cross-modal alignment, and thus reinforces the audio-video fidelity for each other. Extensive experiments show superior results in unconditional audio-video generation, and zeroshot conditional tasks (e.g., video-to-audio). In particular, we achieve the best FVD and FAD on Landscape and AIST++ dancing datasets. Turing tests of 10k votes further demonstrate dominant preferences for our model. The code and pre-trained models can be downloaded at https://github.com/researchmm/MM-Diffusion.",
        "authors": [
            "Ludan Ruan",
            "Y. Ma",
            "Huan Yang",
            "Huiguo He",
            "Bei Liu",
            "Jianlong Fu",
            "Nicholas Jing Yuan",
            "Qin Jin",
            "B. Guo"
        ],
        "citations": 126,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Latent Video Diffusion Models for High-Fidelity Long Video Generation",
        "abstract": "AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models have shown remarkable results recently but require significant computational resources. To address this, we introduce lightweight video diffusion models by leveraging a low-dimensional 3D latent space, significantly outperforming previous pixel-space video diffusion models under a limited computational budget. In addition, we propose hierarchical diffusion in the latent space such that longer videos with more than one thousand frames can be produced. To further overcome the performance degradation issue for long video generation, we propose conditional latent perturbation and unconditional guidance that effectively mitigate the accumulated errors during the extension of video length. Extensive experiments on small domain datasets of different categories suggest that our framework generates more realistic and longer videos than previous strong baselines. We additionally provide an extension to large-scale text-to-video generation to demonstrate the superiority of our work. Our code and models will be made publicly available.",
        "authors": [
            "Yin-Yin He",
            "Tianyu Yang",
            "Yong Zhang",
            "Ying Shan",
            "Qifeng Chen"
        ],
        "citations": 126,
        "references": 47,
        "year": 2022
    },
    {
        "title": "SINE: SINgle Image Editing with Text-to-Image Diffusion Models",
        "abstract": "Recent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion models for tackling a challenging problem-real image editing. Works conducted in this area learn a unique textual token corresponding to several images containing the same object. However, under many circumstances, only one image is available, such as the painting of the Girl with a Pearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new features depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffusion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbitrary resolution. We provide extensive experiments to validate the design choices of our approach and show promising editing capabilities, including changing style, content addition, and object manipulation. Our code is made publicly available here.",
        "authors": [
            "Zhixing Zhang",
            "Ligong Han",
            "Arna Ghosh",
            "Dimitris N. Metaxas",
            "Jian Ren"
        ],
        "citations": 134,
        "references": 65,
        "year": 2022
    },
    {
        "title": "Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models",
        "abstract": "Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-expert ensembles of diffusion models and demonstrate how these may be used for, e.g., style interpolation, a contribution we believe is of independent interest.",
        "authors": [
            "Simon Alexanderson",
            "Rajmund Nagy",
            "J. Beskow",
            "G. Henter"
        ],
        "citations": 129,
        "references": 168,
        "year": 2022
    },
    {
        "title": "Convergence of denoising diffusion models under the manifold hypothesis",
        "abstract": "Denoising diffusion models are a recent class of generative models exhibiting state-of-the-art performance in image and audio synthesis. Such models approximate the time-reversal of a forward noising process from a target distribution to a reference density, which is usually Gaussian. Despite their strong empirical results, the theoretical analysis of such models remains limited. In particular, all current approaches crucially assume that the target density admits a density w.r.t. the Lebesgue measure. This does not cover settings where the target distribution is supported on a lower-dimensional manifold or is given by some empirical distribution. In this paper, we bridge this gap by providing the first convergence results for diffusion models in this more general setting. In particular, we provide quantitative bounds on the Wasserstein distance of order one between the target data distribution and the generative distribution of the diffusion model.",
        "authors": [
            "Valentin De Bortoli"
        ],
        "citations": 135,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Renormalizing Diffusion Models",
        "abstract": "We explain how to use diffusion models to learn inverse renormalization group flows of statistical and quantum field theories. Diffusion models are a class of machine learning models which have been used to generate samples from complex distributions, such as the distribution of natural images. These models achieve sample generation by learning the inverse process to a diffusion process which adds noise to the data until the distribution of the data is pure noise. Nonperturbative renormalization group schemes in physics can naturally be written as diffusion processes in the space of fields. We combine these observations in a concrete framework for building ML-based models for studying field theories, in which the models learn the inverse process to an explicitly-specified renormalization group scheme. We detail how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory. Because renormalization group schemes have a physical meaning, we provide explicit prescriptions for how to compare results derived from models associated to several different renormalization group schemes of interest. We also explain how to use diffusion models in a variational method to find ground states of quantum systems. We apply some of our methods to numerically find RG flows of interacting statistical field theories. From the perspective of machine learning, our work provides an interpretation of multiscale diffusion models, and gives physically-inspired suggestions for diffusion models which should have novel properties.",
        "authors": [
            "Jordan S. Cotler",
            "Semon Rezchikov"
        ],
        "citations": 10,
        "references": 140,
        "year": 2023
    },
    {
        "title": "Boosting GUI Prototyping with Diffusion Models",
        "abstract": "GUI (graphical user interface) prototyping is a widely-used technique in requirements engineering for gathering and refining requirements, reducing development risks and increasing stakeholder engagement. However, GUI prototyping can be a time-consuming and costly process. In recent years, deep learning models such as Stable Diffusion have emerged as a powerful text-to-image tool capable of generating detailed images based on text prompts. In this paper, we propose UI-Diffuser, an approach that leverages Stable Diffusion to generate mobile UIs through simple textual descriptions and UI components. Preliminary results show that UI-Diffuser provides an efficient and cost-effective way to generate mobile GUI designs while reducing the need for extensive prototyping efforts. This approach has the potential to significantly improve the speed and efficiency of GUI prototyping in requirements engineering.",
        "authors": [
            "Jialiang Wei",
            "A. Courbis",
            "Thomas Lambolais",
            "Binbin Xu",
            "P. Bernard",
            "Gérard Dray"
        ],
        "citations": 19,
        "references": 44,
        "year": 2023
    },
    {
        "title": "A Survey on Generative Diffusion Models",
        "abstract": "Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artificial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.",
        "authors": [
            "Hanqun Cao",
            "Cheng Tan",
            "Zhangyang Gao",
            "Yilun Xu",
            "Guangyong Chen",
            "P. Heng",
            "Stan Z. Li"
        ],
        "citations": 122,
        "references": 333,
        "year": 2022
    },
    {
        "title": "Diffusion Models for Video Prediction and Infilling",
        "abstract": "Predicting and anticipating future outcomes or reasoning about missing information in a sequence are critical skills for agents to be able to make intelligent decisions. This requires strong, temporally coherent generative capabilities. Diffusion models have shown remarkable success in several generative tasks, but have not been extensively explored in the video domain. We present Random-Mask Video Diffusion (RaMViD), which extends image diffusion models to videos using 3D convolutions, and introduces a new conditioning technique during training. By varying the mask we condition on, the model is able to perform video prediction, infilling, and upsampling. Due to our simple conditioning scheme, we can utilize the same architecture as used for unconditional training, which allows us to train the model in a conditional and unconditional fashion at the same time. We evaluate RaMViD on two benchmark datasets for video prediction, on which we achieve state-of-the-art results, and one for video generation. High-resolution videos are provided at https://sites.google.com/view/video-diffusion-prediction.",
        "authors": [
            "Tobias Hoppe",
            "Arash Mehrjou",
            "Stefan Bauer",
            "Didrik Nielsen",
            "Andrea Dittadi"
        ],
        "citations": 119,
        "references": 66,
        "year": 2022
    },
    {
        "title": "Retrieval-Augmented Diffusion Models",
        "abstract": "Generative image synthesis with diffusion models has recently achieved excellent visual quality in several tasks such as text-based or class-conditional image synthesis. Much of this success is due to a dramatic increase in the computational capacity invested in training these models. This work presents an alternative approach: inspired by its successful application in natural language processing, we propose to complement the diffusion model with a retrieval-based approach and to introduce an explicit memory in the form of an external database. During training, our diffusion model is trained with similar visual features retrieved via CLIP and from the neighborhood of each training instance. By leveraging CLIP’s joint image-text embedding space, our model achieves highly competitive performance on tasks for which it has not been explicitly trained, such as class-conditional or text-image synthesis, and can be conditioned on both text and image embeddings. Moreover, we can apply our approach to unconditional generation, where it achieves state-of-the-art performance. Our approach incurs low computational and memory overheads and is easy to implement. We discuss its relationship to concurrent work and will publish code and pretrained models soon.",
        "authors": [
            "A. Blattmann",
            "Robin Rombach",
            "K. Oktay",
            "B. Ommer"
        ],
        "citations": 119,
        "references": 105,
        "year": 2022
    },
    {
        "title": "DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics",
        "abstract": "We introduce the first work to explore web-scale diffusion models for robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first inferring a text description of those objects, then generating an image representing a natural, human-like arrangement of those objects, and finally physically arranging the objects according to that goal image. We show that this is possible zero-shot using DALL-E, without needing any further example arrangements, data collection, or training. DALL-E-Bot is fully autonomous and is not restricted to a pre-defined set of objects or scenes, thanks to DALL-E's web-scale pre-training. Encouraging real-world results, with both human studies and objective metrics, show that integrating web-scale diffusion models into robotics pipelines is a promising direction for scalable, unsupervised robot learning.",
        "authors": [
            "Ivan Kapelyukh",
            "Vitalis Vosylius",
            "Edward Johns"
        ],
        "citations": 116,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Fast Sampling of Diffusion Models via Operator Learning",
        "abstract": "Diffusion models have found widespread adoption in various areas. However, their sampling process is slow because it requires hundreds to thousands of network evaluations to emulate a continuous process defined by differential equations. In this work, we use neural operators, an efficient method to solve the probability flow differential equations, to accelerate the sampling process of diffusion models. Compared to other fast sampling methods that have a sequential nature, we are the first to propose a parallel decoding method that generates images with only one model forward pass. We propose diffusion model sampling with neural operator (DSNO) that maps the initial condition, i.e., Gaussian distribution, to the continuous-time solution trajectory of the reverse diffusion process. To model the temporal correlations along the trajectory, we introduce temporal convolution layers that are parameterized in the Fourier space into the given diffusion model backbone. We show our method achieves state-of-the-art FID of 3.78 for CIFAR-10 and 7.83 for ImageNet-64 in the one-model-evaluation setting.",
        "authors": [
            "Hongkai Zheng",
            "Weili Nie",
            "Arash Vahdat",
            "K. Azizzadenesheli",
            "Anima Anandkumar"
        ],
        "citations": 115,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Post-Training Quantization on Diffusion Models",
        "abstract": "Denoising diffusion (score-based) generative models have recently achieved significant accomplishments in generating realistic and diverse data. Unfortunately, the generation process of current denoising diffusion models is notoriously slow due to the lengthy iterative noise estimations, which rely on cumbersome neural networks. It prevents the diffusion models from being widely deployed, especially on edge devices. Previous works accelerate the generation process of diffusion model (DM) via finding shorter yet effective sampling trajectories. However, they overlook the cost of noise estimation with a heavy network in every iteration. In this work, we accelerate generation from the perspective of compressing the noise estimation network. Due to the difficulty of retraining DMs, we exclude mainstream training-aware compression paradigms and introduce post-training quantization (PTQ) into DM acceleration. However, the output distributions of noise estimation networks change with time-step, making previous PTQ methods fail in DMs since they are designed for single-time step scenarios. To devise a DM-specific PTQ method, we explore PTQ on DM in three aspects: quantized operations, calibration dataset, and calibration metric. We summarize and use several observations derived from all-inclusive investigations to formulate our method, which especially targets the unique multi-time-step structure of DMs. Experimentally, our method can directly quantize full-precision DMs into 8-bit models while maintaining or even improving their performance in a training-free manner. Importantly, our method can serve as a plug-and-play module on other fast-sampling methods, e.g., DDIM [24]. The code is available at https://https://github.com/42Shawn/PTQ4DM.",
        "authors": [
            "Yuzhang Shang",
            "Zhihang Yuan",
            "Bin Xie",
            "Bingzhe Wu",
            "Yan Yan"
        ],
        "citations": 109,
        "references": 47,
        "year": 2022
    },
    {
        "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
        "abstract": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines.",
        "authors": [
            "Zhengfu He",
            "Tianxiang Sun",
            "Kuan Wang",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "citations": 93,
        "references": 42,
        "year": 2022
    },
    {
        "title": "BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models",
        "abstract": "Image-to-image translation is an important and challenging problem in computer vision and image processing. Diffusion models (DM) have shown great potentials for high-quality image synthesis, and have gained competitive performance on the task of image-to-image translation. However, most of the existing diffusion models treat image-to-image translation as conditional generation processes, and suffer heavily from the gap between distinct domains. In this paper, a novel image-to-image translation method based on the Brownian Bridge Diffusion Model (BBDM) is proposed, which models image-to-image translation as a stochastic Brownian Bridge process, and learns the translation between two domains directly through the bidirectional diffusion process rather than a conditional generation process. To the best of our knowledge, it is the first work that proposes Brownian Bridge diffusion process for image-to-image translation. Experimental results on various benchmarks demonstrate that the proposed BBDM model achieves competitive performance through both visual inspection and measurable metrics.",
        "authors": [
            "Bo Li",
            "Kaitao Xue",
            "Bin Liu",
            "Yunyu Lai"
        ],
        "citations": 97,
        "references": 47,
        "year": 2022
    },
    {
        "title": "Inversion-based Style Transfer with Diffusion Models",
        "abstract": "The artistic style within a painting is the means of expression, which includes not only the painting material, colors, and brushstrokes, but also the high-level attributes, including semantic elements and object shapes. Previous arbitrary example-guided artistic image generation methods often fail to control shape changes or convey elements. Pre-trained text-to-image synthesis diffusion probabilistic models have achieved remarkable quality but often require extensive textual descriptions to accurately portray the attributes of a particular painting. The uniqueness of an artwork lies in the fact that it cannot be adequately explained with normal language. Our key idea is to learn the artistic style directly from a single painting and then guide the synthesis without providing complex textual descriptions. Specifically, we perceive style as a learnable textual description of a painting. We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. We demonstrate the quality and efficiency of our method on numerous paintings of various artists and styles. Codes are available at https://github.com/zyxElsa/InST.",
        "authors": [
            "Yu-xin Zhang",
            "Nisha Huang",
            "Fan Tang",
            "Haibin Huang",
            "Chongyang Ma",
            "Weiming Dong",
            "Changsheng Xu"
        ],
        "citations": 187,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models",
        "abstract": "Deep generative models have emerged as promising tools for detecting arbitrary anomalies in data, dispensing with the necessity for manual labelling. Recently, autoregressive transformers have achieved state-of-the-art performance for anomaly detection in medical imaging. Nonetheless, these models still have some intrinsic weaknesses, such as requiring images to be modelled as 1D sequences, the accumulation of errors during the sampling process, and the significant inference times associated with transformers. Denoising diffusion probabilistic models are a class of non-autoregressive generative models recently shown to produce excellent samples in computer vision (surpassing Generative Adversarial Networks), and to achieve log-likelihoods that are competitive with transformers while having fast inference times. Diffusion models can be applied to the latent representations learnt by autoencoders, making them easily scalable and great candidates for application to high dimensional data, such as medical images. Here, we propose a method based on diffusion models to detect and segment anomalies in brain imaging. By training the models on healthy data and then exploring its diffusion and reverse steps across its Markov chain, we can identify anomalous areas in the latent space and hence identify anomalies in the pixel space. Our diffusion models achieve competitive performance compared with autoregressive approaches across a series of experiments with 2D CT and MRI data involving synthetic and real pathological lesions with much reduced inference times, making their usage clinically viable.",
        "authors": [
            "W. H. Pinaya",
            "M. Graham",
            "Robert J. Gray",
            "P. F. D. Costa",
            "Petru-Daniel Tudosiu",
            "P. Wright",
            "Y. Mah",
            "A. MacKinnon",
            "J. Teo",
            "H. Jäger",
            "D. Werring",
            "Geraint Rees",
            "P. Nachev",
            "S. Ourselin",
            "M. Cardoso"
        ],
        "citations": 92,
        "references": 32,
        "year": 2022
    },
    {
        "title": "Semantic Image Synthesis via Diffusion Models",
        "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in various image generation tasks compared with Generative Adversarial Nets (GANs). Recent work on semantic image synthesis mainly follows the \\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality or diversity of generated images. In this paper, we propose a novel framework based on DDPM for semantic image synthesis. Unlike previous conditional diffusion model directly feeds the semantic layout and noisy image as input to a U-Net structure, which may not fully leverage the information in the input semantic mask, our framework processes semantic layout and noisy image differently. It feeds noisy image to the encoder of the U-Net structure while the semantic layout to the decoder by multi-layer spatially-adaptive normalization operators. To further improve the generation quality and semantic interpretability in semantic image synthesis, we introduce the classifier-free guidance sampling strategy, which acknowledge the scores of an unconditional model for sampling process. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed method, achieving state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS).",
        "authors": [
            "Weilun Wang",
            "Jianmin Bao",
            "Wen-gang Zhou",
            "Dongdong Chen",
            "Dong Chen",
            "Lu Yuan",
            "Houqiang Li"
        ],
        "citations": 157,
        "references": 62,
        "year": 2022
    },
    {
        "title": "Structure-based drug design with equivariant diffusion models",
        "abstract": null,
        "authors": [
            "Arne Schneuing",
            "Yuanqi Du",
            "Charles Harris",
            "Arian R. Jamasb",
            "Ilia Igashov",
            "Weitao Du",
            "T. Blundell",
            "Pietro Li'o",
            "Carla P. Gomes",
            "Max Welling",
            "Michael M. Bronstein",
            "B. Correia"
        ],
        "citations": 151,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Sketch-Guided Text-to-Image Diffusion Models",
        "abstract": "Text-to-Image models have introduced a remarkable leap in the evolution of machine learning, demonstrating high-quality synthesis of images from a given text-prompt. However, these powerful pretrained models still lack control handles that can guide spatial properties of the synthesized images. In this work, we introduce a universal approach to guide a pretrained text-to-image diffusion model, with a spatial map from another domain (e.g., sketch) during inference time. Unlike previous works, our method does not require to train a dedicated model or a specialized encoder for the task. Our key idea is to train a Latent Guidance Predictor (LGP) - a small, per-pixel, Multi-Layer Perceptron (MLP) that maps latent features of noisy images to spatial maps, where the deep features are extracted from the core Denoising Diffusion Probabilistic Model (DDPM) network. The LGP is trained only on a few thousand images and constitutes a differential guiding map predictor, over which the loss is computed and propagated back to push the intermediate images to agree with the spatial map. The per-pixel training offers flexibility and locality which allows the technique to perform well on out-of-domain sketches, including free-hand style drawings. We take a particular focus on the sketch-to-image translation task, revealing a robust and expressive way to generate images that follow the guidance of a sketch of arbitrary style or domain.",
        "authors": [
            "A. Voynov",
            "Kfir Aberman",
            "D. Cohen-Or"
        ],
        "citations": 180,
        "references": 43,
        "year": 2022
    },
    {
        "title": "Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models",
        "abstract": "Generative models have been widely studied in computer vision. Recently, diffusion models have drawn substantial attention due to the high quality of their generated images. A key desired property of image generative models is the ability to disentangle different attributes, which should enable modification towards a style without changing the semantic content, and the modification parameters should generalize to different images. Previous studies have found that generative adversarial networks (GANs) are inherently endowed with such disentanglement capability, so they can perform disentangled image editing without re-training or fine-tuning the network. In this work, we explore whether diffusion models are also inherently equipped with such a capability. Our finding is that for stable diffusion models, by partially changing the input text embedding from a neutral description (e.g., “a photo of person”) to one with style (e.g., “a photo of person with smile”) while fixing all the Gaussian random noises introduced during the denoising process, the generated images can be modified towards the target style without changing the semantic content. Based on this finding, we further propose a simple, light-weight image editing algorithm where the mixing weights of the two text embeddings are optimized for style matching and content preservation. This entire process only involves optimizing over around 50 parameters and does not fine-tune the diffusion model itself. Experiments show that the proposed method can modify a wide range of attributes, with the performance outperforming diffusion-model-based image-editing algorithms that require fine-tuning. The optimized weights generalize well to different images. Our code is publicly available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.",
        "authors": [
            "Qiucheng Wu",
            "Yujian Liu",
            "Handong Zhao",
            "Ajinkya Kale",
            "T. Bui",
            "Tong Yu",
            "Zhe Lin",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "citations": 80,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models",
        "abstract": "Diffusion models have emerged as the new state-of-the-art generative model with high quality samples, with intriguing properties such as mode coverage and high flexibility. They have also been shown to be effective inverse problem solvers, acting as the prior of the distribution, while the information of the forward model can be granted at the sampling stage. Nonetheless, as the generative process remains in the same high dimensional (i.e. identical to data dimension) space, the models have not been extended to 3D inverse problems due to the extremely high memory and computational cost. In this paper, we combine the ideas from the conventional model-based iterative reconstruction with the modern diffusion models, which leads to a highly effective method for solving 3D medical image reconstruction tasks such as sparse-view tomography, limited angle tomography, compressed sensing MRI from pre-trained 2D diffusion models. In essence, we propose to augment the 2D diffusion prior with a model-based prior in the remaining direction at test time, such that one can achieve coherent reconstructions across all dimensions. Our method can be run in a single commodity GPU, and establishes the new state-of-the-art, showing that the proposed method can perform reconstructions of high fidelity and accuracy even in the most extreme cases (e.g. 2-view 3D tomography). We further reveal that the generalization capacity of the proposed method is surprisingly high, and can be used to reconstruct volumes that are entirely different from the training dataset. Code available: https://github.com/HJ-harry/DiffusionMBIR",
        "authors": [
            "Hyungjin Chung",
            "Dohoon Ryu",
            "Michael T. McCann",
            "M. Klasky",
            "J. C. Ye"
        ],
        "citations": 83,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Diffusion Models for Medical Image Analysis: A Comprehensive Survey",
        "abstract": "provide a systematic taxonomy of diffusion models in the medical domain and propose a multi-perspective categorization based on their application, imaging modality, organ of interest, and algorithms. To this end, we cover extensive applications of diffusion models in the medical domain, including segmentation, anomaly detection, image-to-image translation, 2/3D generation, reconstruction, denoising, and other medically-related challenges. Furthermore, we emphasize the practical use case of some selected approaches, and then we discuss the limitations of the diffusion models in the medical domain and propose several directions to fulﬁll the demands of this ﬁeld. Finally, we gather the overviewed studies with their available open-source implementations at our GitHub 1 . We aim to update the relevant latest papers within it regularly.",
        "authors": [
            "A. Kazerouni",
            "Ehsan Khodapanah Aghdam",
            "Moein Heidari",
            "Reza Azad",
            "Mohsen Fayyaz",
            "I. Hacihaliloglu",
            "D. Merhof"
        ],
        "citations": 104,
        "references": 142,
        "year": 2022
    },
    {
        "title": "Dehazing Ultrasound Using Diffusion Models",
        "abstract": "Echocardiography has been a prominent tool for the diagnosis of cardiac disease. However, these diagnoses can be heavily impeded by poor image quality. Acoustic clutter emerges due to multipath reflections imposed by layers of skin, subcutaneous fat, and intercostal muscle between the transducer and heart. As a result, haze and other noise artifacts pose a real challenge to cardiac ultrasound imaging. In many cases, especially with difficult-to-image patients such as patients with obesity, a diagnosis from B-Mode ultrasound imaging is effectively rendered unusable, forcing sonographers to resort to contrast-enhanced ultrasound examinations or refer patients to other imaging modalities. Tissue harmonic imaging has been a popular approach to combat haze, but in severe cases is still heavily impacted by haze. Alternatively, denoising algorithms are typically unable to remove highly structured and correlated noise, such as haze. It remains a challenge to accurately describe the statistical properties of structured haze, and develop an inference method to subsequently remove it. Diffusion models have emerged as powerful generative models and have shown their effectiveness in a variety of inverse problems. In this work, we present a joint posterior sampling framework that combines two separate diffusion models to model the distribution of both clean ultrasound and haze in an unsupervised manner. Furthermore, we demonstrate techniques for effectively training diffusion models on radio-frequency ultrasound data and highlight the advantages over image data. Experiments on both in-vitro and in-vivo cardiac datasets show that the proposed dehazing method effectively removes haze while preserving signals from weakly reflected tissue.",
        "authors": [
            "Tristan S. W. Stevens",
            "F. C. Meral",
            "Jason J. Yu",
            "I. Apostolakis",
            "J. Robert",
            "Ruud J. G. van Sloun"
        ],
        "citations": 17,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models",
        "abstract": "Recent CLIP-guided 3D optimization methods, such as DreamFields [19] and PureCLIPNeRF [24], have achieved impressive results in zero-shot text-to-3D synthesis. However, due to scratch training and random initialization without prior knowledge, these methods often fail to generate accurate and faithful 3D structures that conform to the input text. In this paper, we make the first attempt to introduce explicit 3D shape priors into the CLIP-guided 3D optimization process. Specifically, we first generate a high-quality 3D shape from the input text in the text-to-shape stage as a 3D shape prior. We then use it as the initialization of a neural radiance field and optimize it with the full prompt. To address the challenging text-to-shape generation task, we present a simple yet effective approach that directly bridges the text and image modalities with a powerful text-to-image diffusion model. To narrow the style domain gap between the images synthesized by the text-to-image diffusion model and shape renderings used to train the image-to-shape generator, we further propose to jointly optimize a learnable text prompt and fine-tune the text-to-image diffusion model for rendering-style image generation. Our method, Dream3D, is capable of generating imaginative 3D content with superior visual quality and shape accuracy compared to state-of-the-art methods. Our project page is at https://bluestyle97.github.io/dream3d/.",
        "authors": [
            "Jiale Xu",
            "Xintao Wang",
            "Weihao Cheng",
            "Yan-Pei Cao",
            "Ying Shan",
            "Xiaohu Qie",
            "Shenghua Gao"
        ],
        "citations": 150,
        "references": 92,
        "year": 2022
    },
    {
        "title": "Improving Sample Quality of Diffusion Models Using Self-Attention Guidance",
        "abstract": "Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity. This success is largely attributed to the use of class- or text-conditional diffusion guidance methods, such as classifier and classifier-free guidance. In this paper, we present a more comprehensive perspective that goes beyond the traditional guidance methods. From this generalized perspective, we introduce novel condition- and training-free strategies to enhance the quality of generated images. As a simple solution, blur guidance improves the suitability of intermediate samples for their fine-scale information and structures, enabling diffusion models to generate higher quality samples with a moderate guidance scale. Improving upon this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy. Specifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteration and guides them accordingly. Our experimental re sults show that our SAG improves the performance of various diffusion models, including ADM, IDDPM, Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidance methods leads to further improvement.",
        "authors": [
            "Susung Hong",
            "Gyuseong Lee",
            "Wooseok Jang",
            "Seung Wook Kim"
        ],
        "citations": 75,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Differentially Private Diffusion Models",
        "abstract": "While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been demonstrated before for DP generative models. Project page and code: https://nv-tlabs.github.io/DPDM.",
        "authors": [
            "Tim Dockhorn",
            "Tianshi Cao",
            "Arash Vahdat",
            "Karsten Kreis"
        ],
        "citations": 76,
        "references": 121,
        "year": 2022
    },
    {
        "title": "All are Worth Words: a ViT Backbone for Score-based Diffusion Models",
        "abstract": "Vision transformers (ViT) have shown promise in various vision tasks including low-level ones while the U-Net remains dominant in score-based diffusion models. In this paper, we perform a systematical empirical study on the ViT-based architectures in diffusion models. Our results suggest that adding extra long skip connections (like the U-Net) to ViT is crucial to diffusion models. The new ViT architecture, together with other improvements, is referred to as U-ViT. On several popular visual datasets, U-ViT achieves competitive generation results to SOTA U-Net while requiring comparable amount of parameters and computation if not less.",
        "authors": [
            "Fan Bao",
            "Chongxuan Li",
            "Yue Cao",
            "Jun Zhu"
        ],
        "citations": 76,
        "references": 19,
        "year": 2022
    },
    {
        "title": "VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models",
        "abstract": "Diffusion models have shown impressive results in text-to-image synthesis. Using massive datasets of captioned images, diffusion models learn to generate raster images of highly diverse objects and scenes. However, designers frequently use vector representations of images like Scalable Vector Graphics (SVGs) for digital icons or art. Vector graphics can be scaled to any size, and are compact. We show that a text-conditioned diffusion model trained on pixel representations of images can be used to generate SVG-exportable vector graphics. We do so without access to large datasets of captioned SVGs. By optimizing a differentiable vector graphics rasterizer, our method, VectorFusion, distills abstract semantic knowledge out of a pretrained diffusion model. Inspired by recent text-to-3D work, we learn an SVG consistent with a caption using Score Distillation Sampling. To accelerate generation and improve fidelity, VectorFusion also initializes from an image sample. Experiments show greater quality than prior work, and demonstrate a range of styles including pixel art and sketches.",
        "authors": [
            "Ajay Jain",
            "Amber Xie",
            "P. Abbeel"
        ],
        "citations": 72,
        "references": 45,
        "year": 2022
    },
    {
        "title": "How to Backdoor Diffusion Models?",
        "abstract": "Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untam-pered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consistently lead to compromised diffusion models with high utility and target specificity. Even worse, BadDiffusion can be made cost-effective by simply finetuning a clean pre-trained diffusion model to implant backdoors. We also explore some possible countermeasures for risk mitigation. Our results call attention to potential risks and possible misuse of diffusion models. Our code is available on https://github.com/IBM/BadDiffusion.",
        "authors": [
            "Sheng-Yen Chou",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "citations": 72,
        "references": 52,
        "year": 2022
    },
    {
        "title": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
        "abstract": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.",
        "authors": [
            "Dongjun Kim",
            "Yeongmin Kim",
            "Wanmo Kang",
            "Il-Chul Moon"
        ],
        "citations": 70,
        "references": 77,
        "year": 2022
    },
    {
        "title": "Blurring Diffusion Models",
        "abstract": "Recently, Rissanen et al., (2022) have presented a new type of diffusion process for generative modeling based on heat dissipation, or blurring, as an alternative to isotropic Gaussian diffusion. Here, we show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise. In making this connection, we bridge the gap between inverse heat dissipation and denoising diffusion, and we shed light on the inductive bias that results from this modeling choice. Finally, we propose a generalized class of diffusion models that offers the best of both standard Gaussian denoising diffusion and inverse heat dissipation, which we call Blurring Diffusion Models.",
        "authors": [
            "Emiel Hoogeboom",
            "Tim Salimans"
        ],
        "citations": 68,
        "references": 21,
        "year": 2022
    },
    {
        "title": "VIDM: Video Implicit Diffusion Models",
        "abstract": "Diffusion models have emerged as a powerful generative method for synthesizing high-quality and diverse set of images. In this paper, we propose a video generation method based on diffusion models, where the effects of motion are modeled in an implicit condition manner, i.e. one can sample plausible video motions according to the latent feature of frames. We improve the quality of the generated videos by proposing multiple strategies such as sampling space truncation, robustness penalty, and positional group normalization. Various experiments are conducted on datasets consisting of videos with different resolutions and different number of frames. Results show that the proposed method outperforms the state-of-the-art generative adversarial network-based methods by a significant margin in terms of FVD scores as well as perceptible visual quality.",
        "authors": [
            "Kangfu Mei",
            "Vishal M. Patel"
        ],
        "citations": 69,
        "references": 68,
        "year": 2022
    },
    {
        "title": "Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance",
        "abstract": "Diffusion models have achieved unprecedented performance in generative modeling. The commonly-adopted formulation of the latent code of diffusion models is a sequence of gradually denoised samples, as opposed to the simpler (e.g., Gaussian) latent space of GANs, VAEs, and normalizing flows. This paper provides an alternative, Gaussian formulation of the latent space of various diffusion models, as well as an invertible DPM-Encoder that maps images into the latent space. While our formulation is purely based on the definition of diffusion models, we demonstrate several intriguing consequences. (1) Empirically, we observe that a common latent space emerges from two diffusion models trained independently on related domains. In light of this finding, we propose CycleDiffusion, which uses DPM-Encoder for unpaired image-to-image translation. Furthermore, applying CycleDiffusion to text-to-image diffusion models, we show that large-scale text-to-image diffusion models can be used as zero-shot image-to-image editors. (2) One can guide pre-trained diffusion models and GANs by controlling the latent codes in a unified, plug-and-play formulation based on energy-based models. Using the CLIP model and a face recognition model as guidance, we demonstrate that diffusion models have better coverage of low-density sub-populations and individuals than GANs. The code is publicly available at https://github.com/ChenWu98/cycle-diffusion.",
        "authors": [
            "Chen Henry Wu",
            "F. D. L. Torre"
        ],
        "citations": 59,
        "references": 81,
        "year": 2022
    },
    {
        "title": "SinFusion: Training Diffusion Models on a Single Image or Video",
        "abstract": "Diffusion models exhibited tremendous progress in image and video generation, exceeding GANs in quality and diversity. However, they are usually trained on very large datasets and are not naturally adapted to manipulate a given input image or video. In this paper we show how this can be resolved by training a diffusion model on a single input image or video. Our image/video-specific diffusion model (SinFusion) learns the appearance and dynamics of the single image or video, while utilizing the conditioning capabilities of diffusion models. It can solve a wide array of image/video-specific manipulation tasks. In particular, our model can learn from few frames the motion and dynamics of a single input video. It can then generate diverse new video samples of the same dynamic scene, extrapolate short videos into long ones (both forward and backward in time) and perform video upsampling. Most of these tasks are not realizable by current video-specific generation methods.",
        "authors": [
            "Yaniv Nikankin",
            "Niv Haim",
            "M. Irani"
        ],
        "citations": 58,
        "references": 72,
        "year": 2022
    },
    {
        "title": "3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models",
        "abstract": "Diffusion models have shown great promise for image generation, beating GANs in terms of generation diversity, with comparable image quality. However, their application to 3D shapes has been limited to point or voxel representations that can in practice not accurately represent a 3D surface. We propose a diffusion model for neural implicit representations of 3D shapes that operates in the latent space of an auto-decoder. This allows us to generate diverse and high quality 3D surfaces. We additionally show that we can condition our model on images or text to enable image-to-3D generation and text-to-3D generation using CLIP embeddings. Furthermore, adding noise to the latent codes of existing shapes allows us to explore shape variations.",
        "authors": [
            "Gimin Nam",
            "Mariem Khlifi",
            "Andrew Rodriguez",
            "Alberto Tono",
            "Linqi Zhou",
            "Paul Guerrero"
        ],
        "citations": 62,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models",
        "abstract": "Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Of particular note is the field of ``AI-Art'', which has seen unprecedented growth with the emergence of powerful multimodal models such as CLIP. By combining speech and image synthesis models, so-called ``prompt-engineering'' has become established, in which carefully selected and composed sentences are used to achieve a certain visual style in the synthesized image. In this note, we present an alternative approach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set of nearest neighbors is retrieved from an external database during training for each training instance, and the diffusion model is conditioned on these informative samples. During inference (sampling), we replace the retrieval database with a more specialized database that contains, for example, only images of a particular visual style. This provides a novel way to prompt a general trained model after training and thereby specify a particular visual style. As shown by our experiments, this approach is superior to specifying the visual style within the text prompt. We open-source code and model weights at https://github.com/CompVis/latent-diffusion .",
        "authors": [
            "Robin Rombach",
            "A. Blattmann",
            "B. Ommer"
        ],
        "citations": 63,
        "references": 30,
        "year": 2022
    },
    {
        "title": "Riemannian Diffusion Models",
        "abstract": "Diffusion models are recent state-of-the-art methods for image generation and likelihood estimation. In this work, we generalize continuous-time diffusion models to arbitrary Riemannian manifolds and derive a variational framework for likelihood estimation. Computationally, we propose new methods for computing the Riemannian divergence which is needed in the likelihood estimation. Moreover, in generalizing the Euclidean case, we prove that maximizing this variational lower-bound is equivalent to Riemannian score matching. Empirically, we demonstrate the expressive power of Riemannian diffusion models on a wide spectrum of smooth manifolds, such as spheres, tori, hyperboloids, and orthogonal groups. Our proposed method achieves new state-of-the-art likelihoods on all benchmarks.",
        "authors": [
            "Chin-Wei Huang",
            "Milad Aghajohari",
            "A. Bose",
            "P. Panangaden",
            "Aaron C. Courville"
        ],
        "citations": 62,
        "references": 68,
        "year": 2022
    },
    {
        "title": "Generating High Fidelity Data from Low-density Regions using Diffusion Models",
        "abstract": "Our work focuses on addressing sample deficiency from low-density regions of data manifold in common image datasets. We leverage diffusion process based generative models to synthesize novel images from low-density regions. We observe that uniform sampling from diffusion models predominantly samples from high-density regions of the data manifold. Therefore, we modify the sampling process to guide it towards low-density regions while simulta-neously maintaining the fidelity of synthetic data. We rigorously demonstrate that our process successfully generates novel high fidelity samples from low-density regions. We further examine generated samples and show that the model does not memorize low-density data and indeed learns to generate novel samples from low-density regions.",
        "authors": [
            "Vikash Sehwag",
            "C. Hazirbas",
            "Albert Gordo",
            "Firat Ozgenel",
            "Cristian Cantón Ferrer"
        ],
        "citations": 58,
        "references": 49,
        "year": 2022
    },
    {
        "title": "Wavelet Diffusion Models are fast and scalable Image Generators",
        "abstract": "Diffusion models are rising as a powerful solution for high-fidelity image generation, which exceeds GANs in quality in many circumstances. However, their slow training and inference speed is a huge bottleneck, blocking them from being used in real-time applications. A recent DiffusionGAN method significantly decreases the models' running time by reducing the number of sampling steps from thousands to several, but their speeds still largely lag behind the GAN counterparts. This paper aims to reduce the speed gap by proposing a novel wavelet-based diffusion scheme. We extract low-and-high frequency components from both image and feature levels via wavelet decomposition and adaptively handle these components for faster processing while maintaining good generation quality. Furthermore, we propose to use a reconstruction term, which effectively boosts the model training convergence. Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets prove our solution is a stepping-stone to offering real-time and high-fidelity diffusion models. Our code and pre-trained checkpoints are available at https://github.com/VinAIResearch/WaveDiff.git.",
        "authors": [
            "Hao Phung",
            "Quan Dao",
            "A. Tran"
        ],
        "citations": 63,
        "references": 58,
        "year": 2022
    },
    {
        "title": "DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Diffusion Models",
        "abstract": "Diffusion models emerge to establish the new state of the art in the visual generation. In particular, text-to-image diffusion models that generate images based on caption descriptions have attracted increasing attention, impressed by their user controllability. Despite encouraging performance, they exaggerate concerns of fake image misuse and cast new pressures on fake image detection. In this work, we pioneer a systematic study of the authenticity of fake images generated by text-to-image diffusion models. In particular, we conduct comprehensive studies from two perspectives unique to the text-to-image model, namely, visual modality and linguistic modality. For visual modality, we propose universal detection that demonstrates fake images of these text-to-image diffusion models share common cues, which enable us to distinguish them apart from real images. We then propose source attribution that reveals the uniqueness of the ﬁnger-prints held by each diffusion model, which can be used to attribute each fake image to its model source. A variety of ablation and analysis studies further interpret the improvements from each of our proposed methods. For linguistic modality, we delve deeper to comprehensively analyze the impacts of text captions (called prompt analysis ) on the image authenticity of text-to-image diffusion models, and reason the impacts to the detection and attribution performance of fake images. All ﬁndings contribute to the community’s insight into the natural properties of text-to-image diffusion models, and we appeal to our community’s consideration on the counterpart solutions, like ours, against the rapidly-evolving fake image generators.",
        "authors": [
            "Zeyang Sha",
            "Zheng Li",
            "Ning Yu",
            "Yang Zhang"
        ],
        "citations": 58,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Score-based Continuous-time Discrete Diffusion Models",
        "abstract": "Score-based modeling through stochastic differential equations (SDEs) has provided a new perspective on diffusion models, and demonstrated superior performance on continuous data. However, the gradient of the log-likelihood function, i.e., the score function, is not properly defined for discrete spaces. This makes it non-trivial to adapt \\textcolor{\\cdiff}{the score-based modeling} to categorical data. In this paper, we extend diffusion models to discrete variables by introducing a stochastic jump process where the reverse process denoises via a continuous-time Markov chain. This formulation admits an analytical simulation during backward sampling. To learn the reverse process, we extend score matching to general categorical data and show that an unbiased estimator can be obtained via simple matching of the conditional marginal distributions. We demonstrate the effectiveness of the proposed method on a set of synthetic and real-world music and image benchmarks.",
        "authors": [
            "Haoran Sun",
            "Lijun Yu",
            "Bo Dai",
            "D. Schuurmans",
            "H. Dai"
        ],
        "citations": 50,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models",
        "abstract": "Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. It also extends the text-conditioned method to multimodal conditioning. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the adopted challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency. Code available at this https URL",
        "authors": [
            "Xichen Pan",
            "Pengda Qin",
            "Yuhong Li",
            "Hui Xue",
            "Wenhu Chen"
        ],
        "citations": 51,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Lossy Image Compression with Conditional Diffusion Models",
        "abstract": "This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional ``content'' latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining ``texture'' variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based model, while also yielding competitive performance with VAE-based models in several distortion metrics. Furthermore, training the diffusion with $\\mathcal{X}$-parameterization enables high-quality reconstructions in only a handful of decoding steps, greatly affecting the model's practicality. Our code is available at: \\url{https://github.com/buggyyang/CDC_compression}",
        "authors": [
            "Ruihan Yang",
            "S. Mandt"
        ],
        "citations": 88,
        "references": 85,
        "year": 2022
    },
    {
        "title": "CARD: Classification and Regression Diffusion Models",
        "abstract": "Learning the distribution of a continuous or categorical response variable $\\boldsymbol y$ given its covariates $\\boldsymbol x$ is a fundamental problem in statistics and machine learning. Deep neural network-based supervised learning algorithms have made great progress in predicting the mean of $\\boldsymbol y$ given $\\boldsymbol x$, but they are often criticized for their ability to accurately capture the uncertainty of their predictions. In this paper, we introduce classification and regression diffusion (CARD) models, which combine a denoising diffusion-based conditional generative model and a pre-trained conditional mean estimator, to accurately predict the distribution of $\\boldsymbol y$ given $\\boldsymbol x$. We demonstrate the outstanding ability of CARD in conditional distribution prediction with both toy examples and real-world datasets, the experimental results on which show that CARD in general outperforms state-of-the-art methods, including Bayesian neural network-based ones that are designed for uncertainty estimation, especially when the conditional distribution of $\\boldsymbol y$ given $\\boldsymbol x$ is multi-modal. In addition, we utilize the stochastic nature of the generative model outputs to obtain a finer granularity in model confidence assessment at the instance level for classification tasks.",
        "authors": [
            "Xizewen Han",
            "Huangjie Zheng",
            "Mingyuan Zhou"
        ],
        "citations": 88,
        "references": 85,
        "year": 2022
    },
    {
        "title": "MagicMix: Semantic Mixing with Diffusion Models",
        "abstract": "Have you ever imagined what a corgi-alike coffee machine or a tiger-alike rabbit would look like? In this work, we attempt to answer these questions by exploring a new task called semantic mixing, aiming at blending two different semantics to create a new concept (e.g., corgi + coffee machine -->corgi-alike coffee machine). Unlike style transfer, where an image is stylized according to the reference style without changing the image content, semantic blending mixes two different concepts in a semantic manner to synthesize a novel concept while preserving the spatial layout and geometry. To this end, we present MagicMix, a simple yet effective solution based on pre-trained text-conditioned diffusion models. Motivated by the progressive generation property of diffusion models where layout/shape emerges at early denoising steps while semantically meaningful details appear at later steps during the denoising process, our method first obtains a coarse layout (either by corrupting an image or denoising from a pure Gaussian noise given a text prompt), followed by injection of conditional prompt for semantic mixing. Our method does not require any spatial mask or re-training, yet is able to synthesize novel objects with high fidelity. To improve the mixing quality, we further devise two simple strategies to provide better control and flexibility over the synthesized content. With our method, we present our results over diverse downstream applications, including semantic style transfer, novel object synthesis, breed mixing, and concept removal, demonstrating the flexibility of our method. More results can be found on the project page https://magicmix.github.io",
        "authors": [
            "J. Liew",
            "Hanshu Yan",
            "Daquan Zhou",
            "Jiashi Feng"
        ],
        "citations": 50,
        "references": 36,
        "year": 2022
    },
    {
        "title": "Diffusion models for missing value imputation in tabular data",
        "abstract": "Missing value imputation in machine learning is the task of estimating the missing values in the dataset accurately using available information. In this task, several deep generative modeling methods have been proposed and demonstrated their usefulness, e.g., generative adversarial imputation networks. Recently, diffusion models have gained popularity because of their effectiveness in the generative modeling task in images, texts, audio, etc. To our knowledge, less attention has been paid to the investigation of the effectiveness of diffusion models for missing value imputation in tabular data. Based on recent development of diffusion models for time-series data imputation, we propose a diffusion model approach called\"Conditional Score-based Diffusion Models for Tabular data\"(TabCSDI). To effectively handle categorical variables and numerical variables simultaneously, we investigate three techniques: one-hot encoding, analog bits encoding, and feature tokenization. Experimental results on benchmark datasets demonstrated the effectiveness of TabCSDI compared with well-known existing methods, and also emphasized the importance of the categorical embedding techniques.",
        "authors": [
            "Shuhan Zheng",
            "Nontawat Charoenphakdee"
        ],
        "citations": 53,
        "references": 27,
        "year": 2022
    },
    {
        "title": "Few-Shot Diffusion Models",
        "abstract": "Denoising diffusion probabilistic models (DDPM) are powerful hierarchical latent variable models with remarkable sample generation quality and training stability. These properties can be attributed to parameter sharing in the generative hierarchy, as well as a parameter-free diffusion-based inference procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a framework for few-shot generation leveraging conditional DDPMs. FSDMs are trained to adapt the generative process conditioned on a small set of images from a given class by aggregating image patch information using a set-based Vision Transformer (ViT). At test time, the model is able to generate samples from previously unseen classes conditioned on as few as 5 samples from that class. We empirically show that FSDM can perform few-shot generation and transfer to new datasets. We benchmark variants of our method on complex vision datasets for few-shot learning and compare to unconditional and conditional DDPM baselines. Additionally, we show how conditioning the model on patch-based input set information improves training convergence.",
        "authors": [
            "Giorgio Giannone",
            "Didrik Nielsen",
            "O. Winther"
        ],
        "citations": 49,
        "references": 112,
        "year": 2022
    },
    {
        "title": "Investigating Prompt Engineering in Diffusion Models",
        "abstract": "With the spread of the use of Text2Img diffusion models such as DALL-E 2, Imagen, Mid Journey and Stable Diffusion, one challenge that artists face is selecting the right prompts to achieve the desired artistic output. We present techniques for measuring the effect that specific words and phrases in prompts have, and (in the Appendix) present guidance on the selection of prompts to produce desired effects.",
        "authors": [
            "Sam Witteveen",
            "Martin Andrews"
        ],
        "citations": 49,
        "references": 13,
        "year": 2022
    },
    {
        "title": "Accelerating Diffusion Models via Early Stop of the Diffusion Process",
        "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved impressive performance on various generation tasks. By modeling the reverse process of gradually diffusing the data distribution into a Gaussian distribution, generating a sample in DDPMs can be regarded as iteratively denoising a randomly sampled Gaussian noise. However, in practice DDPMs often need hundreds even thousands of denoising steps to obtain a high-quality sample from the Gaussian noise, leading to extremely low inference efficiency. In this work, we propose a principled acceleration strategy, referred to as Early-Stopped DDPM (ES-DDPM), for DDPMs. The key idea is to stop the diffusion process early where only the few initial diffusing steps are considered and the reverse denoising process starts from a non-Gaussian distribution. By further adopting a powerful pre-trained generative model, such as GAN and VAE, in ES-DDPM, sampling from the target non-Gaussian distribution can be efficiently achieved by diffusing samples obtained from the pre-trained generative model. In this way, the number of required denoising steps is significantly reduced. In the meantime, the sample quality of ES-DDPM also improves substantially, outperforming both the vanilla DDPM and the adopted pre-trained generative model. On extensive experiments across CIFAR-10, CelebA, ImageNet, LSUN-Bedroom and LSUN-Cat, ES-DDPM obtains promising acceleration effect and performance improvement over representative baseline methods. Moreover, ES-DDPM also demonstrates several attractive properties, including being orthogonal to existing acceleration methods, as well as simultaneously enabling both global semantic and local pixel-level control in image generation.",
        "authors": [
            "Zhaoyang Lyu",
            "Xu Xudong",
            "Ceyuan Yang",
            "Dahua Lin",
            "Bo Dai"
        ],
        "citations": 84,
        "references": 43,
        "year": 2022
    },
    {
        "title": "3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models",
        "abstract": "Text-guided diffusion models have shown superior performance in image/video generation and editing. While few explorations have been performed in 3D scenarios. In this paper, we discuss three fundamental and interesting problems on this topic. First, we equip text-guided diffusion models to achieve 3D-consistent generation. Specifically, we integrate a NeRF-like neural field to generate low-resolution coarse results for a given camera view. Such results can provide 3D priors as condition information for the following diffusion process. During denoising diffusion, we further enhance the 3D consistency by modeling cross-view correspondences with a novel two-stream (corresponding to two different views) asynchronous diffusion process. Second, we study 3D local editing and propose a two-step solution that can generate 360-degree manipulated results by editing an object from a single view. Step 1, we propose to perform 2D local editing by blending the predicted noises. Step 2, we conduct a noise-to-text inversion process that maps 2D blended noises into the view-independent text embedding space. Once the corresponding text embedding is obtained, 360-degree images can be generated. Last but not least, we extend our model to perform one-shot novel view synthesis by fine-tuning on a single image, firstly showing the potential of leveraging text guidance for novel view synthesis. Extensive experiments and various applications show the prowess of our 3DDesigner. The project page is available at https://3ddesigner-diffusion.github.io/.",
        "authors": [
            "Gang Li",
            "Heliang Zheng",
            "Chaoyue Wang",
            "Chang Li",
            "C. Zheng",
            "Dacheng Tao"
        ],
        "citations": 51,
        "references": 46,
        "year": 2022
    },
    {
        "title": "Chemotaxis and cross-diffusion models in complex environments: Models and analytic problems toward a multiscale vision",
        "abstract": "This paper proposes a review focused on exotic chemotaxis and cross-diffusion models in complex environments. The term exotic is used to denote the dynamics of models interacting with a time-evolving external system and, specifically, models derived with the aim of describing the dynamics of living systems. The presentation first, considers the derivation of phenomenological models of chemotaxis and cross-diffusion models with particular attention on nonlinear characteristics. Then, a variety of exotic models is presented with some hints toward the derivation of new models, by accounting for a critical analysis looking ahead to perspectives. The second part of the paper is devoted to a survey of analytical problems concerning the application of models to the study of real world dynamics. Finally, the focus shifts to research perspectives within the framework of a multiscale vision, where different paths are examined to move from the dynamics at the microscopic scale to collective behaviors at the macroscopic scale.",
        "authors": [
            "N. Bellomo",
            "N. Outada",
            "J. Soler",
            "Yi Tao",
            "M. Winkler"
        ],
        "citations": 48,
        "references": 0,
        "year": 2022
    },
    {
        "title": "BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis",
        "abstract": "Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm.",
        "authors": [
            "Max W. Y. Lam",
            "J. Wang",
            "Dan Su",
            "Dong Yu"
        ],
        "citations": 82,
        "references": 47,
        "year": 2022
    },
    {
        "title": "Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design",
        "abstract": "Fragment-based drug discovery has been an effective paradigm in early-stage drug development. An open challenge in this area is designing linkers between disconnected molecular fragments of interest to obtain chemically-relevant candidate drug molecules. In this work, we propose DiffLinker, an E(3)-equivariant 3D-conditional diffusion model for molecular linker design. Given a set of disconnected fragments, our model places missing atoms in between and designs a molecule incorporating all the initial fragments. Unlike previous approaches that are only able to connect pairs of molecular fragments, our method can link an arbitrary number of fragments. Additionally, the model automatically determines the number of atoms in the linker and its attachment points to the input fragments. We demonstrate that DiffLinker outperforms other methods on the standard datasets generating more diverse and synthetically-accessible molecules. Besides, we experimentally test our method in real-world applications, showing that it can successfully generate valid linkers conditioned on target protein pockets.",
        "authors": [
            "Ilia Igashov",
            "Hannes Stärk",
            "Clément Vignac",
            "Victor Garcia Satorras",
            "P. Frossard",
            "Max Welling",
            "Michael M. Bronstein",
            "B. Correia"
        ],
        "citations": 81,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Quantum Diffusion Models",
        "abstract": "We propose a quantum version of a generative diffusion model. In this algorithm, artificial neural networks are replaced with parameterized quantum circuits, in order to directly generate quantum states. We present both a full quantum and a latent quantum version of the algorithm; we also present a conditioned version of these models. The models' performances have been evaluated using quantitative metrics complemented by qualitative assessments. An implementation of a simplified version of the algorithm has been executed on real NISQ quantum hardware.",
        "authors": [
            "Andrea Cacioppo",
            "Lorenzo Colantonio",
            "Simone Bordoni",
            "S. Giagu"
        ],
        "citations": 5,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Diffusion Models Beat GANs on Topology Optimization",
        "abstract": "Structural topology optimization, which aims to find the optimal physical structure that maximizes mechanical performance, is vital in engineering design applications in aerospace, mechanical, and civil engineering. Recently, generative adversarial networks (GANs) have emerged as a popular alternative to traditional iterative topology optimization methods. However, GANs can be challenging to train, have limited generalizability, and often neglect important performance objectives such as mechanical compliance and manufacturability. To address these issues, we propose a new architecture called TopoDiff that uses conditional diffusion models to perform performance-aware and manufacturability-aware topology optimization. Our method introduces a surrogate model-based guidance strategy that actively favors structures with low compliance and good manufacturability. Compared to a state-of-the-art conditional GAN, our approach reduces the average error on physical performance by a factor of eight and produces eleven times fewer infeasible samples. Our work demonstrates the potential of using diffusion models in topology optimization and suggests a general framework for solving engineering optimization problems using external performance with constraint-aware guidance. We provide access to our data, code, and trained models at the following link: https://decode.mit.edu/projects/topodiff/.",
        "authors": [
            "Franccois Maz'e",
            "Faez Ahmed"
        ],
        "citations": 42,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Diffusion Models for Counterfactual Explanations",
        "abstract": "Counterfactual explanations have shown promising results as a post-hoc framework to make image classifiers more explainable. In this paper, we propose DiME, a method allowing the generation of counterfactual images using the recent diffusion models. By leveraging the guided generative diffusion process, our proposed methodology shows how to use the gradients of the target classifier to generate counterfactual explanations of input instances. Further, we analyze current approaches to evaluate spurious correlations and extend the evaluation measurements by proposing a new metric: Correlation Difference. Our experimental validations show that the proposed algorithm surpasses previous State-of-the-Art results on 5 out of 6 metrics on CelebA.",
        "authors": [
            "Guillaume Jeanneret",
            "Loïc Simon",
            "F. Jurie"
        ],
        "citations": 45,
        "references": 78,
        "year": 2022
    },
    {
        "title": "Maximum Likelihood Training of Implicit Nonlinear Diffusion Models",
        "abstract": "Whereas diverse variations of diffusion models exist, extending the linear diffusion into a nonlinear diffusion process is investigated by very few works. The nonlinearity effect has been hardly understood, but intuitively, there would be promising diffusion patterns to efficiently train the generative distribution towards the data distribution. This paper introduces a data-adaptive nonlinear diffusion process for score-based diffusion models. The proposed Implicit Nonlinear Diffusion Model (INDM) learns by combining a normalizing flow and a diffusion process. Specifically, INDM implicitly constructs a nonlinear diffusion on the \\textit{data space} by leveraging a linear diffusion on the \\textit{latent space} through a flow network. This flow network is key to forming a nonlinear diffusion, as the nonlinearity depends on the flow network. This flexible nonlinearity improves the learning curve of INDM to nearly Maximum Likelihood Estimation (MLE) against the non-MLE curve of DDPM++, which turns out to be an inflexible version of INDM with the flow fixed as an identity mapping. Also, the discretization of INDM shows the sampling robustness. In experiments, INDM achieves the state-of-the-art FID of 1.75 on CelebA. We release our code at \\url{https://github.com/byeonghu-na/INDM}.",
        "authors": [
            "Dongjun Kim",
            "Byeonghu Na",
            "S. Kwon",
            "Dongsoo Lee",
            "Wanmo Kang",
            "Il-Chul Moon"
        ],
        "citations": 43,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Efficient Diffusion Models for Vision: A Survey",
        "abstract": "Diffusion Models (DMs) have demonstrated state-of-the-art performance in content generation without requiring adversarial training. These models are trained using a two-step process. First, a forward - diffusion - process gradually adds noise to a datum (usually an image). Then, a backward - reverse diffusion - process gradually removes the noise to turn it into a sample of the target distribution being modelled. DMs are inspired by non-equilibrium thermodynamics and have inherent high computational complexity. Due to the frequent function evaluations and gradient calculations in high-dimensional spaces, these models incur considerable computational overhead during both training and inference stages. This can not only preclude the democratization of diffusion-based modelling, but also hinder the adaption of diffusion models in real-life applications. Not to mention, the efficiency of computational models is fast becoming a significant concern due to excessive energy consumption and environmental scares. These factors have led to multiple contributions in the literature that focus on devising computationally efficient DMs. In this review, we present the most recent advances in diffusion models for vision, specifically focusing on the important design aspects that affect the computational efficiency of DMs. In particular, we emphasize the recently proposed design choices that have led to more efficient DMs. Unlike the other recent reviews, which discuss diffusion models from a broad perspective, this survey is aimed at pushing this research direction forward by highlighting the design strategies in the literature that are resulting in practicable models for the broader research community. We also provide a future outlook of diffusion models in vision from their computational efficiency viewpoint.",
        "authors": [
            "A. Ulhaq",
            "Naveed Akhtar",
            "Ganna Pogrebna"
        ],
        "citations": 46,
        "references": 110,
        "year": 2022
    },
    {
        "title": "Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis",
        "abstract": "Recently, diffusion models have shown remarkable results in image synthesis by gradually removing noise and amplifying signals. Although the simple generative process surprisingly works well, is this the best way to generate image data? For instance, despite the fact that human perception is more sensitive to the low frequencies of an image, diffusion models themselves do not consider any relative importance of each frequency component. Therefore, to incorporate the inductive bias for image data, we propose a novel generative process that synthesizes images in a coarse-to-fine manner. First, we generalize the standard diffusion models by enabling diffusion in a rotated coordinate system with different velocities for each component of the vector. We further propose a blur diffusion as a special case, where each frequency component of an image is diffused at different speeds. Specifically, the proposed blur diffusion consists of a forward process that blurs an image and adds noise gradually, after which a corresponding reverse process deblurs an image and removes noise progressively. Experiments show that the proposed model outperforms the previous method in FID on LSUN bedroom and church datasets. Code is available at https://github.com/sangyun884/blur-diffusion.",
        "authors": [
            "Sangyun Lee",
            "Hyungjin Chung",
            "Jaehyeon Kim",
            "Jong-Chul Ye"
        ],
        "citations": 42,
        "references": 23,
        "year": 2022
    },
    {
        "title": "Spot the fake lungs: Generating Synthetic Medical Images using Neural Diffusion Models",
        "abstract": "Generative models are becoming popular for the synthesis of medical images. Recently, neural diffusion models have demonstrated the potential to generate photo-realistic images of objects. However, their potential to generate medical images is not explored yet. In this work, we explore the possibilities of synthesis of medical images using neural diffusion models. First, we use a pre-trained DALLE2 model to generate lungs X-Ray and CT images from an input text prompt. Second, we train a stable diffusion model with 3165 X-Ray images and generate synthetic images. We evaluate the synthetic image data through a qualitative analysis where two independent radiologists label randomly chosen samples from the generated data as real, fake, or unsure. Results demonstrate that images generated with the diffusion model can translate characteristics that are otherwise very specific to certain medical conditions in chest X-Ray or CT images. Careful tuning of the model can be very promising. To the best of our knowledge, this is the first attempt to generate lungs X-Ray and CT images using neural diffusion models. This work aims to introduce a new dimension in artificial intelligence for medical imaging. Given that this is a new topic, the paper will serve as an introduction and motivation for the research community to explore the potential of diffusion models for medical image synthesis. We have released the synthetic images on https://www.kaggle.com/datasets/hazrat/awesomelungs.",
        "authors": [
            "Hazrat Ali",
            "Shafaq Murad",
            "Zubair Shah"
        ],
        "citations": 41,
        "references": 16,
        "year": 2022
    },
    {
        "title": "Parallel Diffusion Models of Operator and Image for Blind Inverse Problems",
        "abstract": "Diffusion model-based inverse problem solvers have demonstrated state-of-the-art performance in cases where the forward operator is known (i.e. non-blind). However, the applicability of the method to blind inverse problems has yet to be explored. In this work, we show that we can indeed solve a family of blind inverse problems by constructing another diffusion prior for the forward operator. Specifically, parallel reverse diffusion guided by gradients from the intermediate stages enables joint optimization of both the forward operator parameters as well as the image, such that both are jointly estimated at the end of the parallel reverse diffusion procedure. We show the efficacy of our method on two representative tasks - blind deblurring, and imaging through turbulence - and show that our method yields state-of-the-art performance, while also being flexible to be applicable to general blind inverse problems when we know the functional forms. Code available: https://github.com/BlindDPS/blind-dps",
        "authors": [
            "Hyungjin Chung",
            "Jeongsol Kim",
            "Sehui Kim",
            "J. C. Ye"
        ],
        "citations": 71,
        "references": 64,
        "year": 2022
    },
    {
        "title": "Denoising diffusion models for out-of-distribution detection",
        "abstract": "Out-of-distribution detection is crucial to the safe deployment of machine learning systems. Currently, unsupervised out-of-distribution detection is dominated by generative-based approaches that make use of estimates of the likelihood or other measurements from a generative model. Reconstruction-based methods offer an alternative approach, in which a measure of reconstruction error is used to determine if a sample is out-of-distribution. However, reconstruction-based approaches are less favoured, as they require careful tuning of the model’s information bottleneck-such as the size of the latent dimension - to produce good results. In this work, we exploit the view of denoising diffusion probabilistic models (DDPM) as denoising autoencoders where the bottleneck is controlled externally, by means of the amount of noise applied. We propose to use DDPMs to reconstruct an input that has been noised to a range of noise levels, and use the resulting multi-dimensional reconstruction error to classify out-of-distribution inputs. We validate our approach both on standard computer-vision datasets and on higher dimension medical datasets. Our approach outperforms not only reconstruction-based methods, but also state-of-the-art generative-based approaches. Code is available at https://github.com/marksgraham/ddpm-ood.",
        "authors": [
            "M. Graham",
            "W. H. Pinaya",
            "Petru-Daniel Tudosiu",
            "P. Nachev",
            "S. Ourselin",
            "M. Cardoso"
        ],
        "citations": 58,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Improved Vector Quantized Diffusion Models",
        "abstract": "Vector quantized diffusion (VQ-Diffusion) is a powerful generative model for text-to-image synthesis, but sometimes can still generate low-quality samples or weakly correlated images with text input. We find these issues are mainly due to the flawed sampling strategy. In this paper, we propose two important techniques to further improve the sample quality of VQ-Diffusion. 1) We explore classifier-free guidance sampling for discrete denoising diffusion model and propose a more general and effective implementation of classifier-free guidance. 2) We present a high-quality inference strategy to alleviate the joint distribution issue in VQ-Diffusion. Finally, we conduct experiments on various datasets to validate their effectiveness and show that the improved VQ-Diffusion suppresses the vanilla version by large margins. We achieve an 8.44 FID score on MSCOCO, surpassing VQ-Diffusion by 5.42 FID score. When trained on ImageNet, we dramatically improve the FID score from 11.89 to 4.83, demonstrating the superiority of our proposed techniques.",
        "authors": [
            "Zhicong Tang",
            "Shuyang Gu",
            "Jianmin Bao",
            "Dong Chen",
            "Fang Wen"
        ],
        "citations": 59,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Deep Equilibrium Approaches to Diffusion Models",
        "abstract": "Diffusion-based generative models are extremely effective in generating high-quality images, with generated samples often surpassing the quality of those produced by other models under several metrics. One distinguishing feature of these models, however, is that they typically require long sampling chains to produce high-fidelity images. This presents a challenge not only from the lenses of sampling time, but also from the inherent difficulty in backpropagating through these chains in order to accomplish tasks such as model inversion, i.e. approximately finding latent states that generate known images. In this paper, we look at diffusion models through a different perspective, that of a (deep) equilibrium (DEQ) fixed point model. Specifically, we extend the recent denoising diffusion implicit model (DDIM; Song et al. 2020), and model the entire sampling chain as a joint, multivariate fixed point system. This setup provides an elegant unification of diffusion and equilibrium models, and shows benefits in 1) single image sampling, as it replaces the fully-serial typical sampling process with a parallel one; and 2) model inversion, where we can leverage fast gradients in the DEQ setting to much more quickly find the noise that generates a given image. The approach is also orthogonal and thus complementary to other methods used to reduce the sampling time, or improve model inversion. We demonstrate our method's strong performance across several datasets, including CIFAR10, CelebA, and LSUN Bedrooms and Churches.",
        "authors": [
            "Ashwini Pokle",
            "Zhengyang Geng",
            "Zico Kolter"
        ],
        "citations": 31,
        "references": 79,
        "year": 2022
    },
    {
        "title": "Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors",
        "abstract": "Recently, text-to-image diffusion models have shown remarkable capabilities in creating realistic images from natural language prompts. However, few works have explored using these models for semantic localization or grounding. In this work, we explore how an off-the-shelf text-to-image diffusion model, trained without exposure to localization information, can ground various semantic phrases without segmentation-specific re-training. We introduce an inference time optimization process capable of generating segmentation masks conditioned on natural language prompts. Our proposal, Peekaboo, is a first-of-its-kind zero-shot, open-vocabulary, unsupervised semantic grounding technique leveraging diffusion models without any training. We evaluate Peekaboo on the Pascal VOC dataset for unsupervised semantic segmentation and the RefCOCO dataset for referring segmentation, showing results competitive with promising results. We also demonstrate how Peekaboo can be used to generate images with transparency, even though the underlying diffusion model was only trained on RGB images - which to our knowledge we are the first to attempt. Please see our project page, including our code: https://ryanndagreat.github.io/peekaboo",
        "authors": [
            "R. Burgert",
            "Kanchana Ranasinghe",
            "Xiang Li",
            "M. Ryoo"
        ],
        "citations": 33,
        "references": 90,
        "year": 2022
    },
    {
        "title": "High-Fidelity Guided Image Synthesis with Latent Diffusion Models",
        "abstract": "Controllable image synthesis with user scribbles has gained huge public interest with the recent advent of text-conditioned latent diffusion models. The user scribbles control the color composition while the text prompt provides control over the overall image semantics. However, we note that prior works in this direction suffer from an intrinsic domain shift problem wherein the generated outputs often lack details and resemble simplistic representations of the target domain. In this paper, we propose a novel guided image synthesis framework, which addresses this problem by modelling the output image as the solution of a constrained optimization problem. We show that while computing an exact solution to the optimization is infeasible, an approximation of the same can be achieved while just requiring a single pass of the reverse diffusion process. Additionally, we show that by simply defining a cross-attention based correspondence between the input text tokens and the user stroke-painting, the user is also able to control the semantics of different painted regions without requiring any conditional training or finetuning. Human user study results show that the proposed approach outperforms the previous state-of-the-art by over 85.32% on the overall user satisfaction scores. Project page for our paper is available at https://1jsingh.github.io/gradop.",
        "authors": [
            "Jaskirat Singh",
            "Stephen Gould",
            "Liang Zheng"
        ],
        "citations": 35,
        "references": 47,
        "year": 2022
    },
    {
        "title": "DensePure: Understanding Diffusion Models towards Adversarial Robustness",
        "abstract": "Diffusion models have been recently employed to improve certified robustness through the process of denoising. However, the theoretical understanding of why diffusion models are able to improve the certified robustness is still lacking, preventing from further improvement. In this study, we close this gap by analyzing the fundamental properties of diffusion models and establishing the conditions under which they can enhance certified robustness. This deeper understanding allows us to propose a new method DensePure, designed to improve the certified robustness of a pretrained model (i.e. classifier). Given an (adversarial) input, DensePure consists of multiple runs of denoising via the reverse process of the diffusion model (with different random seeds) to get multiple reversed samples, which are then passed through the classifier, followed by majority voting of inferred labels to make the final prediction. This design of using multiple runs of denoising is informed by our theoretical analysis of the conditional distribution of the reversed sample. Specifically, when the data density of a clean sample is high, its conditional density under the reverse process in a diffusion model is also high; thus sampling from the latter conditional distribution can purify the adversarial example and return the corresponding clean sample with a high probability. By using the highest density point in the conditional distribution as the reversed sample, we identify the robust region of a given instance under the diffusion model's reverse process. We show that this robust region is a union of multiple convex sets, and is potentially much larger than the robust regions identified in previous works. In practice, DensePure can approximate the label of the high density region in the conditional distribution so that it can enhance certified robustness.",
        "authors": [
            "Chaowei Xiao",
            "Zhongzhu Chen",
            "Kun Jin",
            "Jiong Wang",
            "Weili Nie",
            "Mingyan Liu",
            "Anima Anandkumar",
            "Bo Li",
            "D. Song"
        ],
        "citations": 29,
        "references": 37,
        "year": 2022
    },
    {
        "title": "RePaint: Inpainting using Denoising Diffusion Probabilistic Models",
        "abstract": "Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint",
        "authors": [
            "Andreas Lugmayr",
            "Martin Danelljan",
            "Andrés Romero",
            "F. Yu",
            "R. Timofte",
            "L. Gool"
        ],
        "citations": 1000,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Self-Guided Diffusion Models",
        "abstract": "Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability and correctness. In this paper, we eliminate the need for such annotation by instead exploiting the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels. When equipped with self-supervised box or mask proposals, our method further generates visually diverse yet semantically consistent images, without the need for any class, box, or segment label annotation. Self-guided diffusion is simple, flexible and expected to profit from deployment at scale.",
        "authors": [
            "Vincent Tao Hu",
            "David W. Zhang",
            "Yuki M. Asano",
            "G. Burghouts",
            "Cees G. M. Snoek"
        ],
        "citations": 27,
        "references": 76,
        "year": 2022
    },
    {
        "title": "DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion Models",
        "abstract": "Traditionally, monocular 3D human pose estimation employs a machine learning model to predict the most likely 3D pose for a given input image. However, a single image can be highly ambiguous and induces multiple plausible solutions for the 2D-3D lifting step, which results in overly confident 3D pose predictors. To this end, we propose DiffPose, a conditional diffusion model that predicts multiple hypotheses for a given input image. Compared to similar approaches, our diffusion model is straightforward and avoids intensive hyperparameter tuning, complex network structures, mode collapse, and unstable training. Moreover, we tackle the problem of over-simplification of the intermediate representation of the common two-step approaches which first estimate a distribution of 2D joint locations via joint-wise heatmaps and consecutively use their maximum argument for the 3D pose estimation step. Since such a simplification of the heatmaps removes valid information about possibly correct, though labeled unlikely, joint locations, we propose to represent the heatmaps as a set of 2D joint candidate samples. To extract information about the original distribution from these samples, we introduce our embedding transformer which conditions the diffusion model. Experimentally, we show that DiffPose improves upon the state of the art for multi-hypothesis pose estimation by 3-5% for simple poses and outperforms it by a large margin for highly ambiguous poses.1",
        "authors": [
            "Karl Holmquist",
            "Bastian Wandt"
        ],
        "citations": 51,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Fine-tuning Diffusion Models with Limited Data",
        "abstract": "Diffusion models have recently shown remarkable progress, demonstrating state-of-the-art image generation qualities. Like the other high-fidelity generative models, diffusion models require a large amount of data and computing time for stable training, which hinders the application of diffusion models for limited data settings. To overcome this issue, one can employ a pre-trained diffusion model built on a large-scale dataset and fine-tune it on a target dataset. Unfortunately, as we show empirically, this easily results in overfitting. In this paper, we propose an efficient fine-tuning algorithm for diffusion models that can efficiently and robustly train on limited data settings. We first show that fine-tuning only the small subset of the pre-trained parameters can efficiently learn the target dataset with much less overfitting. Then we further introduce a lightweight adapter module that can be attached to the pre-trained model with minimal overhead and show that fine-tuning with our adapter module significantly improves the image generation quality. We demonstrate the effectiveness of our method on various real-world image datasets.",
        "authors": [
            "Taehong Moon",
            "Moonseok Choi",
            "Gayoung Lee",
            "Jung-Woo Ha",
            "Juho Lee",
            "AI Kaist",
            "Naver AI Lab",
            "Aitrics"
        ],
        "citations": 25,
        "references": 48,
        "year": 2022
    },
    {
        "title": "MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation",
        "abstract": "We present a novel method for exemplar-based image translation, called matching interleaved diffusion models (MIDMs). Most existing methods for this task were formulated as GAN-based matching-then-generation framework. However, in this framework, matching errors induced by the difficulty of semantic matching across cross-domain, e.g., sketch and photo, can be easily propagated to the generation step, which in turn leads to the degenerated results. Motivated by the recent success of diffusion models, overcoming the shortcomings of GANs, we incorporate the diffusion models to overcome these limitations. Specifically, we formulate a diffusion-based matching-and-generation framework that interleaves cross-domain matching and diffusion steps in the latent space by iteratively feeding the intermediate warp into the noising process and denoising it to generate a translated image. In addition, to improve the reliability of diffusion process, we design confidence-aware process using cycle-consistency to consider only confident regions during translation. Experimental results show that our MIDMs generate more plausible images than state-of-the-art methods.",
        "authors": [
            "Junyoung Seo",
            "Gyuseong Lee",
            "Seokju Cho",
            "Jiyoung Lee",
            "Seung Wook Kim"
        ],
        "citations": 25,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Innovation Diffusion Models",
        "abstract": null,
        "authors": [
            "M. Guidolin"
        ],
        "citations": 5,
        "references": 0,
        "year": 2023
    },
    {
        "title": "The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models",
        "abstract": "Recently, diffusion models were applied to a wide range of image analysis tasks. We build on a method for image-to-image translation using denoising diffusion implicit models and include a regression problem and a segmentation problem for guiding the image generation to the desired output. The main advantage of our approach is that the guidance during the denoising process is done by an external gradient. Consequently, the diffusion model does not need to be retrained for the different tasks on the same dataset. We apply our method to simulate the aging process on facial photos using a regression task, as well as on a brain magnetic resonance (MR) imaging dataset for the simulation of brain tumor growth. Furthermore, we use a segmentation model to inpaint tumors at the desired location in healthy slices of brain MR images. We achieve convincing results for all problems.",
        "authors": [
            "Julia Wolleb",
            "Robin Sandkühler",
            "Florentin Bieder",
            "P. Cattin"
        ],
        "citations": 28,
        "references": 27,
        "year": 2022
    },
    {
        "title": "Dynamic Dual-Output Diffusion Models",
        "abstract": "Iterative denoising-based generation, also known as denoising diffusion models, has recently been shown to be comparable in quality to other classes of generative models, and even surpass them. Including, in particular, Generative Adversarial Networks, which are currently the state of the art in many subtasks of image generation. However, a major drawback of this method is that it requires hundreds of iterations to produce a competitive result. Recent works have proposed solutions that allow for faster generation with fewer iterations, but the image quality gradually deteriorates with increasingly fewer iterations being applied during generation. In this paper, we reveal some of the causes that affect the generation quality of diffusion models, especially when sampling with few iterations, and come up with a simple, yet effective, solution to mitigate them. We consider two opposite equations for the iterative denoising, the first predicts the applied noise, and the second predicts the image directly. Our solution takes the two options and learns to dynamically alternate between them through the denoising process. Our proposed solution is general and can be applied to any existing diffusion model. As we show, when applied to various SOTA architectures, our solution immediately improves their generation quality, with negligible added complexity and parameters. We experiment on multiple datasets and configurations and run an extensive ablation study to support these findings.",
        "authors": [
            "Yaniv Benny",
            "Lior Wolf"
        ],
        "citations": 23,
        "references": 37,
        "year": 2022
    },
    {
        "title": "Conffusion: Confidence Intervals for Diffusion Models",
        "abstract": "Diffusion models have become the go-to method for many generative tasks, particularly for image-to-image generation tasks such as super-resolution and inpainting. Current diffusion-based methods do not provide statistical guarantees regarding the generated results, often preventing their use in high-stakes situations. To bridge this gap, we construct a confidence interval around each generated pixel such that the true value of the pixel is guaranteed to fall within the interval with a probability set by the user. Since diffusion models parametrize the data distribution, a straightforward way of constructing such intervals is by drawing multiple samples and calculating their bounds. However, this method has several drawbacks: i) slow sampling speeds ii) suboptimal bounds iii) requires training a diffusion model per task. To mitigate these shortcomings we propose Conffusion, wherein we fine-tune a pre-trained diffusion model to predict interval bounds in a single forward pass. We show that Conffusion outperforms the baseline method while being three orders of magnitude faster.",
        "authors": [
            "Eliahu Horwitz",
            "Yedid Hoshen"
        ],
        "citations": 23,
        "references": 38,
        "year": 2022
    },
    {
        "title": "Diffusion Models for Graphs Benefit From Discrete State Spaces",
        "abstract": "Denoising diffusion probabilistic models and score-matching models have proven to be very powerful for generative tasks. While these approaches have also been applied to the generation of discrete graphs, they have, so far, relied on continuous Gaussian perturbations. Instead, in this work, we suggest using discrete noise for the forward Markov process. This ensures that in every intermediate step the graph remains discrete. Compared to the previous approach, our experimental results on four datasets and multiple architectures show that using a discrete noising process results in higher quality generated samples indicated with an average MMDs reduced by a factor of 1.5. Furthermore, the number of denoising steps is reduced from 1000 to 32 steps, leading to a 30 times faster sampling procedure.",
        "authors": [
            "K. Haefeli",
            "Karolis Martinkus",
            "Nathanael Perraudin",
            "R. Wattenhofer"
        ],
        "citations": 42,
        "references": 35,
        "year": 2022
    },
    {
        "title": "Grad-StyleSpeech: Any-Speaker Adaptive Text-to-Speech Synthesis with Diffusion Models",
        "abstract": "There has been a significant progress in Text-To-Speech (TTS) synthesis technology in recent years, thanks to the advancement in neural generative modeling. However, existing methods on any-speaker adaptive TTS have achieved unsatisfactory performance, due to their suboptimal accuracy in mimicking the target speakers’ styles. In this work, we present Grad-StyleSpeech, which is an any-speaker adaptive TTS framework that is based on a diffusion model that can generate highly natural speech with extremely high similarity to target speakers’ voice, given a few seconds of reference speech. Grad-StyleSpeech significantly outperforms recent speaker-adaptive TTS baselines on English benchmarks. Audio samples are available at https://nardien.github.io/grad-stylespeech-demo.",
        "authors": [
            "Minki Kang",
            "Dong Min",
            "Sung Ju Hwang"
        ],
        "citations": 45,
        "references": 26,
        "year": 2022
    },
    {
        "title": "Non-Uniform Diffusion Models",
        "abstract": "Diffusion models have emerged as one of the most promising frameworks for deep generative modeling. In this work, we explore the potential of non-uniform diffusion models. We show that non-uniform diffusion leads to multi-scale diffusion models which have similar structure to this of multi-scale normalizing flows. We experimentally find that in the same or less training time, the multi-scale diffusion model achieves better FID score than the standard uniform diffusion model. More importantly, it generates samples $4.4$ times faster in $128\\times 128$ resolution. The speed-up is expected to be higher in higher resolutions where more scales are used. Moreover, we show that non-uniform diffusion leads to a novel estimator for the conditional score function which achieves on par performance with the state-of-the-art conditional denoising estimator. Our theoretical and experimental findings are accompanied by an open source library MSDiff which can facilitate further research of non-uniform diffusion models.",
        "authors": [
            "Georgios Batzolis",
            "Jan Stanczuk",
            "C. Schonlieb",
            "Christian Etmann"
        ],
        "citations": 15,
        "references": 29,
        "year": 2022
    },
    {
        "title": "First Hitting Diffusion Models",
        "abstract": "We propose a family of First Hitting Diffusion Models (FHDM), deep generative models that generate data with a diffusion process that terminates at a random ﬁrst hitting time. This yields an extension of the standard ﬁxed-time diffusion models that terminate at a pre-speciﬁed deterministic time. Although standard diffusion models are designed for continuous unconstrained data, FHDM is naturally designed to learn distributions on continuous as well as a range of discrete and structure domains. Moreover, FHDM enables instance-dependent terminate time and accelerates the diffusion process to sample higher quality data with fewer diffusion steps. Technically, we train FHDM by maximum likelihood estimation on diffusion trajectories augmented from observed data with conditional ﬁrst hitting processes (i.e., bridge) derived based on Doob’s h -transform, deviating from the commonly used time-reversal mechanism. We apply FHDM to generate data in various domains such as point cloud (general continuous distribution), climate and geographical events on earth (continuous distribution on the sphere), unweighted graphs (distribution of binary matrices), and segmentation maps of 2D images (high-dimensional categorical distribution). We observe considerable improvement compared with the state-of-the-art approaches in both quality and speed.",
        "authors": [
            "Mao Ye",
            "Lemeng Wu",
            "Qiang Liu"
        ],
        "citations": 16,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Diffusion Models for Causal Discovery via Topological Ordering",
        "abstract": "Discovering causal relations from observational data becomes possible with additional assumptions such as considering the functional relations to be constrained as nonlinear with additive noise (ANM). Even with strong assumptions, causal discovery involves an expensive search problem over the space of directed acyclic graphs (DAGs). \\emph{Topological ordering} approaches reduce the optimisation space of causal discovery by searching over a permutation rather than graph space. For ANMs, the \\emph{Hessian} of the data log-likelihood can be used for finding leaf nodes in a causal graph, allowing its topological ordering. However, existing computational methods for obtaining the Hessian still do not scale as the number of variables and the number of samples increase. Therefore, inspired by recent innovations in diffusion probabilistic models (DPMs), we propose \\emph{DiffAN}\\footnote{Implementation is available at \\url{https://github.com/vios-s/DiffAN} .}, a topological ordering algorithm that leverages DPMs for learning a Hessian function. We introduce theory for updating the learned Hessian without re-training the neural network, and we show that computing with a subset of samples gives an accurate approximation of the ordering, which allows scaling to datasets with more samples and variables. We show empirically that our method scales exceptionally well to datasets with up to $500$ nodes and up to $10^5$ samples while still performing on par over small datasets with state-of-the-art causal discovery methods. Implementation is available at https://github.com/vios-s/DiffAN .",
        "authors": [
            "Pedro Sanchez",
            "Xiao Liu",
            "Alison Q. O'Neil",
            "S. Tsaftaris"
        ],
        "citations": 35,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Simple and Effective Masked Diffusion Language Models",
        "abstract": "While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/mdlm",
        "authors": [
            "S. Sahoo",
            "Marianne Arriola",
            "Yair Schiff",
            "Aaron Gokaslan",
            "E. Marroquin",
            "Justin T Chiu",
            "Alexander Rush",
            "Volodymyr Kuleshov"
        ],
        "citations": 24,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Comparative assessment and selection of electric vehicle diffusion models: A global outlook",
        "abstract": null,
        "authors": [
            "R. Kumar",
            "Pritha Guha",
            "Abhishek Chakraborty"
        ],
        "citations": 57,
        "references": 98,
        "year": 2022
    },
    {
        "title": "Bayesian MRI reconstruction with joint uncertainty estimation using diffusion models",
        "abstract": "We introduce a framework that enables efficient sampling from learned probability distributions for MRI reconstruction.",
        "authors": [
            "Guanxiong Luo",
            "Moritz Blumenthal",
            "Martin Heide",
            "M. Uecker"
        ],
        "citations": 54,
        "references": 49,
        "year": 2022
    },
    {
        "title": "Pretrained Diffusion Models for Unified Human Motion Synthesis",
        "abstract": "Generative modeling of human motion has broad applications in computer animation, virtual reality, and robotics. Conventional approaches develop separate models for different motion synthesis tasks, and typically use a model of a small size to avoid overfitting the scarce data available in each setting. It remains an open question whether developing a single unified model is feasible, which may 1) benefit the acquirement of novel skills by combining skills learned from multiple tasks, and 2) help in increasing the model capacity without overfitting by combining multiple data sources. Unification is challenging because 1) it involves diverse control signals as well as targets of varying granularity, and 2) motion datasets may use different skeletons and default poses. In this paper, we present MoFusion, a framework for unified motion synthesis. MoFusion employs a Transformer backbone to ease the inclusion of diverse control signals via cross attention, and pretrains the backbone as a diffusion model to support multi-granularity synthesis ranging from motion completion of a body part to whole-body motion generation. It uses a learnable adapter to accommodate the differences between the default skeletons used by the pretraining and the fine-tuning data. Empirical results show that pretraining is vital for scaling the model size without overfitting, and demonstrate MoFusion's potential in various tasks, e.g., text-to-motion, motion completion, and zero-shot mixing of multiple control signals. Project page: \\url{https://ofa-sys.github.io/MoFusion/}.",
        "authors": [
            "Jianxin Ma",
            "Shuai Bai",
            "Chang Zhou"
        ],
        "citations": 28,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Threat Model-Agnostic Adversarial Defense using Diffusion Models",
        "abstract": "Deep Neural Networks (DNNs) are highly sensitive to imperceptible malicious perturbations, known as adversarial attacks. Following the discovery of this vulnerability in real-world imaging and vision applications, the associated safety concerns have attracted vast research attention, and many defense techniques have been developed. Most of these defense methods rely on adversarial training (AT) -- training the classification network on images perturbed according to a specific threat model, which defines the magnitude of the allowed modification. Although AT leads to promising results, training on a specific threat model fails to generalize to other types of perturbations. A different approach utilizes a preprocessing step to remove the adversarial perturbation from the attacked image. In this work, we follow the latter path and aim to develop a technique that leads to robust classifiers across various realizations of threat models. To this end, we harness the recent advances in stochastic generative modeling, and means to leverage these for sampling from conditional distributions. Our defense relies on an addition of Gaussian i.i.d noise to the attacked image, followed by a pretrained diffusion process -- an architecture that performs a stochastic iterative process over a denoising network, yielding a high perceptual quality denoised outcome. The obtained robustness with this stochastic preprocessing step is validated through extensive experiments on the CIFAR-10 dataset, showing that our method outperforms the leading defense methods under various threat models.",
        "authors": [
            "Tsachi Blau",
            "Roy Ganz",
            "Bahjat Kawar",
            "Alex M. Bronstein",
            "Michael Elad"
        ],
        "citations": 26,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Denoising Diffusion Restoration Models",
        "abstract": "Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM's versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set.",
        "authors": [
            "Bahjat Kawar",
            "Michael Elad",
            "Stefano Ermon",
            "Jiaming Song"
        ],
        "citations": 667,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Deep diffusion models for seismic processing",
        "abstract": null,
        "authors": [
            "Ricard Durall",
            "A. Ghanim",
            "M. Fernandez",
            "N. Ettrich",
            "J. Keuper"
        ],
        "citations": 23,
        "references": 39,
        "year": 2022
    },
    {
        "title": "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models",
        "abstract": "Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially in recent large-scale text-to-image generation applications. An essential technique for improving the sample quality of DPMs is guided sampling, which usually needs a large guidance scale to obtain the best sample quality. The commonly-used fast sampler for guided sampling is DDIM, a first-order diffusion ODE solver that generally needs 100 to 250 steps for high-quality samples. Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows large. To further speed up guided sampling, we propose DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++ solves the diffusion ODE with the data prediction model and adopts thresholding methods to keep the solution matches training data distribution. We further propose a multistep variant of DPM-Solver++ to address the instability issue by reducing the effective step size. Experiments show that DPM-Solver++ can generate high-quality samples within only 15 to 20 steps for guided sampling by pixel-space and latent-space DPMs.",
        "authors": [
            "Cheng Lu",
            "Yuhao Zhou",
            "Fan Bao",
            "Jianfei Chen",
            "Chongxuan Li",
            "Jun Zhu"
        ],
        "citations": 425,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Computation of the basic reproduction numbers for reaction-diffusion epidemic models",
        "abstract": "We consider a class of k-dimensional reaction-diffusion epidemic models (k=1,2,⋯) that are developed from autonomous ODE systems. We present a computational approach for the calculation and analysis of their basic reproduction numbers. Particularly, we apply matrix theory to study the relationship between the basic reproduction numbers of the PDE models and those of their underlying ODE models. We show that the basic reproduction numbers are the same for these PDE models and their associated ODE models in several important scenarios. We additionally provide two numerical examples to verify our analytical results.",
        "authors": [
            "Chayu Yang",
            "Jin Wang"
        ],
        "citations": 237,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models",
        "abstract": "Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20x to 80x speed up.",
        "authors": [
            "Fan Bao",
            "Chongxuan Li",
            "Jun Zhu",
            "Bo Zhang"
        ],
        "citations": 293,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision",
        "abstract": "Denoising diffusion models are a powerful type of generative models used to capture complex distributions of real-world signals. However, their applicability is limited to scenarios where training samples are readily available, which is not always the case in real-world applications. For example, in inverse graphics, the goal is to generate samples from a distribution of 3D scenes that align with a given image, but ground-truth 3D scenes are unavailable and only 2D images are accessible. To address this limitation, we propose a novel class of denoising diffusion probabilistic models that learn to sample from distributions of signals that are never directly observed. Instead, these signals are measured indirectly through a known differentiable forward model, which produces partial observations of the unknown signal. Our approach involves integrating the forward model directly into the denoising process. This integration effectively connects the generative modeling of observations with the generative modeling of the underlying signals, allowing for end-to-end training of a conditional generative model over signals. During inference, our approach enables sampling from the distribution of underlying signals that are consistent with a given partial observation. We demonstrate the effectiveness of our method on three challenging computer vision tasks. For instance, in the context of inverse graphics, our model enables direct sampling from the distribution of 3D scenes that align with a single 2D input image.",
        "authors": [
            "A. Tewari",
            "Tianwei Yin",
            "George Cazenavette",
            "Semon Rezchikov",
            "J. Tenenbaum",
            "F. Durand",
            "W. Freeman",
            "Vincent Sitzmann"
        ],
        "citations": 70,
        "references": 94,
        "year": 2023
    },
    {
        "title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models",
        "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
        "authors": [
            "Yinghao Aaron Li",
            "Cong Han",
            "Vinay S. Raghavan",
            "Gavin Mischler",
            "N. Mesgarani"
        ],
        "citations": 73,
        "references": 65,
        "year": 2023
    },
    {
        "title": "AnoDDPM: Anomaly Detection with Denoising Diffusion Probabilistic Models using Simplex Noise",
        "abstract": "Generative models have been shown to provide a powerful mechanism for anomaly detection by learning to model healthy or normal reference data which can subsequently be used as a baseline for scoring anomalies. In this work we consider denoising diffusion probabilistic models (DDPMs) for unsupervised anomaly detection. DDPMs have superior mode coverage over generative adversarial networks (GANs) and higher sample quality than variational autoencoders (VAEs). However, this comes at the expense of poor scalability and increased sampling times due to the long Markov chain sequences required. We observe that within reconstruction-based anomaly detection a full-length Markov chain diffusion is not required. This leads us to develop a novel partial diffusion anomaly detection strategy that scales to high-resolution imagery, named AnoDDPM. A secondary problem is that Gaussian diffusion fails to capture larger anomalies; therefore we develop a multi-scale simplex noise diffusion process that gives control over the target anomaly size. AnoDDPM with simplex noise is shown to significantly outperform both f-AnoGAN and Gaussian diffusion for the tumorous dataset of 22 T1-weighted MRI scans (CCBS Edinburgh) qualitatively and quantitatively (improvement of +25.5% Sørensen–Dice coefficient, +17.6% IoU and +7.4% AUC).",
        "authors": [
            "Julian Wyatt",
            "Adam Leach",
            "Sebastian M. Schmon",
            "Chris G. Willcocks"
        ],
        "citations": 217,
        "references": 30,
        "year": 2022
    },
    {
        "title": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
        "abstract": "Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64x64 resolution (FID 1.92). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's access to the score function can streamline the adoption of established controllable/conditional generation methods from the diffusion community. This access also enables the computation of likelihood. The code is available at https://github.com/sony/ctm.",
        "authors": [
            "Dongjun Kim",
            "Chieh-Hsin Lai",
            "Wei-Hsiang Liao",
            "Naoki Murata",
            "Yuhta Takida",
            "Toshimitsu Uesaka",
            "Yutong He",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "citations": 122,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness",
        "abstract": "Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.",
        "authors": [
            "Felix Friedrich",
            "P. Schramowski",
            "Manuel Brack",
            "Lukas Struppek",
            "Dominik Hintersdorf",
            "Sasha Luccioni",
            "K. Kersting"
        ],
        "citations": 95,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Denoising diffusion probabilistic models for 3D medical image generation",
        "abstract": null,
        "authors": [
            "Firas Khader",
            "Gustav Mueller-Franzes",
            "Soroosh Tayebi Arasteh",
            "T. Han",
            "Christoph Haarburger",
            "M. Schulze-Hagen",
            "P. Schad",
            "S. Engelhardt",
            "B. Baessler",
            "S. Foersch",
            "J. Stegmaier",
            "C. Kuhl",
            "S. Nebelung",
            "Jakob Nikolas Kather",
            "D. Truhn"
        ],
        "citations": 93,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data",
        "abstract": "We present Viewset Diffusion, a diffusion-based generator that outputs 3D objects while only using multi-view 2D data for supervision. We note that there exists a one-to-one mapping between viewsets, i.e., collections of several 2D views of an object, and 3D models. Hence, we train a diffusion model to generate viewsets, but design the neural network generator to reconstruct internally corresponding 3D models, thus generating those too. We fit a diffusion model to a large number of viewsets for a given category of objects. The resulting generator can be conditioned on zero, one or more input views. Conditioned on a single view, it performs 3D reconstruction accounting for the ambiguity of the task and allowing to sample multiple solutions compatible with the input. The model performs reconstruction efficiently, in a feed-forward manner, and is trained using only rendering losses using as few as three views per viewset. Project page: szymanowiczs.github.io/viewset-diffusion.",
        "authors": [
            "Stanislaw Szymanowicz",
            "C. Rupprecht",
            "A. Vedaldi"
        ],
        "citations": 80,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models",
        "abstract": "State-of-the-art Text-to-Image models like Stable Diffusion and DALLE\\cdot2 are revolutionizing how people generate visual content. At the same time, society has serious concerns about how adversaries can exploit such models to generate problematic or unsafe images. In this work, we focus on demystifying the generation of unsafe images and hateful memes from Text-to-Image models. We first construct a typology of unsafe images consisting of five categories (sexually explicit, violent, disturbing, hateful, and political). Then, we assess the proportion of unsafe images generated by four advanced Text-to-Image models using four prompt datasets. We find that Text-to-Image models can generate a substantial percentage of unsafe images; across four models and four prompt datasets, 14.56% of all generated images are unsafe. When comparing the four Text-to-Image models, we find different risk levels, with Stable Diffusion being the most prone to generating unsafe content (18.92% of all generated images are unsafe). Given Stable Diffusion's tendency to generate more unsafe content, we evaluate its potential to generate hateful meme variants if exploited by an adversary to attack a specific individual or community. We employ three image editing methods, DreamBooth, Textual Inversion, and SDEdit, which are supported by Stable Diffusion to generate variants. Our evaluation result shows that 24% of the generated images using DreamBooth are hateful meme variants that present the features of the original hateful meme and the target individual/community; these generated images are comparable to hateful meme variants collected from the real world. Overall, our results demonstrate that the danger of large-scale generation of unsafe images is imminent. We discuss several mitigating measures, such as curating training data, regulating prompts, and implementing safety filters, and encourage better safeguard tools to be developed to prevent unsafe generation.1 Our code is available at https://github.com/YitingQu/unsafe-diffusion.",
        "authors": [
            "Y. Qu",
            "Xinyue Shen",
            "Xinlei He",
            "M. Backes",
            "Savvas Zannettou",
            "Yang Zhang"
        ],
        "citations": 81,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models",
        "abstract": "Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to $\\ell_2$-accurate estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model), we derive a convergence rate proportional to $1/\\sqrt{T}$, matching the state-of-the-art theory. Imposing only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), our results characterize how $\\ell_2$ score estimation errors affect the quality of the data generation processes. In contrast to prior works, our theory is developed based on an elementary yet versatile non-asymptotic approach without resorting to toolboxes for SDEs and ODEs. Further, we design two accelerated variants, improving the convergence to $1/T^2$ for the ODE-based sampler and $1/T$ for the DDPM-type sampler, which might be of independent theoretical and empirical interest.",
        "authors": [
            "Gen Li",
            "Yuting Wei",
            "Yuxin Chen",
            "Yuejie Chi"
        ],
        "citations": 45,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models",
        "abstract": "Image inpainting refers to the task of generating a complete, natural image based on a partially revealed reference image. Recently, many research interests have been focused on addressing this problem using fixed diffusion models. These approaches typically directly replace the revealed region of the intermediate or final generated images with that of the reference image or its variants. However, since the unrevealed regions are not directly modified to match the context, it results in incoherence between revealed and unrevealed regions. To address the incoherence problem, a small number of methods introduce a rigorous Bayesian framework, but they tend to introduce mismatches between the generated and the reference images due to the approximation errors in computing the posterior distributions. In this paper, we propose COPAINT, which can coherently inpaint the whole image without introducing mismatches. COPAINT also uses the Bayesian framework to jointly modify both revealed and unrevealed regions, but approximates the posterior distribution in a way that allows the errors to gradually drop to zero throughout the denoising steps, thus strongly penalizing any mismatches with the reference image. Our experiments verify that COPAINT can outperform the existing diffusion-based methods under both objective and subjective metrics. The codes are available at https://github.com/UCSB-NLP-Chang/CoPaint/.",
        "authors": [
            "Guanhua Zhang",
            "Jiabao Ji",
            "Yang Zhang",
            "Mo Yu",
            "T. Jaakkola",
            "Shiyu Chang"
        ],
        "citations": 48,
        "references": 57,
        "year": 2023
    },
    {
        "title": "DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models",
        "abstract": "Diffusion models have shown remarkable success in a variety of downstream generative tasks, yet remain under-explored in the important and challenging expressive talking head generation. In this work, we propose a DreamTalk framework to fulfill this gap, which employs meticulous de-sign to unlock the potential of diffusion models in generating expressive talking heads. Specifically, DreamTalk consists of three crucial components: a denoising network, a style-aware lip expert, and a style predictor. The diffusion-based denoising network is able to consistently synthesize high-quality audio-driven face motions across diverse expressions. To enhance the expressiveness and accuracy of lip motions, we introduce a style-aware lip expert that can guide lip-sync while being mindful of the speaking styles. To eliminate the need for expression reference video or text, an extra diffusion-based style predictor is utilized to predict the target expression directly from the audio. By this means, DreamTalk can harness powerful diffusion models to generate expressive faces effectively and reduce the reliance on expensive style references. Experimental re-sults demonstrate that DreamTalk is capable of generating photo-realistic talking faces with diverse speaking styles and achieving accurate lip motions, surpassing existing state-of-the-art counterparts.",
        "authors": [
            "Yifeng Ma",
            "Shiwei Zhang",
            "Jiayu Wang",
            "Xiang Wang",
            "Yingya Zhang",
            "Zhidong Deng"
        ],
        "citations": 37,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Denoising Diffusion Bridge Models",
        "abstract": "Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score. Our method naturally unifies several classes of generative models, such as score-based diffusion models and OT-Flow-Matching, allowing us to adapt existing design and architectural choices to our more general problem. Empirically, we apply DDBMs to challenging image datasets in both pixel and latent space. On standard image translation problems, DDBMs achieve significant improvement over baseline methods, and, when we reduce the problem to image generation by setting the source distribution to random noise, DDBMs achieve comparable FID scores to state-of-the-art methods despite being built for a more general task.",
        "authors": [
            "Linqi Zhou",
            "Aaron Lou",
            "Samar Khanna",
            "Stefano Ermon"
        ],
        "citations": 34,
        "references": 59,
        "year": 2023
    },
    {
        "title": "A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models",
        "abstract": "We provide a theoretical justification for sample recovery using diffusion based image inpainting in a linear model setting. While most inpainting algorithms require retraining with each new mask, we prove that diffusion based inpainting generalizes well to unseen masks without retraining. We analyze a recently proposed popular diffusion based inpainting algorithm called RePaint (Lugmayr et al., 2022), and show that it has a bias due to misalignment that hampers sample recovery even in a two-state diffusion process. Motivated by our analysis, we propose a modified RePaint algorithm we call RePaint$^+$ that provably recovers the underlying true sample and enjoys a linear rate of convergence. It achieves this by rectifying the misalignment error present in drift and dispersion of the reverse process. To the best of our knowledge, this is the first linear convergence result for a diffusion based image inpainting algorithm.",
        "authors": [
            "Litu Rout",
            "Advait Parulekar",
            "C. Caramanis",
            "S. Shakkottai"
        ],
        "citations": 62,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Likelihood-Based Diffusion Language Models",
        "abstract": "Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.",
        "authors": [
            "Ishaan Gulrajani",
            "Tatsunori Hashimoto"
        ],
        "citations": 28,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild",
        "abstract": "Diffusion models have shown promising results on single-image super-resolution and other image- to-image translation tasks. Despite this success, they have not outperformed state-of-the-art GAN models on the more challenging blind super-resolution task, where the input images are out of distribution, with unknown degradations. This paper introduces SR3+, a diffusion-based model for blind super-resolution, establishing a new state-of-the-art. To this end, we advocate self-supervised training with a combination of composite, parameterized degradations for self-supervised training, and noise-conditioing augmentation during training and testing. With these innovations, a large-scale convolutional architecture, and large-scale datasets, SR3+ greatly outperforms SR3. It outperforms Real-ESRGAN when trained on the same data, with a DRealSR FID score of 36.82 vs. 37.22, which further improves to FID of 32.37 with larger models, and further still with larger training sets.",
        "authors": [
            "Hshmat Sahak",
            "Daniel Watson",
            "Chitwan Saharia",
            "David J. Fleet"
        ],
        "citations": 37,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Analyzing Bias in Diffusion-based Face Generation Models",
        "abstract": "Diffusion models are becoming increasingly popular in synthetic data generation and image editing applications. However, these models can amplify existing biases and propagate them to downstream applications. Therefore, it is crucial to understand the sources of bias in their outputs. In this paper, we investigate the presence of bias in diffusion-based face generation models with respect to attributes such as gender, race, and age. Moreover, we examine how dataset size affects the attribute composition and perceptual quality of both diffusion and Generative Adversarial Network (GAN) based face generation models across various attribute classes. Our findings suggest that diffusion models tend to worsen distribution bias in the training data for various attributes, which is heavily influenced by the size of the dataset. Conversely, GAN models trained on balanced datasets with a larger number of samples show less bias across different attributes.",
        "authors": [
            "Malsha V. Perera",
            "Vishal M. Patel"
        ],
        "citations": 32,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Classifier-Free Diffusion Guidance",
        "abstract": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
        "authors": [
            "Jonathan Ho"
        ],
        "citations": 1000,
        "references": 25,
        "year": 2022
    },
    {
        "title": "Antigen-Specific Antibody Design and Optimization with Diffusion-Based Generative Models for Protein Structures",
        "abstract": "Antibodies are immune system proteins that protect the host by binding to specific antigens such as viruses and bacteria. The binding between antibodies and antigens is mainly determined by the complementarity-determining regions (CDR) of the antibodies. In this work, we develop a deep generative model that jointly models sequences and structures of CDRs based on diffusion probabilistic models and equivariant neural networks. Our method is the first deep learning-based method that generates antibodies explicitly targeting specific antigen structures and is one of the earliest diffusion probabilistic models for protein structures. The model is a “Swiss Army Knife” capable of sequence-structure co-design, sequence design for given backbone structures, and antibody optimization. We conduct extensive experiments to evaluate the quality of both sequences and structures of designed antibodies. We find that our model could yield competitive results in binding affinity measured by biophysical energy functions and other protein design metrics.",
        "authors": [
            "Shitong Luo",
            "Yufeng Su",
            "Xingang Peng",
            "Sheng Wang",
            "Jian Peng",
            "Jianzhu Ma"
        ],
        "citations": 165,
        "references": 64,
        "year": 2022
    },
    {
        "title": "Speech Enhancement and Dereverberation With Diffusion-Based Generative Models",
        "abstract": "In this work, we build upon our previous publication and use diffusion-based generative models for speech enhancement. We present a detailed overview of the diffusion process that is based on a stochastic differential equation and delve into an extensive theoretical examination of its implications. Opposed to usual conditional generation tasks, we do not start the reverse process from pure Gaussian noise but from a mixture of noisy speech and Gaussian noise. This matches our forward process which moves from clean speech to noisy speech by including a drift term. We show that this procedure enables using only 30 diffusion steps to generate high-quality clean speech estimates. By adapting the network architecture, we are able to significantly improve the speech enhancement performance, indicating that the network, rather than the formalism, was the main limitation of our original approach. In an extensive cross-dataset evaluation, we show that the improved method can compete with recent discriminative models and achieves better generalization when evaluating on a different corpus than used for training. We complement the results with an instrumental evaluation using real-world noisy recordings and a listening experiment, in which our proposed method is rated best. Examining different sampler configurations for solving the reverse process allows us to balance the performance and computational speed of the proposed method. Moreover, we show that the proposed method is also suitable for dereverberation and thus not limited to additive background noise removal.",
        "authors": [
            "Julius Richter",
            "Simon Welker",
            "Jean-Marie Lemercier",
            "Bunlong Lay",
            "Timo Gerkmann"
        ],
        "citations": 150,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion",
        "abstract": "Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose Copilot4D, a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer as discrete diffusion and enhance it with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, Copilot4D reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argoverse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of GPT-like unsupervised learning for robotics.",
        "authors": [
            "Lunjun Zhang",
            "Yuwen Xiong",
            "Ze Yang",
            "Sergio Casas",
            "Rui Hu",
            "R. Urtasun"
        ],
        "citations": 39,
        "references": 65,
        "year": 2023
    },
    {
        "title": "OMS-DPM: Optimizing the Model Schedule for Diffusion Probabilistic Models",
        "abstract": "Diffusion probabilistic models (DPMs) are a new class of generative models that have achieved state-of-the-art generation quality in various domains. Despite the promise, one major drawback of DPMs is the slow generation speed due to the large number of neural network evaluations required in the generation process. In this paper, we reveal an overlooked dimension -- model schedule -- for optimizing the trade-off between generation quality and speed. More specifically, we observe that small models, though having worse generation quality when used alone, could outperform large models in certain generation steps. Therefore, unlike the traditional way of using a single model, using different models in different generation steps in a carefully designed \\emph{model schedule} could potentially improve generation quality and speed \\emph{simultaneously}. We design OMS-DPM, a predictor-based search algorithm, to optimize the model schedule given an arbitrary generation time budget and a set of pre-trained models. We demonstrate that OMS-DPM can find model schedules that improve generation quality and speed than prior state-of-the-art methods across CIFAR-10, CelebA, ImageNet, and LSUN datasets. When applied to the public checkpoints of the Stable Diffusion model, we are able to accelerate the sampling by 2$\\times$ while maintaining the generation quality.",
        "authors": [
            "En-hao Liu",
            "Xuefei Ning",
            "Zi-Han Lin",
            "Huazhong Yang",
            "Yu Wang"
        ],
        "citations": 27,
        "references": 42,
        "year": 2023
    },
    {
        "title": "SafeDiffuser: Safe Planning with Diffusion Probabilistic Models",
        "abstract": "Diffusion model-based approaches have shown promise in data-driven planning, but there are no safety guarantees, thus making it hard to be applied for safety-critical applications. To address these challenges, we propose a new method, called SafeDiffuser, to ensure diffusion probabilistic models satisfy specifications by using a class of control barrier functions. The key idea of our approach is to embed the proposed finite-time diffusion invariance into the denoising diffusion procedure, which enables trustworthy diffusion data generation. Moreover, we demonstrate that our finite-time diffusion invariance method through generative models not only maintains generalization performance but also creates robustness in safe data generation. We test our method on a series of safe planning tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, with results showing the advantages of robustness and guarantees over vanilla diffusion models.",
        "authors": [
            "Wei Xiao",
            "Tsun-Hsuan Wang",
            "Chuang Gan",
            "Daniela Rus"
        ],
        "citations": 20,
        "references": 39,
        "year": 2023
    },
    {
        "title": "DreamFusion: Text-to-3D using 2D Diffusion",
        "abstract": "Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.",
        "authors": [
            "Ben Poole",
            "Ajay Jain",
            "J. Barron",
            "B. Mildenhall"
        ],
        "citations": 1000,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models",
        "abstract": "The imputation of missing values represents a significant obstacle for many real-world data analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation model that relies on two emerging technologies, (conditional) diffusion models as state-of-the-art generative models and structured state space models as internal model architecture, which are particularly suited to capture long-term dependencies in time series data. We demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation and forecasting performance on a broad range of data sets and different missingness scenarios, including the challenging blackout-missing scenarios, where prior approaches failed to provide meaningful results.",
        "authors": [
            "Juan Miguel Lopez Alcaraz",
            "Nils Strodthoff"
        ],
        "citations": 133,
        "references": 68,
        "year": 2022
    },
    {
        "title": "Learning to Schedule in Diffusion Probabilistic Models",
        "abstract": "Recently, the field of generative models has seen a significant advancement with the introduction of Diffusion Probabilistic Models (DPMs). The Denoising Diffusion Implicit Model (DDIM) was designed to reduce computational time by skipping a number of steps in the inference process of DPMs. However, the hand-crafted sampling schedule in DDIM, which relies on human expertise, has its limitations in considering all relevant factors in the sampling process. Additionally, the assumption that all instances should have the same schedule is not always valid. To address these problems, this paper proposes a method that leverages reinforcement learning to automatically search for an optimal sampling schedule for DPMs. This is achieved by a policy network that predicts the next step to visit based on the current state of the noisy image. The optimization of the policy network is accomplished using an episodic actor-critic framework, which incorporates reinforcement learning. Empirical results demonstrate the superiority of our approach over various datasets with different timesteps. We also observe that the trained sampling schedule has a strong generalization ability across different DPM baselines.",
        "authors": [
            "Yunke Wang",
            "Xiyu Wang",
            "Anh-Dung Dinh",
            "Bo Du",
            "Charles Xu"
        ],
        "citations": 22,
        "references": 69,
        "year": 2023
    },
    {
        "title": "gDDIM: Generalized denoising diffusion implicit models",
        "abstract": "Our goal is to extend the denoising diffusion implicit model (DDIM) to general diffusion models~(DMs) besides isotropic diffusions. Instead of constructing a non-Markov noising process as in the original DDIM, we examine the mechanism of DDIM from a numerical perspective. We discover that the DDIM can be obtained by using some specific approximations of the score when solving the corresponding stochastic differential equation. We present an interpretation of the accelerating effects of DDIM that also explains the advantages of a deterministic sampling scheme over the stochastic one for fast sampling. Building on this insight, we extend DDIM to general DMs, coined generalized DDIM (gDDIM), with a small but delicate modification in parameterizing the score network. We validate gDDIM in two non-isotropic DMs: Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model (CLD). We observe more than 20 times acceleration in BDM. In the CLD, a diffusion model by augmenting the diffusion process with velocity, our algorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of score function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs. Code is available at https://github.com/qsh-zh/gDDIM",
        "authors": [
            "Qinsheng Zhang",
            "Molei Tao",
            "Yongxin Chen"
        ],
        "citations": 95,
        "references": 50,
        "year": 2022
    },
    {
        "title": "Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?",
        "abstract": "After many researchers observed fruitfulness from the recent diffusion probabilistic model, its effectiveness in image generation is actively studied these days. In this paper, our objective is to evaluate the potential of diffusion probabilistic models for 3D human motion-related tasks. To this end, this pa-per presents a study of employing diffusion probabilistic models to predict future 3D human motion(s) from the previously observed motion. Based on the Human 3.6M and HumanEva-I datasets, our results show that diffusion probabilistic models are competitive for both single (deterministic) and multiple (stochastic) 3D motion prediction tasks, after finishing a single training process. In addition, we find out that diffusion probabilistic models can offer an attractive compromise, since they can strike the right balance between the likelihood and diversity of the predicted future motions. Our code is publicly available on the project website: https://sites.google.com/view/diffusion-motion-prediction.",
        "authors": [
            "Hyemin Ahn",
            "Esteve Valls Mascaro",
            "Dongheui Lee"
        ],
        "citations": 21,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models",
        "abstract": "There has been considerable recent progress in designing new proteins using deep learning methods1–9. Despite this progress, a general deep learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modeling but limited success when applied to protein modeling, likely due to the complexity of protein backbone geometry and sequence-structure relationships. Here we show that by fine tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold Diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of new designs. In a manner analogous to networks which produce images from user-specified inputs, RFdiffusion enables the design of diverse, complex, functional proteins from simple molecular specifications.",
        "authors": [
            "Joseph L. Watson",
            "David Juergens",
            "N. Bennett",
            "Brian L. Trippe",
            "Jason Yim",
            "Helen E. Eisenach",
            "Woody Ahern",
            "Andrew J. Borst",
            "R. Ragotte",
            "L. Milles",
            "B. Wicky",
            "Nikita Hanikel",
            "S. Pellock",
            "A. Courbet",
            "W. Sheffler",
            "Jue Wang",
            "Preetham Venkatesh",
            "Isaac Sappington",
            "Susana Vázquez Torres",
            "A. Lauko",
            "Valentin De Bortoli",
            "Emile Mathieu",
            "R. Barzilay",
            "T. Jaakkola",
            "F. DiMaio",
            "M. Baek",
            "D. Baker"
        ],
        "citations": 161,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
        "abstract": "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot’s visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 15 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details are available (diffusion-policy.cs.columbia.edu).",
        "authors": [
            "Cheng Chi",
            "S. Feng",
            "Yilun Du",
            "Zhenjia Xu",
            "Eric A. Cousineau",
            "B. Burchfiel",
            "Shuran Song"
        ],
        "citations": 659,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation",
        "abstract": "We introduce LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models.",
        "authors": [
            "Peize Sun",
            "Yi Jiang",
            "Shoufa Chen",
            "Shilong Zhang",
            "Bingyue Peng",
            "Ping Luo",
            "Zehuan Yuan"
        ],
        "citations": 84,
        "references": 98,
        "year": 2024
    },
    {
        "title": "Let us Build Bridges: Understanding and Extending Diffusion Generative Models",
        "abstract": "Diffusion-based generative models have achieved promising results recently, but raise an array of open questions in terms of conceptual understanding, theoretical analysis, algorithm improvement and extensions to discrete, structured, non-Euclidean domains. This work tries to re-exam the overall framework, in order to gain better theoretical understandings and develop algorithmic extensions for data from arbitrary domains. By viewing diffusion models as latent variable models with unobserved diffusion trajectories and applying maximum likelihood estimation (MLE) with latent trajectories imputed from an auxiliary distribution, we show that both the model construction and the imputation of latent trajectories amount to constructing diffusion bridge processes that achieve deterministic values and constraints at end point, for which we provide a systematic study and a suit of tools. Leveraging our framework, we present 1) a first theoretical error analysis for learning diffusion generation models, and 2) a simple and unified approach to learning on data from different discrete and constrained domains. Experiments show that our methods perform superbly on generating images, semantic segments and 3D point clouds.",
        "authors": [
            "Xingchao Liu",
            "Lemeng Wu",
            "Mao Ye",
            "Qiang Liu"
        ],
        "citations": 69,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models",
        "abstract": "Proteins are macromolecules that mediate a significant fraction of the cellular processes that underlie life. An important task in bioengineering is designing proteins with specific 3D structures and chemical properties which enable targeted functions. To this end, we introduce a generative model of both protein structure and sequence that can operate at significantly larger scales than previous molecular generative modeling approaches. The model is learned entirely from experimental data and conditions its generation on a compact specification of protein topology to produce a full-atom backbone configuration as well as sequence and side-chain predictions. We demonstrate the quality of the model via qualitative and quantitative analysis of its samples. Videos of sampling trajectories are available at https://nanand2.github.io/proteins .",
        "authors": [
            "N. Anand",
            "Tudor Achim"
        ],
        "citations": 160,
        "references": 37,
        "year": 2022
    },
    {
        "title": "Lumiere: A Space-Time Diffusion Model for Video Generation",
        "abstract": "We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.",
        "authors": [
            "Omer Bar-Tal",
            "Hila Chefer",
            "Omer Tov",
            "Charles Herrmann",
            "Roni Paiss",
            "Shiran Zada",
            "Ariel Ephrat",
            "Junhwa Hur",
            "Yuanzhen Li",
            "T. Michaeli",
            "Oliver Wang",
            "Deqing Sun",
            "Tali Dekel",
            "Inbar Mosseri"
        ],
        "citations": 147,
        "references": 70,
        "year": 2024
    },
    {
        "title": "JPEG Artifact Correction using Denoising Diffusion Restoration Models",
        "abstract": "Diffusion models can be used as learned priors for solving various inverse problems. However, most existing approaches are restricted to linear inverse problems, limiting their applicability to more general cases. In this paper, we build upon Denoising Diffusion Restoration Models (DDRM) and propose a method for solving some non-linear inverse problems. We leverage the pseudo-inverse operator used in DDRM and generalize this concept for other measurement operators, which allows us to use pre-trained unconditional diffusion models for applications such as JPEG artifact correction. We empirically demonstrate the effectiveness of our approach across various quality factors, attaining performance levels that are on par with state-of-the-art methods trained specifically for the JPEG restoration task.",
        "authors": [
            "Bahjat Kawar",
            "Jiaming Song",
            "Stefano Ermon",
            "Michael Elad"
        ],
        "citations": 49,
        "references": 41,
        "year": 2022
    },
    {
        "title": "Subspace Diffusion Generative Models",
        "abstract": null,
        "authors": [
            "Bowen Jing",
            "Gabriele Corso",
            "Renato Berlinghieri",
            "Tommi Jaakkola"
        ],
        "citations": 70,
        "references": 22,
        "year": 2022
    },
    {
        "title": "Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models",
        "abstract": "—MRI and CT are most widely used medical imaging modalities. It is often necessary to acquire multi-modality images for diagnosis and treatment such as radiotherapy planning. However, multi-modality imaging is not only costly but also introduces misalignment between MRI and CT images. To address this challenge, computational conversion is a viable approach between MRI and CT images, especially from MRI to CT images. In this paper, we propose to use an emerging deep learning framework called diffusion and score-matching models in this context. Speciﬁcally, we adapt denoising diffusion probabilistic and score-matching models, use four different sam- pling strategies, and compare their performance metrics with that using a convolutional neural network and a generative adversarial network model. Our results show that the diffusion and score-matching models generate better synthetic CT images than the CNN and GAN models. Furthermore, we investigate the uncertainties associated with the diffusion and score-matching networks using the Monte-Carlo method, and improve the results by averaging their Monte-Carlo outputs. Our study suggests that diffusion and score-matching models are powerful to generate high quality images conditioned on an image obtained using a complementary imaging modality, analytically rigorous with clear explainability, and highly competitive with CNNs and GANs for image synthesis.",
        "authors": [
            "Qing Lyu",
            "Ge Wang"
        ],
        "citations": 71,
        "references": 51,
        "year": 2022
    },
    {
        "title": "How Much Is Enough? A Study on Diffusion Times in Score-Based Generative Models",
        "abstract": "Score-based diffusion models are a class of generative models whose dynamics is described by stochastic differential equations that map noise into data. While recent works have started to lay down a theoretical foundation for these models, a detailed understanding of the role of the diffusion time T is still lacking. Current best practice advocates for a large T to ensure that the forward dynamics brings the diffusion sufficiently close to a known and simple noise distribution; however, a smaller value of T should be preferred for a better approximation of the score-matching objective and higher computational efficiency. Starting from a variational interpretation of diffusion models, in this work we quantify this trade-off and suggest a new method to improve quality and efficiency of both training and sampling, by adopting smaller diffusion times. Indeed, we show how an auxiliary model can be used to bridge the gap between the ideal and the simulated forward dynamics, followed by a standard reverse diffusion process. Empirical results support our analysis; for image data, our method is competitive with regard to the state of the art, according to standard sample quality metrics and log-likelihood.",
        "authors": [
            "Giulio Franzese",
            "Simone Rossi",
            "Lixuan Yang",
            "A. Finamore",
            "Dario Rossi",
            "M. Filippone",
            "Pietro Michiardi"
        ],
        "citations": 41,
        "references": 48,
        "year": 2022
    },
    {
        "title": "MVDream: Multi-view Diffusion for 3D Generation",
        "abstract": "We introduce MVDream, a diffusion model that is able to generate consistent multi-view images from a given text prompt. Learning from both 2D and 3D data, a multi-view diffusion model can achieve the generalizability of 2D diffusion models and the consistency of 3D renderings. We demonstrate that such a multi-view diffusion model is implicitly a generalizable 3D prior agnostic to 3D representations. It can be applied to 3D generation via Score Distillation Sampling, significantly enhancing the consistency and stability of existing 2D-lifting methods. It can also learn new concepts from a few 2D examples, akin to DreamBooth, but for 3D generation.",
        "authors": [
            "Yichun Shi",
            "Peng Wang",
            "Jianglong Ye",
            "Mai Long",
            "Kejie Li",
            "X. Yang"
        ],
        "citations": 446,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Diffusion Causal Models for Counterfactual Estimation",
        "abstract": "We consider the task of counterfactual estimation from observational imaging data given a known causal structure. In particular, quantifying the causal effect of interventions for high-dimensional data with neural networks remains an open challenge. Herein we propose Diff-SCM, a deep structural causal model that builds on recent advances of generative energy-based models. In our setting, inference is performed by iteratively sampling gradients of the marginal and conditional distributions entailed by the causal model. Counterfactual estimation is achieved by firstly inferring latent variables with deterministic forward diffusion, then intervening on a reverse diffusion process using the gradients of an anti-causal predictor w.r.t the input. Furthermore, we propose a metric for evaluating the generated counterfactuals. We find that Diff-SCM produces more realistic and minimal counterfactuals than baselines on MNIST data and can also be applied to ImageNet data. Code is available https://github.com/vios-s/Diff-SCM.",
        "authors": [
            "Pedro Sanchez",
            "S. Tsaftaris"
        ],
        "citations": 63,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models",
        "abstract": "Diffusion probabilistic models (DPMs) are a class of powerful deep generative models (DGMs). Despite their success, the iterative generation process over the full timesteps is much less efficient than other DGMs such as GANs. Thus, the generation performance on a subset of timesteps is crucial, which is greatly influenced by the covariance design in DPMs. In this work, we consider diagonal and full covariances to improve the expressive power of DPMs. We derive the optimal result for such covariances, and then correct it when the mean of DPMs is imperfect. Both the optimal and the corrected ones can be decomposed into terms of conditional expectations over functions of noise. Building upon it, we propose to estimate the optimal covariance and its correction given imperfect mean by learning these conditional expectations. Our method can be applied to DPMs with both discrete and continuous timesteps. We consider the diagonal covariance in our implementation for computational efficiency. For an efficient practical implementation, we adopt a parameter sharing scheme and a two-stage training process. Empirically, our method outperforms a wide variety of covariance design on likelihood results, and improves the sample quality especially on a small number of timesteps.",
        "authors": [
            "Fan Bao",
            "Chongxuan Li",
            "Jiacheng Sun",
            "Jun Zhu",
            "Bo Zhang"
        ],
        "citations": 62,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Microstructure reconstruction using diffusion-based generative models",
        "abstract": "Microstructure reconstruction has been an essential part of computational material engineering to reveal the relationship between microstructures and material properties. However, finding a general solution for microstructure characterization and reconstruction (MCR) tasks is still challenging, although there have been many attempts such as the descriptor-based MCR methods. To address this generality problem, the denoising diffusion models are first employed for the microstructure reconstruction task in this study. The applicability of the diffusion-based models is validated with several types of microstructures (e.g., polycrystalline alloy, carbonate, ceramics, copolymer, fiber composite, etc.) that have different morphological characteristics. The quality of the generated images is assessed with the quantitative evaluation metrics (FID score, precision, and recall) and the conventional statistical microstructure descriptors. Furthermore, the formulation of implicit probabilistic models (which yields non-Markovian diffusion processes) is adopted to accelerate the sampling process, thereby controlling the computational cost considering the practicability and reliability. The results show that the denoising diffusion models are well applicable to the reconstruction of various types of microstructures with different spatial distributions and morphological features. The diffusion-based approach provides a stable training process with simple implementation for generating visually similar and statistically equivalent microstructures. In these regards, the diffusion model has great potential to be used as a universal microstructure reconstruction method for handling complex microstructures for materials science.",
        "authors": [
            "Kang-Hyun Lee",
            "G. Yun"
        ],
        "citations": 35,
        "references": 85,
        "year": 2022
    },
    {
        "title": "Three-Dimensional Medical Image Synthesis with Denoising Diffusion Probabilistic Models",
        "abstract": "Denoising diffusion probabilistic models (DDPM) have recently shown superior performance in image synthesis and have been extensively studied in various image processing tasks. In this work, we propose a 3D-DDPM for generating three-dimensional (3D) medical images. Different from previous studies, to the best of our knowledge, this work presents the first attempt to investigate the DDPM to enable 3D medical image synthesis. Our study examined the generation of high-resolution magnetic resonance images (MRI) of brain tumors. The proposed method is evaluated through experiments on a semi-public dataset, with both quantitative and qualitative tests showing promising results. Our code will be publicly available at https://github.com/DL-Circle/3D-DDPM",
        "authors": [
            "Zolnamar Dorjsembe",
            "Furen Xiao"
        ],
        "citations": 67,
        "references": 4,
        "year": 2022
    },
    {
        "title": "MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation",
        "abstract": "Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes. Project webpage: https://multidiffusion.github.io",
        "authors": [
            "Omer Bar-Tal",
            "Lior Yariv",
            "Y. Lipman",
            "Tali Dekel"
        ],
        "citations": 281,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Adversarial Diffusion Distillation",
        "abstract": "We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality. We use score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps. ADD is the first method to unlock single-step, real-time image synthesis with foundation models. Code and weights available under https://github.com/Stability-AI/generative-models and https://huggingface.co/stabilityai/ .",
        "authors": [
            "Axel Sauer",
            "Dominik Lorenz",
            "A. Blattmann",
            "Robin Rombach"
        ],
        "citations": 219,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Diffusion Generative Models in Infinite Dimensions",
        "abstract": "Diffusion generative models have recently been applied to domains where the available data can be seen as a discretization of an underlying function, such as audio signals or time series. However, these models operate directly on the discretized data, and there are no semantics in the modeling process that relate the observed data to the underlying functional forms. We generalize diffusion models to operate directly in function space by developing the foundational theory for such models in terms of Gaussian measures on Hilbert spaces. A significant benefit of our function space point of view is that it allows us to explicitly specify the space of functions we are working in, leading us to develop methods for diffusion generative modeling in Sobolev spaces. Our approach allows us to perform both unconditional and conditional generation of function-valued data. We demonstrate our methods on several synthetic and real-world benchmarks.",
        "authors": [
            "Gavin Kerrigan",
            "Justin Ley",
            "Padhraic Smyth"
        ],
        "citations": 25,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Exploiting Diffusion Prior for Real-World Image Super-Resolution",
        "abstract": "We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution (SR). Specifically, by employing our time-aware encoder, we can achieve promising restoration results without altering the pre-trained synthesis model, thereby preserving the generative prior and minimizing training cost. To remedy the loss of fidelity caused by the inherent stochasticity of diffusion models, we introduce a controllable feature wrapping module that allows users to balance quality and fidelity by simply adjusting a scalar value during the inference process. Moreover, we develop a progressive aggregation sampling strategy to overcome the fixed-size constraints of pre-trained diffusion models, enabling adaptation to resolutions of any size. A comprehensive evaluation of our method using both synthetic and real-world benchmarks demonstrates its superiority over current state-of-the-art approaches.",
        "authors": [
            "Jianyi Wang",
            "Zongsheng Yue",
            "Shangchen Zhou",
            "Kelvin C. K. Chan",
            "Chen Change Loy"
        ],
        "citations": 180,
        "references": 106,
        "year": 2023
    },
    {
        "title": "Medical Diffusion: Denoising Diffusion Probabilistic Models for 3D Medical Image Generation",
        "abstract": "Recent advances in computer vision have shown promising results in image generation. Diffusion probabilistic models in particular have generated realistic images from textual input, as demonstrated by DALL-E 2, Imagen and Stable Diffusion. However, their use in medicine, where image data typically comprises three-dimensional volumes, has not been systematically evaluated. Synthetic images may play a crucial role in privacy preserving artificial intelligence and can also be used to augment small datasets. Here we show that diffusion probabilistic models can synthesize high quality medical imaging data, which we show for Magnetic Resonance Images (MRI) and Computed Tomography (CT) images. We provide quantitative measurements of their performance through a reader study with two medical experts who rated the quality of the synthesized images in three categories: Realistic image appearance, anatomical correctness and consistency between slices. Furthermore, we demonstrate that synthetic images can be used in a self-supervised pre-training and improve the performance of breast segmentation models when data is scarce (dice score 0.91 vs. 0.95 without vs. with synthetic data). The code is publicly available on GitHub: https://github.com/FirasGit/medicaldiffusion.",
        "authors": [
            "Firas Khader",
            "Gustav Mueller-Franzes",
            "Soroosh Tayebi Arasteh",
            "T. Han",
            "Christoph Haarburger",
            "M. Schulze-Hagen",
            "P. Schad",
            "S. Engelhardt",
            "B. Baessler",
            "S. Foersch",
            "J. Stegmaier",
            "C. Kuhl",
            "S. Nebelung",
            "Jakob Nikolas Kather",
            "D. Truhn"
        ],
        "citations": 54,
        "references": 35,
        "year": 2022
    },
    {
        "title": "Innovation Diffusion Processes: Concepts, Models, and Predictions",
        "abstract": "Innovation diffusion processes have attracted considerable research attention for their interdisciplinary character, which combines theories and concepts from disciplines such as mathematics, physics, statistics, social sciences, marketing, economics, and technological forecasting. The formal representation of innovation diffusion processes historically used epidemic models borrowed from biology, departing from the logistic equation, under the hypothesis that an innovation spreads in a social system through communication between people like an epidemic through contagion. This review integrates basic innovation diffusion models built upon the Bass model, primarily from the marketing literature, with a number of ideas from the epidemiological literature in order to offer a different perspective on innovation diffusion by focusing on critical diffusions, which are key for the progress of human communities. The article analyzes three key issues: barriers to diffusion, centrality of word-of-mouth, and the management of policy interventions to assist beneficial diffusions and to prevent harmful ones. We focus on deterministic innovation diffusion models described by ordinary differential equations. Expected final online publication date for the Annual Review of Statistics and Its Application, Volume 10 is March 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",
        "authors": [
            "M. Guidolin",
            "P. Manfredi"
        ],
        "citations": 26,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
        "abstract": "Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: https://github.com/YangLing0818/RPG-DiffusionMaster",
        "authors": [
            "Ling Yang",
            "Zhaochen Yu",
            "Chenlin Meng",
            "Minkai Xu",
            "Stefano Ermon",
            "Bin Cui"
        ],
        "citations": 73,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
        "abstract": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available at https://github.com/microsoft/ProphetNet/tree/master/GENIE.",
        "authors": [
            "Zheng-Wen Lin",
            "Yeyun Gong",
            "Yelong Shen",
            "Tong Wu",
            "Zhihao Fan",
            "Chen Lin",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "citations": 47,
        "references": 47,
        "year": 2022
    },
    {
        "title": "SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion",
        "abstract": "We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works.",
        "authors": [
            "Vikram S. Voleti",
            "Chun-Han Yao",
            "Mark Boss",
            "Adam Letts",
            "David Pankratz",
            "Dmitry Tochilkin",
            "Christian Laforte",
            "Robin Rombach",
            "Varun Jampani"
        ],
        "citations": 100,
        "references": 60,
        "year": 2024
    },
    {
        "title": "Truncated Diffusion Probabilistic Models",
        "abstract": "Employing a forward diffusion chain to gradually map the data to a noise distribution, diffusion-based generative models learn how to generate the data by inferring a reverse diffusion chain. However, this approach is slow and costly because it needs many forward and reverse steps. We propose a faster and cheaper approach that adds noise not until the data become pure random noise, but until they reach a hidden noisy data distribution that we can confidently learn. Then, we use fewer reverse steps to generate data by starting from this hidden distribution that is made similar to the noisy data. We reveal that the proposed model can be cast as an adversarial auto-encoder empowered by both the diffusion process and a learnable implicit prior. Experimental results show even with a significantly smaller number of reverse diffusion steps, the proposed truncated diffusion probabilistic models can provide consistent improvements over the non-truncated ones in terms of performance in both unconditional and text-guided image generations.",
        "authors": [
            "Huangjie Zheng",
            "Pengcheng He",
            "Weizhu Chen",
            "Mingyuan Zhou"
        ],
        "citations": 41,
        "references": 101,
        "year": 2022
    },
    {
        "title": "Pyramidal Denoising Diffusion Probabilistic Models",
        "abstract": "Recently, diffusion model have demonstrated impressive image generation performances, and have been extensively studied in various computer vision tasks. Unfortunately, training and evaluating diffusion models consume a lot of time and computational resources. To address this problem, here we present a novel pyramidal diffusion model that can generate high resolution images starting from much coarser resolution images using a {\\em single} score function trained with a positional embedding. This enables a neural network to be much lighter and also enables time-efficient image generation without compromising its performances. Furthermore, we show that the proposed approach can be also efficiently used for multi-scale super-resolution problem using a single score function.",
        "authors": [
            "Dohoon Ryu",
            "Jong-Chul Ye"
        ],
        "citations": 22,
        "references": 46,
        "year": 2022
    },
    {
        "title": "Diffusion Probabilistic Models beat GANs on Medical Images",
        "abstract": null,
        "authors": [
            "Gustav Müller-Franzes",
            "J. Niehues",
            "Firas Khader",
            "Soroosh Tayebi Arasteh",
            "Christoph Haarburger",
            "C. Kuhl",
            "Tian Wang",
            "T. Han",
            "S. Nebelung",
            "Jakob Nikolas Kather",
            "D. Truhn"
        ],
        "citations": 57,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Protein generation with evolutionary diffusion: sequence is all you need",
        "abstract": "Deep generative models are increasingly powerful tools for the in silico design of novel proteins. Recently, a family of generative models called diffusion models has demonstrated the ability to generate biologically plausible proteins that are dissimilar to any actual proteins seen in nature, enabling unprecedented capability and control in de novo protein design. However, current state-of-the-art diffusion models generate protein structures, which limits the scope of their training data and restricts generations to a small and biased subset of protein design space. Here, we introduce a general-purpose diffusion framework, EvoDiff, that combines evolutionary-scale data with the distinct conditioning capabilities of diffusion models for controllable protein generation in sequence space. EvoDiff generates high-fidelity, diverse, and structurally-plausible proteins that cover natural sequence and functional space. We show experimentally that EvoDiff generations express, fold, and exhibit expected secondary structure elements. Critically, EvoDiff can generate proteins inaccessible to structure-based models, such as those with disordered regions, while maintaining the ability to design scaffolds for functional structural motifs. We validate the universality of our sequence-based formulation by experimentally characterizing intrinsically-disordered mitochondrial targeting signals, metal-binding proteins, and protein binders designed using EvoDiff. We envision that EvoDiff will expand capabilities in protein engineering beyond the structure-function paradigm toward programmable, sequence-first design.",
        "authors": [
            "Sarah Alamdari",
            "Nitya Thakkar",
            "Rianne van den Berg",
            "Alex X. Lu",
            "Nicolo Fusi",
            "Ava P. Amini",
            "Kevin Kaichuang Yang"
        ],
        "citations": 64,
        "references": 60,
        "year": 2024
    },
    {
        "title": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
        "abstract": "Diffusion models are the main driver of progress in image and video synthesis, but suffer from slow inference speed. Distillation methods, like the recently introduced adversarial diffusion distillation (ADD) aim to shift the model from many-shot to single-step inference, albeit at the cost of expensive and difficult optimization due to its reliance on a fixed pretrained DINOv2 discriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a novel distillation approach overcoming the limitations of ADD. In contrast to pixel-based ADD, LADD utilizes generative features from pretrained latent diffusion models. This approach simplifies training and enhances performance, enabling high-resolution multi-aspect ratio image synthesis. We apply LADD to Stable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the performance of state-of-the-art text-to-image generators using only four unguided sampling steps. Moreover, we systematically investigate its scaling behavior and demonstrate LADD's effectiveness in various applications such as image editing and inpainting.",
        "authors": [
            "Axel Sauer",
            "Frederic Boesel",
            "Tim Dockhorn",
            "A. Blattmann",
            "Patrick Esser",
            "Robin Rombach"
        ],
        "citations": 63,
        "references": 64,
        "year": 2024
    },
    {
        "title": "On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models",
        "abstract": "Diffusion-based Deep Generative Models (DDGMs) offer state-of-the-art performance in generative modeling. Their main strength comes from their unique setup in which a model (the backward diffusion process) is trained to reverse the forward diffusion process, which gradually adds noise to the input signal. Although DDGMs are well studied, it is still unclear how the small amount of noise is transformed during the backward diffusion process. Here, we focus on analyzing this problem to gain more insight into the behavior of DDGMs and their denoising and generative capabilities. We observe a fluid transition point that changes the functionality of the backward diffusion process from generating a (corrupted) image from noise to denoising the corrupted image to the final sample. Based on this observation, we postulate to divide a DDGM into two parts: a denoiser and a generator. The denoiser could be parameterized by a denoising auto-encoder, while the generator is a diffusion-based model with its own set of parameters. We experimentally validate our proposition, showing its pros and cons.",
        "authors": [
            "K. Deja",
            "Anna Kuzina",
            "Tomasz Trzci'nski",
            "J. Tomczak"
        ],
        "citations": 23,
        "references": 35,
        "year": 2022
    },
    {
        "title": "PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis",
        "abstract": "The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-$\\alpha$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-$\\alpha$'s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-$\\alpha$ only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days), saving nearly \\$300,000 (\\$26,000 vs. \\$320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-$\\alpha$ excels in image quality, artistry, and semantic control. We hope PIXART-$\\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.",
        "authors": [
            "Junsong Chen",
            "Jincheng Yu",
            "Chongjian Ge",
            "Lewei Yao",
            "Enze Xie",
            "Yue Wu",
            "Zhongdao Wang",
            "James T. Kwok",
            "Ping Luo",
            "Huchuan Lu",
            "Zhenguo Li"
        ],
        "citations": 231,
        "references": 79,
        "year": 2023
    },
    {
        "title": "TokenFlow: Consistent Diffusion Features for Consistent Video Editing",
        "abstract": "The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and motion of the input video. Our method is based on a key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos. Webpage: https://diffusion-tokenflow.github.io/",
        "authors": [
            "Michal Geyer",
            "Omer Bar-Tal",
            "Shai Bagon",
            "Tali Dekel"
        ],
        "citations": 189,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model",
        "abstract": "We report Zero123++, an image-conditioned diffusion model for generating 3D-consistent multi-view images from a single input view. To take full advantage of pretrained 2D generative priors, we develop various conditioning and training schemes to minimize the effort of finetuning from off-the-shelf image diffusion models such as Stable Diffusion. Zero123++ excels in producing high-quality, consistent multi-view images from a single image, overcoming common issues like texture degradation and geometric misalignment. Furthermore, we showcase the feasibility of training a ControlNet on Zero123++ for enhanced control over the generation process. The code is available at https://github.com/SUDO-AI-3D/zero123plus.",
        "authors": [
            "Ruoxi Shi",
            "Hansheng Chen",
            "Zhuoyang Zhang",
            "Minghua Liu",
            "Chao Xu",
            "Xinyue Wei",
            "Linghao Chen",
            "Chong Zeng",
            "Hao Su"
        ],
        "citations": 233,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Fast Timing-Conditioned Latent Audio Diffusion",
        "abstract": "Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.",
        "authors": [
            "Zach Evans",
            "CJ Carr",
            "Josiah Taylor",
            "Scott H. Hawley",
            "Jordi Pons"
        ],
        "citations": 73,
        "references": 60,
        "year": 2024
    },
    {
        "title": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "abstract": "We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.",
        "authors": [
            "Shanchuan Lin",
            "Anran Wang",
            "Xiao Yang"
        ],
        "citations": 75,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Emergent Correspondence from Image Diffusion",
        "abstract": "Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io",
        "authors": [
            "Luming Tang",
            "Menglin Jia",
            "Qianqian Wang",
            "Cheng Perng Phoo",
            "Bharath Hariharan"
        ],
        "citations": 175,
        "references": 107,
        "year": 2023
    },
    {
        "title": "Pix2Video: Video Editing using Image Diffusion",
        "abstract": "Image diffusion models, trained on massive image collections, have emerged as the most versatile image generator model in terms of quality and diversity. They support inverting real images and conditional (e.g., text) generation, making them attractive for high-quality image editing applications. We investigate how to use such pre-trained image models for text-guided video editing. The critical challenge is to achieve the target edits while still preserving the content of the source video. Our method works in two simple steps: first, we use a pre-trained structure-guided (e.g., depth) image diffusion model to perform text-guided edits on an anchor frame; then, in the key step, we progressively propagate the changes to the future frames via self-attention feature injection to adapt the core denoising step of the diffusion model. We then consolidate the changes by adjusting the latent code for the frame before continuing the process. Our approach is training-free and generalizes to a wide range of edits. We demonstrate the effectiveness of the approach by extensive experimentation and compare it against four different prior and parallel efforts (on ArXiv). We demonstrate that realistic text-guided video edits are possible, without any compute-intensive preprocessing or video-specific finetuning. https://duyguceylan.github.io/pix2video.github.io/.",
        "authors": [
            "Duygu Ceylan",
            "C. Huang",
            "N. Mitra"
        ],
        "citations": 197,
        "references": 68,
        "year": 2023
    },
    {
        "title": "SVDiff: Compact Parameter Space for Diffusion Fine-Tuning",
        "abstract": "Diffusion models have achieved remarkable success in text-to-image generation, enabling the creation of high-quality images from text prompts or other modalities. However, existing methods for customizing these models are limited by handling multiple personalized subjects and the risk of overfitting. Moreover, their large number of parameters is inefficient for model storage. In this paper, we propose a novel approach to address these limitations in existing text-to-image diffusion models for personalization. Our method involves fine-tuning the singular values of the weight matrices, leading to a compact and efficient parameter space that reduces the risk of overfitting and language-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique to enhance the quality of multi-subject image generation and a simple text-based image editing framework. Our proposed SVDiff method has a significantly smaller model size compared to existing methods (≈2,200 times fewer parameters compared with vanilla DreamBooth), making it more practical for real-world applications.",
        "authors": [
            "Ligong Han",
            "Yinxiao Li",
            "Han Zhang",
            "P. Milanfar",
            "Dimitris N. Metaxas",
            "Feng Yang"
        ],
        "citations": 204,
        "references": 87,
        "year": 2023
    },
    {
        "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
        "abstract": "We present DiffBIR, a general restoration pipeline that could handle different blind image restoration tasks in a unified framework. DiffBIR decouples blind image restoration problem into two stages: 1) degradation removal: removing image-independent content; 2) information regeneration: generating the lost image content. Each stage is developed independently but they work seamlessly in a cascaded manner. In the first stage, we use restoration modules to remove degradations and obtain high-fidelity restored results. For the second stage, we propose IRControlNet that leverages the generative ability of latent diffusion models to generate realistic details. Specifically, IRControlNet is trained based on specially produced condition images without distracting noisy content for stable generation performance. Moreover, we design a region-adaptive restoration guidance that can modify the denoising process during inference without model re-training, allowing users to balance realness and fidelity through a tunable guidance scale. Extensive experiments have demonstrated DiffBIR's superiority over state-of-the-art approaches for blind image super-resolution, blind face restoration and blind image denoising tasks on both synthetic and real-world datasets. The code is available at https://github.com/XPixelGroup/DiffBIR.",
        "authors": [
            "X. Lin",
            "Jingwen He",
            "Zi-Yuan Chen",
            "Zhaoyang Lyu",
            "Ben Fei",
            "Bo Dai",
            "Wanli Ouyang",
            "Y. Qiao",
            "Chao Dong"
        ],
        "citations": 136,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation",
        "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
        "authors": [
            "Lijun Yu",
            "José Lezama",
            "N. B. Gundavarapu",
            "Luca Versari",
            "Kihyuk Sohn",
            "David C. Minnen",
            "Yong Cheng",
            "Agrim Gupta",
            "Xiuye Gu",
            "Alexander G. Hauptmann",
            "Boqing Gong",
            "Ming-Hsuan Yang",
            "Irfan Essa",
            "David A. Ross",
            "Lu Jiang"
        ],
        "citations": 158,
        "references": 82,
        "year": 2023
    },
    {
        "title": "BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing",
        "abstract": "Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Code and models will be released at https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion. Project page at https://dxli94.github.io/BLIP-Diffusion-website/.",
        "authors": [
            "Dongxu Li",
            "Junnan Li",
            "Steven C. H. Hoi"
        ],
        "citations": 219,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks",
        "abstract": "We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.",
        "authors": [
            "Tianhe Ren",
            "Shilong Liu",
            "Ailing Zeng",
            "Jing Lin",
            "Kunchang Li",
            "He Cao",
            "Jiayu Chen",
            "Xinyu Huang",
            "Yukang Chen",
            "Feng Yan",
            "Zhaoyang Zeng",
            "Hao Zhang",
            "Feng Li",
            "Jie Yang",
            "Hongyang Li",
            "Qing Jiang",
            "Lei Zhang"
        ],
        "citations": 202,
        "references": 81,
        "year": 2024
    },
    {
        "title": "Human Motion Diffusion as a Generative Prior",
        "abstract": "Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Lastly, using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing. We evaluate the composition methods using an off-the-shelf motion diffusion model, and further compare the results to dedicated models trained for these specific tasks.",
        "authors": [
            "Yonatan Shafir",
            "Guy Tevet",
            "Roy Kapon",
            "Amit H. Bermano"
        ],
        "citations": 153,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
        "abstract": "The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has stronger multimodal compositional reasoning abilities than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. These models approach the performance of SOTA discriminative classifiers and exhibit strong \"effective robustness\" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations on our website: diffusion-classifier.github.io/",
        "authors": [
            "Alexander C. Li",
            "Mihir Prabhudesai",
            "Shivam Duggal",
            "Ellis L Brown",
            "Deepak Pathak"
        ],
        "citations": 171,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Diffusion Self-Guidance for Controllable Image Generation",
        "abstract": "Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images. For results and an interactive demo, see our project page at https://dave.ml/selfguidance/",
        "authors": [
            "Dave Epstein",
            "A. Jabri",
            "Ben Poole",
            "Alexei A. Efros",
            "Aleksander Holynski"
        ],
        "citations": 182,
        "references": 40,
        "year": 2023
    },
    {
        "title": "simple diffusion: End-to-end diffusion for high resolution images",
        "abstract": "Currently, applying diffusion models in pixel space of high resolution images is difficult. Instead, existing approaches focus on diffusion in lower dimensional spaces (latent diffusion), or have multiple super-resolution levels of generation referred to as cascades. The downside is that these approaches add additional complexity to the diffusion framework. This paper aims to improve denoising diffusion for high resolution images while keeping the model as simple as possible. The paper is centered around the research question: How can one train a standard denoising diffusion models on high resolution images, and still obtain performance comparable to these alternate approaches? The four main findings are: 1) the noise schedule should be adjusted for high resolution images, 2) It is sufficient to scale only a particular part of the architecture, 3) dropout should be added at specific locations in the architecture, and 4) downsampling is an effective strategy to avoid high resolution feature maps. Combining these simple yet effective techniques, we achieve state-of-the-art on image generation among diffusion models without sampling modifiers on ImageNet.",
        "authors": [
            "Emiel Hoogeboom",
            "J. Heek",
            "Tim Salimans"
        ],
        "citations": 193,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Diffusion Model-Based Image Editing: A Survey",
        "abstract": "Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.",
        "authors": [
            "Yi Huang",
            "Jiancheng Huang",
            "Yifan Liu",
            "Mingfu Yan",
            "Jiaxi Lv",
            "Jianzhuang Liu",
            "Wei Xiong",
            "He Zhang",
            "Shifeng Chen",
            "Liangliang Cao"
        ],
        "citations": 50,
        "references": 292,
        "year": 2024
    },
    {
        "title": "Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective",
        "abstract": "Diffusion models have achieved tremendous success in generating high-dimensional data like images, videos and audio. These models provide powerful data priors that can solve linear inverse problems in zero shot through Bayesian posterior sampling. However, exact posterior sampling for diffusion models is intractable. Current solutions often hinge on approximations that are either computationally expensive or lack strong theoretical guarantees. In this work, we introduce an efficient diffusion sampling algorithm for linear inverse problems that is guaranteed to be asymptotically accurate. We reveal a link between Bayesian posterior sampling and Bayesian filtering in diffusion models, proving the former as a specific instance of the latter. Our method, termed filtering posterior sampling , leverages sequential Monte Carlo methods to solve the corresponding filtering problem. It seamlessly integrates with all Markovian diffusion samplers, requires no model re-training",
        "authors": [
            "Zehao Dou",
            "Yang Song"
        ],
        "citations": 33,
        "references": 57,
        "year": 2024
    },
    {
        "title": "3D Diffusion Policy",
        "abstract": "Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 24.2% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Videos, code, and data are available on https://3d-diffusion-policy.github.io .",
        "authors": [
            "Yanjie Ze",
            "Gu Zhang",
            "Kangning Zhang",
            "Chenyuan Hu",
            "Muhan Wang",
            "Huazhe Xu"
        ],
        "citations": 46,
        "references": 88,
        "year": 2024
    },
    {
        "title": "SE(3) diffusion model with application to protein backbone generation",
        "abstract": "The design of novel protein structures remains a challenge in protein engineering for applications across biomedicine and chemistry. In this line of work, a diffusion model over rigid bodies in 3D (referred to as frames) has shown success in generating novel, functional protein backbones that have not been observed in nature. However, there exists no principled methodological framework for diffusion on SE(3), the space of orientation preserving rigid motions in R3, that operates on frames and confers the group invariance. We address these shortcomings by developing theoretical foundations of SE(3) invariant diffusion models on multiple frames followed by a novel framework, FrameDiff, for learning the SE(3) equivariant score over multiple frames. We apply FrameDiff on monomer backbone generation and find it can generate designable monomers up to 500 amino acids without relying on a pretrained protein structure prediction network that has been integral to previous methods. We find our samples are capable of generalizing beyond any known protein structure.",
        "authors": [
            "Jason Yim",
            "Brian L. Trippe",
            "Valentin De Bortoli",
            "Emile Mathieu",
            "A. Doucet",
            "R. Barzilay",
            "T. Jaakkola"
        ],
        "citations": 155,
        "references": 71,
        "year": 2023
    },
    {
        "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps",
        "abstract": "Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a $4\\sim 16\\times$ speedup compared with previous state-of-the-art training-free samplers on various datasets.",
        "authors": [
            "Cheng Lu",
            "Yuhao Zhou",
            "Fan Bao",
            "Jianfei Chen",
            "Chongxuan Li",
            "Jun Zhu"
        ],
        "citations": 1000,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Human Motion Diffusion Model",
        "abstract": "Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. https://guytevet.github.io/mdm-page/ .",
        "authors": [
            "Guy Tevet",
            "Sigal Raab",
            "Brian Gordon",
            "Yonatan Shafir",
            "Daniel Cohen-Or",
            "Amit H. Bermano"
        ],
        "citations": 575,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Consistency Models",
        "abstract": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.",
        "authors": [
            "Yang Song",
            "Prafulla Dhariwal",
            "Mark Chen",
            "I. Sutskever"
        ],
        "citations": 648,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Synthbuster: Towards Detection of Diffusion Model Generated Images",
        "abstract": "Synthetically-generated images are getting increasingly popular. Diffusion models have advanced to the stage where even non-experts can generate photo-realistic images from a simple text prompt. They expand creative horizons but also open a Pandora's box of potential disinformation risks. In this context, the present corpus of synthetic image detection techniques, primarily focusing on older generative models like Generative Adversarial Networks, finds itself ill-equipped to deal with this emerging trend. Recognizing this challenge, we introduce a method specifically designed to detect synthetic images produced by diffusion models. Our approach capitalizes on the inherent frequency artefacts left behind during the diffusion process. Spectral analysis is used to highlight the artefacts in the Fourier transform of a residual image, which are used to distinguish real from fake images. The proposed method can detect diffusion-model-generated images even under mild jpeg compression, and generalizes relatively well to unknown models. By pioneering this novel approach, we aim to fortify forensic methodologies and ignite further research into the detection of AI-generated images.",
        "authors": [
            "Quentin Bammey"
        ],
        "citations": 30,
        "references": 53,
        "year": 2024
    },
    {
        "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
        "abstract": "Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling",
        "authors": [
            "Hyungjin Chung",
            "Jeongsol Kim",
            "Michael T. McCann",
            "M. Klasky",
            "J. C. Ye"
        ],
        "citations": 560,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Diffusion-LM Improves Controllable Text Generation",
        "abstract": "Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.",
        "authors": [
            "Xiang Lisa Li",
            "John Thickstun",
            "Ishaan Gulrajani",
            "Percy Liang",
            "Tatsunori Hashimoto"
        ],
        "citations": 655,
        "references": 59,
        "year": 2022
    },
    {
        "title": "DIRE for Diffusion-Generated Image Detection",
        "abstract": "Diffusion models have shown remarkable success in visual synthesis, but have also raised concerns about potential abuse for malicious purposes. In this paper, we seek to build a detector for telling apart real images from diffusion-generated images. We find that existing detectors struggle to detect images generated by diffusion models, even if we include generated images from a specific diffusion model in their training data. To address this issue, we propose a novel image representation called DIffusion Reconstruction Error (DIRE), which measures the error between an input image and its reconstruction counterpart by a pre-trained diffusion model. We observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. It provides a hint that DIRE can serve as a bridge to distinguish generated and real images. DIRE provides an effective way to detect images generated by most diffusion models, and it is general for detecting generated images from unseen diffusion models and robust to various perturbations. Furthermore, we establish a comprehensive diffusion-generated benchmark including images generated by various diffusion models to evaluate the performance of diffusion-generated image detectors. Extensive experiments on our collected benchmark demonstrate that DIRE exhibits superiority over previous generated-image detectors. The code, models, and dataset are available at https://github.com/ZhendongWang6/DIRE.",
        "authors": [
            "Zhendong Wang",
            "Jianmin Bao",
            "Wen-gang Zhou",
            "Weilun Wang",
            "Hezhen Hu",
            "Hong Chen",
            "Houqiang Li"
        ],
        "citations": 134,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Simplified and Generalized Masked Diffusion for Discrete Data",
        "abstract": "Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64x64) bits per dimension that are better than autoregressive models of similar sizes. Our code is available at https://github.com/google-deepmind/md4.",
        "authors": [
            "Jiaxin Shi",
            "Kehang Han",
            "Zhe Wang",
            "Arnaud Doucet",
            "Michalis K. Titsias"
        ],
        "citations": 23,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Diffusion Model Alignment Using Direct Preference Optimization",
        "abstract": "Large language models (LLMs) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback (RLHF) methods to make them better aligned with users' preferences. In contrast to LLMs, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-DPO, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-DPO is adapted from the recently developed Direct Preference Optimization (DPO) [36], a simpler alternative to RLHF which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate DPO to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our fine-tuned base model significantly outperforms both base SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses AI feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.",
        "authors": [
            "Bram Wallace",
            "Meihua Dang",
            "Rafael Rafailov",
            "Linqi Zhou",
            "Aaron Lou",
            "Senthil Purushwalkam",
            "Stefano Ermon",
            "Caiming Xiong",
            "Shafiq R. Joty",
            "Nikhil Naik"
        ],
        "citations": 119,
        "references": 58,
        "year": 2023
    },
    {
        "title": "DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors",
        "abstract": "Animating a still image offers an engaging visual experience. Traditional image animation techniques mainly focus on animating natural scenes with stochastic dynamics (e.g. clouds and fluid) or domain-specific motions (e.g. human hair or body motions), and thus limits their applicability to more general visual content. To overcome this limitation, we explore the synthesis of dynamic content for open-domain images, converting them into animated videos. The key idea is to utilize the motion prior of text-to-video diffusion models by incorporating the image into the generative process as guidance. Given an image, we first project it into a text-aligned rich context representation space using a query transformer, which facilitates the video model to digest the image content in a compatible fashion. However, some visual details still struggle to be preserved in the resultant videos. To supplement with more precise image information, we further feed the full image to the diffusion model by concatenating it with the initial noises. Experimental results show that our proposed method can produce visually convincing and more logical&natural motions, as well as higher conformity to the input image. Comparative evaluation demonstrates the notable superiority of our approach over existing competitors.",
        "authors": [
            "Jinbo Xing",
            "Menghan Xia",
            "Yong Zhang",
            "Haoxin Chen",
            "Xintao Wang",
            "Tien-Tsin Wong",
            "Ying Shan"
        ],
        "citations": 110,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Multi-Concept Customization of Text-to-Image Diffusion",
        "abstract": "While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~ 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient.",
        "authors": [
            "Nupur Kumari",
            "Bin Zhang",
            "Richard Zhang",
            "Eli Shechtman",
            "Jun-Yan Zhu"
        ],
        "citations": 651,
        "references": 96,
        "year": 2022
    },
    {
        "title": "Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference",
        "abstract": "Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: https://latent-consistency-models.github.io/",
        "authors": [
            "Simian Luo",
            "Yiqin Tan",
            "Longbo Huang",
            "Jian Li",
            "Hang Zhao"
        ],
        "citations": 321,
        "references": 38,
        "year": 2023
    },
    {
        "title": "FiT: Flexible Vision Transformer for Diffusion Model",
        "abstract": "Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at https://github.com/whlzy/FiT.",
        "authors": [
            "Zeyu Lu",
            "Zidong Wang",
            "Di Huang",
            "Chengyue Wu",
            "Xihui Liu",
            "Wanli Ouyang",
            "Lei Bai"
        ],
        "citations": 31,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Diffusion-based Generation, Optimization, and Planning in 3D Scenes",
        "abstract": "We introduce the SceneDiffuser, a conditional generative model for 3D scene understanding. SceneDiffuser provides a unified model for solving scene-conditioned generation, optimization, and planning. In contrast to prior work, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy, SceneDiffuser jointly formulates the scene-aware generation, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differentiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previous scene-conditioned generative models. We evaluate the SceneDiffuser on various 3D scene understanding tasks, including human pose and motion generation, dexterous grasp generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant improvements compared with previous models, demonstrating the tremendous potential of the SceneDiffuser for the broad community of 3D scene understanding.",
        "authors": [
            "Siyuan Huang",
            "Zan Wang",
            "Puhao Li",
            "Baoxiong Jia",
            "Tengyu Liu",
            "Yixin Zhu",
            "Wei Liang",
            "Song-Chun Zhu"
        ],
        "citations": 158,
        "references": 99,
        "year": 2023
    },
    {
        "title": "DiffEdit: Diffusion-based semantic image editing with mask guidance",
        "abstract": "Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.",
        "authors": [
            "Guillaume Couairon",
            "Jakob Verbeek",
            "Holger Schwenk",
            "M. Cord"
        ],
        "citations": 404,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Blended Latent Diffusion",
        "abstract": "The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space and eliminating the need for resource-intensive CLIP gradient calculations at each diffusion step. We first enable LDM to perform local image edits by blending the latents at each step, similarly to Blended Diffusion. Next we propose an optimization-based solution for the inherent inability of LDM to accurately reconstruct images. Finally, we address the scenario of performing local edits using thin masks. We evaluate our method against the available baselines both qualitatively and quantitatively and demonstrate that in addition to being faster, it produces more precise results.",
        "authors": [
            "Omri Avrahami",
            "Ohad Fried",
            "Dani Lischinski"
        ],
        "citations": 300,
        "references": 86,
        "year": 2022
    },
    {
        "title": "Diffusion-TS: Interpretable Diffusion for General Time Series Generation",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-TS can be easily extended to conditional generation tasks, such as forecasting and imputation, without any model changes. This also motivates us to further explore the performance of Diffusion-TS under irregular settings. Finally, through qualitative and quantitative experiments, results show that Diffusion-TS achieves the state-of-the-art results on various realistic analyses of time series.",
        "authors": [
            "Xinyu Yuan",
            "Yan Qiao"
        ],
        "citations": 25,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis",
        "abstract": "Large-scale diffusion models have achieved state-of-the-art results on text-to-image synthesis (T2I) tasks. Despite their ability to generate high-quality yet creative images, we observe that attribution-binding and compositional capabilities are still considered major challenging issues, especially when involving multiple objects. In this work, we improve the compositional skills of T2I models, specifically more accurate attribute binding and better image compositions. To do this, we incorporate linguistic structures with the diffusion guidance process based on the controllable properties of manipulating cross-attention layers in diffusion-based T2I models. We observe that keys and values in cross-attention layers have strong semantic meanings associated with object layouts and content. Therefore, we can better preserve the compositional semantics in the generated image by manipulating the cross-attention representations based on linguistic insights. Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention design is efficient that requires no additional training samples. We achieve better compositional skills in qualitative and quantitative results, leading to a 5-8% advantage in head-to-head user comparison studies. Lastly, we conduct an in-depth analysis to reveal potential causes of incorrect image compositions and justify the properties of cross-attention layers in the generation process.",
        "authors": [
            "Weixi Feng",
            "Xuehai He",
            "Tsu-Jui Fu",
            "Varun Jampani",
            "Arjun Reddy Akula",
            "P. Narayana",
            "Sugato Basu",
            "X. Wang",
            "William Yang Wang"
        ],
        "citations": 257,
        "references": 51,
        "year": 2022
    },
    {
        "title": "BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion",
        "abstract": "Recent text-to-image diffusion models have demonstrated an astonishing capacity to generate high-quality images. However, researchers mainly studied the way of synthesizing images with only text prompts. While some works have explored using other modalities as conditions, considerable paired data, e.g., box/mask-image pairs, and fine-tuning time are required for nurturing models. As such paired data is time-consuming and labor-intensive to acquire and restricted to a closed set, this potentially becomes the bottleneck for applications in an open world. This paper focuses on the simplest form of user-provided conditions, e.g., box or scribble. To mitigate the aforementioned problem, we propose a training-free method to control objects and contexts in the synthesized images adhering to the given spatial conditions. Specifically, three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints, are designed and seamlessly integrated into the denoising step of diffusion models, requiring no additional training and massive annotated layout data. Extensive experimental results demonstrate that the proposed constraints can control what and where to present in the images while retaining the ability of Diffusion models to synthesize with high fidelity and diverse concept coverage.",
        "authors": [
            "Jinheng Xie",
            "Yuexiang Li",
            "Yawen Huang",
            "Haozhe Liu",
            "Wentian Zhang",
            "Yefeng Zheng",
            "Mike Zheng Shou"
        ],
        "citations": 144,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation",
        "abstract": "Large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, synthesizing diverse images with highly complex visual concepts. However, a pivotal challenge in leveraging such models for real-world content creation is providing users with control over the generated content. In this paper, we present a new framework that takes text-to- image synthesis to the realm of image-to-image translation - given a guidance image and a target text prompt as input, our method harnesses the power of a pre-trained text-to-image diffusion model to generate a new image that complies with the target text, while preserving the semantic layout of the guidance image. Specifically, we observe and empirically demonstrate that fine-grained control over the generated structure can be achieved by manipulating spatial features and their self-attention inside the model. This results in a simple and effective approach, where features extracted from the guidance image are directly injected into the generation process of the translated image, requiring no training or fine-tuning. We demonstrate high-quality results on versatile text-guided image translation tasks, including translating sketches, rough drawings and animations into realistic images, changing the class and appearance of objects in a given image, and modifying global qualities such as lighting and color.",
        "authors": [
            "Narek Tumanyan",
            "Michal Geyer",
            "Shai Bagon",
            "Tali Dekel"
        ],
        "citations": 490,
        "references": 54,
        "year": 2022
    },
    {
        "title": "Planning with Diffusion for Flexible Behavior Synthesis",
        "abstract": "Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.",
        "authors": [
            "Michael Janner",
            "Yilun Du",
            "J. Tenenbaum",
            "S. Levine"
        ],
        "citations": 476,
        "references": 76,
        "year": 2022
    },
    {
        "title": "One-Step Diffusion with Distribution Matching Distillation",
        "abstract": "Diffusion models generate high-quality images but require dozens of forward passes. We introduce Distribution Matching Distillation (DMD), a procedure to transform a diffusion model into a one-step image generator with minimal impact on image quality. We enforce the one-step image generator match the diffusion model at distribution level, by minimizing an approximate KL divergence whose gradient can be expressed as the difference between 2 score functions, one of the target distribution and the other of the synthetic distribution being produced by our one-step generator. The score functions are parameterized as two diffusion models trained separately on each distribution. Combined with a simple regression loss matching the large-scale structure of the multi-step diffusion outputs, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64×64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model can generate images at 20 FPS on modern hardware.",
        "authors": [
            "Tianwei Yin",
            "Michael Gharbi",
            "Richard Zhang",
            "Eli Shechtman",
            "Frédo Durand",
            "William T. Freeman",
            "Taesung Park"
        ],
        "citations": 127,
        "references": 89,
        "year": 2023
    },
    {
        "title": "One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion",
        "abstract": "Recent advancements in open-world 3D object generation have been remarkable, with image-to-3D methods of-fering superior fine-grained control over their text-to-3D counterparts. However, most existing models fall short in simultaneously providing rapid generation speeds and high fidelity to input images - two features essential for practi-cal applications. In this paper, we present One-2-3-45++, an innovative method that transforms a single image into a detailed 3D textured mesh in approximately one minute. Our approach aims to fully harness the extensive knowledge embedded in 2D diffusion models and priors from valuable yet limited 3D data. This is achieved by initially finetuning a 2D diffusion model for consistent multi-view image generation, followed by elevating these images to 3D with the aid of multi-view-conditioned 3D native diffusion models. Extensive experimental evaluations demonstrate that our method can produce high-quality, diverse 3D assets that closely mirror the original input image.",
        "authors": [
            "Minghua Liu",
            "Ruoxi Shi",
            "Linghao Chen",
            "Zhuoyang Zhang",
            "Chao Xu",
            "Xinyue Wei",
            "Hansheng Chen",
            "Chong Zeng",
            "Jiayuan Gu",
            "Hao Su"
        ],
        "citations": 134,
        "references": 89,
        "year": 2023
    },
    {
        "title": "3DGen: Triplane Latent Diffusion for Textured Mesh Generation",
        "abstract": "Latent diffusion models for image generation have crossed a quality threshold which enabled them to achieve mass adoption. Recently, a series of works have made advancements towards replicating this success in the 3D domain, introducing techniques such as point cloud VAE, triplane representation, neural implicit surfaces and differentiable rendering based training. We take another step along this direction, combining these developments in a two-step pipeline consisting of 1) a triplane VAE which can learn latent representations of textured meshes and 2) a conditional diffusion model which generates the triplane features. For the first time this architecture allows conditional and unconditional generation of high quality textured or untextured 3D meshes across multiple diverse categories in a few seconds on a single GPU. It outperforms previous work substantially on image-conditioned and unconditional generation on mesh quality as well as texture generation. Furthermore, we demonstrate the scalability of our model to large datasets for increased quality and diversity. We will release our code and trained models.",
        "authors": [
            "Anchit Gupta",
            "Anchit Gupta"
        ],
        "citations": 134,
        "references": 39,
        "year": 2023
    },
    {
        "title": "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation",
        "abstract": "Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \\emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, we create, to the best of our knowledge, the first one-step diffusion-based text-to-image generator with SD-level image quality, achieving an FID (Frechet Inception Distance) of $23.3$ on MS COCO 2017-5k, surpassing the previous state-of-the-art technique, progressive distillation, by a significant margin ($37.2$ $\\rightarrow$ $23.3$ in FID). By utilizing an expanded network with 1.7B parameters, we further improve the FID to $22.4$. We call our one-step models \\emph{InstaFlow}. On MS COCO 2014-30k, InstaFlow yields an FID of $13.1$ in just $0.09$ second, the best in $\\leq 0.1$ second regime, outperforming the recent StyleGAN-T ($13.9$ in $0.1$ second). Notably, the training of InstaFlow only costs 199 A100 GPU days. Codes and pre-trained models are available at \\url{github.com/gnobitab/InstaFlow}.",
        "authors": [
            "Xingchao Liu",
            "Xiwen Zhang",
            "Jianzhu Ma",
            "Jian Peng",
            "Q. Liu"
        ],
        "citations": 134,
        "references": 113,
        "year": 2023
    },
    {
        "title": "Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction",
        "abstract": "3D-aware image synthesis encompasses a variety of tasks, such as scene generation and novel view synthesis from images. Despite numerous task-specific methods, developing a comprehensive model remains challenging. In this paper, we present SSDNeRF, a unified approach that employs an expressive diffusion model to learn a generalizable prior of neural radiance fields (NeRF) from multi-view images of diverse objects. Previous studies have used two-stage approaches that rely on pretrained NeRFs as real data to train diffusion models. In contrast, we propose a new single-stage training paradigm with an end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent diffusion model, enabling simultaneous 3D reconstruction and prior learning, even from sparsely available views. At test time, we can directly sample the diffusion prior for unconditional generation, or combine it with arbitrary observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates robust results comparable to or better than leading task-specific methods in unconditional generation and single/sparse-view 3D reconstruction.6",
        "authors": [
            "Hansheng Chen",
            "Jiatao Gu",
            "Anpei Chen",
            "Wei Tian",
            "Z. Tu",
            "Lingjie Liu",
            "Haoran Su"
        ],
        "citations": 131,
        "references": 66,
        "year": 2023
    },
    {
        "title": "LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation",
        "abstract": "Recently, diffusion models have achieved great success in image synthesis. However, when it comes to the layout-to-image generation where an image often has a complex scene of multiple objects, how to make strong control over both the global layout map and each detailed object remains a challenging task. In this paper, we propose a diffusion model named LayoutDiffusion that can obtain higher generation quality and greater controllability than the previous works. To overcome the difficult multimodal fusion of image and layout, we propose to construct a structural image patch with region information and transform the patched image into a special layout to fuse with the normal layout in a unified form. Moreover, Layout Fusion Module (LFM) and Object-aware Cross Attention (OaCA) are proposed to model the relationship among multiple objects and designed to be object-aware and position-sensitive, allowing for precisely controlling the spatial related information. Extensive experiments show that our LayoutDiffusion out-performs the previous SOTA methods on FID, CAS by relatively 46.35%,26.70% on COCO-stuff and 44.29%,41.82% on VG. Code is available at https://github.com/ZGCTroy/LayoutDiffusion.",
        "authors": [
            "Guangcong Zheng",
            "Xianpan Zhou",
            "Xuewei Li",
            "Zhongang Qi",
            "Ying Shan",
            "Xi Li"
        ],
        "citations": 132,
        "references": 59,
        "year": 2023
    },
    {
        "title": "StableVideo: Text-driven Consistency-aware Diffusion Video Editing",
        "abstract": "Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at this https URL.",
        "authors": [
            "Wenhao Chai",
            "Xun Guo",
            "Gaoang Wang",
            "Yang Lu"
        ],
        "citations": 125,
        "references": 53,
        "year": 2023
    },
    {
        "title": "LCM-LoRA: A Universal Stable-Diffusion Acceleration Module",
        "abstract": "Latent Consistency Models (LCMs) have achieved impressive performance in accelerating text-to-image generative tasks, producing high-quality images with minimal inference steps. LCMs are distilled from pre-trained latent diffusion models (LDMs), requiring only ~32 A100 GPU training hours. This report further extends LCMs' potential in two aspects: First, by applying LoRA distillation to Stable-Diffusion models including SD-V1.5, SSD-1B, and SDXL, we have expanded LCM's scope to larger models with significantly less memory consumption, achieving superior image generation quality. Second, we identify the LoRA parameters obtained through LCM distillation as a universal Stable-Diffusion acceleration module, named LCM-LoRA. LCM-LoRA can be directly plugged into various Stable-Diffusion fine-tuned models or LoRAs without training, thus representing a universally applicable accelerator for diverse image generation tasks. Compared with previous numerical PF-ODE solvers such as DDIM, DPM-Solver, LCM-LoRA can be viewed as a plug-in neural PF-ODE solver that possesses strong generalization abilities. Project page: https://github.com/luosiallen/latent-consistency-model.",
        "authors": [
            "Simian Luo",
            "Yiqin Tan",
            "Suraj Patil",
            "Daniel Gu",
            "Patrick von Platen",
            "Apolin'ario Passos",
            "Longbo Huang",
            "Jian Li",
            "Hang Zhao"
        ],
        "citations": 119,
        "references": 19,
        "year": 2023
    },
    {
        "title": "3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction",
        "abstract": "Rich data and powerful machine learning models allow us to design drugs for a specific protein target \\textit{in silico}. Recently, the inclusion of 3D structures during targeted drug design shows superior performance to other target-free models as the atomic interaction in the 3D space is explicitly modeled. However, current 3D target-aware models either rely on the voxelized atom densities or the autoregressive sampling process, which are not equivariant to rotation or easily violate geometric constraints resulting in unrealistic structures. In this work, we develop a 3D equivariant diffusion model to solve the above challenges. To achieve target-aware molecule design, our method learns a joint generative process of both continuous atom coordinates and categorical atom types with a SE(3)-equivariant network. Moreover, we show that our model can serve as an unsupervised feature extractor to estimate the binding affinity under proper parameterization, which provides an effective way for drug screening. To evaluate our model, we propose a comprehensive framework to evaluate the quality of sampled molecules from different dimensions. Empirical studies show our model could generate molecules with more realistic 3D structures and better affinities towards the protein targets, and improve binding affinity ranking and prediction without retraining.",
        "authors": [
            "Jiaqi Guan",
            "Wesley Wei Qian",
            "Xingang Peng",
            "Yufeng Su",
            "Jian Peng",
            "Jianzhu Ma"
        ],
        "citations": 121,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation",
        "abstract": "Text-to-3D generation has shown rapid progress in recent days with the advent of score distillation, a methodology of using pretrained text-to-2D diffusion models to optimize neural radiance field (NeRF) in the zero-shot setting. However, the lack of 3D awareness in the 2D diffusion models destabilizes score distillation-based methods from reconstructing a plausible 3D scene. To address this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness into pretrained 2D diffusion models, enhancing the robustness and 3D consistency of score distillation-based methods. We realize this by first constructing a coarse 3D structure of a given text prompt and then utilizing projected, view-specific depth map as a condition for the diffusion model. Additionally, we introduce a training strategy that enables the 2D diffusion model learns to handle the errors and sparsity within the coarse 3D structure for robust generation, as well as a method for ensuring semantic consistency throughout all viewpoints of the scene. Our framework surpasses the limitations of prior arts, and has significant implications for 3D consistent generation of 2D diffusion models.",
        "authors": [
            "Junyoung Seo",
            "Wooseok Jang",
            "Minseop Kwak",
            "Jaehoon Ko",
            "Ines Hyeonsu Kim",
            "Junho Kim",
            "Jin-Hwa Kim",
            "Jiyoung Lee",
            "Seung Wook Kim"
        ],
        "citations": 123,
        "references": 84,
        "year": 2023
    },
    {
        "title": "A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence",
        "abstract": "Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.",
        "authors": [
            "Junyi Zhang",
            "Charles Herrmann",
            "Junhwa Hur",
            "Luisa Polania Cabrera",
            "Varun Jampani",
            "Deqing Sun",
            "Ming Yang"
        ],
        "citations": 120,
        "references": 74,
        "year": 2023
    },
    {
        "title": "One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale",
        "abstract": "This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -- perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. In particular, UniDiffuser is able to produce perceptually realistic samples in all tasks and its quantitative results (e.g., the FID and CLIP score) are not only superior to existing general-purpose models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image generation).",
        "authors": [
            "Fan Bao",
            "Shen Nie",
            "Kaiwen Xue",
            "Chongxuan Li",
            "Shiliang Pu",
            "Yaole Wang",
            "Gang Yue",
            "Yue Cao",
            "Hang Su",
            "Jun Zhu"
        ],
        "citations": 123,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Efficient Diffusion Training via Min-SNR Weighting Strategy",
        "abstract": "Denoising diffusion models have been a mainstream approach for image generation, however, training these models often suffers from slow convergence. In this paper, we discovered that the slow convergence is partly due to conflicting optimization directions between timesteps. To address this issue, we treat the diffusion training as a multi-task learning problem, and introduce a simple yet effective approach referred to as Min-SNR-γ. This method adapts loss weights of timesteps based on clamped signal-to-noise ratios, which effectively balances the conflicts among timesteps. Our results demonstrate a significant improvement in converging speed, 3.4× faster than previous weighting strategies. It is also more effective, achieving a new record FID score of 2.06 on the ImageNet 256 × 256 benchmark using smaller architectures than that employed in previous state-of-the-art. The code is available at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training.",
        "authors": [
            "Tiankai Hang",
            "Shuyang Gu",
            "Chen Li",
            "Jianmin Bao",
            "Dong Chen",
            "Han Hu",
            "Xin Geng",
            "B. Guo"
        ],
        "citations": 117,
        "references": 61,
        "year": 2023
    },
    {
        "title": "SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds",
        "abstract": "Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by exploring training strategies and introducing regularization from classifier-free guidance. Our extensive experiments on MS-COCO show that our model with $8$ denoising steps achieves better FID and CLIP scores than Stable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creation by bringing powerful text-to-image diffusion models to the hands of users.",
        "authors": [
            "Yanyu Li",
            "Huan Wang",
            "Qing Jin",
            "Ju Hu",
            "Pavlo Chemerys",
            "Yun Fu",
            "Yanzhi Wang",
            "S. Tulyakov",
            "Jian Ren"
        ],
        "citations": 114,
        "references": 61,
        "year": 2023
    },
    {
        "title": "DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model",
        "abstract": "We propose \\textbf{DMV3D}, a novel 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. Our reconstruction model incorporates a triplane NeRF representation and can denoise noisy multi-view images via NeRF reconstruction and rendering, achieving single-stage 3D generation in $\\sim$30s on single A100 GPU. We train \\textbf{DMV3D} on large-scale multi-view image datasets of highly diverse objects using only image reconstruction losses, without accessing 3D assets. We demonstrate state-of-the-art results for the single-image reconstruction problem where probabilistic modeling of unseen object parts is required for generating diverse reconstructions with sharp textures. We also show high-quality text-to-3D generation results outperforming previous 3D diffusion models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .",
        "authors": [
            "Yinghao Xu",
            "Hao Tan",
            "Fujun Luan",
            "Sai Bi",
            "Peng Wang",
            "Jiahao Li",
            "Zifan Shi",
            "Kalyan Sunkavalli",
            "Gordon Wetzstein",
            "Zexiang Xu",
            "Kai Zhang"
        ],
        "citations": 112,
        "references": 77,
        "year": 2023
    },
    {
        "title": "FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model",
        "abstract": "Recently, conditional diffusion models have gained popularity in numerous applications due to their exceptional generation ability. However, many existing methods are training-required. They need to train a time-dependent classifier or a condition-dependent score estimator, which increases the cost of constructing conditional diffusion models and is inconvenient to transfer across different conditions. Some current works aim to overcome this limitation by proposing training-free solutions, but most can only be applied to a specific category of tasks and not to more general conditions. In this work, we propose a training-Free conditional Diffusion Model (FreeDoM) used for various conditions. Specifically, we leverage off-the-shelf pretrained networks, such as a face detection model, to construct time-independent energy functions, which guide the generation process without requiring training. Furthermore, because the construction of the energy function is very flexible and adaptable to various conditions, our proposed FreeDoM has a broader range of applications than existing training-free methods. FreeDoM is advantageous in its simplicity, effectiveness, and low cost. Experiments demonstrate that FreeDoM is effective for various conditions and suitable for diffusion models of diverse data domains, including image and latent code domains. Code is available at https://github.com/vvictoryuki/FreeDoM.",
        "authors": [
            "Jiwen Yu",
            "Yinhuai Wang",
            "Chen Zhao",
            "Bernard Ghanem",
            "Jian Zhang"
        ],
        "citations": 112,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Goal-Conditioned Imitation Learning using Score-based Diffusion Policies",
        "abstract": "We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn general-purpose goal-specified policies from large uncurated datasets without rewards. Our new goal-conditioned policy architecture\"$\\textbf{BE}$havior generation with $\\textbf{S}$c$\\textbf{O}$re-based Diffusion Policies\"(BESO) leverages a generative, score-based diffusion model as its policy. BESO decouples the learning of the score model from the inference sampling process, and, hence allows for fast sampling strategies to generate goal-specified behavior in just 3 denoising steps, compared to 30+ steps of other diffusion based policies. Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional clustering for effective goal-conditioned behavior learning. Finally, we show how BESO can even be used to learn a goal-independent policy from play-data using classifier-free guidance. To the best of our knowledge this is the first work that a) represents a behavior policy based on such a decoupled SDM b) learns an SDM based policy in the domain of GCIL and c) provides a way to simultaneously learn a goal-dependent and a goal-independent policy from play-data. We evaluate BESO through detailed simulation and show that it consistently outperforms several state-of-the-art goal-conditioned imitation learning methods on challenging benchmarks. We additionally provide extensive ablation studies and experiments to demonstrate the effectiveness of our method for goal-conditioned behavior generation. Demonstrations and Code are available at https://intuitive-robots.github.io/beso-website/",
        "authors": [
            "Moritz Reuss",
            "M. Li",
            "Xiaogang Jia",
            "Rudolf Lioutikov"
        ],
        "citations": 109,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model",
        "abstract": "Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration.",
        "authors": [
            "Yinhuai Wang",
            "Jiwen Yu",
            "Jian Zhang"
        ],
        "citations": 334,
        "references": 44,
        "year": 2022
    },
    {
        "title": "MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer",
        "abstract": "The Diffusion Probabilistic Model (DPM) has recently gained popularity in the field of computer vision, thanks to its image generation applications, such as Imagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated impressive capabilities and sparked much discussion within the community. Recent investigations have further unveiled the utility of DPM in the domain of medical image analysis, as underscored by the commendable performance exhibited by the medical image segmentation model across various tasks. Although these models were originally underpinned by a UNet architecture, there exists a potential avenue for enhancing their performance through the integration of vision transformer mechanisms. However, we discovered that simply combining these two models resulted in subpar performance. To effectively integrate these two cutting-edge techniques for the Medical image segmentation, we propose a novel Transformer-based Diffusion framework, called MedSegDiff-V2. We verify its effectiveness on 20 medical image segmentation tasks with different image modalities. Through comprehensive evaluation, our approach demonstrates superiority over prior state-of-the-art (SOTA) methodologies. Code is released at https://github.com/KidsWithTokens/MedSegDiff.",
        "authors": [
            "Junde Wu",
            "Rao Fu",
            "Huihui Fang",
            "Yu Zhang",
            "Yanwu Xu"
        ],
        "citations": 104,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Subject-Diffusion: Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning",
        "abstract": "Recent progress in personalized image generation using diffusion models has been significant. However, development in the area of open-domain and non-fine-tuning personalized image generation is proceeding rather slowly. In this paper, we propose Subject-Diffusion, a novel open-domain personalized image generation model that, in addition to not requiring test-time fine-tuning, also only requires a single reference image to support personalized generation of single- or multi-subject in any domain. Firstly, we construct an automatic data labeling tool and use the LAION-Aesthetics dataset to construct a large-scale dataset consisting of 76M images and their corresponding subject detection bounding boxes, segmentation masks and text descriptions. Secondly, we design a new unified framework that combines text and image semantics by incorporating coarse location and fine-grained reference image control to maximize subject fidelity and generalization. Furthermore, we also adopt an attention control mechanism to support multi-subject generation. Extensive qualitative and quantitative results demonstrate that our method outperforms other SOTA frameworks in single, multiple, and human customized image generation. Please refer to our \\href{https://oppo-mente-lab.github.io/subject_diffusion/}{project page}",
        "authors": [
            "Jiancang Ma",
            "Junhao Liang",
            "Chen Chen",
            "H. Lu"
        ],
        "citations": 102,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond",
        "abstract": "Although text-to-image diffusion models have made significant strides in generating images from text, they are sometimes more inclined to generate images like the data on which the model was trained rather than the provided text. This limitation has hindered their usage in both 2D and 3D applications. To address this problem, we explored the use of negative prompts but found that the current implementation fails to produce desired results, particularly when there is an overlap between the main and negative prompts. To overcome this issue, we propose Perp-Neg, a new algorithm that leverages the geometrical properties of the score space to address the shortcomings of the current negative prompts algorithm. Perp-Neg does not require any training or fine-tuning of the model. Moreover, we experimentally demonstrate that Perp-Neg provides greater flexibility in generating images by enabling users to edit out unwanted concepts from the initially generated images in 2D cases. Furthermore, to extend the application of Perp-Neg to 3D, we conducted a thorough exploration of how Perp-Neg can be used in 2D to condition the diffusion model to generate desired views, rather than being biased toward the canonical views. Finally, we applied our 2D intuition to integrate Perp-Neg with the state-of-the-art text-to-3D (DreamFusion) method, effectively addressing its Janus (multi-head) problem. Our project page is available at https://Perp-Neg.github.io/",
        "authors": [
            "Mohammadreza Armandpour",
            "A. Sadeghian",
            "Huangjie Zheng",
            "Amir Sadeghian",
            "Mingyuan Zhou"
        ],
        "citations": 106,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
        "abstract": "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}.",
        "authors": [
            "Chenfei Wu",
            "Sheng-Kai Yin",
            "Weizhen Qi",
            "Xiaodong Wang",
            "Zecheng Tang",
            "Nan Duan"
        ],
        "citations": 552,
        "references": 61,
        "year": 2023
    },
    {
        "title": "GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation",
        "abstract": "Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules.",
        "authors": [
            "Minkai Xu",
            "Lantao Yu",
            "Yang Song",
            "Chence Shi",
            "Stefano Ermon",
            "Jian Tang"
        ],
        "citations": 427,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Plug-in Diffusion Model for Sequential Recommendation",
        "abstract": "Pioneering efforts have verified the effectiveness of the diffusion models in exploring the informative uncertainty for recommendation. Considering the difference between recommendation and image synthesis tasks, existing methods have undertaken tailored refinements to the diffusion and reverse process. However, these approaches typically use the highest-score item in corpus for user interest prediction, leading to the ignorance of the user's generalized preference contained within other items, thereby remaining constrained by the data sparsity issue. To address this issue, this paper presents a novel Plug-in Diffusion Model for Recommendation (PDRec) framework, which employs the diffusion model as a flexible plugin to jointly take full advantage of the diffusion-generating user preferences on all items. Specifically, PDRec first infers the users' dynamic preferences on all items via a time-interval diffusion model and proposes a Historical Behavior Reweighting (HBR) mechanism to identify the high-quality behaviors and suppress noisy behaviors. In addition to the observed items, PDRec proposes a Diffusion-based Positive Augmentation (DPA) strategy to leverage the top-ranked unobserved items as the potential positive samples, bringing in informative and diverse soft signals to alleviate data sparsity. To alleviate the false negative sampling issue, PDRec employs Noise-free Negative Sampling (NNS) to select stable negative samples for ensuring effective model optimization. Extensive experiments and analyses on four datasets have verified the superiority of the proposed PDRec over the state-of-the-art baselines and showcased the universality of PDRec as a flexible plugin for commonly-used sequential encoders in different recommendation scenarios. The code is available in https://github.com/hulkima/PDRec.",
        "authors": [
            "Haokai Ma",
            "Ruobing Xie",
            "Lei Meng",
            "Xin Chen",
            "Xu Zhang",
            "Leyu Lin",
            "Zhanhui Kang"
        ],
        "citations": 18,
        "references": 30,
        "year": 2024
    },
    {
        "title": "Controlling Text-to-Image Diffusion by Orthogonal Finetuning",
        "abstract": "Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images given a few images of a subject and a text prompt, and controllable generation where the goal is to enable the model to take in additional control signals. We empirically show that our OFT framework outperforms existing methods in generation quality and convergence speed.",
        "authors": [
            "Zeju Qiu",
            "Wei-yu Liu",
            "Haiwen Feng",
            "Yuxuan Xue",
            "Yao Feng",
            "Zhen Liu",
            "Dan Zhang",
            "Adrian Weller",
            "B. Scholkopf"
        ],
        "citations": 97,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning",
        "abstract": "Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy, and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate the superiority of our method compared to prior works in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks.",
        "authors": [
            "Zhendong Wang",
            "Jonathan J. Hunt",
            "Mingyuan Zhou"
        ],
        "citations": 259,
        "references": 45,
        "year": 2022
    },
    {
        "title": "Masked Diffusion Transformer is a Strong Image Synthesizer",
        "abstract": "Despite its success in image synthesis, we observe that diffusion probabilistic models (DPMs) often lack contextual reasoning ability to learn the relations among object parts in an image, leading to a slow learning process. To solve this issue, we propose a Masked Diffusion Transformer (MDT) that introduces a mask latent modeling scheme to explicitly enhance the DPMs’ ability to contextual relation learning among object semantic parts in an image. During training, MDT operates in the latent space to mask certain tokens. Then, an asymmetric masking diffusion transformer is designed to predict masked tokens from unmasked ones while maintaining the diffusion generation process. Our MDT can reconstruct the full information of an image from its incomplete contextual input, thus enabling it to learn the associated relations among image tokens. Experimental results show that MDT achieves superior image synthesis performance, e.g., a new SOTA FID score in the ImageNet data set, and has about 3× faster learning speed than the previous SOTA DiT. The source code is released at https://github.com/sail-sg/MDT.",
        "authors": [
            "Shanghua Gao",
            "Pan Zhou",
            "Mingg-Ming Cheng",
            "Shuicheng Yan"
        ],
        "citations": 112,
        "references": 65,
        "year": 2023
    },
    {
        "title": "NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation",
        "abstract": "In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a “coarse-to-fine” process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55min to 26s (by 94.26%) at the same hardware setting when generating 1024 frames. The homepage link is [NUWA-XL](https://msra-nuwa.azurewebsites.net)",
        "authors": [
            "Sheng-Siang Yin",
            "Chenfei Wu",
            "Huan Yang",
            "Jianfeng Wang",
            "Xiaodong Wang",
            "Minheng Ni",
            "Zhengyuan Yang",
            "Linjie Li",
            "Shuguang Liu",
            "Fan Yang",
            "Jianlong Fu",
            "Gong Ming",
            "Lijuan Wang",
            "Zicheng Liu",
            "Houqiang Li",
            "Nan Duan"
        ],
        "citations": 96,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration",
        "abstract": "Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called\"regression to the mean\"effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models. Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality. While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising.",
        "authors": [
            "M. Delbracio",
            "P. Milanfar"
        ],
        "citations": 90,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence",
        "abstract": "Diffusion models have been shown to be capable of generating high-quality images, suggesting that they could contain meaningful internal representations. Unfortunately, the feature maps that encode a diffusion model's internal information are spread not only over layers of the network, but also over diffusion timesteps, making it challenging to extract useful descriptors. We propose Diffusion Hyperfeatures, a framework for consolidating multi-scale and multi-timestep feature maps into per-pixel feature descriptors that can be used for downstream tasks. These descriptors can be extracted for both synthetic and real images using the generation and inversion processes. We evaluate the utility of our Diffusion Hyperfeatures on the task of semantic keypoint correspondence: our method achieves superior performance on the SPair-71k real image benchmark. We also demonstrate that our method is flexible and transferable: our feature aggregation network trained on the inversion features of real image pairs can be used on the generation features of synthetic image pairs with unseen objects and compositions. Our code is available at https://diffusion-hyperfeatures.github.io.",
        "authors": [
            "Grace Luo",
            "Lisa Dunlap",
            "Dong Huk Park",
            "Aleksander Holynski",
            "Trevor Darrell"
        ],
        "citations": 89,
        "references": 50,
        "year": 2023
    },
    {
        "title": "SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D",
        "abstract": "It is inherently ambiguous to lift 2D results from pre-trained diffusion models to a 3D world for text-to-3D generation. 2D diffusion models solely learn view-agnostic priors and thus lack 3D knowledge during the lifting, leading to the multi-view inconsistency problem. We find that this problem primarily stems from geometric inconsistency, and avoiding misplaced geometric structures substantially mitigates the problem in the final outputs. Therefore, we improve the consistency by aligning the 2D geometric priors in diffusion models with well-defined 3D shapes during the lifting, addressing the vast majority of the problem. This is achieved by fine-tuning the 2D diffusion model to be viewpoint-aware and to produce view-specific coordinate maps of canonically oriented 3D objects. In our process, only coarse 3D information is used for aligning. This\"coarse\"alignment not only resolves the multi-view inconsistency in geometries but also retains the ability in 2D diffusion models to generate detailed and diversified high-quality objects unseen in the 3D datasets. Furthermore, our aligned geometric priors (AGP) are generic and can be seamlessly integrated into various state-of-the-art pipelines, obtaining high generalizability in terms of unseen shapes and visual appearance while greatly alleviating the multi-view inconsistency problem. Our method represents a new state-of-the-art performance with an 85+% consistency rate by human evaluation, while many previous methods are around 30%. Our project page is https://sweetdreamer3d.github.io/",
        "authors": [
            "Weiyu Li",
            "Rui Chen",
            "Xuelin Chen",
            "Ping Tan"
        ],
        "citations": 91,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust",
        "abstract": "Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at https://github.com/YuxinWenRick/tree-ring-watermark.",
        "authors": [
            "Yuxin Wen",
            "John Kirchenbauer",
            "Jonas Geiping",
            "T. Goldstein"
        ],
        "citations": 68,
        "references": 42,
        "year": 2023
    },
    {
        "title": "3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors",
        "abstract": "We present a two-stage text-to-3D generation system, namely 3DTopia, which generates high-quality general 3D assets within 5 minutes using hybrid diffusion priors. The first stage samples from a 3D diffusion prior directly learned from 3D data. Specifically, it is powered by a text-conditioned tri-plane latent diffusion model, which quickly generates coarse 3D samples for fast prototyping. The second stage utilizes 2D diffusion priors to further refine the texture of coarse 3D models from the first stage. The refinement consists of both latent and pixel space optimization for high-quality texture generation. To facilitate the training of the proposed system, we clean and caption the largest open-source 3D dataset, Objaverse, by combining the power of vision language models and large language models. Experiment results are reported qualitatively and quantitatively to show the performance of the proposed system. Our codes and models are available at https://github.com/3DTopia/3DTopia",
        "authors": [
            "Fangzhou Hong",
            "Jiaxiang Tang",
            "Ziang Cao",
            "Min Shi",
            "Tong Wu",
            "Zhaoxi Chen",
            "Tengfei Wang",
            "Liang Pan",
            "Dahua Lin",
            "Ziwei Liu"
        ],
        "citations": 26,
        "references": 74,
        "year": 2024
    },
    {
        "title": "HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images",
        "abstract": "Diffusion models have emerged as the best approach for generative modeling of 2D images. Part of their success is due to the possibility of training them on millions if not billions of images with a stable learning objective. However, extending these models to 3D remains difficult for two reasons. First, finding a large quantity of 3D training data is much more complex than for 2D images. Second, while it is conceptually trivial to extend the models to operate on 3D rather than 2D grids, the associated cubic growth in memory and compute complexity makes this infeasible. We address the first challenge by introducing a new diffusion setup that can be trained, end-to-end, with only posed 2D images for supervision; and the second challenge by proposing an image formation model that decouples model memory from spatial memory. We evaluate our method on real-world data, using the CO3D dataset which has not been used to train 3D generative models before. We show that our diffusion models are scalable, train robustly, and are competitive in terms of sample quality and fidelity to existing approaches for 3D generative modeling.",
        "authors": [
            "Animesh Karnewar",
            "A. Vedaldi",
            "David Novotný",
            "N. Mitra"
        ],
        "citations": 98,
        "references": 72,
        "year": 2023
    },
    {
        "title": "Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation",
        "abstract": "Monocular depth estimation is a fundamental computer vision task. Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough. The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures. Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains. This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation. We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge. The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data. It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases. Project page: https://marigoldmonodepth.github.io.",
        "authors": [
            "Bingxin Ke",
            "Anton Obukhov",
            "Shengyu Huang",
            "Nando Metzger",
            "R. C. Daudt",
            "Konrad Schindler"
        ],
        "citations": 86,
        "references": 63,
        "year": 2023
    },
    {
        "title": "FreeU: Free Lunch in Diffusion U-Net",
        "abstract": "In this paper, we uncover the untapped potential of dif-fusion U-Net, which serves as a “free lunch” that substan-tially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to denoising, whereas its skip connections mainly introduce high-frequency features into the de-coder module, causing the potential neglect of crucial functions intrinsic to the backbone network. Capitalizing on this discovery, we propose a simple yet effective method, termed “FreeU”, which enhances generation quality without additional training or finetuning. Our key insight is to strategi-cally re-weight the contributions sourced from the U-Net's skip connections and backbone feature maps, to leverage the strengths of both components of the U-Net architec-ture. Promising results on image and video generation tasks demonstrate that our FreeU can be readily integrated to ex-isting diffusion models, e.g., Stable Diffusion, DreamBooth and ControlNet, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference.",
        "authors": [
            "Chenyang Si",
            "Ziqi Huang",
            "Yuming Jiang",
            "Ziwei Liu"
        ],
        "citations": 85,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise",
        "abstract": "Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models",
        "authors": [
            "Arpit Bansal",
            "Eitan Borgnia",
            "Hong-Min Chu",
            "Jie Li",
            "Hamid Kazemi",
            "Furong Huang",
            "Micah Goldblum",
            "Jonas Geiping",
            "T. Goldstein"
        ],
        "citations": 225,
        "references": 38,
        "year": 2022
    },
    {
        "title": "LayoutDM: Discrete Diffusion Model for Controllable Layout Generation",
        "abstract": "Controllable layout generation aims at synthesizing plausible arrangement of element bounding boxes with optional constraints, such as type or position of a specific element. In this work, we try to solve a broad range of layout generation tasks in a single model that is based on discrete state-space diffusion models. Our model, named Lay-outDM, naturally handles the structured layout data in the discrete representation and learns to progressively infer a noiseless layout from the initial input, where we model the layout corruption process by modality-wise discrete diffusion. For conditional generation, we propose to inject layout constraints in the form of masking or logit adjustment during inference. We show in the experiments that our Lay-outDM successfully generates high-quality layouts and outperforms both task-specific and task-agnostic baselines on several layout tasks. 11Please find the code and models at: https://cyberagentailab.github.io/layout-drn.",
        "authors": [
            "Naoto Inoue",
            "Kotaro Kikuchi",
            "E. Simo-Serra",
            "Mayu Otani",
            "Kota Yamaguchi"
        ],
        "citations": 82,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Leapfrog Diffusion Model for Stochastic Trajectory Prediction",
        "abstract": "To model the indeterminacy of human behaviors, stochastic trajectory prediction requires a sophisticated multi-modal distribution of future trajectories. Emerging diffusion models have revealed their tremendous representation capacities in numerous generation tasks, showing potential for stochastic trajectory prediction. However, expensive time consumption prevents diffusion models from real-time prediction, since a large number of denoising steps are required to assure sufficient representation ability. To resolve the dilemma, we present LEapfrog Diffusion model (LED), a novel diffusion-based trajectory prediction model, which provides real-time, precise, and diverse predictions. The core of the proposed LED is to leverage a trainable leapfrog initializer to directly learn an expressive multi-modal distribution of future trajectories, which skips a large number of denoising steps, significantly accelerating inference speed. Moreover, the leapfrog initializer is trained to appropriately allocate correlated samples to provide a diversity of predicted future trajectories, significantly improving prediction performances. Extensive experiments on four real-world datasets, including NBA/NFL/SDD/ETH-UCY, show that LED consistently improves performance and achieves 23.7%/21.9% ADE/FDE improvement on NFL. The proposed LED also speeds up the inference 19.3/30.8/24.3/25.1 times compared to the standard diffusion model on NBA/NFL/SDD/ETH-UCY, satisfying real-time inference needs. Code is available at https://github.com/MediaBrain-SJTU/LED.",
        "authors": [
            "Wei Mao",
            "Chenxin Xu",
            "Qi Zhu",
            "Siheng Chen",
            "Yanfeng Wang"
        ],
        "citations": 82,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation",
        "abstract": "To achieve the highest perceptual quality, state-of-the-art diffusion models are optimized with objectives that typically look very different from the maximum likelihood and the Evidence Lower Bound (ELBO) objectives. In this work, we reveal that diffusion model objectives are actually closely related to the ELBO. Specifically, we show that all commonly used diffusion model objectives equate to a weighted integral of ELBOs over different noise levels, where the weighting depends on the specific objective used. Under the condition of monotonic weighting, the connection is even closer: the diffusion objective then equals the ELBO, combined with simple data augmentation, namely Gaussian noise perturbation. We show that this condition holds for a number of state-of-the-art diffusion models. In experiments, we explore new monotonic weightings and demonstrate their effectiveness, achieving state-of-the-art FID scores on the high-resolution ImageNet benchmark.",
        "authors": [
            "Diederik P. Kingma",
            "Ruiqi Gao"
        ],
        "citations": 81,
        "references": 51,
        "year": 2023
    },
    {
        "title": "LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On",
        "abstract": "The rapidly evolving fields of e-commerce and metaverse continue to seek innovative approaches to enhance the consumer experience. At the same time, recent advancements in the development of diffusion models have enabled generative networks to create remarkably realistic images. In this context, image-based virtual try-on, which consists in generating a novel image of a target model wearing a given in-shop garment, has yet to capitalize on the potential of these powerful generative solutions. This work introduces LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the Virtual Try-ON task. The proposed architecture relies on a latent diffusion model extended with a novel additional autoencoder module that exploits learnable skip connections to enhance the generation process preserving the model's characteristics. To effectively maintain the texture and details of the in-shop garment, we propose a textual inversion component that can map the visual features of the garment to the CLIP token embedding space and thus generate a set of pseudo-word token embeddings capable of conditioning the generation process. Experimental results on Dress Code and VITON-HD datasets demonstrate that our approach outperforms the competitors by a consistent margin, achieving a significant milestone for the task. Source code and trained models are publicly available at: https://github.com/miccunifi/ladi-vton.",
        "authors": [
            "Davide Morelli",
            "Alberto Baldrati",
            "Giuseppe Cartella",
            "Marcella Cornia",
            "Marco Bertini",
            "R. Cucchiara"
        ],
        "citations": 76,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Guided Motion Diffusion for Controllable Human Motion Synthesis",
        "abstract": "Denoising diffusion models have shown great promise in human motion synthesis conditioned on natural language descriptions. However, integrating spatial constraints, such as pre-defined motion trajectories and obstacles, remains a challenge despite being essential for bridging the gap between isolated human motion and its surrounding environment. To address this issue, we propose Guided Motion Diffusion (GMD), a method that incorporates spatial constraints into the motion generation process. Specifically, we propose an effective feature projection scheme that manipulates motion representation to enhance the coherency between spatial information and local poses. Together with a new imputation formulation, the generated motion can reliably conform to spatial constraints such as global motion trajectories. Furthermore, given sparse spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance approach to turn a sparse signal, which is susceptible to being ignored during the reverse steps, into denser signals to guide the generated motion to the given constraints. Our extensive experiments justify the development of GMD, which achieves a significant improvement over state-of-the-art methods in text-based motion generation while allowing control of the synthesized motions with spatial constraints.",
        "authors": [
            "Korrawe Karunratanakul",
            "Konpat Preechakul",
            "Supasorn Suwajanakorn",
            "Siyu Tang"
        ],
        "citations": 74,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and Personalized Stylization",
        "abstract": "Diffusion models have demonstrated impressive performance in various image generation, editing, enhancement and translation tasks. In particular, the pre-trained text-to-image stable diffusion models provide a potential solution to the challenging realistic image super-resolution (Real-ISR) and image stylization problems with their strong generative priors. However, the existing methods along this line often fail to keep faithful pixel-wise image structures. If extra skip connections between the encoder and the decoder of a VAE are used to reproduce details, additional training in image space will be required, limiting the application to tasks in latent space such as image stylization. In this work, we propose a pixel-aware stable diffusion (PASD) network to achieve robust Real-ISR and personalized image stylization. Specifically, a pixel-aware cross attention module is introduced to enable diffusion models perceiving image local structures in pixel-wise level, while a degradation removal module is used to extract degradation insensitive features to guide the diffusion process together with image high level information. An adjustable noise schedule is introduced to further improve the image restoration results. By simply replacing the base diffusion model with a stylized one, PASD can generate diverse stylized images without collecting pairwise training data, and by shifting the base model with an aesthetic one, PASD can bring old photos back to life. Extensive experiments in a variety of image enhancement and stylization tasks demonstrate the effectiveness of our proposed PASD approach. Our source codes are available at \\url{https://github.com/yangxy/PASD/}.",
        "authors": [
            "Tao Yang",
            "Peiran Ren",
            "Xuansong Xie",
            "Lei Zhang"
        ],
        "citations": 72,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Protein Design with Guided Discrete Diffusion",
        "abstract": "A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to several therapeutic targets under locality and developability constraints, attaining a 99% expression rate and 40% binding rate in exploratory in vitro experiments.",
        "authors": [
            "Nate Gruver",
            "S. Stanton",
            "Nathan C Frey",
            "Tim G. J. Rudner",
            "I. Hotzel",
            "J. Lafrance-Vanasse",
            "A. Rajpal",
            "Kyunghyun Cho",
            "A. Wilson"
        ],
        "citations": 76,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA",
        "abstract": "Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e.,\"person\"for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter costs and requires no storage of user data for replay. We show that C-LoRA not only outperforms several baselines for our proposed setting of text-to-image continual customization, which we refer to as Continual Diffusion, but that we achieve a new state-of-the-art in the well-established rehearsal-free continual learning setting for image classification. The high achieving performance of C-LoRA in two separate domains positions it as a compelling solution for a wide range of applications, and we believe it has significant potential for practical impact. Project page: https://jamessealesmith.github.io/continual-diffusion/",
        "authors": [
            "James Smith",
            "Yen-Chang Hsu",
            "Lingyu Zhang",
            "Ting Hua",
            "Z. Kira",
            "Yilin Shen",
            "Hongxia Jin"
        ],
        "citations": 78,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Collaborative Diffusion for Multi-Modal Face Generation and Editing",
        "abstract": "Diffusion models arise as a powerful generative tool recently. Despite the great progress, existing diffusion models mainly focus on uni-modal control, i.e., the diffusion process is driven by only one modality of condition. To further unleash the users' creativity, it is desirable for the model to be controllable by multiple modalities simultaneously, e.g. generating and editing faces by describing the age (text-driven) while drawing the face shape (mask-driven). In this work, we present Collaborative Diffusion, where pre-trained uni-modal diffusion models collaborate to achieve multi-modal face generation and editing without re-training. Our key insight is that diffusion models driven by different modalities are inherently complementary regarding the latent denoising steps, where bilateral connections can be established upon. Specifically, we propose dynamic diffuser, a meta-network that adaptively hallucinates multimodal denoising steps by predicting the spatial-temporal influence functions for each pre-trained uni-modal model. Collaborative Diffusion not only collaborates generation capabilities from uni-modal diffusion models, but also integrates multiple uni-modal manipulations to perform multi-modal editing. Extensive qualitative and quantitative experiments demonstrate the superiority of our framework in both image quality and condition consistency.",
        "authors": [
            "Ziqi Huang",
            "Kelvin C. K. Chan",
            "Yuming Jiang",
            "Ziwei Liu"
        ],
        "citations": 78,
        "references": 76,
        "year": 2023
    },
    {
        "title": "DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization",
        "abstract": "Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.",
        "authors": [
            "Zhiqing Sun",
            "Yiming Yang"
        ],
        "citations": 77,
        "references": 129,
        "year": 2023
    },
    {
        "title": "UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs",
        "abstract": "Text-to-image diffusion models have demonstrated re-markable capabilities in transforming text prompts into co-herent images, yet the computational cost of the multi-step inference remains a persistent challenge. To address this issue, we present UFOGen, a novel generative model de-signed for ultra-fast, one-step text-to-image generation. In contrast to conventional approaches that focus on improving samplers or employing distillation techniques for diffusion models, UFOGen adopts a hybrid methodology, inte-grating diffusion models with a GAN objective. Leveraging a newly introduced diffusion-GAN objective and initialization with pre-trained diffusion models, UFOGen excels in efficiently generating high-quality images conditioned on textual descriptions in a single step. Beyond traditional text-to-image generation, UFOGen showcases versatility in applications. Notably, UFOGen stands among the pioneering models enabling one-step text-to-image generation and diverse downstream tasks, presenting a significant advance-ment in the landscape of efficient generative models.",
        "authors": [
            "Yanwu Xu",
            "Yang Zhao",
            "Zhisheng Xiao",
            "Tingbo Hou"
        ],
        "citations": 74,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Token Merging for Fast Stable Diffusion",
        "abstract": "The landscape of image generation has been forever changed by open vocabulary diffusion models. However, at their core these models use transformers, which makes generation slow. Better implementations to increase the throughput of these transformers have emerged, but they still evaluate the entire model. In this paper, we instead speed up diffusion models by exploiting natural redundancy in generated images by merging redundant tokens. After making some diffusion-specific improvements to Token Merging (ToMe), our ToMe for Stable Diffusion can reduce the number of tokens in an existing Stable Diffusion model by up to 60% while still producing high quality images with-out any extra training. In the process, we speed up image generation by up to 2× and reduce memory consumption by up to 5.6×. Furthermore, this speed-up stacks with efficient implementations such as xFormers, minimally impacting quality while being up to 5.4× faster for large images. Code is available at https://github.com/dbolya/tomesd.",
        "authors": [
            "Daniel Bolya",
            "Judy Hoffman"
        ],
        "citations": 71,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model",
        "abstract": "The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmentation, whereas the prior methods take a random mix.",
        "authors": [
            "Deepanway Ghosal",
            "Navonil Majumder",
            "Ambuj Mehrish",
            "Soujanya Poria"
        ],
        "citations": 118,
        "references": 41,
        "year": 2023
    },
    {
        "title": "ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation",
        "abstract": "We introduce\"ImageDream,\"an innovative image-prompt, multi-view diffusion model for 3D object generation. ImageDream stands out for its ability to produce 3D models of higher quality compared to existing state-of-the-art, image-conditioned methods. Our approach utilizes a canonical camera coordination for the objects in images, improving visual geometry accuracy. The model is designed with various levels of control at each block inside the diffusion model based on the input image, where global control shapes the overall object layout and local control fine-tunes the image details. The effectiveness of ImageDream is demonstrated through extensive evaluations using a standard prompt list. For more information, visit our project page at https://Image-Dream.github.io.",
        "authors": [
            "Peng Wang",
            "Yichun Shi"
        ],
        "citations": 109,
        "references": 51,
        "year": 2023
    },
    {
        "title": "ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model",
        "abstract": "3D human motion generation is crucial for creative industry. Recent advances rely on generative models with domain knowledge for text-driven motion generation, leading to substantial progress in capturing common motions. However, the performance on more diverse motions remains unsatisfactory. In this work, we propose ReMoDiffuse, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process. ReMoDiffuse enhances the generalizability and diversity of text-driven motion generation with three key designs: 1) Hybrid Retrieval finds appropriate references from the database in terms of both semantic and kinematic similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval knowledge, adapting to the difference between retrieved samples and the target motion sequence. 3) Condition Mixture better utilizes the retrieval database during inference, overcoming the scale sensitivity in classifier-free guidance. Extensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality, especially for more diverse motion generation. Project page: https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html",
        "authors": [
            "Mingyuan Zhang",
            "Xinying Guo",
            "Liang Pan",
            "Zhongang Cai",
            "Fangzhou Hong",
            "Huirong Li",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "citations": 111,
        "references": 30,
        "year": 2023
    },
    {
        "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths",
        "abstract": "Text-to-image generation has recently witnessed remarkable achievements. We introduce a text-conditional image diffusion model, termed RAPHAEL, to generate highly artistic images, which accurately portray the text prompts, encompassing multiple nouns, adjectives, and verbs. This is achieved by stacking tens of mixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling billions of diffusion paths (routes) from the network input to the output. Each path intuitively functions as a\"painter\"for depicting a particular textual concept onto a specified image region at a diffusion timestep. Comprehensive experiments reveal that RAPHAEL outperforms recent cutting-edge models, such as Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both image quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior performance in switching images across diverse styles, such as Japanese comics, realism, cyberpunk, and ink illustration. Secondly, a single model with three billion parameters, trained on 1,000 A100 GPUs for two months, achieves a state-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore, RAPHAEL significantly surpasses its counterparts in human evaluation on the ViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the frontiers of image generation research in both academia and industry, paving the way for future breakthroughs in this rapidly evolving field. More details can be found on a webpage: https://raphael-painter.github.io/.",
        "authors": [
            "Zeyue Xue",
            "Guanglu Song",
            "Qiushan Guo",
            "Boxiao Liu",
            "Zhuofan Zong",
            "Yu Liu",
            "Ping Luo"
        ],
        "citations": 99,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Improving and generalizing flow-based generative models with minibatch optimal transport",
        "abstract": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schr\\\"odinger bridge inference.",
        "authors": [
            "Alexander Tong",
            "Nikolay Malkin",
            "G. Huguet",
            "Yanlei Zhang",
            "Jarrid Rector-Brooks",
            "Kilian Fatras",
            "Guy Wolf",
            "Y. Bengio"
        ],
        "citations": 151,
        "references": 90,
        "year": 2023
    },
    {
        "title": "TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition",
        "abstract": "Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pre-trained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains. Code is available at https://github.com/Shilin-LU/TF-ICON",
        "authors": [
            "Shilin Lu",
            "Yanzhu Liu",
            "A. Kong"
        ],
        "citations": 64,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Diffusion Recommender Model",
        "abstract": "Generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are widely utilized to model the generative process of user interactions. However, they suffer from intrinsic limitations such as the instability of GANs and the restricted representation ability of VAEs. Such limitations hinder the accurate modeling of the complex user interaction generation procedure, such as noisy interactions caused by various interference factors. In light of the impressive advantages of Diffusion Models (DMs) over traditional generative models in image synthesis, we propose a novel Diffusion Recommender Model (named DiffRec) to learn the generative process in a denoising manner. To retain personalized information in user interactions, DiffRec reduces the added noises and avoids corrupting users' interactions into pure noises like in image synthesis. In addition, we extend traditional DMs to tackle the unique challenges in recommendation: high resource costs for large-scale item prediction and temporal shifts of user preference. To this end, we propose two extensions of DiffRec: L-DiffRec clusters items for dimension compression and conducts the diffusion processes in the latent space; and T-DiffRec reweights user interactions based on the interaction timestamps to encode temporal information. We conduct extensive experiments on three datasets under multiple settings (e.g., clean training, noisy training, and temporal training). The empirical results validate the superiority of DiffRec with two extensions over competitive baselines.",
        "authors": [
            "Wenjie Wang",
            "Yiyan Xu",
            "Fuli Feng",
            "Xinyu Lin",
            "X. He",
            "Tat-Seng Chua"
        ],
        "citations": 66,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Natural scene reconstruction from fMRI signals using generative latent diffusion",
        "abstract": null,
        "authors": [
            "Furkan Ozcelik",
            "Rufin VanRullen"
        ],
        "citations": 62,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning",
        "abstract": "Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \\textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, we find \\textsc{MTDiff} outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data synthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given a single demonstration as a prompt, which enhances the low-quality datasets for even unseen tasks.",
        "authors": [
            "Haoran He",
            "Chenjia Bai",
            "Kang Xu",
            "Zhuoran Yang",
            "Weinan Zhang",
            "Dong Wang",
            "Bingyan Zhao",
            "Xuelong Li"
        ],
        "citations": 62,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation",
        "abstract": "We propose Latent-Shift -- an efficient text-to-video generation method based on a pretrained text-to-image generation model that consists of an autoencoder and a U-Net diffusion model. Learning a video diffusion model in the latent space is much more efficient than in the pixel space. The latter is often limited to first generating a low-resolution video followed by a sequence of frame interpolation and super-resolution models, which makes the entire pipeline very complex and computationally expensive. To extend a U-Net from image generation to video generation, prior work proposes to add additional modules like 1D temporal convolution and/or temporal attention layers. In contrast, we propose a parameter-free temporal shift module that can leverage the spatial U-Net as is for video generation. We achieve this by shifting two portions of the feature map channels forward and backward along the temporal dimension. The shifted features of the current frame thus receive the features from the previous and the subsequent frames, enabling motion learning without additional parameters. We show that Latent-Shift achieves comparable or better results while being significantly more efficient. Moreover, Latent-Shift can generate images despite being finetuned for T2V generation.",
        "authors": [
            "Jie An",
            "Songyang Zhang",
            "Harry Yang",
            "Sonal Gupta",
            "Jia-Bin Huang",
            "Jiebo Luo",
            "Xiaoyue Yin"
        ],
        "citations": 89,
        "references": 46,
        "year": 2023
    },
    {
        "title": "One-Step Image Translation with Text-to-Image Models",
        "abstract": "In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and Edge2Image, but with a single-step inference. This work suggests that single-step diffusion models can serve as strong backbones for a range of GAN learning objectives. Our code and models are available at https://github.com/GaParmar/img2img-turbo.",
        "authors": [
            "Gaurav Parmar",
            "Taesung Park",
            "Srinivasa Narasimhan",
            "Jun-Yan Zhu"
        ],
        "citations": 25,
        "references": 79,
        "year": 2024
    },
    {
        "title": "ObjectStitch: Object Compositing with Diffusion Model",
        "abstract": "Object compositing based on 2D images is a challenging problem since it typically involves multiple processing stages such as color harmonization, geometry correction and shadow generation to generate realistic results. Furthermore, annotating training data pairs for compositing requires substantial manual effort from professionals, and is hardly scalable. Thus, with the recent advances in generative models, in this work, we propose a selfsupervised framework for object compositing by leveraging the power of conditional diffusion models. Our framework can hollistically address the object compositing task in a unified model, transforming the viewpoint, geometry, color and shadow of the generated object while requiring no manual labeling. To preserve the input object's characteristics, we introduce a content adaptor that helps to maintain categori-cal semantics and object appearance. A data augmentation method is further adopted to improve the fidelity of the generator. Our method outperforms relevant baselines in both realism and faithfulness of the synthesized result images in a user study on various real-world images.",
        "authors": [
            "Yi-Zhe Song",
            "Zhifei Zhang",
            "Zhe Lin",
            "Scott D. Cohen",
            "Brian L. Price",
            "Jianming Zhang",
            "S. Kim",
            "Daniel G. Aliaga"
        ],
        "citations": 61,
        "references": 53,
        "year": 2023
    },
    {
        "title": "ObjectStitch: Object Compositing with Diffusion Model",
        "abstract": "Object compositing based on 2D images is a challenging problem since it typically involves multiple processing stages such as color harmonization, geometry correction and shadow generation to generate realistic results. Furthermore, annotating training data pairs for compositing requires substantial manual effort from professionals, and is hardly scalable. Thus, with the recent advances in generative models, in this work, we propose a selfsupervised framework for object compositing by leveraging the power of conditional diffusion models. Our framework can hollistically address the object compositing task in a unified model, transforming the viewpoint, geometry, color and shadow of the generated object while requiring no manual labeling. To preserve the input object's characteristics, we introduce a content adaptor that helps to maintain categori-cal semantics and object appearance. A data augmentation method is further adopted to improve the fidelity of the generator. Our method outperforms relevant baselines in both realism and faithfulness of the synthesized result images in a user study on various real-world images.",
        "authors": [
            "Yi-Zhe Song",
            "Zhifei Zhang",
            "Zhe Lin",
            "Scott D. Cohen",
            "Brian L. Price",
            "Jianming Zhang",
            "S. Kim",
            "Daniel G. Aliaga"
        ],
        "citations": 61,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Affordance Diffusion: Synthesizing Hand-Object Interactions",
        "abstract": "Recent successes in image synthesis are powered by large-scale diffusion models. However, most methods are currently limited to either text- or image-conditioned generation for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (i.e., an articulated hand) with a given object. Given an RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. We propose a two-step generative approach: a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes of portable-sized objects. The resulting system allows us to predict descriptive affordance information, such as hand articulation and approaching orientation.",
        "authors": [
            "Yufei Ye",
            "Xueting Li",
            "Abhi Gupta",
            "Shalini De Mello",
            "Stan Birchfield",
            "Jiaming Song",
            "Shubham Tulsiani",
            "Sifei Liu"
        ],
        "citations": 60,
        "references": 97,
        "year": 2023
    },
    {
        "title": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild",
        "abstract": "Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions, we augment pretrained text-to-image diffusion models and introduce a task-aware HyperNet to modulate the diffusion models, enabling the adaptation to different C2I tasks simultaneously. Trained on nine unique C2I tasks, UniControl demonstrates impressive zero-shot generation abilities with unseen visual conditions. Experimental results show that UniControl often surpasses the performance of single-task-controlled methods of comparable model sizes. This control versatility positions UniControl as a significant advancement in the realm of controllable visual generation.",
        "authors": [
            "Can Qin",
            "Shu Zhang",
            "Ning Yu",
            "Yihao Feng",
            "Xinyi Yang",
            "Yingbo Zhou",
            "Haiquan Wang",
            "Juan Carlos Niebles",
            "Caiming Xiong",
            "S. Savarese",
            "Stefano Ermon",
            "Yun Fu",
            "Ran Xu"
        ],
        "citations": 85,
        "references": 65,
        "year": 2023
    },
    {
        "title": "MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation",
        "abstract": "Video prediction is a challenging task. The quality of video frames from current state-of-the-art (SOTA) generative models tends to be poor and generalization beyond the training data is difficult. Furthermore, existing prediction frameworks are typically not capable of simultaneously handling other video-related tasks such as unconditional generation or interpolation. In this work, we devise a general-purpose framework called Masked Conditional Video Diffusion (MCVD) for all of these video synthesis tasks using a probabilistic conditional score-based denoising diffusion model, conditioned on past and/or future frames. We train the model in a manner where we randomly and independently mask all the past frames or all the future frames. This novel but straightforward setup allows us to train a single model that is capable of executing a broad range of video tasks, specifically: future/past prediction -- when only future/past frames are masked; unconditional generation -- when both past and future frames are masked; and interpolation -- when neither past nor future frames are masked. Our experiments show that this approach can generate high-quality frames for diverse types of videos. Our MCVD models are built from simple non-recurrent 2D-convolutional architectures, conditioning on blocks of frames and generating blocks of frames. We generate videos of arbitrary lengths autoregressively in a block-wise manner. Our approach yields SOTA results across standard video prediction and interpolation benchmarks, with computation times for training models measured in 1-12 days using $\\le$ 4 GPUs. Project page: https://mask-cond-video-diffusion.github.io ; Code : https://github.com/voletiv/mcvd-pytorch",
        "authors": [
            "Vikram S. Voleti",
            "Alexia Jolicoeur-Martineau",
            "C. Pal"
        ],
        "citations": 252,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Diffusion Schrödinger Bridge Matching",
        "abstract": "Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schr\\\"odinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting (IMF), a new methodology for solving SB problems, and Diffusion Schr\\\"odinger Bridge Matching (DSBM), a novel numerical algorithm for computing IMF iterates. DSBM significantly improves over previous SB numerics and recovers as special/limiting cases various recent transport methods. We demonstrate the performance of DSBM on a variety of problems.",
        "authors": [
            "Yuyang Shi",
            "Valentin De Bortoli",
            "Andrew Campbell",
            "A. Doucet"
        ],
        "citations": 57,
        "references": 105,
        "year": 2023
    },
    {
        "title": "Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion",
        "abstract": "In this work, we investigate the problem of Model-Agnostic Zero-Shot Classification (MA-ZSC), which refers to training non-specific classification architectures (downstream models) to classify real images without using any real images during training. Recent research has demonstrated that generating synthetic training images using diffusion models provides a potential solution to address MA-ZSC. However, the performance of this approach currently falls short of that achieved by large-scale vision-language models. One possible explanation is a potential significant domain gap between synthetic and real images. Our work offers a fresh perspective on the problem by providing initial insights that MA-ZSC performance can be improved by improving the diversity of images in the generated dataset. We propose a set of modifications to the text-to-image generation process using a pre-trained diffusion model to enhance diversity, which we refer to as our bag of tricks. Our approach shows notable improvements in various classification architectures, with results comparable to state-of-the-art models such as CLIP. To validate our approach, we conduct experiments on CIFAR10, CIFAR100, and EuroSAT, which is particularly difficult for zero-shot classification due to its satellite image domain. We evaluate our approach with five classification architectures, including ResNet and ViT. Our findings provide initial insights into the problem of MA-ZSC using diffusion models. All code is available at https://github.com/Jordan-HS/Diversity_is_Definitely_Needed",
        "authors": [
            "Jordan Shipard",
            "A. Wiliem",
            "Kien Nguyen Thanh",
            "Wei Xiang",
            "C. Fookes"
        ],
        "citations": 57,
        "references": 41,
        "year": 2023
    },
    {
        "title": "DiffuRec: A Diffusion Model for Sequential Recommendation",
        "abstract": "Mainstream solutions to sequential recommendation represent items with fixed vectors. These vectors have limited capability in capturing items’ latent aspects and users’ diverse preferences. As a new generative paradigm, diffusion models have achieved excellent performance in areas like computer vision and natural language processing. To our understanding, its unique merit in representation generation well fits the problem setting of sequential recommendation. In this article, we make the very first attempt to adapt the diffusion model to sequential recommendation and propose DiffuRec for item representation construction and uncertainty injection. Rather than modeling item representations as fixed vectors, we represent them as distributions in DiffuRec, which reflect a user’s multiple interests and an item’s various aspects adaptively. In the diffusion phase, DiffuRec corrupts the target item embedding into a Gaussian distribution via noise adding, which is further applied for sequential item distribution representation generation and uncertainty injection. Afterward, the item representation is fed into an approximator for target item representation reconstruction. In the reverse phase, based on a user’s historical interaction behaviors, we reverse a Gaussian noise into the target item representation, then apply a rounding operation for target item prediction. Experiments over four datasets show that DiffuRec outperforms strong baselines by a large margin.1",
        "authors": [
            "Zihao Li",
            "Aixin Sun",
            "Chenliang Li"
        ],
        "citations": 57,
        "references": 66,
        "year": 2023
    },
    {
        "title": "DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation",
        "abstract": "Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled 3D content creation by optimizing a randomly initialized differentiable 3D representation with score distillation. However, the optimization process suffers slow convergence and the resultant 3D models often exhibit two limitations: (a) quality concerns such as missing attributes and distorted shape and texture; (b) extremely low diversity comparing to text-guided image synthesis. In this paper, we show that the conflict between the 3D optimization process and uniform timestep sampling in score distillation is the main reason for these limitations. To resolve this conflict, we propose to prioritize timestep sampling with monotonically non-increasing functions, which aligns the 3D optimization process with the sampling process of diffusion model. Extensive experiments show that our simple redesign significantly improves 3D content creation with faster convergence, better quality and diversity.",
        "authors": [
            "Yukun Huang",
            "Jianan Wang",
            "Yukai Shi",
            "Xianbiao Qi",
            "Zhengjun Zha",
            "Lei Zhang"
        ],
        "citations": 59,
        "references": 39,
        "year": 2023
    },
    {
        "title": "FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling",
        "abstract": "With the availability of large-scale video datasets and the advances of diffusion models, text-driven video generation has achieved substantial progress. However, existing video generation models are typically trained on a limited number of frames, resulting in the inability to generate high-fidelity long videos during inference. Furthermore, these models only support single-text conditions, whereas real-life scenarios often require multi-text conditions as the video content changes over time. To tackle these challenges, this study explores the potential of extending the text-driven capability to generate longer videos conditioned on multiple texts. 1) We first analyze the impact of initial noise in video diffusion models. Then building upon the observation of noise, we propose FreeNoise, a tuning-free and time-efficient paradigm to enhance the generative capabilities of pretrained video diffusion models while preserving content consistency. Specifically, instead of initializing noises for all frames, we reschedule a sequence of noises for long-range correlation and perform temporal attention over them by window-based function. 2) Additionally, we design a novel motion injection method to support the generation of videos conditioned on multiple text prompts. Extensive experiments validate the superiority of our paradigm in extending the generative capabilities of video diffusion models. It is noteworthy that compared with the previous best-performing method which brought about 255% extra time cost, our method incurs only negligible time cost of approximately 17%. Generated video samples are available at our website: http://haonanqiu.com/projects/FreeNoise.html.",
        "authors": [
            "Haonan Qiu",
            "Menghan Xia",
            "Yong Zhang",
            "Yin-Yin He",
            "Xintao Wang",
            "Ying Shan",
            "Ziwei Liu"
        ],
        "citations": 57,
        "references": 34,
        "year": 2023
    },
    {
        "title": "GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models",
        "abstract": "Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after\"fine-tuning\"on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply\"style cloaks\"to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (>92%) and against adaptive countermeasures (>85%).",
        "authors": [
            "Shawn Shan",
            "Jenna Cryan",
            "Emily Wenger",
            "Haitao Zheng",
            "Rana Hanocka",
            "Ben Y. Zhao"
        ],
        "citations": 147,
        "references": 131,
        "year": 2023
    },
    {
        "title": "DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis",
        "abstract": "We present DiffuScene for indoor 3D scene synthesis based on a novel scene graph denoising diffusion probabilistic model, which generates 3D instance properties stored in a fully-connected scene graph and then retrieves the most similar object geometry for each graph node i.e . object instance which is characterized as a concatenation of different attributes, including location, size, orientation, semantic, and geometry features. Based on this scene graph, we designed a diffusion model to determine the placements and types of 3D instances. Our method can facilitate many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes than state-of-the-art methods. Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.",
        "authors": [
            "Jiapeng Tang",
            "Y. Nie",
            "Lev Markhasin",
            "Angela Dai",
            "Justus Thies",
            "M. Nießner"
        ],
        "citations": 54,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code",
        "abstract": "Text-guided diffusion models have revolutionized image generation and editing, offering exceptional realism and diversity. Specifically, in the context of diffusion-based editing, where a source image is edited according to a target prompt, the process commences by acquiring a noisy latent vector corresponding to the source image via the diffusion model. This vector is subsequently fed into separate source and target diffusion branches for editing. The accuracy of this inversion process significantly impacts the final editing outcome, influencing both essential content preservation of the source image and edit fidelity according to the target prompt. Prior inversion techniques aimed at finding a unified solution in both the source and target diffusion branches. However, our theoretical and empirical analyses reveal that disentangling these branches leads to a distinct separation of responsibilities for preserving essential content and ensuring edit fidelity. Building on this insight, we introduce\"Direct Inversion,\"a novel technique achieving optimal performance of both branches with just three lines of code. To assess image editing performance, we present PIE-Bench, an editing benchmark with 700 images showcasing diverse scenes and editing types, accompanied by versatile annotations and comprehensive evaluation metrics. Compared to state-of-the-art optimization-based inversion techniques, our solution not only yields superior performance across 8 editing methods but also achieves nearly an order of speed-up.",
        "authors": [
            "Xu Ju",
            "Ailing Zeng",
            "Yuxuan Bian",
            "Shaoteng Liu",
            "Qiang Xu"
        ],
        "citations": 54,
        "references": 56,
        "year": 2023
    },
    {
        "title": "ReVersion: Diffusion-Based Relation Inversion from Images",
        "abstract": "Diffusion models gain increasing popularity for their generative capabilities. Recently, there have been surging needs to generate customized images by inverting diffusion models from exemplar images, and existing inversion methods mainly focus on capturing object appearances (i.e., the\"look\"). However, how to invert object relations, another important pillar in the visual world, remains unexplored. In this work, we propose the Relation Inversion task, which aims to learn a specific relation (represented as\"relation prompt\") from exemplar images. Specifically, we learn a relation prompt with a frozen pre-trained text-to-image diffusion model. The learned relation prompt can then be applied to generate relation-specific images with new objects, backgrounds, and styles. To tackle the Relation Inversion task, we propose the ReVersion Framework. Specifically, we propose a novel\"relation-steering contrastive learning\"scheme to steer the relation prompt towards relation-dense regions, and disentangle it away from object appearances. We further devise\"relation-focal importance sampling\"to emphasize high-level interactions over low-level appearances (e.g., texture, color). To comprehensively evaluate this new task, we contribute the ReVersion Benchmark, which provides various exemplar images with diverse relations. Extensive experiments validate the superiority of our approach over existing methods across a wide range of visual relations. Our proposed task and method could be good inspirations for future research in various domains like generative inversion, few-shot learning, and visual relation detection.",
        "authors": [
            "Ziqi Huang",
            "Tianxing Wu",
            "Yuming Jiang",
            "Kelvin C. K. Chan",
            "Ziwei Liu"
        ],
        "citations": 51,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Directed Diffusion: Direct Control of Object Placement through Attention Guidance",
        "abstract": "Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion are able to generate an effectively endless variety of images given only a short text prompt describing the desired image content. In many cases the images are of very high quality. However, these models often struggle to compose scenes containing several key objects such as characters in specified positional relationships. The missing capability to ``direct'' the placement of characters and objects both within and across images is crucial in storytelling, as recognized in the literature on film and animation theory. In this work, we take a particularly straightforward approach to providing the needed direction. Drawing on the observation that the cross-attention maps for prompt words reflect the spatial layout of objects denoted by those words, we introduce an optimization objective that produces ``activation'' at desired positions in these cross-attention maps. The resulting approach is a step toward generalizing the applicability of text-guided diffusion models beyond single images to collections of related images, as in storybooks. Directed Diffusion provides easy high-level positional control over multiple objects, while making use of an existing pre-trained model and maintaining a coherent blend between the positioned objects and the background. Moreover, it requires only a few lines to implement.",
        "authors": [
            "W. Ma",
            "J. P. Lewis",
            "W. Kleijn",
            "Thomas Leung"
        ],
        "citations": 54,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Denoising Diffusion Samplers",
        "abstract": "Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal. While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing theoretical results from denoising diffusion models also provide theoretical guarantees for DDS. We discuss the connections between DDS, optimal control and Schr\\\"odinger bridges and finally demonstrate DDS experimentally on a variety of challenging sampling tasks.",
        "authors": [
            "Francisco Vargas",
            "Will Grathwohl",
            "A. Doucet"
        ],
        "citations": 54,
        "references": 66,
        "year": 2023
    },
    {
        "title": "VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet",
        "abstract": "Recently, diffusion models like StableDiffusion have achieved impressive image generation results. However, the generation process of such diffusion models is uncontrollable, which makes it hard to generate videos with continuous and consistent content. In this work, by using the diffusion model with ControlNet, we proposed a new motion-guided video-to-video translation framework called VideoControlNet to generate various videos based on the given prompts and the condition from the input video. Inspired by the video codecs that use motion information for reducing temporal redundancy, our framework uses motion information to prevent the regeneration of the redundant areas for content consistency. Specifically, we generate the first frame (i.e., the I-frame) by using the diffusion model with ControlNet. Then we generate other key frames (i.e., the P-frame) based on the previous I/P-frame by using our newly proposed motion-guided P-frame generation (MgPG) method, in which the P-frames are generated based on the motion information and the occlusion areas are inpainted by using the diffusion model. Finally, the rest frames (i.e., the B-frame) are generated by using our motion-guided B-frame interpolation (MgBI) module. Our experiments demonstrate that our proposed VideoControlNet inherits the generation capability of the pre-trained large diffusion model and extends the image diffusion model to the video diffusion model by using motion information. More results are provided at our project page.",
        "authors": [
            "Zhihao Hu",
            "Dong Xu"
        ],
        "citations": 53,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Diffusion Action Segmentation",
        "abstract": "Temporal action segmentation is crucial for understanding long-form videos. Previous works on this task commonly adopt an iterative refinement paradigm by using multi-stage models. We propose a novel framework via denoising diffusion models, which nonetheless shares the same inherent spirit of such iterative refinement. In this framework, action predictions are iteratively generated from random noise with input video features as conditions. To enhance the modeling of three striking characteristics of human actions, including the position prior, the boundary ambiguity, and the relational dependency, we devise a unified masking strategy for the conditioning inputs in our framework. Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposed method achieves superior or comparable results to state-of-the-art methods, showing the effectiveness of a generative approach for action segmentation. Code is at tinyurl.com/DiffAct.",
        "authors": [
            "Dao-jun Liu",
            "Qiyue Li",
            "A. Dinh",
            "Ting Jiang",
            "Mubarak Shah",
            "Chan Xu"
        ],
        "citations": 54,
        "references": 79,
        "year": 2023
    },
    {
        "title": "DCFace: Synthetic Face Generation with Dual Condition Diffusion Model",
        "abstract": "Generating synthetic datasets for training face recognition models is challenging because dataset generation entails more than creating high fidelity images. It involves generating multiple images of same subjects under different factors (e.g., variations in pose, illumination, expression, aging and occlusion) which follows the real image conditional distribution. Previous works have studied the generation of synthetic datasets using GAN or 3D models. In this work, we approach the problem from the aspect of combining subject appearance (ID) and external factor (style) conditions. These two conditions provide a direct way to control the inter-class and intra-class variations. To this end, we propose a Dual Condition Face Generator (DCFace) based on a diffusion model. Our novel Patch-wise style extractor and Time-step dependent ID loss enables DCFace to consistently produce face images of the same subject under different styles with precise control. Face recognition models trained on synthetic images from the proposed DCFace provide higher verification accuracies compared to previous works by 6.11% on average in 4 out of 5 test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code Link",
        "authors": [
            "Minchul Kim",
            "Feng Liu",
            "Anil Jain",
            "Xiaoming Liu"
        ],
        "citations": 79,
        "references": 92,
        "year": 2023
    },
    {
        "title": "RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D",
        "abstract": "Lifting 2D diffusion for 3D generation is a challenging problem due to the lack of geometric prior and the complex entanglement of materials and lighting in natural images. Existing methods have shown promise by first creating the geometry through score-distillation sampling (SDS) applied to rendered surface normals, followed by appearance modeling. However, relying on a 2D RGB diffusion model to optimize surface normals is suboptimal due to the distribution discrepancy between natural images and normals maps, leading to instability in optimization. In this paper, recognizing that the normal and depth information effectively describe scene geometry and be auto-matically estimated from images, we propose to learn a generalizable Normal-Depth diffusion model for 3D generation. We achieve this by training on the large-scale LAION dataset together with the generalizable image-to-depth and normal prior models. In an attempt to alleviate the mixed illumination effects in the generated materials, we introduce an albedo diffusion model to impose data-driven constraints on the albedo component. Our experiments show that when integrated into existing text-to-3D pipelines, our models significantly enhance the detail richness, achieving state-of-the-art results. Our project page is at https://aigc3d.github.io/richdreamer/.",
        "authors": [
            "Lingteng Qiu",
            "Guanying Chen",
            "Xiaodong Gu",
            "Qi Zuo",
            "Mutian Xu",
            "Yushuang Wu",
            "Weihao Yuan",
            "Zilong Dong",
            "Liefeng Bo",
            "Xiaoguang Han"
        ],
        "citations": 79,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Unsupervised Semantic Correspondence Using Stable Diffusion",
        "abstract": "Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences - locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets.",
        "authors": [
            "Eric Hedlin",
            "Gopal Sharma",
            "Shweta Mahajan",
            "Hossam N. Isack",
            "Abhishek Kar",
            "A. Tagliasacchi",
            "K. M. Yi"
        ],
        "citations": 62,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Compositional 3D Scene Generation using Locally Conditioned Diffusion",
        "abstract": "Designing complex 3D scenes has been a tedious, manual process requiring domain expertise. Emerging text-to-3D generative models show great promise for making this task more intuitive, but existing approaches are limited to object-level generation. We introduce locally conditioned diffusion as an approach to compositional scene diffusion, providing control over semantic parts using text prompts and bounding boxes while ensuring seamless transitions between these parts. We demonstrate a score distillation sampling-based text-to-3D synthesis pipeline that enables compositional 3D scene generation at a higher fidelity than relevant baselines.",
        "authors": [
            "Ryan Po",
            "Gordon Wetzstein"
        ],
        "citations": 73,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Mo\\^usai: Text-to-Music Generation with Long-Context Latent Diffusion",
        "abstract": "Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another\"language\"of communication -- music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Mo\\^usai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model's competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source libraries with the hope of facilitating future work in the field. We open-source the following: Codes: https://github.com/archinetai/audio-diffusion-pytorch; music samples for this paper: http://bit.ly/44ozWDH; all music samples for all models: https://bit.ly/audio-diffusion.",
        "authors": [
            "Flavio Schneider",
            "Ojasv Kamal",
            "Zhijing Jin",
            "Bernhard Scholkopf"
        ],
        "citations": 76,
        "references": 38,
        "year": 2023
    },
    {
        "title": "NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration",
        "abstract": "Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches.",
        "authors": [
            "A. Sridhar",
            "Dhruv Shah",
            "Catherine Glossop",
            "Sergey Levine"
        ],
        "citations": 70,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Regulating ChatGPT and other Large Generative AI Models",
        "abstract": "Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",
        "authors": [
            "P. Hacker",
            "A. Engel",
            "M. Mauer"
        ],
        "citations": 269,
        "references": 176,
        "year": 2023
    },
    {
        "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget",
        "abstract": "Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.",
        "authors": [
            "Ilia Shumailov",
            "Zakhar Shumaylov",
            "Yiren Zhao",
            "Y. Gal",
            "Nicolas Papernot",
            "Ross Anderson"
        ],
        "citations": 236,
        "references": 30,
        "year": 2023
    },
    {
        "title": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
        "abstract": "During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.",
        "authors": [
            "Roberto Gozalo-Brizuela",
            "E.C. Garrido-Merchán"
        ],
        "citations": 222,
        "references": 35,
        "year": 2023
    },
    {
        "title": "3D Neural Field Generation Using Triplane Diffusion",
        "abstract": "Diffusion models have emerged as the state-of-the-art for image generation, among other tasks. Here, we present an efficient diffusion-based model for 3D-aware generation of neural fields. Our approach pre-processes training data, such as ShapeNet meshes, by converting them to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Thus, our 3D training scenes are all represented by 2D feature planes, and we can directly train existing 2D diffusion models on these representations to generate 3D neural fields with high quality and diversity, outperforming alternative approaches to 3D-aware generation. Our approach requires essential modifications to existing triplane factorization pipelines to make the resulting features easy to learn for the diffusion model. We demonstrate state-of-the-art results on 3D generation on several object classes from ShapeNet.",
        "authors": [
            "J. Shue",
            "Eric Chan",
            "Ryan Po",
            "Zachary Ankner",
            "Jiajun Wu",
            "Gordon Wetzstein"
        ],
        "citations": 205,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation",
        "abstract": "Language-guided image generation has achieved great success nowadays by using diffusion models. However, texts can be less detailed to describe highly-specific subjects such as a particular dog or a certain car, which makes pure text-to-image generation not accurate enough to satisfy user requirements. In this work, we present a novel Unified Multi-Modal Latent Diffusion (UMM-Diffusion) which takes joint texts and images containing specified subjects as input sequences and generates customized images with the subjects. To be more specific, both input texts and images are encoded into one unified multi-modal latent space, in which the input images are learned to be projected to pseudo word embedding and can be further combined with text to guide image generation. Besides, to eliminate the irrelevant parts of the input images such as background or illumination, we propose a novel sampling technique of diffusion models used by the image generator which fuses the results guided by multi-modal input and pure text input. By leveraging the large-scale pre-trained text-to-image generator and the designed image encoder, our method is able to generate high-quality images with complex semantics from both aspects of input texts and images.",
        "authors": [
            "Y. Ma",
            "Huan Yang",
            "Wenjing Wang",
            "Jianlong Fu",
            "Jiaying Liu"
        ],
        "citations": 59,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Improved Techniques for Training Consistency Models",
        "abstract": "Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\\times 64$ respectively in a single sampling step. These scores mark a 3.5$\\times$ and 4$\\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.",
        "authors": [
            "Yang Song",
            "Prafulla Dhariwal"
        ],
        "citations": 106,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models",
        "abstract": "Text-to-image personalization aims to teach a pre-trained diffusion model to reason about novel, user provided concepts, embedding them into new scenes guided by natural language prompts. However, current personalization approaches struggle with lengthy training times, high storage requirements or loss of identity. To overcome these limitations, we propose an encoder-based domain-tuning approach. Our key insight is that by underfitting on a large set of concepts from a given domain, we can improve generalization and create a model that is more amenable to quickly adding novel concepts from the same domain. Specifically, we employ two components: First, an encoder that takes as an input a single image of a target concept from a given domain, e.g. a specific face, and learns to map it into a word-embedding representing the concept. Second, a set of regularized weight-offsets for the text-to-image model that learn how to effectively injest additional concepts. Together, these components are used to guide the learning of unseen concepts, allowing us to personalize a model using only a single image and as few as 5 training steps --- accelerating personalization from dozens of minutes to seconds, while preserving quality. Code and trained encoders will be available at our project page.",
        "authors": [
            "Rinon Gal",
            "Moab Arar",
            "Y. Atzmon",
            "Amit H. Bermano",
            "Gal Chechik",
            "D. Cohen-Or"
        ],
        "citations": 165,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Hierarchical Integration Diffusion Model for Realistic Image Deblurring",
        "abstract": "Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising performance, particularly in terms of details reconstruction. However, the diffusion model requires a large number of inference iterations to recover the clean image from pure Gaussian noise, which consumes massive computational resources. Moreover, the distribution synthesized by the diffusion model is often misaligned with the target results, leading to restrictions in distortion-based metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted latent space to generate the prior feature for the deblurring process. The deblurring process is implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and trained models are available at https://github.com/zhengchen1999/HI-Diff.",
        "authors": [
            "Zheng Chen",
            "Yulun Zhang",
            "Ding Liu",
            "Bin Xia",
            "Jinjin Gu",
            "L. Kong",
            "X. Yuan"
        ],
        "citations": 47,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Object-Centric Slot Diffusion",
        "abstract": "The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. In addition, we conduct a preliminary investigation into the integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world image segmentation and generation. Project page is available at https://latentslotdiffusion.github.io",
        "authors": [
            "Jindong Jiang",
            "Fei Deng",
            "Gautam Singh",
            "S. Ahn"
        ],
        "citations": 47,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Denoising Diffusion Autoencoders are Unified Self-supervised Learners",
        "abstract": "Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations within its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for generative-and-discriminative dual learning. To validate this, we conduct linear probe and finetuning evaluations. Our diffusion-based approach achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to contrastive learning and masked autoencoders for the first time. Transfer learning from ImageNet also confirms the suitability of DDAE for Vision Transformers, suggesting the potential to scale DDAEs as unified foundation models. Code is available at github.com/FutureXiang/ddae.",
        "authors": [
            "Weilai Xiang",
            "Hongyu Yang",
            "Di Huang",
            "Yunhong Wang"
        ],
        "citations": 44,
        "references": 75,
        "year": 2023
    },
    {
        "title": "HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation",
        "abstract": "Recent text-to-3D methods employing diffusion models have made significant advancements in 3D human generation. However, these approaches face challenges due to the limitations of text-to-image diffusion models, which lack an understanding of 3D structures. Consequently, these methods struggle to achieve high-quality human generation, resulting in smooth geometry and cartoon-like appearances. In this paper, we propose HumanNorm, a novel approach for high-quality and realistic 3D human generation. The main idea is to enhance the model's 2D perception of 3D geometry by learning a normal-adapted diffusion model and a normal-aligned diffusion model. The normal-adapted diffusion model can generate high-fidelity normal maps corresponding to user prompts with view-dependent and body-aware text. The normal-aligned diffusion model learns to generate color images aligned with the normal maps, thereby transforming physical geometry details into realistic appearance. Leveraging the proposed normal diffusion model, we devise a progressive geometry generation strategy and a multi-step Score Distillation Sampling (SDS) loss to enhance the performance of 3D human generation. Comprehensive experiments substantiate HumanNorm's ability to generate 3D humans with intricate geometry and realistic appearances. HumanNorm outperforms existing text-to-3D methods in both geometry and texture quality. The project page of HumanNorm is https://humannorm.github.io/.",
        "authors": [
            "Xin Huang",
            "Ruizhi Shao",
            "Qi Zhang",
            "Hongwen Zhang",
            "Yingfa Feng",
            "Yebin Liu",
            "Qing Wang"
        ],
        "citations": 47,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer",
        "abstract": "Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn’t require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method. Code is available at https://github.com/YSerin/ZeCon.",
        "authors": [
            "Serin Yang",
            "Hyunmin Hwang",
            "Jong-Chul Ye"
        ],
        "citations": 43,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion",
        "abstract": "The recent surge in popularity of diffusion models for image generation has brought new attention to the potential of these models in other ar-eas of media synthesis. One area that has yet to be fully explored is the application of diffusion models to music generation. Music generation requires to handle multiple aspects, including the temporal dimension, long-term structure, multiple layers of overlapping sounds, and nuances that only trained listeners can detect. In our work, we investigate the potential of diffusion models for text-conditional music generation. We develop a cascading latent diffusion approach that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. For each model, we make an effort to maintain reasonable inference speed, targeting real-time on a single consumer GPU. In addition to trained models, we provide a collection of open-source libraries with the hope of facilitating future work in the ﬁeld. 1",
        "authors": [
            "Flavio Schneider",
            "Zhijing Jin",
            "B. Schölkopf"
        ],
        "citations": 47,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space",
        "abstract": "Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces Tabsyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capture inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data, (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms existing methods. Specifically, it reduces the error rates by 86% and 67% for column-wise distribution and pair-wise column correlation estimations compared with the most competitive baselines.",
        "authors": [
            "Hengrui Zhang",
            "Jiani Zhang",
            "Balasubramaniam Srinivasan",
            "Zhengyuan Shen",
            "Xiao Qin",
            "Christos Faloutsos",
            "H. Rangwala",
            "G. Karypis"
        ],
        "citations": 45,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Flexible Diffusion Modeling of Long Videos",
        "abstract": "We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.",
        "authors": [
            "William Harvey",
            "Saeid Naderiparizi",
            "Vaden Masrani",
            "Christian Weilbach",
            "Frank Wood"
        ],
        "citations": 241,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Single Motion Diffusion",
        "abstract": "Synthesizing realistic animations of humans, animals, and even imaginary creatures, has long been a goal for artists and computer graphics professionals. Compared to the imaging domain, which is rich with large available datasets, the number of data instances for the motion domain is limited, particularly for the animation of animals and exotic creatures (e.g., dragons), which have unique skeletons and motion patterns. In this work, we present a Single Motion Diffusion Model, dubbed SinMDM, a model designed to learn the internal motifs of a single motion sequence with arbitrary topology and synthesize motions of arbitrary length that are faithful to them. We harness the power of diffusion models and present a denoising network explicitly designed for the task of learning from a single input motion. SinMDM is designed to be a lightweight architecture, which avoids overfitting by using a shallow network with local attention layers that narrow the receptive field and encourage motion diversity. SinMDM can be applied in various contexts, including spatial and temporal in-betweening, motion expansion, style transfer, and crowd animation. Our results show that SinMDM outperforms existing methods both in quality and time-space efficiency. Moreover, while current approaches require additional training for different applications, our work facilitates these applications at inference time. Our code and trained models are available at https://sinmdm.github.io/SinMDM-page.",
        "authors": [
            "Sigal Raab",
            "Inbal Leibovitch",
            "Guy Tevet",
            "Moab Arar",
            "Amit H. Bermano",
            "Daniel Cohen-Or"
        ],
        "citations": 41,
        "references": 113,
        "year": 2023
    },
    {
        "title": "SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation",
        "abstract": "Despite their ability to generate high-resolution and diverse images from text prompts, text-to-image diffusion models often suffer from slow iterative sampling processes. Model distillation is one of the most effective directions to accelerate these models. However, previous distillation methods fail to retain the generation quality while requiring a significant amount of images for training, either from real data or synthetically generated by the teacher model. In response to this limitation, we present a novel image-free distillation scheme named SwiftBrush. Drawing inspiration from text-to-3D synthesis, in which a 3D neural radiance field that aligns with the input prompt can be obtained from a 2D text-to-image diffusion prior via a specialized loss without the use of any 3D data ground-truth, our approach re-purposes that same loss for distilling a pretrained multi-step text-to-image model to a student network that can generate high-fidelity images with just a single inference step. In spite of its simplicity, our model stands as one of the first one-step text-to-image generators that can produce images of comparable quality to Stable Diffusion without reliance on any training image data. Remarkably, Swift-Brush achieves an FID score of 16.67 and a CLIP score of 0.29 on the COCO-30K benchmark, achieving competitive results or even substantially surpassing existing state-of-the-art distillation techniques.",
        "authors": [
            "Thuan Hoang Nguyen",
            "Anh Tran"
        ],
        "citations": 39,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Efficient Diffusion Policies for Offline Reinforcement Learning",
        "abstract": "Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion policy training time from 5 days to 5 hours on gym-locomotion tasks. Moreover, we show that EDP is compatible with various offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on D4RL by large margins over previous methods. Our code is available at https://github.com/sail-sg/edp.",
        "authors": [
            "Bingyi Kang",
            "Xiao Ma",
            "Chao Du",
            "Tianyu Pang",
            "Shuicheng Yan"
        ],
        "citations": 41,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Robust Classification via a Single Diffusion Model",
        "abstract": "Diffusion models have been applied to improve adversarial robustness of image classifiers by purifying the adversarial noises or generating realistic data for adversarial training. However, diffusion-based purification can be evaded by stronger adaptive attacks while adversarial training does not perform well under unseen threats, exhibiting inevitable limitations of these methods. To better harness the expressive power of diffusion models, this paper proposes Robust Diffusion Classifier (RDC), a generative classifier that is constructed from a pre-trained diffusion model to be adversarially robust. RDC first maximizes the data likelihood of a given input and then predicts the class probabilities of the optimized input using the conditional likelihood estimated by the diffusion model through Bayes' theorem. To further reduce the computational cost, we propose a new diffusion backbone called multi-head diffusion and develop efficient sampling strategies. As RDC does not require training on particular adversarial attacks, we demonstrate that it is more generalizable to defend against multiple unseen threats. In particular, RDC achieves $75.67\\%$ robust accuracy against various $\\ell_\\infty$ norm-bounded adaptive attacks with $\\epsilon_\\infty=8/255$ on CIFAR-10, surpassing the previous state-of-the-art adversarial training models by $+4.77\\%$. The results highlight the potential of generative classifiers by employing pre-trained diffusion models for adversarial robustness compared with the commonly studied discriminative classifiers. Code is available at \\url{https://github.com/huanranchen/DiffusionClassifier}.",
        "authors": [
            "Huanran Chen",
            "Yinpeng Dong",
            "Zhengyi Wang",
            "X. Yang",
            "Chen-Dong Duan",
            "Hang Su",
            "Jun Zhu"
        ],
        "citations": 38,
        "references": 92,
        "year": 2023
    },
    {
        "title": "Diff-UNet: A Diffusion Embedded Network for Volumetric Segmentation",
        "abstract": "In recent years, Denoising Diffusion Models have demonstrated remarkable success in generating semantically valuable pixel-wise representations for image generative modeling. In this study, we propose a novel end-to-end framework, called Diff-UNet, for medical volumetric segmentation. Our approach integrates the diffusion model into a standard U-shaped architecture to extract semantic information from the input volume effectively, resulting in excellent pixel-level representations for medical volumetric segmentation. To enhance the robustness of the diffusion model's prediction results, we also introduce a Step-Uncertainty based Fusion (SUF) module during inference to combine the outputs of the diffusion models at each step. We evaluate our method on three datasets, including multimodal brain tumors in MRI, liver tumors, and multi-organ CT volumes, and demonstrate that Diff-UNet outperforms other state-of-the-art methods significantly. Our experimental results also indicate the universality and effectiveness of the proposed model. The proposed framework has the potential to facilitate the accurate diagnosis and treatment of medical conditions by enabling more precise segmentation of anatomical structures. The codes of Diff-UNet are available at https://github.com/ge-xing/Diff-UNet",
        "authors": [
            "Zhaohu Xing",
            "Liang Wan",
            "H. Fu",
            "Guang Yang",
            "Lei Zhu"
        ],
        "citations": 40,
        "references": 26,
        "year": 2023
    },
    {
        "title": "LayoutDM: Transformer-based Diffusion Model for Layout Generation",
        "abstract": "Automatic layout generation that can synthesize high-quality layouts is an important tool for graphic design in many applications. Though existing methods based on generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) have progressed, they still leave much room for improving the quality and diversity of the results. Inspired by the recent success of diffusion models in generating high-quality images, this paper explores their potential for conditional layout generation and proposes Transformer-based Layout Diffusion Model (LayoutDM) by instantiating the conditional denoising diffusion probabilistic model (DDPM) with a purely transformer-based architecture. Instead of using convolutional neural networks, a transformer-based conditional Layout Denoiser is proposed to learn the reverse diffusion process to generate samples from noised layout data. Benefitting from both transformer and DDPM, our LayoutDM is of desired properties such as high-quality generation, strong sample diversity, faithful distribution coverage, and stationary training in comparison to GANs and VAEs. Quantitative and qualitative experimental results show that our method outperforms state-of-the-art generative models in terms of quality and diversity.",
        "authors": [
            "Shang Chai",
            "Liansheng Zhuang",
            "Feng Yan"
        ],
        "citations": 40,
        "references": 48,
        "year": 2023
    },
    {
        "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
        "abstract": "This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.",
        "authors": [
            "Lin Zheng",
            "Jianbo Yuan",
            "Lei Yu",
            "Lingpeng Kong"
        ],
        "citations": 42,
        "references": 99,
        "year": 2023
    },
    {
        "title": "2D medical image synthesis using transformer-based denoising diffusion probabilistic model",
        "abstract": "Objective. Artificial intelligence (AI) methods have gained popularity in medical imaging research. The size and scope of the training image datasets needed for successful AI model deployment does not always have the desired scale. In this paper, we introduce a medical image synthesis framework aimed at addressing the challenge of limited training datasets for AI models. Approach. The proposed 2D image synthesis framework is based on a diffusion model using a Swin-transformer-based network. This model consists of a forward Gaussian noise process and a reverse process using the transformer-based diffusion model for denoising. Training data includes four image datasets: chest x-rays, heart MRI, pelvic CT, and abdomen CT. We evaluated the authenticity, quality, and diversity of the synthetic images using visual Turing assessments conducted by three medical physicists, and four quantitative evaluations: the Inception score (IS), Fréchet Inception Distance score (FID), feature similarity and diversity score (DS, indicating diversity similarity) between the synthetic and true images. To leverage the framework value for training AI models, we conducted COVID-19 classification tasks using real images, synthetic images, and mixtures of both images. Main results. Visual Turing assessments showed an average accuracy of 0.64 (accuracy converging to 50% indicates a better realistic visual appearance of the synthetic images), sensitivity of 0.79, and specificity of 0.50. Average quantitative accuracy obtained from all datasets were IS = 2.28, FID = 37.27, FDS = 0.20, and DS = 0.86. For the COVID-19 classification task, the baseline network obtained an accuracy of 0.88 using a pure real dataset, 0.89 using a pure synthetic dataset, and 0.93 using a dataset mixed of real and synthetic data. Significance. A image synthesis framework was demonstrated for medical image synthesis, which can generate high-quality medical images of different imaging modalities with the purpose of supplementing existing training sets for AI model deployment. This method has potential applications in many data-driven medical imaging research.",
        "authors": [
            "Shaoyan Pan",
            "Tonghe Wang",
            "Richard L. J. Qiu",
            "M. Axente",
            "Chih-Wei Chang",
            "Junbo Peng",
            "Ashish B Patel",
            "Joseph Shelton",
            "Sagar Patel",
            "J. Roper",
            "Xiaofeng Yang"
        ],
        "citations": 71,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack",
        "abstract": "Training text-to-image models with web scale image-text pairs enables the generation of a wide range of visual concepts from text. However, these pre-trained models often face challenges when it comes to generating highly aesthetic images. This creates the need for aesthetic alignment post pre-training. In this paper, we propose quality-tuning to effectively guide a pre-trained model to exclusively generate highly visually appealing images, while maintaining generality across visual concepts. Our key insight is that supervised fine-tuning with a set of surprisingly small but extremely visually appealing images can significantly improve the generation quality. We pre-train a latent diffusion model on $1.1$ billion image-text pairs and fine-tune it with only a few thousand carefully selected high-quality images. The resulting model, Emu, achieves a win rate of $82.9\\%$ compared with its pre-trained only counterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred $68.4\\%$ and $71.3\\%$ of the time on visual appeal on the standard PartiPrompts and our Open User Input benchmark based on the real-world usage of text-to-image models. In addition, we show that quality-tuning is a generic approach that is also effective for other architectures, including pixel diffusion and masked generative transformer models.",
        "authors": [
            "Xiaoliang Dai",
            "Ji Hou",
            "Chih-Yao Ma",
            "Sam S. Tsai",
            "Jialiang Wang",
            "Rui Wang",
            "Peizhao Zhang",
            "Simon Vandenhende",
            "Xiaofang Wang",
            "Abhimanyu Dubey",
            "Matthew Yu",
            "Abhishek Kadian",
            "Filip Radenovic",
            "D. Mahajan",
            "Kunpeng Li",
            "Yue Zhao",
            "Vladan Petrovic",
            "Mitesh Kumar Singh",
            "Simran Motwani",
            "Yiqian Wen",
            "Yi-Zhe Song",
            "Roshan Sumbaly",
            "Vignesh Ramanathan",
            "Zijian He",
            "Péter Vajda",
            "Devi Parikh"
        ],
        "citations": 157,
        "references": 36,
        "year": 2023
    },
    {
        "title": "DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion",
        "abstract": "In this paper, we propose DiffSwap, a diffusion model based framework for high-fidelity and controllable face swapping. Unlike previous work that relies on carefully designed network architectures and loss functions to fuse the information from the source and target faces, we reformulate the face swapping as a conditional inpainting task, performed by a powerful diffusion model guided by the desired face attributes (e.g., identity and landmarks). An important issue that makes it nontrivial to apply diffusion models to face swapping is that we cannot perform the time-consuming multi-step sampling to obtain the generated image during training. To overcome this, we propose a mid-point estimation method to efficiently recover a reasonable diffusion result of the swapped face with only 2 steps, which enables us to introduce identity constraints to improve the face swapping quality. Our framework enjoys several favorable properties more appealing than prior arts: 1) Controllable. Our method is based on conditional masked diffusion on the latent space, where the mask and the conditions can be fully controlled and customized. 2) High-fidelity. The formulation of conditional inpainting can fully exploit the generative ability of diffusion models and can preserve the background of target images with minimal artifacts. 3) Shape-preserving. The controllability of our method enables us to use 3D-aware landmarks as the condition during generation to preserve the shape of the source face. Extensive experiments on both FF++ and FFHQ demonstrate that our method can achieve state-of-the-art face swapping results both qualitatively and quantitatively.",
        "authors": [
            "Wenliang Zhao",
            "Yongming Rao",
            "Weikang Shi",
            "Zuyan Liu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "citations": 36,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Manifold Preserving Guided Diffusion",
        "abstract": "Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.",
        "authors": [
            "Yutong He",
            "Naoki Murata",
            "Chieh-Hsin Lai",
            "Yuhta Takida",
            "Toshimitsu Uesaka",
            "Dongjun Kim",
            "Wei-Hsiang Liao",
            "Yuki Mitsufuji",
            "J. Z. Kolter",
            "Ruslan Salakhutdinov",
            "Stefano Ermon"
        ],
        "citations": 36,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter",
        "abstract": "The pre-trained text-image discriminative models, such as CLIP, has been explored for open-vocabulary semantic segmentation with unsatisfactory results due to the loss of crucial localization information and awareness of object shapes. Recently, there has been a growing interest in expanding the application of generative models from generation tasks to semantic segmentation. These approaches utilize generative models either for generating annotated data or extracting features to facilitate semantic segmentation. This typically involves generating a considerable amount of synthetic data or requiring additional mask annotations. To this end, we uncover the potential of generative text-to-image diffusion models (e.g., Stable Diffusion) as highly efficient open-vocabulary semantic segmenters, and introduce a novel training-free approach named DiffSegmenter. The insight is that to generate realistic objects that are semantically faithful to the input text, both the complete object shapes and the corresponding semantics are implicitly learned by diffusion models. We discover that the object shapes are characterized by the self-attention maps while the semantics are indicated through the cross-attention maps produced by the denoising U-Net, forming the basis of our segmentation results.Additionally, we carefully design effective textual prompts and a category filtering mechanism to further enhance the segmentation results. Extensive experiments on three benchmark datasets show that the proposed DiffSegmenter achieves impressive results for open-vocabulary semantic segmentation.",
        "authors": [
            "Jinglong Wang",
            "Xiawei Li",
            "Jing Zhang",
            "Qingyuan Xu",
            "Qin Zhou",
            "Qian Yu",
            "Lu Sheng",
            "Dong Xu"
        ],
        "citations": 37,
        "references": 50,
        "year": 2023
    },
    {
        "title": "DiffiT: Diffusion Vision Transformers for Image Generation",
        "abstract": "Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves a new SOTA FID score of 1.73 on ImageNet256 dataset while having 19.85%, 16.88% less parameters than other Transformer-based diffusion models such as MDT and DiT,respectively. Code: https://github.com/NVlabs/DiffiT",
        "authors": [
            "Ali Hatamizadeh",
            "Jiaming Song",
            "Guilin Liu",
            "Jan Kautz",
            "Arash Vahdat"
        ],
        "citations": 37,
        "references": 88,
        "year": 2023
    },
    {
        "title": "Torsional Diffusion for Molecular Conformer Generation",
        "abstract": "Molecular conformer generation is a fundamental task in computational chemistry. Several machine learning approaches have been developed, but none have outperformed state-of-the-art cheminformatics methods. We propose torsional diffusion, a novel diffusion framework that operates on the space of torsion angles via a diffusion process on the hypertorus and an extrinsic-to-intrinsic score model. On a standard benchmark of drug-like molecules, torsional diffusion generates superior conformer ensembles compared to machine learning and cheminformatics methods in terms of both RMSD and chemical properties, and is orders of magnitude faster than previous diffusion-based models. Moreover, our model provides exact likelihoods, which we employ to build the first generalizable Boltzmann generator. Code is available at https://github.com/gcorso/torsional-diffusion.",
        "authors": [
            "Bowen Jing",
            "Gabriele Corso",
            "Jeffrey Chang",
            "R. Barzilay",
            "T. Jaakkola"
        ],
        "citations": 226,
        "references": 64,
        "year": 2022
    },
    {
        "title": "Language-Guided Traffic Simulation via Scene-Level Diffusion",
        "abstract": "Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effectiveness of our proposed method in generating realistic, query-compliant traffic simulations.",
        "authors": [
            "Ziyuan Zhong",
            "Davis Rempe",
            "Yuxiao Chen",
            "B. Ivanovic",
            "Yulong Cao",
            "Danfei Xu",
            "M. Pavone",
            "Baishakhi Ray"
        ],
        "citations": 57,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation",
        "abstract": "Preparing training data for deep vision models is a labor-intensive task. To address this, generative models have emerged as an effective solution for generating synthetic data. While current generative models produce image-level category labels, we propose a novel method for generating pixel-level semantic segmentation labels using the text-to-image generative model Stable Diffusion (SD). By utilizing the text prompts, cross-attention, and self-attention of SD, we introduce three new techniques: class-prompt appending, class-prompt cross-attention, and self-attention exponentiation. These techniques enable us to generate segmentation maps corresponding to synthetic images. These maps serve as pseudo-labels for training semantic segmenters, eliminating the need for labor-intensive pixel-wise annotation. To account for the imperfections in our pseudo-labels, we incorporate uncertainty regions into the segmentation, allowing us to disregard loss from those regions. We conduct evaluations on two datasets, PASCAL VOC and MSCOCO, and our approach significantly outperforms concurrent work. Our benchmarks and code will be released at https://github.com/VinAIResearch/Dataset-Diffusion",
        "authors": [
            "Q. Nguyen",
            "T. Vu",
            "A. Tran",
            "Kim Dan Nguyen"
        ],
        "citations": 56,
        "references": 57,
        "year": 2023
    },
    {
        "title": "End-to-End Diffusion Latent Optimization Improves Classifier Guidance",
        "abstract": "Classifier guidance—using the gradients of an image classifier to steer the generations of a diffusion model—has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation’s shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidance: using CLIP guidance to improve generations of complex prompts from DrawBench, using fine-grained visual classifiers to expand the vocabulary of Stable Diffusion, enabling image-conditioned generation with a CLIP visual encoder, and improving image aesthetics using an aesthetic scoring network.",
        "authors": [
            "Bram Wallace",
            "Akash Gokul",
            "Stefano Ermon",
            "N. Naik"
        ],
        "citations": 57,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Diffusion Probabilistic Modeling for Video Generation",
        "abstract": "Denoising diffusion probabilistic models are a promising new class of generative models that mark a milestone in high-quality image generation. This paper showcases their ability to sequentially generate video, surpassing prior methods in perceptual and probabilistic forecasting metrics. We propose an autoregressive, end-to-end optimized video diffusion model inspired by recent advances in neural video compression. The model successively generates future frames by correcting a deterministic next-frame prediction using a stochastic residual generated by an inverse diffusion process. We compare this approach against six baselines on four datasets involving natural and simulation-based videos. We find significant improvements in terms of perceptual quality and probabilistic frame forecasting ability for all datasets.",
        "authors": [
            "Ruihan Yang",
            "Prakhar Srivastava",
            "S. Mandt"
        ],
        "citations": 229,
        "references": 100,
        "year": 2022
    },
    {
        "title": "Multistep Consistency Models",
        "abstract": "Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step. In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas a $\\infty$-step consistency model is a diffusion model. Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 steps with consistency distillation, using simple losses without adversarial training. We also show that our method scales to a text-to-image diffusion model, generating samples that are close to the quality of the original model.",
        "authors": [
            "J. Heek",
            "Emiel Hoogeboom",
            "Tim Salimans"
        ],
        "citations": 22,
        "references": 28,
        "year": 2024
    },
    {
        "title": "Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion",
        "abstract": "Text-to-image generation is a significant domain in modern computer vision and has achieved substantial improvements through the evolution of generative architectures. Among these, there are diffusion-based models that have demonstrated essential quality enhancements. These models are generally split into two categories: pixel-level and latent-level approaches. We present Kandinsky1, a novel exploration of latent diffusion architecture, combining the principles of the image prior models with latent diffusion techniques. The image prior model is trained separately to map text embeddings to image embeddings of CLIP. Another distinct feature of the proposed model is the modified MoVQ implementation, which serves as the image autoencoder component. Overall, the designed model contains 3.3B parameters. We also deployed a user-friendly demo system that supports diverse generative modes such as text-to-image generation, image fusion, text and image fusion, image variations generation, and text-guided inpainting/outpainting. Additionally, we released the source code and checkpoints for the Kandinsky models. Experimental evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking our model as the top open-source performer in terms of measurable image generation quality.",
        "authors": [
            "Anton Razzhigaev",
            "Arseniy Shakhmatov",
            "Anastasia Maltseva",
            "V.Ya. Arkhipkin",
            "Igor Pavlov",
            "Ilya Ryabov",
            "Angelina Kuts",
            "Alexander Panchenko",
            "Andrey Kuznetsov",
            "Denis Dimitrov"
        ],
        "citations": 51,
        "references": 44,
        "year": 2023
    },
    {
        "title": "DiffTraj: Generating GPS Trajectory with Diffusion Probabilistic Model",
        "abstract": "Pervasive integration of GPS-enabled devices and data acquisition technologies has led to an exponential increase in GPS trajectory data, fostering advancements in spatial-temporal data mining research. Nonetheless, GPS trajectories contain personal geolocation information, rendering serious privacy concerns when working with raw data. A promising approach to address this issue is trajectory generation, which involves replacing original data with generated, privacy-free alternatives. Despite the potential of trajectory generation, the complex nature of human behavior and its inherent stochastic characteristics pose challenges in generating high-quality trajectories. In this work, we propose a spatial-temporal diffusion probabilistic model for trajectory generation (DiffTraj). This model effectively combines the generative abilities of diffusion models with the spatial-temporal features derived from real trajectories. The core idea is to reconstruct and synthesize geographic trajectories from white noise through a reverse trajectory denoising process. Furthermore, we propose a Trajectory UNet (Traj-UNet) deep neural network to embed conditional information and accurately estimate noise levels during the reverse process. Experiments on two real-world datasets show that DiffTraj can be intuitively applied to generate high-fidelity trajectories while retaining the original distributions. Moreover, the generated results can support downstream trajectory analysis tasks and significantly outperform other methods in terms of geo-distribution evaluations.",
        "authors": [
            "Yuanshao Zhu",
            "Yongchao Ye",
            "Shiyao Zhang",
            "Xiangyu Zhao",
            "James J. Q. Yu"
        ],
        "citations": 33,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Learning Diffusion Bridges on Constrained Domains",
        "abstract": "Diffusion models have achieved promising results on generative learning recently. However, because diffusion processes are most naturally applied on the unconstrained Euclidean space R d , key challenges arise for developing diffusion based models for learning data on constrained and structured domains. We present a simple and unified framework to achieve this that can be easily adopted to various types of domains, including product spaces of any type (be it bounded/unbounded, continuous/discrete, categorical/ordinal",
        "authors": [
            "Xingchao Liu",
            "Lemeng Wu",
            "Mao Ye",
            "Qiang Liu"
        ],
        "citations": 32,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Knowledge Diffusion for Distillation",
        "abstract": "The representation gap between teacher and student is an emerging topic in knowledge distillation (KD). To reduce the gap and improve the performance, current methods often resort to complicated training schemes, loss functions, and feature alignments, which are task-specific and feature-specific. In this paper, we state that the essence of these methods is to discard the noisy information and distill the valuable information in the feature, and propose a novel KD method dubbed DiffKD, to explicitly denoise and match features using diffusion models. Our approach is based on the observation that student features typically contain more noises than teacher features due to the smaller capacity of student model. To address this, we propose to denoise student features using a diffusion model trained by teacher features. This allows us to perform better distillation between the refined clean feature and teacher feature. Additionally, we introduce a light-weight diffusion model with a linear autoencoder to reduce the computation cost and an adaptive noise matching module to improve the denoising performance. Extensive experiments demonstrate that DiffKD is effective across various types of features and achieves state-of-the-art performance consistently on image classification, object detection, and semantic segmentation tasks. Code is available at https://github.com/hunto/DiffKD.",
        "authors": [
            "Tao Huang",
            "Yuan Zhang",
            "Mingkai Zheng",
            "Shan You",
            "Fei Wang",
            "Chen Qian",
            "Chang Xu"
        ],
        "citations": 32,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Global Structure-Aware Diffusion Process for Low-Light Image Enhancement",
        "abstract": "This paper studies a diffusion-based framework to address the low-light image enhancement problem. To harness the capabilities of diffusion models, we delve into this intricate process and advocate for the regularization of its inherent ODE-trajectory. To be specific, inspired by the recent research that low curvature ODE-trajectory results in a stable and effective diffusion process, we formulate a curvature regularization term anchored in the intrinsic non-local structures of image data, i.e., global structure-aware regularization, which gradually facilitates the preservation of complicated details and the augmentation of contrast during the diffusion process. This incorporation mitigates the adverse effects of noise and artifacts resulting from the diffusion process, leading to a more precise and flexible enhancement. To additionally promote learning in challenging regions, we introduce an uncertainty-guided regularization technique, which wisely relaxes constraints on the most extreme regions of the image. Experimental evaluations reveal that the proposed diffusion-based framework, complemented by rank-informed regularization, attains distinguished performance in low-light enhancement. The outcomes indicate substantial advancements in image quality, noise suppression, and contrast amplification in comparison with state-of-the-art methods. We believe this innovative approach will stimulate further exploration and advancement in low-light image processing, with potential implications for other applications of diffusion models. The code is publicly available at https://github.com/jinnh/GSAD.",
        "authors": [
            "Jinhui Hou",
            "Zhiyu Zhu",
            "Junhui Hou",
            "Hui Liu",
            "Huanqiang Zeng",
            "Hui Yuan"
        ],
        "citations": 43,
        "references": 68,
        "year": 2023
    },
    {
        "title": "Guided Image Synthesis via Initial Image Editing in Diffusion Model",
        "abstract": "Diffusion models have the ability to generate high quality images by denoising pure Gaussian noise images. While previous research has primarily focused on improving the control of image generation through adjusting the denoising process, we propose a novel direction of manipulating the initial noise to control the generated image. Through experiments on stable diffusion, we show that blocks of pixels in the initial latent images have a preference for generating specific content, and that modifying these blocks can significantly influence the generated image. In particular, we show that modifying a part of the initial image affects the corresponding region of the generated image while leaving other regions unaffected, which is useful for repainting tasks. Furthermore, we find that the generation preferences of pixel blocks are primarily determined by their values, rather than their position. By moving pixel blocks with a tendency to generate user-desired content to user-specified regions, our approach achieves state-of-the-art performance in layout-to-image generation. Our results highlight the flexibility and power of initial image manipulation in controlling the generated image.",
        "authors": [
            "Jiafeng Mao",
            "Xueting Wang",
            "K. Aizawa"
        ],
        "citations": 44,
        "references": 33,
        "year": 2023
    },
    {
        "title": "Motion-Conditioned Diffusion Model for Controllable Video Synthesis",
        "abstract": "Recent advancements in diffusion models have greatly improved the quality and diversity of synthesized content. To harness the expressive power of diffusion models, researchers have explored various controllable mechanisms that allow users to intuitively guide the content synthesis process. Although the latest efforts have primarily focused on video synthesis, there has been a lack of effective methods for controlling and describing desired content and motion. In response to this gap, we introduce MCDiff, a conditional diffusion model that generates a video from a starting image frame and a set of strokes, which allow users to specify the intended content and dynamics for synthesis. To tackle the ambiguity of sparse motion inputs and achieve better synthesis quality, MCDiff first utilizes a flow completion model to predict the dense video motion based on the semantic understanding of the video frame and the sparse motion control. Then, the diffusion model synthesizes high-quality future frames to form the output video. We qualitatively and quantitatively show that MCDiff achieves the state-the-of-art visual quality in stroke-guided controllable video synthesis. Additional experiments on MPII Human Pose further exhibit the capability of our model on diverse content and motion synthesis.",
        "authors": [
            "Tsai-Shien Chen",
            "C. Lin",
            "Hung-Yu Tseng",
            "Tsung-Yi Lin",
            "Ming Yang"
        ],
        "citations": 46,
        "references": 55,
        "year": 2023
    },
    {
        "title": "PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics",
        "abstract": "In this paper, we present a new method to efficiently generate jets in High Energy Physics called PC-JeDi. This method utilises score-based diffusion models in conjunction with transformers which are well suited to the task of generating jets as particle clouds due to their permutation equivariance. PC-JeDi achieves competitive performance with current state-of-the-art methods across several metrics that evaluate the quality of the generated jets. Although slower than other models, due to the large number of forward passes required by diffusion models, it is still substantially faster than traditional detailed simulation. Furthermore, PC-JeDi uses conditional generation to produce jets with a desired mass and transverse momentum for two different particles, top quarks and gluons.",
        "authors": [
            "M. Leigh",
            "Debasish Sengupta",
            "Guillaume Qu'etant",
            "J. Raine",
            "K. Zoch",
            "T. Golling"
        ],
        "citations": 44,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Diffusion-Based Signed Distance Fields for 3D Shape Generation",
        "abstract": "We propose a 3D shape generation framework (SDF-Diffusion in short) that uses denoising diffusion models with continuous 3D representation via signed distance fields (SDF). Unlike most existing methods that depend on discontinuous forms, such as point clouds, SDF-Diffusion generates high-resolution 3D shapes while alleviating memory issues by separating the generative process into two-stage: generation and super-resolution. In the first stage, a diffusion-based generative model generates a low-resolution SDF of 3D shapes. Using the estimated low-resolution SDF as a condition, the second stage diffusion model performs super-resolution to generate high-resolution SDF. Our framework can generate a high-fidelity 3D shape despite the extreme spatial complexity. On the ShapeNet dataset, our model shows competitive performance to the state-of-the-art methods and shows applicability on the shape completion task without modification.",
        "authors": [
            "Jaehyeok Shim",
            "Changwoo Kang",
            "Kyungdon Joo"
        ],
        "citations": 38,
        "references": 86,
        "year": 2023
    },
    {
        "title": "EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior",
        "abstract": "While the image diffusion model has made significant strides in text-driven 3D content creation, it often falls short in accurately capturing the intended meaning of the text prompt, particularly with respect to direction information. This shortcoming gives rise to the Janus problem, where multi-faced 3D models are produced with the guidance of such diffusion models. In this paper, we present a robust pipeline for generating high-fidelity 3D content with orthogonal-view image guidance. Specifically, we introduce a novel 2D diffusion model that generates an image",
        "authors": [
            "Minda Zhao",
            "Chaoyi Zhao",
            "Xinyue Liang",
            "Lincheng Li",
            "Zeng Zhao",
            "Zhipeng Hu",
            "Changjie Fan",
            "Xin Yu"
        ],
        "citations": 41,
        "references": 44,
        "year": 2023
    },
    {
        "title": "DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification",
        "abstract": "Diffusion Probabilistic Models have recently shown remarkable performance in generative image modeling, attracting significant attention in the computer vision community. However, while a substantial amount of diffusion-based research has focused on generative tasks, few studies have applied diffusion models to general medical image classification. In this paper, we propose the first diffusion-based model (named DiffMIC) to address general medical image classification by eliminating unexpected noise and perturbations in medical images and robustly capturing semantic representation. To achieve this goal, we devise a dual conditional guidance strategy that conditions each diffusion step with multiple granularities to improve step-wise regional attention. Furthermore, we propose learning the mutual information in each granularity by enforcing Maximum-Mean Discrepancy regularization during the diffusion forward process. We evaluate the effectiveness of our DiffMIC on three medical classification tasks with different image modalities, including placental maturity grading on ultrasound images, skin lesion classification using dermatoscopic images, and diabetic retinopathy grading using fundus images. Our experimental results demonstrate that DiffMIC outperforms state-of-the-art methods by a significant margin, indicating the universality and effectiveness of the proposed model. Our code will be publicly available at https://github.com/scott-yjyang/DiffMIC.",
        "authors": [
            "Yijun Yang",
            "H. Fu",
            "Angelica I. Avilés-Rivero",
            "C. Schonlieb",
            "Lei Zhu"
        ],
        "citations": 39,
        "references": 31,
        "year": 2023
    },
    {
        "title": "GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration",
        "abstract": "Pre-trained diffusion models have been successfully used as priors in a variety of linear inverse problems, where the goal is to reconstruct a signal from noisy linear measurements. However, existing approaches require knowledge of the linear operator. In this paper, we propose GibbsDDRM, an extension of Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the linear measurement operator is unknown. GibbsDDRM constructs a joint distribution of the data, measurements, and linear operator by using a pre-trained diffusion model for the data prior, and it solves the problem by posterior sampling with an efficient variant of a Gibbs sampler. The proposed method is problem-agnostic, meaning that a pre-trained diffusion model can be applied to various inverse problems without fine-tuning. In experiments, it achieved high performance on both blind image deblurring and vocal dereverberation tasks, despite the use of simple generic priors for the underlying linear operators.",
        "authors": [
            "Naoki Murata",
            "Koichi Saito",
            "Chieh-Hsin Lai",
            "Yuhta Takida",
            "Toshimitsu Uesaka",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "citations": 38,
        "references": 68,
        "year": 2023
    },
    {
        "title": "DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting",
        "abstract": "While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for efficiently training diffusion models for probabilistic spatiotemporal forecasting, where generating stable and accurate rollout forecasts remains challenging, Our method, DYffusion, leverages the temporal dynamics in the data, directly coupling it with the diffusion steps in the model. We train a stochastic, time-conditioned interpolator and a forecaster network that mimic the forward and reverse processes of standard diffusion models, respectively. DYffusion naturally facilitates multi-step and long-range forecasting, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process in DYffusion imposes a strong inductive bias and significantly improves computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic forecasting of complex dynamics in sea surface temperatures, Navier-Stokes flows, and spring mesh systems.",
        "authors": [
            "Salva Rühling Cachay",
            "Bo Zhao",
            "Hailey James",
            "Rose Yu"
        ],
        "citations": 39,
        "references": 78,
        "year": 2023
    },
    {
        "title": "DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration",
        "abstract": "Diffusion models have recently received a surge of interest due to their impressive performance for image restoration, especially in terms of noise robustness. However, existing diffusion-based methods are trained on a large amount of training data and perform very well in-distribution, but can be quite susceptible to distribution shift. This is especially inappropriate for data-starved hyperspectral image (HSI) restoration. To tackle this problem, this work puts forth a self-supervised diffusion model for HSI restoration, namely Denoising Diffusion Spatio-Spectral Model (DDS2M), which works by inferring the parameters of the proposed Variational Spatio-Spectral Module (VS2M) during the reverse diffusion process, solely using the degraded HSI without any extra training data. In VS2M, a variational inference-based loss function is customized to enable the untrained spatial and spectral networks to learn the posterior distribution, which serves as the transitions of the sampling chain to help reverse the diffusion process. Benefiting from its self-supervised nature and the diffusion process, DDS2M enjoys stronger generalization ability to various HSIs compared to existing diffusion-based methods and superior robustness to noise compared to existing HSI restoration methods. Extensive experiments on HSI denoising, noisy HSI completion and super-resolution on a variety of HSIs demonstrate DDS2M’s superiority over the existing task-specific state-of-the-arts. Code is available at: https://github.com/miaoyuchun/DDS2M.",
        "authors": [
            "Yuchun Miao",
            "Lefei Zhang",
            "L. Zhang",
            "D. Tao"
        ],
        "citations": 29,
        "references": 60,
        "year": 2023
    },
    {
        "title": "SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation",
        "abstract": "We present a cascaded diffusion model based on a part-level implicit 3D representation. Our model achieves state-of-the-art generation quality and also enables part-level shape editing and manipulation without any additional training in conditional setup. Diffusion models have demonstrated impressive capabilities in data generation as well as zero-shot completion and editing via a guided reverse process. Recent research on 3D diffusion models has focused on improving their generation capabilities with various data representations, while the absence of structural information has limited their capability in completion and editing tasks. We thus propose our novel diffusion model using a part-level implicit representation. To effectively learn diffusion with high-dimensional embedding vectors of parts, we propose a cascaded framework, learning diffusion first on a low-dimensional subspace encoding extrinsic parameters of parts and then on the other high-dimensional subspace encoding intrinsic attributes. In the experiments, we demonstrate the outperformance of our method compared with the previous ones both in generation and part-level completion and manipulation tasks. Our project page is https://salad3d.github.io.",
        "authors": [
            "Juil Koo",
            "Seungwoo Yoo",
            "Minh Hoai Nguyen",
            "Minhyuk Sung"
        ],
        "citations": 29,
        "references": 72,
        "year": 2023
    },
    {
        "title": "CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography",
        "abstract": "Current image steganography techniques are mainly focused on cover-based methods, which commonly have the risk of leaking secret images and poor robustness against degraded container images. Inspired by recent developments in diffusion models, we discovered that two properties of diffusion models, the ability to achieve translation between two images without training, and robustness to noisy data, can be used to improve security and natural robustness in image steganography tasks. For the choice of diffusion model, we selected Stable Diffusion, a type of conditional diffusion model, and fully utilized the latest tools from open-source communities, such as LoRAs and ControlNets, to improve the controllability and diversity of container images. In summary, we propose a novel image steganography framework, named Controllable, Robust and Secure Image Steganography (CRoSS), which has significant advantages in controllability, robustness, and security compared to cover-based image steganography methods. These benefits are obtained without additional training. To our knowledge, this is the first work to introduce diffusion models to the field of image steganography. In the experimental section, we conducted detailed experiments to demonstrate the advantages of our proposed CRoSS framework in controllability, robustness, and security.",
        "authors": [
            "Jiwen Yu",
            "Xuanyu Zhang",
            "You-song Xu",
            "Jian Zhang"
        ],
        "citations": 29,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
        "abstract": "Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$\\%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",
        "authors": [
            "Aaron Lou",
            "Chenlin Meng",
            "Stefano Ermon"
        ],
        "citations": 28,
        "references": 58,
        "year": 2023
    },
    {
        "title": "AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration",
        "abstract": "Diffusion models are emerging expressive generative models, in which a large number of time steps (inference steps) are required for a single image generation. To accelerate such tedious process, reducing steps uniformly is considered as an undisputed principle of diffusion models. We consider that such a uniform assumption is not the optimal solution in practice; i.e., we can find different optimal time steps for different models. Therefore, we propose to search the optimal time steps sequence and compressed model architecture in a unified framework to achieve effective image generation for diffusion models without any further training. Specifically, we first design a unified search space that consists of all possible time steps and various architectures. Then, a two stage evolutionary algorithm is introduced to find the optimal solution in the designed search space. To further accelerate the search process, we employ FID score between generated and real samples to estimate the performance of the sampled examples. As a result, the proposed method is (i).training-free, obtaining the optimal time steps and model architecture without any training process; (ii). orthogonal to most advanced diffusion samplers and can be integrated to gain better sample quality. (iii). generalized, where the searched time steps and architectures can be directly applied on different diffusion models with the same guidance scale. Experimental results show that our method achieves excellent performance by using only a few time steps, e.g. 17.86 FID score on ImageNet 64 × 64 with only four steps, compared to 138.66 with DDIM.",
        "authors": [
            "Lijiang Li",
            "Huixia Li",
            "Xiawu Zheng",
            "Jie Wu",
            "Xuefeng Xiao",
            "Rui Wang",
            "Minghang Zheng",
            "Xin Pan",
            "Fei Chao",
            "R. Ji"
        ],
        "citations": 28,
        "references": 44,
        "year": 2023
    },
    {
        "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
        "abstract": "Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained with a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right. In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be $100\\times\\sim600\\times$ faster when achieving comparable results. Our code is available at https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.",
        "authors": [
            "Tong Wu",
            "Zhihao Fan",
            "Xiao Liu",
            "Yeyun Gong",
            "Yelong Shen",
            "Jian Jiao",
            "Haitao Zheng",
            "Juntao Li",
            "Zhongyu Wei",
            "Jian Guo",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "citations": 35,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Jet Diffusion versus JetGPT -- Modern Networks for the LHC",
        "abstract": "We introduce two diffusion models and an autoregressive transformer for LHC physics simulations. Bayesian versions allow us to control the networks and capture training uncertainties. After illustrating their different density estimation methods for simple toy models, we discuss their advantages for Z plus jets event generation. While diffusion networks excel through their precision, the transformer scales best with the phase space dimensionality. Given the different training and evaluation speed, we expect LHC physics to benefit from dedicated use cases for normalizing flows, diffusion models, and autoregressive transformers.",
        "authors": [
            "A. Butter",
            "Nathan Huetsch",
            "Sofia Palacios Schweitzer",
            "T. Plehn",
            "P. Sorrenson",
            "Jonas Spinner"
        ],
        "citations": 35,
        "references": 84,
        "year": 2023
    },
    {
        "title": "ControlCom: Controllable Image Composition using Diffusion Model",
        "abstract": "Image composition targets at synthesizing a realistic composite image from a pair of foreground and background images. Recently, generative composition methods are built on large pretrained diffusion models to generate composite images, considering their great potential in image generation. However, they suffer from lack of controllability on foreground attributes and poor preservation of foreground identity. To address these challenges, we propose a controllable image composition method that unifies four tasks in one diffusion model: image blending, image harmonization, view synthesis, and generative composition. Meanwhile, we design a self-supervised training framework coupled with a tailored pipeline of training data preparation. Moreover, we propose a local enhancement module to enhance the foreground details in the diffusion model, improving the foreground fidelity of composite images. The proposed method is evaluated on both public benchmark and real-world data, which demonstrates that our method can generate more faithful and controllable composite images than existing approaches. The code and model will be available at https://github.com/bcmi/ControlCom-Image-Composition.",
        "authors": [
            "Bo Zhang",
            "Yuxuan Duan",
            "Jun Lan",
            "Y. Hong",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Li Niu"
        ],
        "citations": 26,
        "references": 61,
        "year": 2023
    },
    {
        "title": "MatFusion: A Generative Diffusion Model for SVBRDF Capture",
        "abstract": "We formulate SVBRDF estimation from photographs as a diffusion task. To model the distribution of spatially varying materials, we first train a novel unconditional SVBRDF diffusion backbone model on a large set of 312, 165 synthetic spatially varying material exemplars. This SVBRDF diffusion backbone model, named MatFusion, can then serve as a basis for refining a conditional diffusion model to estimate the material properties from a photograph under controlled or uncontrolled lighting. Our backbone MatFusion model is trained using only a loss on the reflectance properties, and therefore refinement can be paired with more expensive rendering methods without the need for backpropagation during training. Because the conditional SVBRDF diffusion models are generative, we can synthesize multiple SVBRDF estimates from the same input photograph from which the user can select the one that best matches the users’ expectation. We demonstrate the flexibility of our method by refining different SVBRDF diffusion models conditioned on different types of incident lighting, and show that for a single photograph under colocated flash lighting our method achieves equal or better accuracy than existing SVBRDF estimation methods.",
        "authors": [
            "Sam Sartor",
            "Pieter Peers"
        ],
        "citations": 23,
        "references": 47,
        "year": 2023
    },
    {
        "title": "BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation",
        "abstract": null,
        "authors": [
            "Tao Chen",
            "Chenhui Wang",
            "Hongming Shan"
        ],
        "citations": 24,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Watermarking Diffusion Model",
        "abstract": "The availability and accessibility of diffusion models (DMs) have significantly increased in recent years, making them a popular tool for analyzing and predicting the spread of information, behaviors, or phenomena through a population. Particularly, text-to-image diffusion models (e.g., DALLE 2 and Latent Diffusion Models (LDMs) have gained significant attention in recent years for their ability to generate high-quality images and perform various image synthesis tasks. Despite their widespread adoption in many fields, DMs are often susceptible to various intellectual property violations. These can include not only copyright infringement but also more subtle forms of misappropriation, such as unauthorized use or modification of the model. Therefore, DM owners must be aware of these potential risks and take appropriate steps to protect their models. In this work, we are the first to protect the intellectual property of DMs. We propose a simple but effective watermarking scheme that injects the watermark into the DMs and can be verified by the pre-defined prompts. In particular, we propose two different watermarking methods, namely NAIVEWM and FIXEDWM. The NAIVEWM method injects the watermark into the LDMs and activates it using a prompt containing the watermark. On the other hand, the FIXEDWM is considered more advanced and stealthy compared to the NAIVEWM, as it can only activate the watermark when using a prompt containing a trigger in a fixed position. We conducted a rigorous evaluation of both approaches, demonstrating their effectiveness in watermark injection and verification with minimal impact on the LDM's functionality.",
        "authors": [
            "Yugeng Liu",
            "Zheng Li",
            "M. Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "citations": 26,
        "references": 29,
        "year": 2023
    },
    {
        "title": "A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion",
        "abstract": "Despite the record-breaking performance in Text-to-Image (T2I) generation by Stable Diffusion, less research attention is paid to its adversarial robustness. In this work, we study the problem of adversarial attack generation for Stable Diffusion and ask if an adversarial text prompt can be obtained even in the absence of end-to-end model queries. We call the resulting problem ‘query-free attack generation’. To resolve this problem, we show that the vulnerability of T2I models is rooted in the lack of robustness of text encoders, e.g., the CLIP text encoder used for attacking Stable Diffusion. Based on such insight, we propose both untargeted and targeted query-free attacks, where the former is built on the most influential dimensions in the text embedding space, which we call steerable key dimensions. By leveraging the proposed attacks, we empirically show that only a five-character perturbation to the text prompt is able to cause the significant content shift of synthesized images using Stable Diffusion. Moreover, we show that the proposed target attack can precisely steer the diffusion model to scrub the targeted image content without causing much change in untargeted image content.",
        "authors": [
            "Haomin Zhuang",
            "Yihua Zhang",
            "Sijia Liu"
        ],
        "citations": 47,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Autoregressive Diffusion Model for Graph Generation",
        "abstract": "Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \\emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \\emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \\emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutation invariance of graph, we show that the two networks can be jointly trained by optimizing a simple lower bound of data likelihood. Our experiments on six diverse generic graph datasets and two molecule datasets show that our model achieves better or comparable generation performance with previous state-of-the-art, and meanwhile enjoys fast generation speed.",
        "authors": [
            "Lingkai Kong",
            "Jiaming Cui",
            "Haotian Sun",
            "Yuchen Zhuang",
            "B. Prakash",
            "Chao Zhang"
        ],
        "citations": 48,
        "references": 54,
        "year": 2023
    },
    {
        "title": "DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation",
        "abstract": "Monocular depth estimation is a challenging task that predicts the pixel-wise depth from a single 2D image. Current methods typically model this problem as a regression or classification task. We propose DiffusionDepth, a new approach that reformulates monocular depth estimation as a denoising diffusion process. It learns an iterative denoising process to `denoise' random depth distribution into a depth map with the guidance of monocular visual conditions. The process is performed in the latent space encoded by a dedicated depth encoder and decoder. Instead of diffusing ground truth (GT) depth, the model learns to reverse the process of diffusing the refined depth of itself into random depth distribution. This self-diffusion formulation overcomes the difficulty of applying generative models to sparse GT depth scenarios. The proposed approach benefits this task by refining depth estimation step by step, which is superior for generating accurate and highly detailed depth maps. Experimental results on KITTI and NYU-Depth-V2 datasets suggest that a simple yet efficient diffusion approach could reach state-of-the-art performance in both indoor and outdoor scenarios with acceptable inference time.",
        "authors": [
            "Yiqun Duan",
            "Xianda Guo",
            "Zhengbiao Zhu"
        ],
        "citations": 46,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling",
        "abstract": "Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation quality: graphs generated by our approach have more similar graph statistics to those of the training graphs.",
        "authors": [
            "Xiaohui Chen",
            "Jiaxing He",
            "Xuhong Han",
            "Liping Liu"
        ],
        "citations": 38,
        "references": 61,
        "year": 2023
    },
    {
        "title": "CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization",
        "abstract": "Low-dose computed tomography (CT) images suffer from noise and artifacts due to photon starvation and electronic noise. Recently, some works have attempted to use diffusion models to address the over-smoothness and training instability encountered by previous deep-learning-based denoising models. However, diffusion models suffer from long inference time due to a large number of sampling steps involved. Very recently, cold diffusion model generalizes classical diffusion models and has greater flexibility. Inspired by cold diffusion, this paper presents a novel COntextual eRror-modulated gEneralized Diffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First, CoreDiff utilizes LDCT images to displace the random Gaussian noise and employs a novel mean-preserving degradation operator to mimic the physical process of CT degradation, significantly reducing sampling steps thanks to the informative LDCT images as the starting point of the sampling process. Second, to alleviate the error accumulation problem caused by the imperfect restoration operator in the sampling process, we propose a novel ContextuaL Error-modulAted Restoration Network (CLEAR-Net), which can leverage contextual information to constrain the sampling process from structural distortion and modulate time step embedding features for better alignment with the input at the next time step. Third, to rapidly generalize the trained model to a new, unseen dose level with as few resources as possible, we devise a one-shot learning framework to make CoreDiff generalize faster and better using only one single LDCT image (un)paired with normal-dose CT (NDCT). Extensive experimental results on four datasets demonstrate that our CoreDiff outperforms competing methods in denoising and generalization performance, with clinically acceptable inference time. Source code is made available at https://github.com/qgao21/CoreDiff.",
        "authors": [
            "Qi Gao",
            "Zilong Li",
            "Junping Zhang",
            "Yi Zhang",
            "Hongming Shan"
        ],
        "citations": 32,
        "references": 75,
        "year": 2023
    },
    {
        "title": "PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play",
        "abstract": "Learning from unstructured and uncurated data has become the dominant paradigm for generative approaches in language and vision. Such unstructured and unguided behavior data, commonly known as play, is also easier to collect in robotics but much more difficult to learn from due to its inherently multimodal, noisy, and suboptimal nature. In this paper, we study this problem of learning goal-directed skill policies from unstructured play data which is labeled with language in hindsight. Specifically, we leverage advances in diffusion models to learn a multi-task diffusion model to extract robotic skills from play data. Using a conditional denoising diffusion process in the space of states and actions, we can gracefully handle the complexity and multimodality of play data and generate diverse and interesting robot behaviors. To make diffusion models more useful for skill learning, we encourage robotic agents to acquire a vocabulary of skills by introducing discrete bottlenecks into the conditional behavior generation process. In our experiments, we demonstrate the effectiveness of our approach across a wide variety of environments in both simulation and the real world. Results visualizations and videos at https://play-fusion.github.io",
        "authors": [
            "Lili Chen",
            "Shikhar Bahl",
            "Deepak Pathak"
        ],
        "citations": 34,
        "references": 74,
        "year": 2023
    },
    {
        "title": "MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model",
        "abstract": "Diffusion probabilistic model (DPM) recently becomes one of the hottest topic in computer vision. Its image generation application such as Imagen, Latent Diffusion Models and Stable Diffusion have shown impressive generation capabilities, which aroused extensive discussion in the community. Many recent studies also found it is useful in many other vision tasks, like image deblurring, super-resolution and anomaly detection. Inspired by the success of DPM, we propose the first DPM based model toward general medical image segmentation tasks, which we named MedSegDiff. In order to enhance the step-wise regional attention in DPM for the medical image segmentation, we propose dynamic conditional encoding, which establishes the state-adaptive conditions for each sampling step. We further propose Feature Frequency Parser (FF-Parser), to eliminate the negative effect of high-frequency noise component in this process. We verify MedSegDiff on three medical segmentation tasks with different image modalities, which are optic cup segmentation over fundus images, brain tumor segmentation over MRI images and thyroid nodule segmentation over ultrasound images. The experimental results show that MedSegDiff outperforms state-of-the-art (SOTA) methods with considerable performance gap, indicating the generalization and effectiveness of the proposed model. Our code is released at https://github.com/WuJunde/MedSegDiff.",
        "authors": [
            "Junde Wu",
            "Huihui Fang",
            "Yu Zhang",
            "Yehui Yang",
            "Yanwu Xu"
        ],
        "citations": 183,
        "references": 38,
        "year": 2022
    },
    {
        "title": "DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery",
        "abstract": "Learning from a large corpus of data, pre-trained models have achieved impressive progress nowadays. As popular generative pre-training, diffusion models capture both low-level visual knowledge and high-level semantic relations. In this paper, we propose to exploit such knowledgeable diffusion models for mainstream discriminative tasks, i.e., unsupervised object discovery: saliency segmentation and object localization. However, the challenges exist as there is one structural difference between generative and discriminative models, which limits the direct use. Besides, the lack of explicitly labeled data significantly limits performance in unsupervised settings. To tackle these issues, we introduce DiffusionSeg, one novel synthesis-exploitation framework containing two-stage strategies. To alleviate data insufficiency, we synthesize abundant images, and propose a novel training-free AttentionCut to obtain masks in the first synthesis stage. In the second exploitation stage, to bridge the structural gap, we use the inversion technique, to map the given image back to diffusion features. These features can be directly used by downstream architectures. Extensive experiments and ablation studies demonstrate the superiority of adapting diffusion for unsupervised object discovery.",
        "authors": [
            "Chaofan Ma",
            "Yu-Hao Yang",
            "Chen Ju",
            "Feifan Zhang",
            "Jinxian Liu",
            "Yu Wang",
            "Ya Zhang",
            "Yanfeng Wang"
        ],
        "citations": 33,
        "references": 86,
        "year": 2023
    },
    {
        "title": "Dirichlet Diffusion Score Model for Biological Sequence Generation",
        "abstract": "Designing biological sequences is an important challenge that requires satisfying complex constraints and thus is a natural problem to address with deep generative modeling. Diffusion generative models have achieved considerable success in many applications. Score-based generative stochastic differential equations (SDE) model is a continuous-time diffusion model framework that enjoys many benefits, but the originally proposed SDEs are not naturally designed for modeling discrete data. To develop generative SDE models for discrete data such as biological sequences, here we introduce a diffusion process defined in the probability simplex space with stationary distribution being the Dirichlet distribution. This makes diffusion in continuous space natural for modeling discrete data. We refer to this approach as Dirchlet diffusion score model. We demonstrate that this technique can generate samples that satisfy hard constraints using a Sudoku generation task. This generative model can also solve Sudoku, including hard puzzles, without additional training. Finally, we applied this approach to develop the first human promoter DNA sequence design model and showed that designed sequences share similar properties with natural promoter sequences.",
        "authors": [
            "P. Avdeyev",
            "Chenlai Shi",
            "Yuhao Tan",
            "Kseniia Dudnyk",
            "Jian Zhou"
        ],
        "citations": 37,
        "references": 48,
        "year": 2023
    },
    {
        "title": "PhysDiff: Physics-Guided Human Motion Diffusion Model",
        "abstract": "Denoising diffusion models hold great promise for generating diverse and realistic human motions. However, existing motion diffusion models largely disregard the laws of physics in the diffusion process and often generate physically-implausible motions with pronounced artifacts such as floating, foot sliding, and ground penetration. This seriously impacts the quality of generated motions and limits their real-world application. To address this issue, we present a novel physics-guided motion diffusion model (PhysDiff), which incorporates physical constraints into the diffusion process. Specifically, we propose a physics-based motion projection module that uses motion imitation in a physics simulator to project the denoised motion of a diffusion step to a physically-plausible motion. The projected motion is further used in the next diffusion step to guide the denoising diffusion process. Intuitively, the use of physics in our model iteratively pulls the motion toward a physically-plausible space, which cannot be achieved by simple post-processing. Experiments on large-scale human motion datasets show that our approach achieves state-of-the-art motion quality and improves physical plausibility drastically (>78% for all datasets).",
        "authors": [
            "Ye Yuan",
            "Jiaming Song",
            "Umar Iqbal",
            "Arash Vahdat",
            "J. Kautz"
        ],
        "citations": 183,
        "references": 102,
        "year": 2022
    },
    {
        "title": "Dual Diffusion Implicit Bridges for Image-to-Image Translation",
        "abstract": "Common image-to-image translation methods rely on joint training over data from both source and target domains. The training process requires concurrent access to both datasets, which hinders data separation and privacy protection; and existing models cannot be easily adapted for translation of new domain pairs. We present Dual Diffusion Implicit Bridges (DDIBs), an image translation method based on diffusion models, that circumvents training on domain pairs. Image translation with DDIBs relies on two diffusion models trained independently on each domain, and is a two-step process: DDIBs first obtain latent encodings for source images with the source diffusion model, and then decode such encodings using the target model to construct target images. Both steps are defined via ordinary differential equations (ODEs), thus the process is cycle consistent only up to discretization errors of the ODE solvers. Theoretically, we interpret DDIBs as concatenation of source to latent, and latent to target Schrodinger Bridges, a form of entropy-regularized optimal transport, to explain the efficacy of the method. Experimentally, we apply DDIBs on synthetic and high-resolution image datasets, to demonstrate their utility in a wide variety of translation tasks and their inherent optimal transport properties.",
        "authors": [
            "Xu Su",
            "Jiaming Song",
            "Chenlin Meng",
            "Stefano Ermon"
        ],
        "citations": 168,
        "references": 56,
        "year": 2022
    },
    {
        "title": "DiffRF: Rendering-Guided 3D Radiance Field Diffusion",
        "abstract": "We introduce DiffRF, a novel approach for 3D radiance field synthesis based on denoising diffusion probabilistic models. While existing diffusion-based methods operate on images, latent codes, or point cloud data, we are the first to directly generate volumetric radiance fields. To this end, we propose a 3D denoising model which directly operates on an explicit voxel grid representation. However, as radiance fields generated from a set of posed images can be ambiguous and contain artifacts, obtaining ground truth radiance field samples is non-trivial. We address this challenge by pairing the denoising formulation with a rendering loss, enabling our model to learn a deviated prior that favours good image quality instead of trying to replicate fitting errors like floating artifacts. In contrast to 2D-diffusion models, our model learns multi-view consistent priors, enabling free-view synthesis and accurate shape generation. Compared to 3D GANs, our diffusion-based approach naturally enables conditional generation such as masked completion or single-view 3D synthesis at inference time.",
        "authors": [
            "Norman Muller",
            "Yawar Siddiqui",
            "L. Porzi",
            "S. R. Bulò",
            "P. Kontschieder",
            "M. Nießner"
        ],
        "citations": 163,
        "references": 83,
        "year": 2022
    },
    {
        "title": "UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation",
        "abstract": "Recent advancements in text-to-3D generation technology have significantly advanced the conversion of textual descriptions into imaginative well-geometrical and finely textured 3D objects. Despite these developments, a prevalent limitation arises from the use of RGB data in diffusion or reconstruction models, which often results in models with inherent lighting and shadows effects that detract from their realism, thereby limiting their usability in applications that demand accurate relighting capabilities. To bridge this gap, we present UniDream, a text-to-3D generation framework by incorporating unified diffusion priors. Our approach consists of three main components: (1) a dual-phase training process to get albedo-normal aligned multi-view diffusion and reconstruction models, (2) a progressive generation procedure for geometry and albedo-textures based on Score Distillation Sample (SDS) using the trained reconstruction and diffusion models, and (3) an innovative application of SDS for finalizing PBR generation while keeping a fixed albedo based on Stable Diffusion model. Extensive evaluations demonstrate that UniDream surpasses existing methods in generating 3D objects with clearer albedo textures, smoother surfaces, enhanced realism, and superior relighting capabilities.",
        "authors": [
            "Zexiang Liu",
            "Yangguang Li",
            "Youtian Lin",
            "Xin Yu",
            "Sida Peng",
            "Yan-Pei Cao",
            "Xiaojuan Qi",
            "Xiaoshui Huang",
            "Ding Liang",
            "Wanli Ouyang"
        ],
        "citations": 30,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model",
        "abstract": "Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion.",
        "authors": [
            "Xingqian Xu",
            "Zhangyang Wang",
            "Eric Zhang",
            "Kai Wang",
            "Humphrey Shi"
        ],
        "citations": 152,
        "references": 117,
        "year": 2022
    },
    {
        "title": "NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors",
        "abstract": "2D-to-3D reconstruction is an ill-posed problem, yet humans are good at solving this problem due to their prior knowledge of the 3D world developed over years. Driven by this observation, we propose NeRDi, a single-view NeRF synthesis framework with general image priors from 2D diffusion models. Formulating single-view reconstruction as an image-conditioned 3D generation problem, we optimize the NeRF representations by minimizing a diffusion loss on its arbitrary view renderings with a pretrained image diffusion model under the input-view constraint. We leverage off-the-shelf vision-language models and introduce a two-section language guidance as conditioning inputs to the diffusion model. This is essentially helpful for improving multiview content coherence as it narrows down the general image prior conditioned on the semantic and visual features of the single-view input image. Additionally, we introduce a geometric loss based on estimated depth maps to regularize the underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset show that our method can synthesize novel views with higher quality even compared to existing methods trained on this dataset. We also demonstrate our generalizability in zero-shot NeRF synthesis for in-the-wild images.",
        "authors": [
            "Congyue Deng",
            "C. Jiang",
            "C. Qi",
            "Xinchen Yan",
            "Yin Zhou",
            "L. Guibas",
            "Drago Anguelov"
        ],
        "citations": 149,
        "references": 60,
        "year": 2022
    },
    {
        "title": "What the DAAM: Interpreting Stable Diffusion Using Cross Attention",
        "abstract": "Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. On two generated datasets, we attain a competitive 58.8-64.8 mIoU on noun segmentation and fair to good mean opinion scores (3.4-4.2) on generalized attribution. Then, we apply DAAM to study the role of syntax in the pixel space across head–dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9%, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is at https://github.com/castorini/daam.",
        "authors": [
            "Raphael Tang",
            "Akshat Pandey",
            "Zhiying Jiang",
            "Gefei Yang",
            "K. Kumar",
            "Jimmy J. Lin",
            "Ferhan Ture"
        ],
        "citations": 134,
        "references": 60,
        "year": 2022
    },
    {
        "title": "EDICT: Exact Diffusion Inversion via Coupled Transformations",
        "abstract": "Finding an initial noise vector that produces an input image when fed into the diffusion process (known as inversion) is an important problem in denoising diffusion models (DDMs), with applications for real image editing. The standard approach for real image editing with inversion uses denoising diffusion implicit models (DDIMs [29]) to deterministically noise the image to the intermediate state along the path that the denoising would follow given the original conditioning. However, DDIM inversion for real images is unstable as it relies on local linearization assumptions, which result in the propagation of errors, leading to incorrect image reconstruction and loss of content. To alleviate these problems, we propose Exact Diffusion Inversion via Coupled Transformations (EDICT), an inversion method that draws inspiration from affine coupling layers. EDICT enables mathematically exact inversion of real and model-generated images by maintaining two coupled noise vectors which are used to invert each other in an alternating fashion. Using Stable Diffusion [25], a state-of-the-art latent diffusion model, we demonstrate that EDICT successfully reconstructs real images with high fidelity. On complex image datasets like MS-COCO, EDICT reconstruction significantly outperforms DDIM, improving the mean square error of reconstruction by a factor of two. Using noise vectors inverted from real images, EDICT enables a wide range of image edits—from local and global semantic edits to image stylization—while maintaining fidelity to the original image structure. EDICT requires no model training/finetuning, prompt tuning, or extra data and can be combined with any pretrained DDM.",
        "authors": [
            "Bram Wallace",
            "Akash Gokul",
            "N. Naik"
        ],
        "citations": 134,
        "references": 34,
        "year": 2022
    },
    {
        "title": "Exposing the Fake: Effective Diffusion-Generated Images Detection",
        "abstract": "Image synthesis has seen significant advancements with the advent of diffusion-based generative models like Denoising Diffusion Probabilistic Models (DDPM) and text-to-image diffusion models. Despite their efficacy, there is a dearth of research dedicated to detecting diffusion-generated images, which could pose potential security and privacy risks. This paper addresses this gap by proposing a novel detection method called Stepwise Error for Diffusion-generated Image Detection (SeDID). Comprising statistical-based $\\text{SeDID}_{\\text{Stat}}$ and neural network-based $\\text{SeDID}_{\\text{NNs}}$, SeDID exploits the unique attributes of diffusion models, namely deterministic reverse and deterministic denoising computation errors. Our evaluations demonstrate SeDID's superior performance over existing methods when applied to diffusion models. Thus, our work makes a pivotal contribution to distinguishing diffusion model-generated images, marking a significant step in the domain of artificial intelligence security.",
        "authors": [
            "Ruipeng Ma",
            "Jinhao Duan",
            "Fei Kong",
            "Xiaoshuang Shi",
            "Kaidi Xu"
        ],
        "citations": 22,
        "references": 41,
        "year": 2023
    },
    {
        "title": "A Survey on Generative Diffusion Model",
        "abstract": "—Deep learning shows excellent potential in generation tasks thanks to deep latent representation. Generative models are classes of models that can generate observations randomly with respect to certain implied parameters. Recently, the diffusion Model has become a raising class of generative models by virtue of its power-generating ability. Nowadays, great achievements have been reached. More applications except for computer vision, speech generation, bioinformatics, and natural language processing are to be explored in this ﬁeld. However, the diffusion model has its genuine drawback of a slow generation process, leading to many enhanced works. This survey makes a summary of the ﬁeld of the diffusion model. We ﬁrst state the main problem with two landmark works – DDPM and DSM. Then, we present a diverse range of advanced techniques to speed up the diffusion models – training schedule, training-free sampling, mixed-modeling, and score & diffusion uniﬁcation. Regarding existing models, we also provide a benchmark of FID score, IS, and NLL according to speciﬁc NFE. Moreover, applications with diffusion models are introduced including computer vision, sequence modeling, audio, and AI for science. Finally, there is a summarization of this ﬁeld together with limitations & further directions.",
        "authors": [
            "Hanqun Cao",
            "Cheng Tan",
            "Zhangyang Gao",
            "Yilun Xu",
            "Guangyong Chen",
            "P. Heng",
            "Stan Z. Li"
        ],
        "citations": 146,
        "references": 360,
        "year": 2022
    },
    {
        "title": "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model",
        "abstract": "Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronized without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing.",
        "authors": [
            "Dan Bigioi",
            "Shubhajit Basak",
            "H. Jordan",
            "R. Mcdonnell",
            "Peter Corcoran"
        ],
        "citations": 24,
        "references": 106,
        "year": 2023
    },
    {
        "title": "DiffUTE: Universal Text Editing Diffusion Model",
        "abstract": "Diffusion model based language-guided image editing has achieved great success recently. However, existing state-of-the-art diffusion models struggle with rendering correct text and text style during generation. To tackle this problem, we propose a universal self-supervised text editing diffusion model (DiffUTE), which aims to replace or modify words in the source image with another one while maintaining its realistic appearance. Specifically, we build our model on a diffusion model and carefully modify the network structure to enable the model for drawing multilingual characters with the help of glyph and position information. Moreover, we design a self-supervised learning framework to leverage large amounts of web data to improve the representation ability of the model. Experimental results show that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity. Our code will be avaliable in \\url{https://github.com/chenhaoxing/DiffUTE}.",
        "authors": [
            "Haoxing Chen",
            "Zhuoer Xu",
            "Zhangxuan Gu",
            "Jun Lan",
            "Xing Zheng",
            "Yaohui Li",
            "Changhua Meng",
            "Huijia Zhu",
            "Weiqiang Wang"
        ],
        "citations": 26,
        "references": 56,
        "year": 2023
    },
    {
        "title": "EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion Generation",
        "abstract": "We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal data distributions among arbitrary (and potentially larger) step sizes conditioned on control signals, enabling fewer-step motion sampling with high fidelity and diversity. To minimize undesired motion artifacts, geometric losses are imposed during network learning. As a result, EMDM achieves real-time motion generation and significantly improves the efficiency of motion diffusion models compared to existing methods while achieving high-quality motion generation. Our code will be publicly available upon publication.",
        "authors": [
            "Wenyang Zhou",
            "Zhiyang Dou",
            "Zeyu Cao",
            "Zhouyingcheng Liao",
            "Jingbo Wang",
            "Wenjia Wang",
            "Yuan Liu",
            "Taku Komura",
            "Wenping Wang",
            "Lingjie Liu"
        ],
        "citations": 25,
        "references": 103,
        "year": 2023
    },
    {
        "title": "DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model",
        "abstract": "Speech-driven gesture synthesis is a field of growing interest in virtual human creation. However, a critical challenge is the inherent intricate one-to-many mapping between speech and gestures. Previous studies have explored and achieved significant progress with generative models. Notwithstanding, most synthetic gestures are still vastly less natural. This paper presents DiffMotion, a novel speech-driven gesture synthesis architecture based on diffusion models. The model comprises an autoregressive temporal encoder and a denoising diffusion probability Module. The encoder extracts the temporal context of the speech input and historical gestures. The diffusion module learns a parameterized Markov chain to gradually convert a simple distribution into a complex distribution and generates the gestures according to the accompanied speech. Compared with baselines, objective and subjective evaluations confirm that our approach can produce natural and diverse gesticulation and demonstrate the benefits of diffusion-based models on speech-driven gesture synthesis.",
        "authors": [
            "Fan Zhang",
            "Naye Ji",
            "Fuxing Gao",
            "Yongping Li"
        ],
        "citations": 25,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Graph Denoising Diffusion for Inverse Protein Folding",
        "abstract": "Inverse protein folding is challenging due to its inherent one-to-many mapping characteristic, where numerous possible amino acid sequences can fold into a single, identical protein backbone. This task involves not only identifying viable sequences but also representing the sheer diversity of potential solutions. However, existing discriminative models, such as transformer-based auto-regressive models, struggle to encapsulate the diverse range of plausible solutions. In contrast, diffusion probabilistic models, as an emerging genre of generative approaches, offer the potential to generate a diverse set of sequence candidates for determined protein backbones. We propose a novel graph denoising diffusion model for inverse protein folding, where a given protein backbone guides the diffusion process on the corresponding amino acid residue types. The model infers the joint distribution of amino acids conditioned on the nodes' physiochemical properties and local environment. Moreover, we utilize amino acid replacement matrices for the diffusion forward process, encoding the biologically-meaningful prior knowledge of amino acids from their spatial and sequential neighbors as well as themselves, which reduces the sampling space of the generative process. Our model achieves state-of-the-art performance over a set of popular baseline methods in sequence recovery and exhibits great potential in generating diverse protein sequences for a determined protein backbone structure.",
        "authors": [
            "Kai Yi",
            "Bingxin Zhou",
            "Yiqing Shen",
            "Pietro Lio'",
            "Yu Guang Wang"
        ],
        "citations": 33,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model",
        "abstract": "Recently, text-to-image denoising diffusion probabilistic models (DDPMs) have demonstrated impressive image generation capabilities and have also been successfully applied to image inpainting. However, in practice, users often require more control over the inpainting process beyond textual guidance, especially when they want to composite objects with customized appearance, color, shape, and layout. Unfortunately, existing diffusion-based inpainting methods are limited to single-modal guidance and require task-specific training, hindering their cross-modal scalability. To address these limitations, we propose Uni-paint, a unified framework for multimodal inpainting that offers various modes of guidance, including unconditional, text-driven, stroke-driven, exemplar-driven inpainting, as well as a combination of these modes. Furthermore, our Uni-paint is based on pretrained Stable Diffusion and does not require task-specific training on specific datasets, enabling few-shot generalizability to customized images. We have conducted extensive qualitative and quantitative evaluations that show our approach achieves comparable results to existing single-modal methods while offering multimodal inpainting capabilities not available in other methods. Code is available at https://github.com/ysy31415/unipaint.",
        "authors": [
            "Shiyuan Yang",
            "Xiaodong Chen",
            "Jing Liao"
        ],
        "citations": 43,
        "references": 52,
        "year": 2023
    },
    {
        "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models",
        "abstract": "Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth—a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10,000x smaller than a normal DreamBooth model.",
        "authors": [
            "Nataniel Ruiz",
            "Yuanzhen Li",
            "Varun Jampani",
            "Wei Wei",
            "Tingbo Hou",
            "Y. Pritch",
            "N. Wadhwa",
            "Michael Rubinstein",
            "Kfir Aberman"
        ],
        "citations": 135,
        "references": 39,
        "year": 2023
    },
    {
        "title": "An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization",
        "abstract": "Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by $\\epsilon$ initialized in $t=0$ and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-to-speech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. {Code is available at \\url{https://github.com/kong13661/PIA}}.",
        "authors": [
            "Fei Kong",
            "Jinhao Duan",
            "Ruipeng Ma",
            "Hengtao Shen",
            "Xiaofeng Zhu",
            "Xiaoshuang Shi",
            "Kaidi Xu"
        ],
        "citations": 21,
        "references": 52,
        "year": 2023
    },
    {
        "title": "DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
        "abstract": "Existing text-video retrieval solutions are, in essence, discriminant models focused on maximizing the conditional likelihood, i.e., p(candidates|query). While straightforward, this de facto paradigm overlooks the underlying data distribution p(query), which makes it challenging to identify out-of-distribution data. To address this limitation, we creatively tackle this task from a generative viewpoint and model the correlation between the text and the video as their joint probability p(candidates,query). This is accomplished through a diffusion-based text-video retrieval framework (Diffusion-Ret), which models the retrieval task as a process of gradually generating joint distribution from noise. During training, DiffusionRet is optimized from both the generation and discrimination perspectives, with the generator being optimized by generation loss and the feature extractor trained with contrastive loss. In this way, DiffusionRet cleverly leverages the strengths of both generative and discriminative methods. Extensive experiments on five commonly used text-video retrieval benchmarks, including MSRVTT, LSMDC, MSVD, ActivityNet Captions, and DiDeMo, with superior performances, justify the efficacy of our method. More encouragingly, without any modification, DiffusionRet even performs well in out-domain retrieval settings. We believe this work brings fundamental insights into the related fields. Code is available at https://github.com/jpthu17/DiffusionRet.",
        "authors": [
            "Peng Jin",
            "Hao Li",
            "Zesen Cheng",
            "Kehan Li",
            "Xiang Ji",
            "Chang Liu",
            "Li-ming Yuan",
            "Jie Chen"
        ],
        "citations": 39,
        "references": 95,
        "year": 2023
    },
    {
        "title": "DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models",
        "abstract": "With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models. DiffusionDB is publicly available at: https://poloclub.github.io/diffusiondb.",
        "authors": [
            "Zijie J. Wang",
            "Evan Montoya",
            "David Munechika",
            "Haoyang Yang",
            "Benjamin Hoover",
            "Duen Horng Chau"
        ],
        "citations": 227,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Feature Prediction Diffusion Model for Video Anomaly Detection",
        "abstract": "Anomaly detection in the video is an important research area and a challenging task in real applications. Due to the unavailability of large-scale annotated anomaly events, most existing video anomaly detection (VAD) methods focus on learning the distribution of normal samples to detect the substantially deviated samples as anomalies. To well learn the distribution of normal motion and appearance, many auxiliary networks are employed to extract foreground object or action information. These high-level semantic features effectively filter the noise from the background to decrease its influence on detection models. However, the capability of these extra semantic models heavily affects the performance of the VAD methods. Motivated by the impressive generative and anti-noise capacity of diffusion model (DM), in this work, we introduce a novel DM-based method to predict the features of video frames for anomaly detection. We aim to learn the distribution of normal samples without any extra high-level semantic feature extraction models involved. To this end, we build two denoising diffusion implicit modules to predict and refine the features. The first module concentrates on feature motion learning, while the last focuses on feature appearance learning. To the best of our knowledge, it is the first DM-based method to predict frame features for VAD. The strong capacity of DMs also enables our method to more accurately predict the normal features than non-DM-based feature prediction-based VAD methods. Extensive experiments show that the proposed approach substantially outperforms state-of-the-art competing methods. The code is available atFPDM.",
        "authors": [
            "Cheng Yan",
            "Shiyu Zhang",
            "Yang Liu",
            "Guansong Pang",
            "Wenjun Wang"
        ],
        "citations": 30,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Face Animation with an Attribute-Guided Diffusion Model",
        "abstract": "Face animation has achieved much progress in computer vision. However, prevailing GAN-based methods suffer from unnatural distortions and artifacts due to sophisticated motion deformation. In this paper, we propose a Face Animation framework with an attribute-guided Diffusion Model (FADM), which is the first work to exploit the superior modeling capacity of diffusion models for photorealistic talking-head generation. To mitigate the uncontrollable synthesis effect of the diffusion model, we design an Attribute-Guided Conditioning Network (AGCN) to adaptively combine the coarse animation features and 3D face reconstruction results, which can incorporate appearance and motion conditions into the diffusion process. These specific designs help FADM rectify unnatural artifacts and distortions, and also enrich high-fidelity facial details through iterative diffusion refinements with accurate animation attributes. FADM can flexibly and effectively improve existing animation videos. Extensive experiments on widely used talking-head benchmarks validate the effectiveness of FADM over prior arts. The source code is available in https://github.com/zengbohan0217/FADM.",
        "authors": [
            "Bo-Wen Zeng",
            "Xuhui Liu",
            "Sicheng Gao",
            "Boyu Liu",
            "Hong Li",
            "Jianzhuang Liu",
            "Baochang Zhang"
        ],
        "citations": 19,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Diffusion-based Image Translation using Disentangled Style and Content Representation",
        "abstract": "Diffusion-based image translation guided by semantic texts or a single target image has enabled flexible style transfer which is not limited to the specific domains. Unfortunately, due to the stochastic nature of diffusion models, it is often difficult to maintain the original content of the image during the reverse diffusion. To address this, here we present a novel diffusion-based unsupervised image translation method using disentangled style and content representation. Specifically, inspired by the splicing Vision Transformer, we extract intermediate keys of multihead self attention layer from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer. To further accelerate the semantic change during the reverse diffusion, we also propose a novel semantic divergence loss and resampling strategy. Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks.",
        "authors": [
            "Gihyun Kwon",
            "Jong-Chul Ye"
        ],
        "citations": 128,
        "references": 56,
        "year": 2022
    },
    {
        "title": "CoLa-Diff: Conditional Latent Diffusion Model for Multi-Modal MRI Synthesis",
        "abstract": "MRI synthesis promises to mitigate the challenge of missing MRI modality in clinical practice. Diffusion model has emerged as an effective technique for image synthesis by modelling complex and variable data distributions. However, most diffusion-based MRI synthesis models are using a single modality. As they operate in the original image domain, they are memory-intensive and less feasible for multi-modal synthesis. Moreover, they often fail to preserve the anatomical structure in MRI. Further, balancing the multiple conditions from multi-modal MRI inputs is crucial for multi-modal synthesis. Here, we propose the first diffusion-based multi-modality MRI synthesis model, namely Conditioned Latent Diffusion Model (CoLa-Diff). To reduce memory consumption, we design CoLa-Diff to operate in the latent space. We propose a novel network architecture, e.g., similar cooperative filtering, to solve the possible compression and noise in latent space. To better maintain the anatomical structure, brain region masks are introduced as the priors of density distributions to guide diffusion process. We further present auto-weight adaptation to employ multi-modal information effectively. Our experiments demonstrate that CoLa-Diff outperforms other state-of-the-art MRI synthesis methods, promising to serve as an effective tool for multi-modal MRI synthesis.",
        "authors": [
            "Lan Jiang",
            "Ye Mao",
            "Xi Chen",
            "Xiangfeng Wang",
            "Chao Li"
        ],
        "citations": 35,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Diffusion Model for Generative Image Denoising",
        "abstract": "In supervised learning for image denoising, usually the paired clean images and noisy images are collected or synthesised to train a denoising model. L2 norm loss or other distance functions are used as the objective function for training. It often leads to an over-smooth result with less image details. In this paper, we regard the denoising task as a problem of estimating the posterior distribution of clean images conditioned on noisy images. We apply the idea of diffusion model to realize generative image denoising. According to the noise model in denoising tasks, we redefine the diffusion process such that it is different from the original one. Hence, the sampling of the posterior distribution is a reverse process of dozens of steps from the noisy image. We consider three types of noise model, Gaussian, Gamma and Poisson noise. With the guarantee of theory, we derive a unified strategy for model training. Our method is verified through experiments on three types of noise models and achieves excellent performance.",
        "authors": [
            "Yutong Xie",
            "Minne Yuan",
            "Bin Dong",
            "Quanzheng Li"
        ],
        "citations": 27,
        "references": 33,
        "year": 2023
    },
    {
        "title": "ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting.",
        "authors": [
            "Rongjie Huang",
            "Zhou Zhao",
            "Huadai Liu",
            "Jinglin Liu",
            "Chenye Cui",
            "Yi Ren"
        ],
        "citations": 169,
        "references": 60,
        "year": 2022
    },
    {
        "title": "PathLDM: Text conditioned Latent Diffusion Model for Histopathology",
        "abstract": "To achieve high-quality results, diffusion models must be trained on large datasets. This can be notably prohibitive for models in specialized domains, such as computational pathology. Conditioning on labeled data is known to help in data-efficient model training. Therefore, histopathology reports, which are rich in valuable clinical information, are an ideal choice as guidance for a histopathology generative model. In this paper, we introduce PathLDM, the first text-conditioned Latent Diffusion Model tailored for generating high-quality histopathology images. Leveraging the rich contextual information provided by pathology text reports, our approach fuses image and textual data to enhance the generation process. By utilizing GPT’s capabilities to distill and summarize complex text reports, we establish an effective conditioning mechanism. Through strategic conditioning and necessary architectural enhancements, we achieved a SoTA FID score of 7.64 for text-to-image generation on the TCGA-BRCA dataset, significantly outperforming the closest text-conditioned competitor with FID 30.1. 1",
        "authors": [
            "Srikar Yellapragada",
            "Alexandros Graikos",
            "P. Prasanna",
            "T. Kurç",
            "J. Saltz",
            "D. Samaras"
        ],
        "citations": 22,
        "references": 51,
        "year": 2023
    },
    {
        "title": "DifFSS: Diffusion Model for Few-Shot Semantic Segmentation",
        "abstract": "Diffusion models have demonstrated excellent performance in image generation. Although various few-shot semantic segmentation (FSS) models with different network structures have been proposed, performance improvement has reached a bottleneck. This paper presents the first work to leverage the diffusion model for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve the performance of the state-of-the-art FSS models by a large margin without modifying their network structure. Specifically, we utilize the powerful generation ability of diffusion models to generate diverse auxiliary support images by using the semantic mask, scribble or soft HED boundary of the support image as control conditions. This generation process simulates the variety within the class of the query image, such as color, texture variation, lighting, $etc$. As a result, FSS models can refer to more diverse support images, yielding more robust representations, thereby achieving a consistent improvement in segmentation performance. Extensive experiments on three publicly available datasets based on existing advanced FSS models demonstrate the effectiveness of the diffusion model for FSS task. Furthermore, we explore in detail the impact of different input settings of the diffusion model on segmentation performance. Hopefully, this completely new paradigm will bring inspiration to the study of FSS task integrated with AI-generated content. Code is available at https://github.com/TrinitialChan/DifFSS",
        "authors": [
            "Weimin Tan",
            "Siyuan Chen",
            "Bo Yan"
        ],
        "citations": 22,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Unsupervised Surface Anomaly Detection with Diffusion Probabilistic Model",
        "abstract": "Unsupervised surface anomaly detection aims at discovering and localizing anomalous patterns using only anomaly-free training samples. Reconstruction-based models are among the most popular and successful methods, which rely on the assumption that anomaly regions are more difficult to reconstruct. However, there are three major challenges to the practical application of this approach: 1) the reconstruction quality needs to be further improved since it has a great impact on the final result, especially for images with structural changes; 2) it is observed that for many neural networks, the anomalies can also be well reconstructed, which severely violates the underlying assumption; 3) since reconstruction is an ill-conditioned problem, a test instance may correspond to multiple normal patterns, but most current reconstruction-based methods have ignored this critical fact. In this paper, we propose DiffAD, a method for unsupervised anomaly detection based on the latent diffusion model, inspired by its ability to generate high-quality and diverse images. We further propose noisy condition embedding and interpolated channels to address the aforementioned challenges in the general reconstruction-based pipeline. Extensive experiments show that our method achieves state-of-the-art performance on the challenging MVTec dataset, especially in localization accuracy.",
        "authors": [
            "Xinyi Zhang",
            "Naiqi Li",
            "Jiawei Li",
            "Tao Dai",
            "Yong Jiang",
            "Shu-Tao Xia"
        ],
        "citations": 31,
        "references": 33,
        "year": 2023
    },
    {
        "title": "Conditional Diffusion Probabilistic Model for Speech Enhancement",
        "abstract": "Speech enhancement is a critical component of many user-oriented audio applications, yet current systems still suffer from distorted and unnatural outputs. While generative models have shown strong potential in speech synthesis, they are still lagging behind in speech enhancement. This work leverages recent advances in diffusion probabilistic models, and proposes a novel speech enhancement algorithm that incorporates characteristics of the observed noisy speech signal into the diffusion and reverse processes. More specifically, we propose a generalized formulation of the diffusion probabilistic model named conditional diffusion probabilistic model that, in its reverse process, can adapt to non-Gaussian real noises in the estimated speech signal. In our experiments, we demonstrate strong performance of the proposed approach compared to representative generative models, and investigate the generalization capability of our models to other datasets with noise characteristics unseen during training.",
        "authors": [
            "Yen-Ju Lu",
            "Zhongqiu Wang",
            "Shinji Watanabe",
            "Alexander Richard",
            "Cheng Yu",
            "Yu Tsao"
        ],
        "citations": 146,
        "references": 36,
        "year": 2022
    },
    {
        "title": "Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs",
        "abstract": "Cellular sheaves equip graphs with a\"geometrical\"structure by assigning vector spaces and linear maps to nodes and edges. Graph Neural Networks (GNNs) implicitly assume a graph with a trivial underlying sheaf. This choice is reflected in the structure of the graph Laplacian operator, the properties of the associated diffusion equation, and the characteristics of the convolutional models that discretise this equation. In this paper, we use cellular sheaf theory to show that the underlying geometry of the graph is deeply linked with the performance of GNNs in heterophilic settings and their oversmoothing behaviour. By considering a hierarchy of increasingly general sheaves, we study how the ability of the sheaf diffusion process to achieve linear separation of the classes in the infinite time limit expands. At the same time, we prove that when the sheaf is non-trivial, discretised parametric diffusion processes have greater control than GNNs over their asymptotic behaviour. On the practical side, we study how sheaves can be learned from data. The resulting sheaf diffusion models have many desirable properties that address the limitations of classical graph diffusion equations (and corresponding GNN models) and obtain competitive results in heterophilic settings. Overall, our work provides new connections between GNNs and algebraic topology and would be of interest to both fields.",
        "authors": [
            "Cristian Bodnar",
            "Francesco Di Giovanni",
            "B. Chamberlain",
            "Pietro Lio'",
            "Michael M. Bronstein"
        ],
        "citations": 146,
        "references": 92,
        "year": 2022
    },
    {
        "title": "Diffusion Model as Representation Learner",
        "abstract": "Diffusion Probabilistic Models (DPMs) have recently demonstrated impressive results on various generative tasks. Despite its promises, the learned representations of pre-trained DPMs, however, have not been fully understood. In this paper, we conduct an in-depth investigation of the representation power of DPMs, and propose a novel knowledge transfer method that leverages the knowledge acquired by generative DPMs for recognition tasks. Our study begins by examining the feature space of DPMs, revealing that DPMs are inherently denoising autoencoders that balance the representation learning with regularizing model capacity. To this end, we introduce a novel knowledge transfer paradigm named RepFusion. Our paradigm extracts representations at different time steps from off-the-shelf DPMs and dynamically employs them as supervision for student networks, in which the optimal time is determined through reinforcement learning. We evaluate our approach on several image classification, semantic segmentation, and landmark detection benchmarks, and demonstrate that it outperforms state-of-the-art methods. Our results uncover the potential of DPMs as a powerful tool for representation learning and provide insights into the usefulness of generative models beyond sample generation. The code is available at https://github.com/Adamdad/Repfusion.",
        "authors": [
            "Xingyi Yang",
            "Xinchao Wang"
        ],
        "citations": 32,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Red-Teaming the Stable Diffusion Safety Filter",
        "abstract": "Stable Diffusion is a recent open-source image generation model comparable to proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with a safety filter that aims to prevent generating explicit images. Unfortunately, the filter is obfuscated and poorly documented. This makes it hard for users to prevent misuse in their applications, and to understand the filter's limitations and improve it. We first show that it is easy to generate disturbing content that bypasses the safety filter. We then reverse-engineer the filter and find that while it aims to prevent sexual content, it ignores violence, gore, and other similarly disturbing content. Based on our analysis, we argue safety measures in future model releases should strive to be fully open and properly documented to stimulate security contributions from the community.",
        "authors": [
            "Javier Rando",
            "Daniel Paleka",
            "David Lindner",
            "Lennard Heim",
            "Florian Tramèr"
        ],
        "citations": 141,
        "references": 18,
        "year": 2022
    },
    {
        "title": "GSURE-Based Diffusion Model Training with Corrupted Data",
        "abstract": "Diffusion models have demonstrated impressive results in both data generation and downstream tasks such as inverse problems, text-based editing, classification, and more. However, training such models usually requires large amounts of clean signals which are often difficult or impossible to obtain. In this work, we propose a novel training technique for generative diffusion models based only on corrupted data. We introduce a loss function based on the Generalized Stein's Unbiased Risk Estimator (GSURE), and prove that under some conditions, it is equivalent to the training objective used in fully supervised diffusion models. We demonstrate our technique on face images as well as Magnetic Resonance Imaging (MRI), where the use of undersampled data significantly alleviates data collection costs. Our approach achieves generative performance comparable to its fully supervised counterpart without training on any clean signals. In addition, we deploy the resulting diffusion model in various downstream tasks beyond the degradation present in the training set, showcasing promising results.",
        "authors": [
            "Bahjat Kawar",
            "Noam Elata",
            "T. Michaeli",
            "Michael Elad"
        ],
        "citations": 21,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Human Preference Score: Better Aligning Text-to-image Models with Human Preference",
        "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human preferences. Our experiments show that HPS outperforms CLIP in predicting human choices and has good generalization capability toward images generated from other models. By tuning Stable Diffusion with the guidance of HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/alignsd-web/.",
        "authors": [
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Feng Zhu",
            "Rui Zhao",
            "Hongsheng Li"
        ],
        "citations": 94,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Better Aligning Text-to-Image Models with Human Preference",
        "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classiﬁer with the collected dataset and derive a Human Preference Score (HPS) based on the classiﬁer. Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. By tuning Stable Diffusion with the guidance of the HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/align sd web/.",
        "authors": [
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Feng Zhu",
            "Rui Zhao",
            "Hongsheng Li"
        ],
        "citations": 87,
        "references": 42,
        "year": 2023
    },
    {
        "title": "ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts",
        "abstract": "Recent progress in diffusion models has revolutionized the popular technology of text-to-image generation. While existing approaches could produce photorealistic high-resolution images with text conditions, there are still several open problems to be solved, which limits the further improvement of image fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model, to progressively upgrade the quality of generated images by: (1) incorporating fine-grained textual and visual knowledge of key elements in the scene, and (2) utilizing different denoising experts at different denoising stages. With the proposed mechanisms, ERNIE-ViLG 2.01 not only achieves a new state-of-the-art on MS-COCO with zero-shot FID-30k score of 6.75, but also significantly outperforms recent models in terms of image fidelity and image-text alignment, with side-by-side human evaluation on the bilingual prompt set ViLG-300.",
        "authors": [
            "Zhida Feng",
            "Zhenyu Zhang",
            "Xintong Yu",
            "Yewei Fang",
            "Lanxin Li",
            "Xuyi Chen",
            "Yuxiang Lu",
            "Jiaxiang Liu",
            "Weichong Yin",
            "Shi Feng",
            "Yu Sun",
            "Hao Tian",
            "Hua Wu",
            "Haifeng Wang"
        ],
        "citations": 105,
        "references": 50,
        "year": 2022
    },
    {
        "title": "Soft Diffusion: Score Matching for General Corruptions",
        "abstract": "We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, \\textit{that after corruption}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art FID score $1.85$ on CelebA-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion.",
        "authors": [
            "Giannis Daras",
            "M. Delbracio",
            "Hossein Talebi",
            "A. Dimakis",
            "P. Milanfar"
        ],
        "citations": 95,
        "references": 57,
        "year": 2022
    },
    {
        "title": "SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion",
        "abstract": "Multi-objective optimization problems are ubiquitous in robotics, e.g., the optimization of a robot manipulation task requires a joint consideration of grasp pose configurations, collisions and joint limits. While some demands can be easily hand-designed, e.g., the smoothness of a trajectory, several task-specific objectives need to be learned from data. This work introduces a method for learning data-driven SE(3) cost functions as diffusion models. Diffusion models can represent highly-expressive multimodal distributions and exhibit proper gradients over the entire space due to their score-matching training objective. Learning costs as diffusion models allows their seamless integration with other costs into a single differentiable objective function, enabling joint gradient-based motion optimization. In this work, we focus on learning SE(3) diffusion models for 6DoF grasping, giving rise to a novel framework for joint grasp and motion optimization without needing to decouple grasp selection from trajectory generation. We evaluate the representation power of our SE(3) diffusion models w.r.t. classical generative models, and we showcase the superior performance of our proposed optimization framework in a series of simulated and real-world robotic manipulation tasks against representative baselines. Videos, code and additional details are available at: https://sites.google.com/view/se3dif",
        "authors": [
            "Julen Urain",
            "Niklas Funk",
            "Jan Peters",
            "Georgia Chalvatzaki"
        ],
        "citations": 93,
        "references": 79,
        "year": 2022
    },
    {
        "title": "GENIE: Higher-Order Denoising Diffusion Solvers",
        "abstract": "Denoising diffusion models (DDMs) have emerged as a powerful class of generative models. A forward diffusion process slowly perturbs the data, while a deep model learns to gradually denoise. Synthesis amounts to solving a differential equation (DE) defined by the learnt model. Solving the DE requires slow iterative solvers for high-quality generation. In this work, we propose Higher-Order Denoising Diffusion Solvers (GENIE): Based on truncated Taylor methods, we derive a novel higher-order solver that significantly accelerates synthesis. Our solver relies on higher-order gradients of the perturbed data distribution, that is, higher-order score functions. In practice, only Jacobian-vector products (JVPs) are required and we propose to extract them from the first-order score network via automatic differentiation. We then distill the JVPs into a separate neural network that allows us to efficiently compute the necessary higher-order terms for our novel sampler during synthesis. We only need to train a small additional head on top of the first-order score network. We validate GENIE on multiple image generation benchmarks and demonstrate that GENIE outperforms all previous solvers. Unlike recent methods that fundamentally alter the generation process in DDMs, our GENIE solves the true generative DE and still enables applications such as encoding and guided sampling. Project page and code: https://nv-tlabs.github.io/GENIE.",
        "authors": [
            "Tim Dockhorn",
            "Arash Vahdat",
            "Karsten Kreis"
        ],
        "citations": 93,
        "references": 120,
        "year": 2022
    },
    {
        "title": "Scalable Diffusion for Materials Generation",
        "abstract": "Generative models trained on internet-scale data are capable of generating novel and realistic texts, images, and videos. A natural next question is whether these models can advance science, for example by generating novel stable materials. Traditionally, models with explicit structures (e.g., graphs) have been used in modeling structural relationships in scientific data (e.g., atoms and bonds in crystals), but generating structures can be difficult to scale to large and complex systems. Another challenge in generating materials is the mismatch between standard generative modeling metrics and downstream applications. For instance, common metrics such as the reconstruction error do not correlate well with the downstream goal of discovering stable materials. In this work, we tackle the scalability challenge by developing a unified crystal representation that can represent any crystal structure (UniMat), followed by training a diffusion probabilistic model on these UniMat representations. Our empirical results suggest that despite the lack of explicit structure modeling, UniMat can generate high fidelity crystal structures from larger and more complex chemical systems, outperforming previous graph-based approaches under various generative modeling metrics. To better connect the generation quality of materials to downstream applications, such as discovering novel stable materials, we propose additional metrics for evaluating generative models of materials, including per-composition formation energy and stability with respect to convex hulls through decomposition energy from Density Function Theory (DFT). Lastly, we show that conditional generation with UniMat can scale to previously established crystal datasets with up to millions of crystals structures, outperforming random structure search (the current leading method for structure discovery) in discovering new stable materials.",
        "authors": [
            "Mengjiao Yang",
            "KwangHwan Cho",
            "Amil Merchant",
            "Pieter Abbeel",
            "D. Schuurmans",
            "Igor Mordatch",
            "E. D. Cubuk"
        ],
        "citations": 27,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation",
        "abstract": "Diffusion probabilistic models have achieved remarkable success in text guided image generation. However, generating 3D shapes is still challenging due to the lack of sufficient data containing 3D models along with their descriptions. Moreover, text based descriptions of 3D shapes are inherently ambiguous and lack details. In this paper, we propose a sketch and text guided probabilistic diffusion model for colored point cloud generation that conditions the denoising process jointly with a hand drawn sketch of the object and its textual description. We incrementally diffuse the point coordinates and color values in a joint diffusion process to reach a Gaussian distribution. Colored point cloud generation thus amounts to learning the reverse diffusion process, conditioned by the sketch and text, to iteratively recover the desired shape and color. Specifically, to learn effective sketch-text embedding, our model adaptively aggregates the joint embedding of text prompt and the sketch based on a capsule attention network. Our model uses staged diffusion to generate the shape and then assign colors to different parts conditioned on the appearance prompt while preserving precise shapes from the first stage. This gives our model the flexibility to extend to multiple tasks, such as appearance re-editing and part segmentation. Experimental results demonstrate that our model outperforms recent state-of-the-art in point cloud generation.",
        "authors": [
            "Zijie Wu",
            "Yaonan Wang",
            "Mingtao Feng",
            "He-ping Xie",
            "A. Mian"
        ],
        "citations": 23,
        "references": 69,
        "year": 2023
    },
    {
        "title": "PFGM++: Unlocking the Potential of Physics-Inspired Generative Models",
        "abstract": "We introduce a new family of physics-inspired generative models termed PFGM++ that unifies diffusion models and Poisson Flow Generative Models (PFGM). These models realize generative trajectories for $N$ dimensional data by embedding paths in $N{+}D$ dimensional space while still controlling the progression with a simple scalar norm of the $D$ additional variables. The new models reduce to PFGM when $D{=}1$ and to diffusion models when $D{\\to}\\infty$. The flexibility of choosing $D$ allows us to trade off robustness against rigidity as increasing $D$ results in more concentrated coupling between the data and the additional variable norms. We dispense with the biased large batch field targets used in PFGM and instead provide an unbiased perturbation-based objective similar to diffusion models. To explore different choices of $D$, we provide a direct alignment method for transferring well-tuned hyperparameters from diffusion models ($D{\\to} \\infty$) to any finite $D$ values. Our experiments show that models with finite $D$ can be superior to previous state-of-the-art diffusion models on CIFAR-10/FFHQ $64{\\times}64$ datasets, with FID scores of $1.91/2.43$ when $D{=}2048/128$. In class-conditional setting, $D{=}2048$ yields current state-of-the-art FID of $1.74$ on CIFAR-10. In addition, we demonstrate that models with smaller $D$ exhibit improved robustness against modeling errors. Code is available at https://github.com/Newbeeer/pfgmpp",
        "authors": [
            "Yilun Xu",
            "Ziming Liu",
            "Yonglong Tian",
            "Shangyuan Tong",
            "Max Tegmark",
            "T. Jaakkola"
        ],
        "citations": 53,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Controllable Mind Visual Diffusion Model",
        "abstract": "Brain signal visualization has emerged as an active research area, serving as a critical interface between the human visual system and computer vision models. Diffusion-based methods have recently shown promise in analyzing functional magnetic resonance imaging (fMRI) data, including the reconstruction of high-quality images consistent with original visual stimuli. Nonetheless, it remains a critical challenge to effectively harness the semantic and silhouette information extracted from brain signals. In this paper, we propose a novel approach, termed as Controllable Mind Visual Diffusion Model (CMVDM). Specifically, CMVDM first extracts semantic and silhouette information from fMRI data using attribute alignment and assistant networks. Then, a control model is introduced in conjunction with a residual block to fully exploit the extracted information for image synthesis, generating high-quality images that closely resemble the original visual stimuli in both semantic content and silhouette characteristics. Through extensive experimentation, we demonstrate that CMVDM outperforms existing state-of-the-art methods both qualitatively and quantitatively. Our code is available at https://github.com/zengbohan0217/CMVDM.",
        "authors": [
            "Bo-Wen Zeng",
            "Shanglin Li",
            "Xuhui Liu",
            "Sicheng Gao",
            "Xiaolong Jiang",
            "Xu Tang",
            "Yao Hu",
            "Jianzhuang Liu",
            "Baochang Zhang"
        ],
        "citations": 20,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions",
        "abstract": "Probabilistic diffusion models have achieved state-of-the-art results for image synthesis, inpainting, and text-to-image tasks. However, they are still in the early stages of generating complex 3D shapes. This work proposes Diffusion-SDF, a generative model for shape completion, single-view reconstruction, and reconstruction of real-scanned point clouds. We use neural signed distance functions (SDFs) as our 3D representation to parameterize the geometry of various signals (e.g., point clouds, 2D images) through neural networks. Neural SDFs are implicit functions and diffusing them amounts to learning the reversal of their neural network weights, which we solve using a custom modulation module. Extensive experiments show that our method is capable of both realistic unconditional generation and conditional generation from partial inputs. This work expands the domain of diffusion models from learning 2D, explicit representations, to 3D, implicit representations. Code is released at https://github.com/princeton-computational-imaging/Diffusion-SDF.",
        "authors": [
            "Gene Chou",
            "Yuval Bahat",
            "Felix Heide"
        ],
        "citations": 88,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Person Image Synthesis via Denoising Diffusion Model",
        "abstract": "The pose-guided person image generation task requires synthesizing photorealistic images of humans in arbitrary poses. The existing approaches use generative adversarial networks that do not necessarily maintain realistic textures or need dense correspondences that struggle to handle complex deformations and severe occlusions. In this work, we show how denoising diffusion models can be applied for high-fidelity person image synthesis with strong sample diversity and enhanced mode coverage of the learnt data distribution. Our proposed Person Image Diffusion Model (PIDM) disintegrates the complex transfer problem into a series of simpler forward-backward denoising steps. This helps in learning plausible source-to-target transformation trajectories that result in faithful textures and undistorted appearance details. We introduce a ‘texture diffusion module’ based on cross-attention to accurately model the correspondences between appearance and pose information available in source and target images. Further, we propose ‘disentangled classifier-free guidance’ to ensure close resemblance between the conditional inputs and the synthesized output in terms of both pose and appearance information. Our extensive results on two large-scale benchmarks and a user study demonstrate the photorealism of our proposed approach under challenging scenarios. We also show how our generated images can help in downstream tasks. Code is available at https://github.com/ankanbhunia/PIDM.",
        "authors": [
            "A. Bhunia",
            "Salman H. Khan",
            "Hisham Cholakkal",
            "R. Anwer",
            "J. Laaksonen",
            "M. Shah",
            "F. Khan"
        ],
        "citations": 79,
        "references": 30,
        "year": 2022
    },
    {
        "title": "Towards the Detection of Diffusion Model Deepfakes",
        "abstract": "In the course of the past few years, diffusion models (DMs) have reached an unprecedented level of visual quality. However, relatively little attention has been paid to the detection of DM-generated images, which is critical to prevent adverse impacts on our society. In contrast, generative adversarial networks (GANs), have been extensively studied from a forensic perspective. In this work, we therefore take the natural next step to evaluate whether previous methods can be used to detect images generated by DMs. Our experiments yield two key findings: (1) state-of-the-art GAN detectors are unable to reliably distinguish real from DM-generated images, but (2) re-training them on DM-generated images allows for almost perfect detection, which remarkably even generalizes to GANs. Together with a feature space analysis, our results lead to the hypothesis that DMs produce fewer detectable artifacts and are thus more difficult to detect compared to GANs. One possible reason for this is the absence of grid-like frequency artifacts in DM-generated images, which are a known weakness of GANs. However, we make the interesting observation that diffusion models tend to underestimate high frequencies, which we attribute to the learning objective.",
        "authors": [
            "Jonas Ricker",
            "Simon Damm",
            "Thorsten Holz",
            "Asja Fischer"
        ],
        "citations": 83,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Continuous diffusion for categorical data",
        "abstract": "Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.",
        "authors": [
            "S. Dieleman",
            "Laurent Sartran",
            "Arman Roshannai",
            "Nikolay Savinov",
            "Yaroslav Ganin",
            "Pierre H. Richemond",
            "A. Doucet",
            "Robin Strudel",
            "Chris Dyer",
            "Conor Durkan",
            "Curtis Hawthorne",
            "Rémi Leblond",
            "Will Grathwohl",
            "J. Adler"
        ],
        "citations": 78,
        "references": 110,
        "year": 2022
    },
    {
        "title": "Target Speech Extraction with Conditional Diffusion Model",
        "abstract": "Diffusion model-based speech enhancement has received increased attention since it can generate very natural enhanced signals and generalizes well to unseen conditions. Diffusion models have been explored for several sub-tasks of speech enhancement, such as speech denoising, dereverberation, and source separation. In this paper, we investigate their use for target speech extraction (TSE), which consists of estimating the clean speech signal of a target speaker in a mixture of multi-talkers. TSE is realized by conditioning the extraction process on a clue identifying the target speaker. We show we can realize TSE using a conditional diffusion model conditioned on the clue. Besides, we introduce ensemble inference to reduce potential extraction errors caused by the diffusion process. In experiments on Libri2mix corpus, we show that the proposed diffusion model-based TSE combined with ensemble inference outperforms a comparable TSE system trained discriminatively.",
        "authors": [
            "Naoyuki Kamo",
            "Marc Delcroix",
            "Tomohiro Nakatan"
        ],
        "citations": 15,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis",
        "abstract": "Diffusion models (DMs) have shown great potential for high-quality image synthesis. However, when it comes to producing images with complex scenes, how to properly describe both image global structures and object details remains a challenging task. In this paper, we present Frido, a Feature Pyramid Diffusion model performing a multi-scale coarse-to-fine denoising process for image synthesis. Our model decomposes an input image into scale-dependent vector quantized features, followed by a coarse-to-fine gating for producing image output. During the above multi-scale representation learning stage, additional input conditions like text, scene graph, or image layout can be further exploited. Thus, Frido can be also applied for conditional or cross-modality image synthesis. We conduct extensive experiments over various unconditioned and conditional image generation tasks, ranging from text-to-image synthesis, layout-to-image, scene-graph-to-image, to label-to-image. More specifically, we achieved state-of-the-art FID scores on five benchmarks, namely layout-to-image on COCO and OpenImages, scene-graph-to-image on COCO and Visual Genome, and label-to-image on COCO.",
        "authors": [
            "Wanshu Fan",
            "Yen-Chun Chen",
            "Dongdong Chen",
            "Yu Cheng",
            "Lu Yuan",
            "Yu-Chiang Frank Wang"
        ],
        "citations": 78,
        "references": 99,
        "year": 2022
    },
    {
        "title": "Fast Diffusion Model",
        "abstract": "Diffusion models (DMs) have been adopted across diverse fields with its remarkable abilities in capturing intricate data distributions. In this paper, we propose a Fast Diffusion Model (FDM) to significantly speed up DMs from a stochastic optimization perspective for both faster training and sampling. We first find that the diffusion process of DMs accords with the stochastic optimization process of stochastic gradient descent (SGD) on a stochastic time-variant problem. Then, inspired by momentum SGD that uses both gradient and an extra momentum to achieve faster and more stable convergence than SGD, we integrate momentum into the diffusion process of DMs. This comes with a unique challenge of deriving the noise perturbation kernel from the momentum-based diffusion process. To this end, we frame the process as a Damped Oscillation system whose critically damped state -- the kernel solution -- avoids oscillation and yields a faster convergence speed of the diffusion process. Empirical results show that our FDM can be applied to several popular DM frameworks, e.g., VP, VE, and EDM, and reduces their training cost by about 50% with comparable image synthesis performance on CIFAR-10, FFHQ, and AFHQv2 datasets. Moreover, FDM decreases their sampling steps by about 3x to achieve similar performance under the same samplers. The code is available at https://github.com/sail-sg/FDM.",
        "authors": [
            "Zike Wu",
            "Pan Zhou",
            "Kenji Kawaguchi",
            "Hanwang Zhang"
        ],
        "citations": 15,
        "references": 52,
        "year": 2023
    },
    {
        "title": "On Diffusion Modeling for Anomaly Detection",
        "abstract": "Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Estimation (DTE). DTE estimates the distribution over diffusion time for a given input and uses the mode or mean of this distribution as the anomaly score. We derive an analytical form for this density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods perform competitively for both semi-supervised and unsupervised settings. Notably, DTE achieves orders of magnitude faster inference time than DDPM, while outperforming it on this benchmark. These results establish diffusion-based anomaly detection as a scalable alternative to traditional methods and recent deep-learning techniques for standard unsupervised and semi-supervised anomaly detection settings.",
        "authors": [
            "Victor Livernoche",
            "V. Jain",
            "Y. Hezaveh",
            "Siamak Ravanbakhsh"
        ],
        "citations": 15,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Diffusion Model for Camouflaged Object Detection",
        "abstract": "Camouflaged object detection is a challenging task that aims to identify objects that are highly similar to their background. Due to the powerful noise-to-image denoising capability of denoising diffusion models, in this paper, we propose a diffusion-based framework for camouflaged object detection, termed diffCOD, a new framework that considers the camouflaged object segmentation task as a denoising diffusion process from noisy masks to object masks. Specifically, the object mask diffuses from the ground-truth masks to a random distribution, and the designed model learns to reverse this noising process. To strengthen the denoising learning, the input image prior is encoded and integrated into the denoising diffusion model to guide the diffusion process. Furthermore, we design an injection attention module (IAM) to interact conditional semantic features extracted from the image with the diffusion noise embedding via the cross-attention mechanism to enhance denoising learning. Extensive experiments on four widely used COD benchmark datasets demonstrate that the proposed method achieves favorable performance compared to the existing 11 state-of-the-art methods, especially in the detailed texture segmentation of camouflaged objects. Our code will be made publicly available at: https://github.com/ZNan-Chen/diffCOD.",
        "authors": [
            "Zhe Chen",
            "Rongrong Gao",
            "Tian-Zhu Xiang",
            "Fanzhao Lin"
        ],
        "citations": 16,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Guided Conditional Diffusion for Controllable Traffic Simulation",
        "abstract": "Controllable and realistic traffic simulation is critical for developing and verifying autonomous vehicles. Typical heuristic-based traffic models offer flexible control to make vehicles follow specific trajectories and traffic rules. On the other hand, data-driven approaches generate realistic and human-like behaviors, improving transfer from simulated to real-world traffic. However, to the best of our knowledge, no traffic model offers both controllability and realism. In this work, we develop a conditional diffusion model for controllable traffic generation (CTG) that allows users to control desired properties of trajectories at test time (e.g., reach a goal or follow a speed limit) while maintaining realism and physical feasibility through enforced dynamics. The key technical idea is to leverage recent advances from diffusion modeling and differentiable logic to guide generated trajectories to meet rules defined using signal temporal logic (STL). We further extend guidance to multi-agent settings and enable interaction-based rules like collision avoidance. CTG is extensively evaluated on the nuScenes dataset for diverse and composite rules, demonstrating improvement over strong baselines in terms of the controllability-realism tradeoff. Demo videos can be found at https://aiasd.github.io/ctg.github.io",
        "authors": [
            "Ziyuan Zhong",
            "Davis Rempe",
            "Danfei Xu",
            "Yuxiao Chen",
            "Sushant Veer",
            "Tong Che",
            "Baishakhi Ray",
            "M. Pavone"
        ],
        "citations": 112,
        "references": 37,
        "year": 2022
    },
    {
        "title": "Neural Wavelet-domain Diffusion for 3D Shape Generation",
        "abstract": "This paper presents a new approach for 3D shape generation, enabling direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets, and formulate a pair of neural networks: a generator based on the diffusion model to produce diverse shapes in the form of coarse coefficient volumes; and a detail predictor to further produce compatible detail coefficient volumes for enriching the generated shapes with fine structures and details. Both quantitative and qualitative experimental results manifest the superiority of our approach in generating diverse and high-quality shapes with complex topology and structures, clean surfaces, and fine details, exceeding the 3D generation capabilities of the state-of-the-art models.",
        "authors": [
            "Ka-Hei Hui",
            "Ruihui Li",
            "Jingyu Hu",
            "Chi-Wing Fu"
        ],
        "citations": 106,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Self-conditioned Embedding Diffusion for Text Generation",
        "abstract": "Can continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditioned Embedding Diffusion, a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. Through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models - while being in theory more efficient on accelerator hardware at inference time. Our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion.",
        "authors": [
            "Robin Strudel",
            "Corentin Tallec",
            "Florent Altch'e",
            "Yilun Du",
            "Yaroslav Ganin",
            "A. Mensch",
            "Will Grathwohl",
            "Nikolay Savinov",
            "S. Dieleman",
            "L. Sifre",
            "Rémi Leblond"
        ],
        "citations": 73,
        "references": 35,
        "year": 2022
    },
    {
        "title": "Diffusion-based Molecule Generation with Informative Prior Bridges",
        "abstract": "AI-based molecule generation provides a promising approach to a large area of biomedical sciences and engineering, such as antibody design, hydrolase engineering, or vaccine development. Because the molecules are governed by physical laws, a key challenge is to incorporate prior information into the training procedure to generate high-quality and realistic molecules. We propose a simple and novel approach to steer the training of diffusion-based generative models with physical and statistics prior information. This is achieved by constructing physically informed diffusion bridges, stochastic processes that guarantee to yield a given observation at the fixed terminal time. We develop a Lyapunov function based method to construct and determine bridges, and propose a number of proposals of informative prior bridges for both high-quality molecule generation and uniformity-promoted 3D point cloud generation. With comprehensive experiments, we show that our method provides a powerful approach to the 3D generation task, yielding molecule structures with better quality and stability scores and more uniformly distributed point clouds of high qualities.",
        "authors": [
            "Lemeng Wu",
            "Chengyue Gong",
            "Xingchao Liu",
            "Mao Ye",
            "Qiang Liu"
        ],
        "citations": 94,
        "references": 56,
        "year": 2022
    },
    {
        "title": "KNN-Diffusion: Image Generation via Large-Scale Retrieval",
        "abstract": "Recent text-to-image models have achieved impressive results. However, since they require large-scale datasets of text-image pairs, it is impractical to train them on new domains where data is scarce or not labeled. In this work, we propose using large-scale retrieval methods, in particular, efficient k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a substantially small and efficient text-to-image diffusion model without any text, (2) generating out-of-distribution images by simply swapping the retrieval database at inference time, and (3) performing text-driven local semantic manipulations while preserving object identity. To demonstrate the robustness of our method, we apply our kNN approach on two state-of-the-art diffusion backbones, and show results on several different datasets. As evaluated by human studies and automatic metrics, our method achieves state-of-the-art results compared to existing approaches that train text-to-image generation models using images only (without paired text data)",
        "authors": [
            "Oron Ashual",
            "Shelly Sheynin",
            "Adam Polyak",
            "Uriel Singer",
            "Oran Gafni",
            "Eliya Nachmani",
            "Yaniv Taigman"
        ],
        "citations": 101,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Latent Diffusion Energy-Based Model for Interpretable Text Modeling",
        "abstract": "Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned latent space. Experiments on several challenging tasks demonstrate the superior performance of our model on interpretable text modeling over strong counterparts.",
        "authors": [
            "Peiyu Yu",
            "Sirui Xie",
            "Xiaojian Ma",
            "Baoxiong Jia",
            "Bo Pang",
            "Ruigi Gao",
            "Yixin Zhu",
            "Song-Chun Zhu",
            "Y. Wu"
        ],
        "citations": 72,
        "references": 80,
        "year": 2022
    },
    {
        "title": "Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2",
        "abstract": "The field of image synthesis has made great strides in the last couple of years. Recent models are capable of generating images with astonishing quality. Fine-grained evaluation of these models on some interesting categories such as faces is still missing. Here, we conduct a quantitative comparison of three popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their ability to generate photorealistic faces in the wild. We find that Stable Diffusion generates better faces than the other systems, according to the FID score. We also introduce a dataset of generated faces in the wild dubbed GFW, including a total of 15,076 faces. Furthermore, we hope that our study spurs follow-up research in assessing the generative models and improving them. Data and code are available at data and code, respectively.",
        "authors": [
            "A. Borji"
        ],
        "citations": 101,
        "references": 14,
        "year": 2022
    },
    {
        "title": "Diffusion Model with Perceptual Loss",
        "abstract": "Diffusion models without guidance tend to generate unrealistic samples, yet the cause of this problem is not fully studied. Our analysis suggests that the loss objective plays an important role in shaping the learned distribution and the common mean squared error loss is not optimal. We hypothesize that a better loss objective can be designed with inductive biases and propose a novel self-perceptual loss that utilizes the diffusion model itself as the perceptual loss. Our work demonstrates that perceptual loss can be used in diffusion training to improve sample quality effectively. Models trained using our objective can generate realistic samples without guidance. We hope our work paves the way for more future explorations of the diffusion loss objective.",
        "authors": [
            "Shanchuan Lin",
            "Xiao Yang"
        ],
        "citations": 13,
        "references": 62,
        "year": 2023
    },
    {
        "title": "MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion",
        "abstract": "Patient scans from MRI often suffer from noise, which hampers the diagnostic capability of such images. As a method to mitigate such artifacts, denoising is largely studied both within the medical imaging community and beyond the community as a general subject. However, recent deep neural network-based approaches mostly rely on the minimum mean squared error (MMSE) estimates, which tend to produce a blurred output. Moreover, such models suffer when deployed in real-world situations: out-of-distribution data, and complex noise distributions that deviate from the usual parametric noise models. In this work, we propose a new denoising method based on score-based reverse diffusion sampling, which overcomes all the aforementioned drawbacks. Our network, trained only with coronal knee scans, excels even on out-of-distribution in vivo liver MRI data, contaminated with a complex mixture of noise. Even more, we propose a method to enhance the resolution of the denoised image with the same network. With extensive experiments, we show that our method establishes state-of-the-art performance while having desirable properties which prior MMSE denoisers did not have: flexibly choosing the extent of denoising, and quantifying uncertainty.",
        "authors": [
            "Hyungjin Chung",
            "Eunha Lee",
            "Jong-Chul Ye"
        ],
        "citations": 89,
        "references": 66,
        "year": 2022
    },
    {
        "title": "Diffusion Probabilistic Model Made Slim",
        "abstract": "Despite the recent visually-pleasing results achieved, the massive computational cost has been a long-standing flaw for diffusion probabilistic models (DPMs), which, in turn, greatly limits their applications on resource-limited platforms. Prior methods towards efficient DPM, however, have largely focused on accelerating the testing yet overlooked their huge complexity and sizes. In this paper, we make a dedicated attempt to lighten DPM while striving to preserve its favourable performance. We start by training a small-sized latent diffusion model (LDM) from scratch, but observe a significant fidelity drop in the synthetic images. Through a thorough assessment, we find that DPM is intrinsically biased against high-frequency generation, and learns to recover different frequency components at different time-steps. These properties make compact networks unable to represent frequency dynamics with accurate high-frequency estimation. Towards this end, we introduce a customized design for slim DPM, which we term as Spectral Diffusion (SD), for light-weight image synthesis. SD incorporates wavelet gating in its architecture to enable frequency dynamic feature extraction at every reverse step, and conducts spectrum-aware distillation to promote high-frequency recovery by inverse weighting the objective based on spectrum magnitude. Experimental results demonstrate that, SD achieves 8–18 × computational complexity reduction as compared to the latent diffusion models on a series of conditional and unconditional image generation tasks while retaining competitive image fidelity.",
        "authors": [
            "Xingyi Yang",
            "Daquan Zhou",
            "Jiashi Feng",
            "Xinchao Wang"
        ],
        "citations": 77,
        "references": 90,
        "year": 2022
    },
    {
        "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
        "abstract": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM—a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
        "authors": [
            "Xiaochuang Han",
            "Sachin Kumar",
            "Yulia Tsvetkov"
        ],
        "citations": 64,
        "references": 98,
        "year": 2022
    },
    {
        "title": "SinDDM: A Single Image Denoising Diffusion Model",
        "abstract": "Denoising diffusion models (DDMs) have led to staggering performance leaps in image generation, editing and restoration. However, existing DDMs use very large datasets for training. Here, we introduce a framework for training a DDM on a single image. Our method, which we coin SinDDM, learns the internal statistics of the training image by using a multi-scale diffusion process. To drive the reverse diffusion process, we use a fully-convolutional light-weight denoiser, which is conditioned on both the noise level and the scale. This architecture allows generating samples of arbitrary dimensions, in a coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality samples, and is applicable in a wide array of tasks, including style transfer and harmonization. Furthermore, it can be easily guided by external supervision. Particularly, we demonstrate text-guided generation from a single image using a pre-trained CLIP model.",
        "authors": [
            "V. Kulikov",
            "Shahar Yadin",
            "Matan Kleiner",
            "T. Michaeli"
        ],
        "citations": 63,
        "references": 35,
        "year": 2022
    },
    {
        "title": "StoRM: A Diffusion-Based Stochastic Regeneration Model for Speech Enhancement and Dereverberation",
        "abstract": "Diffusion models have shown a great ability at bridging the performance gap between predictive and generative approaches for speech enhancement. We have shown that they may even outperform their predictive counterparts for non-additive corruption types or when they are evaluated on mismatched conditions. However, diffusion models suffer from a high computational burden, mainly as they require to run a neural network for each reverse diffusion step, whereas predictive approaches only require one pass. As diffusion models are generative approaches they may also produce vocalizing and breathing artifacts in adverse conditions. In comparison, in such difficult scenarios, predictive models typically do not produce such artifacts but tend to distort the target speech instead, thereby degrading the speech quality. In this work, we present a stochastic regeneration approach where an estimate given by a predictive model is provided as a guide for further diffusion. We show that the proposed approach uses the predictive model to remove the vocalizing and breathing artifacts while producing very high quality samples thanks to the diffusion model, even in adverse conditions. We further show that this approach enables to use lighter sampling schemes with fewer diffusion steps without sacrificing quality, thus lifting the computational burden by an order of magnitude. Source code and audio examples are available online.",
        "authors": [
            "Jean-Marie Lemercier",
            "Julius Richter",
            "Simon Welker",
            "Timo Gerkmann"
        ],
        "citations": 60,
        "references": 73,
        "year": 2022
    },
    {
        "title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers",
        "abstract": "Diffusion model, a new generative modelling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of text, it is not trivial to extend continuous diffusion models to natural language, and text diffusion models are less studied. Sequence-to-sequence text generation is one of the essential natural language processing topics. In this work, we apply diffusion models to approach sequence-to-sequence text generation, and explore whether the superiority generation performance of diffusion model can transfer to natural language domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to model denoising function. In order to improve generation quality, SeqDiffuSeq combines the self-conditioning technique and a newly proposed adaptive noise schedule technique. The adaptive noise schedule has the difficulty of denoising evenly distributed across time steps, and considers exclusive noise schedules for tokens at different positional order. Experiment results illustrate the good performance on sequence-to-sequence generation in terms of text quality and inference time.",
        "authors": [
            "Hongyi Yuan",
            "Zheng Yuan",
            "Chuanqi Tan",
            "Fei Huang",
            "Songfang Huang"
        ],
        "citations": 61,
        "references": 62,
        "year": 2022
    },
    {
        "title": "On the Generalization of Diffusion Model",
        "abstract": "The diffusion probabilistic generative models are widely used to generate high-quality data. Though they can synthetic data that does not exist in the training set, the rationale behind such generalization is still unexplored. In this paper, we formally define the generalization of the generative model, which is measured by the mutual information between the generated data and the training set. The definition originates from the intuition that the model which generates data with less correlation to the training set exhibits better generalization ability. Meanwhile, we show that for the empirical optimal diffusion model, the data generated by a deterministic sampler are all highly related to the training set, thus poor generalization. This result contradicts the observation of the trained diffusion model's (approximating empirical optima) extrapolation ability (generating unseen data). To understand this contradiction, we empirically verify the difference between the sufficiently trained diffusion model and the empirical optima. We found, though obtained through sufficient training, there still exists a slight difference between them, which is critical to making the diffusion model generalizable. Moreover, we propose another training objective whose empirical optimal solution has no potential generalization problem. We empirically show that the proposed training objective returns a similar model to the original one, which further verifies the generalization ability of the trained diffusion model.",
        "authors": [
            "Mingyang Yi",
            "Jiacheng Sun",
            "Zhenguo Li"
        ],
        "citations": 15,
        "references": 39,
        "year": 2023
    },
    {
        "title": "A Morphology Focused Diffusion Probabilistic Model for Synthesis of Histopathology Images",
        "abstract": "Visual microscopic study of diseased tissue by pathologists has been the cornerstone for cancer diagnosis and prognostication for more than a century. Recently, deep learning methods have made significant advances in the analysis and classification of tissue images. However, there has been limited work on the utility of such models in generating histopathology images. These synthetic images have several applications in pathology including utilities in education, proficiency testing, privacy, and data sharing. Recently, diffusion probabilistic models were introduced to generate high quality images. Here, for the first time, we investigate the potential use of such models along with prioritized morphology weighting and color normalization to synthesize high quality histopathology images of brain cancer. Our detailed results show that diffusion probabilistic models are capable of synthesizing a wide range of histopathology images and have superior performance compared to generative adversarial networks.",
        "authors": [
            "Puria Azadi Moghadam",
            "Sanne Van Dalen",
            "K. C. Martin",
            "J. Lennerz",
            "Stephen S. F. Yip",
            "H. Farahani",
            "Ali Bashashati"
        ],
        "citations": 71,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Adaptively-Realistic Image Generation from Stroke and Sketch with Diffusion Model",
        "abstract": "Generating images from hand-drawings is a crucial and fundamental task in content creation. The translation is difficult as there exist infinite possibilities and the different users usually expect different outcomes. Therefore, we propose a unified framework supporting a three-dimensional control over the image synthesis from sketches and strokes based on diffusion models. Users can not only decide the level of faithfulness to the input strokes and sketches, but also the degree of realism, as the user inputs are usually not consistent with the real images. Qualitative and quantitative experiments demonstrate that our framework achieves state-of-the-art performance while providing flexibility in generating customized images with control over shape, color, and realism. Moreover, our method unleashes applications such as editing on real images, generation with partial sketches and strokes, and multi-domain multi-modal synthesis.",
        "authors": [
            "Shin-I Cheng",
            "Yu-Jie Chen",
            "Wei-Chen Chiu",
            "Hsin-Ying Lee",
            "Hung-Yu Tseng"
        ],
        "citations": 51,
        "references": 34,
        "year": 2022
    },
    {
        "title": "Latent Diffusion for Language Generation",
        "abstract": "Diffusion models have achieved great success in modeling continuous data modalities such as images, audio, and video, but have seen limited use in discrete domains such as language. Recent attempts to adapt diffusion to language have presented diffusion as an alternative to autoregressive language generation. We instead view diffusion as a complementary method that can augment the generative capabilities of existing pre-trained language models. We demonstrate that continuous diffusion models can be learned in the latent space of a pre-trained encoder-decoder model, enabling us to sample continuous latent representations that can be decoded into natural language with the pre-trained decoder. We show that our latent diffusion models are more effective at sampling novel text from data distributions than a strong autoregressive baseline and also enable controllable generation.",
        "authors": [
            "Justin Lovelace",
            "Varsha Kishore",
            "Chao-gang Wan",
            "Eliot Shekhtman",
            "Kilian Q. Weinberger"
        ],
        "citations": 54,
        "references": 81,
        "year": 2022
    },
    {
        "title": "BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for Binaural Audio Synthesis",
        "abstract": "Binaural audio plays a significant role in constructing immersive augmented and virtual realities. As it is expensive to record binaural audio from the real world, synthesizing them from mono audio has attracted increasing attention. This synthesis process involves not only the basic physical warping of the mono audio, but also room reverberations and head/ear related filtrations, which, however, are difficult to accurately simulate in traditional digital signal processing. In this paper, we formulate the synthesis process from a different perspective by decomposing the binaural audio into a common part that shared by the left and right channels as well as a specific part that differs in each channel. Accordingly, we propose BinauralGrad, a novel two-stage framework equipped with diffusion models to synthesize them respectively. Specifically, in the first stage, the common information of the binaural audio is generated with a single-channel diffusion model conditioned on the mono audio, based on which the binaural audio is generated by a two-channel diffusion model in the second stage. Combining this novel perspective of two-stage synthesis with advanced generative models (i.e., the diffusion models),the proposed BinauralGrad is able to generate accurate and high-fidelity binaural audio samples. Experiment results show that on a benchmark dataset, BinauralGrad outperforms the existing baselines by a large margin in terms of both object and subject evaluation metrics (Wave L2: 0.128 vs. 0.157, MOS: 3.80 vs. 3.61). The generated audio samples (https://speechresearch.github.io/binauralgrad) and code (https://github.com/microsoft/NeuralSpeech/tree/master/BinauralGrad) are available online.",
        "authors": [
            "Yichong Leng",
            "Zehua Chen",
            "Junliang Guo",
            "Haohe Liu",
            "Jiawei Chen",
            "Xu Tan",
            "Danilo P. Mandic",
            "Lei He",
            "Xiang-Yang Li",
            "Tao Qin",
            "Sheng Zhao",
            "Tie-Yan Liu"
        ],
        "citations": 50,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Diffusion Deformable Model for 4D Temporal Medical Image Generation",
        "abstract": "Temporal volume images with 3D+t (4D) information are often used in medical imaging to statistically analyze temporal dynamics or capture disease progression. Although deep-learning-based generative models for natural images have been extensively studied, approaches for temporal medical image generation such as 4D cardiac volume data are limited. In this work, we present a novel deep learning model that generates intermediate temporal volumes between source and target volumes. Specifically, we propose a diffusion deformable model (DDM) by adapting the denoising diffusion probabilistic model that has recently been widely investigated for realistic image generation. Our proposed DDM is composed of the diffusion and the deformation modules so that DDM can learn spatial deformation information between the source and target volumes and provide a latent code for generating intermediate frames along a geodesic path. Once our model is trained, the latent code estimated from the diffusion module is simply interpolated and fed into the deformation module, which enables DDM to generate temporal frames along the continuous trajectory while preserving the topology of the source image. We demonstrate the proposed method with the 4D cardiac MR image generation between the diastolic and systolic phases for each subject. Compared to the existing deformation methods, our DDM achieves high performance on temporal volume generation.",
        "authors": [
            "Boah Kim",
            "Jong-Chul Ye"
        ],
        "citations": 69,
        "references": 20,
        "year": 2022
    },
    {
        "title": "DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras",
        "abstract": "We propose DiffuStereo, a novel system using only sparse cameras (8 in this work) for high-quality 3D human reconstruction. At its core is a novel diffusion-based stereo module, which introduces diffusion models, a type of powerful generative models, into the iterative stereo matching network. To this end, we design a new diffusion kernel and additional stereo constraints to facilitate stereo matching and depth estimation in the network. We further present a multi-level stereo network architecture to handle high-resolution (up to 4k) inputs without requiring unaffordable memory footprint. Given a set of sparse-view color images of a human, the proposed multi-level diffusion-based stereo network can produce highly accurate depth maps, which are then converted into a high-quality 3D human model through an efficient multi-view fusion strategy. Overall, our method enables automatic reconstruction of human models with quality on par to high-end dense-view camera rigs, and this is achieved using a much more light-weight hardware setup. Experiments show that our method outperforms state-of-the-art methods by a large margin both qualitatively and quantitatively.",
        "authors": [
            "Ruizhi Shao",
            "Zerong Zheng",
            "Hongwen Zhang",
            "Jingxiang Sun",
            "Yebin Liu"
        ],
        "citations": 48,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Conditional Simulation Using Diffusion Schrödinger Bridges",
        "abstract": "Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr\\\"odinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schr\\\"odinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks. Our code can be found at https://github.com/vdeborto/cdsb.",
        "authors": [
            "Yuyang Shi",
            "Valentin De Bortoli",
            "George Deligiannidis",
            "A. Doucet"
        ],
        "citations": 49,
        "references": 60,
        "year": 2022
    },
    {
        "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
        "abstract": "Text-to-image generative models such as Stable Diffusion and DALL•E raise many ethical concerns due to the generation of harmful images such as Not-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety filters are often adopted to prevent the generation of NSFW images. In this work, we propose SneakyPrompt, the first automated attack framework, to jailbreak text-to-image generative models such that they generate NSFW images even if safety filters are adopted. Given a prompt that is blocked by a safety filter, SneakyPrompt repeatedly queries the text-to-image generative model and strategically perturbs tokens in the prompt based on the query results to bypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement learning to guide the perturbation of tokens. Our evaluation shows that SneakyPrompt successfully jailbreaks DALL•E 2 with closed-box safety filters to generate NSFW images. Moreover, we also deploy several state-of-the-art, open-source safety filters on a Stable Diffusion model. Our evaluation shows that SneakyPrompt not only successfully generates NSFW images, but also outperforms existing text adversarial attacks when extended to jailbreak text-to-image generative models, in terms of both the number of queries and qualities of the generated NSFW images. SneakyPrompt is open-source and available at this repository: https://github.com/Yuchen413/text2image_safety.",
        "authors": [
            "Yuchen Yang",
            "Bo Hui",
            "Haolin Yuan",
            "Neil Gong",
            "Yinzhi Cao"
        ],
        "citations": 44,
        "references": 49,
        "year": 2023
    },
    {
        "title": "DALL-EVAL: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models",
        "abstract": "Recently, DALL-E [45], a multimodal transformer language model, and its variants including diffusion models have shown high-quality text-to-image generation capabilities. However, despite the realistic image generation results, there has not been a detailed analysis of how to evaluate such models. In this work, we investigate the visual reasoning capabilities and social biases of different text-to-image models, covering both multimodal transformer language models and diffusion models. First, we measure three visual reasoning skills: object recognition, object counting, and spatial relation understanding. For this, we propose PaintSkills, a compositional diagnostic evaluation dataset that measures these skills. Despite the high-fidelity image generation capability, a large gap exists between the performance of recent models and the upper bound accuracy in object counting and spatial relation understanding skills. Second, we assess the gender and skin tone biases by measuring the gender/skin tone distribution of generated images across various professions and attributes. We demonstrate that recent text-to-image generation models learn specific biases about gender and skin tone from web image-text pairs. We hope our work will help guide future progress in improving text-to-image generation models on visual reasoning skills and learning socially unbiased representations.1",
        "authors": [
            "Jaemin Cho",
            "Abhaysinh Zala",
            "Mohit Bansal"
        ],
        "citations": 138,
        "references": 104,
        "year": 2022
    },
    {
        "title": "Semantic-Conditional Diffusion Networks for Image Captioning*",
        "abstract": "Recent advances on text-to-image generation have witnessed the rise of diffusion models which act as powerful generative models. Nevertheless, it is not trivial to exploit such latent variable models to capture the dependency among discrete words and meanwhile pursue complex visual-language alignment in image captioning. In this paper, we break the deeply rooted conventions in learning Transformer-based encoder-decoder, and propose a new diffusion model based paradigm tailored for image captioning, namely Semantic-Conditional Diffusion Networks (SCD-Net). Technically, for each input image, we first search the semantically relevant sentences via cross-modal retrieval model to convey the comprehensive semantic information. The rich semantics are further regarded as semantic prior to trigger the learning of Diffusion Transformer, which produces the output sentence in a diffusion process. In SCD-Net, multiple Diffusion Transformer structures are stacked to progressively strengthen the output sentence with better visional-language alignment and linguistical coherence in a cascaded manner. Furthermore, to stabilize the diffusion process, a new self-critical sequence training strategy is designed to guide the learning of SCD-Net with the knowledge of a standard autoregressive Transformer model. Extensive experiments on COCO dataset demonstrate the promising potential of using diffusion models in the challenging image captioning task. Source code is available at",
        "authors": [
            "Jianjie Luo",
            "Yehao Li",
            "Yingwei Pan",
            "Ting Yao",
            "Jianlin Feng",
            "Hongyang Chao",
            "Tao Mei"
        ],
        "citations": 46,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Difformer: Empowering Diffusion Model on Embedding Space for Text Generation",
        "abstract": "Diffusion models have achieved state-of-the-art synthesis quality on visual and audio tasks, and recent works adapt them to textual data by diffusing on the embedding space. But the difference between the continuous data space and the embedding space raises challenges to the diffusion model, which have not been carefully explored. In this paper, we conduct systematic studies and analyze the challenges threefold. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the loss function. Secondly, as the norm of embedding varies between popular and rare words, adding the same noise scale will lead to sub-optimal results. In addition, we ﬁnd that noises sampled from a standard Gaussian distribution may distract the diffusion process. To solve the above challenges, we propose Difformer, a denoising diffusion probabilistic model based on Transformer, which consists of three techniques including utilizing an anchor loss function, a layer normalization module for embeddings, and a norm factor to the Gaussian noise. All techniques are complementary to each other and critical to boosting the model performance together. Experiments are conducted on benchmark datasets over two seminal text generation tasks including machine translation and text summarization. The results show that Difformer significantly outperforms the embedding diffusion baselines, while achieving competitive results with strong autoregressive baselines.",
        "authors": [
            "Zhujin Gao",
            "Junliang Guo",
            "Xuejiao Tan",
            "Yongxin Zhu",
            "Fang Zhang",
            "Jiang Bian",
            "Linli Xu"
        ],
        "citations": 39,
        "references": 37,
        "year": 2022
    },
    {
        "title": "Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching",
        "abstract": "Score-based generative models have excellent performance in terms of generation quality and likelihood. They model the data distribution by matching a parameterized score network with first-order data score functions. The score network can be used to define an ODE (\"score-based diffusion ODE\") for exact likelihood evaluation. However, the relationship between the likelihood of the ODE and the score matching objective is unclear. In this work, we prove that matching the first-order score is not sufficient to maximize the likelihood of the ODE, by showing a gap between the maximum likelihood and score matching objectives. To fill up this gap, we show that the negative likelihood of the ODE can be bounded by controlling the first, second, and third-order score matching errors; and we further present a novel high-order denoising score matching method to enable maximum likelihood training of score-based diffusion ODEs. Our algorithm guarantees that the higher-order matching error is bounded by the training error and the lower-order errors. We empirically observe that by high-order score matching, score-based diffusion ODEs achieve better likelihood on both synthetic data and CIFAR-10, while retaining the high generation quality.",
        "authors": [
            "Cheng Lu",
            "Kaiwen Zheng",
            "Fan Bao",
            "Jianfei Chen",
            "Chongxuan Li",
            "Jun Zhu"
        ],
        "citations": 65,
        "references": 35,
        "year": 2022
    },
    {
        "title": "DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction",
        "abstract": "Limited-Angle Computed Tomography (LACT) is a nondestructive 3D imaging technique used in a variety of applications ranging from security to medicine. The limited angle coverage in LACT is often a dominant source of severe artifacts in the reconstructed images, making it a challenging imaging inverse problem. Diffusion models are a recent class of deep generative models for synthesizing realistic images using image denoisers. In this work, we present DOLCE as the first framework for integrating conditionally-trained diffusion models and explicit physical measurement models for solving imaging inverse problems. DOLCE achieves the SOTA performance in highly ill-posed LACT by alternating between the data-fidelity and sampling updates of a diffusion model conditioned on the transformed sinogram. We show through extensive experimentation that unlike existing methods, DOLCE can synthesize high-quality and structurally coherent 3D volumes by using only 2D conditionally pre-trained diffusion models. We further show on several challenging real LACT datasets that the same pretrained DOLCE model achieves the SOTA performance on drastically different types of images.",
        "authors": [
            "Jiaming Liu",
            "Rushil Anirudh",
            "J. Thiagarajan",
            "Stewart He",
            "K. A. Mohan",
            "U. Kamilov",
            "Hyojin Kim"
        ],
        "citations": 45,
        "references": 103,
        "year": 2022
    },
    {
        "title": "SinDiffusion: Learning a Diffusion Model from a Single Natural Image",
        "abstract": "We present SinDiffusion, leveraging denoising diffusion models to capture internal distribution of patches from a single natural image. SinDiffusion significantly improves the quality and diversity of generated samples compared with existing GAN-based approaches. It is based on two core designs. First, SinDiffusion is trained with a single model at a single scale instead of multiple models with progressive growing of scales which serves as the default setting in prior work. This avoids the accumulation of errors, which cause characteristic artifacts in generated results. Second, we identify that a patch-level receptive field of the diffusion network is crucial and effective for capturing the image's patch statistics, therefore we redesign the network structure of the diffusion model. Coupling these two designs enables us to generate photorealistic and diverse images from a single image. Furthermore, SinDiffusion can be applied to various applications, i.e., text-guided image generation, and image outpainting, due to the inherent capability of diffusion models. Extensive experiments on a wide range of images demonstrate the superiority of our proposed method for modeling the patch distribution.",
        "authors": [
            "Weilun Wang",
            "Jianmin Bao",
            "Wen-gang Zhou",
            "Dongdong Chen",
            "Dong Chen",
            "Lu Yuan",
            "Houqiang Li"
        ],
        "citations": 41,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems",
        "abstract": "With the rapid development of diffusion models and flow-based generative models, there has been a surge of interests in solving noisy linear inverse problems, e.g., super-resolution, deblurring, denoising, colorization, etc, with generative models. However, while remarkable reconstruction performances have been achieved, their inference time is typically too slow since most of them rely on the seminal diffusion posterior sampling (DPS) framework and thus to approximate the intractable likelihood score, time-consuming gradient calculation through back-propagation is needed. To address this issue, this paper provides a fast and effective solution by proposing a simple closed-form approximation to the likelihood score. For both diffusion and flow-based models, extensive experiments are conducted on various noisy linear inverse problems such as noisy super-resolution, denoising, deblurring, and colorization. In all these tasks, our method (namely DMPS) demonstrates highly competitive or even better reconstruction performances while being significantly faster than all the baseline methods.",
        "authors": [
            "Xiangming Meng",
            "Y. Kabashima"
        ],
        "citations": 43,
        "references": 84,
        "year": 2022
    },
    {
        "title": "DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) are expressive generative models that have been used to solve a variety of speech synthesis problems. However, because of their high sampling costs, DDPMs are difficult to use in real-time speech processing applications. In this paper, we introduce DiffGAN-TTS, a novel DDPM-based text-to-speech (TTS) model achieving high-fidelity and efficient speech synthesis. DiffGAN-TTS is based on denoising diffusion generative adversarial networks (GANs), which adopt an adversarially-trained expressive model to approximate the denoising distribution. We show with multi-speaker TTS experiments that DiffGAN-TTS can generate high-fidelity speech samples within only 4 denoising steps. We present an active shallow diffusion mechanism to further speed up inference. A two-stage training scheme is proposed, with a basic TTS acoustic model trained at stage one providing valuable prior information for a DDPM trained at stage two. Our experiments show that DiffGAN-TTS can achieve high synthesis performance with only 1 denoising step.",
        "authors": [
            "Songxiang Liu",
            "Dan Su",
            "Dong Yu"
        ],
        "citations": 59,
        "references": 53,
        "year": 2022
    },
    {
        "title": "MDM: Molecular Diffusion Model for 3D Molecule Generation",
        "abstract": "Molecule generation, especially generating 3D molecular geometries from scratch (i.e., 3D de novo generation), has become a fundamental task in drug design. Existing diffusion based 3D molecule generation methods could suffer from unsatisfactory performances, especially when generating large molecules. At the same time, the generated molecules lack enough diversity. This paper proposes a novel diffusion model to address those two challenges. \n\nFirst, interatomic relations are not included in molecules' 3D point cloud representations. Thus, it is difficult for existing generative models to capture the potential interatomic forces and abundant local constraints. \nTo tackle this challenge, we propose to augment the potential interatomic forces and further involve dual equivariant encoders to encode interatomic forces of different strengths.\nSecond, existing diffusion-based models essentially shift elements in geometry along the gradient of data density. Such a process lacks enough exploration in the intermediate steps of the Langevin dynamics. To address this issue, we introduce a distributional controlling variable in each diffusion/reverse step to enforce thorough explorations and further improve generation diversity.\n\nExtensive experiments on multiple benchmarks demonstrate that the proposed model significantly outperforms existing methods for both unconditional and conditional generation tasks. We also conduct case studies to help understand the physicochemical properties of the generated molecules. The codes are available at https://github.com/tencent-ailab/MDM.",
        "authors": [
            "Lei Huang",
            "Hengtong Zhang",
            "Tingyang Xu",
            "Ka-chun Wong"
        ],
        "citations": 64,
        "references": 28,
        "year": 2022
    },
    {
        "title": "Lossy Compression with Gaussian Diffusion",
        "abstract": "We consider a novel lossy compression approach based on unconditional diffusion generative models, which we call DiffC. Unlike modern compression schemes which rely on transform coding and quantization to restrict the transmitted information, DiffC relies on the efficient communication of pixels corrupted by Gaussian noise. We implement a proof of concept and find that it works surprisingly well despite the lack of an encoder transform, outperforming the state-of-the-art generative compression method HiFiC on ImageNet 64x64. DiffC only uses a single model to encode and denoise corrupted pixels at arbitrary bitrates. The approach further provides support for progressive coding, that is, decoding from partial bit streams. We perform a rate-distortion analysis to gain a deeper understanding of its performance, providing analytical results for multivariate Gaussian data as well as theoretic bounds for general distributions. Furthermore, we prove that a flow-based reconstruction achieves a 3 dB gain over ancestral sampling at high bitrates.",
        "authors": [
            "Lucas Theis",
            "Tim Salimans",
            "M. Hoffman",
            "Fabian Mentzer"
        ],
        "citations": 64,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Guided Diffusion Model for Adversarial Purification from Random Noise",
        "abstract": "In this paper, we propose a novel guided diffusion purification approach to provide a strong defense against adversarial attacks. Our model achieves 89.62% robust accuracy under PGD-L_inf attack (eps = 8/255) on the CIFAR-10 dataset. We first explore the essential correlations between unguided diffusion models and randomized smoothing, enabling us to apply the models to certified robustness. The empirical results show that our models outperform randomized smoothing by 5% when the certified L2 radius r is larger than 0.5.",
        "authors": [
            "Quanlin Wu",
            "Hang Ye",
            "Yuntian Gu"
        ],
        "citations": 37,
        "references": 18,
        "year": 2022
    },
    {
        "title": "Cold Diffusion for Speech Enhancement",
        "abstract": "Diffusion models have recently shown promising results for difficult enhancement tasks such as the conditional and unconditional restoration of natural images and audio signals. In this work, we explore the possibility of leveraging a recently proposed advanced iterative diffusion model, namely cold diffusion, to recover clean speech signals from noisy signals. The unique mathematical properties of the sampling process from cold diffusion could be utilized to restore high-quality samples from arbitrary degradations. Based on these properties, we propose an improved training algorithm and objective to help the model generalize better during the sampling process. We verify our proposed framework by investigating two model architectures. Experimental results on benchmark speech enhancement dataset VoiceBank-DEMAND demonstrate the strong performance of the proposed approach compared to representative discriminative models and diffusion-based enhancement models.",
        "authors": [
            "Hao Yen",
            "François G. Germain",
            "G. Wichern",
            "Jonathan Le Roux"
        ],
        "citations": 36,
        "references": 42,
        "year": 2022
    },
    {
        "title": "Bayesian deep learning for error estimation in the analysis of anomalous diffusion",
        "abstract": null,
        "authors": [
            "Henrik Seckler",
            "R. Metzler"
        ],
        "citations": 43,
        "references": 111,
        "year": 2022
    },
    {
        "title": "Diffusioninst: Diffusion Model for Instance Segmentation",
        "abstract": "Diffusion frameworks have achieved comparable performance with previous state-of-the-art image generation models. This paper proposes DiffusionInst, a novel framework representing instances as vectors and formulates instance segmentation as a noise-to-vector denoising process. The model is trained to reverse the noisy groundtruth mask without any inductive bias from RPN. It takes a randomly generated vector as input and outputs mask with multi-step denoising during inference. Extensive experimental results on COCO and LVIS show that DiffusionInst achieves competitive performance. Our code is available at https://github.com/chenhaoxing/DiffusionInst.",
        "authors": [
            "Zhangxuan Gu",
            "Haoxing Chen",
            "Zhuoer Xu",
            "Jun Lan",
            "Changhua Meng",
            "Weiqiang Wang"
        ],
        "citations": 59,
        "references": 60,
        "year": 2022
    },
    {
        "title": "An optimal control perspective on diffusion-based generative modeling",
        "abstract": "We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton-Jacobi-Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback-Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approaches on multiple numerical examples.",
        "authors": [
            "Julius Berner",
            "Lorenz Richter",
            "Karen Ullrich"
        ],
        "citations": 60,
        "references": 99,
        "year": 2022
    },
    {
        "title": "High-Frequency Space Diffusion Model for Accelerated MRI",
        "abstract": "Diffusion models with continuous stochastic differential equations (SDEs) have shown superior performances in image generation. It can serve as a deep generative prior to solving the inverse problem in magnetic resonance (MR) reconstruction. However, low-frequency regions of ${k}$ -space data are typically fully sampled in fast MR imaging, while existing diffusion models are performed throughout the entire image or ${k}$ -space, inevitably introducing uncertainty in the reconstruction of low-frequency regions. Additionally, existing diffusion models often demand substantial iterations to converge, resulting in time-consuming reconstructions. To address these challenges, we propose a novel SDE tailored specifically for MR reconstruction with the diffusion process in high-frequency space (referred to as HFS-SDE). This approach ensures determinism in the fully sampled low-frequency regions and accelerates the sampling procedure of reverse diffusion. Experiments conducted on the publicly available fastMRI dataset demonstrate that the proposed HFS-SDE method outperforms traditional parallel imaging methods, supervised deep learning, and existing diffusion models in terms of reconstruction accuracy and stability. The fast convergence properties are also confirmed through theoretical and experimental validation. Our code and weights are available at https://github.com/Aboriginer/HFS-SDE.",
        "authors": [
            "Chentao Cao",
            "Zhuoxu Cui",
            "Yue Wang",
            "Shaonan Liu",
            "Taijin Chen",
            "Hairong Zheng",
            "Dong Liang",
            "Yanjie Zhu"
        ],
        "citations": 36,
        "references": 66,
        "year": 2022
    },
    {
        "title": "A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images",
        "abstract": "Diffusion models are a special type of generative model, capable of synthesising new data from a learnt distribution. We introduce DISPR, a diffusion-based model for solving the inverse problem of three-dimensional (3D) cell shape prediction from two-dimensional (2D) single cell microscopy images. Using the 2D microscopy image as a prior, DISPR is conditioned to predict realistic 3D shape reconstructions. To showcase the applicability of DISPR as a data augmentation tool in a feature-based single cell classification task, we extract morphological features from the red blood cells grouped into six highly imbalanced classes. Adding features from the DISPR predictions to the three minority classes improved the macro F1 score from F1macro = 55.2 ± 4.6% to F1macro = 72.2 ± 4.9%. We thus demonstrate that diffusion models can be successfully applied to inverse biomedical problems, and that they learn to reconstruct 3D shapes with realistic morphological features from 2D microscopy images.",
        "authors": [
            "Dominik Jens Elias Waibel",
            "Ernst Rooell",
            "Bastian Alexander Rieck",
            "R. Giryes",
            "Carsten Marr"
        ],
        "citations": 35,
        "references": 33,
        "year": 2022
    },
    {
        "title": "Neural Diffusion Processes",
        "abstract": "Neural network approaches for meta-learning distributions over functions have desirable properties such as increased flexibility and a reduced complexity of inference. Building on the successes of denoising diffusion models for generative modelling, we propose Neural Diffusion Processes (NDPs), a novel approach that learns to sample from a rich distribution over functions through its finite marginals. By introducing a custom attention block we are able to incorporate properties of stochastic processes, such as exchangeability, directly into the NDP's architecture. We empirically show that NDPs can capture functional distributions close to the true Bayesian posterior, demonstrating that they can successfully emulate the behaviour of Gaussian processes and surpass the performance of neural processes. NDPs enable a variety of downstream tasks, including regression, implicit hyperparameter marginalisation, non-Gaussian posterior prediction and global optimisation.",
        "authors": [
            "Vincent Dutordoir",
            "Alan D. Saul",
            "Z. Ghahramani",
            "F. Simpson"
        ],
        "citations": 34,
        "references": 41,
        "year": 2022
    },
    {
        "title": "Diffusion Visual Counterfactual Explanations",
        "abstract": "Visual Counterfactual Explanations (VCEs) are an important tool to understand the decisions of an image classifier. They are 'small' but 'realistic' semantic changes of the image changing the classifier decision. Current approaches for the generation of VCEs are restricted to adversarially robust models and often contain non-realistic artefacts, or are limited to image classification problems with few classes. In this paper, we overcome this by generating Diffusion Visual Counterfactual Explanations (DVCEs) for arbitrary ImageNet classifiers via a diffusion process. Two modifications to the diffusion process are key for our DVCEs: first, an adaptive parameterization, whose hyperparameters generalize across images and models, together with distance regularization and late start of the diffusion process, allow us to generate images with minimal semantic changes to the original ones but different classification. Second, our cone regularization via an adversarially robust model ensures that the diffusion process does not converge to trivial non-semantic changes, but instead produces realistic images of the target class which achieve high confidence by the classifier.",
        "authors": [
            "Maximilian Augustin",
            "Valentyn Boreiko",
            "Francesco Croce",
            "Matthias Hein"
        ],
        "citations": 56,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Your ViT is Secretly a Hybrid Discriminative-Generative Diffusion Model",
        "abstract": "Diffusion Denoising Probability Models (DDPM) and Vision Transformer (ViT) have demonstrated significant progress in generative tasks and discriminative tasks, respectively, and thus far these models have largely been developed in their own domains. In this paper, we establish a direct connection between DDPM and ViT by integrating the ViT architecture into DDPM, and introduce a new generative model called Generative ViT (GenViT). The modeling flexibility of ViT enables us to further extend GenViT to hybrid discriminative-generative modeling, and introduce a Hybrid ViT (HybViT). Our work is among the first to explore a single ViT for image generation and classification jointly. We conduct a series of experiments to analyze the performance of proposed models and demonstrate their superiority over prior state-of-the-arts in both generative and discriminative tasks. Our code and pre-trained models can be found in https://github.com/sndnyang/Diffusion_ViT .",
        "authors": [
            "Xiulong Yang",
            "Sheng-Min Shih",
            "Yinlin Fu",
            "Xiaoting Zhao",
            "Shihao Ji"
        ],
        "citations": 50,
        "references": 68,
        "year": 2022
    },
    {
        "title": "DiffusER: Discrete Diffusion via Edit-based Reconstruction",
        "abstract": "In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DiffusER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models -- a class of models that use a Markov chain of denoising steps to incrementally generate data. DiffusER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DiffusER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps.",
        "authors": [
            "Machel Reid",
            "Vincent J. Hellendoorn",
            "Graham Neubig"
        ],
        "citations": 35,
        "references": 34,
        "year": 2022
    },
    {
        "title": "An Invertible Graph Diffusion Neural Network for Source Localization",
        "abstract": "Localizing the source of graph diffusion phenomena, such as misinformation propagation, is an important yet extremely challenging task in the real world. Existing source localization models typically are heavily dependent on the hand-crafted rules and only tailored for certain domain-specific applications. Unfortunately, a large portion of the graph diffusion process for many applications is still unknown to human beings so it is important to have expressive models for learning such underlying rules automatically. Recently, there is a surge of research body on expressive models such as Graph Neural Networks (GNNs) for automatically learning the underlying graph diffusion. However, source localization is instead the inverse of graph diffusion, which is a typical inverse problem in graphs that is well-known to be ill-posed because there can be multiple solutions and hence different from the traditional (semi-)supervised learning settings. This paper aims to establish a generic framework of invertible graph diffusion models for source localization on graphs, namely Invertible Validity-aware Graph Diffusion (IVGD), to handle major challenges including 1) Difficulty to leverage knowledge in graph diffusion models for modeling their inverse processes in an end-to-end fashion, 2) Difficulty to ensure the validity of the inferred sources, and 3) Efficiency and scalability in source inference. Specifically, first, to inversely infer sources of graph diffusion, we propose a graph residual scenario to make existing graph diffusion models invertible with theoretical guarantees; second, we develop a novel error compensation mechanism that learns to offset the errors of the inferred sources. Finally, to ensure the validity of the inferred sources, a new set of validity-aware layers have been devised to project inferred sources to feasible regions by flexibly encoding constraints with unrolled optimization techniques. A linearization technique is proposed to strengthen the efficiency of our proposed layers. The convergence of the proposed IVGD is proven theoretically. Extensive experiments on nine real-world datasets demonstrate that our proposed IVGD outperforms state-of-the-art comparison methods significantly. We have released our code at https://github.com/xianggebenben/IVGD.",
        "authors": [
            "Junxiang Wang",
            "Junji Jiang",
            "Liang Zhao"
        ],
        "citations": 28,
        "references": 74,
        "year": 2022
    },
    {
        "title": "Diff-Font: Diffusion Model for Robust One-Shot Font Generation",
        "abstract": "Font generation is a difficult and time-consuming task, especially in those languages using ideograms that have complicated structures with a large number of characters, such as Chinese. To solve this problem, few-shot font generation and even one-shot font generation have attracted a lot of attention. However, most existing font generation methods may still suffer from (i) large cross-font gap challenge; (ii) subtle cross-font variation problem; and (iii) incorrect generation of complicated characters. In this paper, we propose a novel one-shot font generation method based on a diffusion model, named Diff-Font, which can be stably trained on large datasets. The proposed model aims to generate the entire font library by giving only one sample as the reference. Specifically, a large stroke-wise dataset is constructed, and a stroke-wise diffusion model is proposed to preserve the structure and the completion of each generated character. To our best knowledge, the proposed Diff-Font is the first work that developed diffusion models to handle the font generation task. The well-trained Diff-Font is not only robust to font gap and font variation, but also achieved promising performance on difficult character generation. Compared to previous font generation methods, our model reaches state-of-the-art performance both qualitatively and quantitatively.",
        "authors": [
            "Haibin He",
            "Xinyuan Chen",
            "Chaoyue Wang",
            "Juhua Liu",
            "Bo Du",
            "Dacheng Tao",
            "Y. Qiao"
        ],
        "citations": 27,
        "references": 61,
        "year": 2022
    },
    {
        "title": "It's Raw! Audio Generation with State-Space Models",
        "abstract": "Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling. We identify that S4 can be unstable during autoregressive generation, and provide a simple improvement to its parameterization by drawing connections to Hurwitz matrices. SaShiMi yields state-of-the-art performance for unconditional waveform generation in the autoregressive setting. Additionally, SaShiMi improves non-autoregressive generation performance when used as the backbone architecture for a diffusion model. Compared to prior architectures in the autoregressive generation setting, SaShiMi generates piano and speech waveforms which humans find more musical and coherent respectively, e.g. 2x better mean opinion scores than WaveNet on an unconditional speech generation task. On a music generation task, SaShiMi outperforms WaveNet on density estimation and speed at both training and inference even when using 3x fewer parameters. Code can be found at https://github.com/HazyResearch/state-spaces and samples at https://hazyresearch.stanford.edu/sashimi-examples.",
        "authors": [
            "Karan Goel",
            "Albert Gu",
            "Chris Donahue",
            "Christopher R'e"
        ],
        "citations": 160,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Back to the Source: Diffusion-Driven Test-Time Adaptation",
        "abstract": "Test-time adaptation harnesses test inputs to improve the accuracy of a model trained on source data when tested on shifted target data. Existing methods up-date the source model by (re-)training on each target domain. While effective, re-training is sensitive to the amount and order of the data and the hyperparameters for optimization. We instead update the target data , by projecting all test inputs toward the source domain with a generative diffusion model. Our diffusion-driven adaptation method, DDA, shares its models for classiﬁcation and generation across all domains. Both models are trained on the source domain, then ﬁxed during testing. We augment diffusion with image guidance and self-ensembling to automatically decide how much to adapt. Input adaptation by DDA is more robust than prior model adaptation approaches across a variety of corruptions, architectures, and data regimes on the ImageNet-C benchmark. With its input-wise updates, DDA succeeds where model adaptation degrades on too little data in small batches, dependent data in non-uniform order, or mixed data with multiple corruptions.",
        "authors": [
            "Jin Gao",
            "Jialing Zhang",
            "Xihui Liu",
            "Trevor Darrell",
            "Evan Shelhamer",
            "Dequan Wang"
        ],
        "citations": 44,
        "references": 54,
        "year": 2022
    },
    {
        "title": "DiffFace: Diffusion-based Face Swapping with Facial Guidance",
        "abstract": "In this paper, we propose a diffusion-based face swapping framework for the first time, called DiffFace, composed of training ID conditional DDPM, sampling with facial guidance, and a target-preserving blending. In specific, in the training process, the ID conditional DDPM is trained to generate face images with the desired identity. In the sampling process, we use the off-the-shelf facial expert models to make the model transfer source identity while preserving target attributes faithfully. During this process, to preserve the background of the target image and obtain the desired face swapping result, we additionally propose a target-preserving blending strategy. It helps our model to keep the attributes of the target face from noise while transferring the source facial identity. In addition, without any re-training, our model can flexibly apply additional facial guidance and adaptively control the ID-attributes trade-off to achieve the desired results. To the best of our knowledge, this is the first approach that applies the diffusion model in face swapping task. Compared with previous GAN-based approaches, by taking advantage of the diffusion model for the face swapping task, DiffFace achieves better benefits such as training stability, high fidelity, diversity of the samples, and controllability. Extensive experiments show that our DiffFace is comparable or superior to the state-of-the-art methods on several standard face swapping benchmarks.",
        "authors": [
            "Kihong Kim",
            "Yunho Kim",
            "Seokju Cho",
            "Junyoung Seo",
            "Jisu Nam",
            "Kychul Lee",
            "Seung Wook Kim",
            "Kwanghee Lee"
        ],
        "citations": 38,
        "references": 58,
        "year": 2022
    },
    {
        "title": "Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation",
        "abstract": "Diffusion probabilistic models (DPMs) have become a popular approach to conditional generation, due to their promising results and support for cross-modal synthesis. A key desideratum in conditional synthesis is to achieve high correspondence between the conditioning input and generated output. Most existing methods learn such relationships implicitly, by incorporating the prior into the variational lower bound. In this work, we take a different route -- we explicitly enhance input-output connections by maximizing their mutual information. To this end, we introduce a Conditional Discrete Contrastive Diffusion (CDCD) loss and design two contrastive diffusion mechanisms to effectively incorporate it into the denoising process, combining the diffusion training and contrastive learning for the first time by connecting it with the conventional variational objectives. We demonstrate the efficacy of our approach in evaluations with diverse multimodal conditional synthesis tasks: dance-to-music generation, text-to-image synthesis, as well as class-conditioned image synthesis. On each, we enhance the input-output correspondence and achieve higher or competitive general synthesis quality. Furthermore, the proposed approach improves the convergence of diffusion models, reducing the number of required diffusion steps by more than 35% on two benchmarks, significantly increasing the inference speed.",
        "authors": [
            "Ye Zhu",
            "Yuehua Wu",
            "Kyle Olszewski",
            "Jian Ren",
            "S. Tulyakov",
            "Yan Yan"
        ],
        "citations": 35,
        "references": 110,
        "year": 2022
    },
    {
        "title": "A Continuous Time Framework for Discrete Denoising Models",
        "abstract": "We provide the first complete continuous time framework for denoising diffusion models of discrete data. This is achieved by formulating the forward noising process and corresponding reverse time generative process as Continuous Time Markov Chains (CTMCs). The model can be efficiently trained using a continuous time version of the ELBO. We simulate the high dimensional CTMC using techniques developed in chemical physics and exploit our continuous time framework to derive high performance samplers that we show can outperform discrete time methods for discrete data. The continuous time treatment also enables us to derive a novel theoretical result bounding the error between the generated sample distribution and the true data distribution.",
        "authors": [
            "Andrew Campbell",
            "Joe Benton",
            "Valentin De Bortoli",
            "Tom Rainforth",
            "George Deligiannidis",
            "A. Doucet"
        ],
        "citations": 93,
        "references": 47,
        "year": 2022
    },
    {
        "title": "Diffusion Model with Detail Complement for Super-Resolution of Remote Sensing",
        "abstract": "Remote sensing super-resolution (RSSR) aims to improve remote sensing (RS) image resolution while providing finer spatial details, which is of great significance for high-quality RS image interpretation. The traditional RSSR is based on the optimization method, which pays insufficient attention to small targets and lacks the ability of model understanding and detail supplement. To alleviate the above problems, we propose the generative Diffusion Model with Detail Complement (DMDC) for RS super-resolution. Firstly, unlike traditional optimization models with insufficient image understanding, we introduce the diffusion model as a generation model into RSSR tasks and regard low-resolution images as condition information to guide image generation. Next, considering that generative models may not be able to accurately recover specific small objects and complex scenes, we propose the detail supplement task to improve the recovery ability of DMDC. Finally, the strong diversity of the diffusion model makes it possibly inappropriate in RSSR, for this purpose, we come up with joint pixel constraint loss and denoise loss to optimize the direction of inverse diffusion. The extensive qualitative and quantitative experiments demonstrate the superiority of our method in RSSR with small and dense targets. Moreover, the results from direct transfer to different datasets also prove the superior generalization ability of DMDC.",
        "authors": [
            "Jinzhe Liu",
            "Zhiqiang Yuan",
            "Zhaoying Pan",
            "Yiqun Fu",
            "Li Liu",
            "Bin Lu"
        ],
        "citations": 41,
        "references": 47,
        "year": 2022
    },
    {
        "title": "Multi-instrument Music Synthesis with Spectrogram Diffusion",
        "abstract": "An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fr\\'echet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.",
        "authors": [
            "Curtis Hawthorne",
            "Ian Simon",
            "Adam Roberts",
            "Neil Zeghidour",
            "Josh Gardner",
            "Ethan Manilow",
            "Jesse Engel"
        ],
        "citations": 43,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain",
        "abstract": "Score-based generative models (SGMs) have recently shown impressive results for difficult generative tasks such as the unconditional and conditional generation of natural images and audio signals. In this work, we extend these models to the complex short-time Fourier transform (STFT) domain, proposing a novel training task for speech enhancement using a complex-valued deep neural network. We derive this training task within the formalism of stochastic differential equations (SDEs), thereby enabling the use of predictor-corrector samplers. We provide alternative formulations inspired by previous publications on using generative diffusion models for speech enhancement, avoiding the need for any prior assumptions on the noise distribution and making the training task purely generative which, as we show, results in improved enhancement performance.",
        "authors": [
            "Simon Welker",
            "Julius Richter",
            "Timo Gerkmann"
        ],
        "citations": 87,
        "references": 47,
        "year": 2022
    },
    {
        "title": "DiffMD: A Geometric Diffusion Model for Molecular Dynamics Simulations",
        "abstract": "Molecular dynamics (MD) has long been the de facto choice for simulating complex atomistic systems from first principles. Recently deep learning models become a popular way to accelerate MD. Notwithstanding, existing models depend on intermediate variables such as the potential energy or force fields to update atomic positions, which requires additional computations to perform back-propagation. To waive this requirement, we propose a novel model called DiffMD by directly estimating the gradient of the log density of molecular conformations. DiffMD relies on a score-based denoising diffusion generative model that perturbs the molecular structure with a conditional noise depending on atomic accelerations and treats conformations at previous timeframes as the prior distribution for sampling. Another challenge of modeling such a conformation generation process is that a molecule is kinetic instead of static, which no prior works have strictly studied. To solve this challenge, we propose an equivariant geometric Transformer as the score function in the diffusion process to calculate corresponding gradients. It incorporates the directions and velocities of atomic motions via 3D spherical Fourier-Bessel representations. With multiple architectural improvements, we outperform state-of-the-art baselines on MD17 and isomers of C7O2H10 datasets. This work contributes to accelerating material and drug discovery.",
        "authors": [
            "Fang Wu",
            "Stan Z. Li"
        ],
        "citations": 26,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Diffusion Coefficient of a Brownian Particle in Equilibrium and Nonequilibrium: Einstein Model and Beyond",
        "abstract": "The diffusion of small particles is omnipresent in many processes occurring in nature. As such, it is widely studied and exerted in almost all branches of sciences. It constitutes such a broad and often rather complex subject of exploration that we opt here to narrow our survey to the case of the diffusion coefficient for a Brownian particle that can be modeled in the framework of Langevin dynamics. Our main focus centers on the temperature dependence of the diffusion coefficient for several fundamental models of diverse physical systems. Starting out with diffusion in equilibrium for which the Einstein theory holds, we consider a number of physical situations outside of free Brownian motion and end by surveying nonequilibrium diffusion for a time-periodically driven Brownian particle dwelling randomly in a periodic potential. For this latter situation the diffusion coefficient exhibits an intriguingly non-monotonic dependence on temperature.",
        "authors": [
            "J. Spiechowicz",
            "I. Marchenko",
            "P. Hänggi",
            "J. Luczka"
        ],
        "citations": 25,
        "references": 165,
        "year": 2022
    },
    {
        "title": "Improving Diffusion Model Efficiency Through Patching",
        "abstract": "Diffusion models are a powerful class of generative models that iteratively denoise samples to produce data. While many works have focused on the number of iterations in this sampling procedure, few have focused on the cost of each iteration. We find that adding a simple ViT-style patching transformation can considerably reduce a diffusion model's sampling time and memory usage. We justify our approach both through an analysis of the diffusion model objective, and through empirical experiments on LSUN Church, ImageNet 256, and FFHQ 1024. We provide implementations in Tensorflow and Pytorch.",
        "authors": [
            "Troy Luhman",
            "Eric Luhman"
        ],
        "citations": 17,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Improving Diffusion Model Efficiency Through Patching",
        "abstract": "Diffusion models are a powerful class of generative models that iteratively denoise samples to produce data. While many works have focused on the number of iterations in this sampling procedure, few have focused on the cost of each iteration. We find that adding a simple ViT-style patching transformation can considerably reduce a diffusion model's sampling time and memory usage. We justify our approach both through an analysis of the diffusion model objective, and through empirical experiments on LSUN Church, ImageNet 256, and FFHQ 1024. We provide implementations in Tensorflow and Pytorch.",
        "authors": [
            "Troy Luhman",
            "Eric Luhman"
        ],
        "citations": 17,
        "references": 40,
        "year": 2022
    },
    {
        "title": "Adsorption Kinetics and Isotherm Models: A Review",
        "abstract": "Adsorption Kinetics describes the rate at which solute is adsorbed and the resident time of the adsorbates on the solid-liquid interface. Adsorption isotherms play important role in determining the interaction between adsorbate and adsorbent and the optimum adsorption capacity of adsorbent. This article considered selected adsorption kinetics and isotherms models. Pseudo first order, Pseudo second order, Elovich, Bhattacharya and Venkobachar, and Natarajan and Khalaf were adsorption kinetics reviewed on the assumption that the process behaves as heterogeneous reaction at solid-liquid interface. Adsorption kinetics equation presented takes the form of straight line, the slopes and intercepts of the plots are used to determine adsorption capacity of adsorbent, rate constant, rate of adsorption and intraparticle diffusion. Value of correlation coefficient obtained is used in determining the adsorption kinetics model that best describe the adsorption process. Langmuir, Freundlich, Radlich-Peterson, Temkin and Dubinin-Radushkevic adsorption isotherms were presented. Their slopes and intercepts provide insight on adsorption affinity, mean free energy, whether the adsorption is physisorption or chemisorptions, single or multilayer. Adsorption kinetics and isotherms reviewed provide essential information required for understanding adsorption process. \nKeywords:   Adsorption; kinetics; isotherms; pseudo first order; Langmuir",
        "authors": [
            "M. Musah",
            "Y. Azeh",
            "J. Mathew",
            "Musa Umar",
            "Z. Abdulhamid",
            "Aishetu I. Muhammad"
        ],
        "citations": 110,
        "references": 27,
        "year": 2022
    },
    {
        "title": "Score-based Generative Models for Calorimeter Shower Simulation",
        "abstract": "Score-based generative models are a new class of generative algorithms that have been shown to produce realistic images even in high dimensional spaces, currently surpassing other state-of-the-art models for different benchmark categories and applications. In this work we introduce CaloScore, a score-based generative model for collider physics applied to calorimeter shower generation. Three different diffusion models are investigated using the Fast Calorimeter Simulation Challenge 2022 dataset. CaloScore is the first application of a score-based generative model in collider physics and is able to produce high-fidelity calorimeter images for all datasets, providing an alternative paradigm for calorimeter shower simulation.",
        "authors": [
            "V. Mikuni",
            "B. Nachman"
        ],
        "citations": 71,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains",
        "abstract": "Multi-modal foundation models are typically trained on millions of pairs of natural images and text captions, frequently obtained through web-crawling approaches. Although such models depict excellent generative capabilities, they do not typically generalize well to specific domains such as medical images that have fundamentally shifted distributions compared to natural images. Building generative models for medical images that faithfully depict clinical context may help alleviate the paucity of healthcare datasets. Thus, in this study, we seek to research and expand the representational capabilities of large pretrained foundation models to medical concepts, specifically for leveraging the Stable Diffusion model to generate domain specific images found in medical imaging. We explore the sub-components of the Stable Diffusion pipeline (the variational autoencoder, the U-Net and the text-encoder) to fine-tune the model to generate medical images. We benchmark the efficacy of these efforts using quantitative image quality metrics and qualitative radiologist-driven evaluations that accurately represent the clinical content of conditional text prompts. Our best-performing model improves upon the stable diffusion baseline and can be conditioned to insert a realistic-looking abnormality on a synthetic radiology image, while maintaining a 95% accuracy on a classifier trained to detect the abnormality.",
        "authors": [
            "P. Chambon",
            "Christian Blüthgen",
            "C. Langlotz",
            "Akshay Chaudhari"
        ],
        "citations": 94,
        "references": 21,
        "year": 2022
    },
    {
        "title": "DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models",
        "abstract": "Text-to-image generation models that generate images based on prompt descriptions have attracted an increasing amount of attention during the past few months. Despite their encouraging performance, these models raise concerns about the misuse of their generated fake images. To tackle this problem, we pioneer a systematic study on the detection and attribution of fake images generated by text-to-image generation models. Concretely, we first build a machine learning classifier to detect the fake images generated by various text-to-image generation models. We then attribute these fake images to their source models, such that model owners can be held responsible for their models' misuse. We further investigate how prompts that generate fake images affect detection and attribution. We conduct extensive experiments on four popular text-to-image generation models, including DALL·E 2, Stable Diffusion, GLIDE, and Latent Diffusion, and two benchmark prompt-image datasets. Empirical results show that (1) fake images generated by various models can be distinguished from real ones, as there exists a common artifact shared by fake images from different models; (2) fake images can be effectively attributed to their source models, as different models leave unique fingerprints in their generated images; (3) prompts with the \"person'' topic or a length between 25 and 75 enable models to generate fake images with higher authenticity. All findings contribute to the community's insight into the threats caused by text-to-image generation models. We appeal to the community's consideration of the counterpart solutions, like ours, against the rapidly-evolving fake image generation.",
        "authors": [
            "Zeyang Sha",
            "Zheng Li",
            "Ning Yu",
            "Yang Zhang"
        ],
        "citations": 87,
        "references": 50,
        "year": 2022
    },
    {
        "title": "Propagation of chaos: A review of models, methods and applications. I. Models and methods",
        "abstract": "The notion of propagation of chaos for large systems of interacting particles originates in statistical physics and has recently become a central notion in many areas of applied mathematics. The present review describes old and new methods as well as several important results in the field. The models considered include the McKean-Vlasov diffusion, the mean-field jump models and the Boltzmann models. The first part of this review is an introduction to modelling aspects of stochastic particle systems and to the notion of propagation of chaos. The second part presents concrete applications and a more detailed study of some of the important models in the field.",
        "authors": [
            "Louis-Pierre Chaintron",
            "A. Diez"
        ],
        "citations": 84,
        "references": 81,
        "year": 2022
    },
    {
        "title": "A two-phase model that unifies and extends the classical models of membrane transport",
        "abstract": "Two models describe solvent transport through swollen, nonporous membranes. The pore-flow model, based on fluid mechanics, works for porous membranes, whereas the solution-diffusion model invokes molecular diffusion to treat nonporous membranes. Both approaches make valid arguments for swollen polymer membranes, but they disagree in their predictions of intramembrane pressure and concentration profiles. Using a fluid-solid model that treats the solvent and membrane matrix as separate phases, we show both classical models to be valid, to represent complementary approaches to the same phenomenon, and to make identical predictions. The fluid-solid model clarifies recent reverse osmosis measurements; provides a predictive and mechanistic basis for empirical high-pressure limiting flux phenomena, in quantitative agreement with classic measurements; and gives a framework to treat nonporous but mechanically heterogeneous membrane materials. Description Unite and expand for better models Transport through swollen polymer membranes has traditionally been described by either a pore flow or a solution-diffusion model. The former works best for porous membranes, in which transport is controlled by size exclusion and electrostatic effects, whereas the latter works best for nonporous membranes, in which transport is driven by solubility and diffusivity. Hegde et al. show that is possible to unite these two classical models using a fluid-solid model that treats the solvent and the matrix as separate phases (see the Perspective by Geise). This approach captures the predictions of the earlier models and allows for the determination of both the intramembrane pressure and the concentration profiles. It also allows for the modeling of mechanically heterogeneous membranes, and thus may be a useful tool in membrane design. —MSL A fluid-solid model for flow in polymer membranes expands upon and captures the predictions of earlier models.",
        "authors": [
            "Varun H. Hegde",
            "M. Doherty",
            "T. Squires"
        ],
        "citations": 35,
        "references": 46,
        "year": 2022
    },
    {
        "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
        "abstract": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
        "authors": [
            "A. Ramesh",
            "Prafulla Dhariwal",
            "Alex Nichol",
            "Casey Chu",
            "Mark Chen"
        ],
        "citations": 1000,
        "references": 66,
        "year": 2022
    },
    {
        "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
        "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.",
        "authors": [
            "Patrick Esser",
            "Sumith Kulal",
            "A. Blattmann",
            "Rahim Entezari",
            "Jonas Muller",
            "Harry Saini",
            "Yam Levi",
            "Dominik Lorenz",
            "Axel Sauer",
            "Frederic Boesel",
            "Dustin Podell",
            "Tim Dockhorn",
            "Zion English",
            "Kyle Lacey",
            "Alex Goodwin",
            "Yannik Marek",
            "Robin Rombach"
        ],
        "citations": 475,
        "references": 75,
        "year": 2024
    },
    {
        "title": "Zero-1-to-3: Zero-shot One Image to 3D Object",
        "abstract": "We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.",
        "authors": [
            "Ruoshi Liu",
            "Rundi Wu",
            "Basile Van Hoorick",
            "P. Tokmakov",
            "Sergey Zakharov",
            "Carl Vondrick"
        ],
        "citations": 824,
        "references": 73,
        "year": 2023
    },
    {
        "title": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation",
        "abstract": "Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page and codes: https://ml.cs.tsinghua.edu.cn/prolificdreamer/",
        "authors": [
            "Zhengyi Wang",
            "Cheng Lu",
            "Yikai Wang",
            "Fan Bao",
            "Chongxuan Li",
            "Hang Su",
            "Jun Zhu"
        ],
        "citations": 635,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Enhancements in Immediate Speech Emotion Detection: Harnessing Prosodic and Spectral Characteristics",
        "abstract": "Speech is essential to human communication for expressing and understanding feelings. Emotional speech processing has challenges with expert data sampling, dataset organization, and computational complexity in large-scale analysis. This study aims to reduce data redundancy and high dimensionality by introducing a new speech emotion recognition system. The system employs Diffusion Map to reduce dimensionality and includes Decision Trees and K-Nearest Neighbors(KNN)ensemble classifiers. These strategies are suggested to increase voice emotion recognition accuracy. Speech emotion recognition is gaining popularity in affective computing for usage in medical, industry, and academics. This project aims to provide an efficient and robust real-time emotion identification framework. In order to identify emotions using supervised machine learning models, this work makes use of paralinguistic factors such as intensity, pitch, and MFCC. In order to classify data, experimental analysis integrates prosodic and spectral information utilizing methods like Random Forest, Multilayer Perceptron, SVM, KNN, and Gaussian Naïve Bayes. Fast training times make these machine learning models excellent for real-time applications. SVM and MLP have the highest accuracy at 70.86% and 79.52%, respectively. Comparisons to benchmarks show significant improvements over earlier models.",
        "authors": [
            "Zewar Shah",
            "Shan Zhiyong",
            "Adnan"
        ],
        "citations": 1000,
        "references": 34,
        "year": 2024
    },
    {
        "title": "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation",
        "abstract": "3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.",
        "authors": [
            "Jiaxiang Tang",
            "Zhaoxi Chen",
            "Xiaokang Chen",
            "Tengfei Wang",
            "Gang Zeng",
            "Ziwei Liu"
        ],
        "citations": 215,
        "references": 63,
        "year": 2024
    },
    {
        "title": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation",
        "abstract": "Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function (BRDF) into the text-to-3D task, and learn the surface material for photorealistic rendering of the generated surface. Our disentangled framework is more compatible with popular graphics engines, supporting relighting, editing, and physical simulation of the generated 3D assets. We conduct thorough experiments that show the advantages of our method over existing ones under different text-to-3D task settings. Project page and source codes: https://fantasia3d.github.io/.",
        "authors": [
            "Rui Chen",
            "Y. Chen",
            "Ningxin Jiao",
            "K. Jia"
        ],
        "citations": 460,
        "references": 43,
        "year": 2023
    },
    {
        "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
        "abstract": "Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.",
        "authors": [
            "Yuheng Li",
            "Haotian Liu",
            "Qingyang Wu",
            "Fangzhou Mu",
            "Jianwei Yang",
            "Jianfeng Gao",
            "Chunyuan Li",
            "Yong Jae Lee"
        ],
        "citations": 459,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Flow Matching for Generative Modeling",
        "abstract": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",
        "authors": [
            "Y. Lipman",
            "Ricky T. Q. Chen",
            "Heli Ben-Hamu",
            "Maximilian Nickel",
            "Matt Le"
        ],
        "citations": 613,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
        "abstract": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
        "authors": [
            "Huiwen Chang",
            "Han Zhang",
            "Jarred Barber",
            "AJ Maschinot",
            "José Lezama",
            "Lu Jiang",
            "Ming Yang",
            "K. Murphy",
            "W. Freeman",
            "Michael Rubinstein",
            "Yuanzhen Li",
            "Dilip Krishnan"
        ],
        "citations": 456,
        "references": 87,
        "year": 2023
    },
    {
        "title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation",
        "abstract": "We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: https://justimyhxu.github.io/projects/grm/.",
        "authors": [
            "Yinghao Xu",
            "Zifan Shi",
            "Wang Yifan",
            "Hansheng Chen",
            "Ceyuan Yang",
            "Sida Peng",
            "Yujun Shen",
            "Gordon Wetzstein"
        ],
        "citations": 86,
        "references": 111,
        "year": 2024
    },
    {
        "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization",
        "abstract": "Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.",
        "authors": [
            "Minghua Liu",
            "Chao Xu",
            "Haian Jin",
            "Ling Chen",
            "T. MukundVarma",
            "Zexiang Xu",
            "Hao Su"
        ],
        "citations": 342,
        "references": 101,
        "year": 2023
    },
    {
        "title": "SyncDreamer: Generating Multiview-consistent Images from a Single-view Image",
        "abstract": "In this paper, we present a novel diffusion model called that generates multiview-consistent images from a single-view image. Using pretrained large-scale 2D diffusion models, recent work Zero123 demonstrates the ability to generate plausible novel views from a single-view image of an object. However, maintaining consistency in geometry and colors for the generated images remains a challenge. To address this issue, we propose a synchronized multiview diffusion model that models the joint probability distribution of multiview images, enabling the generation of multiview-consistent images in a single reverse process. SyncDreamer synchronizes the intermediate states of all the generated images at every step of the reverse process through a 3D-aware feature attention mechanism that correlates the corresponding features across different views. Experiments show that SyncDreamer generates images with high consistency across different views, thus making it well-suited for various 3D generation tasks such as novel-view-synthesis, text-to-3D, and image-to-3D.",
        "authors": [
            "Yuan Liu",
            "Chu-Hsing Lin",
            "Zijiao Zeng",
            "Xiaoxiao Long",
            "Lingjie Liu",
            "Taku Komura",
            "Wenping Wang"
        ],
        "citations": 315,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Scaling up GANs for Text-to-Image Synthesis",
        "abstract": "The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL.E 2, autoregressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that naïvely increasing the capacity of the StyleGan architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.",
        "authors": [
            "Minguk Kang",
            "Jun-Yan Zhu",
            "Richard Zhang",
            "Jaesik Park",
            "Eli Shechtman",
            "Sylvain Paris",
            "Taesung Park"
        ],
        "citations": 364,
        "references": 115,
        "year": 2023
    },
    {
        "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
        "abstract": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models—a language model (GPT-3) and a text-to-image model (Stable Diffusion)—to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.",
        "authors": [
            "Tim Brooks",
            "Aleksander Holynski",
            "Alexei A. Efros"
        ],
        "citations": 1000,
        "references": 74,
        "year": 2022
    },
    {
        "title": "InstantID: Zero-shot Identity-Preserving Generation in Seconds",
        "abstract": "There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at https://github.com/InstantID/InstantID.",
        "authors": [
            "Qixun Wang",
            "Xu Bai",
            "Haofan Wang",
            "Zekui Qin",
            "Anthony Chen"
        ],
        "citations": 145,
        "references": 28,
        "year": 2024
    },
    {
        "title": "Magic3D: High-Resolution Text-to-3D Content Creation",
        "abstract": "DreamFusion [31] has recently demonstrated the utility of a pretrained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF) [23], achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2× faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.",
        "authors": [
            "Chen-Hsuan Lin",
            "Jun Gao",
            "Luming Tang",
            "Towaki Takikawa",
            "Xiaohui Zeng",
            "Xun Huang",
            "Karsten Kreis",
            "S. Fidler",
            "Ming-Yu Liu",
            "Tsung-Yi Lin"
        ],
        "citations": 940,
        "references": 56,
        "year": 2022
    },
    {
        "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation",
        "abstract": "We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. To begin with, we build ImageReward -- the first general-purpose text-to-image human preference reward model -- to effectively encode human preferences. Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis. On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer. Both automatic and human evaluation support ReFL's advantages over compared methods. All code and datasets are provided at \\url{https://github.com/THUDM/ImageReward}.",
        "authors": [
            "Jiazheng Xu",
            "Xiao Liu",
            "Yuchen Wu",
            "Yuxuan Tong",
            "Qinkai Li",
            "Ming Ding",
            "Jie Tang",
            "Yuxiao Dong"
        ],
        "citations": 196,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
        "abstract": "We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.",
        "authors": [
            "Chunting Zhou",
            "Lili Yu",
            "Arun Babu",
            "Kushal Tirumala",
            "Michihiro Yasunaga",
            "Leonid Shamis",
            "Jacob Kahn",
            "Xuezhe Ma",
            "Luke Zettlemoyer",
            "Omer Levy"
        ],
        "citations": 59,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation",
        "abstract": "Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on image animation benchmarks, achieving state-of-the-art results.",
        "authors": [
            "Liucheng Hu",
            "Xin Gao",
            "Peng Zhang",
            "Ke Sun",
            "Bang Zhang",
            "Liefeng Bo"
        ],
        "citations": 204,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
        "abstract": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine\"next-scale prediction\"or\"next-resolution prediction\", diverging from the standard raster-scan\"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.",
        "authors": [
            "Keyu Tian",
            "Yi Jiang",
            "Zehuan Yuan",
            "Bingyue Peng",
            "Liwei Wang"
        ],
        "citations": 102,
        "references": 103,
        "year": 2024
    },
    {
        "title": "De novo design of protein structure and function with RFdiffusion",
        "abstract": null,
        "authors": [
            "Joseph L. Watson",
            "David Juergens",
            "N. Bennett",
            "Brian L. Trippe",
            "Jason Yim",
            "Helen E. Eisenach",
            "Woody Ahern",
            "Andrew J. Borst",
            "Robert J. Ragotte",
            "L. Milles",
            "B. Wicky",
            "Nikita Hanikel",
            "S. Pellock",
            "A. Courbet",
            "W. Sheffler",
            "Jue Wang",
            "Preetham Venkatesh",
            "Isaac Sappington",
            "Susana Vázquez Torres",
            "Anna Lauko",
            "Valentin De Bortoli",
            "Emile Mathieu",
            "Sergey Ovchinnikov",
            "R. Barzilay",
            "T. Jaakkola",
            "F. DiMaio",
            "M. Baek",
            "D. Baker"
        ],
        "citations": 321,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
        "abstract": "While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.",
        "authors": [
            "Alex Nichol",
            "Heewoo Jun",
            "Prafulla Dhariwal",
            "Pamela Mishkin",
            "Mark Chen"
        ],
        "citations": 488,
        "references": 79,
        "year": 2022
    },
    {
        "title": "FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention",
        "abstract": "Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300$$\\times $$\n ×\n –2500$$\\times $$\n ×\n speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available here (https://github.com/mit-han-lab/fastcomposer).",
        "authors": [
            "Guangxuan Xiao",
            "Tianwei Yin",
            "W. Freeman",
            "F. Durand",
            "Song Han"
        ],
        "citations": 179,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation",
        "abstract": "Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos. Code is available at our project page: https://www.mmlab-ntu.com/project/rerender/",
        "authors": [
            "Shuai Yang",
            "Yifan Zhou",
            "Ziwei Liu",
            "Chen Change Loy"
        ],
        "citations": 169,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model",
        "abstract": "Text-to-3D with diffusion models has achieved remarkable progress in recent years. However, existing methods either rely on score distillation-based optimization which suffer from slow inference, low diversity and Janus problems, or are feed-forward methods that generate low-quality results due to the scarcity of 3D training data. In this paper, we propose Instant3D, a novel method that generates high-quality and diverse 3D assets from text prompts in a feed-forward manner. We adopt a two-stage paradigm, which first generates a sparse set of four structured and consistent views from text in one shot with a fine-tuned 2D text-to-image diffusion model, and then directly regresses the NeRF from the generated images with a novel transformer-based sparse-view reconstructor. Through extensive experiments, we demonstrate that our method can generate diverse 3D assets of high visual quality within 20 seconds, which is two orders of magnitude faster than previous optimization-based methods that can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.",
        "authors": [
            "Jiahao Li",
            "Hao Tan",
            "Kai Zhang",
            "Zexiang Xu",
            "Fujun Luan",
            "Yinghao Xu",
            "Yicong Hong",
            "Kalyan Sunkavalli",
            "Greg Shakhnarovich",
            "Sai Bi"
        ],
        "citations": 173,
        "references": 90,
        "year": 2023
    },
    {
        "title": "AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining",
        "abstract": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called “language of audio” (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 framework against previous approaches.",
        "authors": [
            "Haohe Liu",
            "Qiao Tian",
            "Yiitan Yuan",
            "Xubo Liu",
            "Xinhao Mei",
            "Qiuqiang Kong",
            "Yuping Wang",
            "Wenwu Wang",
            "Yuxuan Wang",
            "Mark D. Plumbley"
        ],
        "citations": 167,
        "references": 100,
        "year": 2023
    },
    {
        "title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery",
        "abstract": "The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical\"hard\"prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also\"soft\"prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.",
        "authors": [
            "Yuxin Wen",
            "Neel Jain",
            "John Kirchenbauer",
            "Micah Goldblum",
            "Jonas Geiping",
            "T. Goldstein"
        ],
        "citations": 191,
        "references": 39,
        "year": 2023
    },
    {
        "title": "ModelScope Text-to-Video Technical Report",
        "abstract": "This paper introduces ModelScopeT2V, a text-to-video synthesis model that evolves from a text-to-image synthesis model (i.e., Stable Diffusion). ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame generation and smooth movement transitions. The model could adapt to varying frame numbers during training and inference, rendering it suitable for both image-text and video-text datasets. ModelScopeT2V brings together three components (i.e., VQGAN, a text encoder, and a denoising UNet), totally comprising 1.7 billion parameters, in which 0.5 billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.",
        "authors": [
            "Jiuniu Wang",
            "Hangjie Yuan",
            "Dayou Chen",
            "Yingya Zhang",
            "Xiang Wang",
            "Shiwei Zhang"
        ],
        "citations": 275,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Emu3: Next-Token Prediction is All You Need",
        "abstract": "While next-token prediction is considered a promising path towards artificial general intelligence, it has struggled to excel in multimodal tasks, which are still dominated by diffusion models (e.g., Stable Diffusion) and compositional approaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction. By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences. Emu3 outperforms several well-established task-specific models in both generation and perception tasks, surpassing flagship models such as SDXL and LLaVA-1.6, while eliminating the need for diffusion or compositional architectures. Emu3 is also capable of generating high-fidelity video via predicting the next token in a video sequence. We simplify complex multimodal model designs by converging on a singular focus: tokens, unlocking great potential for scaling both during training and inference. Our results demonstrate that next-token prediction is a promising path towards building general multimodal intelligence beyond language. We open-source key techniques and models to support further research in this direction.",
        "authors": [
            "Xinlong Wang",
            "Xiaosong Zhang",
            "Zhengxiong Luo",
            "Quan Sun",
            "Yufeng Cui",
            "Jinsheng Wang",
            "Fan Zhang",
            "Yueze Wang",
            "Zhen Li",
            "Qiying Yu",
            "Yingli Zhao",
            "Yulong Ao",
            "Xuebin Min",
            "Tao Li",
            "Boya Wu",
            "Bo Zhao",
            "Bowen Zhang",
            "Lian-zi Wang",
            "Guang Liu",
            "Zheqi He",
            "Xi Yang",
            "Jingjing Liu",
            "Yonghua Lin",
            "Tiejun Huang",
            "Zhongyuan Wang"
        ],
        "citations": 44,
        "references": 102,
        "year": 2024
    },
    {
        "title": "Art and the science of generative AI",
        "abstract": "Understanding shifts in creative work will help guide AI’s impact on the media ecosystem The capabilities of a new class of tools, colloquially known as generative artificial intelligence (AI), is a topic of much debate. One prominent application thus far is the production of high-quality artistic media for visual arts, concept art, music, and literature, as well as video and animation. For example, diffusion models can synthesize high-quality images (1), and large language models (LLMs) can produce sensible-sounding and impressive prose and verse in a wide range of contexts (2). The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI—and making policy decisions around it—requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity.",
        "authors": [
            "Ziv Epstein",
            "Aaron Hertzmann",
            "L. Herman",
            "Robert Mahari",
            "M. Frank",
            "Matthew Groh",
            "Hope Schroeder",
            "Amy Smith",
            "Memo Akten",
            "Jessica Fjeld",
            "H. Farid",
            "Neil Leach",
            "A. Pentland",
            "Olga Russakovsky"
        ],
        "citations": 217,
        "references": 140,
        "year": 2023
    },
    {
        "title": "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
        "abstract": "Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed.",
        "authors": [
            "Axel Sauer",
            "Tero Karras",
            "S. Laine",
            "Andreas Geiger",
            "Timo Aila"
        ],
        "citations": 178,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Zero-shot Image-to-Image Translation",
        "abstract": "Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse, high-quality images. However, directly applying these models for real image editing remains challenging for two reasons. First, it is hard for users to craft a perfect text prompt depicting every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. In this work, we introduce pix2pix-zero, an image-to-image translation method that can preserve the original image’s content without manual prompting. We first automatically discover editing directions that reflect desired edits in the text embedding space. To preserve the content structure, we propose cross-attention guidance, which aims to retain the cross-attention maps of the input image throughout the diffusion process. Finally, to enable interactive editing, we distill the diffusion model into a fast conditional GAN. We conduct extensive experiments and show that our method outperforms existing and concurrent works for both real and synthetic image editing. In addition, our method does not need additional training for these edits and can directly use the existing pre-trained text-to-image diffusion model.",
        "authors": [
            "Gaurav Parmar",
            "Krishna Kumar Singh",
            "Richard Zhang",
            "Yijun Li",
            "Jingwan Lu",
            "Jun-Yan Zhu"
        ],
        "citations": 353,
        "references": 77,
        "year": 2023
    },
    {
        "title": "Autoregressive Image Generation without Vector Quantization",
        "abstract": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
        "authors": [
            "Tianhong Li",
            "Yonglong Tian",
            "He Li",
            "Mingyang Deng",
            "Kaiming He"
        ],
        "citations": 62,
        "references": 55,
        "year": 2024
    },
    {
        "title": "Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures",
        "abstract": "Text-guided image generation has progressed rapidly in recent years, inspiring major breakthroughs in text-guided shape generation. Recently, it has been shown that using score distillation, one can successfully text-guide a NeRF model to generate a 3D object. We adapt the score distillation to the publicly available, and computationally efficient, Latent Diffusion Models, which apply the entire diffusion process in a compact latent space of a pretrained autoencoder. As NeRFs operate in image space, a naive solution for guiding them with latent score distillation would require encoding to the latent space at each guidance step. Instead, we propose to bring the NeRF to the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we show that while Text-to-3D models can generate impressive results, they are inherently unconstrained and may lack the ability to guide or enforce a specific 3D structure. To assist and direct the 3D generation, we propose to guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines the coarse structure of the desired object. Then, we present means to integrate such a constraint directly into a Latent-NeRF. This unique combination of text and shape guidance allows for increased control over the generation process. We also show that latent score distillation can be successfully applied directly on 3D meshes. This allows for generating high-quality textures on a given geometry. Our experiments validate the power of our different forms of guidance and the efficiency of using latent rendering.",
        "authors": [
            "G. Metzer",
            "Elad Richardson",
            "Or Patashnik",
            "R. Giryes",
            "D. Cohen-Or"
        ],
        "citations": 384,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Shap-E: Generating Conditional 3D Implicit Functions",
        "abstract": "We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.",
        "authors": [
            "Heewoo Jun",
            "Alex Nichol"
        ],
        "citations": 252,
        "references": 74,
        "year": 2023
    },
    {
        "title": "TEXTure: Text-Guided Texturing of 3D Shapes",
        "abstract": "In this paper, we present TEXTure, a novel method for text-guided generation, editing, and transfer of textures for 3D shapes. Leveraging a pretrained depth-to-image diffusion model, TEXTure applies an iterative scheme that paints a 3D model from different viewpoints. Yet, while depth-to-image models can create plausible textures from a single viewpoint, the stochastic nature of the generation process can cause many inconsistencies when texturing an entire 3D object. To tackle these problems, we dynamically define a trimap partitioning of the rendered image into three progression states, and present a novel elaborated diffusion sampling process that uses this trimap representation to generate seamless textures from different views. We then show that one can transfer the generated texture maps to new 3D geometries without requiring explicit surface-to-surface mapping, as well as extract semantic textures from a set of images without requiring any explicit reconstruction. Finally, we show that TEXTure can be used to not only generate new textures but also edit and refine existing textures using either a text prompt or user-provided scribbles. We demonstrate that our TEXTuring method excels at generating, transferring, and editing textures through extensive evaluation, and further close the gap between 2D image generation and 3D texturing. Code is available via our project page: https://texturepaper.github.io/TEXTurePaper/.",
        "authors": [
            "Elad Richardson",
            "G. Metzer",
            "Yuval Alaluf",
            "R. Giryes",
            "D. Cohen-Or"
        ],
        "citations": 222,
        "references": 55,
        "year": 2023
    },
    {
        "title": "I2SB: Image-to-Image Schrödinger Bridge",
        "abstract": "We propose Image-to-Image Schr\\\"odinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schr\\\"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. Moreover, I$^2$SB matches the performance of inverse methods that additionally require the knowledge of the corruption operators. Our work opens up new algorithmic opportunities for developing efficient nonlinear diffusion models on a large scale. scale. Project page and codes: https://i2sb.github.io/",
        "authors": [
            "Guan-Horng Liu",
            "Arash Vahdat",
            "De-An Huang",
            "Evangelos A. Theodorou",
            "Weili Nie",
            "Anima Anandkumar"
        ],
        "citations": 111,
        "references": 78,
        "year": 2023
    },
    {
        "title": "TextMesh: Generation of Realistic 3D Meshes From Text Prompts",
        "abstract": "The ability to generate highly realistic 2D images from mere text prompts has recently made huge progress in terms of speed and quality, thanks to the advent of image diffusion models. Naturally, the question arises if this can be also achieved in the generation of 3D content from such text prompts. To this end, a new line of methods recently emerged trying to harness diffusion models, trained on 2D images, for supervision of 3D model generation using view dependent prompts. While achieving impressive results, these methods, however, have two major drawbacks. First, rather than commonly used 3D meshes, they instead generate neural radiance fields (NeRFs), making them impractical for most real applications. Second, these approaches tend to produce over-saturated models, giving the output a cartoonish looking effect. Therefore, in this work we propose a novel method for generation of highly realistic-looking 3D meshes. To this end, we extend NeRF to employ an SDF backbone, leading to improved 3D mesh extraction. In addition, we propose a novel way to finetune the mesh texture, removing the effect of high saturation and improving the details of the output 3D mesh.",
        "authors": [
            "Christina Tsalicoglou",
            "Fabian Manhardt",
            "A. Tonioni",
            "Michael Niemeyer",
            "F. Tombari"
        ],
        "citations": 116,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Scaling Robot Learning with Semantically Imagined Experience",
        "abstract": "Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. One of the key contributing factors to this progress is the scale of robot data used to train the models. To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which are challenging to scale. To mitigate this issue, we propose an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. We term our method Robot Learning with Semantically Imagened Experience (ROSIE). Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting various unseen objects for manipulation, backgrounds, and distractors with text guidance. Through extensive real-world experiments, we show that manipulation policies trained on data augmented this way are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation. The project's website and videos can be found at diffusion-rosie.github.io",
        "authors": [
            "Tianhe Yu",
            "Ted Xiao",
            "Austin Stone",
            "Jonathan Tompson",
            "Anthony Brohan",
            "Su Wang",
            "Jaspiar Singh",
            "Clayton Tan",
            "M. Dee",
            "Jodilyn Peralta",
            "Brian Ichter",
            "Karol Hausman",
            "F. Xia"
        ],
        "citations": 117,
        "references": 82,
        "year": 2023
    },
    {
        "title": "Is Conditional Generative Modeling all you need for Decision-Making?",
        "abstract": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.",
        "authors": [
            "Anurag Ajay",
            "Yilun Du",
            "Abhi Gupta",
            "J. Tenenbaum",
            "T. Jaakkola",
            "Pulkit Agrawal"
        ],
        "citations": 281,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Raising the Cost of Malicious AI-Powered Image Editing",
        "abstract": "We present an approach to mitigating the risks of malicious image editing posed by large diffusion models. The key idea is to immunize images so as to make them resistant to manipulation by these models. This immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. We provide two methods for crafting such perturbations, and then demonstrate their efficacy. Finally, we discuss a policy component necessary to make our approach fully effective and practical -- one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process.",
        "authors": [
            "Hadi Salman",
            "Alaa Khaddaj",
            "Guillaume Leclerc",
            "Andrew Ilyas",
            "A. Madry"
        ],
        "citations": 85,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Dense Text-to-Image Generation with Attention Modulation",
        "abstract": "Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images’ layouts and the pre-trained model’s intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions. Code and data are available at https://github.com/naver-ai/DenseDiffusion.",
        "authors": [
            "Yunji Kim",
            "Jiyoung Lee",
            "Jin-Hwa Kim",
            "Jung-Woo Ha",
            "Jun-Yan Zhu"
        ],
        "citations": 93,
        "references": 60,
        "year": 2023
    },
    {
        "title": "ReNoise: Real Image Inversion Through Iterative Noising",
        "abstract": "Recent advancements in text-guided diffusion models have unlocked powerful image manipulation capabilities. However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained diffusion model. Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps. In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations. Building on reversing the diffusion sampling process, our method employs an iterative renoising mechanism at each inversion sampling step. This mechanism refines the approximation of a predicted point along the forward diffusion trajectory, by iteratively applying the pretrained diffusion model, and averaging these predictions. We evaluate the performance of our ReNoise technique using various sampling algorithms and models, including recent accelerated diffusion models. Through comprehensive evaluations and comparisons, we show its effectiveness in terms of both accuracy and speed. Furthermore, we confirm that our method preserves editability by demonstrating text-driven image editing on real images.",
        "authors": [
            "Daniel Garibi",
            "Or Patashnik",
            "Andrey Voynov",
            "Hadar Averbuch-Elor",
            "Daniel Cohen-Or"
        ],
        "citations": 31,
        "references": 49,
        "year": 2024
    },
    {
        "title": "Score-based generative modeling for de novo protein design",
        "abstract": null,
        "authors": [
            "Jin Sub Lee",
            "Jisun Kim",
            "Philip M. Kim"
        ],
        "citations": 88,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Stochastic Interpolants: A Unifying Framework for Flows and Diffusions",
        "abstract": "A class of generative models that unifies flow-based and diffusion-based methods is introduced. These models extend the framework proposed in Albergo&Vanden-Eijnden (2023), enabling the use of a broad class of continuous-time stochastic processes called `stochastic interpolants' to bridge any two arbitrary probability density functions exactly in finite time. These interpolants are built by combining data from the two prescribed densities with an additional latent variable that shapes the bridge in a flexible way. The time-dependent probability density function of the stochastic interpolant is shown to satisfy a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion. Upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on probability flow equations or stochastic differential equations with an adjustable level of noise. The drift coefficients entering these models are time-dependent velocity fields characterized as the unique minimizers of simple quadratic objective functions, one of which is a new objective for the score of the interpolant density. Remarkably, we show that minimization of these quadratic objectives leads to control of the likelihood for any of our generative models built upon stochastic dynamics. By contrast, we establish that generative models based upon a deterministic dynamics must, in addition, control the Fisher divergence between the target and the model. We also construct estimators for the likelihood and the cross-entropy of interpolant-based generative models, discuss connections with other stochastic bridges, and demonstrate that such models recover the Schr\\\"odinger bridge between the two target densities when explicitly optimizing over the interpolant.",
        "authors": [
            "M. S. Albergo",
            "Nicholas M. Boffi",
            "E. Vanden-Eijnden"
        ],
        "citations": 181,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Composer: Creative and Controllable Image Synthesis with Composable Conditions",
        "abstract": "Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.",
        "authors": [
            "Lianghua Huang",
            "Di Chen",
            "Yu Liu",
            "Yujun Shen",
            "Deli Zhao",
            "Jingren Zhou"
        ],
        "citations": 241,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Illuminating protein space with a programmable generative model",
        "abstract": null,
        "authors": [
            "John Ingraham",
            "Max Baranov",
            "Zak Costello",
            "Vincent Frappier",
            "Ahmed Ismail",
            "Shan Tie",
            "Wujie Wang",
            "Vincent Xue",
            "F. Obermeyer",
            "Andrew L. Beam",
            "G. Grigoryan"
        ],
        "citations": 273,
        "references": 126,
        "year": 2022
    },
    {
        "title": "Video-P2P: Video Editing with Cross-Attention Control",
        "abstract": "Video-P2P is the first framework for real-world video editing with cross-attention control. While attention control has proven effective for image editing with pre-trained image generation models, there are currently no large-scale video generation models publicly available. Video-P2P addresses this limitation by adapting an image generation diffusion model to complete various video editing tasks. Specifically, we propose to first tune a Text-to-Set (T2S) model to complete an approximate inversion and then optimize a shared unconditional embedding to achieve accurate video inversion with a small memory cost. We further prove that it is crucial for consistent video editing. For attention control, we introduce a novel decoupled-guidance strategy, which uses different guidance strategies for the source and target prompts. The optimized unconditional embedding for the source prompt improves reconstruction ability, while an initialized unconditional embedding for the target prompt enhances editability. Incorporating the attention maps of these two branches enables detailed editing. These technical designs enable various text-driven editing applications, including word swap, prompt refinement, and attention re-weighting. Video-P2P works well on real-world videos for generating new characters while optimally preserving their original poses and scenes. It significantly outperforms previous approaches.",
        "authors": [
            "Shaoteng Liu",
            "Yuechen Zhang",
            "Wenbo Li",
            "Zhe Lin",
            "Jiaya Jia"
        ],
        "citations": 153,
        "references": 56,
        "year": 2023
    },
    {
        "title": "GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis",
        "abstract": "Synthesizing high-fidelity complex images from text is challenging. Based on large pretraining, the autoregressive and diffusion models can synthesize photo-realistic images. Although these large models have shown notable progress, there remain three flaws. 1) These models require tremendous training data and parameters to achieve good performance. 2) The multi-step generation design slows the image synthesis process heavily. 3) The synthesized visual features are challenging to control and require delicately designed prompts. To enable high-quality, efficient, fast, and controllable text-to-image synthesis, we propose Generative Adversarial CLIPs, namely GALIP. GALIP leverages the powerful pretrained CLIP model both in the discriminator and generator. Specifically, we propose a CLIP-based discriminator. The complex scene understanding ability of CLIP enables the discriminator to accurately assess the image quality. Furthermore, we propose a CLIP-empowered generator that induces the visual concepts from CLIP through bridge features and prompts. The CLIP-integrated generator and discriminator boost training efficiency, and as a result, our model only requires about 3% training data and 6% learnable parameters, achieving comparable results to large pretrained autoregressive and diffusion models. Moreover, our model achieves ~120×faster synthesis speed and inherits the smooth latent space from GAN. The extensive experimental results demonstrate the excellent performance of our GALIP. Code is available at https://github.com/tobran/GALIP.",
        "authors": [
            "Ming Tao",
            "Bingkun Bao",
            "Hao Tang",
            "Changsheng Xu"
        ],
        "citations": 84,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Grounded Text-to-Image Synthesis with Attention Refocusing",
        "abstract": "Driven by the scalable diffusion models trained on large-scale datasets, text-to-image synthesis methods have shown compelling results. However, these models still fail to pre-cisely follow the text prompt involving multiple objects, attributes, or spatial compositions. In this paper, we reveal the potential causes in the diffusion model's cross-attention and self-attention layers. We propose two novel losses to refocus attention maps according to a given spatial layout during sampling. Creating the layouts manually requires additional effort and can be tedious. Therefore, we explore using large language models (LLM) to produce these lay-outs for our method. We conduct extensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate our proposed method. We show that our proposed attention re-focusing effectively improves the controllability of existing approaches.",
        "authors": [
            "Quynh Phung",
            "Songwei Ge",
            "Jia-Bin Huang"
        ],
        "citations": 76,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning",
        "abstract": "We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.",
        "authors": [
            "Rohit Girdhar",
            "Mannat Singh",
            "Andrew Brown",
            "Quentin Duval",
            "S. Azadi",
            "Sai Saketh Rambhatla",
            "Akbar Shah",
            "Xi Yin",
            "Devi Parikh",
            "Ishan Misra"
        ],
        "citations": 143,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Anti-DreamBooth: Protecting users from personalized text-to-image synthesis",
        "abstract": "Text-to-image diffusion models are nothing but a revolution, allowing anyone, even without design skills, to create realistic images from simple text inputs. With powerful personalization tools like DreamBooth, they can generate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual victim, posing a severe negative social impact. In this paper, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each user’s image before publishing in order to disrupt the generation quality of any DreamBooth model trained on these perturbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of Dream-Booth and Diffusion-based text-to-image models, our methods effectively defend users from the malicious use of those models. Their effectiveness withstands even adverse conditions, such as model or prompt/term mismatching between training and testing. Our code will be available at https://github.com/VinAIResearch/Anti-DreamBooth.git.",
        "authors": [
            "Van Thanh Le",
            "Hao Phung",
            "Thuan Hoang Nguyen",
            "Quan Dao",
            "Ngoc N. Tran",
            "A. Tran"
        ],
        "citations": 67,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Vox-E: Text-guided Voxel Editing of 3D Objects",
        "abstract": "Large scale text-guided diffusion models have garnered significant attention due to their ability to synthesize diverse images that convey complex visual concepts. This generative power has more recently been leveraged to perform text-to-3D synthesis. In this work, we present a technique that harnesses the power of latent diffusion models for editing existing 3D objects. Our method takes oriented 2D images of a 3D object as input and learns a grid-based volumetric representation of it. To guide the volumetric representation to conform to a target text prompt, we follow unconditional text-to-3D methods and optimize a Score Distillation Sampling (SDS) loss. However, we observe that combining this diffusion-guided loss with an image-based regularization loss that encourages the representation not to deviate too strongly from the input object is challenging, as it requires achieving two conflicting goals while viewing only structure-and-appearance coupled 2D projections. Thus, we introduce a novel volumetric regularization loss that operates directly in 3D space, utilizing the explicit nature of our 3D representation to enforce correlation between the global structure of the original and edited object. Furthermore, we present a technique that optimizes cross-attention volumetric grids to refine the spatial extent of the edits. Extensive experiments and comparisons demonstrate the effectiveness of our approach in creating a myriad of edits which cannot be achieved by prior works1.",
        "authors": [
            "Etai Sella",
            "Gal Fiebelman",
            "Peter Hedman",
            "Hadar Averbuch-Elor"
        ],
        "citations": 62,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising",
        "abstract": "Leveraging large-scale image-text datasets and advancements in diffusion models, text-driven generative models have made remarkable strides in the field of image generation and editing. This study explores the potential of extending the text-driven ability to the generation and editing of multi-text conditioned long videos. Current methodologies for video generation and editing, while innovative, are often confined to extremely short videos (typically less than 24 frames) and are limited to a single text condition. These constraints significantly limit their applications given that real-world videos usually consist of multiple segments, each bearing different semantic information. To address this challenge, we introduce a novel paradigm dubbed as Gen-L-Video, capable of extending off-the-shelf short video diffusion models for generating and editing videos comprising hundreds of frames with diverse semantic segments without introducing additional training, all while preserving content consistency. We have implemented three mainstream text-driven video generation and editing methodologies and extended them to accommodate longer videos imbued with a variety of semantic segments with our proposed paradigm. Our experimental outcomes reveal that our approach significantly broadens the generative and editing capabilities of video diffusion models, offering new possibilities for future research and applications. The code is available at https://github.com/G-U-N/Gen-L-Video.",
        "authors": [
            "Fu-Yun Wang",
            "Wenshuo Chen",
            "Guanglu Song",
            "Han-Jia Ye",
            "Yu Liu",
            "Hongsheng Li"
        ],
        "citations": 62,
        "references": 76,
        "year": 2023
    },
    {
        "title": "FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing",
        "abstract": "Text-to-video editing aims to edit the visual appearance of a source video conditional on textual prompts. A major challenge in this task is to ensure that all frames in the edited video are visually consistent. Most recent works apply advanced text-to-image diffusion models to this task by inflating 2D spatial attention in the U-Net into spatio-temporal attention. Although temporal context can be added through spatio-temporal attention, it may introduce some irrelevant information for each patch and therefore cause inconsistency in the edited video. In this paper, for the first time, we introduce optical flow into the attention module in the diffusion model's U-Net to address the inconsistency issue for text-to-video editing. Our method, FLATTEN, enforces the patches on the same flow path across different frames to attend to each other in the attention module, thus improving the visual consistency in the edited videos. Additionally, our method is training-free and can be seamlessly integrated into any diffusion-based text-to-video editing methods and improve their visual consistency. Experiment results on existing text-to-video editing benchmarks show that our proposed method achieves the new state-of-the-art performance. In particular, our method excels in maintaining the visual consistency in the edited videos.",
        "authors": [
            "Yuren Cong",
            "Mengmeng Xu",
            "Christian Simon",
            "Shoufa Chen",
            "Jiawei Ren",
            "Yanping Xie",
            "Juan-Manuel Pérez-Rúa",
            "Bodo Rosenhahn",
            "Tao Xiang",
            "Sen He"
        ],
        "citations": 51,
        "references": 59,
        "year": 2023
    },
    {
        "title": "(Certified!!) Adversarial Robustness for Free!",
        "abstract": "In this paper we show how to achieve state-of-the-art certified adversarial robustness to 2-norm bounded perturbations by relying exclusively on off-the-shelf pretrained models. To do so, we instantiate the denoised smoothing approach of Salman et al. 2020 by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This allows us to certify 71% accuracy on ImageNet under adversarial perturbations constrained to be within an 2-norm of 0.5, an improvement of 14 percentage points over the prior certified SoTA using any approach, or an improvement of 30 percentage points over denoised smoothing. We obtain these results using only pretrained diffusion models and image classifiers, without requiring any fine tuning or retraining of model parameters.",
        "authors": [
            "Nicholas Carlini",
            "Florian Tramèr",
            "K. Dvijotham",
            "J. Z. Kolter"
        ],
        "citations": 129,
        "references": 31,
        "year": 2022
    },
    {
        "title": "Freestyle Layout-to-Image Synthesis",
        "abstract": "Typical layout-to-image synthesis (LIS) models generate images for a closed set of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work, we explore the freestyle capability of the model, i.e., how far can it generate unseen semantics (e.g., classes, attributes, and styles) onto a given layout, and call the task Freestyle LIS (FLIS). Thanks to the development of large-scale pre-trained language-image models, a number of discriminative models (e.g., image classification and object detection) trained on limited base classes are empowered with the ability of unseen class prediction. Inspired by this, we opt to leverage large-scale pre-trained text-to-image diffusion models to achieve the generation of unseen semantics. The key challenge of FLIS is how to enable the diffusion model to synthesize images from a specific layout which very likely violates its pre-learned knowledge, e.g., the model never sees “a unicorn sitting on a bench” during its pre-training. To this end, we introduce a new module called Rectified Cross-Attention (RCA) that can be conveniently plugged in the diffusion model to integrate semantic masks. This “plug-in” is applied in each cross-attention layer of the model to rectify the attention maps between image and text tokens. The key idea of RCA is to enforce each text token to act on the pixels in a specified region, allowing us to freely put a wide variety of semantics from pre-trained knowledge (which is general) onto the given layout (which is specific). Extensive experiments show that the proposed diffusion network produces realistic and freestyle layout-to-image generation results with diverse text inputs, which has a high potential to spawn a bunch of interesting applications. Code is available at https://github.com/essunny310/FreestyleNet.",
        "authors": [
            "Han Xue",
            "Z. Huang",
            "Qianru Sun",
            "Li Song",
            "Wenjun Zhang"
        ],
        "citations": 51,
        "references": 65,
        "year": 2023
    },
    {
        "title": "SpaText: Spatio-Textual Representation for Controllable Image Generation",
        "abstract": "Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText — a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.",
        "authors": [
            "Omri Avrahami",
            "Thomas Hayes",
            "Oran Gafni",
            "Sonal Gupta",
            "Yaniv Taigman",
            "Devi Parikh",
            "D. Lischinski",
            "Ohad Fried",
            "Xiaoyue Yin"
        ],
        "citations": 173,
        "references": 97,
        "year": 2022
    },
    {
        "title": "MAGVIT: Masked Generative Video Transformer",
        "abstract": "We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle various video synthesis tasks with a single model. We introduce a 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning. We conduct extensive experiments to demonstrate the quality, efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT performs favorably against state-of-the-art approaches and establishes the best-published FVD on three video generation benchmarks, including the challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference time by two orders of magnitude against diffusion models and by 60x against autoregressive models. (iii) A single MAGVIT model supports ten diverse generation tasks and generalizes across videos from different visual domains. The source code and trained models will be released to the public at https://magvit.cs.cmu.edu.",
        "authors": [
            "Lijun Yu",
            "Yong Cheng",
            "Kihyuk Sohn",
            "José Lezama",
            "Han Zhang",
            "Huiwen Chang",
            "A. Hauptmann",
            "Ming-Hsuan Yang",
            "Yuan Hao",
            "Irfan Essa",
            "Lu Jiang"
        ],
        "citations": 164,
        "references": 86,
        "year": 2022
    },
    {
        "title": "FLAME: Free-form Language-based Motion Synthesis & Editing",
        "abstract": "Text-based motion generation models are drawing a surge of interest for their potential for automating the motion-making process in the game, animation, or robot industries. In this paper, we propose a diffusion-based motion synthesis and editing model named FLAME. Inspired by the recent successes in diffusion models, we integrate diffusion-based generative models into the motion domain. FLAME can generate high-fidelity motions well aligned with the given text. Also, it can edit the parts of the motion, both frame-wise and joint-wise, without any fine-tuning. FLAME involves a new transformer-based architecture we devise to better handle motion data, which is found to be crucial to manage variable-length motions and well attend to free-form text. In experiments, we show that FLAME achieves state-of-the-art generation performances on three text-motion datasets: HumanML3D, BABEL, and KIT. We also demonstrate that FLAME’s editing capability can be extended to other tasks such as motion prediction or motion in-betweening, which have been previously covered by dedicated models.",
        "authors": [
            "Jihoon Kim",
            "Jiseob Kim",
            "Sungjoon Choi"
        ],
        "citations": 165,
        "references": 47,
        "year": 2022
    },
    {
        "title": "IT3D: Improved Text-to-3D Generation with Explicit View Synthesis",
        "abstract": "Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models. For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data. We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches.",
        "authors": [
            "Yiwen Chen",
            "Chi Zhang",
            "Xiaofeng Yang",
            "Zhongang Cai",
            "Gang Yu",
            "Lei Yang",
            "Guo-Shing Lin"
        ],
        "citations": 54,
        "references": 26,
        "year": 2023
    },
    {
        "title": "NeuralLift-360: Lifting an in-the-Wild 2D Photo to A 3D Object with 360° Views",
        "abstract": "Virtual reality and augmented reality (XR) bring increasing demand for 3D content generation. However, creating high-quality 3D content requires tedious work from a human expert. In this work, we study the challenging task of lifting a single image to a 3D object and, for the first time, demonstrate the ability to generate a plausible 3D object with 360° views that corresponds well with the given reference image. By conditioning on the reference image, our model can fulfill the everlasting curiosity for synthesizing novel views of objects from images. Our technique sheds light on a promising direction of easing the workflows for 3D artists and XR designers. We propose a novel framework, dubbed NeuralLift-360, that utilizes a depth-aware neural radiance representation (NeRF) and learns to craft the scene guided by denoising diffusion models. By introducing a ranking loss, our NeuralLift-360 can be guided with rough depth estimation in the wild. We also adopt a CLIP-guided sampling strategy for the diffusion prior to provide coherent guidance. Extensive experiments demonstrate that our NeuralLift-360 significantly outperforms existing state-of-the-art baselines. Project page: https://vita-group.github.io/NeuralLift-360/",
        "authors": [
            "Dejia Xu",
            "Yifan Jiang",
            "Peihao Wang",
            "Zhiwen Fan",
            "Yi Wang",
            "Zhangyang Wang"
        ],
        "citations": 137,
        "references": 82,
        "year": 2022
    },
    {
        "title": "Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations",
        "abstract": "Generating graph-structured data requires learning the underlying distribution of graphs. Yet, this is a challenging problem, and the previous graph generative methods either fail to capture the permutation-invariance property of graphs or cannot sufficiently model the complex dependency between nodes and edges, which is crucial for generating real-world graphs such as molecules. To overcome such limitations, we propose a novel score-based generative model for graphs with a continuous-time framework. Specifically, we propose a new graph diffusion process that models the joint distribution of the nodes and edges through a system of stochastic differential equations (SDEs). Then, we derive novel score matching objectives tailored for the proposed diffusion process to estimate the gradient of the joint log-density with respect to each component, and introduce a new solver for the system of SDEs to efficiently sample from the reverse diffusion process. We validate our graph generation method on diverse datasets, on which it either achieves significantly superior or competitive performance to the baselines. Further analysis shows that our method is able to generate molecules that lie close to the training distribution yet do not violate the chemical valency rule, demonstrating the effectiveness of the system of SDEs in modeling the node-edge relationships. Our code is available at https://github.com/harryjo97/GDSS.",
        "authors": [
            "Jaehyeong Jo",
            "Seul Lee",
            "Sung Ju Hwang"
        ],
        "citations": 180,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Optimizing DDPM Sampling with Shortcut Fine-Tuning",
        "abstract": "In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting in sample quality comparable to or even surpassing that of the full-step model across various datasets.",
        "authors": [
            "Ying Fan",
            "Kangwook Lee"
        ],
        "citations": 43,
        "references": 53,
        "year": 2023
    },
    {
        "title": "DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises",
        "abstract": "While diffusion models have achieved great success in generating continuous signals such as images and audio, it remains elusive for diffusion models in learning discrete sequence data like natural languages. Although recent advances circumvent this challenge of discreteness by embedding discrete tokens as continuous surrogates, they still fall short of satisfactory generation quality. To understand this, we first dive deep into the denoised training protocol of diffusion-based sequence generative models and determine their three severe problems, i.e., 1) failing to learn, 2) lack of scalability, and 3) neglecting source conditions. We argue that these problems can be boiled down to the pitfall of the not completely eliminated discreteness in the embedding space, and the scale of noises is decisive herein. In this paper, we introduce DINOISER to facilitate diffusion models for sequence generation by manipulating noises. We propose to adaptively determine the range of sampled noise scales for counter-discreteness training; and encourage the proposed diffused sequence learner to leverage source conditions with amplified noise scales during inference. Experiments show that DINOISER enables consistent improvement over the baselines of previous diffusion-based sequence generative models on several conditional sequence modeling benchmarks thanks to both effective training and inference strategies. Analyses further verify that DINOISER can make better use of source conditions to govern its generative process.",
        "authors": [
            "Jiasheng Ye",
            "Zaixiang Zheng",
            "Yu Bao",
            "Lihua Qian",
            "Mingxuan Wang"
        ],
        "citations": 38,
        "references": 77,
        "year": 2023
    },
    {
        "title": "VideoLCM: Video Latent Consistency Model",
        "abstract": "Consistency models have demonstrated powerful capability in efficient image generation and allowed synthesis within a few sampling steps, alleviating the high computational cost in diffusion models. However, the consistency model in the more challenging and resource-consuming video generation is still less explored. In this report, we present the VideoLCM framework to fill this gap, which leverages the concept of consistency models from image generation to efficiently synthesize videos with minimal steps while maintaining high quality. VideoLCM builds upon existing latent video diffusion models and incorporates consistency distillation techniques for training the latent consistency model. Experimental results reveal the effectiveness of our VideoLCM in terms of computational efficiency, fidelity and temporal consistency. Notably, VideoLCM achieves high-fidelity and smooth video synthesis with only four sampling steps, showcasing the potential for real-time synthesis. We hope that VideoLCM can serve as a simple yet effective baseline for subsequent research. The source code and models will be publicly available.",
        "authors": [
            "Xiang Wang",
            "Shiwei Zhang",
            "Han Zhang",
            "Yu Liu",
            "Yingya Zhang",
            "Changxin Gao",
            "Nong Sang"
        ],
        "citations": 36,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Convergence of score-based generative modeling for general data distributions",
        "abstract": "Score-based generative modeling (SGM) has grown to be a hugely successful method for learning to generate samples from complex data distributions such as that of images and audio. It is based on evolving an SDE that transforms white noise into a sample from the learned distribution, using estimates of the score function, or gradient log-pdf. Previous convergence analyses for these methods have suffered either from strong assumptions on the data distribution or exponential dependencies, and hence fail to give efficient guarantees for the multimodal and non-smooth distributions that arise in practice and for which good empirical performance is observed. We consider a popular kind of SGM -- denoising diffusion models -- and give polynomial convergence guarantees for general data distributions, with no assumptions related to functional inequalities or smoothness. Assuming $L^2$-accurate score estimates, we obtain Wasserstein distance guarantees for any distribution of bounded support or sufficiently decaying tails, as well as TV guarantees for distributions with further smoothness assumptions.",
        "authors": [
            "Holden Lee",
            "Jianfeng Lu",
            "Yixin Tan"
        ],
        "citations": 110,
        "references": 25,
        "year": 2022
    },
    {
        "title": "Mixture of Diffusers for scene composition and high resolution image generation",
        "abstract": "Diffusion methods have been proven to be very effective to generate images while conditioning on a text prompt. However, and although the quality of the generated images is unprecedented, these methods seem to struggle when trying to generate specific image compositions. In this paper we present Mixture of Diffusers, an algorithm that builds over existing diffusion models to provide a more detailed control over composition. By harmonizing several diffusion processes acting on different regions of a canvas, it allows generating larger images, where the location of each object and style is controlled by a separate diffusion process.",
        "authors": [
            "Á. Jiménez"
        ],
        "citations": 35,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Scalable Adaptive Computation for Iterative Generation",
        "abstract": "Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to 1024X1024 images without cascades or guidance, while being domain-agnostic and up to 10X more efficient than 2D and 3D U-Nets.",
        "authors": [
            "A. Jabri",
            "David J. Fleet",
            "Ting Chen"
        ],
        "citations": 95,
        "references": 63,
        "year": 2022
    },
    {
        "title": "MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices",
        "abstract": "The deployment of large-scale text-to-image diffusion models on mobile devices is impeded by their substantial model size and slow inference speed. In this paper, we pro-pose MobileDiffusion , a highly efficient text-to-image diffusion model obtained through extensive optimizations in both architecture and sampling techniques. We conduct a comprehensive examination of model architecture design to reduce redundancy, enhance computational efficiency, and minimize model’s parameter count, while preserving image generation quality. Additionally, we employ distillation and diffusion-GAN finetuning techniques on MobileDiffusion to achieve 8-step and 1-step inference respectively. Empirical studies, conducted both quantitatively and qualitatively, demonstrate the effectiveness of our proposed techniques. MobileDiffusion achieves a remarkable sub-second inference speed for generating a 512 × 512 image on mobile devices, establishing a new state of the art.",
        "authors": [
            "Yang Zhao",
            "Yanwu Xu",
            "Zhisheng Xiao",
            "Tingbo Hou"
        ],
        "citations": 33,
        "references": 61,
        "year": 2023
    },
    {
        "title": "DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents",
        "abstract": "Diffusion probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, standard Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design novel conditional parameterizations for diffusion models. We show that the resulting model equips diffusion models with a low-dimensional VAE inferred latent code which can be used for downstream tasks like controllable synthesis. The proposed method also improves upon the speed vs quality tradeoff exhibited in standard unconditional DDPM/DDIM models (for instance, FID of 16.47 vs 34.36 using a standard DDIM on the CelebA-HQ-128 benchmark using T=10 reverse process steps) without having explicitly trained for such an objective. Furthermore, the proposed model exhibits synthesis quality comparable to state-of-the-art models on standard image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most existing VAE-based methods. Lastly, we show that the proposed method exhibits inherent generalization to different types of noise in the conditioning signal. For reproducibility, our source code is publicly available at https://github.com/kpandey008/DiffuseVAE.",
        "authors": [
            "Kushagra Pandey",
            "Avideep Mukherjee",
            "Piyush Rai",
            "Abhishek Kumar"
        ],
        "citations": 98,
        "references": 84,
        "year": 2022
    },
    {
        "title": "Pretraining is All You Need for Image-to-Image Translation",
        "abstract": "We propose to use pretraining to boost general image-to-image translation. Prior image-to-image translation methods usually need dedicated architectural design and train individual translation models from scratch, struggling for high-quality generation of complex scenes, especially when paired training data are not abundant. In this paper, we regard each image-to-image translation problem as a downstream task and introduce a simple and generic framework that adapts a pretrained diffusion model to accommodate various kinds of image-to-image translation. We also propose adversarial training to enhance the texture synthesis in the diffusion model training, in conjunction with normalized guidance sampling to improve the generation quality. We present extensive empirical comparison across various tasks on challenging benchmarks such as ADE20K, COCO-Stuff, and DIODE, showing the proposed pretraining-based image-to-image translation (PITI) is capable of synthesizing images of unprecedented realism and faithfulness.",
        "authors": [
            "Tengfei Wang",
            "Ting Zhang",
            "Bo Zhang",
            "Hao Ouyang",
            "Dong Chen",
            "Qifeng Chen",
            "Fang Wen"
        ],
        "citations": 168,
        "references": 62,
        "year": 2022
    },
    {
        "title": "Riemannian Score-Based Generative Modeling",
        "abstract": "Score-based generative models (SGMs) are a powerful class of generative models that exhibit remarkable empirical performance. Score-based generative modelling (SGM) consists of a ``noising'' stage, whereby a diffusion is used to gradually add Gaussian noise to data, and a generative model, which entails a ``denoising'' process defined by approximating the time-reversal of the diffusion. Existing SGMs assume that data is supported on a Euclidean space, i.e. a manifold with flat geometry. In many domains such as robotics, geoscience or protein modelling, data is often naturally described by distributions living on Riemannian manifolds and current SGM techniques are not appropriate. We introduce here Riemannian Score-based Generative Models (RSGMs), a class of generative models extending SGMs to Riemannian manifolds. We demonstrate our approach on a variety of manifolds, and in particular with earth and climate science spherical data.",
        "authors": [
            "Valentin De Bortoli",
            "Emile Mathieu",
            "M. Hutchinson",
            "James Thornton",
            "Y. Teh",
            "A. Doucet"
        ],
        "citations": 137,
        "references": 168,
        "year": 2022
    },
    {
        "title": "Generative Modelling With Inverse Heat Dissipation",
        "abstract": "While diffusion models have shown great success in image generation, their noise-inverting generative process does not explicitly consider the structure of images, such as their inherent multi-scale nature. Inspired by diffusion models and the empirical success of coarse-to-fine modelling, we propose a new diffusion-like model that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image. We interpret the solution of the forward heat equation with constant additive noise as a variational approximation in the diffusion latent variable model. Our new model shows emergent qualitative properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images. Spectral analysis on natural images highlights connections to diffusion models and reveals an implicit coarse-to-fine inductive bias in them.",
        "authors": [
            "Severi Rissanen",
            "Markus Heinonen",
            "A. Solin"
        ],
        "citations": 87,
        "references": 93,
        "year": 2022
    },
    {
        "title": "DiffPose: Toward More Reliable 3D Pose Estimation",
        "abstract": "Monocular 3D human pose estimation is quite challenging due to the inherent ambiguity and occlusion, which often lead to high uncertainty and indeterminacy. On the other hand, diffusion models have recently emerged as an effective tool for generating high-quality images from noise. In-spired by their capability, we explore a novel pose estimation framework (DiffPose) that formulates 3D pose estimation as a reverse diffusion process. We incorporate novel designs into our DiffPose to facilitate the diffusion process for 3D pose estimation: a pose-specific initialization of pose uncertainty distributions, a Gaussian Mixture Model-based forward diffusion process, and a context-conditioned re-verse diffusion process. Our proposed DiffPose significantly outperforms existing methods on the widely used pose estimation benchmarks Human3.6M and MPI-INF-3DHP. Project page: https://gongjia0208.github.io/Diffpose/.",
        "authors": [
            "Jia Gong",
            "Lin Geng Foo",
            "Zhipeng Fan",
            "Qiuhong Ke",
            "H. Rahmani",
            "J. Liu"
        ],
        "citations": 82,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
        "abstract": "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
        "authors": [
            "Shin-Ying Yeh",
            "Yu-Guan Hsieh",
            "Zhidong Gao",
            "Bernard B. W. Yang",
            "Giyeong Oh",
            "Yanmin Gong"
        ],
        "citations": 50,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Fake it Till You Make it: Learning Transferable Representations from Synthetic ImageNet Clones",
        "abstract": "Recent image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a simple text prompt. Could such models render real images obsolete for training image prediction models? In this paper, we answer part of this provocative question by investigating the need for real images when training models for ImageNet classification. Provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful these are for training classification models from scratch. We show that with minimal and class-agnostic prompt engineering, ImageNet clones are able to close a large part of the gap between models produced by synthetic images and models trained with real images, for the several standard classification benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhibit strong generalization properties and perform on par with models trained on real data for transfer. Project page: https://europe.naverlabs.com/imagenet-sd",
        "authors": [
            "Mert Bulent Sariyildiz",
            "Alahari Karteek",
            "Diane Larlus",
            "Yannis Kalantidis"
        ],
        "citations": 123,
        "references": 115,
        "year": 2022
    },
    {
        "title": "ADriver-I: A General World Model for Autonomous Driving",
        "abstract": "Typically, autonomous driving adopts a modular design, which divides the full stack into perception, prediction, planning and control parts. Though interpretable, such modular design tends to introduce a substantial amount of redundancy. Recently, multimodal large language models (MLLM) and diffusion techniques have demonstrated their superior performance on comprehension and generation ability. In this paper, we first introduce the concept of interleaved vision-action pair, which unifies the format of visual features and control signals. Based on the vision-action pairs, we construct a general world model based on MLLM and diffusion model for autonomous driving, termed ADriver-I. It takes the vision-action pairs as inputs and autoregressively predicts the control signal of the current frame. The generated control signals together with the historical vision-action pairs are further conditioned to predict the future frames. With the predicted next frame, ADriver-I performs further control signal prediction. Such a process can be repeated infinite times, ADriver-I achieves autonomous driving in the world created by itself. Extensive experiments are conducted on nuScenes and our large-scale private datasets. ADriver-I shows impressive performance compared to several constructed baselines. We hope our ADriver-I can provide some new insights for future autonomous driving and embodied intelligence.",
        "authors": [
            "Fan Jia",
            "Weixin Mao",
            "Yingfei Liu",
            "Yucheng Zhao",
            "Yuqing Wen",
            "Chi Zhang",
            "Xiangyu Zhang",
            "Tiancai Wang"
        ],
        "citations": 43,
        "references": 63,
        "year": 2023
    },
    {
        "title": "Non-Denoising Forward-Time Diffusions",
        "abstract": "The scope of this paper is generative modeling through diffusion processes. An approach falling within this paradigm is the work of Song et al. (2021), which relies on a time-reversal argument to construct a diffusion process targeting the desired data distribution. We show that the time-reversal argument, common to all denoising diffusion probabilistic modeling proposals, is not necessary. We obtain diffusion processes targeting the desired data distribution by taking appropriate mixtures of diffusion bridges. The resulting transport is exact by construction, allows for greater flexibility in choosing the dynamics of the underlying diffusion, and can be approximated by means of a neural network via novel training objectives. We develop a unifying view of the drift adjustments corresponding to our and to time-reversal approaches and make use of this representation to inspect the inner workings of diffusion-based generative models. Finally, we leverage on scalable simulation and inference techniques common in spatial statistics to move beyond fully factorial distributions in the underlying diffusion dynamics. The methodological advances contained in this work contribute toward establishing a general framework for generative modeling based on diffusion processes.",
        "authors": [
            "Stefano Peluchetti"
        ],
        "citations": 42,
        "references": 25,
        "year": 2023
    },
    {
        "title": "Distilling Model Failures as Directions in Latent Space",
        "abstract": "Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes. Code available at https://github.com/MadryLab/failure-directions",
        "authors": [
            "Saachi Jain",
            "Hannah Lawrence",
            "Ankur Moitra",
            "A. Madry"
        ],
        "citations": 72,
        "references": 96,
        "year": 2022
    },
    {
        "title": "CasFlow: Exploring Hierarchical Structures and Propagation Uncertainty for Cascade Prediction",
        "abstract": "Understanding in-network information diffusion is a fundamental problem in many applications and one of the primary challenges is to predict the information cascade size. Most of the existing models rely either on hypothesized point process (e.g., Poisson and Hawkes processes), or simply predict the information propagation via deep neural networks. However, they fail to simultaneously capture the underlying global and local structures of a cascade and the propagation uncertainty in the diffusion, which may result in unsatisfactory prediction performance. To address these, in this work we propose a novel probabilistic cascade prediction framework CasFlow: Hierarchical Cascade Normalizing Flows. CasFlow allows a non-linear information diffusion inference and models the information diffusion process by learning the latent representation of both the structural and temporal information. It is a pattern-agnostic model leveraging normalizing flows to learn the node-level and cascade-level latent factors in an unsupervised manner. In addition, CasFlow is capable of capturing both the cascade representation uncertainty and node infection uncertainty, while enabling hierarchical pattern learning of information diffusion. Extensive experiments conducted on real-world datasets demonstrate that CasFlow reduces the prediction error to 21.0% by only observing half an hour of cascades, compared to state-of-the-art approaches, while also enabling model interpretability.",
        "authors": [
            "Xovee Xu",
            "Fan Zhou",
            "Kunpeng Zhang",
            "Siyuan Liu",
            "Goce Trajcevski"
        ],
        "citations": 34,
        "references": 91,
        "year": 2023
    },
    {
        "title": "CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have shown promising performance for speech synthesis. However, a large number of iterative steps are required to achieve high sample quality, which restricts the inference speed. Maintaining sample quality while increasing sampling speed has become a challenging task. In this paper, we propose a Consistency Model-based Speech synthesis method, CoMoSpeech, which achieve speech synthesis through a single diffusion sampling step while achieving high audio quality. The consistency constraint is applied to distill a consistency model from a well-designed diffusion-based teacher model, which ultimately yields superior performances in the distilled CoMoSpeech. Our experiments show that by generating audio recordings by a single sampling step, the CoMoSpeech achieves an inference speed more than 150 times faster than real-time on a single NVIDIA A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based speech synthesis truly practical. Meanwhile, objective and subjective evaluations on text-to-speech and singing voice synthesis show that the proposed teacher models yield the best audio quality, and the one-step sampling based CoMoSpeech achieves the best inference speed with better or comparable audio quality to other conventional multi-step diffusion model baselines. Audio samples and codes are available at https://comospeech.github. https://comospeech.github.io/.",
        "authors": [
            "Zhe Ye",
            "Wei Xue",
            "Xuejiao Tan",
            "Jie Chen",
            "Qi-fei Liu",
            "Yi-Ting Guo"
        ],
        "citations": 33,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Theoretical, Equilibrium, Kinetics and Thermodynamic Investigations of Methylene Blue Adsorption onto Lignite Coal",
        "abstract": "The interaction of methylene blue (MB) dye with natural coal (collected from coal landfills of the Kosovo Energy Corporation) in aqueous solutions was studied using adsorption, kinetics, and thermodynamic data, and Monte Carlo (MC) calculations. In a batch procedure, the effects of contact duration, initial MB concentration, pH, and solution temperature on the adsorption process were examined. The Langmuir, Freundlich, Temkin, and Dubinin–Radushkevich (D–R) isotherms were used to examine the equilibrium adsorption data. The equilibrium data fit well to the Freundlich and Langmuir adsorption isotherm models; however, the Freundlich model suited the adsorption data to a slightly better extent than the Langmuir model. The kinetics experimental data was fitted using pseudo-first-order, first-order, pseudo-second-order, second-order, Elvoich equation, and diffusion models. The pseudo-second-order rate model manifested a superlative fit to the experimental data, while the adsorption of MB onto coal is regulated by both liquid film and intraparticle diffusions at the same time. Thermodynamic parameters, such as Gibbs free energy (ΔG0), enthalpy (ΔH0), and entropy (ΔS0) were calculated. The adsorption of MB was confirmed to be spontaneous and endothermic. The theoretical results were in agreement with the experimental ones.",
        "authors": [
            "Naim Hasani",
            "T. Selimi",
            "A. Mele",
            "V. Thaçi",
            "J. Halili",
            "Avni Berisha",
            "M. Sadiku"
        ],
        "citations": 72,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Understanding DDPM Latent Codes Through Optimal Transport",
        "abstract": "Diffusion models have recently outperformed alternative approaches to model the distribution of natural images, such as GANs. Such diffusion models allow for deterministic sampling via the probability flow ODE, giving rise to a latent space and an encoder map. While having important practical applications, such as estimation of the likelihood, the theoretical properties of this map are not yet fully understood. In the present work, we partially address this question for the popular case of the VP SDE (DDPM) approach. We show that, perhaps surprisingly, the DDPM encoder map coincides with the optimal transport map for common distributions; we support this claim theoretically and by extensive numerical experiments.",
        "authors": [
            "Valentin Khrulkov",
            "I. Oseledets"
        ],
        "citations": 48,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Synchronization of Epidemic Systems with Neumann Boundary Value under Delayed Impulse",
        "abstract": "This paper reports the construction of synchronization criteria for the delayed impulsive epidemic models with reaction–diffusion under the Neumann boundary value. Different from the previous literature, the reaction–diffusion epidemic model with a delayed impulse brings mathematical difficulties to this paper. In fact, due to the existence of second-order partial derivatives in the reaction–diffusion model with a delayed impulse, the methods of first-order ordinary differential equations from the previous literature cannot be effectively applied in this paper. However, with the help of the variational method and an appropriate boundedness assumption, a new synchronization criterion is derived, and its effectiveness is illustrated by numerical examples.",
        "authors": [
            "R. Rao",
            "Zhi-jian Lin",
            "X. Ai",
            "Jiarui Wu"
        ],
        "citations": 71,
        "references": 19,
        "year": 2022
    },
    {
        "title": "Modeling Interfacial Interaction between Gas Molecules and Semiconductor Metal Oxides: A New View Angle on Gas Sensing",
        "abstract": "With the development of internet of things and artificial intelligence electronics, metal oxide semiconductor (MOS)‐based sensing materials have attracted increasing attention from both fundamental research and practical applications. MOS materials possess intrinsic physicochemical properties, tunable compositions, and electronic structure, and are particularly suitable for integration and miniaturization in developing chemiresistive gas sensors. During sensing processes, the dynamic gas–solid interface interactions play crucial roles in improving sensors’ performance, and most studies emphasize the gas–MOS chemical reactions. Herein, from a new view angle focusing more on physical gas–solid interactions during gas sensing, basic theory overview and latest progress for the dynamic process of gas molecules including adsorption, desorption, and diffusion, are systematically summarized and elucidated. The unique electronic sensing mechanisms are also discussed from various aspects including molecular interaction models, gas diffusion mechanism, and interfacial reaction behaviors, where structure–activity relationship and diffusion behavior are overviewed in detail. Especially, the surface adsorption–desorption dynamics are discussed and evaluated, and their potential effects on sensing performance are elucidated from the gas–solid interfacial regulation perspective. Finally, the prospect for further research directions in improving gas dynamic processes in MOS gas sensors is discussed, aiming to supplement the approaches for the development of high‐performance MOS gas sensors.",
        "authors": [
            "Chenyi Yuan",
            "Junhao Ma",
            "Yidong Zou",
            "Guisheng Li",
            "Hualong Xu",
            "V. Sysoev",
            "Xiaowei Cheng",
            "Yonghui Deng"
        ],
        "citations": 67,
        "references": 179,
        "year": 2022
    },
    {
        "title": "Latte: Latent Diffusion Transformer for Video Generation",
        "abstract": "We propose a novel Latent Diffusion Transformer, namely Latte, for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to text-to-video generation (T2V) task, where Latte achieves comparable results compared to recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation.",
        "authors": [
            "Xin Ma",
            "Yaohui Wang",
            "Gengyun Jia",
            "Xinyuan Chen",
            "Ziwei Liu",
            "Yuan-Fang Li",
            "Cunjian Chen",
            "Yu Qiao"
        ],
        "citations": 134,
        "references": 74,
        "year": 2024
    },
    {
        "title": "Wonder3D: Single Image to 3D Using Cross-Domain Diffusion",
        "abstract": "In this work, we introduce Wonder3D, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works di-rectly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we pro-pose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a geometry-aware normal fusion algorithm that extracts high-quality surfaces from the multi-view 2D representations in only 2 r-;» 3 minutes. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works.",
        "authors": [
            "Xiaoxiao Long",
            "Yuanchen Guo",
            "Cheng Lin",
            "Yuan Liu",
            "Zhiyang Dou",
            "Lingjie Liu",
            "Yuexin Ma",
            "Song-Hai Zhang",
            "Marc Habermann",
            "C. Theobalt",
            "Wenping Wang"
        ],
        "citations": 282,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors",
        "abstract": "We present Magic123, a two-stage coarse-to-fine approach for high-quality, textured 3D meshes generation from a single unposed image in the wild using both2D and 3D priors. In the first stage, we optimize a neural radiance field to produce a coarse geometry. In the second stage, we adopt a memory-efficient differentiable mesh representation to yield a high-resolution mesh with a visually appealing texture. In both stages, the 3D content is learned through reference view supervision and novel views guided by a combination of 2D and 3D diffusion priors. We introduce a single trade-off parameter between the 2D and 3D priors to control exploration (more imaginative) and exploitation (more precise) of the generated geometry. Additionally, we employ textual inversion and monocular depth regularization to encourage consistent appearances across views and to prevent degenerate solutions, respectively. Magic123 demonstrates a significant improvement over previous image-to-3D techniques, as validated through extensive experiments on synthetic benchmarks and diverse real-world images. Our code, models, and generated 3D assets are available at https://github.com/guochengqian/Magic123.",
        "authors": [
            "Guocheng Qian",
            "Jinjie Mai",
            "Abdullah Hamdi",
            "Jian Ren",
            "Aliaksandr Siarohin",
            "Bing Li",
            "Hsin-Ying Lee",
            "Ivan Skorokhodov",
            "Peter Wonka",
            "S. Tulyakov",
            "Bernard Ghanem"
        ],
        "citations": 290,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Depth-dependent diffusion algorithm for simulation of sedimentation in shallow marine depositional systems",
        "abstract": "An algorithm has been developed to simulate sediment dispersal on shallow marine siliciclastic, carbonate, and mixed siliciclastic-carbonate shelves. The algorithm is based on a diffusion scheme in which the diffusion coefficient decays exponentially with water depth. The rationale for using a varying diffusion coefficient lies in the observation that on marine shelves wave energy and therefore bed shear stress decay exponentially with water depth. Thus sediment flux cannot be modeled by a diffusive process based on a linear dependence on slope alone. This approach is probably most appropriate for wave-dominated shelves. The model simulates deposition in two dimensions. Siliciclastic shelf sedimentation occurs solely by lateral transport in the plane of section by diffusion; carbonate sedimentation occurs by depth-dependent in situ sediment production with subsequent lateral dispersal of sediment by diffusion. The effects of early cementation can be modeled by varying the transport coefficient that governs the efficiency of diffusion.",
        "authors": [
            "P. Kaufman",
            "J. Grotzinger",
            "D. McCormick"
        ],
        "citations": 62,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior",
        "abstract": "In this work, we investigate the problem of creating high-fidelity 3D content from only a single image. This is inherently challenging: it essentially involves estimating the underlying 3D geometry while simultaneously hallucinating unseen textures. To address this challenge, we leverage prior knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision for 3D creation. Our approach, Make-It-3D, employs a two-stage optimization pipeline: the first stage optimizes a neural radiance field by incorporating constraints from the reference image at the frontal view and diffusion prior at novel views; the second stage transforms the coarse model into textured point clouds and further elevates the realism with diffusion prior while leveraging the high-quality textures from the reference image. Extensive experiments demonstrate that our method outperforms prior works by a large margin, resulting in faithful reconstructions and impressive visual quality. Our method presents the first attempt to achieve high-quality 3D creation from a single image for general objects and enables various applications such as text-to-3D creation and texture editing.",
        "authors": [
            "Junshu Tang",
            "Tengfei Wang",
            "Bo Zhang",
            "Ting Zhang",
            "Ran Yi",
            "Lizhuang Ma",
            "Dong Chen"
        ],
        "citations": 269,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Equivariant 3D-conditional diffusion model for molecular linker design",
        "abstract": null,
        "authors": [
            "Ilia Igashov",
            "Hannes Stärk",
            "Clément Vignac",
            "Arne Schneuing",
            "Victor Garcia Satorras",
            "Pascal Frossard",
            "Max Welling",
            "Michael M. Bronstein",
            "Bruno E. Correia"
        ],
        "citations": 40,
        "references": 28,
        "year": 2024
    },
    {
        "title": "MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model",
        "abstract": "This paper studies the human image animation task, which aims to generate a video of a certain reference iden-tity following a particular motion sequence. Existing an-imation works typically employ the frame-warping technique to animate the reference image towards the target motion. Despite achieving reasonable results, these approaches face challenges in maintaining temporal consistency throughout the animation due to the lack of temporal modeling and poor preservation of reference identity. In this work, we introduce Magic/snimate, a diffusion-based framework that aims at enhancing temporal consistency, preserving reference image faithfully, and improving animation fidelity. To achieve this, we first develop a video diffusion model to encode temporal information. Second, to maintain the appearance coherence across frames, we introduce a novel appearance encoder to retain the intricate details of the reference image. Leveraging these two inno-vations, we further employ a simple video fusion technique to encourage smooth transitions for long video animation. Empirical results demonstrate the superiority of our method over baseline approaches on two benchmarks. Notably, our approach outperforms the strongest baseline by over 38% in terms of video fidelity on the challenging TikTok dancing dataset. Code and model will be made available at https://showlab.github.io/magicanimate.",
        "authors": [
            "Zhongcong Xu",
            "Jianfeng Zhang",
            "J. Liew",
            "Hanshu Yan",
            "Jia-Wei Liu",
            "Chenxu Zhang",
            "Jiashi Feng",
            "Mike Zheng Shou"
        ],
        "citations": 127,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Common Diffusion Noise Schedules and Sample Steps are Flawed",
        "abstract": "We discover that common diffusion noise schedules do not enforce the last timestep to have zero signal-to-noise ratio (SNR), and some implementations of diffusion samplers do not start from the last timestep. Such designs are flawed and do not reflect the fact that the model is given pure Gaussian noise at inference, creating a discrepancy between training and inference. We show that the flawed design causes real problems in existing implementations. In Stable Diffusion, it severely limits the model to only generate images with medium brightness and prevents it from generating very bright and dark samples. We propose a few simple fixes: (1) rescale the noise schedule to enforce zero terminal SNR; (2) train the model with v prediction; (3) change the sampler to always start from the last timestep; (4) rescale classifier-free guidance to prevent over-exposure. These simple changes ensure the diffusion process is congruent between training and inference and allow the model to generate samples more faithful to the original data distribution.",
        "authors": [
            "Shanchuan Lin",
            "Bingchen Liu",
            "Jiashi Li",
            "Xiao Yang"
        ],
        "citations": 153,
        "references": 16,
        "year": 2023
    },
    {
        "title": "MotionDiffuse: Text-Driven Human Motion Generation With Diffusion Model",
        "abstract": "Human motion modeling is important for many modern graphics applications, which typically require professional skills. In order to remove the skill barriers for laymen, recent motion generation methods can directly generate human motions conditioned on natural languages. However, it remains challenging to achieve diverse and fine-grained motion generation with various text inputs. To address this problem, we propose MotionDiffuse, one of the first diffusion model-based text-driven motion generation frameworks, which demonstrates several desired properties over existing methods. 1) Probabilistic Mapping. Instead of a deterministic language-motion mapping, MotionDiffuse generates motions through a series of denoising steps in which variations are injected. 2) Realistic Synthesis. MotionDiffuse excels at modeling complicated data distribution and generating vivid motion sequences. 3) Multi-Level Manipulation. MotionDiffuse responds to fine-grained instructions on body parts, and arbitrary-length motion synthesis with time-varied text prompts. Our experiments show MotionDiffuse outperforms existing SoTA methods by convincing margins on text-driven motion generation and action-conditioned motion generation. A qualitative analysis further demonstrates MotionDiffuse's controllability for comprehensive motion generation.",
        "authors": [
            "Mingyuan Zhang",
            "Zhongang Cai",
            "Liang Pan",
            "Fangzhou Hong",
            "Xinying Guo",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "citations": 424,
        "references": 89,
        "year": 2022
    },
    {
        "title": "Equivariant Diffusion for Molecule Generation in 3D",
        "abstract": "This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and efficiency at training time.",
        "authors": [
            "Emiel Hoogeboom",
            "Victor Garcia Satorras",
            "Clément Vignac",
            "M. Welling"
        ],
        "citations": 486,
        "references": 53,
        "year": 2022
    },
    {
        "title": "DiffIR: Efficient Diffusion Model for Image Restoration",
        "abstract": "Diffusion model (DM) has achieved SOTA performance by modeling the image synthesis process into a sequential application of a denoising network. However, different from image synthesis, image restoration (IR) has a strong constraint to generate results in accordance with ground-truth. Thus, for IR, traditional DMs running massive iterations on a large model to estimate whole images or feature maps is inefficient. To address this issue, we propose an efficient DM for IR (DiffIR), which consists of a compact IR prior extraction network (CPEN), dynamic IR transformer (DIRformer), and denoising network. Specifically, DiffIR has two training stages: pretraining and training DM. In pretraining, we input ground-truth images into CPENS1 to capture a compact IR prior representation (IPR) to guide DIRformer. In the second stage, we train the DM to directly estimate the same IRP as pretrained CPENS1 only using LQ images. We observe that since the IPR is only a compact vector, DiffIR can use fewer iterations than traditional DM to obtain accurate estimations and generate more stable and realistic results. Since the iterations are few, our DiffIR can adopt a joint optimization of CPENS2, DIRformer, and denoising network, which can further reduce the estimation error influence. We conduct extensive experiments on several IR tasks and achieve SOTA performance while consuming less computational costs. Code is available at https://github.com/Zj-BinXia/DiffIR.",
        "authors": [
            "Bin Xia",
            "Yulun Zhang",
            "Shiyin Wang",
            "Yitong Wang",
            "Xing Wu",
            "Yapeng Tian",
            "Wenming Yang",
            "L. Gool"
        ],
        "citations": 141,
        "references": 89,
        "year": 2023
    },
    {
        "title": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",
        "abstract": "Predicting the binding structure of a small molecule ligand to a protein -- a task known as molecular docking -- is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.",
        "authors": [
            "Gabriele Corso",
            "Hannes Stärk",
            "Bowen Jing",
            "R. Barzilay",
            "T. Jaakkola"
        ],
        "citations": 338,
        "references": 53,
        "year": 2022
    },
    {
        "title": "Any-to-Any Generation via Composable Diffusion",
        "abstract": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io",
        "authors": [
            "Zineng Tang",
            "Ziyi Yang",
            "Chenguang Zhu",
            "Michael Zeng",
            "Mohit Bansal"
        ],
        "citations": 133,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Generative Diffusion Prior for Unified Image Restoration and Enhancement",
        "abstract": "Existing image restoration methods mostly leverage the posterior distribution of natural images. However, they often assume known degradation and also require supervised training, which restricts their adaptation to complex real applications. In this work, we propose the Generative Diffusion Prior (GDP) to effectively model the posterior distributions in an unsupervised sampling manner. GDP utilizes a pre-train denoising diffusion generative model (DDPM) for solving linear inverse, non-linear, or blind problems. Specifically, GDP systematically explores a protocol of conditional guidance, which is verified more practical than the commonly used guidance way. Furthermore, GDP is strength at optimizing the parameters of degradation model during the denoising process, achieving blind image restoration. Besides, we devise hierarchical guidance and patch-based methods, enabling the GDP to generate images of arbitrary resolutions. Experimentally, we demonstrate GDP's versatility on several image datasets for linear problems, such as super-resolution, deblurring, inpainting, and colorization, as well as non-linear and blind issues, such as low-light enhancement and HDR image recovery. GDP outperforms the current leading unsupervised methods on the diverse benchmarks in reconstruction quality and perceptual quality. Moreover, GDP also generalizes well for natural images or synthesized images with arbitrary sizes from various tasks out of the distribution of the ImageNet training set. The project page is available at https://generativediffusionprior.github.io/",
        "authors": [
            "Ben Fei",
            "Zhaoyang Lyu",
            "Liang Pan",
            "Junzhe Zhang",
            "Weidong Yang",
            "Tian-jian Luo",
            "Bo Zhang",
            "Bo Dai"
        ],
        "citations": 131,
        "references": 108,
        "year": 2023
    },
    {
        "title": "DiGress: Discrete Denoising diffusion for graph generation",
        "abstract": "This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories. A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks. We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features. A procedure for conditioning the generation on graph-level features is also proposed. DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.",
        "authors": [
            "Clément Vignac",
            "Igor Krawczuk",
            "Antoine Siraudin",
            "Bohan Wang",
            "V. Cevher",
            "P. Frossard"
        ],
        "citations": 277,
        "references": 62,
        "year": 2022
    },
    {
        "title": "ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting",
        "abstract": "Diffusion-based image super-resolution (SR) methods are mainly limited by the low inference speed due to the requirements of hundreds or even thousands of sampling steps. Existing acceleration sampling techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To address this issue, we propose a novel and efficient diffusion model for SR that significantly reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during inference and its associated performance deterioration. Our method constructs a Markov chain that transfers between the high-resolution image and the low-resolution image by shifting the residual between them, substantially improving the transition efficiency. Additionally, an elaborate noise schedule is developed to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experiments demonstrate that the proposed method obtains superior or at least comparable performance to current state-of-the-art methods on both synthetic and real-world datasets, even only with 15 sampling steps. Our code and model are available at https://github.com/zsyOAOA/ResShift.",
        "authors": [
            "Zongsheng Yue",
            "Jianyi Wang",
            "Chen Change Loy"
        ],
        "citations": 120,
        "references": 70,
        "year": 2023
    },
    {
        "title": "MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion",
        "abstract": "This paper introduces MVDiffusion, a simple yet effective method for generating consistent multi-view images from text prompts given pixel-to-pixel correspondences (e.g., perspective crops from a panorama or multi-view images given depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion simultaneously generates all images with a global awareness, effectively addressing the prevalent error accumulation issue. At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10k panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh.",
        "authors": [
            "Shitao Tang",
            "Fuyang Zhang",
            "Jiacheng Chen",
            "Peng Wang",
            "Yasutaka Furukawa"
        ],
        "citations": 120,
        "references": 51,
        "year": 2023
    },
    {
        "title": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion",
        "abstract": "We present DreamPose, a diffusion-based method for generating animated fashion videos from still images. Given an image and a sequence of human body poses, our method synthesizes a video containing both human and fabric motion. To achieve this, we transform a pre-trained text-to-image model (Stable Diffusion [16]) into a pose-and-image guided video synthesis model, using a novel finetuning strategy, a set of architectural changes to support the added conditioning signals, and techniques to encourage temporal consistency. We fine-tune on a collection of fashion videos from the UBC Fashion dataset [50]. We evaluate our method on a variety of clothing styles and poses, and demonstrate that our method produces state-of-the-art results on fashion video animation. Video results are available on our project page: https://grail.cs.washington.edu/projects/dreampose",
        "authors": [
            "J. Karras",
            "Aleksander Holynski",
            "Ting-Chun Wang",
            "Ira Kemelmacher-Shlizerman"
        ],
        "citations": 114,
        "references": 59,
        "year": 2023
    },
    {
        "title": "GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents",
        "abstract": "The automatic generation of stylized co-speech gestures has recently received increasing attention. Previous systems typically allow style control via predefined text labels or example motion clips, which are often not flexible enough to convey user intent accurately. In this work, we present GestureDiffuCLIP, a neural network framework for synthesizing realistic, stylized co-speech gestures with flexible style control. We leverage the power of the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and present a novel CLIP-guided mechanism that extracts efficient style representations from multiple input modalities, such as a piece of text, an example motion clip, or a video. Our system learns a latent diffusion model to generate high-quality gestures and infuses the CLIP representations of style into the generator via an adaptive instance normalization (AdaIN) layer. We further devise a gesture-transcript alignment mechanism that ensures a semantically correct gesture generation based on contrastive learning. Our system can also be extended to allow fine-grained style control of individual body parts. We demonstrate an extensive set of examples showing the flexibility and generalizability of our model to a variety of style descriptions. In a user study, we show that our system outperforms the state-of-the-art approaches regarding human likeness, appropriateness, and style correctness.",
        "authors": [
            "Tenglong Ao",
            "Zeyi Zhang",
            "Libin Liu"
        ],
        "citations": 114,
        "references": 99,
        "year": 2023
    },
    {
        "title": "DDP: Diffusion Model for Dense Visual Prediction",
        "abstract": "We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional diffusion pipeline. Our approach follows a \"noise-to-map\" generative paradigm for prediction by progressively removing noise from a random Gaussian distribution, guided by the image. The method, called DDP, efficiently extends the denoising diffusion process into the modern perception pipeline. Without task-specific design and architecture customization, DDP is easy to generalize to most dense prediction tasks, e.g., semantic segmentation and depth estimation. In addition, DDP shows attractive properties such as dynamic inference and uncertainty awareness, in contrast to previous single-step discriminative methods. We show top results on three representative tasks with six diverse benchmarks, without tricks, DDP achieves state-of-the-art or competitive performance on each task compared to the specialist counterparts. For example, semantic segmentation (83.9 mIoU on Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation (0.05 REL on KITTI). We hope that our approach will serve as a solid baseline and facilitate future research.",
        "authors": [
            "Yuanfeng Ji",
            "Zhe Chen",
            "Enze Xie",
            "Lanqing Hong",
            "Xihui Liu",
            "Zhaoqiang Liu",
            "Tong Lu",
            "Zhenguo Li",
            "P. Luo"
        ],
        "citations": 101,
        "references": 98,
        "year": 2023
    },
    {
        "title": "HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion",
        "abstract": "Implicit neural fields, typically encoded by a multilayer perceptron (MLP) that maps from coordinates (e.g., xyz) to signals (e.g., signed distances), have shown remarkable promise as a high-fidelity and compact representation. However, the lack of a regular and explicit grid structure also makes it challenging to apply generative modeling directly on implicit neural fields in order to synthesize new data. To this end, we propose HyperDiffusion, a novel approach for unconditional generative modeling of implicit neural fields. HyperDiffusion operates directly on MLP weights and generates new neural implicit fields encoded by synthesized MLP parameters. Specifically, a collection of MLPs is first optimized to faithfully represent individual data samples. Subsequently, a diffusion process is trained in this MLP weight space to model the underlying distribution of neural implicit fields. HyperDiffusion enables diffusion modeling over a implicit, compact, and yet high-fidelity representation of complex signals across 3D shapes and 4D mesh animations within one single unified framework.",
        "authors": [
            "Ziya Erkoç",
            "Fangchang Ma",
            "Qi Shan",
            "M. Nießner",
            "Angela Dai"
        ],
        "citations": 104,
        "references": 54,
        "year": 2023
    },
    {
        "title": "DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior",
        "abstract": "We present DreamCraft3D, a hierarchical 3D content generation method that produces high-fidelity and coherent 3D objects. We tackle the problem by leveraging a 2D reference image to guide the stages of geometry sculpting and texture boosting. A central focus of this work is to address the consistency issue that existing works encounter. To sculpt geometries that render coherently, we perform score distillation sampling via a view-dependent diffusion model. This 3D prior, alongside several training strategies, prioritizes the geometry consistency but compromises the texture fidelity. We further propose Bootstrapped Score Distillation to specifically boost the texture. We train a personalized diffusion model, Dreambooth, on the augmented renderings of the scene, imbuing it with 3D knowledge of the scene being optimized. The score distillation from this 3D-aware diffusion prior provides view-consistent guidance for the scene. Notably, through an alternating optimization of the diffusion prior and 3D scene representation, we achieve mutually reinforcing improvements: the optimized 3D scene aids in training the scene-specific diffusion model, which offers increasingly view-consistent guidance for 3D optimization. The optimization is thus bootstrapped and leads to substantial texture boosting. With tailored 3D priors throughout the hierarchical generation, DreamCraft3D generates coherent 3D objects with photorealistic renderings, advancing the state-of-the-art in 3D content generation. Code available at https://github.com/deepseek-ai/DreamCraft3D.",
        "authors": [
            "Jingxiang Sun",
            "Bo Zhang",
            "Ruizhi Shao",
            "Lizhen Wang",
            "Wen Liu",
            "Zhenda Xie",
            "Yebin Liu"
        ],
        "citations": 103,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Locally Attentional SDF Diffusion for Controllable 3D Shape Generation",
        "abstract": "Although the recent rapid evolution of 3D generative neural networks greatly improves 3D shape generation, it is still not convenient for ordinary users to create 3D shapes and control the local geometry of generated shapes. To address these challenges, we propose a diffusion-based 3D generation framework --- locally attentional SDF diffusion, to model plausible 3D shapes, via 2D sketch image input. Our method is built on a two-stage diffusion model. The first stage, named occupancy-diffusion, aims to generate a low-resolution occupancy field to approximate the shape shell. The second stage, named SDF-diffusion, synthesizes a high-resolution signed distance field within the occupied voxels determined by the first stage to extract fine geometry. Our model is empowered by a novel view-aware local attention mechanism for image-conditioned shape generation, which takes advantage of 2D image patch features to guide 3D voxel feature learning, greatly improving local controllability and model generalizability. Through extensive experiments in sketch-conditioned and category-conditioned 3D shape generation tasks, we validate and demonstrate the ability of our method to provide plausible and diverse 3D shapes, as well as its superior controllability and generalizability over existing work.",
        "authors": [
            "Xin Zheng",
            "Hao Pan",
            "Peng-Shuai Wang",
            "Xin Tong",
            "Yang Liu",
            "H. Shum"
        ],
        "citations": 97,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Executing your Commands via Motion Diffusion in Latent Space",
        "abstract": "We study a challenging task, conditional human motion generation, which produces plausible human motion sequences according to various conditional inputs, such as action classes or textual descriptors. Since human motions are highly diverse and have a property of quite different distribution from conditional modalities, such as textual descriptors in natural languages, it is hard to learn a probabilistic mapping from the desired conditional modality to the human motion sequences. Besides, the raw motion data from the motion capture system might be redundant in sequences and contain noises; directly modeling the joint distribution over the raw motion sequences and conditional modalities would need a heavy computational over-head and might result in artifacts introduced by the captured noises. To learn a better representation of the various human motion sequences, we first design a powerful Variational AutoEncoder (VAE) and arrive at a representative and low-dimensional latent code for a human motion sequence. Then, instead of using a diffusion model to establish the connections between the raw motion sequences and the conditional inputs, we perform a diffusion process on the motion latent space. Our proposed Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences conforming to the given conditional inputs and substantially reduce the computational overhead in both the training and inference stages. Extensive experiments on various human motion generation tasks demonstrate that our MLD achieves significant improvements over the state-of-the-art methods among extensive human motion generation tasks, with two orders of magnitude faster than previous diffusion models on raw motion sequences.",
        "authors": [
            "Xin Chen",
            "Biao Jiang",
            "Wen Liu",
            "Zilong Huang",
            "Bin Fu",
            "Tao Chen",
            "Jingyi Yu",
            "Gang Yu"
        ],
        "citations": 246,
        "references": 82,
        "year": 2022
    },
    {
        "title": "Water transport in reverse osmosis membranes is governed by pore flow, not a solution-diffusion mechanism",
        "abstract": "We performed nonequilibrium molecular dynamics (NEMD) simulations and solvent permeation experiments to unravel the mechanism of water transport in reverse osmosis (RO) membranes. The NEMD simulations reveal that water transport is driven by a pressure gradient within the membranes, not by a water concentration gradient, in marked contrast to the classic solution-diffusion model. We further show that water molecules travel as clusters through a network of pores that are transiently connected. Permeation experiments with water and organic solvents using polyamide and cellulose triacetate RO membranes showed that solvent permeance depends on the membrane pore size, kinetic diameter of solvent molecules, and solvent viscosity. This observation is not consistent with the solution-diffusion model, where permeance depends on the solvent solubility. Motivated by these observations, we demonstrate that the solution-friction model, in which transport is driven by a pressure gradient, can describe water and solvent transport in RO membranes.",
        "authors": [
            "Li Wang",
            "Jinlong He",
            "M. Heiranian",
            "Hanqing Fan",
            "Lianfa Song",
            "Ying Li",
            "M. Elimelech"
        ],
        "citations": 104,
        "references": 94,
        "year": 2023
    },
    {
        "title": "DiffusionDet: Diffusion Model for Object Detection",
        "abstract": "We propose DiffusionDet, a new framework that formulates object detection as a denoising diffusion process from noisy boxes to object boxes. During the training stage, object boxes diffuse from ground-truth boxes to random distribution, and the model learns to reverse this noising process. In inference, the model refines a set of randomly generated boxes to the output results in a progressive way. Our work possesses an appealing property of flexibility, which enables the dynamic number of boxes and iterative evaluation. The extensive experiments on the standard benchmarks show that DiffusionDet achieves favorable performance compared to previous well-established detectors. For example, DiffusionDet achieves 5.3 AP and 4.8 AP gains when evaluated with more boxes and iteration steps, under a zero-shot transfer setting from COCO to CrowdHuman. Our code is available at https://github.com/ShoufaChen/DiffusionDet.",
        "authors": [
            "Shoufa Chen",
            "Pei Sun",
            "Yibing Song",
            "P. Luo"
        ],
        "citations": 360,
        "references": 118,
        "year": 2022
    },
    {
        "title": "DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion",
        "abstract": "Multi-modality image fusion aims to combine different modalities to produce fused images that retain the complementary features of each modality, such as functional highlights and texture details. To leverage strong generative priors and address challenges such as unstable training and lack of interpretability for GAN-based generative methods, we propose a novel fusion algorithm based on the denoising diffusion probabilistic model (DDPM). The fusion task is formulated as a conditional generation problem under the DDPM sampling framework, which is further divided into an unconditional generation subproblem and a maximum likelihood subproblem. The latter is modeled in a hierarchical Bayesian manner with latent variables and inferred by the expectation-maximization (EM) algorithm. By integrating the inference solution into the diffusion sampling iteration, our method can generate high-quality fused images with natural image generative priors and cross-modality information from source images. Note that all we required is an unconditional pre-trained generative model, and no fine-tuning is needed. Our extensive experiments indicate that our approach yields promising fusion results in infrared-visible image fusion and medical image fusion. The code is available at https://github.com/Zhaozixiang1228/MMIF-DDFM.",
        "authors": [
            "Zixiang Zhao",
            "Hao Bai",
            "Yuanzhi Zhu",
            "Jiangshe Zhang",
            "Shuang Xu",
            "Yulun Zhang",
            "K. Zhang",
            "Deyu Meng",
            "R. Timofte",
            "L. Gool"
        ],
        "citations": 93,
        "references": 77,
        "year": 2023
    },
    {
        "title": "MotionDiffuser: Controllable Multi-Agent Motion Prediction Using Diffusion",
        "abstract": "We present MotionDiffuser, a diffusion based representation for the joint distribution of future trajectories over multiple agents. Such representation has several key advantages: first, our model learns a highly multimodal distribution that captures diverse future outcomes. Second, the simple predictor design requires only a single L2 loss training objective, and does not depend on trajectory anchors. Third, our model is capable of learning the joint distribution for the motion of multiple agents in a permutation-invariant manner. Furthermore, we utilize a compressed trajectory representation via PCA, which improves model performance and allows for efficient computation of the exact sample log probability. Subsequently, we propose a general constrained sampling framework that enables controlled trajectory sampling based on differentiable cost functions. This strategy enables a host of applications such as enforcing rules and physical priors, or creating tailored simulation scenarios. MotionDiffuser can be combined with existing backbone architectures to achieve top motion forecasting results. We obtain state-of-the-art results for multi-agent motion prediction on the Waymo Open Motion Dataset.",
        "authors": [
            "C. Jiang",
            "Andre Cornman",
            "C. Park",
            "Benjamin Sapp",
            "Yin Zhou",
            "Drago Anguelov"
        ],
        "citations": 94,
        "references": 57,
        "year": 2023
    },
    {
        "title": "SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction",
        "abstract": "Recently video generation has achieved substantial progress with realistic results. Nevertheless, existing AI-generated videos are usually very short clips (\"shot-level\") depicting a single scene. To deliver a coherent long video (\"story-level\"), it is desirable to have creative transition and prediction effects across different clips. This paper presents a short-to-long video diffusion model, SEINE, that focuses on generative transition and prediction. The goal is to generate high-quality long videos with smooth and creative transitions between scenes and varying lengths of shot-level videos. Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions. By providing the images of different scenes as inputs, combined with text-based control, our model generates transition videos that ensure coherence and visual quality. Furthermore, the model can be readily extended to various tasks such as image-to-video animation and autoregressive video prediction. To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment. Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos. Project page: https://vchitect.github.io/SEINE-project/ .",
        "authors": [
            "Xinyuan Chen",
            "Yaohui Wang",
            "Lingjun Zhang",
            "Shaobin Zhuang",
            "Xin Ma",
            "Jiashuo Yu",
            "Yali Wang",
            "Dahua Lin",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "citations": 84,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement",
        "abstract": "Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.",
        "authors": [
            "Y. Li",
            "Xin-xin Lu",
            "Yaqing Wang",
            "De-Yu Dou"
        ],
        "citations": 80,
        "references": 110,
        "year": 2023
    },
    {
        "title": "Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion",
        "abstract": "We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page.",
        "authors": [
            "Davis Rempe",
            "Zhengyi Luo",
            "X. B. Peng",
            "Ye Yuan",
            "Kris Kitani",
            "Karsten Kreis",
            "S. Fidler",
            "O. Litany"
        ],
        "citations": 82,
        "references": 77,
        "year": 2023
    },
    {
        "title": "EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution",
        "abstract": "Recently, convolutional networks have achieved remarkable development in remote sensing image (RSI) super-resolution (SR) by minimizing the regression objectives, e.g., MSE loss. However, despite achieving impressive performance, these methods often suffer from poor visual quality with oversmooth issues. Generative adversarial networks (GANs) have the potential to infer intricate details, but they are easy to collapse, resulting in undesirable artifacts. To mitigate these issues, in this article, we first introduce diffusion probabilistic model (DPM) for efficient RSI SR, dubbed efficient diffusion model for RSI SR (EDiffSR). EDiffSR is easy to train and maintains the merits of DPM in generating perceptual-pleasant images. Specifically, different from previous works using heavy UNet for noise prediction, we develop an efficient activation network (EANet) to achieve favorable noise prediction performance by simplified channel attention and simple gate operation, which dramatically reduces the computational budget. Moreover, to introduce more valuable prior knowledge into the proposed EDiffSR, a practical conditional prior enhancement module (CPEM) is developed to help extract an enriched condition. Unlike most DPM-based SR models that directly generate conditions by amplifying LR images, the proposed CPEM helps to retain more informative cues for accurate SR. Extensive experiments on four remote sensing datasets demonstrate that EDiffSR can restore visual-pleasant images on simulated and real-world RSIs, both quantitatively and qualitatively. The code of EDiffSR will be available at https://github.com/XY-boy/EDiffSR.",
        "authors": [
            "Yi Xiao",
            "Qiangqiang Yuan",
            "Kui Jiang",
            "Jiang He",
            "Xianyu Jin",
            "Liangpei Zhang"
        ],
        "citations": 80,
        "references": 73,
        "year": 2023
    },
    {
        "title": "InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion",
        "abstract": "This paper addresses a novel task of anticipating 3D human-object interactions (HOIs). Most existing research on HOI synthesis lacks comprehensive whole-body interactions with dynamic objects, e.g., often limited to manipulating small or static objects. Our task is significantly more challenging, as it requires modeling dynamic objects with various shapes, capturing whole-body motion, and ensuring physically valid interactions. To this end, we propose InterDiff, a framework comprising two key steps: (i) interaction diffusion, where we leverage a diffusion model to encode the distribution of future human-object interactions; (ii) interaction correction, where we introduce a physics-informed predictor to correct denoised HOIs in a diffusion step. Our key insight is to inject prior knowledge that the interactions under reference with respect to contact points follow a simple pattern and are easily predictable. Experiments on multiple human-object interaction datasets demonstrate the effectiveness of our method for this task, capable of producing realistic, vivid, and remarkably longterm 3D HOI predictions.",
        "authors": [
            "Sirui Xu",
            "Zhengyu Li",
            "Yu-Xiong Wang",
            "Liangyan Gui"
        ],
        "citations": 75,
        "references": 136,
        "year": 2023
    },
    {
        "title": "InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions",
        "abstract": null,
        "authors": [
            "Hanming Liang",
            "Wenqian Zhang",
            "Wenxu Li",
            "Jingyi Yu",
            "Lan Xu"
        ],
        "citations": 71,
        "references": 110,
        "year": 2023
    },
    {
        "title": "DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion",
        "abstract": "Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t.~a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instance numbers, and an advanced version for learning complex structures. Experiments highlight the wide applicability of our model as a general-purpose encoder backbone with superior performance in various tasks, such as node classification on large graphs, semi-supervised image/text classification, and spatial-temporal dynamics prediction.",
        "authors": [
            "Qitian Wu",
            "Chenxiao Yang",
            "Wen-Long Zhao",
            "Yixuan He",
            "David Wipf",
            "Junchi Yan"
        ],
        "citations": 72,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model",
        "abstract": "In this paper, we rethink the low-light image enhancement task and propose a physically explainable and generative diffusion model for low-light image enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the physical model and the generative network. Furthermore, we hope to supplement and even deduce the information missing in the low-light image through the generative network. Therefore, Diff-Retinex formulates the lowlight image enhancement problem into Retinex decomposition and conditional image generation. In the Retinex decomposition, we integrate the superiority of attention in Transformer and meticulously design a Retinex Transformer decomposition network (TDN) to decompose the image into illumination and reflectance maps. Then, we design multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution and solve the various degradations in these components respectively, including dark illumination, noise, color deviation, loss of scene contents, etc. Owing to generative diffusion model, Diff-Retinex puts the restoration of low-light subtle detail into practice. Extensive experiments conducted on real-world low-light datasets qualitatively and quantitatively demonstrate the effectiveness, superiority, and generalization of the proposed method.",
        "authors": [
            "Xunpeng Yi",
            "Han Xu",
            "H. Zhang",
            "Linfeng Tang",
            "Jiayi Ma"
        ],
        "citations": 69,
        "references": 46,
        "year": 2023
    },
    {
        "title": "HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation",
        "abstract": "Controllable human image generation (HIG) has numerous real-life applications. State-of-the-art solutions, such as ControlNet and T2I-Adapter, introduce an additional learnable branch on top of the frozen pre-trained stable diffusion (SD) model, which can enforce various conditions, including skeleton guidance of HIG. While such a plug-and-play approach is appealing, the inevitable and uncertain conflicts between the original images produced from the frozen SD branch and the given condition incur significant challenges for the learnable branch, which essentially conducts image feature editing for condition enforcement.In this work, we propose a native skeleton-guided diffusion model for controllable HIG called HumanSD. Instead of performing image editing with dual-branch diffusion, we fine-tune the original SD model using a novel heatmap-guided denoising loss. This strategy effectively and efficiently strengthens the given skeleton condition during model training while mitigating the catastrophic forgetting effects. HumanSD is fine-tuned on the assembly of three large-scale human-centric datasets with text-image-pose information, two of which are established in this work. Experimental results show that HumanSD outperforms ControlNet in terms of pose control and image quality, particularly when the given skeleton guidance is sophisticated. Code and data are available at: https://idea-research.github.io/HumanSD/.",
        "authors": [
            "Xu Ju",
            "Ailing Zeng",
            "Chenchen Zhao",
            "Jianan Wang",
            "Lei Zhang",
            "Qian Xu"
        ],
        "citations": 63,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Stable VITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On",
        "abstract": "Given a clothing image and a person image, an image-based virtual try-on aims to generate a customized image that appears natural and accurately reflects the character-istics of the clothing image. In this work, we aim to expand the applicability of the pre-trained diffusion model so that it can be utilized independently for the virtual try-on task. The main challenge is to preserve the clothing details while effectively utilizing the robust generative capability of the pre-trained model. In order to tackle these issues, we propose StableVITON, learning the semantic correspon-dence between the clothing and the human body within the latent space of the pre-trained diffusion model in an end-to-end manner. Our proposed zero cross-attention blocks not only preserve the clothing details by learning the semantic correspondence but also generate high-fidelity images by utilizing the inherent knowledge of the pre-trained model in the warping process. Through our proposed novel attention total variation loss and applying augmentation, we achieve the sharp attention map, resulting in a more precise representation of clothing details. Stable VITON out-performs the baselines in qualitative and quantitative evaluation, showing promising quality in arbitrary person images. Our code is available at https://github.com/rlawjdghek/StableVITON.",
        "authors": [
            "Jeongho Kim",
            "Gyojung Gu",
            "Minho Park",
            "S. Park",
            "J. Choo"
        ],
        "citations": 58,
        "references": 39,
        "year": 2023
    },
    {
        "title": "ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution",
        "abstract": "Adapting the Diffusion Probabilistic Model (DPM) for direct image super-resolution is wasteful, given that a simple Convolutional Neural Network (CNN) can recover the main low-frequency content. Therefore, we present ResDiff, a novel Diffusion Probabilistic Model based on Residual structure for Single Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN, which restores primary low-frequency components, and a DPM, which predicts the residual between the ground-truth image and the CNN predicted image. In contrast to the common diffusion-based methods that directly use LR space to guide the noise towards HR space, ResDiff utilizes the CNN’s initial prediction to direct the noise towards the residual space between HR space and CNN-predicted space, which not only accelerates the generation process but also acquires superior sample quality. Additionally, a frequency-domain-based loss function for CNN is introduced to facilitate its restoration, and a frequency-domain guided diffusion is designed for DPM on behalf of predicting high-frequency details. The extensive experiments on multiple benchmark datasets demonstrate that ResDiff outperforms previous diffusion based methods in terms of shorter model convergence time, superior generation quality, and more diverse samples.",
        "authors": [
            "Shuyao Shang",
            "Zhengyang Shan",
            "Guangxing Liu",
            "Jingling Zhang"
        ],
        "citations": 56,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model",
        "abstract": "With the recent surge in popularity of AR/VR applications, realistic and accurate control of 3D full-body avatars has become a highly demanded feature. A particular challenge is that only a sparse tracking signal is available from standalone HMDs (Head Mounted Devices), often limited to tracking the user's head and wrists. While this signal is resourceful for reconstructing the upper body motion, the lower body is not tracked and must be synthesized from the limited information provided by the upper body joints. In this paper, we present AGRoL, a novel conditional diffusion model specifically designed to track full bodies given sparse upper-body tracking signals. Our model is based on a simple multi-layer perceptron (MLP) architecture and a novel conditioning scheme for motion data. It can predict accurate and smooth full-body motion, particularly the challenging lower body movement. Unlike common diffusion architectures, our compact architecture can run in real-time, making it suitable for online body-tracking applications. We train and evaluate our model on AMASS motion capture dataset, and demonstrate that our approach outperforms state-of-the-art methods in generated motion accuracy and smoothness. We further justify our design choices through extensive experiments and ablation studies.",
        "authors": [
            "Yuming Du",
            "Robin Kips",
            "Albert Pumarola",
            "S. Starke",
            "Ali K. Thabet",
            "A. Sanakoyeu"
        ],
        "citations": 59,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Towards Universal Fake Image Detectors that Generalize Across Generative Models",
        "abstract": "With generative models proliferating at a rapid rate, there is a growing need for general purpose fake image detectors. In this work, we first show that the existing paradigm, which consists of training a deep network for real-vs-fake classification, fails to detect fake images from newer breeds of generative models when trained to detect GAN fake images. Upon analysis, we find that the resulting classifier is asymmetrically tuned to detect patterns that make an image fake. The real class becomes a ‘sink’ class holding anything that is not fake, including generated images from models not accessible during training. Building upon this discovery, we propose to perform real-vs-fake classification without learning; i.e., using a feature space not explicitly trained to distinguish real from fake images. We use nearest neighbor and linear probing as instantiations of this idea. When given access to the feature space of a large pretrained vision-language model, the very simple baseline of nearest neighbor classification has surprisingly good generalization ability in detecting fake images from a wide variety of generative models; e.g., it improves upon the SoTA [50] by +15.07 mAP and +25.90% acc when tested on unseen diffusion and autoregressive models. Our code, models, and data can be found at https://github.com/Yuheng-Li/UniversalFakeDetect",
        "authors": [
            "Utkarsh Ojha",
            "Yuheng Li",
            "Yong Jae Lee"
        ],
        "citations": 136,
        "references": 49,
        "year": 2023
    },
    {
        "title": "RODIN: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion",
        "abstract": "This paper presents a 3D diffusion model that automatically generates 3D digital avatars represented as neural radiance fields (NeRFs). A significant challenge for 3D diffusion is that the memory and processing costs are prohibitive for producing high-quality results with rich details. To tackle this problem, we propose the roll-out diffusion network (RODIN), which takes a 3D NeRF model represented as multiple 2D feature maps and rolls out them onto a single 2D feature plane within which we perform 3D-aware diffusion. The RODIN model brings much-needed computational efficiency while preserving the integrity of 3D diffusion by using 3D-aware convolution that attends to projected features in the 2D plane according to their original relationships in 3D. We also use latent conditioning to orchestrate the feature generation with global coherence, leading to high-fidelity avatars and enabling semantic editing based on text prompts. Finally, we use hierarchical synthesis to further enhance details. The 3D avatars generated by our model compare favorably with those produced by existing techniques. We can generate highly detailed avatars with realistic hairstyles and facial hair. We also demonstrate 3D avatar generation from image or text, as well as text-guided editability.",
        "authors": [
            "Tengfei Wang",
            "Bo Zhang",
            "Ting Zhang",
            "Shuyang Gu",
            "Jianmin Bao",
            "T. Baltrušaitis",
            "Jingjing Shen",
            "Dong Chen",
            "Fang Wen",
            "Qifeng Chen",
            "B. Guo"
        ],
        "citations": 240,
        "references": 83,
        "year": 2022
    },
    {
        "title": "Texture Generation on 3D Meshes with Point-UV Diffusion",
        "abstract": "In this work, we focus on synthesizing high-quality textures on 3D meshes. We present Point-UV diffusion, a coarse-to-fine pipeline that marries the denoising diffusion model with UV mapping to generate 3D consistent and high-quality texture images in UV space. We start with introducing a point diffusion model to synthesize low-frequency texture components with our tailored style guidance to tackle the biased color distribution. The derived coarse texture offers global consistency and serves as a condition for the subsequent UV diffusion stage, aiding in regularizing the model to generate a 3D consistent UV texture image. Then, a UV diffusion model with hybrid conditions is developed to enhance the texture fidelity in the 2D UV space. Our method can process meshes of any genus, generating diversified, geometry-compatible, and high-fidelity textures. Code is available at https://cvmi-lab.github.io/Point-UV-Diffusion.",
        "authors": [
            "Xin Yu",
            "Peng Dai",
            "Wenbo Li",
            "Lan Ma",
            "Zhengzhe Liu",
            "Xiaojuan Qi"
        ],
        "citations": 38,
        "references": 57,
        "year": 2023
    },
    {
        "title": "VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation",
        "abstract": "In this paper, we present VideoGen, a text-to-video generation approach, which can generate a high-definition video with high frame fidelity and strong temporal consistency using reference-guided latent diffusion. We leverage an off-the-shelf text-to-image generation model, e.g., Stable Diffusion, to generate an image with high content quality from the text prompt, as a reference image to guide video generation. Then, we introduce an efficient cascaded latent diffusion module conditioned on both the reference image and the text prompt, for generating latent video representations, followed by a flow-based temporal upsampling step to improve the temporal resolution. Finally, we map latent video representations into a high-definition video through an enhanced video decoder. During training, we use the first frame of a ground-truth video as the reference image for training the cascaded latent diffusion module. The main characterises of our approach include: the reference image generated by the text-to-image model improves the visual fidelity; using it as the condition makes the diffusion model focus more on learning the video dynamics; and the video decoder is trained over unlabeled video data, thus benefiting from high-quality easily-available videos. VideoGen sets a new state-of-the-art in text-to-video generation in terms of both qualitative and quantitative evaluation. See \\url{https://videogen.github.io/VideoGen/} for more samples.",
        "authors": [
            "Xin Li",
            "Wenqing Chu",
            "Ye Wu",
            "Weihang Yuan",
            "Fanglong Liu",
            "Qi Zhang",
            "Fu Li",
            "Haocheng Feng",
            "Errui Ding",
            "Jingdong Wang"
        ],
        "citations": 41,
        "references": 68,
        "year": 2023
    },
    {
        "title": "AnomalyDiffusion: Few-Shot Anomaly Image Generation with Diffusion Model",
        "abstract": "Anomaly inspection plays an important role in industrial manufacture. Existing anomaly inspection methods are limited in their performance due to insufficient anomaly data. Although anomaly generation methods have been proposed to augment the anomaly data, they either suffer from poor generation authenticity or inaccurate alignment between the generated anomalies and masks. To address the above problems, we propose AnomalyDiffusion, a novel diffusion-based few-shot anomaly generation model, which utilizes the strong prior information of latent diffusion model learned from large-scale dataset to enhance the generation authenticity under few-shot training data. Firstly, we propose Spatial Anomaly Embedding, which consists of a learnable anomaly embedding and a spatial embedding encoded from an anomaly mask, disentangling the anomaly information into anomaly appearance and location information. Moreover, to improve the alignment between the generated anomalies and the anomaly masks, we introduce a novel Adaptive Attention Re-weighting Mechanism. Based on the disparities between the generated anomaly image and normal sample, it dynamically guides the model to focus more on the areas with less noticeable generated anomalies, enabling generation of accurately-matched anomalous image-mask pairs. Extensive experiments demonstrate that our model significantly outperforms the state-of-the-art methods in generation authenticity and diversity, and effectively improves the performance of downstream anomaly inspection tasks. The code and data are available in https://github.com/sjtuplayer/anomalydiffusion.",
        "authors": [
            "Teng Hu",
            "Jiangning Zhang",
            "Ran Yi",
            "Yuzhen Du",
            "Xu Chen",
            "Liang Liu",
            "Yabiao Wang",
            "Chengjie Wang"
        ],
        "citations": 36,
        "references": 48,
        "year": 2023
    },
    {
        "title": "CBCT-Based synthetic CT image generation using conditional denoising diffusion probabilistic model.",
        "abstract": "BACKGROUND\nDaily or weekly cone-beam computed tomography (CBCT) scans are commonly used for accurate patient positioning during the image-guided radiotherapy (IGRT) process, making it an ideal option for adaptive radiotherapy (ART) replanning. However, the presence of severe artifacts and inaccurate Hounsfield unit (HU) values prevent its use for quantitative applications such as organ segmentation and dose calculation. To enable the clinical practice of online ART, it is crucial to obtain CBCT scans with a quality comparable to that of a CT scan.\n\n\nPURPOSE\nThis work aims to develop a conditional diffusion model to perform image translation from the CBCT to the CT distribution for the image quality improvement of CBCT.\n\n\nMETHODS\nThe proposed method is a conditional denoising diffusion probabilistic model (DDPM) that utilizes a time-embedded U-net architecture with residual and attention blocks to gradually transform the white Gaussian noise sample to the target CT distribution conditioned on the CBCT. The model was trained on deformed planning CT (dpCT) and CBCT image pairs, and its feasibility was verified in brain patient study and head-and-neck (H&N) patient study. The performance of the proposed algorithm was evaluated using mean absolute error (MAE), peak signal-to-noise ratio (PSNR) and normalized cross-correlation (NCC) metrics on generated synthetic CT (sCT) samples. The proposed method was also compared to four other diffusion model-based sCT generation methods.\n\n\nRESULTS\nIn the brain patient study, the MAE, PSNR, and NCC of the generated sCT were 25.99 HU, 30.49 dB, and 0.99, respectively, compared to 40.63 HU, 27.87 dB, and 0.98 of the CBCT images. In the H&N patient study, the metrics were 32.56 HU, 27.65 dB, 0.98 and 38.99 HU, 27.00, 0.98 for sCT and CBCT, respectively. Compared to the other four diffusion models and one Cycle generative adversarial network (Cycle GAN), the proposed method showed superior results in both visual quality and quantitative analysis.\n\n\nCONCLUSIONS\nThe proposed conditional DDPM method can generate sCT from CBCT with accurate HU numbers and reduced artifacts, enabling accurate CBCT-based organ segmentation and dose calculation for online ART.",
        "authors": [
            "Junbo Peng",
            "Richard L. J. Qiu",
            "J. Wynne",
            "Chih-Wei Chang",
            "Shaoyan Pan",
            "Tonghe Wang",
            "J. Roper",
            "Tian Liu",
            "P. Patel",
            "D. Yu",
            "Xiaofeng Yang"
        ],
        "citations": 52,
        "references": 49,
        "year": 2023
    },
    {
        "title": "SimDA: Simple Diffusion Adapter for Efficient Video Generation",
        "abstract": "The recent wave of AI-generated content has witnessed the great development and success of Text-to-Image (T2I) technologies. By contrast, Text-to- Video (T2V) still falls short of expectations though attracting increasing interest. Existing works either train from scratch or adapt large T2I model to videos, both of which are computation and re-source expensive. In this work, we propose a Simple Dif-fusion Adapter (SimDA) that fine-tunes only 24M out of I.IB parameters of a strong T2I model, adapting it to video generation in a parameter-efficient way. In particular, we turn the T2I model for T2V by designing light-weight spatial and temporal adapters for transfer learning. Besides, we change the original spatial attention to the proposed Latent-Shift Attention (LSA) for temporal consistency. With a similar model architecture, we further train a video super-resolution model to generate high-definition (1024 x 1024) videos. In addition to T2V generation in the wild, SimDA could also be utilized in one-shot video editing with only 2 minutes tuning. Doing so, our method could minimize the training effort with extremely few tunable parameters for model adaptation.",
        "authors": [
            "Zhen Xing",
            "Qi Dai",
            "Hang-Rui Hu",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "citations": 59,
        "references": 139,
        "year": 2023
    },
    {
        "title": "Diffsound: Discrete Diffusion Model for Text-to-Sound Generation",
        "abstract": "Generating sound effects that people want is an important topic. However, there are limited studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a token-decoder, and a vocoder. The framework first uses the token-decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the token-decoder significantly influences the generation performance. Thus, we focus on designing a good token-decoder in this study. We begin with the traditional autoregressive (AR) token-decoder. However, the AR token-decoder always predicts the mel-spectrogram tokens one by one in order, which may introduce the unidirectional bias and accumulation of errors problems. Moreover, with the AR token-decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR token-decoders, we propose a non-autoregressive token-decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound model predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained by iteration. Our experiments show that our proposed Diffsound model not only produces better generation results when compared with the AR token-decoder but also has a faster generation speed, i.e., MOS: 3.56 v.s 2.786.",
        "authors": [
            "Dongchao Yang",
            "Jianwei Yu",
            "Helin Wang",
            "Wen Wang",
            "Chao Weng",
            "Yuexian Zou",
            "Dong Yu"
        ],
        "citations": 257,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Rethinking of the intraparticle diffusion adsorption kinetics model: Interpretation, solving methods and applications.",
        "abstract": null,
        "authors": [
            "Jianlong Wang",
            "Xuan Guo"
        ],
        "citations": 261,
        "references": 81,
        "year": 2022
    },
    {
        "title": "Large-Vocabulary 3D Diffusion Model with Transformer",
        "abstract": "Creating diverse and high-quality 3D assets with an automatic generative model is highly desirable. Despite extensive efforts on 3D generation, most existing works focus on the generation of a single category or a few categories. In this paper, we introduce a diffusion-based feed-forward framework for synthesizing massive categories of real-world 3D objects with a single generative model. Notably, there are three major challenges for this large-vocabulary 3D generation: a) the need for expressive yet efficient 3D representation; b) large diversity in geometry and texture across categories; c) complexity in the appearances of real-world objects. To this end, we propose a novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for handling challenges via three aspects. 1) Considering efficiency and robustness, we adopt a revised triplane representation and improve the fitting speed and accuracy. 2) To handle the drastic variations in geometry and texture, we regard the features of all 3D objects as a combination of generalized 3D knowledge and specialized 3D features. To extract generalized 3D knowledge from diverse categories, we propose a novel 3D-aware transformer with shared cross-plane attention. It learns the cross-plane relations across different planes and aggregates the generalized 3D knowledge with specialized 3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance the generalized 3D knowledge in the encoded triplanes for handling categories with complex appearances. Extensive experiments on ShapeNet and OmniObject3D (over 200 diverse real-world categories) convincingly demonstrate that a single DiffTF model achieves state-of-the-art large-vocabulary 3D object generation performance with large diversity, rich semantics, and high quality.",
        "authors": [
            "Ziang Cao",
            "Fangzhou Hong",
            "Tong Wu",
            "Liang Pan",
            "Ziwei Liu"
        ],
        "citations": 30,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Unifying Layout Generation with a Decoupled Diffusion Model",
        "abstract": "Layout generation aims to synthesize realistic graphic scenes consisting of elements with different attributes in-cluding category, size, position, and between-element relation. It is a crucial task for reducing the burden on heavyduty graphic design works for formatted scenes, e.g., publications, documents, and user interfaces (UIs). Diverse application scenarios impose a big challenge in unifying various layout generation subtasks, including conditional and unconditional generation. In this paper, we propose a Layout Diffusion Generative Model (LDGM) to achieve such unification with a single decoupled diffusion model. LDGM views a layout of arbitrary missing or coarse element attributes as an intermediate diffusion status from a completed layout. Since different attributes have their individual semantics and characteristics, we propose to decouple the diffusion processes for them to improve the diversity of training samples and learn the reverse process jointly to exploit global-scope contexts for facilitating generation. As a result, our LDGM can generate layouts either from scratch or conditional on arbitrary available attributes. Extensive qualitative and quantitative experiments demonstrate our proposed LDGM outperforms existing layout generation models in both functionality and performance.",
        "authors": [
            "Mude Hui",
            "Zhizheng Zhang",
            "Xiaoyi Zhang",
            "Wenxuan Xie",
            "Yuwang Wang",
            "Yan Lu"
        ],
        "citations": 28,
        "references": 34,
        "year": 2023
    },
    {
        "title": "CLE Diffusion: Controllable Light Enhancement Diffusion Model",
        "abstract": "Low light enhancement has gained increasing importance with the rapid development of visual creation and editing. However, most existing enhancement algorithms are designed to homogeneously increase the brightness of images to a pre-defined extent, limiting the user experience. To address this issue, we propose Controllable Light Enhancement Diffusion Model, dubbed CLE Diffusion, a novel diffusion framework to provide users with rich controllability.Built with a conditional diffusion model, we introduce an illumination embedding to let users control their desired brightness level. Additionally, we incorporate the Segment-Anything Model (SAM) to enable user-friendly region controllability, where users can click on objects to specify the regions they wish to enhance. Extensive experiments demonstrate that CLE Diffusion achieves competitive performance regarding quantitative metrics, qualitative results, and versatile controllability. Project page: https://yuyangyin.github.io/CLEDiffusion",
        "authors": [
            "Yuyang Yin",
            "Dejia Xu",
            "Chuangchuang Tan",
            "P. Liu",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "citations": 28,
        "references": 75,
        "year": 2023
    },
    {
        "title": "MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL",
        "abstract": "Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning(RL). However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.",
        "authors": [
            "Fei Ni",
            "Jianye Hao",
            "Yao Mu",
            "Yifu Yuan",
            "Yan Zheng",
            "Bin Wang",
            "Zhixuan Liang"
        ],
        "citations": 35,
        "references": 45,
        "year": 2023
    },
    {
        "title": "A Latent Diffusion Model for Protein Structure Generation",
        "abstract": "Proteins are complex biomolecules that perform a variety of crucial functions within living organisms. Designing and generating novel proteins can pave the way for many future synthetic biology applications, including drug discovery. However, it remains a challenging computational task due to the large modeling space of protein structures. In this study, we propose a latent diffusion model that can reduce the complexity of protein modeling while flexibly capturing the distribution of natural protein structures in a condensed latent space. Specifically, we propose an equivariant protein autoencoder that embeds proteins into a latent space and then uses an equivariant diffusion model to learn the distribution of the latent protein representations. Experimental results demonstrate that our method can effectively generate novel protein backbone structures with high designability and efficiency. The code will be made publicly available at https://github.com/divelab/AIRS/tree/main/OpenProt/LatentDiff",
        "authors": [
            "Cong Fu",
            "Keqiang Yan",
            "Limei Wang",
            "Wing Yee Au",
            "Michael McThrow",
            "Tao Komikado",
            "Koji Maruhashi",
            "Kanji Uchino",
            "Xiaoning Qian",
            "Shuiwang Ji"
        ],
        "citations": 24,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Policy Representation via Diffusion Probability Model for Reinforcement Learning",
        "abstract": "Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.",
        "authors": [
            "Long Yang",
            "Zhixiong Huang",
            "Fenghao Lei",
            "Yucun Zhong",
            "Yiming Yang",
            "Cong Fang",
            "Shiting Wen",
            "Binbin Zhou",
            "Zhouchen Lin"
        ],
        "citations": 23,
        "references": 98,
        "year": 2023
    },
    {
        "title": "LDM3D: Latent Diffusion Model for 3D",
        "abstract": "This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that generates both image and depth map data from a given text prompt, allowing users to generate RGBD images from text prompts. The LDM3D model is fine-tuned on a dataset of tuples containing an RGB image, depth map and caption, and validated through extensive experiments. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360-degree-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize content creation and digital experiences. A short video summarizing the approach can be found at https://t.ly/tdi2.",
        "authors": [
            "Gabriela Ben Melech Stan",
            "Diana Wofk",
            "Scottie Fox",
            "Alex Redden",
            "Will Saxton",
            "Jean Yu",
            "Estelle Aflalo",
            "Shao-Yen Tseng",
            "Fabio Nonato",
            "Matthias Müller",
            "Vasudev Lal"
        ],
        "citations": 33,
        "references": 31,
        "year": 2023
    },
    {
        "title": "A Diffusion Model for POI Recommendation",
        "abstract": "Next Point-of-Interest (POI) recommendation is a critical task in location-based services that aim to provide personalized suggestions for the user’s next destination. Previous works on POI recommendation have laid focus on modeling the user’s spatial preference. However, existing works that leverage spatial information are only based on the aggregation of users’ previous visited positions, which discourages the model from recommending POIs in novel areas. This trait of position-based methods will harm the model’s performance in many situations. Additionally, incorporating sequential information into the user’s spatial preference remains a challenge. In this article, we propose Diff-POI: a Diffusion-based model that samples the user’s spatial preference for the next POI recommendation. Inspired by the wide application of diffusion algorithm in sampling from distributions, Diff-POI encodes the user’s visiting sequence and spatial character with two tailor-designed graph encoding modules, followed by a diffusion-based sampling strategy to explore the user’s spatial visiting trends. We leverage the diffusion process and its reverse form to sample from the posterior distribution and optimized the corresponding score function. We design a joint training and inference framework to optimize and evaluate the proposed Diff-POI. Extensive experiments on four real-world POI recommendation datasets demonstrate the superiority of our Diff-POI over state-of-the-art baseline methods. Further ablation and parameter studies on Diff-POI reveal the functionality and effectiveness of the proposed diffusion-based sampling strategy for addressing the limitations of existing methods.",
        "authors": [
            "Yifang Qin",
            "Hongjun Wu",
            "Wei Ju",
            "Xiao Luo",
            "Ming Zhang"
        ],
        "citations": 28,
        "references": 72,
        "year": 2023
    },
    {
        "title": "DiffKG: Knowledge Graph Diffusion Model for Recommendation",
        "abstract": "Knowledge Graphs (KGs) have emerged as invaluable resources for enriching recommendation systems by providing a wealth of factual information and capturing semantic relationships among items. Leveraging KGs can significantly enhance recommendation performance. However, not all relations within a KG are equally relevant or beneficial for the target recommendation task. In fact, certain item-entity connections may introduce noise or lack informative value, thus potentially misleading our understanding of user preferences. To bridge this research gap, we propose a novel knowledge graph diffusion model for recommendation, referred to as DiffKG. Our framework integrates a generative diffusion model with a data augmentation paradigm, enabling robust knowledge graph representation learning. This integration facilitates a better alignment between knowledge-aware item semantics and collaborative relation modeling. Moreover, we introduce a collaborative knowledge graph convolution mechanism that incorporates collaborative signals reflecting user-item interaction patterns, guiding the knowledge graph diffusion process. We conduct extensive experiments on three publicly available datasets, consistently demonstrating the superiority of our DiffKG compared to various competitive baselines. We provide the source code repository of our proposed DiffKG model at the following link: https://github.com/HKUDS/DiffKG",
        "authors": [
            "Ya Jiang",
            "Yuhao Yang",
            "Lianghao Xia",
            "Chao Huang"
        ],
        "citations": 28,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Guiding Text-to-Image Diffusion Model Towards Grounded Generation",
        "abstract": "The goal of this paper is to augment a pre-trained text-toimage diffusion model with the ability of open-vocabulary objects grounding, i.e., simultaneously generating images and segmentation masks for the corresponding visual entities described in the text prompt. We make the following contributions: (i) we insert a grounding module into the existing diffusion model, that can be trained to align the visual and textual embedding space of the diffusion model with only a small number of object categories; (ii) we propose an automatic pipeline for constructing a dataset, that con* Both the authors have contributed equally to this project. † denotes corresponding author. sists of {image, segmentation mask, text prompt} triplets, to train the proposed grounding module; (iii) we evaluate the performance of open-vocabulary grounding on images generated from the text-to-image diffusion model and show that the module can well segment the objects of categories beyond seen ones at training time, as shown in Fig. 1; (iv) we adopt the guided diffusion model to build a synthetic semantic segmentation dataset, and show that, training a standard segmentation model on such dataset demonstrates competitive performance on zero-shot segmentation (ZS3) benchmark, which opens up new opportunities for adopting the powerful diffusion model for discriminative tasks. 1 ar X iv :2 30 1. 05 22 1v 1 [ cs .C V ] 1 2 Ja n 20 23",
        "authors": [
            "Ziyi Li",
            "Qinye Zhou",
            "Xiaoyun Zhang",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "citations": 24,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Diffusion Pixelation: A Game Diffusion Model of Rumor & Anti-Rumor Inspired by Image Restoration",
        "abstract": "This study is inspired by the current image restoration technology. If we regard the users participating in the rumor as image pixels, similar to social networks, the recovery of pixel data is affected by the pixels themselves and neighbor pixels, then the prediction of user behavior in the rumor diffusion can be regarded as the process of image restoration for pixel-blurred user behavior images. We first propose a Diffusion2pixel algorithm that transforms the user relationship network of topic diffusion into image pixel matrix. To cope with the diversity and complexity of the diffusion feature space, the user relationship network is reduced to a low-rank dense vectorization by representation learning before being pixelated by cutting and diffusion. Second, considering the competitive relationship between rumor and anti-rumor, transition matrix of rumor mutual influences is established by evolutionary game theory. A mutual influence model of rumor and anti-rumor is then proposed. Finally, we combine the transition matrix of rumor mutual influence into a simple prediction method Graph-CNN of rumor and anti-rumor topic diffusion based on dynamic iteration mechanism. Experiments confirmed the proposed model can effectively predict the group diffusion trends of rumor, and reflects the competitive relationship between rumor and anti-rumor.",
        "authors": [
            "Yunpeng Xiao",
            "Zhenhai Huang",
            "Qian Li",
            "Xingyu Lu",
            "Tun Li"
        ],
        "citations": 25,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion",
        "abstract": "Human behavior has the nature of indeterminacy, which requires the pedestrian trajectory prediction system to model the multi-modality of future motion states. Unlike existing stochastic trajectory prediction methods which usually use a latent variable to represent multi-modality, we explicitly simulate the process of human motion variation from indeterminate to determinate. In this paper, we present a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion (MID), in which we progressively discard indeterminacy from all the walkable areas until reaching the desired trajectory. This process is learned with a parameterized Markov chain conditioned by the observed trajectories. We can adjust the length of the chain to control the degree of indeterminacy and balance the diversity and determinacy of the predictions. Specifically, we encode the history behavior information and the social interactions as a state embedding and devise a Transformer-based diffusion model to capture the temporal dependencies of trajectories. Extensive experiments on the human trajectory prediction benchmarks including the Stanford Drone and ETH/UCY datasets demonstrate the superiority of our method. Code is available at https://github.com/gutianpei/MID.",
        "authors": [
            "Tianpei Gu",
            "Guangyi Chen",
            "Junlong Li",
            "Chunze Lin",
            "Yongming Rao",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "citations": 156,
        "references": 55,
        "year": 2022
    },
    {
        "title": "Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem",
        "abstract": "Construction of a scaffold structure that supports a desired motif, conferring protein function, shows promise for the design of vaccines and enzymes. But a general solution to this motif-scaffolding problem remains open. Current machine-learning techniques for scaffold design are either limited to unrealistically small scaffolds (up to length 20) or struggle to produce multiple diverse scaffolds. We propose to learn a distribution over diverse and longer protein backbone structures via an E(3)-equivariant graph neural network. We develop SMCDiff to efficiently sample scaffolds from this distribution conditioned on a given motif; our algorithm is the first to theoretically guarantee conditional samples from a diffusion model in the large-compute limit. We evaluate our designed backbones by how well they align with AlphaFold2-predicted structures. We show that our method can (1) sample scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for a fixed motif.",
        "authors": [
            "Brian L. Trippe",
            "Jason Yim",
            "D. Tischer",
            "Tamara Broderick",
            "D. Baker",
            "R. Barzilay",
            "T. Jaakkola"
        ],
        "citations": 190,
        "references": 53,
        "year": 2022
    },
    {
        "title": "SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction",
        "abstract": "We propose SparseFusion, a sparse view 3D reconstruction approach that unifies recent advances in neural rendering and probabilistic image generation. Existing approaches typically build on neural rendering with reprojected features but fail to generate unseen regions or handle uncertainty under large viewpoint changes. Alternate methods treat this as a (probabilistic) 2D synthesis task, and while they can generate plausible 2D images, they do not infer a consistent underlying 3D. However, we find that this trade-off between 3D consistency and probabilistic image generation does not need to exist. In fact, we show that geometric consistency and generative inference can be complementary in a mode-seeking behavior. By distilling a 3D consistent scene representation from a view-conditioned latent diffusion model, we are able to recover a plausible 3D representation whose renderings are both accurate and realistic. We evaluate our approach across 51 categories in the CO3D dataset and show that it outperforms existing methods, in both distortion and perception metrics, for sparse-view novel view synthesis.",
        "authors": [
            "Zhizhuo Zhou",
            "Shubham Tulsiani"
        ],
        "citations": 177,
        "references": 56,
        "year": 2022
    },
    {
        "title": "SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model",
        "abstract": "Generic image inpainting aims to complete a corrupted image by borrowing surrounding information, which barely generates novel content. By contrast, multi-modal inpainting provides more flexible and useful controls on the inpainted content, e.g., a text prompt can be used to describe an object with richer attributes, and a mask can be used to constrain the shape of the inpainted object rather than being only considered as a missing area. We propose a new diffusion-based model named SmartBrush for completing a missing region with an object using both text and shape-guidance. While previous work such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not support shape guidance and tend to modify background texture surrounding the generated object. Our model incorporates both text and shape guidance with precision control. To preserve the background better, we propose a novel training and sampling strategy by augmenting the diffusion U-net with object-mask prediction. Lastly, we introduce a multi-task training strategy by jointly training inpainting with text-to-image generation to leverage more training data. We conduct extensive experiments showing that our model outperforms all baselines in terms of visual quality, mask controllability, and background preservation.",
        "authors": [
            "Shaoan Xie",
            "Zhifei Zhang",
            "Zhe Lin",
            "T. Hinz",
            "Kun Zhang"
        ],
        "citations": 176,
        "references": 32,
        "year": 2022
    },
    {
        "title": "Diffusion Model-Augmented Behavioral Cloning",
        "abstract": "Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a). Despite the simplicity of modeling the conditional probability with BC, it usually struggles with generalization. While modeling the joint probability can improve generalization performance, the inference procedure is often time-consuming, and the model can suffer from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed Diffusion Model-Augmented Behavioral Cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to optimize both the BC loss (conditional) and our proposed diffusion model loss (joint). DBC outperforms baselines in various continuous control tasks in navigation, robot arm manipulation, dexterous manipulation, and locomotion. We design additional experiments to verify the limitations of modeling either the conditional probability or the joint probability of the expert distribution, as well as compare different generative models. Ablation studies justify the effectiveness of our design choices.",
        "authors": [
            "Hsiang-Chun Wang",
            "Shangcheng Chen",
            "Shao-Hua Sun"
        ],
        "citations": 21,
        "references": 80,
        "year": 2023
    },
    {
        "title": "Generative Models as an Emerging Paradigm in the Chemical Sciences",
        "abstract": "Traditional computational approaches to design chemical species are limited by the need to compute properties for a vast number of candidates, e.g., by discriminative modeling. Therefore, inverse design methods aim to start from the desired property and optimize a corresponding chemical structure. From a machine learning viewpoint, the inverse design problem can be addressed through so-called generative modeling. Mathematically, discriminative models are defined by learning the probability distribution function of properties given the molecular or material structure. In contrast, a generative model seeks to exploit the joint probability of a chemical species with target characteristics. The overarching idea of generative modeling is to implement a system that produces novel compounds that are expected to have a desired set of chemical features, effectively sidestepping issues found in the forward design process. In this contribution, we overview and critically analyze popular generative algorithms like generative adversarial networks, variational autoencoders, flow, and diffusion models. We highlight key differences between each of the models, provide insights into recent success stories, and discuss outstanding challenges for realizing generative modeling discovered solutions in chemical applications.",
        "authors": [
            "Dylan M. Anstine",
            "O. Isayev"
        ],
        "citations": 107,
        "references": 128,
        "year": 2023
    },
    {
        "title": "Protein structure generation via folding diffusion",
        "abstract": null,
        "authors": [
            "Kevin E. Wu",
            "Kevin Kaichuang Yang",
            "Rianne van den Berg",
            "James Zou",
            "Alex X. Lu",
            "Ava P. Amini"
        ],
        "citations": 153,
        "references": 87,
        "year": 2022
    },
    {
        "title": "Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding",
        "abstract": "Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human and computer vision through the Brain-Computer Interface. However, reconstructing high-quality images with correct semantics from brain recordings is a challenging problem due to the complex underlying representations of brain signals and the scarcity of data annotations. In this work, we present MinD-Vis: Sparse Masked Brain Modeling with Double-Conditioned Latent Diffusion Model for Human Vision Decoding. Firstly, we learn an effective self-supervised representation of fMRI data using mask modeling in a large latent space inspired by the sparse coding of information in the primary visual cortex. Then by augmenting a latent diffusion model with double-conditioning, we show that MinD-Vis can reconstruct highly plausible images with semantically matching details from brain recordings using very few paired annotations. We benchmarked our model qualitatively and quantitatively; the experimental results indicate that our method outperformed state-of-the-art in both semantic mapping (100-way semantic classification) and generation quality (FID) by 66% and 41% respectively. An exhaustive ablation study was also conducted to analyze our framework.",
        "authors": [
            "Zijiao Chen",
            "Jiaxin Qing",
            "Tiange Xiang",
            "Wan Lin Yue",
            "J. Zhou"
        ],
        "citations": 124,
        "references": 72,
        "year": 2022
    },
    {
        "title": "Diffusion-SDF: Text-to-Shape via Voxelized Diffusion",
        "abstract": "With the rising industrial attention to 3D virtual mod-eling technology, generating novel 3D content based on specified conditions (e.g. text) has become a hot issue. In this paper, we propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. Previous approaches lack flexibility in both 3D data representation and shape generation, thereby failing to generate highly diversified 3D shapes conforming to the given text descriptions. To address this, we propose a SDF autoencoder together with the voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. Specifically, we design a novel Uinll-Net architecture that implants a local-focused inner network inside the standard U-Net architecture, which enables better reconstruction of patch-independent SDF representations. We extend our approach to further text-to-shape tasks including text-conditioned shape completion and manipulation. Experimental results show that Diffusion-SDF generates both higher quality and more diversified 3D shapes that conform well to given text descriptions when compared to previous approaches. Code is available at: https://github.com/ttlmh/Diffusion-SDF.",
        "authors": [
            "Muheng Li",
            "Yueqi Duan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "citations": 103,
        "references": 61,
        "year": 2022
    },
    {
        "title": "Universal Speech Enhancement with Score-based Diffusion",
        "abstract": "Removing background noise from speech audio has been the subject of considerable effort, especially in recent years due to the rise of virtual communication and amateur recordings. Yet background noise is not the only unpleasant disturbance that can prevent intelligibility: reverb, clipping, codec artifacts, problematic equalization, limited bandwidth, or inconsistent loudness are equally disturbing and ubiquitous. In this work, we propose to consider the task of speech enhancement as a holistic endeavor, and present a universal speech enhancement system that tackles 55 different distortions at the same time. Our approach consists of a generative model that employs score-based diffusion, together with a multi-resolution conditioning network that performs enhancement with mixture density networks. We show that this approach significantly outperforms the state of the art in a subjective test performed by expert listeners. We also show that it achieves competitive objective scores with just 4-8 diffusion steps, despite not considering any particular strategy for fast sampling. We hope that both our methodology and technical contributions encourage researchers and practitioners to adopt a universal approach to speech enhancement, possibly framing it as a generative task.",
        "authors": [
            "J. Serrà",
            "Santiago Pascual",
            "Jordi Pons",
            "R. O. Araz",
            "D. Scaini"
        ],
        "citations": 83,
        "references": 70,
        "year": 2022
    },
    {
        "title": "Towards performant and reliable undersampled MR reconstruction via diffusion model sampling",
        "abstract": "Magnetic Resonance (MR) image reconstruction from under-sampled acquisition promises faster scanning time. To this end, current State-of-The-Art (SoTA) approaches leverage deep neural networks and supervised training to learn a recovery model. While these approaches achieve impressive performances, the learned model can be fragile on unseen degradation, e.g. when given a different acceleration factor. These methods are also generally deterministic and provide a single solution to an ill-posed problem; as such, it can be difficult for practitioners to understand the reliability of the reconstruction. We introduce DiffuseRecon, a novel diffusion model-based MR reconstruction method. DiffuseRecon guides the generation process based on the observed signals and a pre-trained diffusion model, and does not require additional training on specific acceleration factors. DiffuseRecon is stochastic in nature and generates results from a distribution of fully-sampled MR images; as such, it allows us to explicitly visualize different potential reconstruction solutions. Lastly, DiffuseRecon proposes an accelerated, coarse-to-fine Monte-Carlo sampling scheme to approximate the most likely reconstruction candidate. The proposed DiffuseRecon achieves SoTA performances reconstructing from raw acquisition signals in fastMRI and SKM-TEA. Code will be open-sourced at www.github.com/cpeng93/DiffuseRecon.",
        "authors": [
            "Cheng Peng",
            "Pengfei Guo",
            "S. K. Zhou",
            "Vishal M. Patel",
            "Ramalingam Chellappa"
        ],
        "citations": 75,
        "references": 25,
        "year": 2022
    },
    {
        "title": "Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction",
        "abstract": "We propose a novel and unified method, measurement-conditioned denoising diffusion probabilistic model (MC-DDPM), for under-sampled medical image reconstruction based on DDPM. Different from previous works, MC-DDPM is defined in measurement domain (e.g. k-space in MRI reconstruction) and conditioned on under-sampling mask. We apply this method to accelerate MRI reconstruction and the experimental results show excellent performance, outperforming full supervision baseline and the state-of-the-art score-based reconstruction method. Due to its generative nature, MC-DDPM can also quantify the uncertainty of reconstruction. Our code is available on github.",
        "authors": [
            "Yutong Xie",
            "Quanzheng Li"
        ],
        "citations": 74,
        "references": 25,
        "year": 2022
    },
    {
        "title": "GRAND++: Graph Neural Diffusion with A Source Term",
        "abstract": "We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i.e., low-labeling rate. GRAND++ is a class of continuous-depth graph deep learning architectures whose theoretical underpinning is the diffusion process on graphs with a source term. The source term guarantees two interesting theoretical properties of GRAND++: (i) the representation of graph nodes, under the dynamics of GRAND++, will not converge to a constant vector over all nodes even as the time goes to infinity, which mitigates the over-smoothing issue of graph neural networks and enables graph learning in very deep architectures. (ii) GRAND++ can provide accurate classification even when the model is trained with a very limited number of labeled training data. We experimentally verify the above two advantages on various graph deep learning benchmark tasks, showing a significant improvement over many existing graph neural networks.",
        "authors": [
            "Matthew Thorpe",
            "T. Nguyen",
            "Hedi Xia",
            "T. Strohmer",
            "A. Bertozzi",
            "S. Osher",
            "Bao Wang"
        ],
        "citations": 71,
        "references": 68,
        "year": 2022
    },
    {
        "title": "ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal",
        "abstract": "Recent deep learning methods have achieved promising results in image shadow removal. However, their restored images still suffer from unsatisfactory boundary artifacts, due to the lack of degradation prior embedding and the deficiency in modeling capacity. Our work addresses these issues by proposing a unified diffusion framework that integrates both the image and degradation priors for highly effective shadow removal. In detail, we first propose a shadow degradation model, which inspires us to build a novel unrolling diffusion model, dubbed ShandowDiffusion. It remarkably improves the model's capacity in shadow removal via progressively refining the desired output with both degradation prior and diffusive generative prior, which by nature can serve as a new strong baseline for image restoration. Furthermore, ShadowDiffusion progressively refines the estimated shadow mask as an auxiliary task of the diffusion generator, which leads to more accurate and robust shadow-free image generation. We conduct extensive experiments on three popular public datasets, including ISTD, ISTD+, and SRD, to validate our method's effectiveness. Compared to the state-of-the-art methods, our model achieves a significant improvement in terms of PSNR, increasing from 31.69dB to 34. 73dB over SRD dataset. 11https://github.com/GuoLanqing/ShadowDiffusion",
        "authors": [
            "Lanqing Guo",
            "Chong Wang",
            "Wenhan Yang",
            "Siyu Huang",
            "Yufei Wang",
            "H. Pfister",
            "B. Wen"
        ],
        "citations": 72,
        "references": 84,
        "year": 2022
    },
    {
        "title": "A Physics-informed Diffusion Model for High-fidelity Flow Field Reconstruction",
        "abstract": null,
        "authors": [
            "Dule Shu",
            "Zijie Li",
            "A. Farimani"
        ],
        "citations": 103,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Guided Diffusion Model for Adversarial Purification",
        "abstract": "With wider application of deep neural networks (DNNs) in various algorithms and frameworks, security threats have become one of the concerns. Adversarial attacks disturb DNN-based image classifiers, in which attackers can intentionally add imperceptible adversarial perturbations on input images to fool the classifiers. In this paper, we propose a novel purification approach, referred to as guided diffusion model for purification (GDMP), to help protect classifiers from adversarial attacks. The core of our approach is to embed purification into the diffusion denoising process of a Denoised Diffusion Probabilistic Model (DDPM), so that its diffusion process could submerge the adversarial perturbations with gradually added Gaussian noises, and both of these noises can be simultaneously removed following a guided denoising process. On our comprehensive experiments across various datasets, the proposed GDMP is shown to reduce the perturbations raised by adversarial attacks to a shallow range, thereby significantly improving the correctness of classification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under PGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on the challenging ImageNet dataset.",
        "authors": [
            "Jinyi Wang",
            "Zhaoyang Lyu",
            "Dahua Lin",
            "Bo Dai",
            "Hongfei Fu"
        ],
        "citations": 66,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Lithium‐Diffusion Induced Capacity Losses in Lithium‐Based Batteries",
        "abstract": "Rechargeable lithium‐based batteries generally exhibit gradual capacity losses resulting in decreasing energy and power densities. For negative electrode materials, the capacity losses are largely attributed to the formation of a solid electrolyte interphase layer and volume expansion effects. For positive electrode materials, the capacity losses are, instead, mainly ascribed to structural changes and metal ion dissolution. This review focuses on another, so far largely unrecognized, type of capacity loss stemming from diffusion of lithium atoms or ions as a result of concentration gradients present in the electrode. An incomplete delithiation step is then seen for a negative electrode material while an incomplete lithiation step is obtained for a positive electrode material. Evidence for diffusion‐controlled capacity losses is presented based on published experimental data and results obtained in recent studies focusing on this trapping effect. The implications of the diffusion‐controlled Li‐trapping induced capacity losses, which are discussed using a straightforward diffusion‐based model, are compared with those of other phenomena expected to give capacity losses. Approaches that can be used to identify and circumvent the diffusion‐controlled Li‐trapping problem (e.g., regeneration of cycled batteries) are discussed, in addition to remaining challenges and proposed future research directions within this important research area.",
        "authors": [
            "David Rehnlund",
            "Zhaohui Wang",
            "L. Nyholm"
        ],
        "citations": 65,
        "references": 2,
        "year": 2022
    },
    {
        "title": "PET image denoising based on denoising diffusion probabilistic model",
        "abstract": "Purpose Due to various physical degradation factors and limited counts received, PET image quality needs further improvements. The denoising diffusion probabilistic model (DDPM) was a distribution learning-based model, which tried to transform a normal distribution into a specific data distribution based on iterative refinements. In this work, we proposed and evaluated different DDPM-based methods for PET image denoising. Methods Under the DDPM framework, one way to perform PET image denoising was to provide the PET image and/or the prior image as the input. Another way was to supply the prior image as the network input with the PET image included in the refinement steps, which could fit for scenarios of different noise levels. 150 brain [ $$^{18}$$ 18 F]FDG datasets and 140 brain [ $$^{18}$$ 18 F]MK-6240 (imaging neurofibrillary tangles deposition) datasets were utilized to evaluate the proposed DDPM-based methods. Results Quantification showed that the DDPM-based frameworks with PET information included generated better results than the nonlocal mean, Unet and generative adversarial network (GAN)-based denoising methods. Adding additional MR prior in the model helped achieved better performance and further reduced the uncertainty during image denoising. Solely relying on MR prior while ignoring the PET information resulted in large bias. Regional and surface quantification showed that employing MR prior as the network input while embedding PET image as a data-consistency constraint during inference achieved the best performance. Conclusion DDPM-based PET image denoising is a flexible framework, which can efficiently utilize prior information and achieve better performance than the nonlocal mean, Unet and GAN-based denoising methods.",
        "authors": [
            "Kuang Gong",
            "Keith A. Johnson",
            "G. Fakhri",
            "Quanzheng Li",
            "T. Pan"
        ],
        "citations": 52,
        "references": 50,
        "year": 2022
    },
    {
        "title": "Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation",
        "abstract": "Vessel segmentation in medical images is one of the important tasks in the diagnosis of vascular diseases and therapy planning. Although learning-based segmentation approaches have been extensively studied, a large amount of ground-truth labels are required in supervised methods and confusing background structures make neural networks hard to segment vessels in an unsupervised manner. To address this, here we introduce a novel diffusion adversarial representation learning (DARL) model that leverages a denoising diffusion probabilistic model with adversarial learning, and apply it to vessel segmentation. In particular, for self-supervised vessel segmentation, DARL learns the background signal using a diffusion module, which lets a generation module effectively provide vessel representations. Also, by adversarial learning based on the proposed switchable spatially-adaptive denormalization, our model estimates synthetic fake vessel images as well as vessel segmentation masks, which further makes the model capture vessel-relevant semantic information. Once the proposed model is trained, the model generates segmentation masks in a single step and can be applied to general vascular structure segmentation of coronary angiography and retinal images. Experimental results on various datasets show that our method significantly outperforms existing unsupervised and self-supervised vessel segmentation methods.",
        "authors": [
            "Boah Kim",
            "Y. Oh",
            "Jong-Chul Ye"
        ],
        "citations": 57,
        "references": 40,
        "year": 2022
    },
    {
        "title": "HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising",
        "abstract": "The paper presents a novel approach for vector-floorplan generation via a diffusion model, which denoises 2D coordinates of room/door corners with two inference objectives: 1) a single-step noise as the continuous quantity to precisely invert the continuous forward process; and 2) the final 2D coordinate as the discrete quantity to establish geometric incident relationships such as parallelism, orthogonality, and corner-sharing. Our task is graph-conditioned floorplan generation, a common workflow in floorplan design. We represent a floorplan as 1D polygonal loops, each of which corresponds to a room or a door. Our diffusion model employs a Transformer architecture at the core, which controls the attention masks based on the input graph-constraint and directly generates vector-graphics floorplans via a discrete and continuous denoising process. We have evaluated our approach on RPLAN dataset. The proposed approach makes significant improvements in all the metrics against the state-of-the-art with significant margins, while being capable of generating non-Manhattan structures and controlling the exact number of corners per room. A project website with supplementary video and document is here https://aminshabani.github.io/housediffusion.",
        "authors": [
            "M. Shabani",
            "Sepidehsadat Hosseini",
            "Yasutaka Furukawa"
        ],
        "citations": 47,
        "references": 52,
        "year": 2022
    },
    {
        "title": "Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data",
        "abstract": "We propose Guided-TTS 2, a diffusion-based generative model for high-quality adaptive TTS using untranscribed data. Guided-TTS 2 combines a speaker-conditional diffusion model with a speaker-dependent phoneme classifier for adaptive text-to-speech. We train the speaker-conditional diffusion model on large-scale untranscribed datasets for a classifier-free guidance method and further fine-tune the diffusion model on the reference speech of the target speaker for adaptation, which only takes 40 seconds. We demonstrate that Guided-TTS 2 shows comparable performance to high-quality single-speaker TTS baselines in terms of speech quality and speaker similarity with only a ten-second untranscribed data. We further show that Guided-TTS 2 outperforms adaptive TTS baselines on multi-speaker datasets even with a zero-shot adaptation setting. Guided-TTS 2 can adapt to a wide range of voices only using untranscribed speech, which enables adaptive TTS with the voice of non-human characters such as Gollum in \\textit{\"The Lord of the Rings\"}.",
        "authors": [
            "Sungwon Kim",
            "Heeseung Kim",
            "Sung-Hoon Yoon"
        ],
        "citations": 47,
        "references": 48,
        "year": 2022
    },
    {
        "title": "Solving Audio Inverse Problems with a Diffusion Model",
        "abstract": "This paper presents CQT-Diff, a data-driven generative audio model that can, once trained, be used for solving various different audio inverse problems in a problem-agnostic setting. CQT-Diff is a neural diffusion model with an architecture that is carefully constructed to exploit pitch-equivariant symmetries in music. This is achieved by preconditioning the model with an invertible Constant-Q Transform (CQT), whose logarithmically-spaced frequency axis represents pitch equivariance as translation equivariance. The proposed method is evaluated with solo piano music, using objective and subjective metrics in three different and varied tasks: audio bandwidth extension, inpainting, and declipping. The results show that CQT-Diff outperforms the compared baselines and ablations in audio bandwidth extension and, without retraining, delivers competitive performance against modern baselines in audio inpainting and declipping. This work represents the first diffusion-based general framework for solving inverse problems in audio processing.",
        "authors": [
            "Eloi Moliner",
            "J. Lehtinen",
            "V. Välimäki"
        ],
        "citations": 44,
        "references": 36,
        "year": 2022
    },
    {
        "title": "SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping",
        "abstract": "Neural vocoder using denoising diffusion probabilistic model (DDPM) has been improved by adaptation of the diffusion noise distribution to given acoustic features. In this study, we propose SpecGrad that adapts the diffusion noise so that its time-varying spectral envelope becomes close to the conditioning log-mel spectrogram. This adaptation by time-varying filtering improves the sound quality especially in the high-frequency bands. It is processed in the time-frequency domain to keep the computational cost almost the same as the conventional DDPM-based neural vocoders. Experimental results showed that SpecGrad generates higher-fidelity speech waveform than conventional DDPM-based neural vocoders in both analysis-synthesis and speech enhancement scenarios. Audio demos are available at wavegrad.github.io/specgrad/.",
        "authors": [
            "Yuma Koizumi",
            "H. Zen",
            "K. Yatabe",
            "Nanxin Chen",
            "M. Bacchiani"
        ],
        "citations": 40,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Modeling Atomistic Dynamic Fracture Mechanisms Using a Progressive Transformer Diffusion Model.",
        "abstract": "Dynamic fracture is an important area of materials analysis, assessing the atomic-level mechanisms by which materials fail over time. Here, we focus on brittle materials failure and show that an atomistically derived progressive transformer diffusion machine learning model can effectively describe the dynamics of fracture, capturing important aspects such as crack dynamics, instabilities, and initiation mechanisms. Trained on a small dataset of atomistic simulations, the model generalizes well and offers a rapid assessment of dynamic fracture mechanisms for complex geometries, expanding well beyond the original set of atomistic simulation results. Various validation cases, progressively more distinct from the data used for training, are presented and analyzed. The validation cases feature distinct geometric details, including microstructures generated by a generative neural network used here to identify novel bio-inspired material designs for mechanical performance. For all cases, the model performs well and captures key aspects of material failure.",
        "authors": [
            "M. Buehler"
        ],
        "citations": 40,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Source Localization of Graph Diffusion via Variational Autoencoders for Graph Inverse Problems",
        "abstract": "Graph diffusion problems such as the propagation of rumors, computer viruses, or smart grid failures are ubiquitous and societal. Hence it is usually crucial to identify diffusion sources according to the current graph diffusion observations. Despite its tremendous necessity and significance in practice, source localization, as the inverse problem of graph diffusion, is extremely challenging as it is ill-posed: different sources may lead to the same graph diffusion patterns. Different from most traditional source localization methods, this paper focuses on a probabilistic manner to account for the uncertainty of different candidate sources. Such endeavors require to overcome significant challenges along the way including: 1) the uncertainty in graph diffusion source localization is hard to be quantified; 2) the complex patterns of the graph diffusion sources are difficult to be probabilistically characterized; 3) the generalization under any underlying diffusion patterns is hard to be imposed. To solve the above challenges, this paper presents a generic framework: Source Localization Variational AutoEncoder (SL-VAE) for locating the diffusion sources under arbitrary diffusion patterns. Particularly, we propose a probabilistic model that leverages the forward diffusion estimation model along with deep generative models to approximate the diffusion source distribution for quantifying the uncertainty. SL-VAE further utilizes prior knowledge of the source-observation pairs to characterize the complex patterns of diffusion sources by a learned generative prior. Lastly, a unified objective that integrates the forward diffusion estimation model is derived to enforce the model to generalize under arbitrary diffusion patterns. Extensive experiments are conducted on $7$ real-world datasets to demonstrate the superiority of SL-VAE in reconstructing the diffusion sources by excelling the state-of-the-arts on average 20% in AUC score. The code and data are available at: https://github.com/triplej0079/SLVAE.",
        "authors": [
            "Chen Ling",
            "Junji Jiang",
            "Junxiang Wang",
            "Liang Zhao"
        ],
        "citations": 36,
        "references": 65,
        "year": 2022
    },
    {
        "title": "SAR Despeckling Using a Denoising Diffusion Probabilistic Model",
        "abstract": "Speckle is a type of multiplicative noise that affects all coherent imaging modalities including synthetic aperture radar (SAR) images. The presence of speckle degrades the image quality and can adversely affect the performance of SAR image applications such as automatic target recognition and change detection. Thus, SAR despeckling is an important problem in remote sensing. In this letter, we introduce SAR-DDPM, a denoising diffusion probabilistic model for SAR despeckling. The proposed method uses a Markov chain that transforms clean images into white Gaussian noise by successively adding random noise. The despeckled image is obtained through a reverse process that predicts the added noise iteratively, using a noise predictor conditioned on the speckled image. In addition, we propose a new inference strategy based on cycle spinning to improve the despeckling performance. Our experiments on both synthetic and real SAR images demonstrate that the proposed method leads to significant improvements in both quantitative and qualitative results over the state-of-the-art despeckling methods. The code is available at: https://github.com/malshaV/SAR_DDPM",
        "authors": [
            "Malsha V. Perera",
            "Nithin Gopalakrishnan Nair",
            "W. G. C. Bandara",
            "Vishal M. Patel"
        ],
        "citations": 37,
        "references": 30,
        "year": 2022
    },
    {
        "title": "Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion",
        "abstract": "Digital art synthesis is receiving increasing attention in the multimedia community because of engaging the public with art effectively. Current digital art synthesis methods usually use single-modality inputs as guidance, thereby limiting the expressiveness of the model and the diversity of generated results. To solve this problem, we propose the multimodal guided artwork diffusion (MGAD) model, which is a diffusion-based digital artwork generation approach that utilizes multimodal prompts as guidance to control the classifier-free diffusion model. Additionally, the contrastive language-image pretraining (CLIP) model is used to unify text and image modalities. Extensive experimental results on the quality and quantity of the generated digital art paintings confirm the effectiveness of the combination of the diffusion model and multimodal guidance. Code is available at https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion.",
        "authors": [
            "Nisha Huang",
            "Fan Tang",
            "Weiming Dong",
            "Changsheng Xu"
        ],
        "citations": 37,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model",
        "abstract": "We propose a simple and novel method for generating 3D human motion from complex natural language sentences, which describe different velocity, direction and composition of all kinds of actions. Different from existing methods that use classical generative architecture, we apply the Denoising Diffusion Probabilistic Model to this task, synthesizing diverse motion results under the guidance of texts. The diffusion model converts white noise into structured 3D motion by a Markov process with a series of denoising steps and is efficiently trained by optimizing a variational lower bound. To achieve the goal of text-conditioned image synthesis, we use the classifier-free guidance strategy to add text embedding into the model during training. Our experiments demonstrate that our model achieves competitive results on HumanML3D test set quantitatively and can generate more visually natural and diverse examples. We also show with experiments that our model is capable of zero-shot generation of motions for unseen text guidance.",
        "authors": [
            "Zhiyuan Ren",
            "Zhihong Pan",
            "Xingfa Zhou",
            "Le Kang"
        ],
        "citations": 32,
        "references": 22,
        "year": 2022
    },
    {
        "title": "Unsupervised denoising of retinal OCT with diffusion probabilistic model",
        "abstract": "Optical coherence tomography (OCT) is a prevalent non-invasive imaging method which provides high resolution volumetric visualization of retina. However, its inherent defect, the speckle noise, can seriously deteriorate the tissue visibility in OCT. Deep learning based approaches have been widely used for image restoration, but most of these require a noise-free reference image for supervision. In this study, we present a diffusion probabilistic model that is fully unsupervised to learn from noise instead of signal. A diffusion process is defined by adding a sequence of Gaussian noise to self-fused OCT b-scans. Then the reverse process of diffusion, modeled by a Markov chain, provides an adjustable level of denoising. Our experiment results demonstrate that our method can significantly improve the image quality with a simple working pipeline and a small amount of training data. The implementation is available at https://github.com/DeweiHu/OCT_DDPM.",
        "authors": [
            "Dewei Hu",
            "Yuankai K. Tao",
            "I. Oguz"
        ],
        "citations": 29,
        "references": 17,
        "year": 2022
    },
    {
        "title": "Dynamic analysis of a plant-water model with spatial diffusion",
        "abstract": null,
        "authors": [
            "Gui‐Quan Sun",
            "Hong-Tao Zhang",
            "Yongli Song",
            "Li Li",
            "Zhen Jin"
        ],
        "citations": 53,
        "references": 69,
        "year": 2022
    },
    {
        "title": "Two-stage column–hemispherical penetration diffusion model considering porosity tortuosity and time-dependent viscosity behavior",
        "abstract": null,
        "authors": [
            "Chaojie Wang",
            "Yueliang Diao",
            "Chengchao Guo",
            "Pang Li",
            "Xueming Du",
            "Yanhui Pan"
        ],
        "citations": 21,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
        "abstract": null,
        "authors": [
            "Josh Abramson",
            "Jonas Adler",
            "Jack Dunger",
            "Richard Evans",
            "Tim Green",
            "A. Pritzel",
            "Olaf Ronneberger",
            "Lindsay Willmore",
            "Andrew J Ballard",
            "Joshua Bambrick",
            "Sebastian W Bodenstein",
            "David A Evans",
            "Chia-Chun Hung",
            "Michael O’Neill",
            "D. Reiman",
            "Kathryn Tunyasuvunakool",
            "Zachary Wu",
            "Akvilė Žemgulytė",
            "Eirini Arvaniti",
            "Charles Beattie",
            "Ottavia Bertolli",
            "Alex Bridgland",
            "Alexey Cherepanov",
            "Miles Congreve",
            "A. Cowen-Rivers",
            "Andrew Cowie",
            "Michael Figurnov",
            "Fabian B Fuchs",
            "Hannah Gladman",
            "Rishub Jain",
            "Yousuf A. Khan",
            "Caroline M R Low",
            "Kuba Perlin",
            "Anna Potapenko",
            "Pascal Savy",
            "Sukhdeep Singh",
            "A. Stecula",
            "Ashok Thillaisundaram",
            "Catherine Tong",
            "Sergei Yakneen",
            "Ellen D. Zhong",
            "Michal Zielinski",
            "Augustin Žídek",
            "V. Bapst",
            "Pushmeet Kohli",
            "Max Jaderberg",
            "D. Hassabis",
            "J. Jumper"
        ],
        "citations": 1000,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions",
        "abstract": "We propose a method for editing NeRF scenes with text-instructions. Given a NeRF of a scene and the collection of images used to reconstruct it, our method uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit the input images while optimizing the underlying scene, resulting in an optimized 3D scene that respects the edit instruction. We demonstrate that our proposed method is able to edit large-scale, real-world scenes, and is able to accomplish more realistic, targeted edits than prior work. Result videos can be found on the project website: https://instruct-nerf2nerf.github.io.",
        "authors": [
            "Ayaan Haque",
            "Matthew Tancik",
            "Alexei A. Efros",
            "Aleksander Holynski",
            "Angjoo Kanazawa"
        ],
        "citations": 293,
        "references": 61,
        "year": 2023
    },
    {
        "title": "RealFusion 360° Reconstruction of Any Object from a Single Image",
        "abstract": "We consider the problem of reconstructing a full 360° photographic model of an object from a single image of it. We do so by fitting a neural radiance field to the image, but find this problem to be severely ill-posed. We thus take an off-the-self conditional image generator based on diffusion and engineer a prompt that encourages it to “dream up” novel views of the object. Using the recent DreamFusion method, we fuse the given input view, the conditional prior, and other regularizers into a final, consistent reconstruction. We demonstrate state-of-the-art reconstruction results on benchmark images when compared to prior methods for monocular 3D reconstruction of objects. Qualitatively, our reconstructions provide a faithful match of the input view and a plausible extrapolation of its appearance and 3D shape, including to the side of the object not visible in the image.",
        "authors": [
            "Luke Melas-Kyriazi",
            "Iro Laina",
            "C. Rupprecht",
            "A. Vedaldi"
        ],
        "citations": 259,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Feature Manipulation for DDPM based Change Detection",
        "abstract": "—Change Detection is a classic task of computer vision that receives a bi-temporal image pair as input and separates the semantically changed and unchanged regions of it. The diffusion model is used in image synthesis and as a feature extractor and has been applied to various downstream tasks. Using this, a feature map is extracted from the pre-trained diffusion model from the large-scale data set, and changes are detected through the additional network. On the one hand, the current diffusion-based change detection approach focuses only on extracting a good feature map using the diffusion model. It obtains and uses differences without further adjustment to the created feature map. Our method focuses on manipulating the feature map extracted from the Diffusion Model to be more semantically useful, and for this, we propose two methods: Feature Attention and FDAF. Our model with Feature Attention achieved a state-of-the-art F1 score (90.18) and IoU (83.86) on the LEVIR-CD dataset.",
        "authors": [
            "Zhenglin Li",
            "Yangchen Huang",
            "Mengran Zhu",
            "Jingyu Zhang",
            "Jinghao Chang",
            "Houze Liu"
        ],
        "citations": 23,
        "references": 29,
        "year": 2024
    },
    {
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
        "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.",
        "authors": [
            "Hanze Dong",
            "Wei Xiong",
            "Deepanshu Goyal",
            "Rui Pan",
            "Shizhe Diao",
            "Jipeng Zhang",
            "Kashun Shum",
            "T. Zhang"
        ],
        "citations": 323,
        "references": 81,
        "year": 2023
    },
    {
        "title": "MeshDiffusion: Score-based Generative 3D Mesh Modeling",
        "abstract": "We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.",
        "authors": [
            "Zhen Liu",
            "Yao Feng",
            "Michael J. Black",
            "D. Nowrouzezahrai",
            "L. Paull",
            "Wei-yu Liu"
        ],
        "citations": 124,
        "references": 60,
        "year": 2023
    },
    {
        "title": "The probability flow ODE is provably fast",
        "abstract": "We provide the first polynomial-time convergence guarantees for the probability flow ODE implementation (together with a corrector step) of score-based generative modeling. Our analysis is carried out in the wake of recent results obtaining such guarantees for the SDE-based implementation (i.e., denoising diffusion probabilistic modeling or DDPM), but requires the development of novel techniques for studying deterministic dynamics without contractivity. Through the use of a specially chosen corrector step based on the underdamped Langevin diffusion, we obtain better dimension dependence than prior works on DDPM ($O(\\sqrt{d})$ vs. $O(d)$, assuming smoothness of the data distribution), highlighting potential advantages of the ODE framework.",
        "authors": [
            "Sitan Chen",
            "Sinho Chewi",
            "Holden Lee",
            "Yuanzhi Li",
            "Jianfeng Lu",
            "A. Salim"
        ],
        "citations": 64,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Text2NeRF: Text-Driven 3D Scene Generation With Neural Radiance Fields",
        "abstract": "Text-driven 3D scene generation is widely applicable to video gaming, film industry, and metaverse applications that have a large demand for 3D scenes. However, existing text-to-3D generation methods are limited to producing 3D objects with simple geometries and dreamlike styles that lack realism. In this work, we present Text2NeRF, which is able to generate a wide range of 3D scenes with complicated geometric structures and high-fidelity textures purely from a text prompt. To this end, we adopt NeRF as the 3D representation and leverage a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF to reflect the scene description. Specifically, we employ the diffusion model to infer the text-related image as the content prior and use a monocular depth estimation method to offer the geometric prior. Both content and geometric priors are utilized to update the NeRF model. To guarantee textured and geometric consistency between different views, we introduce a progressive scene inpainting and updating strategy for novel view synthesis of the scene. Our method requires no additional training data but only a natural language description of the scene as the input. Extensive experiments demonstrate that our Text2NeRF outperforms existing methods in producing photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of natural language prompts. Our code and model are available at https://github.com/eckertzhang/Text2NeRF.",
        "authors": [
            "Jingbo Zhang",
            "Xiaoyu Li",
            "Ziyu Wan",
            "Can Wang",
            "Jing Liao"
        ],
        "citations": 63,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation",
        "abstract": "With the explosive popularity of AI-generated content (AIGC), video generation has recently received a lot of attention. Generating videos guided by text instructions poses significant challenges, such as modeling the complex relationship between space and time, and the lack of large-scale text-video paired data. Existing text-video datasets suffer from limitations in both content quality and scale, or they are not open-source, rendering them inaccessible for study and use. For model design, previous approaches extend pretrained text-to-image generation models by adding temporal 1D convolution/attention modules for video generation. However, these approaches overlook the importance of jointly modeling space and time, inevitably leading to temporal distortions and misalignment between texts and videos. In this paper, we propose a novel approach that strengthens the interaction between spatial and temporal perceptions. In particular, we utilize a swapped cross-attention mechanism in 3D windows that alternates the\"query\"role between spatial and temporal blocks, enabling mutual reinforcement for each other. Moreover, to fully unlock model capabilities for high-quality video generation and promote the development of the field, we curate a large-scale and open-source video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters. A smaller-scale yet more meticulously cleaned subset further enhances the data quality, aiding models in achieving superior performance. Experimental quantitative and qualitative results demonstrate the superiority of our approach in terms of per-frame quality, temporal correlation, and text-video alignment, with clear margins.",
        "authors": [
            "Wenjing Wang",
            "Huan Yang",
            "Zixi Tuo",
            "Huiguo He",
            "Junchen Zhu",
            "Jianlong Fu",
            "Jiaying Liu"
        ],
        "citations": 95,
        "references": 87,
        "year": 2023
    },
    {
        "title": "SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation",
        "abstract": "With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, SalUn yields a stability advantage in high-variance random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from generating harmful images, SalUn achieves nearly 100% unlearning accuracy, outperforming current state-of-the-art baselines like Erased Stable Diffusion and Forget-Me-Not.",
        "authors": [
            "Chongyu Fan",
            "Jiancheng Liu",
            "Yihua Zhang",
            "Dennis Wei",
            "Eric Wong",
            "Sijia Liu"
        ],
        "citations": 72,
        "references": 75,
        "year": 2023
    },
    {
        "title": "AnyText: Multilingual Visual Text Generation And Editing",
        "abstract": "Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced on https://github.com/tyxsspa/AnyText to improve and promote the development of text generation technology.",
        "authors": [
            "Yuxiang Tuo",
            "Wangmeng Xiang",
            "Jun-Yan He",
            "Yifeng Geng",
            "Xuansong Xie"
        ],
        "citations": 49,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Animate124: Animating One Image to 4D Dynamic Scene",
        "abstract": "We introduce Animate124 (Animate-one-image-to-4D), the first work to animate a single in-the-wild image into 3D video through textual motion descriptions, an underexplored problem with significant applications. Our 4D generation leverages an advanced 4D grid dynamic Neural Radiance Field (NeRF) model, optimized in three distinct stages using multiple diffusion priors. Initially, a static model is optimized using the reference image, guided by 2D and 3D diffusion priors, which serves as the initialization for the dynamic NeRF. Subsequently, a video diffusion model is employed to learn the motion specific to the subject. However, the object in the 3D videos tends to drift away from the reference image over time. This drift is mainly due to the misalignment between the text prompt and the reference image in the video diffusion model. In the final stage, a personalized diffusion prior is therefore utilized to address the semantic drift. As the pioneering image-text-to-4D generation framework, our method demonstrates significant advancements over existing baselines, evidenced by comprehensive quantitative and qualitative assessments.",
        "authors": [
            "Yuyang Zhao",
            "Zhiwen Yan",
            "Enze Xie",
            "Lanqing Hong",
            "Zhenguo Li",
            "Gim Hee Lee"
        ],
        "citations": 48,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Efficient Neural Music Generation",
        "abstract": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.",
        "authors": [
            "Max W. Y. Lam",
            "Qiao Tian",
            "Tang-Chun Li",
            "Zongyu Yin",
            "Siyuan Feng",
            "Ming Tu",
            "Yuliang Ji",
            "Rui Xia",
            "Mingbo Ma",
            "Xuchen Song",
            "Jitong Chen",
            "Yuping Wang",
            "Yuxuan Wang"
        ],
        "citations": 42,
        "references": 73,
        "year": 2023
    },
    {
        "title": "SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation",
        "abstract": "In this work, we present a novel framework built to sim-plify 3D asset generation for amateur users. To enable interactive generation, our method supports a variety of input modalities that can be easily provided by a human, in-cluding images, text, partially observed shapes and combinations of these, further allowing to adjust the strength of each input. At the core of our approach is an encoder-decoder, compressing 3D shapes into a compact latent representation, upon which a diffusion model is learned. To enable a variety of multimodal inputs, we employ task-specific encoders with dropout followed by a cross-attention mechanism. Due to its flexibility, our model naturally supports a variety of tasks, outperforming prior works on shape completion, image-based 3D reconstruction, and text-to-3D. Most interestingly, our model can combine all these tasks into one swiss-army-knife tool, enabling the user to perform shape generation using incomplete shapes, images, and textual descriptions at the same time, providing the relative weights for each input and facilitating interactivity. Despite our approach being shape-only, we further show an efficient method to texture the generated shape using large-scale text-to-image models.",
        "authors": [
            "Yen-Chi Cheng",
            "Hsin-Ying Lee",
            "S. Tulyakov",
            "A. Schwing",
            "Liangyan Gui"
        ],
        "citations": 204,
        "references": 59,
        "year": 2022
    },
    {
        "title": "EDGE: Editable Dance Generation From Music",
        "abstract": "Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.",
        "authors": [
            "Jo-Han Tseng",
            "Rodrigo Castellon",
            "C. Liu"
        ],
        "citations": 167,
        "references": 68,
        "year": 2022
    }
]