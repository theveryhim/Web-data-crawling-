[
    {
        "title": "PaliGemma: A versatile 3B VLM for transfer",
        "abstract": "PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.",
        "authors": [
            "Lucas Beyer",
            "A. Steiner",
            "Andr√© Susano Pinto",
            "Alexander Kolesnikov",
            "Xiao Wang",
            "Daniel M. Salz",
            "Maxim Neumann",
            "Ibrahim M. Alabdulmohsin",
            "Michael Tschannen",
            "Emanuele Bugliarello",
            "Thomas Unterthiner",
            "Daniel Keysers",
            "Skanda Koppula",
            "Fangyu Liu",
            "Adam Grycner",
            "A. Gritsenko",
            "N. Houlsby",
            "Manoj Kumar",
            "Keran Rong",
            "Julian Martin Eisenschlos",
            "Rishabh Kabra",
            "Matthias Bauer",
            "Matko Bovsnjak",
            "Xi Chen",
            "Matthias Minderer",
            "P. Voigtlaender",
            "Ioana Bica",
            "Ivana Balazevic",
            "J. Puigcerver",
            "Pinelopi Papalampidi",
            "Olivier Henaff",
            "Xi Xiong",
            "Radu Soricut",
            "Jeremiah Harmsen",
            "Xiao-Qi Zhai"
        ],
        "citations": 75,
        "references": 0,
        "year": 2024
    },
    {
        "title": "An Image Grid Can Be Worth a Video: Zero-Shot Video Question Answering Using a VLM",
        "abstract": "Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging multiple frames in our proposed grid layout. The resulting single image is termed as an image grid. This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure at the pixel level. Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training. Our extensive experimental analysis across ten zero-shot VQA benchmarks, including five open-ended and five multiple-choice benchmarks, reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten benchmarks. We also discuss how IG-VLM can be extended for long videos and provide an extension method that consistently and reliably improves the performance. Our code is are available at: https://github.com/imagegridworth/IG-VLM",
        "authors": [
            "Wonkyun Kim",
            "Changin Choi",
            "Wonseok Lee",
            "Wonjong Rhee"
        ],
        "citations": 35,
        "references": 59,
        "year": 2024
    },
    {
        "title": "NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation",
        "abstract": "Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.",
        "authors": [
            "Jiazhao Zhang",
            "Kunyu Wang",
            "Rongtao Xu",
            "Gengze Zhou",
            "Yicong Hong",
            "Xiaomeng Fang",
            "Qi Wu",
            "Zhizheng Zhang",
            "Wang He"
        ],
        "citations": 27,
        "references": 118,
        "year": 2024
    },
    {
        "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
        "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
        "authors": [
            "Yufei Wang",
            "Zhanyi Sun",
            "Jesse Zhang",
            "Zhou Xian",
            "Erdem Biyik",
            "David Held",
            "Zackory Erickson"
        ],
        "citations": 25,
        "references": 54,
        "year": 2024
    },
    {
        "title": "LLavaGuard: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment",
        "abstract": "We introduce LlavaGuard, a family of VLM-based safeguard models, offering a versatile framework for evaluating the safety compliance of visual content. Specifically, we designed LlavaGuard for dataset annotation and generative model safeguarding. To this end, we collected and annotated a high-quality visual dataset incorporating a broad safety taxonomy, which we use to tune VLMs on context-aware safety risks. As a key innovation, LlavaGuard's new responses contain comprehensive information, including a safety rating, the violated safety categories, and an in-depth rationale. Further, our introduced customizable taxonomy categories enable the context-specific alignment of LlavaGuard to various scenarios. Our experiments highlight the capabilities of LlavaGuard in complex and real-world applications. We provide checkpoints ranging from 7B to 34B parameters demonstrating state-of-the-art performance, with even the smallest models outperforming baselines like GPT-4. We make our dataset and model weights publicly available and invite further research to address the diverse needs of communities and contexts.",
        "authors": [
            "Lukas Helff",
            "Felix Friedrich",
            "Manuel Brack",
            "K. Kersting",
            "P. Schramowski"
        ],
        "citations": 5,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach",
        "abstract": "The integration of large-scale Vision-Language Models (VLMs) with embodied AI can greatly enhance the generalizability and the capacity to follow open instructions for robots. However, existing studies on object manipulation are not up to full consideration of the 6-DoF requirements, let alone establishing a comprehensive benchmark. In this paper, we propel the pioneer construction of the benchmark and approach for Open-instruction 6-DoF Object Rearrangement (Open6DOR). Specifically, we collect a synthetic dataset of 200+ objects and carefully design 5400+ Open6DOR tasks. These tasks are divided into the Position-track, Rotation-track, and 6-DoF-track for evaluating different embodied agents in predicting the positions and rotations of target objects.Besides, we also propose a VLM-based approach for Open6DOR, named Open6DOR-GPT, which empowers GPT-4V with 3D-awareness and simulation-assistance while exploiting its strengths in generalizability and instruction-following. We compare the existing embodied agents with our Open6DOR-GPT on the proposed Open6DOR benchmark and find that Open6DOR-GPT achieves the state-of-the-art performance. We further show the impressive performance of Open6DOR-GPT in diverse real-world experiments.",
        "authors": [
            "Yufei Ding",
            "Haoran Geng",
            "Chaoyi Xu",
            "Xiaomeng Fang",
            "Jiazhao Zhang",
            "Songlin Wei",
            "Qiyu Dai",
            "Zhizheng Zhang",
            "He Wang"
        ],
        "citations": 5,
        "references": 37,
        "year": 2024
    },
    {
        "title": "VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought",
        "abstract": "Large-scale LLMs and VLMs excel at few-shot learning but require high-quality examples. We introduce In-Context Abstraction Learning (ICAL), which iteratively refines suboptimal trajectories into high-quality data with optimized actions and detailed reasoning. Given an inefficient demonstration, a VLM corrects actions and annotates causal relationships, object states, subgoals, and task-relevant visuals, forming\"programs of thought.\"With human feedback, these programs are improved as the agent executes them in a similar environment. The resulting examples, used as prompt context or fine-tuning data, significantly boost decision-making while reducing human feedback needs. ICAL surpasses state-of-the-art in TEACh (dialogue-based instruction following), VisualWebArena (multimodal web agents), and Ego4D (egocentric video action anticipation). In TEACh, combining fine-tuning and retrieval on ICAL examples outperforms raw human demonstrations and expert examples, achieving a 17.5% increase in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over GPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL outperforms few-shot GPT-4V and remains competitive with supervised models. Overall, ICAL scales 2x better than raw human demonstrations and reduces manual prompt engineering.",
        "authors": [
            "Gabriel Sarch",
            "Lawrence Jang",
            "Michael J. Tarr",
            "William W. Cohen",
            "Kenneth Marino",
            "Katerina Fragkiadaki"
        ],
        "citations": 4,
        "references": 106,
        "year": 2024
    },
    {
        "title": "VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model",
        "abstract": "Vision Language Models (VLMs) have recently been adopted in robotics for their capability in common sense reasoning and generalizability. Existing work has applied VLMs to generate task and motion planning from natural language instructions and simulate training data for robot learning. In this work, we explore using VLM to interpret human demonstration videos and generate robot task planning. Our method integrates keyframe selection, visual perception, and VLM reasoning into a pipeline. We named it SeeDo because it enables the VLM to ''see'' human demonstrations and explain the corresponding plans to the robot for it to ''do''. To validate our approach, we collected a set of long-horizon human videos demonstrating pick-and-place tasks in three diverse categories and designed a set of metrics to comprehensively benchmark SeeDo against several baselines, including state-of-the-art video-input VLMs. The experiments demonstrate SeeDo's superior performance. We further deployed the generated task plans in both a simulation environment and on a real robot arm.",
        "authors": [
            "Beichen Wang",
            "Juexiao Zhang",
            "Shuwen Dong",
            "Irving Fang",
            "Chen Feng"
        ],
        "citations": 4,
        "references": 83,
        "year": 2024
    },
    {
        "title": "VLM-PL: Advanced Pseudo Labeling approach for Class Incremental Object Detection via Vision-Language Model",
        "abstract": "In the field of Class Incremental Object Detection (CIOD), creating models that can continuously learn like humans is a major challenge. Pseudo-labeling methods, although initially powerful, struggle with multi-scenario incremental learning due to their tendency to forget past knowledge. To overcome this, we introduce a new approach called Vision-Language Model assisted Pseudo-Labeling (VLM-PL). This technique uses Vision-Language Model (VLM) to verify the correctness of pseudo ground-truths (GTs) without requiring additional model training. VLM-PL starts by deriving pseudo GTs from a pre-trained detector. Then, we generate custom queries for each pseudo GT using carefully designed prompt templates that combine image and text features. This allows the VLM to classify the correctness through its responses. Furthermore, VLM-PL integrates refined pseudo and real GTs from upcoming training, effectively combining new and old knowledge. Extensive experiments conducted on the Pascal VOC and MS COCO datasets not only highlight VLM-PL‚Äôs exceptional performance in multi-scenario but also illuminate its effectiveness in dual-scenario by achieving state-of-the-art results in both.",
        "authors": [
            "Junsu Kim",
            "Yunhoe Ku",
            "Jihyeon Kim",
            "Junuk Cha",
            "Seungryul Baek"
        ],
        "citations": 4,
        "references": 95,
        "year": 2024
    },
    {
        "title": "Slot-VLM: SlowFast Slots for Video-Language Modeling",
        "abstract": "Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.",
        "authors": [
            "Jiaqi Xu",
            "Cuiling Lan",
            "Wenxuan Xie",
            "Xuejin Chen",
            "Yan Lu"
        ],
        "citations": 5,
        "references": 34,
        "year": 2024
    },
    {
        "title": "F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models",
        "abstract": "We present F-VLM, a simple open-vocabulary object detection method built upon Frozen Vision and Language Models. F-VLM simplifies the current multi-stage training pipeline by eliminating the need for knowledge distillation or detection-tailored pretraining. Surprisingly, we observe that a frozen VLM: 1) retains the locality-sensitive features necessary for detection, and 2) is a strong region classifier. We finetune only the detector head and combine the detector and VLM outputs for each region at inference time. F-VLM shows compelling scaling behavior and achieves +6.5 mask AP improvement over the previous state of the art on novel categories of LVIS open-vocabulary detection benchmark. In addition, we demonstrate very competitive results on COCO open-vocabulary detection benchmark and cross-dataset transfer detection, in addition to significant training speed-up and compute savings. Code will be released at the https://sites.google.com/view/f-vlm/home",
        "authors": [
            "Weicheng Kuo",
            "Yin Cui",
            "Xiuye Gu",
            "A. Piergiovanni",
            "A. Angelova"
        ],
        "citations": 109,
        "references": 59,
        "year": 2022
    },
    {
        "title": "V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models",
        "abstract": "Advancements in autonomous driving have increasingly focused on end-to-end (E2E) systems that manage the full spectrum of driving tasks, from environmental perception to vehicle navigation and control. This paper introduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework with Vehicle-to-Everything (V2X) systems and large vision-language models (VLMs). V2X-VLM is designed to enhance situational awareness, decision-making, and ultimate trajectory planning by integrating multimodel data from vehicle-mounted cameras, infrastructure sensors, and textual information. The contrastive learning method is further employed to complement VLM by refining feature discrimination, assisting the model to learn robust representations of the driving environment. Evaluations on the DAIR-V2X dataset show that V2X-VLM outperforms state-of-the-art cooperative autonomous driving methods, while additional tests on corner cases validate its robustness in real-world driving conditions.",
        "authors": [
            "Junwei You",
            "Haotian Shi",
            "Zhuoyu Jiang",
            "Zilin Huang",
            "Rui Gan",
            "Keshu Wu",
            "Xi Cheng",
            "Xiaopeng Li",
            "Bin Ran"
        ],
        "citations": 3,
        "references": 34,
        "year": 2024
    },
    {
        "title": "VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for Human Annotation-Free Pathological Image Classification",
        "abstract": "Despite that deep learning methods have achieved remarkable performance in pathology image classification, they heavily rely on labeled data, demanding extensive human annotation efforts. In this study, we present a novel human annotation-free method for pathology image classification by leveraging pre-trained Vision-Language Models (VLMs). Without human annotation, pseudo labels of the training set are obtained by utilizing the zero-shot inference capabilities of VLM, which may contain a lot of noise due to the domain shift between the pre-training data and the target dataset. To address this issue, we introduce VLM-CPL, a novel approach based on consensus pseudo labels that integrates two noisy label filtering techniques with a semi-supervised learning strategy. Specifically, we first obtain prompt-based pseudo labels with uncertainty estimation by zero-shot inference with the VLM using multiple augmented views of an input. Then, by leveraging the feature representation ability of VLM, we obtain feature-based pseudo labels via sample clustering in the feature space. Prompt-feature consensus is introduced to select reliable samples based on the consensus between the two types of pseudo labels. By rejecting low-quality pseudo labels, we further propose High-confidence Cross Supervision (HCS) to learn from samples with reliable pseudo labels and the remaining unlabeled samples. Experimental results showed that our method obtained an accuracy of 87.1% and 95.1% on the HPH and LC25K datasets, respectively, and it largely outperformed existing zero-shot classification and noisy label learning methods. The code is available at https://github.com/lanfz2000/VLM-CPL.",
        "authors": [
            "Lanfeng Zhong",
            "Xin Liao",
            "Shaoting Zhang",
            "Xiaofan Zhang",
            "Guotai Wang"
        ],
        "citations": 3,
        "references": 25,
        "year": 2024
    },
    {
        "title": "VLM-Social-Nav: Socially Aware Robot Navigation Through Scoring Using Vision-Language Models",
        "abstract": "We propose VLM-Social-Nav, a novel Vision-Language Model (VLM) based navigation approach to compute a robot's motion in human-centered environments. Our goal is to make real-time decisions on robot actions that are socially compliant with human expectations. We utilize a perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant robot behavior. VLM-Social-Nav uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. Our overall approach reduces reliance on large training datasets and enhances adaptability in decision-making. In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot. We observe at least 27.38% improvement in the average success rate and 19.05% improvement in the average collision rate in the four social navigation scenarios. Our user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior.",
        "authors": [
            "Daeun Song",
            "Jing Liang",
            "Amirreza Payandeh",
            "Amir Hossain Raj",
            "Xuesu Xiao",
            "Dinesh Manocha"
        ],
        "citations": 3,
        "references": 62,
        "year": 2024
    },
    {
        "title": "MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention",
        "abstract": "In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present \\textit{MemeGuard}, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. \\textit{MemeGuard} harnesses a specially fine-tuned VLM, \\textit{VLMeme}, for meme interpretation, and a multimodal knowledge selection and ranking mechanism (\\textit{MKS}) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the \\textit{\\textbf{I}ntervening} \\textit{\\textbf{C}yberbullying in \\textbf{M}ultimodal \\textbf{M}emes (ICMM)} dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage \\textit{ICMM} to test \\textit{MemeGuard}, demonstrating its proficiency in generating relevant and effective responses to toxic memes.",
        "authors": [
            "Prince Jha",
            "Raghav Jain",
            "Konika Mandal",
            "Aman Chadha",
            "Sriparna Saha",
            "P. Bhattacharyya"
        ],
        "citations": 3,
        "references": 47,
        "year": 2024
    },
    {
        "title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model",
        "abstract": "The rapid advancement of Large Vision-Language models (LVLMs) has demonstrated a spectrum of emergent capabilities. Nevertheless, current models only focus on the visual content of a single scenario, while their ability to associate instances across different scenes has not yet been explored, which is essential for understanding complex visual content, such as movies with multiple characters and intricate plots. Towards movie understanding, a critical initial step for LVLMs is to unleash the potential of character identities memory and recognition across multiple visual scenarios. To achieve the goal, we propose visual instruction tuning with ID reference and develop an ID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research introduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and recognition across four dimensions: matching, location, question-answering, and captioning. Our findings highlight the limitations of existing LVLMs in recognizing and associating instance identities with ID reference. This paper paves the way for future artificial intelligence systems to possess multi-identity visual inputs, thereby facilitating the comprehension of complex visual narratives like movies.",
        "authors": [
            "Yatai Ji",
            "Shilong Zhang",
            "Jie Wu",
            "Peize Sun",
            "Weifeng Chen",
            "Xuefeng Xiao",
            "Sidi Yang",
            "Yujiu Yang",
            "Ping Luo"
        ],
        "citations": 2,
        "references": 46,
        "year": 2024
    },
    {
        "title": "VLM-MPC: Vision Language Foundation Model (VLM)-Guided Model Predictive Controller (MPC) for Autonomous Driving",
        "abstract": "Motivated by the emergent reasoning capabilities of Vision Language Models (VLMs) and their potential to improve the comprehensibility of autonomous driving systems, this paper introduces a closed-loop autonomous driving controller called VLM-MPC, which combines the Model Predictive Controller (MPC) with VLM to evaluate how model-based control could enhance VLM decision-making. The proposed VLM-MPC is structured into two asynchronous components: The upper layer VLM generates driving parameters (e.g., desired speed, desired headway) for lower-level control based on front camera images, ego vehicle state, traffic environment conditions, and reference memory; The lower-level MPC controls the vehicle in real-time using these parameters, considering engine lag and providing state feedback to the entire system. Experiments based on the nuScenes dataset validated the effectiveness of the proposed VLM-MPC across various environments (e.g., night, rain, and intersections). The results demonstrate that the VLM-MPC consistently maintains Post Encroachment Time (PET) above safe thresholds, in contrast to some scenarios where the VLM-based control posed collision risks. Additionally, the VLM-MPC enhances smoothness compared to the real-world trajectories and VLM-based control. By comparing behaviors under different environmental settings, we highlight the VLM-MPC's capability to understand the environment and make reasoned inferences. Moreover, we validate the contributions of two key components, the reference memory and the environment encoder, to the stability of responses through ablation tests.",
        "authors": [
            "Keke Long",
            "Haotian Shi",
            "Jiaxi Liu",
            "Xiaopeng Li"
        ],
        "citations": 2,
        "references": 34,
        "year": 2024
    },
    {
        "title": "VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding",
        "abstract": "3D visual grounding is crucial for robots, requiring integration of natural language and 3D scene understanding. Traditional methods depending on supervised learning with 3D point clouds are limited by scarce datasets. Recently zero-shot methods leveraging LLMs have been proposed to address the data issue. While effective, these methods only use object-centric information, limiting their ability to handle complex queries. In this work, we present VLM-Grounder, a novel framework using vision-language models (VLMs) for zero-shot 3D visual grounding based solely on 2D images. VLM-Grounder dynamically stitches image sequences, employs a grounding and feedback scheme to find the target object, and uses a multi-view ensemble projection to accurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D datasets show VLM-Grounder outperforms previous zero-shot methods, achieving 51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3D geometry or object priors. Codes are available at https://github.com/OpenRobotLab/VLM-Grounder .",
        "authors": [
            "Runsen Xu",
            "Zhiwei Huang",
            "Tai Wang",
            "Yilun Chen",
            "Jiangmiao Pang",
            "Dahua Lin"
        ],
        "citations": 2,
        "references": 56,
        "year": 2024
    },
    {
        "title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models",
        "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large Language Models (MLLMs) can automate the creation of accurate and coherent radiological reports. Existing methods often hallucinate details in text-based reports that don‚Äôt accurately reflect the image content. To mitigate this, we introduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework. We employ a unique self-supervised loss that leverages similarity between pooled image representations and the contextual representations of the generated radiological text, alongside the standard Causal Language Modeling objective, to refine image-text representations. This allows the model to scrutinize and align the generated text through dynamic interaction between a given image and the generated text, therefore reducing hallucination and continuously enhancing nuanced report generation. SERPENT-VLM outperforms existing baselines such as LlaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and Radiology Objects in COntext (ROCO) datasets, and also proves to be robust against noisy images. A qualitative case study emphasizes the significant advancements towards more sophisticated MLLM frameworks for R2Gen, opening paths for further research into self-supervised refinement in the medical imaging domain.",
        "authors": [
            "M. Kapadnis",
            "Sohan Patnaik",
            "Abhilash Nandy",
            "Sourjyadip Ray",
            "Pawan Goyal",
            "Debdoot Sheet"
        ],
        "citations": 2,
        "references": 31,
        "year": 2024
    },
    {
        "title": "IKIM at MEDIQA-M3G 2024: Multilingual Visual Question-Answering for Dermatology through VLM Fine-tuning and LLM Translations",
        "abstract": "This paper presents our solution to the MEDIQA-M3G Challenge at NAACL-ClinicalNLP 2024. We participated in all three languages, ranking first in Chinese and Spanish and third in English. Our approach utilizes LLaVA-med, an open-source, medical vision-language model (VLM) for visual question-answering in Chinese, and Mixtral-8x7B-instruct, a Large Language Model (LLM) for a subsequent translation into English and Spanish. In addition to our final method, we experiment with alternative approaches: Training three different models for each language instead of translating the results from one model, using different combinations and numbers of input images, and additional training on publicly available data that was not part of the original challenge training set.",
        "authors": [
            "Marie Bauer",
            "Constantin Seibold",
            "J. Kleesiek",
            "Amin Dada"
        ],
        "citations": 1,
        "references": 40,
        "year": 2024
    },
    {
        "title": "REO-VLM: Transforming VLM to Meet Regression Challenges in Earth Observation",
        "abstract": "The rapid evolution of Vision Language Models (VLMs) has catalyzed significant advancements in artificial intelligence, expanding research across various disciplines, including Earth Observation (EO). While VLMs have enhanced image understanding and data processing within EO, their applications have predominantly focused on image content description. This limited focus overlooks their potential in geographic and scientific regression tasks, which are essential for diverse EO applications. To bridge this gap, this paper introduces a novel benchmark dataset, called \\textbf{REO-Instruct} to unify regression and generation tasks specifically for the EO domain. Comprising 1.6 million multimodal EO imagery and language pairs, this dataset is designed to support both biomass regression and image content interpretation tasks. Leveraging this dataset, we develop \\textbf{REO-VLM}, a groundbreaking model that seamlessly integrates regression capabilities with traditional generative functions. By utilizing language-driven reasoning to incorporate scientific domain knowledge, REO-VLM goes beyond solely relying on EO imagery, enabling comprehensive interpretation of complex scientific attributes from EO data. This approach establishes new performance benchmarks and significantly enhances the capabilities of environmental monitoring and resource management.",
        "authors": [
            "Xizhe Xue",
            "Guoting Wei",
            "Hao Chen",
            "Haokui Zhang",
            "Feng Lin",
            "Chunhua Shen",
            "Xiao Xiang Zhu"
        ],
        "citations": 1,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research",
        "abstract": "Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs. However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia. To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs' robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources. By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings. Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations. Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach. Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm. Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry.",
        "authors": [
            "Qinglong Cao",
            "Yuntian Chen",
            "Lu Lu",
            "Hao Sun",
            "Zhenzhong Zeng",
            "Xiaokang Yang",
            "Dong-juan Zhang"
        ],
        "citations": 1,
        "references": 45,
        "year": 2024
    },
    {
        "title": "VLM-KD: Knowledge Distillation from VLM for Long-Tail Visual Recognition",
        "abstract": "For visual recognition, knowledge distillation typically involves transferring knowledge from a large, well-trained teacher model to a smaller student model. In this paper, we introduce an effective method to distill knowledge from an off-the-shelf vision-language model (VLM), demonstrating that it provides novel supervision in addition to those from a conventional vision-only teacher model. Our key technical contribution is the development of a framework that generates novel text supervision and distills free-form text into a vision encoder. We showcase the effectiveness of our approach, termed VLM-KD, across various benchmark datasets, showing that it surpasses several state-of-the-art long-tail visual classifiers. To our knowledge, this work is the first to utilize knowledge distillation with text supervision generated by an off-the-shelf VLM and apply it to vanilla randomly initialized vision encoders.",
        "authors": [
            "Zaiwei Zhang",
            "Gregory P. Meyer",
            "Zhichao Lu",
            "Ashish Shrivastava",
            "Avinash Ravichandran",
            "Eric M. Wolff"
        ],
        "citations": 1,
        "references": 86,
        "year": 2024
    },
    {
        "title": "Enhancing Video Transformers for Action Understanding with VLM-aided Training",
        "abstract": "Owing to their ability to extract relevant spatio-temporal video embeddings, Vision Transformers (ViTs) are currently the best performing models in video action understanding. However, their generalization over domains or datasets is somewhat limited. In contrast, Visual Language Models (VLMs) have demonstrated exceptional generalization performance, but are currently unable to process videos. Consequently, they cannot extract spatio-temporal patterns that are crucial for action understanding. In this paper, we propose the Four-tiered Prompts (FTP) framework that takes advantage of the complementary strengths of ViTs and VLMs. We retain ViTs' strong spatio-temporal representation ability but improve the visual encodings to be more comprehensive and general by aligning them with VLM outputs. The FTP framework adds four feature processors that focus on specific aspects of human action in videos: action category, action components, action description, and context information. The VLMs are only employed during training, and inference incurs a minimal computation cost. Our approach consistently yields state-of-the-art performance. For instance, we achieve remarkable top-1 accuracy of 93.8% on Kinetics-400 and 83.4% on Something-Something V2, surpassing VideoMAEv2 by 2.8% and 2.6%, respectively.",
        "authors": [
            "Hui Lu",
            "Hu Jian",
            "Ronald Poppe",
            "A. A. Salah"
        ],
        "citations": 1,
        "references": 74,
        "year": 2024
    },
    {
        "title": "AlignBot: Aligning VLM-powered Customized Task Planning with User Reminders Through Fine-Tuning for Household Robots",
        "abstract": "This paper presents AlignBot, a novel framework designed to optimize VLM-powered customized task planning for household robots by effectively aligning with user reminders. In domestic settings, aligning task planning with user reminders poses significant challenges due to the limited quantity, diversity, and multimodal nature of the reminders. To address these challenges, AlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for GPT-4o. This adapter model internalizes diverse forms of user reminders-such as personalized preferences, corrective guidance, and contextual assistance-into structured instruction-formatted cues that prompt GPT-4o in generating customized task plans. Additionally, AlignBot integrates a dynamic retrieval mechanism that selects task-relevant historical successes as prompts for GPT-4o, further enhancing task planning accuracy. To validate the effectiveness of AlignBot, experiments are conducted in real-world household environments, which are constructed within the laboratory to replicate typical household settings. A multimodal dataset with over 1,500 entries derived from volunteer reminders is used for training and evaluation. The results demonstrate that AlignBot significantly improves customized task planning, outperforming existing LLM- and VLM-powered planners by interpreting and aligning with user reminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline at 21.6%, reflecting a 65% improvement and over four times greater effectiveness. Supplementary materials are available at: https://yding25.com/AlignBot/",
        "authors": [
            "Zhaxizhuoma",
            "Pengan Chen",
            "Ziniu Wu",
            "Jiawei Sun",
            "Dong Wang",
            "Peng Zhou",
            "Nieqing Cao",
            "Yan Ding",
            "Bin Zhao",
            "Xuelong Li"
        ],
        "citations": 1,
        "references": 42,
        "year": 2024
    },
    {
        "title": "VisionArena: 230K Real World User-VLM Conversations with Preference Labels",
        "abstract": "With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source platform where users interact with VLMs and submit preference votes - VisionArena spans 73K unique users, 45 VLMs, and 138 languages. Our dataset contains three subsets: VisionArena-Chat, 200k single and multi-turn conversations between a user and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous VLMs with user preference votes; and VisionArena-Bench, an automatic benchmark of 500 diverse user prompts that efficiently approximate the live Chatbot Arena model rankings. Additionally, we highlight the types of question asked by users, the influence of response style on preference, and areas where models often fail. We find open-ended tasks like captioning and humor are highly style-dependent, and current VLMs struggle with spatial reasoning and planning tasks. Lastly, we show finetuning the same base model on VisionArena-Chat outperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point gain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai",
        "authors": [
            "Christopher Chou",
            "Lisa Dunlap",
            "Koki Mashita",
            "Krishna Mandal",
            "Trevor Darrell",
            "I. Stoica",
            "Joseph Gonzalez",
            "Wei-Lin Chiang"
        ],
        "citations": 1,
        "references": 44,
        "year": 2024
    },
    {
        "title": "Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration",
        "abstract": "To create culturally inclusive vision-language models (VLMs), developing a benchmark that tests their ability to address culturally relevant questions is essential. Existing approaches typically rely on human annotators, making the process labor-intensive and creating a cognitive burden in generating diverse questions. To address this, we propose a semi-automated framework for constructing cultural VLM benchmarks, specifically targeting multiple-choice QA. This framework combines human-VLM collaboration, where VLMs generate questions based on guidelines, a small set of annotated examples, and relevant knowledge, followed by a verification process by native speakers. We demonstrate the effectiveness of this framework through the creation of K-Viscuit, a dataset focused on Korean culture. Our experiments on this dataset reveal that open-source models lag behind proprietary ones in understanding Korean culture, highlighting key areas for improvement. We also present a series of further analyses, including human evaluation, augmenting VLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our dataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.",
        "authors": [
            "Yujin Baek",
            "chaeHun Park",
            "Jaeseok Kim",
            "Yu-Jung Heo",
            "Du-Seong Chang",
            "Jaegul Choo"
        ],
        "citations": 1,
        "references": 36,
        "year": 2024
    },
    {
        "title": "Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes",
        "abstract": "‚ÄîRecent research about Large Language Model based autonomous driving solutions shows a promising picture in planning and control fields. However, heavy computational resources and hallucinations of Large Language Models continue to hinder the tasks of predicting precise trajectories and instructing control signals. To address this problem, we propose Co-driver, a novel autonomous driving assistant system to empower autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes. A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model. Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the Visual Language Model module of our system. In the real-world driving dataset, our system achieved 96 . 16% success rate in night scenes and 89 . 7% in gloomy scenes regarding reasonable predictions. Our Co-driver dataset will be released at https://github.com/ZionGo6/Co-driver .",
        "authors": [
            "Ziang Guo",
            "Artem Lykov",
            "Zakhar Yagudin",
            "Mikhail Konenkov",
            "D. Tsetserukou"
        ],
        "citations": 5,
        "references": 24,
        "year": 2024
    },
    {
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "abstract": "Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.",
        "authors": [
            "Di Zhang",
            "Jingdi Lei",
            "Junxian Li",
            "Xunzhi Wang",
            "Yujie Liu",
            "Zonglin Yang",
            "Jiatong Li",
            "Weida Wang",
            "Suorong Yang",
            "Jianbo Wu",
            "Peng Ye",
            "Wanli Ouyang",
            "Dongzhan Zhou"
        ],
        "citations": 3,
        "references": 62,
        "year": 2024
    },
    {
        "title": "VLM-guided Explicit-Implicit Complementary novel class semantic learning for few-shot object detection",
        "abstract": null,
        "authors": [
            "Taijin Zhao",
            "Heqian Qiu",
            "Yu Dai",
            "Lanxiao Wang",
            "Hefei Mei",
            "Fanman Meng",
            "Qingbo Wu",
            "Hongliang Li"
        ],
        "citations": 3,
        "references": 31,
        "year": 2024
    },
    {
        "title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models",
        "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate how a VLM perceives images, specifically focusing on key elements of visual recognition, from primitive color and shape to semantic levels. To this end, we introduce a dataset named LENS to guide a VLM to follow the examination and check its readiness. Once the model is ready, we conduct the examination. Through this examination, we quantify and visualize VLMs' sensitivities to color and shape, and semantic matching. Our findings reveal that VLMs have varying sensitivity to different colors while consistently showing insensitivity to green across different VLMs. Also, we found different shape sensitivity and semantic recognition depending on LLM's capacity despite using the same fixed visual encoder. Our analyses and findings have potential to inspire the design of VLMs and the pre-processing of visual input to VLMs for improving application performance.",
        "authors": [
            "Nam Hyeon-Woo",
            "Moon Ye-Bin",
            "Wonseok Choi",
            "Lee Hyun",
            "Tae-Hyun Oh"
        ],
        "citations": 0,
        "references": 51,
        "year": 2024
    },
    {
        "title": "ViTA: An Efficient Video-to-Text Algorithm using VLM for RAG-based Video Analysis System",
        "abstract": "Retrieval-augmented generation (RAG) is used in natural language processing (NLP) to provide query-relevant information in enterprise documents to large language models (LLMs). Such enterprise context enables the LLMs to generate more informed and accurate responses. When enterprise data is primarily videos, AI models like vision language models (VLMs) are necessary to convert information in videos into text. While essential, this conversion is a bottleneck, especially for large corpus of videos. It delays the timely use of enterprise videos to generate useful responses.We propose ViTA, a novel method that leverages two unique characteristics of VLMs to expedite the conversion process. As VLMs output more text tokens, they incur higher latency. In addition, large (heavyweight) VLMs can extract intricate details from images and videos, but they incur much higher latency per output token when compared to smaller (lightweight) VLMs that may miss details. To expedite conversion, ViTA first employs a lightweight VLM to quickly understand the gist or overview of an image or a video clip, and directs a heavyweight VLM (through prompt engineering) to extract additional details by using only a few (preset number of) output tokens. Our experimental results show that ViTA expedites the conversion time by as much as 43%, without compromising the accuracy of responses when compared to a baseline system that only uses a heavyweight VLM.",
        "authors": [
            "Md. Adnan Arefeen",
            "Biplob K. Debnath",
            "M. Y. S. Uddin",
            "S. Chakradhar"
        ],
        "citations": 0,
        "references": 29,
        "year": 2024
    },
    {
        "title": "AP-VLM: Active Perception Enabled by Vision-Language Models",
        "abstract": "Active perception enables robots to dynamically gather information by adjusting their viewpoints, a crucial capability for interacting with complex, partially observable environments. In this paper, we present AP-VLM, a novel framework that combines active perception with a Vision-Language Model (VLM) to guide robotic exploration and answer semantic queries. Using a 3D virtual grid overlaid on the scene and orientation adjustments, AP-VLM allows a robotic manipulator to intelligently select optimal viewpoints and orientations to resolve challenging tasks, such as identifying objects in occluded or inclined positions. We evaluate our system on two robotic platforms: a 7-DOF Franka Panda and a 6-DOF UR5, across various scenes with differing object configurations. Our results demonstrate that AP-VLM significantly outperforms passive perception methods and baseline models, including Toward Grounded Common Sense Reasoning (TGCSR), particularly in scenarios where fixed camera views are inadequate. The adaptability of AP-VLM in real-world settings shows promise for enhancing robotic systems' understanding of complex environments, bridging the gap between high-level semantic reasoning and low-level control.",
        "authors": [
            "Venkatesh Sripada",
            "Samuel Carter",
            "Frank Guerin",
            "Amir Ghalamzan"
        ],
        "citations": 0,
        "references": 20,
        "year": 2024
    },
    {
        "title": "BDNF Val66Met influences the mediation of sex differences in VLM scores by plasma BDNF in a cohort enriched with risk for Alzheimer‚Äòs disease",
        "abstract": "Abstract Background Brain‚Äêderived neurotrophic factor (BDNF)‚Äîa key neurotrophin involved in synaptic plasticity, neurogenesis, and neuroprotection‚Äîhas been shown to mediate sex differences in verbal learning and memory (VLM) ability, but it remains unclear whether this relationship is conditionally dependent upon carriage of the Val66Met polymorphism in the BDNF gene. This study investigates how BDNFVal66Met carriage influences the mediation of sex differences in VLM scores by plasma BDNF levels in a cohort enriched for AD risk. Method Cognitively unimpaired participants in the Wisconsin Registry for Alzheimer‚Äôs Prevention (WRAP; n=198, age 63.8¬±6y, 66% women, 66% family history of AD, 38% apolipoprotein E4 (APOE‚ÄêŒµ4) carriers, 31% BDNFVal66Met carriers) underwent the Rey Auditory Verbal Learning Test (RAVLT). Scores from learning trials 3‚Äê5 and the delayed recall test were aggregated as a VLM performance index. Plasma BDNF levels were measured using a Human BDNF Quantikine Immunoassay (R&D Systems). Striatified mediation analysis and bootstrapping were performed to test the conditional dependence of BDNF mediation on BDNFVal66Met carriage, and model covariates included age, APOE‚ÄêŒµ4 carriage, parental history of AD, education, hippocampal volume, and date difference between VLM and plasma data acquisition. Result Stratified mediation models showed a significant association between sex and VLM scores in BDNFVal66Met carriers [Œ≤=‚Äê0.61; p=0.04], but no significant association between sex and BDNF levels [Œ≤=0.06; p=0.85]. By comparison, in BDNFVal66Val homozygotes, women had significantly higher BDNF levels [Œ≤=‚Äê0.62; p<0.01] and VLM scores [Œ≤=‚Äê0.77; p<0.01], and bootstrapping showed BDNF to be a significant partial mediator of the effect of sex on VLM [Œ≤=‚Äê0.14; 95% CI: ‚Äê.292, ‚Äê.021]. Conclusion This study indicates that BDNFVal66Met carriage may attenuate the mediating role of plasma BDNF expression on the relationship between sex and VLM scores.",
        "authors": [
            "Kyle J. Edmunds",
            "Gabriella M. Mamlouk",
            "Sarah R. Lose",
            "Sanjay Asthana",
            "M. Sager",
            "Matthew Stremlau",
            "Sterling C Johnson",
            "Henriette van Praag",
            "O. Okonkwo"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs",
        "abstract": "Vision-language models (VLMs) have shown remarkable success across various multi-modal tasks, yet large VLMs encounter significant efficiency challenges due to processing numerous visual tokens. A promising approach to accelerating large VLM inference is using partial information, such as attention maps from specific layers, to assess token importance and prune less essential tokens. However, our study reveals three key insights: (i) Partial attention information is insufficient for accurately identifying critical visual tokens, resulting in suboptimal performance, especially at low token retention ratios; (ii) Global attention information, such as the attention map aggregated across all layers, more effectively preserves essential tokens and maintains comparable performance under aggressive pruning. However, the attention maps from all layers requires a full inference pass, which increases computational load and is therefore impractical in existing methods; and (iii) The global attention map aggregated from a small VLM closely resembles that of a large VLM, suggesting an efficient alternative. Based on these findings, we introduce a \\textbf{training-free} method, \\underline{\\textbf{S}}mall VLM \\underline{\\textbf{G}}uidance for accelerating \\underline{\\textbf{L}}arge VLMs (\\textbf{SGL}). Specifically, we employ the attention map aggregated from a small VLM to guide visual token pruning in a large VLM. Additionally, an early exiting mechanism is developed to fully use the small VLM's predictions, dynamically invoking the larger VLM only when necessary, yielding a superior trade-off between accuracy and computation. Extensive evaluations across 11 benchmarks demonstrate the effectiveness and generalizability of SGL, achieving up to 91\\% pruning ratio for visual tokens while retaining competitive performance.",
        "authors": [
            "Wangbo Zhao",
            "Yizeng Han",
            "Jiasheng Tang",
            "Zhikai Li",
            "Yibing Song",
            "Kai Wang",
            "Zhangyang Wang",
            "Yang You"
        ],
        "citations": 0,
        "references": 67,
        "year": 2024
    },
    {
        "title": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation",
        "abstract": "The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e., solely natural language supervision is sub-optimal for the MLLM's visual understanding ability. To that end, we propose OLA-VLM, the first approach distilling knowledge into the LLM's hidden representations from a set of target visual representations. Firstly, we formulate the objective during the pretraining stage in MLLMs as a coupled optimization of predictive visual embedding and next text-token prediction. Secondly, we investigate MLLMs trained solely with natural language supervision and identify a positive correlation between the quality of visual representations within these models and their downstream performance. Moreover, upon probing our OLA-VLM, we observe improved representation quality owing to the embedding optimization. Thirdly, we demonstrate that our OLA-VLM outperforms the single and multi-encoder baselines, proving our approach's superiority over explicitly feeding the corresponding features to the LLM. Particularly, OLA-VLM boosts performance by an average margin of up to 2.5% on various benchmarks, with a notable improvement of 8.7% on the Depth task in CV-Bench. Our code is open-sourced at https://github.com/SHI-Labs/OLA-VLM .",
        "authors": [
            "Jitesh Jain",
            "Zhengyuan Yang",
            "Humphrey Shi",
            "Jianfeng Gao",
            "Jianwei Yang"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Barking Up The Syntactic Tree: Enhancing VLM Training with Syntactic Losses",
        "abstract": "Vision-Language Models (VLMs) achieved strong performance on a variety of tasks (e.g., image-text retrieval, visual question answering). However, most VLMs rely on coarse-grained image-caption pairs for alignment, relying on data volume to resolve ambiguities and ground linguistic concepts in images. The richer semantic and syntactic structure within text is largely overlooked. To address this, we propose HIerarchically STructured Learning (HIST) that enhances VLM training without any additional supervision, by hierarchically decomposing captions into the constituent Subject, Noun Phrases, and Composite Phrases. Entailment between these constituent components allows us to formulate additional regularization constraints on the VLM attention maps. Specifically, we introduce two novel loss functions: (1) Subject Loss, which aligns image content with the subject of corresponding phrase, acting as an entailment of standard contrastive/matching losses at the Phrase level; (2) Addition Loss, to balance attention across multiple objects. HIST is general, and can be applied to any VLM for which attention between vision and language can be computed; we illustrate its efficacy on BLIP and ALBEF. HIST outperforms baseline VLMs, achieving up to +9.8% improvement in visual grounding, +6.3% in multi-object referring segmentation, +1.1% in image-text retrieval, and +0.2% in visual question answering, underscoring the value of structuring learning in VLMs.",
        "authors": [
            "Jiayun Luo",
            "Mir Rayat Imtiaz Hossain",
            "Boyang Li",
            "Leonid Sigal"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
        "abstract": "The Large Vision Language Model (VLM) has recently addressed remarkable progress in bridging two fundamental modalities. VLM, trained by a sufficiently large dataset, exhibits a comprehensive understanding of both visual and linguistic to perform diverse tasks. To distill this knowledge accurately, in this paper, we introduce a novel approach that explicitly utilizes VLM as an objective function form for the Human-Object Interaction (HOI) detection task (\\textbf{VLM-HOI}). Specifically, we propose a method that quantifies the similarity of the predicted HOI triplet using the Image-Text matching technique. We represent HOI triplets linguistically to fully utilize the language comprehension of VLMs, which are more suitable than CLIP models due to their localization and object-centric nature. This matching score is used as an objective for contrastive optimization. To our knowledge, this is the first utilization of VLM language abilities for HOI detection. Experiments demonstrate the effectiveness of our method, achieving state-of-the-art HOI detection accuracy on benchmarks. We believe integrating VLMs into HOI detection represents important progress towards more advanced and interpretable analysis of human-object interactions.",
        "authors": [
            "Donggoo Kang",
            "Dasol Jeong",
            "Hyunmin Lee",
            "Sangwoo Park",
            "Hasil Park",
            "Sunkyu Kwon",
            "Yeongjoon Kim",
            "Joonki Paik"
        ],
        "citations": 0,
        "references": 56,
        "year": 2024
    },
    {
        "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
        "abstract": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they fall short in addressing the unique demands of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities of geospatial data, which is critical for applications such as environmental monitoring, urban planning, and disaster management. Some of the unique challenges in geospatial domain include temporal analysis for changes, counting objects in large quantities, detecting tiny objects, and understanding relationships between entities occurring in Remote Sensing imagery. To address this gap in the geospatial domain, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and covers a diverse set of variations in visual conditions, object type, and scale. We evaluate several state-of-the-art VLMs to assess their accuracy within the geospatial context. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific examples, highlighting the room for further improvements. Specifically, the best-performing GPT4o achieves only 40\\% accuracy on MCQs, which is only double the random guess performance. Our benchmark is publicly available at https://github.com/The-AI-Alliance/GEO-Bench-VLM .",
        "authors": [
            "M. S. Danish",
            "Muhammad Akhtar Munir",
            "Syed Roshaan Ali Shah",
            "Kartik Kuckreja",
            "F. Khan",
            "Paolo Fraccaro",
            "Alexandre Lacoste",
            "Salman Khan"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "VLM-EMO: Context-Aware Emotion Classification with CLIP",
        "abstract": "Emotion recognition is a pivotal component in various sectors, including medical care, education, service industries, and public safety, due to its potential to enhance interaction and understanding within these contexts. Historically, traditional methods of emotion recognition have primarily concentrated on analyzing facial expressions. While effective to a degree, this approach offers restricted capacity for encoding context, and unable to capture the breadth and depth of emotional responses. In this paper, we proposed the VLM-EMO Model, a novel approach that enhances the capabilities of VLM (Visual Language Model) in the domain of emotion recognition. While VLM itself is a powerful tool for understanding the correlation between images and text, the VLM-EMO Model fine-tunes this ability to focus more intricately on emotional aspects within visual data. This is achieved by integrating emotional intelligence into the model's architecture, allowing it to discern subtle emotional cues and contexts within images.",
        "authors": [
            "Yanyin Yao",
            "Xue Mei",
            "Jianping Xu",
            "Zhichuang Sun",
            "Cheng Zeng",
            "Yuming Chen"
        ],
        "citations": 0,
        "references": 23,
        "year": 2024
    },
    {
        "title": "VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision",
        "abstract": "Human drivers rely on commonsense reasoning to navigate diverse and dynamic real-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models are typically optimized to mimic driving patterns observed in data, without capturing the underlying reasoning processes. This limitation constrains their ability to handle challenging driving scenarios. To close this gap, we propose VLM-AD, a method that leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information and structured action labels. Such supervision enhances the model's ability to learn richer feature representations that capture the rationale behind driving patterns. Importantly, our method does not require a VLM during inference, making it practical for real-time deployment. When integrated with state-of-the-art methods, VLM-AD achieves significant improvements in planning accuracy and reduced collision rates on the nuScenes dataset.",
        "authors": [
            "Yi Xu",
            "Yuxin Hu",
            "Zaiwei Zhang",
            "Gregory P. Meyer",
            "Siva Karthik Mustikovela",
            "Siddhartha Srinivasa",
            "Eric M. Wolff",
            "Xin Huang"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Make VLM Recognize Visual Hallucination on Cartoon Character Image with Pose Information",
        "abstract": "Leveraging large-scale Text-to-Image (TTI) models have become a common technique for generating exemplar or training dataset in the fields of image synthesis, video editing, 3D reconstruction. However, semantic structural visual hallucinations involving perceptually severe defects remain a concern, especially in the domain of non-photorealistic rendering (NPR) such as cartoons and pixelization-style character. To detect these hallucinations in NPR, We propose a novel semantic structural hallucination detection system using Vision-Language Model (VLM). Our approach is to leverage the emerging capability of large language model, in-context learning which denotes that VLM has seen some examples by user for specific downstream task, here hallucination detection. Based on in-context learning, we introduce pose-aware in-context visual learning (PA-ICVL) which improve the overall performance of VLM by further inputting visual data beyond prompts, RGB images and pose information. By incorporating pose guidance, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. Within selected two VLMs, GPT-4v, Gemini pro vision, our proposed PA-ICVL improves the hallucination detection with 50% to 78%, 57% to 80%, respectively. This research advances a capability of TTI models toward real-world applications by mitigating visual hallucinations via in-context visual learning, expanding their potential in non-photorealistic domains. In addition, it showcase how users can boost the downstream-specialized capability of open VLM by harnessing additional conditions. We collect synthetic cartoon-hallucination dataset with TTI models, this dataset and final tuned VLM will be publicly available.",
        "authors": [
            "Bumsoo Kim",
            "Wonseop Shin",
            "Kyuchul Lee",
            "Sanghyun Seo"
        ],
        "citations": 0,
        "references": 49,
        "year": 2024
    },
    {
        "title": "VLM-Auto: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes",
        "abstract": "Recent research on Large Language Models for autonomous driving shows promise in planning and control. However, high computational demands and hallucinations still challenge accurate trajectory prediction and control signal generation. Deterministic algorithms offer reliability but lack adaptability to complex driving scenarios and struggle with context and uncertainty. To address this problem, we propose VLM-Auto, a novel autonomous driving assistant system to empower the autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes. A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model (VLM). Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the VLM module of our system. In CARLA experiments, our system achieved $97.82\\%$ average precision on 5 types of labels in our dataset. In the real-world driving dataset, our system achieved $96.97\\%$ prediction accuracy in night scenes and gloomy scenes. Our VLM-Auto dataset will be released at https://github.com/ZionGo6/VLM-Auto.",
        "authors": [
            "Ziang Guo",
            "Zakhar Yagudin",
            "Artem Lykov",
            "Mikhail Konenkov",
            "D. Tsetserukou"
        ],
        "citations": 0,
        "references": 26,
        "year": 2024
    },
    {
        "title": "VLM-Vac: Enhancing Smart Vacuums through VLM Knowledge Distillation and Language-Guided Experience Replay",
        "abstract": "In this paper, we propose VLM-Vac, a novel framework designed to enhance the autonomy of smart robot vacuum cleaners. Our approach integrates the zero-shot object detection capabilities of a Vision-Language Model (VLM) with a Knowledge Distillation (KD) strategy. By leveraging the VLM, the robot can categorize objects into actionable classes -- either to avoid or to suck -- across diverse backgrounds. However, frequently querying the VLM is computationally expensive and impractical for real-world deployment. To address this issue, we implement a KD process that gradually transfers the essential knowledge of the VLM to a smaller, more efficient model. Our real-world experiments demonstrate that this smaller model progressively learns from the VLM and requires significantly fewer queries over time. Additionally, we tackle the challenge of continual learning in dynamic home environments by exploiting a novel experience replay method based on language-guided sampling. Our results show that this approach is not only energy-efficient but also surpasses conventional vision-based clustering methods, particularly in detecting small objects across diverse backgrounds.",
        "authors": [
            "Reihaneh Mirjalili",
            "Michael Krawez",
            "Florian Walter",
            "Wolfram Burgard"
        ],
        "citations": 0,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Img2CAD: Reverse Engineering 3D CAD Models from Images through VLM-Assisted Conditional Factorization",
        "abstract": "Reverse engineering 3D computer-aided design (CAD) models from images is an important task for many downstream applications including interactive editing, manufacturing, architecture, robotics, etc. The difficulty of the task lies in vast representational disparities between the CAD output and the image input. CAD models are precise, programmatic constructs that involves sequential operations combining discrete command structure with continuous attributes -- making it challenging to learn and optimize in an end-to-end fashion. Concurrently, input images introduce inherent challenges such as photo-metric variability and sensor noise, complicating the reverse engineering process. In this work, we introduce a novel approach that conditionally factorizes the task into two sub-problems. First, we leverage large foundation models, particularly GPT-4V, to predict the global discrete base structure with semantic information. Second, we propose TrAssembler that conditioned on the discrete structure with semantics predicts the continuous attribute values. To support the training of our TrAssembler, we further constructed an annotated CAD dataset of common objects from ShapeNet. Putting all together, our approach and data demonstrate significant first steps towards CAD-ifying images in the wild. Our project page: https://anonymous123342.github.io/",
        "authors": [
            "Yang You",
            "M. Uy",
            "Jiaqi Han",
            "R. Thomas",
            "Haotong Zhang",
            "Suya You",
            "Leonidas J. Guibas"
        ],
        "citations": 2,
        "references": 79,
        "year": 2024
    },
    {
        "title": "VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use",
        "abstract": "While vision-language models (VLMs) have demonstrated remarkable performance across various tasks combining textual and visual information, they continue to struggle with fine-grained visual perception tasks that require detailed pixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs on such intricate visual elements remains an open challenge. In this paper, we present VipAct, an agent framework that enhances VLMs by integrating multi-agent collaboration and vision expert models, enabling more precise visual understanding and comprehensive reasoning. VipAct consists of an orchestrator agent, which manages task requirement analysis, planning, and coordination, along with specialized agents that handle specific tasks such as image captioning and vision expert models that provide high-precision perceptual information. This multi-agent approach allows VLMs to better perform fine-grained visual perception tasks by synergizing planning, reasoning, and tool use. We evaluate VipAct on benchmarks featuring a diverse set of visual perception tasks, with experimental results demonstrating significant performance improvements over state-of-the-art baselines across all tasks. Furthermore, comprehensive ablation studies reveal the critical role of multi-agent collaboration in eliciting more detailed System-2 reasoning and highlight the importance of image input for task planning. Additionally, our error analysis identifies patterns of VLMs' inherent limitations in visual perception, providing insights into potential future improvements. VipAct offers a flexible and extensible framework, paving the way for more advanced visual perception systems across various real-world applications.",
        "authors": [
            "Zhehao Zhang",
            "Ryan A. Rossi",
            "Tong Yu",
            "Franck Dernoncourt",
            "Ruiyi Zhang",
            "Jiuxiang Gu",
            "Sungchul Kim",
            "Xiang Chen",
            "Zichao Wang",
            "Nedim Lipka"
        ],
        "citations": 1,
        "references": 111,
        "year": 2024
    },
    {
        "title": "Q-VLM: Post-training Quantization for Large Vision-Language Models",
        "abstract": "In this paper, we propose a post-training quantization framework of large vision-language models (LVLMs) for efficient multi-modal inference. Conventional quantization methods sequentially search the layer-wise rounding functions by minimizing activation discretization errors, which fails to acquire optimal quantization strategy without considering cross-layer dependency. On the contrary, we mine the cross-layer dependency that significantly influences discretization errors of the entire vision-language model, and embed this dependency into optimal quantization strategy searching with low search cost. Specifically, we observe the strong correlation between the activation entropy and the cross-layer dependency concerning output discretization errors. Therefore, we employ the entropy as the proxy to partition blocks optimally, which aims to achieve satisfying trade-offs between discretization errors and the search cost. Moreover, we optimize the visual encoder to disentangle the cross-layer dependency for fine-grained decomposition of search space, so that the search cost is further reduced without harming the quantization accuracy. Experimental results demonstrate that our method compresses the memory by 2.78x and increase generate speed by 1.44x about 13B LLaVA model without performance degradation on diverse multi-modal reasoning tasks. Code is available at https://github.com/ChangyuanWang17/QVLM.",
        "authors": [
            "Changyuan Wang",
            "Ziwei Wang",
            "Xiuwei Xu",
            "Yansong Tang",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "citations": 1,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Towards vaccine lifecycle management (VLM): A systematic literature review of the issues and challenges",
        "abstract": null,
        "authors": [
            "AL Sanae",
            "Aicha Sekhari Seklouli",
            "A. E. B. E. Idrissi",
            "El Kinani Noredine"
        ],
        "citations": 1,
        "references": 116,
        "year": 2024
    },
    {
        "title": "Can VLM Understand Children's Handwriting? An Analysis on Handwritten Mathematical Equation Recognition",
        "abstract": null,
        "authors": [
            "Cleon Pereira J√∫nior",
            "Luiz Rodrigues",
            "N. Costa",
            "Valmir Mac√°rio Filho",
            "R. Mello"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents",
        "abstract": "Vision Language Models (VLMs) have revolutionized the creation of generalist web agents, empowering them to autonomously complete diverse tasks on real-world websites, thereby boosting human efficiency and productivity. However, despite their remarkable capabilities, the safety and security of these agents against malicious attacks remain critically underexplored, raising significant concerns about their safe deployment. To uncover and exploit such vulnerabilities in web agents, we provide AdvWeb, a novel black-box attack framework designed against web agents. AdvWeb trains an adversarial prompter model that generates and injects adversarial prompts into web pages, misleading web agents into executing targeted adversarial actions such as inappropriate stock purchases or incorrect bank transactions, actions that could lead to severe real-world consequences. With only black-box access to the web agent, we train and optimize the adversarial prompter model using DPO, leveraging both successful and failed attack strings against the target agent. Unlike prior approaches, our adversarial string injection maintains stealth and control: (1) the appearance of the website remains unchanged before and after the attack, making it nearly impossible for users to detect tampering, and (2) attackers can modify specific substrings within the generated adversarial string to seamlessly change the attack objective (e.g., purchasing stocks from a different company), enhancing attack flexibility and efficiency. We conduct extensive evaluations, demonstrating that AdvWeb achieves high success rates in attacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings expose critical vulnerabilities in current LLM/VLM-based agents, emphasizing the urgent need for developing more reliable web agents and effective defenses. Our code and data are available at https://ai-secure.github.io/AdvWeb/ .",
        "authors": [
            "Chejian Xu",
            "Mintong Kang",
            "Jiawei Zhang",
            "Zeyi Liao",
            "Lingbo Mo",
            "Mengqi Yuan",
            "Huan Sun",
            "Bo Li"
        ],
        "citations": 1,
        "references": 37,
        "year": 2024
    },
    {
        "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
        "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community.",
        "authors": [
            "Davide Paglieri",
            "Bart≈Çomiej Cupia≈Ç",
            "Samuel Coward",
            "Ulyana Piterbarg",
            "Maciej Wolczyk",
            "Akbir Khan",
            "Eduardo Pignatelli",
            "Lukasz Kuci'nski",
            "Lerrel Pinto",
            "Rob Fergus",
            "Jakob N. Foerster",
            "Jack Parker-Holder",
            "Tim Rocktaschel"
        ],
        "citations": 1,
        "references": 94,
        "year": 2024
    },
    {
        "title": "BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations",
        "abstract": "This paper presents Bag-of-Concept Graph (BACON) to gift models with limited linguistic abilities to taste the privilege of Vision Language Models (VLMs) and boost downstream tasks such as detection, visual question answering (VQA), and image generation. Since the visual scenes in physical worlds are structured with complex relations between objects, BACON breaks down annotations into basic minimum elements and presents them in a graph structure. Element-wise style enables easy understanding, and structural composition liberates difficult locating. Careful prompt design births the BACON captions with the help of public-available VLMs and segmentation methods. In this way, we gather a dataset with 100K annotated images, which endow VLMs with remarkable capabilities, such as accurately generating BACON, transforming prompts into BACON format, envisioning scenarios in the style of BACONr, and dynamically modifying elements within BACON through interactive dialogue and more. Wide representative experiments, including detection, VQA, and image generation tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel in their current cutting-edge solutions.",
        "authors": [
            "Zhantao Yang",
            "Ruili Feng",
            "Keyu Yan",
            "Huangji Wang",
            "Zhicai Wang",
            "Shangwen Zhu",
            "Han Zhang",
            "Jie Xiao",
            "Ping Wu",
            "Kai Zhu",
            "Jixuan Chen",
            "Chenwei Xie",
            "Chaojie Mao",
            "Yue Yang",
            "Hongyang Zhang",
            "Yu Liu",
            "Fan Cheng"
        ],
        "citations": 1,
        "references": 61,
        "year": 2024
    },
    {
        "title": "AltChart: Enhancing VLM-based Chart Summarization Through Multi-Pretext Tasks",
        "abstract": "Chart summarization is a crucial task for blind and visually impaired individuals as it is their primary means of accessing and interpreting graphical data. Crafting high-quality descriptions is challenging because it requires precise communication of essential details within the chart without vision perception. Many chart analysis methods, however, produce brief, unstructured responses that may contain significant hallucinations, affecting their reliability for blind people. To address these challenges, this work presents three key contributions: (1) We introduce the AltChart dataset, comprising 10,000 real chart images, each paired with a comprehensive summary that features long-context, and semantically rich annotations. (2) We propose a new method for pretraining Vision-Language Models (VLMs) to learn fine-grained chart representations through training with multiple pretext tasks, yielding a performance gain with ${\\sim}2.5\\%$. (3) We conduct extensive evaluations of four leading chart summarization models, analyzing how accessible their descriptions are. Our dataset and codes are publicly available on our project page: https://github.com/moured/AltChart.",
        "authors": [
            "Omar Moured",
            "Jiaming Zhang",
            "M. Sarfraz",
            "Rainer Stiefelhagen"
        ],
        "citations": 1,
        "references": 56,
        "year": 2024
    },
    {
        "title": "HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks",
        "abstract": "Human robot interaction is an exciting task, which aimed to guide robots following instructions from human. Since huge gap lies between human natural language and machine codes, end to end human robot interaction models is fair challenging. Further, visual information receiving from sensors of robot is also a hard language for robot to perceive. In this work, HuBo-VLM is proposed to tackle perception tasks associated with human robot interaction including object detection and visual grounding by a unified transformer based vision language model. Extensive experiments on the Talk2Car benchmark demonstrate the effectiveness of our approach. Code would be publicly available in https://github.com/dzcgaara/HuBo-VLM.",
        "authors": [
            "Zichao Dong",
            "Weikun Zhang",
            "Xufeng Huang",
            "Hang Ji",
            "Xin Zhan",
            "Junbo Chen"
        ],
        "citations": 3,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Prediction of Mean Sea Level with GNSS-VLM Correction Using a Hybrid Deep Learning Model in Australia",
        "abstract": "The prediction of sea level rise is extremely important for improved future climate change mitigation and adaptation strategies. This study uses a hybrid convolutional neural Network (CNN) and a bidirectional long short-term (BiLSTM) model with successive variational mode decomposition (SVMD) to predict the absolute sea level for two study sites in Australia (Port Kembla and Milner Bay). More importantly, the sea level measurements using a tide gauge were corrected using Global Navigation Satellite System (GNSS) measurements of the vertical land movement (VLM). The SVMD-CNN-BiLSTM model was benchmarked by a multi-layer perceptron (MLP), support vector regression (SVR) and gradient boosting (GB). The SVMD-CNN-BiLSTM model outperformed all the comparative models with high correlation values of more than 0.95 for Port Kembla and Milner Bay. Similarly, the SVMD-CNN-BiLSTM model achieved the highest values for the Willmott index, the Nash‚ÄìSutcliffe index and the Legates and McCabe index for both study sites. The projected linear trend showed the expected annual mean sea rise for 2030. Using the current trend, Port Kembla was projected to have an MSL value of 1.03 m with a rate rise of approx. 4.5 mm/year. The rate of the MSL for Milner Bay was comparatively lower with a value of approx. 2.75 mm/year and an expected MSL value of 1.27 m for the year 2030.",
        "authors": [
            "N. Raj",
            "Jason Brown"
        ],
        "citations": 4,
        "references": 49,
        "year": 2023
    },
    {
        "title": "X 2 -VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
        "abstract": "Vision language pre-training aims to learn alignments between vision and language from a large amount of data. We proposed multi-grained vision language pre-training, a uniÔ¨Åed approach which can learn vision language alignments in multiple granularity. This paper advances the proposed method by unifying image and video encoding in one model and scaling up the model with large-scale data. We present X 2 -VLM , a pre-trained VLM with a modular architecture for both image-text tasks and video-text tasks. Experiment results show that X 2 -VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X 2 -VLM results in high transferability for X 2 -VLM to be utilized in any language or domain. For example, by simply replacing the text encoder with XLM-R, X 2 -VLM outperforms state-of-the-art multilingual multi-modal pre-trained models without any multilingual pre-training. The code and pre-trained models will be available at github.com/zengyan-97/X2-VLM .",
        "authors": [
            "Yan Zeng",
            "Xinsong Zhang",
            "Hang Li",
            "Jiawei Wang",
            "Jipeng Zhang",
            "Hkust Wangchunshu Zhou",
            "E. Zurich"
        ],
        "citations": 40,
        "references": 79,
        "year": 2022
    },
    {
        "title": "Leveraging VLM-Based Pipelines to Annotate 3D Objects",
        "abstract": "Pretrained vision language models (VLMs) present an opportunity to caption unlabeled 3D objects at scale. The leading approach to summarize VLM descriptions from different views of an object (Luo et al., 2023) relies on a language model (GPT4) to produce the final output. This text-based aggregation is susceptible to hallucinations as it merges potentially contradictory descriptions. We propose an alternative algorithm to marginalize over factors such as the viewpoint that affect the VLM's response. Instead of merging text-only responses, we utilize the VLM's joint image-text likelihoods. We show our probabilistic aggregation is not only more reliable and efficient, but sets the SoTA on inferring object types with respect to human-verified labels. The aggregated annotations are also useful for conditional inference; they improve downstream predictions (e.g., of object material) when the object's type is specified as an auxiliary text-based input. Such auxiliary inputs allow ablating the contribution of visual reasoning over visionless reasoning in an unsupervised setting. With these supervised and unsupervised evaluations, we show how a VLM-based pipeline can be leveraged to produce reliable annotations for 764K objects from the Objaverse dataset.",
        "authors": [
            "Rishabh Kabra",
            "L. Matthey",
            "Alexander Lerchner",
            "N. Mitra"
        ],
        "citations": 1,
        "references": 59,
        "year": 2023
    },
    {
        "title": "ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints",
        "abstract": "Recent text-to-image generative models have enabled us to transform our words into vibrant, captivating imagery. The surge of personalization techniques that has followed has also allowed us to imagine unique concepts in new scenes. However, an intriguing question remains: How can we generate a new, imaginary concept that has never been seen before? In this article, we present the task of creative text-to-image generation, where we seek to generate new members of a broad category (e.g., generating a pet that differs from all existing pets). We leverage the under-studied Diffusion Prior models and show that the creative generation problem can be formulated as an optimization process over the output space of the diffusion prior, resulting in a set of ‚Äúprior constraints.‚Äù To keep our generated concept from converging into existing members, we incorporate a question-answering Vision-Language Model that adaptively adds new constraints to the optimization problem, encouraging the model to discover increasingly more unique creations. Finally, we show that our prior constraints can also serve as a strong mixing mechanism allowing us to create hybrids between generated concepts, introducing even more flexibility into the creative process.",
        "authors": [
            "Elad Richardson",
            "Kfir Goldberg",
            "Yuval Alaluf",
            "D. Cohen-Or"
        ],
        "citations": 2,
        "references": 52,
        "year": 2023
    },
    {
        "title": "VLM-Eval: A General Evaluation on Video Large Language Models",
        "abstract": "Despite the rapid development of video Large Language Models (LLMs), a comprehensive evaluation is still absent. In this paper, we introduce a unified evaluation that encompasses multiple video tasks, including captioning, question and answering, retrieval, and action recognition. In addition to conventional metrics, we showcase how GPT-based evaluation can match human-like performance in assessing response quality across multiple aspects. We propose a simple baseline: Video-LLaVA, which uses a single linear projection and outperforms existing video LLMs. Finally, we evaluate video LLMs beyond academic datasets, which show encouraging recognition and reasoning capabilities in driving scenarios with only hundreds of video-instruction pairs for fine-tuning. We hope our work can serve as a unified evaluation for video LLMs, and help expand more practical scenarios. The evaluation code will be available soon.",
        "authors": [
            "Shuailin Li",
            "Yuang Zhang",
            "Yucheng Zhao",
            "Qiuyue Wang",
            "Fan Jia",
            "Yingfei Liu",
            "Tiancai Wang"
        ],
        "citations": 1,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Trajectory optimization of the Brazilian multistage launch vehicle VLM-1",
        "abstract": null,
        "authors": [
            "Guilherme da Silveira",
            "Sandro da Silva Fernandes"
        ],
        "citations": 1,
        "references": 40,
        "year": 2023
    },
    {
        "title": "X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq1-3339661.gif\"/></alternatives></inline-formula>-VLM: All-in-One Pre-Trained Model for Vision-Language Tasks",
        "abstract": "Vision language pre-training aims to learn alignments between vision and language from a large amount of data. Most existing methods only learn image-text alignments. Some others utilize pre-trained object detectors to leverage vision language alignments at the object level. In this paper, we propose to learn multi-grained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq3-3339661.gif\"/></alternatives></inline-formula>-VLM, an all-in-one model with a flexible modular architecture, in which we further unify image-text pre-training and video-text pre-training in one model. X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq4-3339661.gif\"/></alternatives></inline-formula>-VLM is able to learn unlimited visual concepts associated with diverse text descriptions. Experiment results show that X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq5-3339661.gif\"/></alternatives></inline-formula>-VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq6-3339661.gif\"/></alternatives></inline-formula>-VLM results in high transferability for it to be utilized in any language or domain. For example, by simply replacing the text encoder with XLM-R, X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq7-3339661.gif\"/></alternatives></inline-formula>-VLM outperforms state-of-the-art multilingual multi-modal pre-trained models without any multilingual pre-training.",
        "authors": [
            "Yan Zeng",
            "Xinsong Zhang",
            "Hang Li",
            "Jiawei Wang",
            "Jipeng Zhang",
            "Hkust Wangchunshu Zhou",
            "E. Zurich"
        ],
        "citations": 11,
        "references": 84,
        "year": 2022
    },
    {
        "title": "An optimisation Model for minimising Totes exchange in VLM and SBS/RS integrated System",
        "abstract": null,
        "authors": [
            "Jakob Marolt",
            "Goran ƒêukiƒá",
            "F. Sgarbossa",
            "T. Lerher"
        ],
        "citations": 2,
        "references": 7,
        "year": 2022
    },
    {
        "title": "SST-VLM: Sparse Sampling-Twice Inspired Video-Language Model",
        "abstract": null,
        "authors": [
            "Yizhao Gao",
            "Zhiwu Lu"
        ],
        "citations": 1,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
        "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
        "authors": [
            "Jean-Baptiste Alayrac",
            "Jeff Donahue",
            "Pauline Luc",
            "Antoine Miech",
            "Iain Barr",
            "Yana Hasson",
            "Karel Lenc",
            "A. Mensch",
            "Katie Millican",
            "Malcolm Reynolds",
            "Roman Ring",
            "Eliza Rutherford",
            "Serkan Cabi",
            "Tengda Han",
            "Zhitao Gong",
            "Sina Samangooei",
            "Marianne Monteiro",
            "Jacob Menick",
            "Sebastian Borgeaud",
            "Andy Brock",
            "Aida Nematzadeh",
            "Sahand Sharifzadeh",
            "Mikolaj Binkowski",
            "Ricardo Barreira",
            "O. Vinyals",
            "Andrew Zisserman",
            "K. Simonyan"
        ],
        "citations": 1000,
        "references": 182,
        "year": 2022
    },
    {
        "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
        "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. Code and models are available at https://github.com/dvlab-research/MiniGemini.",
        "authors": [
            "Yanwei Li",
            "Yuechen Zhang",
            "Chengyao Wang",
            "Zhisheng Zhong",
            "Yixin Chen",
            "Ruihang Chu",
            "Shaoteng Liu",
            "Jiaya Jia"
        ],
        "citations": 151,
        "references": 67,
        "year": 2024
    },
    {
        "title": "What matters when building vision-language models?",
        "abstract": "The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.",
        "authors": [
            "Hugo Lauren√ßon",
            "L√©o Tronchon",
            "Matthieu Cord",
            "Victor Sanh"
        ],
        "citations": 95,
        "references": 144,
        "year": 2024
    },
    {
        "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
        "abstract": "Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size difference. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in training recipe including data quality, training pipeline and VLM architecture. Our workfeatures the first Internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qual-itative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of thought spatial reasoning and robotics due to its quantitative estimation capability. Website: https://spatial-vlm.github.iol",
        "authors": [
            "Boyuan Chen",
            "Zhuo Xu",
            "Sean Kirmani",
            "Brian Ichter",
            "Danny Driess",
            "Pete Florence",
            "Dorsa Sadigh",
            "Leonidas J. Guibas",
            "Fei Xia"
        ],
        "citations": 94,
        "references": 72,
        "year": 2024
    },
    {
        "title": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model",
        "abstract": "We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at https://github.com/Meituan-AutoML/MobileVLM .",
        "authors": [
            "Xiangxiang Chu",
            "Limeng Qiao",
            "Xinyu Zhang",
            "Shuang Xu",
            "Fei Wei",
            "Yang Yang",
            "Xiaofei Sun",
            "Yiming Hu",
            "Xinyang Lin",
            "Bo Zhang",
            "Chunhua Shen"
        ],
        "citations": 75,
        "references": 68,
        "year": 2024
    },
    {
        "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
        "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
        "authors": [
            "Wenlong Huang",
            "Chen Wang",
            "Ruohan Zhang",
            "Yunzhu Li",
            "Jiajun Wu",
            "Li Fei-Fei"
        ],
        "citations": 375,
        "references": 146,
        "year": 2023
    },
    {
        "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
        "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization, and challenge sets that probe properties such as hallucination; evaluations that provide fine-grained insight VLM capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and training from base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible training code, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open VLMs.",
        "authors": [
            "Siddharth Karamcheti",
            "Suraj Nair",
            "A. Balakrishna",
            "Percy Liang",
            "Thomas Kollar",
            "Dorsa Sadigh"
        ],
        "citations": 64,
        "references": 74,
        "year": 2024
    },
    {
        "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
        "abstract": "Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
        "authors": [
            "Soroush Nasiriany",
            "Fei Xia",
            "Wenhao Yu",
            "Ted Xiao",
            "Jacky Liang",
            "Ishita Dasgupta",
            "Annie Xie",
            "Danny Driess",
            "Ayzaan Wahid",
            "Zhuo Xu",
            "Q. Vuong",
            "Tingnan Zhang",
            "T. Lee",
            "Kuang-Huei Lee",
            "Peng Xu",
            "Sean Kirmani",
            "Yuke Zhu",
            "Andy Zeng",
            "Karol Hausman",
            "N. Heess",
            "Chelsea Finn",
            "Sergey Levine",
            "Brian Ichter"
        ],
        "citations": 61,
        "references": 66,
        "year": 2024
    },
    {
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "abstract": "People are spending an enormous amount of time on dig-ital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogA-gent supports input at a resolution of1120 √ó 1120, enabling it to recognize tiny page elements and text. As a general-ist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK- VQA, Text- Vqa, St- Vqa, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks-Mind2Web and AITW, ad-vancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM.",
        "authors": [
            "Wenyi Hong",
            "Weihan Wang",
            "Qingsong Lv",
            "Jiazheng Xu",
            "Wenmeng Yu",
            "Junhui Ji",
            "Yan Wang",
            "Zihan Wang",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "citations": 215,
        "references": 42,
        "year": 2023
    },
    {
        "title": "VILA: On Pre-training for Visual Language Models",
        "abstract": "Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge. VILA is also deployable on Jetson Orin for on-device VLM.",
        "authors": [
            "Ji Lin",
            "Hongxu Yin",
            "Wei Ping",
            "Yao Lu",
            "Pavlo Molchanov",
            "Andrew Tao",
            "Huizi Mao",
            "Jan Kautz",
            "Mohammad Shoeybi",
            "Song Han"
        ],
        "citations": 204,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Vision-Language Models for Vision Tasks: A Survey",
        "abstract": "Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlation from web-scale image-text pairs that are almost infinitely available on the Internet and enables zero-shot predictions on various visual recognition tasks with a single VLM. This paper provides a systematic review of visual language models for various visual recognition tasks, including: (1) the background that introduces the development of visual recognition paradigms; (2) the foundations of VLM that summarize the widely-adopted network architectures, pre-training objectives, and downstream tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4) the review and categorization of existing VLM pre-training methods, VLM transfer learning methods, and VLM knowledge distillation methods; (5) the benchmarking, analysis and discussion of the reviewed methods; (6) several research challenges and potential research directions that could be pursued in the future VLM studies for visual recognition.",
        "authors": [
            "Jingyi Zhang",
            "Jiaxing Huang",
            "Sheng Jin",
            "Shijian Lu"
        ],
        "citations": 273,
        "references": 233,
        "year": 2023
    },
    {
        "title": "Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization",
        "abstract": "Prompt tuning is an effective way to adapt the pretrained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge is worse generalization to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. The key insight of KgCoOp is that the forgetting about essential knowledge can be alleviated by reducing the discrepancy between the learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the discrepancy between the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can make a discriminative prompt for both seen and unseen tasks. Extensive evaluation of several benchmarks demonstrates that the proposed Knowledge-guided Context Optimization is an efficient method for prompt tuning, i.e., achieves better performance with less training time. code.",
        "authors": [
            "Hantao Yao",
            "Rui Zhang",
            "Changsheng Xu"
        ],
        "citations": 131,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Multi-Modal Hallucination Control by Visual Information Grounding",
        "abstract": "Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as ‚Äúhallucination‚Äù and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt. M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If training is an option, we show that M3ID can be paired with Direct Preference Optimization (DPO) to improve the model's reliance on the prompt image without requiring any labels. Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24%.",
        "authors": [
            "Alessandro Favero",
            "L. Zancato",
            "Matthew Trager",
            "Siddharth Choudhary",
            "Pramuditha Perera",
            "A. Achille",
            "Ashwin Swaminathan",
            "S. Soatto"
        ],
        "citations": 34,
        "references": 32,
        "year": 2024
    },
    {
        "title": "DriveLM: Driving with Graph Visual Question Answering",
        "abstract": "We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen objects or sensor configurations. We hope this work can be the starting point to shed new light on how to apply VLMs for autonomous driving. To facilitate future research, all code, data, and models are available to the public.",
        "authors": [
            "Chonghao Sima",
            "Katrin Renz",
            "Kashyap Chitta",
            "Li Chen",
            "Hanxue Zhang",
            "Chengen Xie",
            "Ping Luo",
            "Andreas Geiger",
            "Hongyang Li"
        ],
        "citations": 106,
        "references": 101,
        "year": 2023
    },
    {
        "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
        "abstract": "Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks. In this paper, we address the limitation above by 1) introducing vision-language Model with Multi-Modal In-Context Learning(MMICL), a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts. Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context. Our code, dataset, dataset tool, and model are available at https://github.com/PKUnlp-icler/MIC",
        "authors": [
            "Haozhe Zhao",
            "Zefan Cai",
            "Shuzheng Si",
            "Xiaojian Ma",
            "Kaikai An",
            "Liang Chen",
            "Zixuan Liu",
            "Sheng Wang",
            "Wenjuan Han",
            "Baobao Chang"
        ],
        "citations": 115,
        "references": 140,
        "year": 2023
    },
    {
        "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
        "abstract": "Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.",
        "authors": [
            "Lei Li",
            "Yuwei Yin",
            "Shicheng Li",
            "Liang Chen",
            "Peiyi Wang",
            "Shuhuai Ren",
            "Mukai Li",
            "Yazheng Yang",
            "Jingjing Xu",
            "Xu Sun",
            "Lingpeng Kong",
            "Qi Liu"
        ],
        "citations": 102,
        "references": 64,
        "year": 2023
    },
    {
        "title": "HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models",
        "abstract": "Large language models (LLMs), after being aligned with vision models and integrated into vision-language models (VLMs), can bring impressive improvement in image reasoning tasks. This was shown by the recently released GPT-4V(ison), LLaVA-1.5, etc. However, the strong language prior in these SOTA LVLMs can be a double-edged sword: they may ignore the image context and solely rely on the (even contradictory) language prior for reasoning. In contrast, the vision modules in VLMs are weaker than LLMs and may result in misleading visual representations, which are then translated to confident mistakes by LLMs. To study these two types of VLM mistakes, i.e., language hallucination and visual illusion , we curated ‚ÄúH ALLUSION B ENCH 1 ,‚Äù an image-context reasoning benchmark that is still challenging to even GPT-4V and LLaVA-1.5. We provide a detailed analysis of examples in H ALLUSION B ENCH , which sheds novel insights on the illusion or hallucination of VLMs and how to improve them in the future. The benchmark and codebase will be released at https://github.com/tianyi-lab/HallusionBench.",
        "authors": [
            "Fuxiao Liu",
            "Tianrui Guan",
            "Xiyang Wu",
            "Zongxia Li",
            "Lichang Chen",
            "Yaser Yacoob",
            "Dinesh Manocha",
            "Tianyi Zhou"
        ],
        "citations": 93,
        "references": 30,
        "year": 2023
    },
    {
        "title": "Aligning Bag of Regions for Open-Vocabulary Object Detection",
        "abstract": "Pre-trained vision-language models (VLMs) learn to align vision and language representations on large-scale datasets, where each image-text pair usually contains a bag of semantic concepts. However, existing open-vocabulary object detectors only align region embeddings individually with the corresponding features extracted from the VLMs. Such a design leaves the compositional structure of semantic concepts in a scene under-exploited, although the structure may be implicitly learned by the VLMs. In this work, we propose to align the embedding of bag of regions beyond individual regions. The proposed method groups contextually interrelated regions as a bag. The embeddings of regions in a bag are treated as embeddings of words in a sentence, and they are sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which is learned to be aligned to the corresponding features extracted by a frozen VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the previous best results by 4.6 box AP50 and 2.8 mask AP on novel categories of open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are available at https://github.com/wusize/ovdet.",
        "authors": [
            "Size Wu",
            "Wenwei Zhang",
            "Sheng Jin",
            "Wentao Liu",
            "Chen Change Loy"
        ],
        "citations": 87,
        "references": 62,
        "year": 2023
    },
    {
        "title": "An Introduction to Vision-Language Modeling",
        "abstract": "Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.",
        "authors": [
            "Florian Bordes",
            "Richard Yuanzhe Pang",
            "Anurag Ajay",
            "Alexander C. Li",
            "Adrien Bardes",
            "Suzanne Petryk",
            "Oscar Ma√±as",
            "Zhiqiu Lin",
            "Anas Mahmoud",
            "Bargav Jayaraman",
            "Mark Ibrahim",
            "Melissa Hall",
            "Yunyang Xiong",
            "Jonathan Lebensold",
            "Candace Ross",
            "Srihari Jayakumar",
            "Chuan Guo",
            "Diane Bouchacourt",
            "Haider Al-Tahan",
            "Karthik Padthe",
            "Vasu Sharma",
            "Huijuan Xu",
            "Xiaoqing Ellen Tan",
            "Megan Richards",
            "Samuel Lavoie",
            "Pietro Astolfi",
            "Reyhane Askari Hemmat",
            "Jun Chen",
            "Kushal Tirumala",
            "Rim Assouel",
            "Mazda Moayeri",
            "Arjang Talattof",
            "Kamalika Chaudhuri",
            "Zechun Liu",
            "Xilun Chen",
            "Q. Garrido",
            "Karen Ullrich",
            "Aishwarya Agrawal",
            "Kate Saenko",
            "Asli Celikyilmaz",
            "Vikas Chandra"
        ],
        "citations": 33,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Vision-language models for medical report generation and visual question answering: a review",
        "abstract": "Medical vision-language models (VLMs) combine computer vision (CV) and natural language processing (NLP) to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on publicly available models designed for medical report generation and visual question answering (VQA). We provide background on NLP and CV, explaining how techniques from both fields are integrated into VLMs, with visual and language data often fused using Transformer-based architectures to enable effective learning from multimodal data. Key areas we address include the exploration of 18 public medical vision-language datasets, in-depth analyses of the architectures and pre-training strategies of 16 recent noteworthy medical VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs' performance in medical report generation and VQA. We also highlight current challenges facing medical VLM development, including limited data availability, concerns with data privacy, and lack of proper evaluation metrics, among others, while also proposing future directions to address these obstacles. Overall, our review summarizes the recent progress in developing VLMs to harness multimodal medical data for improved healthcare applications.",
        "authors": [
            "Iryna Hartsock",
            "Ghulam Rasool"
        ],
        "citations": 29,
        "references": 217,
        "year": 2024
    },
    {
        "title": "GeoChat:Grounded Large Vision-Language Model for Remote Sensing",
        "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have shown great promise in natural image domains, allowing users to hold a dialogue about given visual content. However, such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios, leading to inaccurate or fabricated information when presented with RS domain-specific queries. Such a behavior emerges due to the unique challenges introduced by RS imagery. For example, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-level reasoning is necessary alongside holistic scene inter-pretation. Furthermore, the lack of domain-specific multimodal instruction following data as well as strong back-bone models for RS make it hard for the models to align their behavior with user queries. To address these limitations, we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabilities with high-resolution RS images. Specifically, GeoChat can not only answer image-level queries but also accepts region inputs to hold region-specific dialogue. Further-more, it can visually ground objects in its responses by referring to their spatial coordinates. To address the lack of domain-specific datasets, we generate a novel RS multimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets. We establish a comprehensive benchmarkfor RS multitask conversations and compare with a number of baseline methods. GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conversations and referring detection. Our code is available here.",
        "authors": [
            "Kartik Kuckreja",
            "M. S. Danish",
            "Muzammal Naseer",
            "Abhijit Das",
            "Salman H. Khan",
            "F. Khan"
        ],
        "citations": 75,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
        "abstract": "Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct in-depth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs' capabilities but rather modulates the model's responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.",
        "authors": [
            "Zhiyang Xu",
            "Chao Feng",
            "Rulin Shao",
            "Trevor Ashby",
            "Ying Shen",
            "dingnan jin",
            "Yu Cheng",
            "Qifan Wang",
            "Lifu Huang"
        ],
        "citations": 26,
        "references": 0,
        "year": 2024
    },
    {
        "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
        "abstract": "This paper presents PaLI-3, a smaller, faster, and stronger vision language model (VLM) that compares favorably to similar models that are 10x larger. As part of arriving at this strong performance, we compare Vision Transformer (ViT) models pretrained using classification objectives to contrastively (SigLIP) pretrained ones. We find that, while slightly underperforming on standard image classification benchmarks, SigLIP-based PaLI shows superior performance across various multimodal benchmarks, especially on localization and visually-situated text understanding. We scale the SigLIP image encoder up to 2 billion parameters, and achieves a new state-of-the-art on multilingual cross-modal retrieval. We hope that PaLI-3, at only 5B parameters, rekindles research on fundamental pieces of complex VLMs, and could fuel a new generation of scaled-up models.",
        "authors": [
            "Xi Chen",
            "Xiao Wang",
            "Lucas Beyer",
            "Alexander Kolesnikov",
            "Jialin Wu",
            "P. Voigtlaender",
            "Basil Mustafa",
            "Sebastian Goodman",
            "Ibrahim M. Alabdulmohsin",
            "Piotr Padlewski",
            "Daniel M. Salz",
            "Xi Xiong",
            "Daniel Vlasic",
            "Filip Pavetic",
            "Keran Rong",
            "Tianli Yu",
            "Daniel Keysers",
            "Xiao-Qi Zhai",
            "Radu Soricut"
        ],
        "citations": 77,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology",
        "abstract": "Recent advances in AI combine large language models (LLMs) with vision encoders that bring forward unprecedented technical capabilities to leverage for a wide range of healthcare applications. Focusing on the domain of radiology, vision-language models (VLMs) achieve good performance results for tasks such as generating radiology findings based on a patient‚Äôs medical image, or answering visual questions (e.g., ‚ÄúWhere are the nodules in this chest X-ray?‚Äù). However, the clinical utility of potential applications of these capabilities is currently underexplored. We engaged in an iterative, multidisciplinary design process to envision clinically relevant VLM interactions, and co-designed four VLM use concepts: Draft Report Generation, Augmented Report Review, Visual Search and Querying, and Patient Imaging History Highlights. We studied these concepts with 13 radiologists and clinicians who assessed the VLM concepts as valuable, yet articulated many design considerations. Reflecting on our findings, we discuss implications for integrating VLM capabilities in radiology, and for healthcare AI more generally.",
        "authors": [
            "Nur Yildirim",
            "Hannah Richardson",
            "M. Wetscherek",
            "Junaid Bajwa",
            "Joseph Jacob",
            "Mark A. Pinnock",
            "Stephen Harris",
            "Daniel Coelho De Castro",
            "Shruthi Bannur",
            "Stephanie L. Hyland",
            "Pratik Ghosh",
            "M. Ranjit",
            "Kenza Bouzid",
            "Anton Schwaighofer",
            "Fernando P'erez-Garc'ia",
            "Harshita Sharma",
            "O. Oktay",
            "M. Lungren",
            "Javier Alvarez-Valle",
            "A. Nori",
            "Anja Thieme"
        ],
        "citations": 24,
        "references": 148,
        "year": 2024
    },
    {
        "title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
        "abstract": "Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PHYSOBJECTS, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically grounded VLMs. We additionally illustrate the benefits of our physically grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford.edu/pg-vlm/.",
        "authors": [
            "Jensen Gao",
            "Bidipta Sarkar",
            "F. Xia",
            "Ted Xiao",
            "Jiajun Wu",
            "Brian Ichter",
            "Anirudha Majumdar",
            "Dorsa Sadigh"
        ],
        "citations": 81,
        "references": 86,
        "year": 2023
    },
    {
        "title": "MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting",
        "abstract": "Open-world generalization requires robotic systems to have a profound understanding of the physical world and the user command to solve diverse and complex tasks. While the recent advancement in vision-language models (VLMs) has offered unprecedented opportunities to solve open-world problems, how to leverage their capabilities to control robots remains a grand challenge. In this paper, we introduce Marking Open-world Keypoint Affordances (MOKA), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language instructions. Central to our approach is a compact point-based representation of affordance, which bridges the VLM's predictions on observed images and the robot's actions in the physical world. By prompting the pre-trained VLM, our approach utilizes the VLM's commonsense knowledge and concept understanding acquired from broad data sources to predict affordances and generate motions. To facilitate the VLM's reasoning in zero-shot and few-shot manners, we propose a visual prompting technique that annotates marks on images, converting affordance reasoning into a series of visual question-answering problems that are solvable by the VLM. We further explore methods to enhance performance with robot experiences collected by MOKA through in-context learning and policy distillation. We evaluate and analyze MOKA's performance on various table-top manipulation tasks including tool use, deformable body manipulation, and object rearrangement.",
        "authors": [
            "Kuan Fang",
            "Fangchen Liu",
            "Pieter Abbeel",
            "Sergey Levine"
        ],
        "citations": 23,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Prompt-aligned Gradient for Prompt Tuning",
        "abstract": "Thanks to the large pre-trained vision-language models (VLMs) like CLIP [37], we can craft a zero-shot classifier by discrete prompt design, e.g., the confidence score of an image being \"[CLASS]\" can be obtained by using the VLM provided similarity between the image and the prompt sentence \"a photo of a [CLASS]\". Furthermore, prompting shows great potential for fast adaptation of VLMs to downstream tasks if we fine-tune the soft prompts with few samples. However, we find a common failure that improper fine-tuning or learning with extremely few-shot samples may even under-perform the zero-shot prediction. Existing methods still address this problem by using traditional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution specific to prompting. In this paper, we present Prompt-aligned Gradient, dubbed ProGrad to prevent prompt tuning from forgetting the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradient is aligned (or non-conflicting) to the general knowledge, which is represented as the optimization direction offered by the pre-defined prompt predictions. Extensive experiments under the few-shot learning, domain generalization, base-to-new generalization and cross-dataset transfer settings demonstrate the stronger few-shot generalization ability of ProGrad over state-of-the-art prompt tuning methods.",
        "authors": [
            "Beier Zhu",
            "Yulei Niu",
            "Yucheng Han",
            "Yuehua Wu",
            "Hanwang Zhang"
        ],
        "citations": 210,
        "references": 66,
        "year": 2022
    },
    {
        "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset",
        "abstract": "Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.",
        "authors": [
            "Hugo Lauren√ßon",
            "L√©o Tronchon",
            "Victor Sanh"
        ],
        "citations": 21,
        "references": 11,
        "year": 2024
    },
    {
        "title": "On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving",
        "abstract": "The pursuit of autonomous driving technology hinges on the sophisticated integration of perception, decision-making, and control systems. Traditional approaches, both data-driven and rule-based, have been hindered by their inability to grasp the nuance of complex driving environments and the intentions of other road users. This has been a significant bottleneck, particularly in the development of common sense reasoning and nuanced scene understanding necessary for safe and reliable autonomous driving. The advent of Visual Language Models (VLM) represents a novel frontier in realizing fully autonomous vehicle driving. This report provides an exhaustive evaluation of the latest state-of-the-art VLM, GPT-4V(ision), and its application in autonomous driving scenarios. We explore the model's abilities to understand and reason about driving scenes, make decisions, and ultimately act in the capacity of a driver. Our comprehensive tests span from basic scene recognition to complex causal reasoning and real-time decision-making under varying conditions. Our findings reveal that GPT-4V demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems. It showcases the potential to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts. However, challenges remain, particularly in direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks. These limitations underscore the need for further research and development. Project is now available on GitHub for interested parties to access and utilize: \\url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}",
        "authors": [
            "Licheng Wen",
            "Xuemeng Yang",
            "Daocheng Fu",
            "Xiaofeng Wang",
            "Pinlong Cai",
            "Xin Li",
            "Tao Ma",
            "Yingxuan Li",
            "Linran Xu",
            "Dengke Shang",
            "Zheng Zhu",
            "Shaoyan Sun",
            "Yeqi Bai",
            "Xinyu Cai",
            "Min Dou",
            "Shuanglu Hu",
            "Botian Shi",
            "Yu Qiao"
        ],
        "citations": 66,
        "references": 26,
        "year": 2023
    },
    {
        "title": "Teaching CLIP to Count to Ten",
        "abstract": "Large vision-language models (VLMs), such as CLIP, learn rich joint image-text representations, facilitating advances in numerous downstream tasks, including zero-shot classification and text-to-image generation. Nevertheless, existing VLMs exhibit a prominent well-documented limitation ‚Äì they fail to encapsulate compositional concepts such as counting. We introduce a simple yet effective method to improve the quantitative understanding of VLMs, while maintaining their overall performance on common benchmarks. Specifically, we propose a new counting-contrastive loss used to finetune a pre-trained VLM in tandem with its original objective. Our counting loss is deployed over automatically-created counterfactual examples, each consisting of an image and a caption containing an incorrect object count. For example, an image depicting three dogs is paired with the caption \"Six dogs playing in the yard\" as a negative example. Our loss encourages discrimination between the correct caption and its counterfactual variant which serves as a hard negative example. To the best of our knowledge, this work is the first to extend CLIP‚Äôs capabilities to object counting. Furthermore, we introduce \"CountBench\" ‚Äì a new image-text counting benchmark for evaluating object counting capabilities. We demonstrate a significant improvement over state-of-the-art baseline models on this task. Finally, we leverage our counting-aware CLIP model for image retrieval and text-conditioned image generation, demonstrating that our model can produce specific counts of objects more reliably than existing ones.",
        "authors": [
            "Roni Paiss",
            "Ariel Ephrat",
            "Omer Tov",
            "Shiran Zada",
            "Inbar Mosseri",
            "M. Irani",
            "Tali Dekel"
        ],
        "citations": 67,
        "references": 74,
        "year": 2023
    },
    {
        "title": "Building and better understanding vision-language models: insights and future directions",
        "abstract": "The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting promising research directions for underexplored areas. We then walk through the practical steps to build Idefics3-8B, a powerful VLM that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline. These steps include the creation of Docmatix, a dataset for improving document understanding capabilities, which is 240 times larger than previously available datasets. We release the model along with the datasets created for its training.",
        "authors": [
            "Hugo Lauren√ßon",
            "Andr'es Marafioti",
            "Victor Sanh",
            "L√©o Tronchon"
        ],
        "citations": 22,
        "references": 213,
        "year": 2024
    },
    {
        "title": "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics",
        "abstract": "From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train RoboPoint, a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real-world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, RoboPoint is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that RoboPoint outperforms state-of-the-art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy of predicting spatial affordance and by 30.5% in the success rate of downstream tasks. Project website: https://robo-point.github.io.",
        "authors": [
            "Wentao Yuan",
            "Jiafei Duan",
            "Valts Blukis",
            "Wilbert Pumacay",
            "Ranjay Krishna",
            "Adithyavairavan Murali",
            "A. Mousavian",
            "Dieter Fox"
        ],
        "citations": 22,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases",
        "abstract": "Large Vision-Language Models (LVLMs) have received widespread attention for advancing the interpretable self-driving. Existing evaluations of LVLMs primarily focus on multi-faceted capabilities in natural circumstances, lacking automated and quantifiable assessment for self-driving, let alone the severe road corner cases. In this work, we propose CODA-LM, the very first benchmark for the automatic evaluation of LVLMs for self-driving corner cases. We adopt a hierarchical data structure and prompt powerful LVLMs to analyze complex driving scenes and generate high-quality pre-annotations for the human annotators, while for LVLM evaluation, we show that using the text-only large language models (LLMs) as judges reveals even better alignment with human preferences than the LVLM judges. Moreover, with our CODA-LM, we build CODA-VLM, a new driving LVLM surpassing all open-sourced counterparts on CODA-LM. Our CODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V by +21.42% on the regional perception task. We hope CODA-LM can become the catalyst to promote interpretable self-driving empowered by LVLMs.",
        "authors": [
            "Yanze Li",
            "Wenhua Zhang",
            "Kai Chen",
            "Yanxin Liu",
            "Pengxiang Li",
            "Ruiyuan Gao",
            "Lanqing Hong",
            "Meng Tian",
            "Xinhai Zhao",
            "Zhenguo Li",
            "Dit-Yan Yeung",
            "Huchuan Lu",
            "Xu Jia"
        ],
        "citations": 20,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
        "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.",
        "authors": [
            "Yuexiang Zhai",
            "Hao Bai",
            "Zipeng Lin",
            "Jiayi Pan",
            "Shengbang Tong",
            "Yifei Zhou",
            "Alane Suhr",
            "Saining Xie",
            "Yann LeCun",
            "Yi Ma",
            "Sergey Levine"
        ],
        "citations": 21,
        "references": 79,
        "year": 2024
    },
    {
        "title": "Negative Label Guided OOD Detection with Pretrained Vision-Language Models",
        "abstract": "Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM architectures. Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts. The codes are available at https://github.com/tmlr-group/NegLabel.",
        "authors": [
            "Xue Jiang",
            "Feng Liu",
            "Zhengfeng Fang",
            "Hong Chen",
            "Tongliang Liu",
            "Feng Zheng",
            "Bo Han"
        ],
        "citations": 17,
        "references": 51,
        "year": 2024
    },
    {
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations",
        "abstract": "Vision-Language Models (VLMs) have demonstrated their broad effectiveness thanks to extensive training in aligning visual instructions to responses. However, such training of conclusive alignment leads models to ignore essential visual reasoning, further resulting in failures in meticulous visual problems and unfaithful responses. Drawing inspiration from human cognition in solving visual problems (e.g., marking, zoom in), this paper introduces Chain of Manipulations, a mechanism that enables VLMs to solve problems step-by-step with evidence. After training, models can solve various visual problems by eliciting intrinsic manipulations (e.g., grounding, zoom in) with results (e.g., boxes, image) actively without involving external tools, while also allowing users to trace error causes. We study the roadmap to implement this mechanism, including (1) a flexible design of manipulations upon extensive analysis, (2) an efficient automated data generation pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image, and (4) a model training process for versatile capabilities. With the design, we also manually annotate 6K high-quality samples for the challenging graphical mathematical problems. Our trained model, \\textbf{CogCoM}, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability. Our code, model weights, and collected data are publicly available at https://github.com/THUDM/CogCoM.",
        "authors": [
            "Ji Qi",
            "Ming Ding",
            "Weihan Wang",
            "Yushi Bai",
            "Qingsong Lv",
            "Wenyi Hong",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "citations": 15,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Explore until Confident: Efficient Exploration for Embodied Question Answering",
        "abstract": "We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's question answering confidence, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence. Webpage with experiment videos and code: https://explore-eqa.github.io/",
        "authors": [
            "Allen Ren",
            "Jaden Clark",
            "Anushri Dixit",
            "Masha Itkina",
            "Anirudha Majumdar",
            "Dorsa Sadigh"
        ],
        "citations": 16,
        "references": 62,
        "year": 2024
    },
    {
        "title": "RePLan: Robotic Replanning with Perception and Language Models",
        "abstract": "Advancements in large language models (LLMs) have demonstrated their potential in facilitating high-level reasoning, logical reasoning and robotics planning. Recently, LLMs have also been able to generate reward functions for low-level robot actions, effectively bridging the interface between high-level planning and low-level robot control. However, the challenge remains that even with syntactically correct plans, robots can still fail to achieve their intended goals due to imperfect plans or unexpected environmental issues. To overcome this, Vision Language Models (VLMs) have shown remarkable success in tasks such as visual question answering. Leveraging the capabilities of VLMs, we present a novel framework called Robotic Replanning with Perception and Language Models (RePLan) that enables online replanning capabilities for long-horizon tasks. This framework utilizes the physical grounding provided by a VLM's understanding of the world's state to adapt robot actions when the initial plan fails to achieve the desired goal. We developed a Reasoning and Control (RC) benchmark with eight long-horizon tasks to test our approach. We find that RePLan enables a robot to successfully adapt to unforeseen obstacles while accomplishing open-ended, long-horizon goals, where baseline models cannot, and can be readily applied to real robots. Find more information at https://replan-lm.github.io/replan.github.io/",
        "authors": [
            "Marta Skreta",
            "Zihan Zhou",
            "Jia Lin Yuan",
            "Kourosh Darvish",
            "Al√°n Aspuru-Guzik",
            "Animesh Garg"
        ],
        "citations": 18,
        "references": 68,
        "year": 2024
    },
    {
        "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
        "abstract": "Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphical User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing daily computer tasks. Finally, we train a model, ScreenAgent, which achieves comparable computer control capabilities to GPT-4V and demonstrated more precise UI positioning capabilities. Our attempts could inspire further research on building a generalist LLM agent. The code and more detailed information are at https://github.com/niuzaisheng/ScreenAgent.",
        "authors": [
            "Runliang Niu",
            "Jindong Li",
            "Shiqi Wang",
            "Yali Fu",
            "Xiyu Hu",
            "Xueyuan Leng",
            "He Kong",
            "Yi Chang",
            "Qi Wang"
        ],
        "citations": 19,
        "references": 37,
        "year": 2024
    },
    {
        "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning",
        "abstract": "Training corpuses for vision language models (VLMs) typically lack sufficient amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal for decision-making tasks such as in-the-wild device control through graphical user interfaces (GUIs). While training with static demonstrations has shown some promise, we show that such methods fall short for controlling real GUIs due to their failure to deal with real-world stochasticity and non-stationarity not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model, followed by offline-to-online RL. To do this, we build a scalable and parallelizable Android learning environment equipped with a VLM-based evaluator and develop a simple yet effective RL approach for learning in this domain. Our approach runs advantage-weighted RL with advantage estimators enhanced to account for stochasticity along with an automatic curriculum for deriving maximal learning signal. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.3B VLM trained with RL achieves a 49.5% absolute improvement -- from 17.7 to 67.2% success rate -- over supervised fine-tuning with static human demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent trained with AitW data (38.5%), but also the prior best autonomous RL approach based on filtered behavior cloning (57.8%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.",
        "authors": [
            "Hao Bai",
            "Yifei Zhou",
            "Mert Cemri",
            "Jiayi Pan",
            "Alane Suhr",
            "Sergey Levine",
            "Aviral Kumar"
        ],
        "citations": 15,
        "references": 0,
        "year": 2024
    },
    {
        "title": "BRAVE: Broadening the visual encoding of vision-language models",
        "abstract": "Vision-language models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks. Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g.\"blindness\"to certain image features, visual hallucination, etc. To address these issues, we study broadening the visual encoding capabilities of VLMs. We first comprehensively benchmark several vision encoders with different inductive biases for solving VLM tasks. We observe that there is no single encoding configuration that consistently achieves top performance across different tasks, and encoders with different biases can perform surprisingly similarly. Motivated by this, we introduce a method, named BRAVE, that consolidates features from multiple frozen encoders into a more versatile representation that can be directly fed as the input to a frozen LM. BRAVE achieves state-of-the-art performance on a broad range of captioning and VQA benchmarks and significantly reduces the aforementioned issues of VLMs, while requiring a smaller number of trainable parameters than existing methods and having a more compressed representation. Our results highlight the potential of incorporating different visual biases for a more broad and contextualized visual understanding of VLMs.",
        "authors": [
            "Ouguzhan Fatih Kar",
            "A. Tonioni",
            "Petra Poklukar",
            "Achin Kulshrestha",
            "Amir Zamir",
            "Federico Tombari"
        ],
        "citations": 16,
        "references": 99,
        "year": 2024
    },
    {
        "title": "CLIP2: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data",
        "abstract": "Contrastive Language-Image Pre-training, benefiting from large-scale unlabeled text-image pairs, has demonstrated great performance in open-world vision understanding tasks. However, due to the limited Text-3D data pairs, adapting the success of 2D Vision-Language Models (VLM) to the 3D space remains an open problem. Existing works that leverage VLM for 3D understanding generally resort to constructing intermediate 2D representations for the 3D data, but at the cost of losing 3D geometry information. To take a step toward open-world 3D vision understanding, we propose Contrastive Language-Image-Point Cloud Pretraining (CLIP2) to directly learn the transferable 3D point cloud representation in realistic scenarios with a novel proxy alignment mechanism. Specifically, we exploit naturally-existed correspondences in 2D and 3D scenarios, and build well-aligned and instance-based text-image-point proxies from those complex scenarios. On top of that, we propose a cross-modal contrastive objective to learn semantic and instance-level aligned point cloud representation. Experimental results on both indoor and outdoor scenarios show that our learned 3D representation has great transfer ability in downstream tasks, including zero-shot and few-shot 3D recognition, which boosts the state-of-the-art methods by large margins. Furthermore, we provide analyses of the capability of different representations in real scenarios and present the optional ensemble scheme.",
        "authors": [
            "Yi Zeng",
            "Chenhan Jiang",
            "Jiageng Mao",
            "Jianhua Han",
            "Chao Ye",
            "Qingqiu Huang",
            "Dit-Yan Yeung",
            "Zhen Yang",
            "Xiaodan Liang",
            "Hang Xu"
        ],
        "citations": 57,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
        "abstract": "Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that encode semantic features of visual observations based on the VLM's internal knowledge and reasoning capabilities, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings from off-the-shelf, general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings. Finally, we show that our approach can use chain-of-thought prompting to produce representations of common-sense semantic reasoning, improving policy performance in novel scenes by 1.5 times.",
        "authors": [
            "William Chen",
            "Oier Mees",
            "Aviral Kumar",
            "Sergey Levine"
        ],
        "citations": 15,
        "references": 70,
        "year": 2024
    },
    {
        "title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views",
        "abstract": "Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.",
        "authors": [
            "Francis Engelmann",
            "Fabian Manhardt",
            "Michael Niemeyer",
            "Keisuke Tateno",
            "Marc Pollefeys",
            "Federico Tombari"
        ],
        "citations": 15,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
        "abstract": "Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",
        "authors": [
            "Luke Bailey",
            "Euan Ong",
            "Stuart Russell",
            "Scott Emmons"
        ],
        "citations": 54,
        "references": 53,
        "year": 2023
    },
    {
        "title": "WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences",
        "abstract": "Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar. Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.",
        "authors": [
            "Yujie Lu",
            "Dongfu Jiang",
            "Wenhu Chen",
            "William Yang Wang",
            "Yejin Choi",
            "Bill Yuchen Lin"
        ],
        "citations": 14,
        "references": 79,
        "year": 2024
    },
    {
        "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
        "abstract": "LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity to process state information as visual-textual prompts and respond with policy decisions in text. We propose LLaRA: Large Language and Robotics Assistant, a framework that formulates robot action policy as conversations and provides improved action outputs when trained with auxiliary data that complements policy learning. We first introduce an automated pipeline to generate conversation-style instruction tuning data from existing behavior cloning data. Then we enrich the dataset in a self-supervised fashion by formulating six auxiliary tasks. A VLM finetuned with the resulting collection of datasets can generate meaningful robot action policy decisions. Our experiments across multiple simulated and real-world environments demonstrate the state-of-the-art performance of the proposed LLaRA framework. The code, datasets, and pretrained models are available at https://github.com/LostXine/LLaRA.",
        "authors": [
            "Xiang Li",
            "Cristina Mata",
            "Jong Sung Park",
            "Kumara Kahatapitiya",
            "Yoo Sung Jang",
            "Jinghuan Shang",
            "Kanchana Ranasinghe",
            "R. Burgert",
            "Mu Cai",
            "Yong Jae Lee",
            "M. Ryoo"
        ],
        "citations": 13,
        "references": 80,
        "year": 2024
    },
    {
        "title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation",
        "abstract": "Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision",
        "authors": [
            "Seongyun Lee",
            "Seungone Kim",
            "Sue Hyun Park",
            "Geewook Kim",
            "Minjoon Seo"
        ],
        "citations": 13,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Why are Visually-Grounded Language Models Bad at Image Classification?",
        "abstract": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet. To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs. Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data. Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models. Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset.",
        "authors": [
            "Yuhui Zhang",
            "Alyssa Unell",
            "Xiaohan Wang",
            "Dhruba Ghosh",
            "Yuchang Su",
            "Ludwig Schmidt",
            "S. Yeung-Levy"
        ],
        "citations": 12,
        "references": 55,
        "year": 2024
    },
    {
        "title": "Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data",
        "abstract": "Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess world knowledge that leads to strong performance on location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting. Our benchmark is publicly available on this website. A full version of this paper can be found here.",
        "authors": [
            "Chenhui Zhang",
            "Sherrie Wang"
        ],
        "citations": 12,
        "references": 60,
        "year": 2024
    },
    {
        "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second\"baseline\"prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.",
        "authors": [
            "Juan Rocamonde",
            "Victoriano Montesinos",
            "Elvis Nava",
            "Ethan Perez",
            "David Lindner"
        ],
        "citations": 43,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Fasting-activated ventrolateral medulla neurons regulate T cell homing and suppress autoimmune disease in mice",
        "abstract": null,
        "authors": [
            "Liang Wang",
            "Mingxiu Cheng",
            "Yuchen Wang",
            "Jing Chen",
            "Famin Xie",
            "Li-Hao Huang",
            "Cheng Zhan"
        ],
        "citations": 10,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Benchmarking Vision Language Models for Cultural Understanding",
        "abstract": "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM‚Äôs geo-diverse cultural understanding. We curate a diverse collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly weaker capabilities for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.",
        "authors": [
            "Shravan Nayak",
            "Kanishk Jain",
            "Rabiul Awal",
            "Siva Reddy",
            "Sjoerd van Steenkiste",
            "Lisa Anne Hendricks",
            "Karolina Sta≈Ñczak",
            "Aishwarya Agrawal"
        ],
        "citations": 11,
        "references": 39,
        "year": 2024
    },
    {
        "title": "CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments",
        "abstract": "We present CoNVOI, a novel method for autonomous robot navigation in real-world indoor and outdoor environments using Vision Language Models (VLMs). We employ VLMs in two ways: first, we leverage their zero-shot image classification capability to identify the context or scenario (e.g., indoor corridor, outdoor terrain, crosswalk, etc) of the robot‚Äôs surroundings, and formulate context-based navigation behaviors as simple text prompts (e.g. \"stay on the pavement\"). Second, we utilize their state-of-the-art semantic understanding and logical reasoning capabilities to compute a suitable trajectory given the identified context. To this end, we propose a novel multi-modal visual marking approach to annotate the obstacle-free regions in the RGB image used as input to the VLM with numbers, by correlating it with a local occupancy map of the environment. The marked numbers ground image locations in the real-world, direct the VLM‚Äôs attention solely to navigable locations, and elucidate the spatial relationships between them and terrains depicted in the image to the VLM. Next, we query the VLM to select numbers on the marked image that satisfy the context-based behavior text prompt, and construct a reference path using the selected numbers. Finally, we propose a method to extrapolate the reference trajectory when the robot‚Äôs environmental context has not changed to prevent unnecessary VLM queries. We use the reference trajectory to guide a motion planner, and demonstrate that it leads to human-like behaviors (e.g. not cutting through a group of people, using crosswalks, etc.) in various real-world indoor and outdoor scenarios. We perform several ablations and navigation comparisons and demonstrate that CoNVOI‚Äôs trajectories are most similar to human teleoperated ground truth in terms of Fr√©chet distance (9.7-58.2% closer), lowest path errors (up to 88.13% lower), and up to 86.09% lower % of unacceptable paths.",
        "authors": [
            "A. Sathyamoorthy",
            "Kasun Weerakoon",
            "Mohamed Bashir Elnoor",
            "Anuj Zore",
            "Brian Ichter",
            "Fei Xia",
            "Jie Tan",
            "Wenhao Yu",
            "Dinesh Manocha"
        ],
        "citations": 11,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection",
        "abstract": "Data selection in instruction tuning emerges as a pivotal process for acquiring high-quality data and training instruction-following large language models (LLMs), but it is still a new and unexplored research area for vision-language models (VLMs). Existing data selection approaches on LLMs either rely on single unreliable scores, or use downstream tasks for selection, which is time-consuming and can lead to potential over-fitting on the chosen evaluation datasets. To address this challenge, we introduce a novel dataset selection method, Self-Filter, that utilizes the VLM itself as a filter. This approach is inspired by the observation that VLMs benefit from training with the most challenging instructions. Self-Filter operates in two stages. In the first stage, we devise a scoring network to evaluate the difficulty of training instructions, which is co-trained with the VLM. In the second stage, we use the trained score net to measure the difficulty of each instruction, select the most challenging samples, and penalize similar samples to encourage diversity. Comprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can reach better results compared to full data settings with merely about 15% samples, and can achieve superior performance against competitive baselines.",
        "authors": [
            "Ruibo Chen",
            "Yihan Wu",
            "Lichang Chen",
            "Guodong Liu",
            "Qi He",
            "Tianyi Xiong",
            "Chenxi Liu",
            "Junfeng Guo",
            "Heng Huang"
        ],
        "citations": 10,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography",
        "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current shortage of both general and specialized radiologists, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies while simultaneously using the images to extract novel physiological insights. Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs) that utilize both the image and the corresponding textual radiology reports. However, current medical VLMs are generally limited to 2D images and short reports. To overcome these shortcomings for abdominal CT interpretation, we introduce Merlin - a 3D VLM that leverages both structured electronic health records (EHR) and unstructured radiology reports for pretraining without requiring additional manual annotations. We train Merlin using a high-quality clinical dataset of paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens) for training. We comprehensively evaluate Merlin on 6 task types and 752 individual tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year chronic disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs). We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines. We derive data scaling laws to empirically assess training data needs for requisite downstream task performance. Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU. This computationally efficient design can help democratize foundation model training, especially for health systems with compute constraints. We plan to release our trained models, code, and dataset, pending manual removal of all protected health information.",
        "authors": [
            "Louis Blankemeier",
            "Joseph Paul Cohen",
            "Ashwin Kumar",
            "Dave Van Veen",
            "Syed Jamal Safdar Gardezi",
            "Magdalini Paschali",
            "Zhihong Chen",
            "Jean-Benoit Delbrouck",
            "E. Reis",
            "C. Truyts",
            "Christian Bluethgen",
            "Malte E. K. Jensen",
            "Sophie Ostmeier",
            "Maya Varma",
            "Jeya Maria Jose Valanarasu",
            "Zhongnan Fang",
            "Zepeng Huo",
            "Zaid Nabulsi",
            "Diego Ardila",
            "Wei-Hung Weng",
            "Edson Amaro Junior",
            "Neera Ahuja",
            "J. Fries",
            "Nigam H. Shah",
            "Andrew Johnston",
            "Robert D. Boutin",
            "Andrew Wentland",
            "C. Langlotz",
            "Jason Hom",
            "S. Gatidis",
            "Akshay S. Chaudhari"
        ],
        "citations": 11,
        "references": 94,
        "year": 2024
    },
    {
        "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models",
        "abstract": "The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT",
        "authors": [
            "Haoxuan You",
            "Rui Sun",
            "Zhecan Wang",
            "Long Chen",
            "Gengyu Wang",
            "Hammad A. Ayyubi",
            "Kai-Wei Chang",
            "Shih-Fu Chang"
        ],
        "citations": 39,
        "references": 51,
        "year": 2023
    },
    {
        "title": "VILA2: VILA Augmented VILA",
        "abstract": "Visual language models (VLMs) have rapidly progressed, driven by the success of large language models (LLMs). While model architectures and training infrastructures advance rapidly, data curation remains under-explored. When data quantity and quality become a bottleneck, existing work either directly crawls more raw data from the Internet that does not have a guarantee of data quality or distills from black-box commercial models ( e.g ., GPT-4V [1] / Gemini [2]) causing the performance upper bounded by that model. In this work, we introduce a novel approach that includes a self-augment step and a specialist-augment step to iteratively improve data quality and model performance. In the self-augment step, a VLM recaptions its own pretraining data to enhance data quality, and then retrains from scratch using this refined dataset to improve model performance. This process can iterate for several rounds. Once self-augmentation saturates, we employ several specialist VLMs finetuned from the self-augmented VLM with domain-specific expertise, to further infuse specialist knowledge into the generalist VLM through task-oriented recaptioning and retraining. With the combined self-augmented and specialist-augmented training, we introduce VILA 2 ( VILA-augmented-VILA ), a VLM family that consistently improves the accuracy on a wide range of tasks over prior art, and achieves new state-of-the-art results on MMMU leaderboard [3] among open-sourced models.",
        "authors": [
            "Yunhao Fang",
            "Ligeng Zhu",
            "Yao Lu",
            "Yan Wang",
            "Pavlo Molchanov",
            "Jang Hyun Cho",
            "Marco Pavone",
            "Song Han",
            "Hongxu Yin"
        ],
        "citations": 8,
        "references": 84,
        "year": 2024
    },
    {
        "title": "SpatialBot: Precise Spatial Understanding with Vision Language Models",
        "abstract": "Vision Language Models (VLMs) have achieved impressive performance in 2D image understanding, however they are still struggling with spatial understanding which is the foundation of Embodied AI. In this paper, we propose SpatialBot for better spatial understanding by feeding both RGB and depth images. Additionally, we have constructed the SpatialQA dataset, which involves multi-level depth-related questions to train VLMs for depth understanding. Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilities in spatial understanding at different levels. Extensive experiments on our spatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks, demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The model, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.",
        "authors": [
            "Wenxiao Cai",
            "Yaroslav Ponomarenko",
            "Jianhao Yuan",
            "Xiaoqi Li",
            "Wankou Yang",
            "Hao Dong",
            "Bo Zhao"
        ],
        "citations": 9,
        "references": 90,
        "year": 2024
    },
    {
        "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging VLMs' text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker's intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.",
        "authors": [
            "Yuancheng Xu",
            "Jiarui Yao",
            "Manli Shu",
            "Yanchao Sun",
            "Zichu Wu",
            "Ning Yu",
            "Tom Goldstein",
            "Furong Huang"
        ],
        "citations": 9,
        "references": 44,
        "year": 2024
    },
    {
        "title": "LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors",
        "abstract": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.",
        "authors": [
            "Sheng Jin",
            "Xueying Jiang",
            "Jiaxing Huang",
            "Lewei Lu",
            "Shijian Lu"
        ],
        "citations": 9,
        "references": 41,
        "year": 2024
    },
    {
        "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs",
        "abstract": "Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multi-modal data containing mostly captions without explicit spa-tial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spa-tial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.",
        "authors": [
            "Michael Dorkenwald",
            "Nimrod Barazani",
            "Cees G. M. Snoek",
            "Yuki M. Asano"
        ],
        "citations": 9,
        "references": 76,
        "year": 2024
    },
    {
        "title": "Vision-Language Models for Feature Detection of Macular Diseases on Optical Coherence Tomography.",
        "abstract": "Importance\nVision-language models (VLMs) are a novel artificial intelligence technology capable of processing image and text inputs. While demonstrating strong generalist capabilities, their performance in ophthalmology has not been extensively studied.\n\n\nObjective\nTo assess the performance of the Gemini Pro VLM in expert-level tasks for macular diseases from optical coherence tomography (OCT) scans.\n\n\nDesign, Setting, and Participants\nThis was a cross-sectional diagnostic accuracy study evaluating a generalist VLM on ophthalmology-specific tasks using the open-source Optical Coherence Tomography Image Database. The dataset included OCT B-scans from 50 unique patients: healthy individuals and those with macular hole, diabetic macular edema, central serous chorioretinopathy, and age-related macular degeneration. Each OCT scan was labeled for 10 key pathological features, referral recommendations, and treatments. The images were captured using a Cirrus high definition OCT machine (Carl Zeiss Meditec) at Sankara Nethralaya Eye Hospital, Chennai, India, and the dataset was published in December 2018. Image acquisition dates were not specified.\n\n\nExposures\nGemini Pro, using a standard prompt to extract structured responses on December 15, 2023.\n\n\nMain Outcomes and Measures\nThe primary outcome was model responses compared against expert labels, calculating F1 scores for each pathological feature. Secondary outcomes included accuracy in diagnosis, referral urgency, and treatment recommendation. The model's internal concordance was evaluated by measuring the alignment between referral and treatment recommendations, independent of diagnostic accuracy.\n\n\nResults\nThe mean F1 score was 10.7% (95% CI, 2.4-19.2). Measurable F1 scores were obtained for macular hole (36.4%; 95% CI, 0-71.4), pigment epithelial detachment (26.1%; 95% CI, 0-46.2), subretinal hyperreflective material (24.0%; 95% CI, 0-45.2), and subretinal fluid (20.0%; 95% CI, 0-45.5). A correct diagnosis was achieved in 17 of 50 cases (34%; 95% CI, 22-48). Referral recommendations varied: 28 of 50 were correct (56%; 95% CI, 42-70), 10 of 50 were overcautious (20%; 95% CI, 10-32), and 12 of 50 were undercautious (24%; 95% CI, 12-36). Referral and treatment concordance were very high, with 48 of 50 (96%; 95 % CI, 90-100) and 48 of 49 (98%; 95% CI, 94-100) correct answers, respectively.\n\n\nConclusions and Relevance\nIn this study, a generalist VLM demonstrated limited vision capabilities for feature detection and management of macular disease. However, it showed low self-contradiction, suggesting strong language capabilities. As VLMs continue to improve, validating their performance on large benchmarking datasets will help ascertain their potential in ophthalmology.",
        "authors": [
            "F. Antaki",
            "Reena Chopra",
            "P. Keane"
        ],
        "citations": 9,
        "references": 5,
        "year": 2024
    },
    {
        "title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs",
        "abstract": "Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of category-specific prompts resulting in a strong zero-shot classifier. MPVR generalizes effectively across various popular zero-shot image recognition benchmarks belonging to widely different domains when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively",
        "authors": [
            "M. J. Mirza",
            "Leonid Karlinsky",
            "Wei Lin",
            "Sivan Doveh",
            "Jakub Micorek",
            "M. Kozi≈Ñski",
            "Hilde Kuhene",
            "Horst Possegger"
        ],
        "citations": 9,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
        "abstract": "Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We propose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by \\citet{chen2023pali3}, while also enabling much better performance on PlotQA and FigureQA. We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by \\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model is fine-tuned using the multitask loss introduced by \\citet{hsieh2023distilling}. Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought prompt \\cite{chen2023program}, our model outperforms the recently introduced Gemini Ultra and GPT-4V.",
        "authors": [
            "Victor Carbune",
            "Hassan Mansoor",
            "Fangyu Liu",
            "Rahul Aralikatte",
            "Gilles Baechler",
            "Jindong Chen",
            "Abhanshu Sharma"
        ],
        "citations": 8,
        "references": 24,
        "year": 2024
    },
    {
        "title": "SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing",
        "abstract": "Remote sensing imagery, despite its broad applications in helping achieve Sustainable Development Goals and tackle climate change, has not yet benefited from the recent advancements of versatile, task-agnostic vision language models (VLMs). A key reason is that the large-scale, semantically diverse image-text dataset required for developing VLMs is still absent for remote sensing images. Unlike natural images, remote sensing images and their associated text descriptions cannot be efficiently collected from the public Internet at scale. In this work, we bridge this gap by using geo-coordinates to automatically connect open, unlabeled remote sensing images with rich semantics covered in OpenStreetMap, and thus construct SkyScript, a comprehensive vision-language dataset for remote sensing images, comprising 2.6 million image-text pairs covering 29K distinct semantic tags. \nWith continual pre-training on this dataset, we obtain a VLM that surpasses baseline models with a 6.2% average accuracy gain in zero-shot scene classification across seven benchmark datasets. It also demonstrates the ability of zero-shot transfer for fine-grained object attribute classification and cross-modal retrieval. We hope this dataset can support the advancement of VLMs for various multi-modal tasks in remote sensing, such as open-vocabulary classification, retrieval, captioning, and text-to-image synthesis.",
        "authors": [
            "Zhecheng Wang",
            "R. Prabha",
            "Tianyuan Huang",
            "Jiajun Wu",
            "Ram Rajagopal"
        ],
        "citations": 31,
        "references": 45,
        "year": 2023
    },
    {
        "title": "RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model",
        "abstract": "Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we tried several Parameter-Efficient Fine-Tuning methods on RS5M to implement the DFM. Experimental results show that our proposed dataset is highly effective for various tasks, improving upon the baseline by 8% ‚Äû 16% in zero-shot classification tasks, and obtaining good results in both Vision-Language Retrieval and Semantic Localization tasks. https",
        "authors": [
            "Zilun Zhang",
            "Tiancheng Zhao",
            "Yulong Guo",
            "Jianwei Yin"
        ],
        "citations": 30,
        "references": 130,
        "year": 2023
    },
    {
        "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
        "abstract": "Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. When integrated into an embodied agent, existing embodied VLM works either output detailed action sequences at the manipulation level or only provide plans at an abstract level, leaving a gap between high-level planning and real-world manipulation. To bridge this gap, we introduce Octopus, an embodied vision-language programmer that uses executable code generation as a medium to connect planning and manipulation. Octopus is designed to 1) proficiently comprehend an agent's visual and textual task objectives, 2) formulate intricate action sequences, and 3) generate executable code. To facilitate Octopus model development, we introduce OctoVerse: a suite of environments tailored for benchmarking vision-based code generators on a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games such as Grand Theft Auto (GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an explorative agent that generates training data, i.e., action blueprints and corresponding executable code. We also collect feedback that enables an enhanced training scheme called Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's functionality and present compelling results, showing that the proposed RLEF refines the agent's decision-making. By open-sourcing our simulation environments, dataset, and model architecture, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.",
        "authors": [
            "Jingkang Yang",
            "Yuhao Dong",
            "Shuai Liu",
            "Bo Li",
            "Ziyue Wang",
            "Chencheng Jiang",
            "Haoran Tan",
            "Jiamu Kang",
            "Yuanhan Zhang",
            "Kaiyang Zhou",
            "Ziwei Liu"
        ],
        "citations": 30,
        "references": 102,
        "year": 2023
    },
    {
        "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
        "abstract": "Given an image and a target modification (e.g an image of the Eiffel tower and the text\"without people and at night-time\"), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we propose to tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods. Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.",
        "authors": [
            "Shyamgopal Karthik",
            "Karsten Roth",
            "Massimiliano Mancini",
            "Zeynep Akata"
        ],
        "citations": 30,
        "references": 76,
        "year": 2023
    },
    {
        "title": "œÄ0: A Vision-Language-Action Flow Model for General Robot Control",
        "abstract": "Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.",
        "authors": [
            "Kevin Black",
            "Noah Brown",
            "Danny Driess",
            "Adnan Esmail",
            "Michael Equi",
            "Chelsea Finn",
            "Niccolo Fusai",
            "Lachy Groom",
            "Karol Hausman",
            "Brian Ichter",
            "Szymon Jakubczak",
            "Tim Jones",
            "Liyiming Ke",
            "Sergey Levine",
            "Adrian Li-Bell",
            "Mohith Mothukuri",
            "Suraj Nair",
            "Karl Pertsch",
            "Lucy Xiaoyang Shi",
            "James Tanner",
            "Quan Vuong",
            "Anna Walling",
            "Haohuan Wang",
            "Ury Zhilinsky"
        ],
        "citations": 6,
        "references": 61,
        "year": 2024
    },
    {
        "title": "MyVLM: Personalizing VLMs for User-Specific Queries",
        "abstract": "Recent large-scale vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and generating textual descriptions for visual content. However, these models lack an understanding of user-specific concepts. In this work, we take a first step toward the personalization of VLMs, enabling them to learn and reason over user-provided concepts. For example, we explore whether these models can learn to recognize you in an image and communicate what you are doing, tailoring the model to reflect your personal experiences and relationships. To effectively recognize a variety of user-specific concepts, we augment the VLM with external concept heads that function as toggles for the model, enabling the VLM to identify the presence of specific target concepts in a given image. Having recognized the concept, we learn a new concept embedding in the intermediate feature space of the VLM. This embedding is tasked with guiding the language model to naturally integrate the target concept in its generated response. We apply our technique to BLIP-2 and LLaVA for personalized image captioning and further show its applicability for personalized visual question-answering. Our experiments demonstrate our ability to generalize to unseen images of learned concepts while preserving the model behavior on unrelated inputs.",
        "authors": [
            "Yuval Alaluf",
            "Elad Richardson",
            "Sergey Tulyakov",
            "Kfir Aberman",
            "D. Cohen-Or"
        ],
        "citations": 6,
        "references": 98,
        "year": 2024
    },
    {
        "title": "Harmonizing Generalization and Personalization in Federated Prompt Learning",
        "abstract": "Federated Prompt Learning (FPL) incorporates large pre-trained Vision-Language models (VLM) into federated learning through prompt tuning. The transferable representations and remarkable generalization capacity of VLM make them highly compatible with the integration of federated learning. Addressing data heterogeneity in federated learning requires personalization, but excessive focus on it across clients could compromise the model's ability to generalize effectively. To preserve the impressive generalization capability of VLM, it is crucial to strike a balance between personalization and generalization in FPL. To tackle this challenge, we proposed Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP), which employs pre-trained CLIP to provide knowledge-guidance on the global prompt for improved generalization and incorporates a low-rank adaptation term to personalize the global prompt. Further, FedPGP integrates a prompt-wise contrastive loss to achieve knowledge guidance and personalized adaptation simultaneously, enabling a harmonious balance between personalization and generalization in FPL. We conduct extensive experiments on various datasets to explore base-to-novel generalization in both category-level and domain-level scenarios with heterogeneous data, showing the superiority of FedPGP in balancing generalization and personalization.",
        "authors": [
            "Tianyu Cui",
            "Hongxia Li",
            "Jingya Wang",
            "Ye Shi"
        ],
        "citations": 6,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback",
        "abstract": "Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). The previous approaches for VLMMs involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and adding additional learnable modules. Video and text multimodal alignment remains challenging, primarily due to the deficient volume and quality of multimodal instruction-tune data compared to text-only data. We present a novel alignment strategy that employs multimodal AI system to oversee itself called Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content. Demonstrating enhanced performance across diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.",
        "authors": [
            "Daechul Ahn",
            "Yura Choi",
            "Youngjae Yu",
            "Dongyeop Kang",
            "Jonghyun Choi"
        ],
        "citations": 6,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Learning without Forgetting for Vision-Language Models",
        "abstract": "Class-Incremental Learning (CIL) or continual learning is a desired capability in the real world, which requires a learning system to adapt to new tasks without forgetting former ones. While traditional CIL methods focus on visual information to grasp core features, recent advances in Vision-Language Models (VLM) have shown promising capabilities in learning generalizable representations with the aid of textual information. However, when continually trained with new classes, VLMs often suffer from catastrophic forgetting of former knowledge. Applying VLMs to CIL poses two major challenges: 1) how to adapt the model without forgetting; and 2) how to make full use of the multi-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that enables VLMs to learn without forgetting. To handle the first challenge, we propose training task-specific projections based on the frozen image/text encoders. When facing new tasks, new projections are expanded and former projections are fixed, alleviating the forgetting of old concepts. For the second challenge, we propose the fusion module to better utilize the cross-modality information. By jointly adjusting visual and textual features, the model can capture semantic information with stronger representation ability. Extensive experiments on nine benchmark datasets validate PROOF achieves state-of-the-art performance.",
        "authors": [
            "Da-Wei Zhou",
            "Yuanhan Zhang",
            "Jingyi Ning",
            "Han-Jia Ye",
            "De-chuan Zhan",
            "Ziwei Liu"
        ],
        "citations": 29,
        "references": 83,
        "year": 2023
    },
    {
        "title": "Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?",
        "abstract": "Rapid advancements in Autonomous Driving (AD) tasks turned a significant shift toward end-to-end fashion, particularly in the utilization of vision-language models (VLMs) that integrate robust logical reasoning and cognitive abilities to enable comprehensive end-to-end planning. However, these VLM-based approaches tend to integrate 2D vision tokenizers and a large language model (LLM) for ego-car planning, which lack 3D geometric priors as a cornerstone of reliable planning. Naturally, this observation raises a critical concern: Can a 2D-tokenized LLM accurately perceive the 3D environment? Our evaluation of current VLM-based methods across 3D object detection, vectorized map construction, and environmental caption suggests that the answer is, unfortunately, NO. In other words, 2D-tokenized LLM fails to provide reliable autonomous driving. In response, we introduce DETR-style 3D perceptrons as 3D tokenizers, which connect LLM with a one-layer linear projector. This simple yet elegant strategy, termed Atlas, harnesses the inherent priors of the 3D physical world, enabling it to simultaneously process high-resolution multi-view images and employ spatiotemporal modeling. Despite its simplicity, Atlas demonstrates superior performance in both 3D detection and ego planning tasks on nuScenes dataset, proving that 3D-tokenized LLM is the key to reliable autonomous driving. The code and datasets will be released.",
        "authors": [
            "Yifan Bai",
            "Dongming Wu",
            "Yingfei Liu",
            "Fan Jia",
            "Weixin Mao",
            "Ziheng Zhang",
            "Yucheng Zhao",
            "Jianbing Shen",
            "Xing Wei",
            "Tiancai Wang",
            "Xiangyu Zhang"
        ],
        "citations": 7,
        "references": 48,
        "year": 2024
    },
    {
        "title": "CarLLaVA: Vision language models for camera-only closed-loop driving",
        "abstract": "In this technical report, we present CarLLaVA, a Vision Language Model (VLM) for autonomous driving, developed for the CARLA Autonomous Driving Challenge 2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA architecture as backbone, achieving state-of-the-art closed-loop driving performance with only camera input and without the need for complex or expensive labels. Additionally, we show preliminary results on predicting language commentary alongside the driving output. CarLLaVA uses a semi-disentangled output representation of both path predictions and waypoints, getting the advantages of the path for better lateral control and the waypoints for better longitudinal control. We propose an efficient training recipe to train on large driving datasets without wasting compute on easy, trivial data. CarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving Challenge 2.0 outperforming the previous state of the art by 458% and the best concurrent submission by 32.6%.",
        "authors": [
            "Katrin Renz",
            "Long Chen",
            "Ana-Maria Marcu",
            "Jan H√ºnermann",
            "Beno√Æt Hanotte",
            "Alice Karnsund",
            "Jamie Shotton",
            "Elahe Arani",
            "Oleg Sinavski"
        ],
        "citations": 6,
        "references": 33,
        "year": 2024
    },
    {
        "title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)",
        "abstract": "With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their prompt faithfulness -- the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines. We introduce T2IScoreScore, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.",
        "authors": [
            "Michael Stephen Saxon",
            "Fatima Jahara",
            "Mahsa Khoshnoodi",
            "Yujie Lu",
            "Aditya Sharma",
            "William Yang Wang"
        ],
        "citations": 6,
        "references": 58,
        "year": 2024
    },
    {
        "title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling",
        "abstract": "Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU.",
        "authors": [
            "Haider Al-Tahan",
            "Q. Garrido",
            "Randall Balestriero",
            "Diane Bouchacourt",
            "C. Hazirbas",
            "Mark Ibrahim"
        ],
        "citations": 6,
        "references": 58,
        "year": 2024
    },
    {
        "title": "BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis",
        "abstract": "This paper presents BattleAgent, a detailed emulation demonstration system that combines the Large Vision-Language Model (VLM) and Multi-Agent System (MAS). This novel system aims to emulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner. This methodology holds the potential to substantially improve visualization of historical events and deepen our understanding of historical events especially from the perspective of decision making. The data and code for this project are accessible at https://github.com/agiresearch/battleagent and the demo is accessible at https://drive.google.com/file/d/1I5B3KWiYCSSP1uMiPGNmXlTmild-MzRJ/view?usp=sharing.",
        "authors": [
            "Shuhang Lin",
            "Wenyue Hua",
            "Lingyao Li",
            "Che-Jui Chang",
            "Lizhou Fan",
            "Jianchao Ji",
            "Hang Hua",
            "Mingyu Jin",
            "Jiebo Luo",
            "Yongfeng Zhang"
        ],
        "citations": 6,
        "references": 83,
        "year": 2024
    },
    {
        "title": "Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models",
        "abstract": "Legged robots are physically capable of navigating a diverse variety of environments and overcoming a wide range of obstructions. For example, in a search and rescue mission, a legged robot could climb over debris, crawl through gaps, and navigate out of dead ends. However, the robot's controller needs to respond intelligently to such varied obstacles, and this requires handling unexpected and unusual scenarios successfully. This presents an open challenge to current learning methods, which often struggle with generalization to the long tail of unexpected situations without heavy human supervision. To address this issue, we investigate how to leverage the broad knowledge about the structure of the world and commonsense reasoning capabilities of vision-language models (VLMs) to aid legged robots in handling difficult, ambiguous situations. We propose a system, VLM-Predictive Control (VLM-PC), combining two key components that we find to be crucial for eliciting on-the-fly, adaptive behavior selection with VLMs: (1) in-context adaptation over previous robot interactions and (2) planning multiple skills into the future and replanning. We evaluate VLM-PC on several challenging real-world obstacle courses, involving dead ends and climbing and crawling, on a Go1 quadruped robot. Our experiments show that by reasoning over the history of interactions and future plans, VLMs enable the robot to autonomously perceive, navigate, and act in a wide range of complex scenarios that would otherwise require environment-specific engineering or human guidance.",
        "authors": [
            "Annie S. Chen",
            "Alec M. Lessing",
            "Andy Tang",
            "Govind Chada",
            "Laura M. Smith",
            "Sergey Levine",
            "Chelsea Finn"
        ],
        "citations": 6,
        "references": 79,
        "year": 2024
    },
    {
        "title": "LOC-ZSON: Language-driven Object-Centric Zero-Shot Object Retrieval and Navigation",
        "abstract": "In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes. We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries. In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference. We implement our method on Astro robot and deploy it in both simulated and real-world environments for zero-shot object navigation. We show that our proposed method can achieve an improvement of 1.38 - 13.38% in terms of text-to-image recall on different benchmark settings for the retrieval task. For object navigation, we show the benefit of our approach in simulation and real world, showing 5% and 16.67% improvement in terms of navigation success rate, respectively.",
        "authors": [
            "Tianrui Guan",
            "Yurou Yang",
            "Harry Cheng",
            "Muyuan Lin",
            "Richard Kim",
            "R. Madhivanan",
            "Arnie Sen",
            "Dinesh Manocha"
        ],
        "citations": 6,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Optical-OFDM VLC System: Peak-to-Average Power Ratio Enhancement and Performance Evaluation",
        "abstract": "Visible Light Communication (VLC) systems are favoured for numerous applications due to their extensive bandwidth and resilience to electromagnetic interference. This study delineates various constructions of Optical Orthogonal Frequency Division Multiplexing (O-OFDM) approaches employed in VLC systems. Various factors are elaborated within this context to ascertain a more effective O-OFDM approach, including constellation size, data arrangement and spectral efficiency, power efficiency, computational complexity, bit error rate (BER), and peak-to-average power ratio (PAPR). This paper seeks to assess these approaches‚Äô BER and PAPR performance across varying modulation orders. Regrettably, in VLC systems based on OFDM methodology, the superposition of multiple subcarriers results in a high PAPR. Therefore, this study aims to diminish the PAPR in VLC systems, enhancing system performance. We propose a non-distorting PAPR reduction technique, namely the Vandermonde-Like Matrix (VLM) precoding technique. The suggested technique is implemented across various O-OFDM approaches, including DCO-OFDM, ADO-OFDM, ACO-OFDM, FLIP-OFDM, ASCO-OFDM, and LACO-OFDM. Notably, this method does not affect the system‚Äôs data rate because it does not require the mandatory transmission of side information. Furthermore, this technique can decrease the PAPR without impacting the system‚Äôs BER performance. This study compares the proposed PAPR reduction technique against established methods documented in the literature to evaluate their efficacy and validity rigorously.",
        "authors": [
            "Yasser A. Zenhom",
            "Ehab K. I. Hamad",
            "Mohammed A. Alghassab",
            "Mohamed M. Elnabawy"
        ],
        "citations": 6,
        "references": 53,
        "year": 2024
    },
    {
        "title": "R+X: Retrieval and Execution from Everyday Human Videos",
        "abstract": "We present R+X, a framework which enables robots to learn skills from long, unlabelled, first-person videos of humans performing everyday tasks. Given a language command from a human, R+X first retrieves short video clips containing relevant behaviour, and then executes the skill by conditioning an in-context imitation learning method on this behaviour. By leveraging a Vision Language Model (VLM) for retrieval, R+X does not require any manual annotation of the videos, and by leveraging in-context learning for execution, robots can perform commanded skills immediately, without requiring a period of training on the retrieved videos. Experiments studying a range of everyday household tasks show that R+X succeeds at translating unlabelled human videos into robust robot skills, and that R+X outperforms several recent alternative methods. Videos are available at https://www.robot-learning.uk/r-plus-x.",
        "authors": [
            "Georgios Papagiannis",
            "Norman Di Palo",
            "Pietro Vitiello",
            "Edward Johns"
        ],
        "citations": 7,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Harnessing Large Language Models for Training-Free Video Anomaly Detection",
        "abstract": "Video anomaly detection (VAD) aims to temporally locate abnormal events in a video. Existing works mostly rely on training deep models to learn the distribution of normality with either video-level supervision, one-class supervision, or in an unsupervised setting. Training-based methods are prone to be domain-specific, thus being costly for practical deployment as any domain change will involve data collection and model training. In this paper, we radically depart from previous efforts and propose LAnguage-based VAD (LAVAD), a method tackling VAD in a novel, training-free paradigm, exploiting the capabilities of pre-trained large language models (LLMs) and existing vision-language models (VLMs). We leverage VLM-based captioning models to generate textual descriptions for each frame of any test video. With the textual scene description, we then devise a prompting mechanism to unlock the capability of LLMs in terms of temporal aggregation and anomaly score estimation, turning LLMs into an effective video anomaly detector. We further leverage modality-aligned VLMs and propose effective techniques based on cross-modal similarity for cleaning noisy captions and refining the LLM-based anomaly scores. We evaluate LAVAD on two large datasets featuring real-world surveillance scenarios (UCF-Crime and XD- Violence), showing that it outperforms both unsupervised and one-class methods without requiring any training or data collection.",
        "authors": [
            "Luca Zanella",
            "Willi Menapace",
            "Massimiliano Mancini",
            "Yiming Wang",
            "Elisa Ricci"
        ],
        "citations": 6,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Failures to Find Transferable Image Jailbreaks Between Vision-Language Models",
        "abstract": "The integration of new modalities into frontier AI systems offers exciting capabilities, but also increases the possibility such systems can be adversarially manipulated in undesirable ways. In this work, we focus on a popular class of vision-language models (VLMs) that generate text outputs conditioned on visual and textual inputs. We conducted a large-scale empirical study to assess the transferability of gradient-based universal image ``jailbreaks\"using a diverse set of over 40 open-parameter VLMs, including 18 new VLMs that we publicly release. Overall, we find that transferable gradient-based image jailbreaks are extremely difficult to obtain. When an image jailbreak is optimized against a single VLM or against an ensemble of VLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits little-to-no transfer to any other VLMs; transfer is not affected by whether the attacked and target VLMs possess matching vision backbones or language models, whether the language model underwent instruction-following and/or safety-alignment training, or many other factors. Only two settings display partially successful transfer: between identically-pretrained and identically-initialized VLMs with slightly different VLM training data, and between different training checkpoints of a single VLM. Leveraging these results, we then demonstrate that transfer can be significantly improved against a specific target VLM by attacking larger ensembles of ``highly-similar\"VLMs. These results stand in stark contrast to existing evidence of universal and transferable text jailbreaks against language models and transferable adversarial attacks against image classifiers, suggesting that VLMs may be more robust to gradient-based transfer attacks.",
        "authors": [
            "Rylan Schaeffer",
            "Dan Valentine",
            "Luke Bailey",
            "James Chua",
            "Cristobal Eyzaguirre",
            "Zane Durante",
            "Joe Benton",
            "Brando Miranda",
            "Henry Sleight",
            "John Hughes",
            "Rajashree Agrawal",
            "Mrinank Sharma",
            "Scott Emmons",
            "Sanmi Koyejo",
            "Ethan Perez"
        ],
        "citations": 6,
        "references": 0,
        "year": 2024
    },
    {
        "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
        "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-CoT-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-CoT not only outperforms its base model by 7.4% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.",
        "authors": [
            "Guowei Xu",
            "Peng Jin",
            "Hao Li",
            "Yibing Song",
            "Lichao Sun",
            "Li Yuan"
        ],
        "citations": 7,
        "references": 0,
        "year": 2024
    },
    {
        "title": "From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models",
        "abstract": "Scene graph generation (SGG) aims to parse a visual scene into an intermediate graph representation for downstream reasoning tasks. Despite recent advancements, existing methods struggle to generate scene graphs with novel visual relation concepts. To address this challenge, we introduce a new open-vocabulary SGG framework based on sequence generation. Our framework leverages vision-language pre-trained models (VLM) by incorporating an image-to-graph generation paradigm. Specifically, we generate scene graph sequences via image-to-text generation with VLM and then construct scene graphs from these sequences. By doing so, we harness the strong capabilities of VLM for open-vocabulary SGG and seamlessly integrate explicit relational modeling for enhancing the VL tasks. Experimental results demonstrate that our design not only achieves superior performance with an open vocabulary but also enhances downstream vision-language task performance through explicit relation modeling knowledge.",
        "authors": [
            "Rongjie Li",
            "Songyang Zhang",
            "Dahua Lin",
            "Kai Chen",
            "Xuming He"
        ],
        "citations": 6,
        "references": 56,
        "year": 2024
    },
    {
        "title": "Falcon2-11B Technical Report",
        "abstract": "We introduce Falcon2-11B, a foundation model trained on over five trillion tokens, and its multimodal counterpart, Falcon2-11B-vlm, which is a vision-to-text model. We report our findings during the training of the Falcon2-11B which follows a multi-stage approach where the early stages are distinguished by their context length and a final stage where we use a curated, high-quality dataset. Additionally, we report the effect of doubling the batch size mid-training and how training loss spikes are affected by the learning rate. The downstream performance of the foundation model is evaluated on established benchmarks, including multilingual and code datasets. The foundation model shows strong generalization across all the tasks which makes it suitable for downstream finetuning use cases. For the vision language model, we report the performance on several benchmarks and show that our model achieves a higher average score compared to open-source models of similar size. The model weights and code of both Falcon2-11B and Falcon2-11B-vlm are made available under a permissive license.",
        "authors": [
            "Quentin Malartic",
            "Nilabhra Roy Chowdhury",
            "Ruxandra-Aim√©e Cojocaru",
            "Mugariya Farooq",
            "Giulia Campesan",
            "Y. A. D. Djilali",
            "Sanath Narayan",
            "Ankit Singh",
            "Maksim Velikanov",
            "Basma El Amel Boussaha",
            "Mohammed Al-Yafeai",
            "Hamza Alobeidli",
            "Leen Al Qadi",
            "M. Seddik",
            "Kirill Fedyanin",
            "R√©da Alami",
            "Hakim Hacid"
        ],
        "citations": 6,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
        "abstract": "Vision Language Models (VLMs) demonstrate remarkable proficiency in addressing a wide array of visual questions, which requires strong perception and reasoning faculties. Assessing these two competencies independently is crucial for model refinement, despite the inherent difficulty due to the intertwined nature of seeing and reasoning in existing VLMs. To tackle this issue, we present Prism, an innovative framework designed to disentangle the perception and reasoning processes involved in visual question solving. Prism comprises two distinct stages: a perception stage that utilizes a VLM to extract and articulate visual information in textual form, and a reasoning stage that formulates responses based on the extracted visual information using a Large Language Model (LLM). This modular design enables the systematic comparison and assessment of both proprietary and open-source VLM for their perception and reasoning strengths. Our analytical framework provides several valuable insights, underscoring Prism's potential as a cost-effective solution for vision-language tasks. By combining a streamlined VLM focused on perception with a powerful LLM tailored for reasoning, Prism achieves superior results in general vision-language tasks while substantially cutting down on training and operational expenses. Quantitative evaluations show that Prism, when configured with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on par with VLMs $10 \\times$ larger on the rigorous multimodal benchmark MMStar. The project is released at: https://github.com/SparksJoe/Prism.",
        "authors": [
            "Yu Qiao",
            "Haodong Duan",
            "Xinyu Fang",
            "Junming Yang",
            "Lin Chen",
            "Songyang Zhang",
            "Jiaqi Wang",
            "Dahua Lin",
            "Kai Chen"
        ],
        "citations": 7,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?",
        "abstract": "Data science and engineering workflows often span multiple stages, from warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code generation, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably automate full data workflows (14.0% success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted workspaces (10.6%). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.",
        "authors": [
            "Ruisheng Cao",
            "Fangyu Lei",
            "Haoyuan Wu",
            "Jixuan Chen",
            "Yeqiao Fu",
            "Hongcheng Gao",
            "Xinzhuang Xiong",
            "Hanchong Zhang",
            "Yuchen Mao",
            "Wenjing Hu",
            "Tianbao Xie",
            "Hongshen Xu",
            "Danyang Zhang",
            "Sida Wang",
            "Ruoxi Sun",
            "Pengcheng Yin",
            "Caiming Xiong",
            "Ansong Ni",
            "Qian Liu",
            "Victor Zhong",
            "Lu Chen",
            "Kai Yu",
            "Tao Yu"
        ],
        "citations": 6,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Descriptive Image Quality Assessment in the Wild",
        "abstract": "With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Depicted image Quality Assessment in the Wild (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released in https://depictqa.github.io/depictqa-wild/.",
        "authors": [
            "Zhiyuan You",
            "Jinjin Gu",
            "Zheyuan Li",
            "Xin Cai",
            "Kaiwen Zhu",
            "Tianfan Xue",
            "Chao Dong"
        ],
        "citations": 6,
        "references": 92,
        "year": 2024
    },
    {
        "title": "Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation",
        "abstract": "Learned language-conditioned robot policies often struggle to effectively adapt to new real-world tasks even when pre-trained across a diverse set of instructions. We propose a novel approach for few-shot adaptation to unseen tasks that exploits the semantic understanding of task decomposition provided by vision-language models (VLMs). Our method, Policy Adaptation via Language Optimization (PALO), combines a handful of demonstrations of a task with proposed language decompositions sampled from a VLM to quickly enable rapid nonparametric adaptation, avoiding the need for a larger fine-tuning dataset. We evaluate PALO on extensive real-world experiments consisting of challenging unseen, long-horizon robot manipulation tasks. We find that PALO is able of consistently complete long-horizon, multi-tier tasks in the real world, outperforming state of the art pre-trained generalist policies, and methods that have access to the same demonstrations.",
        "authors": [
            "Vivek Myers",
            "Bill Chunyuan Zheng",
            "Oier Mees",
            "Sergey Levine",
            "Kuan Fang"
        ],
        "citations": 7,
        "references": 95,
        "year": 2024
    },
    {
        "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts",
        "abstract": "Large Vision Language Models (VLMs) extend and enhance the perceptual abilities of Large Language Models (LLMs). Despite offering new possibilities for LLM applications, these advancements raise significant security and ethical concerns, particularly regarding the generation of harmful content. While LLMs have undergone extensive security evaluations with the aid of red teaming frameworks, VLMs currently lack a well-developed one. To fill this gap, we introduce Arondight, a standardized red team framework tailored specifically for VLMs. Arondight is dedicated to resolving issues related to the absence of visual modality and inadequate diversity encountered when transitioning existing red teaming methodologies from LLMs to VLMs. Our framework features an automated multi-modal jailbreak attack, wherein visual jailbreak prompts are produced by a red team VLM, and textual prompts are generated by a red team LLM guided by a reinforcement learning agent. To enhance the comprehensiveness of VLM security evaluation, we integrate entropy bonuses and novelty reward metrics. These elements incentivize the RL agent to guide the red team LLM in creating a wider array of diverse and previously unseen test cases. Our evaluation of ten cutting-edge VLMs exposes significant security vulnerabilities, particularly in generating toxic images and aligning multi-modal prompts. In particular, our Arondight achieves an average attack success rate of 84.5\\% on GPT-4 in all fourteen prohibited scenarios defined by OpenAI in terms of generating toxic text. For a clearer comparison, we also categorize existing VLMs based on their safety levels and provide corresponding reinforcement recommendations. Our multimodal prompt dataset and red team code will be released after ethics committee approval. CONTENT WARNING: THIS PAPER CONTAINS HARMFUL MODEL RESPONSES.",
        "authors": [
            "Yi Liu",
            "Chengjun Cai",
            "Xiaoli Zhang",
            "Xingliang Yuan",
            "Cong Wang"
        ],
        "citations": 6,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment",
        "abstract": "We introduce a method to train vision-language models for remote-sensing images without using any textual annotations. Our key insight is to use co-located internet imagery taken on the ground as an intermediary for connecting remote-sensing images and language. Specifically, we train an image encoder for remote sensing images to align with the image encoder of CLIP using a large amount of paired internet and satellite images. Our unsupervised approach enables the training of a first-of-its-kind large-scale vision language model (VLM) for remote sensing images at two different resolutions. We show that these VLMs enable zero-shot, open-vocabulary image classification, retrieval, segmentation and visual question answering for satellite images. On each of these tasks, our VLM trained without textual annotations outperforms existing VLMs trained with supervision, with gains of up to 20% for classification and 80% for segmentation.",
        "authors": [
            "Utkarsh Mall",
            "Cheng Perng Phoo",
            "Meilin Kelsey Liu",
            "Carl Vondrick",
            "B. Hariharan",
            "Kavita Bala"
        ],
        "citations": 25,
        "references": 53,
        "year": 2023
    },
    {
        "title": "WonderJourney: Going from Anywhere to Everywhere",
        "abstract": "We introduce WonderJourney, a modular framework for perpetual 3D scene generation. Unlike prior work on view generation that focuses on a single type of scenes, we start at any user-provided location (by a text description or an image), and generate a journey through a long sequence of diverse yet coherently connected 3D scenes. We leverage an LLM to generate textual descriptions of the scenes in this journey, a text-driven point cloud generation pipeline to make a compelling and coherent sequence of 3D scenes, and a large VLM to verify the generated scenes. We show compelling, diverse visual results across various scene types and styles, forming imaginary ‚Äúwonder journeys ‚Äù. Project website: https://kovenyu.com/WonderJourney/. ‚ÄúNo, no! The adventures first, explanations take such a dreadful time.‚Äú - Alice's Adventures in Wonderland",
        "authors": [
            "Hong-Xing Yu",
            "Haoyi Duan",
            "Junhwa Hur",
            "Kyle Sargent",
            "Michael Rubinstein",
            "William T. Freeman",
            "Forrester Cole",
            "Deqing Sun",
            "Noah Snavely",
            "Jiajun Wu",
            "Charles Herrmann"
        ],
        "citations": 26,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Empowering Vision-Language Models for Reasoning Ability through Large Language Models",
        "abstract": "Vision-language models (VLM) have shown excellent performance in vision-language tasks. However, they sometimes lack sufficient reasoning ability. In contrast, large language models (LLMs) have emerged with powerful reasoning capabilities. Therefore, we propose a framework called TReE, which transfers the reasoning ability of the LLM to the VLM in learning-free settings. TReE is a three-stage framework: observation, thinking, and re-thinking. The observation stage requires the VLM to obtain overall visual information about the image. Then, the thinking stage combines the visual information and task description as the prompt for the LLM, allowing it to present the thinking process (namely, rationale). Lastly, the re-thinking stage learns useful information from the rationale and then predicts the final result using the VLM. We are the first to explore enhancing the VLM‚Äôs reasoning ability without any training, finetuning, or access to the LLM‚Äôs parameters, which we refer to as a plug-in mode, leading to the model-agnostic feature. Experiments show that TReE performed well on general visual questionanswering (VQA) tasks and outperformed KOSMOS-1 on the challenging Raven IQ test dataset by 6%. Furthermore, with additional lightweight finetuning using a smaller amount of parameters, TReE achieved a high accuracy of 81.7% on GQA and 67.3% on VQAv2.",
        "authors": [
            "Yueting Yang",
            "Xintong Zhang",
            "Jinan Xu",
            "Wenjuan Han"
        ],
        "citations": 5,
        "references": 19,
        "year": 2024
    },
    {
        "title": "Self-Supervised Visual Preference Alignment",
        "abstract": "This paper makes the first attempt towards unsupervised preference alignment in Vision-Language Models (VLMs). We generate chosen and rejected responses with regard to the original and augmented image pairs, and conduct preference alignment with direct preference optimization. It is based on a core idea: properly designed augmentation to the image input will induce VLM to generate false but hard negative responses, which helps the model to learn from and produce more robust and powerful answers. The whole pipeline no longer hinges on supervision from GPT-4 or human involvement during alignment, and is highly efficient with few lines of code. With only 8k randomly sampled unsupervised data, it achieves 90\\% relative score to GPT-4 on complex reasoning in LLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex multi-modal benchmark MM-Vet. Visualizations shows its improved ability to align with user-intentions. A series of ablations are firmly conducted to reveal the latent mechanism of the approach, which also indicates its potential towards further scaling. Code are available in https://github.com/Kevinz-code/SeVa.",
        "authors": [
            "Ke Zhu",
            "Liang Zhao",
            "Zheng Ge",
            "Xiangyu Zhang"
        ],
        "citations": 5,
        "references": 49,
        "year": 2024
    },
    {
        "title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models",
        "abstract": "This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of more practical and reliable VLMs.",
        "authors": [
            "Atsuyuki Miyai",
            "Jingkang Yang",
            "Jingyang Zhang",
            "Yifei Ming",
            "Qing Yu",
            "Go Irie",
            "Yixuan Li",
            "Hai Li",
            "Ziwei Liu",
            "Kiyoharu Aizawa"
        ],
        "citations": 5,
        "references": 107,
        "year": 2024
    },
    {
        "title": "SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model",
        "abstract": "In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved.",
        "authors": [
            "Bin Cao",
            "Jianhao Yuan",
            "Yexin Liu",
            "Jian Li",
            "Shuyang Sun",
            "Jing Liu",
            "Bo Zhao"
        ],
        "citations": 5,
        "references": 40,
        "year": 2024
    },
    {
        "title": "OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding",
        "abstract": "In order for robots to interact with objects effectively, they must understand the form and function of each object they encounter. Essentially, robots need to understand which actions each object affords, and where those affordances can be acted on. Robots are ultimately expected to operate in unstructured human environments, where the set of objects and affordances is not known to the robot before deployment (i.e. the open-vocabulary setting). In this work, we introduce OVAL-Prompt, a prompt-based approach for open-vocabulary affordance localization in RGB-D images. By leveraging a Vision Language Model (VLM) for open-vocabulary object part segmentation and a Large Language Model (LLM) to ground each part-segment-affordance, OVAL-Prompt demonstrates generalizability to novel object instances, categories, and affordances without domain-specific finetuning. Quantitative experiments demonstrate that without any finetuning, OVAL-Prompt achieves localization accuracy that is competitive with supervised baseline models. Moreover, qualitative experiments show that OVAL-Prompt enables affordance-based robot manipulation of open-vocabulary object instances and categories. Project Page: https://ekjt.github.io/OVAL-Prompt/",
        "authors": [
            "Edmond Tong",
            "A. Opipari",
            "Stanley Lewis",
            "Zhen Zeng",
            "O. C. Jenkins"
        ],
        "citations": 5,
        "references": 40,
        "year": 2024
    },
    {
        "title": "CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps",
        "abstract": "This paper describes a multi-modal data association method for global localization using object-based maps and camera images. In global localization, or relocalization, using object-based maps, existing methods typically resort to matching all possible combinations of detected objects and landmarks with the same object category, followed by inlier extraction using RANSAC or brute-force search. This approach becomes infeasible as the number of landmarks increases due to the exponential growth of correspondence candidates. In this paper, we propose labeling landmarks with natural language descriptions and extracting correspondences based on conceptual similarity with image observations using a Vision Language Model (VLM). By leveraging detailed text information, our approach efficiently extracts correspondences compared to methods using only object categories. Through experiments, we demonstrate that the proposed method enables more accurate global localization with fewer iterations compared to baseline methods, exhibiting its efficiency.",
        "authors": [
            "Shigemichi Matsuzaki",
            "Takuma Sugino",
            "Kazuhito Tanaka",
            "Zijun Sha",
            "Shintaro Nakaoka",
            "Shintaro Yoshizawa",
            "Kazuhiro Shintani"
        ],
        "citations": 5,
        "references": 26,
        "year": 2024
    },
    {
        "title": "Agent3D-Zero: An Agent for Zero-shot 3D Understanding",
        "abstract": "The ability to understand and reason the 3D real world is a crucial milestone towards artificial general intelligence. The current common practice is to finetune Large Language Models (LLMs) with 3D data and texts to enable 3D understanding. Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data. Alternatively, in this work, we introduce Agent3D-Zero, an innovative 3D-aware agent framework addressing the 3D scene understanding in a zero-shot manner. The essence of our approach centers on reconceptualizing the challenge of 3D scene perception as a process of understanding and synthesizing insights from multiple images, inspired by how our human beings attempt to understand 3D scenes. By consolidating this idea, we propose a novel way to make use of a Large Visual Language Model (VLM) via actively selecting and analyzing a series of viewpoints for 3D understanding. Specifically, given an input 3D scene, Agent3D-Zero first processes a bird's-eye view image with custom-designed visual prompts, then iteratively chooses the next viewpoints to observe and summarize the underlying knowledge. A distinctive advantage of Agent3D-Zero is the introduction of novel visual prompts, which significantly unleash the VLMs' ability to identify the most informative viewpoints and thus facilitate observing 3D scenes. Extensive experiments demonstrate the effectiveness of the proposed framework in understanding diverse and previously unseen 3D environments.",
        "authors": [
            "Sha Zhang",
            "Di Huang",
            "Jiajun Deng",
            "Shixiang Tang",
            "Wanli Ouyang",
            "Tong He",
            "Yanyong Zhang"
        ],
        "citations": 5,
        "references": 47,
        "year": 2024
    },
    {
        "title": "ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation",
        "abstract": "Navigating and understanding complex environments over extended periods of time is a significant challenge for robots. People interacting with the robot may want to ask questions like where something happened, when it occurred, or how long ago it took place, which would require the robot to reason over a long history of their deployment. To address this problem, we introduce a Retrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed for long-horizon video question answering for robot navigation. To evaluate ReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal, and descriptive questions to long-horizon robot navigation videos. ReMEmbR employs a structured approach involving a memory building and a querying phase, leveraging temporal information, spatial information, and images to efficiently handle continuously growing robot histories. Our experiments demonstrate that ReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve effective long-horizon reasoning with low latency. Additionally, we deploy ReMEmbR on a robot and show that our approach can handle diverse queries. The dataset, code, videos, and other material can be found at the following link: https://nvidia-ai-iot.github.io/remembr",
        "authors": [
            "Abrar Anwar",
            "John Welsh",
            "Joydeep Biswas",
            "Soha Pouya",
            "Yan Chang"
        ],
        "citations": 5,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Bi-LORA: A Vision-Language Approach for Synthetic Image Detection",
        "abstract": "Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT.",
        "authors": [
            "Mamadou Keita",
            "W. Hamidouche",
            "Hessen Bougueffa Eutamene",
            "Abdenour Hadid",
            "Abdelmalik Taleb-Ahmed"
        ],
        "citations": 5,
        "references": 80,
        "year": 2024
    },
    {
        "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning",
        "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem of reward misalignment when applying VLM as a reward in RL tasks. To address this issue, we introduce a lightweight fine-tuning method, named Fuzzy VLM reward-aided RL (FuRL), based on reward alignment and relay RL. Specifically, we enhance the performance of SAC/DrQ baseline agents on sparse reward tasks by fine-tuning VLM representations and using relay RL to avoid local minima. Extensive experiments on the Meta-world benchmark tasks demonstrate the efficacy of the proposed method. Code is available at: https://github.com/fuyw/FuRL.",
        "authors": [
            "Yuwei Fu",
            "Haichao Zhang",
            "Di Wu",
            "Wei Xu",
            "Benoit Boulet"
        ],
        "citations": 5,
        "references": 70,
        "year": 2024
    },
    {
        "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
        "abstract": "Recently, Vision-Language Models (VLMs) have achieved remarkable progress in multimodal tasks, and multimodal instruction data serves as the foundation for enhancing VLM capabilities. Despite the availability of several open-source multimodal datasets, limitations in the scale and quality of open-source instruction data hinder the performance of VLMs trained on these datasets, leading to a significant gap compared to models trained on closed-source data. To address this challenge, we introduce Infinity-MM, a large-scale multimodal instruction dataset. We collected the available multimodal instruction datasets and performed unified preprocessing, resulting in a dataset with over 40 million samples that ensures diversity and accuracy. Furthermore, to enable large-scale expansion of instruction data and support the continuous acquisition of high-quality data, we propose a synthetic instruction generation method based on a tagging system and open-source VLMs. By establishing correspondences between different types of images and associated instruction types, this method can provide essential guidance during data synthesis. Leveraging this high-quality data, we have trained a 2-billion-parameter Vision-Language Model, Aquila-VL-2B, which achieves state-of-the-art (SOTA) performance among models of similar scale. The data is available at: https://huggingface.co/datasets/BAAI/Infinity-MM.",
        "authors": [
            "Shuhao Gu",
            "Jialing Zhang",
            "Siyuan Zhou",
            "Kevin Yu",
            "Zhaohu Xing",
            "Liangdong Wang",
            "Zhou Cao",
            "Jintao Jia",
            "Zhuoyi Zhang",
            "Yixuan Wang",
            "Zhenchong Hu",
            "Bo-Wen Zhang",
            "Jijie Li",
            "Dong Liang",
            "Yingli Zhao",
            "Yulong Ao",
            "Yaoqi Liu",
            "Fangxiang Feng",
            "Guang Liu"
        ],
        "citations": 5,
        "references": 51,
        "year": 2024
    },
    {
        "title": "IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models",
        "abstract": "The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially degrade the performance of Gemini-Pro in the localization task. Tangentially, we discover a potential weakness in the ICL capabilities of VLMs: they fail to locate optical illusions even when the correct answer is in the context window as a few-shot example.",
        "authors": [
            "H. S. Shahgir",
            "Khondker Salman Sayeed",
            "Abhik Bhattacharjee",
            "Wasi Uddin Ahmad",
            "Yue Dong",
            "Rifat Shahriyar"
        ],
        "citations": 5,
        "references": 65,
        "year": 2024
    },
    {
        "title": "Prompt learning in computer vision: a survey",
        "abstract": "Prompt learning has attracted broad attention in computer vision since the large pre-trained vision-language models (VLMs) exploded. Based on the close relationship between vision and language information built by VLM, prompt learning becomes a crucial technique in many important applications such as artificial intelligence generated content (AIGC). In this survey, we provide a progressive and comprehensive review of visual prompt learning as related to AIGC. We begin by introducing VLM, the foundation of visual prompt learning. Then, we review the vision prompt learning methods and prompt-guided generative models, and discuss how to improve the efficiency of adapting AIGC models to specific downstream tasks. Finally, we provide some promising research directions concerning prompt learning. Ëá™Â§ßÂûãÈ¢ÑËÆ≠ÁªÉËßÜËßâ‚ÄîËØ≠Ë®ÄÊ®°Âûã(VLM)ÁàÜÂèë‰ª•Êù•,ÊèêÁ§∫Â≠¶‰π†Â∑≤Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÂºïÂèëÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÂü∫‰∫éVLMÊûÑÂª∫ÁöÑËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØ‰πãÈó¥ÁöÑÂØÜÂàáÂÖ≥Á≥ª,ÊèêÁ§∫Â≠¶‰π†Êàê‰∏∫ËÆ∏Â§öÈáçË¶ÅÂ∫îÁî®È¢ÜÂüü(Â¶Ç‰∫∫Â∑•Êô∫ËÉΩÂÜÖÂÆπÁîüÊàê(AIGC))‰∏≠ÁöÑÂÖ≥ÈîÆÊäÄÊúØ„ÄÇÊú¨ÁªºËø∞Âæ™Â∫èÊ∏êËøõ‰∏îÂÖ®Èù¢Âú∞ÊÄªÁªì‰∫Ü‰∏éAIGCÁõ∏ÂÖ≥ÁöÑËßÜËßâÊèêÁ§∫Â≠¶‰π†„ÄÇÈ¶ñÂÖà‰ªãÁªç‰∫ÜVLM,ÂÆÉÊòØËßÜËßâÊèêÁ§∫Â≠¶‰π†ÁöÑÂü∫Á°Ä„ÄÇÁÑ∂Âêé,ÂõûÈ°æ‰∫ÜËßÜËßâÊèêÁ§∫Â≠¶‰π†ÊñπÊ≥ïÂíåÊèêÁ§∫ÂºïÂØºÁîüÊàêÊ®°Âûã,Âπ∂ËÆ®ËÆ∫‰∫ÜÂ¶Ç‰ΩïÊèêÈ´òÂ∞ÜAIGCÊ®°ÂûãÈÄÇÁî®‰∫é‰∏ãÊ∏∏ÁâπÂÆö‰ªªÂä°ÁöÑÊïàÁéá„ÄÇÊúÄÂêé,Êèê‰æõ‰∫Ü‰∏Ä‰∫õÊúâÂâçÊôØÁöÑÂÖ≥‰∫éÊèêÁ§∫Â≠¶‰π†ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇ",
        "authors": [
            "Yiming Lei",
            "Jingqi Li",
            "Zilong Li",
            "Yuan Cao",
            "Hongming Shan"
        ],
        "citations": 5,
        "references": 13,
        "year": 2024
    },
    {
        "title": "Language Reward Modulation for Pretraining Reinforcement Learning",
        "abstract": "Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\\textbf{LA}$nguage Reward $\\textbf{M}$odulated $\\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy. Our VLM pretraining approach, which is a departure from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench.",
        "authors": [
            "Ademi Adeniji",
            "Amber Xie",
            "Carmelo Sferrazza",
            "Younggyo Seo",
            "Stephen James",
            "P. Abbeel"
        ],
        "citations": 22,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese",
        "abstract": "Vision Language Models (VLMs) have undergone a rapid evolution, giving rise to significant advancements in the realm of multimodal understanding tasks. However, the majority of these models are trained and evaluated on English-centric datasets, leaving a gap in the development and evaluation of VLMs for other languages, such as Japanese. This gap can be attributed to the lack of methodologies for constructing VLMs and the absence of benchmarks to accurately measure their performance. To address this issue, we introduce a novel benchmark, Japanese Heron-Bench, for evaluating Japanese capabilities of VLMs. The Japanese Heron-Bench consists of a variety of imagequestion answer pairs tailored to the Japanese context. Additionally, we present a baseline Japanese VLM that has been trained with Japanese visual instruction tuning datasets. Our Heron-Bench reveals the strengths and limitations of the proposed VLM across various ability dimensions. Furthermore, we clarify the capability gap between strong closed models like GPT-4V and the baseline model, providing valuable insights for future research in this domain. We release the benchmark dataset and training code to facilitate further developments in Japanese VLM research.",
        "authors": [
            "Yuichi Inoue",
            "Kento Sasaki",
            "Yuma Ochi",
            "Kazuki Fujii",
            "Kotaro Tanahashi",
            "Yu Yamaguchi"
        ],
        "citations": 4,
        "references": 25,
        "year": 2024
    },
    {
        "title": "VILA$^2$: VILA Augmented VILA",
        "abstract": "While visual language model architectures and training infrastructures advance rapidly, data curation remains under-explored where quantity and quality become a bottleneck. Existing work either crawls extra Internet data with a loose guarantee of quality or distills from black-box proprietary models, e.g., GPT-4V / Gemini that are API frequency and performance bounded. This work enables a VLM to improve itself via data enhancement, exploiting its generative nature. We introduce a simple yet effective VLM augmentation scheme that includes a self-augment step and a specialist-augment step to iteratively improve data quality and hence, model performance. In the self-augment step, the instruction-finetuned VLM recaptions its pretraining caption datasets and then retrains from scratch leveraging refined data. Without any expensive human-in-the-loop annotation, we observe improvements in data quality and downstream accuracy boosts with three self-augmentation rounds -- a viable free lunch to the current VLM training recipe. When self-augmentation saturates, we augment the caption diversity by leveraging specialty skills picked up from instruction finetuning. We finetune VLM specialists from the self-augmented VLM with domain-specific experts, including spatial, grounding, and OCR, to fuse task-aware synthetic data into the pretraining stage. Data quality improvements and hallucination reductions are cross-checked by VLM (GPT-4V, Gemini) and human judges. Combining self-augmentation and specialist-augmented training, VILA$^2$ consistently improves the accuracy on a wide range of benchmarks over the prior art, producing a reusable pretraining dataset that is 300x more cost-efficient than human labeling.",
        "authors": [
            "Yunhao Fang",
            "Ligeng Zhu",
            "Yao Lu",
            "Yan Wang",
            "Pavlo Molchanov",
            "Jan Kautz",
            "Jang Hyun Cho",
            "Marco Pavone",
            "Song Han",
            "Hongxu Yin"
        ],
        "citations": 4,
        "references": 91,
        "year": 2024
    },
    {
        "title": "Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks",
        "abstract": "Augmenting Large Language Models (LLMs) with image-understanding capabilities has resulted in a boom of high-performing Vision-Language models (VLMs). While studying the alignment of LLMs to human values has received widespread attention, the safety of VLMs has not received the same attention. In this paper, we explore the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct modeling approach. By comparing each VLM to their respective LLM backbone, we find that each VLM is more susceptible to jailbreaking. We consider this as an undesirable outcome from visual instruction-tuning, which imposes a forgetting effect on an LLM‚Äôs safety guardrails. Therefore, we provide recommendations for future work based on evaluation strategies that aim to highlight the weaknesses of a VLM, as well as take safety measures into account during visual instruction tuning.",
        "authors": [
            "Georgios Pantazopoulos",
            "Amit Parekh",
            "Malvina Nikandrou",
            "Alessandro Suglia"
        ],
        "citations": 4,
        "references": 68,
        "year": 2024
    },
    {
        "title": "The Significance of Interseismic Vertical Land Movement at Convergent Plate Boundaries in Probabilistic Sea‚ÄêLevel Projections for AR6 Scenarios: The New Zealand Case",
        "abstract": "Anticipating and managing the impacts of sea‚Äêlevel rise for nations astride active tectonic margins requires understanding of rates of sea surface elevation change in relation to coastal land elevation. Vertical land motion (VLM) can either exacerbate or reduce sea‚Äêlevel changes with impacts varying significantly along a coastline. Determining rate, pattern, and variability of VLM near coasts leads to a direct improvement of location‚Äêspecific relative sea level (RSL) estimates for coastal hazard risk assessment. Here, we utilize vertical velocity field from interferometric synthetic aperture radar (InSAR) data, calibrated with campaign and continuous Global Navigation Satellite System data, to determine the VLM for the entire coastline of New Zealand. Guided by available knowledge of the seismic cycle, the VLM data infer secular, interseismic rates of land surface deformation. Using the Framework for Assessing Changes to Sea‚Äêlevel (FACTS), we build probabilistic RSL projections using the same emissions scenarios employed in IPCC Assessment Report 6 and local VLM data at 8,179 sites, thereby enhancing spatial coverage that was previously limited to four tide gauges. We present ensembles of probability distributions of RSL for each scenario to 2150, and for low confidence sea‚Äêlevel processes to 2300. Where land subsidence is occurring at rates >2 mm/y VLM makes a significant contribution to RSL projections for all scenarios out to 2150. Our approach can be applied to similar locations across the world and has significant implications for adaptation planning, as timing of threshold exceedance for coastal inundation can be brought forward (or delayed) by decades.",
        "authors": [
            "T. Naish",
            "R. Levy",
            "I. Hamling",
            "S. Hreinsd√≥ttir",
            "P. Kumar",
            "G. G. Garner",
            "R. Kopp",
            "N. Golledge",
            "R. Bell",
            "R. Paulik",
            "J. Lawrence",
            "P. Denys",
            "T. Gillies",
            "S. Bengtson",
            "A. Howell",
            "K. Clark",
            "D. King",
            "N. Litchfield",
            "R. Newnham"
        ],
        "citations": 4,
        "references": 104,
        "year": 2024
    },
    {
        "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
        "abstract": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the safety of machine learning systems and has shaped the field of OOD detection. Meanwhile, several other problems are closely related to OOD detection, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD). To unify these problems, a generalized OOD detection framework was proposed, taxonomically categorizing these five problems. However, Vision Language Models (VLMs) such as CLIP have significantly changed the paradigm and blurred the boundaries between these fields, again confusing researchers. In this survey, we first present a generalized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD detection, and OD in the VLM era. Our framework reveals that, with some field inactivity and integration, the demanding challenges have become OOD detection and AD. In addition, we also highlight the significant shift in the definition, problem settings, and benchmarks; we thus feature a comprehensive review of the methodology for OOD detection, including the discussion over other related tasks to clarify their relationship to OOD detection. Finally, we explore the advancements in the emerging Large Vision Language Model (LVLM) era, such as GPT-4V. We conclude this survey with open challenges and future directions.",
        "authors": [
            "Atsuyuki Miyai",
            "Jingkang Yang",
            "Jingyang Zhang",
            "Yifei Ming",
            "Yueqian Lin",
            "Qing Yu",
            "Go Irie",
            "Shafiq R. Joty",
            "Yixuan Li",
            "Hai Li",
            "Ziwei Liu",
            "T. Yamasaki",
            "Kiyoharu Aizawa"
        ],
        "citations": 4,
        "references": 204,
        "year": 2024
    },
    {
        "title": "VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications",
        "abstract": "The advent of immersive Virtual Reality applications has transformed various domains, yet their integration with advanced artificial intelligence technologies like Visual Language Models remains underexplored. This study introduces a pioneering approach utilizing VLMs within VR environments to enhance user interaction and task efficiency. Leveraging the Unity engine and a custom-developed VLM, our system facilitates real-time, intuitive user interactions through natural language processing, without relying on visual text instructions. The incorporation of speech-to-text and text-to-speech technologies allows for seamless communication between the user and the VLM, enabling the system to guide users through complex tasks effectively. Preliminary experimental results indicate that utilizing VLMs not only reduces task completion times but also improves user comfort and task engagement compared to traditional VR interaction methods.",
        "authors": [
            "Mikhail Konenkov",
            "Artem Lykov",
            "Daria Trinitatova",
            "D. Tsetserukou"
        ],
        "citations": 4,
        "references": 27,
        "year": 2024
    },
    {
        "title": "SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-Supervised Change Detector",
        "abstract": "Change detection (CD) aims to identify pixels with semantic changes between images. However, annotating massive numbers of pixel-level images is labor-intensive and costly, especially for multitemporal images, which require pixel-wise comparisons by human experts. Considering the excellent performance of visual-language models (VLMs) for zero-shot, OV, etc., with prompt-based reasoning, it is promising to utilize VLMs to make better CD under limited labeled data. In this article, we propose a VLM guidance-based semi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to synthesize free change labels using VLMs to provide additional supervision signals for unlabeled data. However, almost all current VLMs are designed for single-temporal images and cannot be directly applied to bi- or multitemporal images. Motivated by this, we first propose a VLM-based mixed change event generation (CEG) strategy to yield pseudo-labels for unlabeled CD data. Since the additional supervised signals provided by these VLM-driven pseudo-labels may conflict with the original pseudo-labels from the consistency regularization paradigm (e.g., FixMatch), we propose the dual projection head for de-entangling different signal sources. Further, we explicitly decouple the bitemporal images semantic representation through two auxiliary segmentation decoders, which are also guided by VLM. Finally, to make the model more adequately capture change representations, we introduce contrastive consistency regularization (CCR) by constructing feature-level contrastive loss in auxiliary branches. Extensive experiments show the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch baseline by $+ 5.3~\\text {IoU}^{c}$ on WHU-CD and by $+ 2.4~\\text {IoU}^{c}$ on LEVIR-CD with 5% labels, and SemiCD-VL requires only 5%‚Äì10% of the labels to achieve performance similar to the supervised methods. In addition, our CEG strategy, in an unsupervised manner, can achieve performance far superior to state-of-the-art (SOTA) unsupervised CD methods (e.g., IoU improved from 18.8% to 46.3% on LEVIR-CD dataset). The code is available at https://github.com/likyoo/SemiCD-VL.",
        "authors": [
            "Kaiyu Li",
            "Xiangyong Cao",
            "Yupeng Deng",
            "Jiayi Song",
            "Junmin Liu",
            "Deyu Meng",
            "Zhi Wang"
        ],
        "citations": 4,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Bridge the Modality and Capability Gaps in Vision-Language Model Selection",
        "abstract": "Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. To better reuse the VLM resource and fully leverage its potential on different zero-shot image classification tasks, a promising strategy is selecting appropriate Pre-Trained VLMs from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the\"Modality Gap\"- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the\"Capability Gap\"- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the negative impact of two gaps. SWAB first adopts optimal transport to capture the relevance between open-source and target datasets with a transportation matrix. It then uses this matrix to transfer useful statistics of VLMs from open-source datasets to the target dataset for bridging two gaps. By bridging two gaps to obtain better substitutes for test images, SWAB can accurately predict the performance ranking of different VLMs on the target task without the need for the dataset's images. Experiments across various VLMs and image classification datasets validate SWAB's effectiveness.",
        "authors": [
            "Chao Yi",
            "Yu-Hang He",
            "De-chuan Zhan",
            "Han-Jia Ye"
        ],
        "citations": 4,
        "references": 0,
        "year": 2024
    },
    {
        "title": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification",
        "abstract": "Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However, these methods heavily depend on a substantial number of bag-level labels and solely learn from the original slides, which are easily affected by variations in data distribution. Recently, vision language model (VLM)-based methods introduced the language prior by pre-training on large-scale pathological image-text pairs. However, the previous text prompt lacks the consideration of pathological prior knowledge, there-fore does not substantially boost the model's performance. Moreover, the collection of such pairs and the pre-training process are very time-consuming and source-intensive. To solve the above problems, we propose a dual-scale vision-language multiple instance learning (ViLa-MIL) framework for whole slide image classification. Specifically, we propose a dual-scale visual descriptive text prompt based on the frozen large language model (LLM) to boost the performance of VLM effectively. To transfer the VLM to process WSI efficiently, for the image branch, we propose a prototype-guided patch decoder to aggregate the patch features progressively by grouping similar patches into the same prototype; for the text branch, we introduce a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts. Extensive studies on three multi-cancer and multi-center subtyping datasets demonstrate the superiority of ViLa-MIL.",
        "authors": [
            "Jiangbo Shi",
            "Chen Li",
            "Tieliang Gong",
            "Yefeng Zheng",
            "Huazhu Fu"
        ],
        "citations": 4,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Highlighting the Safety Concerns of Deploying LLMs/VLMs in Robotics",
        "abstract": "In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works focus on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation and navigation. Despite these improvements, analyzing the safety of such systems remains underexplored yet extremely critical. LLMs and VLMs are highly susceptible to adversarial inputs, prompting a significant inquiry into the safety of robotic systems. This concern is important because robotics operate in the physical world where erroneous actions can result in severe consequences. This paper explores this issue thoroughly, presenting a mathematical formulation of potential attacks on LLM/VLM-based robotic systems and offering experimental evidence of the safety challenges. Our empirical findings highlight a significant vulnerability: simple modifications to the input can drastically reduce system effectiveness. Specifically, our results demonstrate an average performance deterioration of 19.4% under minor input prompt modifications and a more alarming 29.1% under slight perceptual changes. These findings underscore the urgent need for robust countermeasures to ensure the safe and reliable deployment of advanced LLM/VLM-based robotic systems.",
        "authors": [
            "Xiyang Wu",
            "Ruiqi Xian",
            "Tianrui Guan",
            "Jing Liang",
            "Souradip Chakraborty",
            "Fuxiao Liu",
            "Brian M. Sadler",
            "Dinesh Manocha",
            "A. S. Bedi"
        ],
        "citations": 4,
        "references": 40,
        "year": 2024
    },
    {
        "title": "AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection",
        "abstract": "Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories. This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP. AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data. Two types of learnable prompts are proposed: static and dynamic. Static prompts are shared across all images, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic prompts are generated for each test image, providing CLIP with dynamic adaptation capabilities. The combination of static and dynamic prompts is referred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive experiments conducted across 14 real-world anomaly detection datasets from industrial and medical domains indicate that AdaCLIP outperforms other ZSAD methods and can generalize better to different categories and even domains. Finally, our analysis highlights the importance of diverse auxiliary data and optimized prompts for enhanced generalization capacity. Code is available at https://github.com/caoyunkang/AdaCLIP.",
        "authors": [
            "Yunkang Cao",
            "Jiangning Zhang",
            "Luca Frittoli",
            "Yuqi Cheng",
            "Weiming Shen",
            "Giacomo Boracchi"
        ],
        "citations": 4,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Improve Vision Language Model Chain-of-thought Reasoning",
        "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.",
        "authors": [
            "Ruohong Zhang",
            "Bowen Zhang",
            "Yanghao Li",
            "Haotian Zhang",
            "Zhiqing Sun",
            "Zhe Gan",
            "Yinfei Yang",
            "Ruoming Pang",
            "Yiming Yang"
        ],
        "citations": 4,
        "references": 42,
        "year": 2024
    },
    {
        "title": "GPT4Ego: Unleashing the Potential of Pre-Trained Models for Zero-Shot Egocentric Action Recognition",
        "abstract": "Vision-Language Models (VLMs), pre-trained on large-scale datasets, have shown impressive performance in various visual recognition tasks. This advancement paves the way for notable performance in some egocentric tasks, Zero-Shot Egocentric Action Recognition (ZS-EAR), entailing VLMs zero-shot to recognize actions from first-person videos enriched in more realistic human-environment interactions. Typically, VLMs handle ZS-EAR as a global video-text matching task, which often leads to suboptimal alignment of vision and linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs, emphasizing fine-grained concept-description alignment that capitalizes on the rich semantic and contextual details in egocentric videos. In this work, we introduce a straightforward yet remarkably potent VLM framework, <italic>aka</italic> GPT4Ego, designed to enhance the fine-grained alignment of concept and description between vision and language. Specifically, we first propose a new Ego-oriented Text Prompting (EgoTP<inline-formula><tex-math notation=\"LaTeX\">$\\spadesuit$</tex-math></inline-formula>) scheme, which effectively prompts action-related text-contextual semantics by evolving word-level class names to sentence-level contextual descriptions by ChatGPT with well-designed chain-of-thought textual prompts. Moreover, we design a new Ego-oriented Visual Parsing (EgoVP<inline-formula><tex-math notation=\"LaTeX\">$\\clubsuit$</tex-math></inline-formula>) strategy that learns action-related vision-contextual semantics by refining global-level images to part-level contextual concepts with the help of SAM. Extensive experiments demonstrate GPT4Ego significantly outperforms existing VLMs on three large-scale egocentric video benchmarks, i.e., EPIC-KITCHENS-100 (33.2%<inline-formula><tex-math notation=\"LaTeX\">$\\uparrow$</tex-math></inline-formula><inline-formula><tex-math notation=\"LaTeX\">$_{\\bm {+9.4}}$</tex-math></inline-formula>), EGTEA (39.6%<inline-formula><tex-math notation=\"LaTeX\">$\\uparrow$</tex-math></inline-formula><inline-formula><tex-math notation=\"LaTeX\">$_{\\bm {+5.5}}$</tex-math></inline-formula>), and CharadesEgo (31.5%<inline-formula><tex-math notation=\"LaTeX\">$\\uparrow$</tex-math></inline-formula><inline-formula><tex-math notation=\"LaTeX\">$_{\\bm {+2.6}}$</tex-math></inline-formula>). In addition, benefiting from the novel mechanism of fine-grained concept and description alignment, GPT4Ego can sustainably evolve with the advancement of ever-growing pre-trained foundational models. We hope this work can encourage the egocentric community to build more investigation into pre-trained vision-language models.",
        "authors": [
            "Guangzhao Dai",
            "Xiangbo Shu",
            "Wenhao Wu",
            "Rui Yan",
            "Jiachao Zhang"
        ],
        "citations": 4,
        "references": 145,
        "year": 2024
    },
    {
        "title": "LLMC: Benchmarking Large Language Model Quantization with a Versatile Compression Toolkit",
        "abstract": "Recent advancements in large language models (LLMs) are propelling us toward artificial general intelligence with their remarkable emergent abilities and reasoning capabilities. However, the substantial computational and memory requirements limit the widespread adoption. Quantization, a key compression technique, can effectively mitigate these demands by compressing and accelerating LLMs, albeit with potential risks to accuracy. Numerous studies have aimed to minimize the accuracy loss associated with quantization. However, their quantization configurations vary from each other and cannot be fairly compared. In this paper, we present LLMC, a plug-and-play compression toolkit, to fairly and systematically explore the impact of quantization. LLMC integrates dozens of algorithms, models, and hardware, offering high extensibility from integer to floating-point quantization, from LLM to vision-language (VLM) model, from fixed-bit to mixed precision, and from quantization to sparsification. Powered by this versatile toolkit, our benchmark covers three key aspects: calibration data, algorithms (three strategies), and data formats, providing novel insights and detailed analyses for further research and practical guidance for users. Our toolkit is available at https://github.com/ModelTC/llmc.",
        "authors": [
            "Ruihao Gong",
            "Yang Yong",
            "Shiqiao Gu",
            "Yushi Huang",
            "Yunchen Zhang",
            "Xianglong Liu",
            "Dacheng Tao"
        ],
        "citations": 4,
        "references": 59,
        "year": 2024
    },
    {
        "title": "Code as Reward: Empowering Reinforcement Learning with VLMs",
        "abstract": "Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards.",
        "authors": [
            "David Venuto",
            "Sami Nur Islam",
            "Martin Klissarov",
            "D. Precup",
            "Sherry Yang",
            "Ankit Anand"
        ],
        "citations": 4,
        "references": 47,
        "year": 2024
    },
    {
        "title": "MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models",
        "abstract": "We introduce MultiMedEval, an open-source toolkit for fair and reproducible evaluation of large, medical vision-language models (VLM). MultiMedEval comprehensively assesses the models' performance on a broad array of six multi-modal tasks, conducted over 23 datasets, and spanning over 11 medical domains. The chosen tasks and performance metrics are based on their widespread adoption in the community and their diversity, ensuring a thorough evaluation of the model's overall generalizability. We open-source a Python toolkit (github.com/corentin-ryr/MultiMedEval) with a simple interface and setup process, enabling the evaluation of any VLM in just a few lines of code. Our goal is to simplify the intricate landscape of VLM evaluation, thus promoting fair and uniform benchmarking of future models.",
        "authors": [
            "Corentin Royer",
            "Bjoern H Menze",
            "A. Sekuboyina"
        ],
        "citations": 4,
        "references": 39,
        "year": 2024
    },
    {
        "title": "Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models",
        "abstract": "Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics. Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database. However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall. These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness. To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting. This system incorporates a vision language model (VLM) based image captioner to enhance the quality of text-based queries, resulting in more informative queries with each iteration. Moreover, we introduce a large language model (LLM) based denoiser to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models. To evaluate our system, we curate a new dataset by adapting the MSR-VTT video retrieval dataset to the image retrieval task, offering multiple relevant ground truth images for each query. Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10% improvement in terms of recall. Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation.",
        "authors": [
            "Hongyi Zhu",
            "Jia-Hong Huang",
            "S. Rudinac",
            "E. Kanoulas"
        ],
        "citations": 4,
        "references": 94,
        "year": 2024
    },
    {
        "title": "MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion",
        "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Mu ltimodal-L LM a ge n t (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on https:github.com/ measure-infinity/mulan-code .",
        "authors": [
            "Sen Li",
            "Ruochen Wang",
            "Cho-Jui Hsieh",
            "Minhao Cheng",
            "Tianyi Zhou"
        ],
        "citations": 4,
        "references": 20,
        "year": 2024
    },
    {
        "title": "Synth2: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings",
        "abstract": "The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). In this work, we investigate an approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs a pretrained text-to-image model to synthesize image embeddings from captions generated by an LLM. Despite the text-to-image model and VLM initially being trained on the same data, our approach leverages the image generator's ability to create novel compositions, resulting in synthetic image embeddings that expand beyond the limitations of the original dataset. Extensive experiments demonstrate that our VLM, finetuned on synthetic data achieves comparable performance to models trained solely on human-annotated data, while requiring significantly less data. Furthermore, we perform a set of analyses on captions which reveals that semantic diversity and balance are key aspects for better downstream performance. Finally, we show that synthesizing images in the image embedding space is 25\\% faster than in the pixel space. We believe our work not only addresses a significant challenge in VLM training but also opens up promising avenues for the development of self-improving multi-modal models.",
        "authors": [
            "Sahand Sharifzadeh",
            "Christos Kaplanis",
            "Shreya Pathak",
            "D. Kumaran",
            "Anastasija Ilic",
            "Jovana Mitrovic",
            "Charles Blundell",
            "Andrea Banino"
        ],
        "citations": 4,
        "references": 54,
        "year": 2024
    },
    {
        "title": "RS5M and GeoRSCLIP: A Large-Scale Vision- Language Dataset and a Large Vision-Language Model for Remote Sensing",
        "abstract": "Pretrained vision-language models (VLMs) utilizing extensive image‚Äìtext paired data have demonstrated unprecedented image‚Äìtext association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pretrained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this article, we present an image‚Äìtext paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image‚Äìtext paired datasets and captioning label-only RS datasets with pretrained VLM. These constitute the first large-scale RS image‚Äìtext paired dataset. Additionally, we present GeoRSCLIP by fine-tuning (FT) or applying parameter-efficient FT (PEFT) methods to the CLIP model using RS5M. Experimental results show that our proposed dataset is highly effective for various tasks, and our model GeoRSCLIP improves upon the baseline or previous state-of-the-art model by 3%‚Äì20% in zero-shot classification (ZSC) tasks, 3%‚Äì6% in RS cross-modal text‚Äìimage retrieval (RSCTIR) and 4%‚Äì5% in semantic localization (SeLo) tasks. Dataset and models have been released in: https://github.com/om-ai-lab/RS5M.",
        "authors": [
            "Zilun Zhang",
            "Tiancheng Zhao",
            "Yulong Guo",
            "Jianwei Yin"
        ],
        "citations": 20,
        "references": 130,
        "year": 2023
    },
    {
        "title": "AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description",
        "abstract": "Our objective is to generate Audio Descriptions (ADs) for both movies and TV series in a training-free manner. We use the power of off-the-shelf Visual-Language Models (VLMs) and Large Language Models (LLMs), and develop visual and text prompting strategies for this task. Our contributions are three-fold: (i) We demonstrate that a VLM can successfully name and refer to characters if directly prompted with character information through visual indications without requiring any fine-tuning; (ii) A two-stage process is developed to generate ADs, with the first stage asking the VLM to comprehensively describe the video, followed by a second stage utilising a LLM to summarise dense textual information into one succinct AD sentence; (iii) A new dataset for TV audio description is formulated. Our approach, named AutoAD-Zero, demonstrates outstanding performance (even competitive with some models fine-tuned on ground truth ADs) in AD generation for both movies and TV series, achieving state-of-the-art CRITIC scores.",
        "authors": [
            "Junyu Xie",
            "Tengda Han",
            "Max Bain",
            "Arsha Nagrani",
            "G√ºl Varol",
            "Weidi Xie",
            "Andrew Zisserman"
        ],
        "citations": 3,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",
        "abstract": "The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA along with human annotations for the categories as mentioned earlier.",
        "authors": [
            "Anku Rani",
            "Vipula Rawte",
            "Harshad Sharma",
            "Neeraj Anand",
            "Krishnav Rajbangshi",
            "Amit P. Sheth",
            "Amitava Das"
        ],
        "citations": 3,
        "references": 23,
        "year": 2024
    },
    {
        "title": "Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and Semantically-Rich Vision-Language Models",
        "abstract": "Vision-language models (VLMs) have achieved significant strides in recent times specially in multimodal tasks, yet they remain susceptible to adversarial attacks on their vision components. To address this, we propose Sim-CLIP, an unsupervised adversarial fine-tuning method that enhances the robustness of the widely-used CLIP vision encoder against such attacks while maintaining semantic richness and specificity. By employing a Siamese architecture with cosine similarity loss, Sim-CLIP learns semantically meaningful and attack-resilient visual representations without requiring large batch sizes or momentum encoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned CLIP encoder exhibit significantly enhanced robustness against adversarial attacks, while preserving semantic meaning of the perturbed images. Notably, Sim-CLIP does not require additional training or fine-tuning of the VLM itself; replacing the original vision encoder with our fine-tuned Sim-CLIP suffices to provide robustness. This work underscores the significance of reinforcing foundational models like CLIP to safeguard the reliability of downstream VLM applications, paving the way for more secure and effective multimodal systems.",
        "authors": [
            "Md. Zarif Hossain",
            "Ahmed Imteaj"
        ],
        "citations": 3,
        "references": 44,
        "year": 2024
    },
    {
        "title": "Advancing Cross-domain Discriminability in Continual Learning of Vison-Language Models",
        "abstract": "Continual learning (CL) with Vision-Language Models (VLMs) has overcome the constraints of traditional CL, which only focuses on previously encountered classes. During the CL of VLMs, we need not only to prevent the catastrophic forgetting on incrementally learned knowledge but also to preserve the zero-shot ability of VLMs. However, existing methods require additional reference datasets to maintain such zero-shot ability and rely on domain-identity hints to classify images across different domains. In this study, we propose Regression-based Analytic Incremental Learning (RAIL), which utilizes a recursive ridge regression-based adapter to learn from a sequence of domains in a non-forgetting manner and decouple the cross-domain correlations by projecting features to a higher-dimensional space. Cooperating with a training-free fusion module, RAIL absolutely preserves the VLM's zero-shot ability on unseen domains without any reference data. Additionally, we introduce Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting. In this setting, a CL learner is required to incrementally learn from multiple domains and classify test images from both seen and unseen domains without any domain-identity hint. We theoretically prove RAIL's absolute memorization on incrementally learned domains. Experiment results affirm RAIL's state-of-the-art performance in both X-TAIL and existing Multi-domain Task-Incremental Learning settings. The code is released at https://github.com/linghan1997/Regression-based-Analytic-Incremental-Learning.",
        "authors": [
            "Yicheng Xu",
            "Yuxin Chen",
            "Jiahao Nie",
            "Yusong Wang",
            "Huiping Zhuang",
            "Manabu Okumura"
        ],
        "citations": 3,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Selective \"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning",
        "abstract": "Selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without increasing the error rate of the system's predictions. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables three VLMs (BLIP2, InstructBLIP, and LLaVA-1.5) to answer up to 20% more questions on the VQAv2 and A-OKVQA tasks without decreasing system accuracy, thus improving overall system reliability. Our code is available at https://github.com/tejas1995/ReCoVERR.",
        "authors": [
            "Tejas Srinivasan",
            "Jack Hessel",
            "Tanmay Gupta",
            "Bill Yuchen Lin",
            "Yejin Choi",
            "Jesse Thomason",
            "Khyathi Raghavi Chandu"
        ],
        "citations": 3,
        "references": 43,
        "year": 2024
    },
    {
        "title": "3rd Place Solution for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation",
        "abstract": "Referring video object segmentation (RVOS) relies on natural language expressions to segment target objects in video, emphasizing modeling dense text-video relations. The current RVOS methods typically use independently pre-trained vision and language models as backbones, resulting in a significant domain gap between video and text. In cross-modal feature interaction, text features are only used as query initialization and do not fully utilize important information in the text. In this work, we propose using frozen pre-trained vision-language models (VLM) as backbones, with a specific emphasis on enhancing cross-modal feature interaction. Firstly, we use frozen convolutional CLIP backbone to generate feature-aligned vision and text features, alleviating the issue of domain gap and reducing training costs. Secondly, we add more cross-modal feature fusion in the pipeline to enhance the utilization of multi-modal information. Furthermore, we propose a novel video query initialization method to generate higher quality video queries. Without bells and whistles, our method achieved 51.5 J&F on the MeViS test set and ranked 3rd place for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation.",
        "authors": [
            "Feiyu Pan",
            "Hao Fang",
            "Xiankai Lu"
        ],
        "citations": 3,
        "references": 23,
        "year": 2024
    },
    {
        "title": "Prompt Injection Attacks on Large Language Models in Oncology",
        "abstract": "Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be attacked by prompt injection attacks, which can be used to output harmful information just by interacting with the VLM, without any access to its parameters. We performed a quantitative study to evaluate the vulnerabilities to these attacks in four state of the art VLMs which have been proposed to be of utility in healthcare: Claude 3 Opus, Claude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show that all of these models are susceptible. Specifically, we show that embedding sub-visual prompts in medical imaging data can cause the model to provide harmful output, and that these prompts are non-obvious to human observers. Thus, our study demonstrates a key vulnerability in medical VLMs which should be mitigated before widespread clinical adoption.",
        "authors": [
            "J. Clusmann",
            "Dyke Ferber",
            "I. Wiest",
            "C. V. Schneider",
            "T. Brinker",
            "S. Foersch",
            "Daniel Truhn",
            "J. N. Kather"
        ],
        "citations": 3,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Unified Embedding Alignment for Open-Vocabulary Video Instance Segmentation",
        "abstract": "Open-Vocabulary Video Instance Segmentation (VIS) is attracting increasing attention due to its ability to segment and track arbitrary objects. However, the recent Open-Vocabulary VIS attempts obtained unsatisfactory results, especially in terms of generalization ability of novel categories. We discover that the domain gap between the VLM features (e.g., CLIP) and the instance queries and the underutilization of temporal consistency are two central causes. To mitigate these issues, we design and train a novel Open-Vocabulary VIS baseline called OVFormer. OVFormer utilizes a lightweight module for unified embedding alignment between query embeddings and CLIP image embeddings to remedy the domain gap. Unlike previous image-based training methods, we conduct video-based model training and deploy a semi-online inference scheme to fully mine the temporal consistency in the video. Without bells and whistles, OVFormer achieves 21.9 mAP with a ResNet-50 backbone on LV-VIS, exceeding the previous state-of-the-art performance by 7.7. Extensive experiments on some Close-Vocabulary VIS datasets also demonstrate the strong zero-shot generalization ability of OVFormer (+ 7.6 mAP on YouTube-VIS 2019, + 3.9 mAP on OVIS). Code is available at https://github.com/fanghaook/OVFormer.",
        "authors": [
            "Hao Fang",
            "Peng Wu",
            "Yawei Li",
            "Xinxin Zhang",
            "Xiankai Lu"
        ],
        "citations": 3,
        "references": 51,
        "year": 2024
    },
    {
        "title": "GameVLM: A Decision-making Framework for Robotic Task Planning Based on Visual Language Models and Zero-sum Games",
        "abstract": "With their prominent scene understanding and reasoning capabilities, pre-trained visual-language models (VLMs) such as GPT-4V have attracted increasing attention in robotic task planning. Compared with traditional task planning strategies, VLMs are strong in multimodal information parsing and code generation and show remarkable efficiency. Although VLMs demonstrate great potential in robotic task planning, they suffer from challenges like hallucination, semantic complexity, and limited context. To handle such issues, this paper proposes a multi-agent framework, i.e., GameVLM, to enhance the decision-making process in robotic task planning. In this study, VLM-based decision and expert agents are presented to conduct the task planning. Specifically, decision agents are used to plan the task, and the expert agent is employed to evaluate these task plans. Zero-sum game theory is introduced to resolve inconsistencies among different agents and determine the optimal solution. Experimental results on real robots demonstrate the efficacy of the proposed framework, with an average success rate of 83.3%. Videos of our experiments are available at https://youtu.be/sam-MKCPP7Y.",
        "authors": [
            "Aoran Mei",
            "Jianhua Wang",
            "Guo-Niu Zhu",
            "Zhongxue Gan"
        ],
        "citations": 3,
        "references": 17,
        "year": 2024
    },
    {
        "title": "VLMPC: Vision-Language Model Predictive Control for Robotic Manipulation",
        "abstract": "Although Model Predictive Control (MPC) can effectively predict the future states of a system and thus is widely used in robotic manipulation tasks, it does not have the capability of environmental perception, leading to the failure in some complex scenarios. To address this issue, we introduce Vision-Language Model Predictive Control (VLMPC), a robotic manipulation framework which takes advantage of the powerful perception capability of vision language model (VLM) and integrates it with MPC. Specifically, we propose a conditional action sampling module which takes as input a goal image or a language instruction and leverages VLM to sample a set of candidate action sequences. Then, a lightweight action-conditioned video prediction model is designed to generate a set of future frames conditioned on the candidate action sequences. VLMPC produces the optimal action sequence with the assistance of VLM through a hierarchical cost function that formulates both pixel-level and knowledge-level consistence between the current observation and the goal image. We demonstrate that VLMPC outperforms the state-of-the-art methods on public benchmarks. More importantly, our method showcases excellent performance in various real-world tasks of robotic manipulation. Code is available at~\\url{https://github.com/PPjmchen/VLMPC}.",
        "authors": [
            "Wentao Zhao",
            "Jiaming Chen",
            "Ziyu Meng",
            "Donghui Mao",
            "Ran Song",
            "Wei Zhang"
        ],
        "citations": 3,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Advancing Cross-domain Discriminability in Continual Learning of Vison-Language Models",
        "abstract": "Continual learning (CL) with Vision-Language Models (VLMs) has overcome the constraints of traditional CL, which only focuses on previously encountered classes. During the CL of VLMs, we need not only to prevent the catastrophic forgetting on incrementally learned knowledge but also to preserve the zero-shot ability of VLMs. However, existing methods require additional reference datasets to maintain such zero-shot ability and rely on domain-identity hints to classify images across different domains. In this study, we propose Regression-based Analytic Incremental Learning (RAIL), which utilizes a recursive ridge regression-based adapter to learn from a sequence of domains in a non-forgetting manner and decouple the cross-domain correlations by projecting features to a higher-dimensional space. Cooperating with a training-free fusion module, RAIL absolutely preserves the VLM's zero-shot ability on unseen domains without any reference data. Additionally, we introduce Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting. In this setting, a CL learner is required to incrementally learn from multiple domains and classify test images from both seen and unseen domains without any domain-identity hint. We theoretically prove RAIL's absolute memorization on incrementally learned domains. Experiment results affirm RAIL's state-of-the-art performance in both X-TAIL and existing Multi-domain Task-Incremental Learning settings. The code is released at https://github.com/linghan1997/Regression-based-Analytic-Incremental-Learning.",
        "authors": [
            "Yicheng Xu",
            "Yuxin Chen",
            "Jiahao Nie",
            "Yusong Wang",
            "Huiping Zhuang",
            "Manabu Okumura"
        ],
        "citations": 3,
        "references": 45,
        "year": 2024
    },
    {
        "title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models",
        "abstract": "Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality benchmarks, while maintaining or improving the performance on standard object-recognition and retrieval benchmarks. Our code and pre-trained models are publicly available at https://github.com/netflix/clove.",
        "authors": [
            "Santiago Castro",
            "Amir Ziai",
            "Avneesh Saluja",
            "Zhuoning Yuan",
            "Rada Mihalcea"
        ],
        "citations": 3,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?",
        "abstract": "Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.",
        "authors": [
            "Letitia Parcalabescu",
            "Anette Frank"
        ],
        "citations": 3,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models",
        "abstract": "Language and Vision-Language Models (LLMs/VLMs) have revolutionized the field of AI by their ability to generate human-like text and understand images, but ensuring their reliability is crucial. This paper aims to evaluate the ability of LLMs (GPT4, GPT-3.5, LLaMA2, and PaLM 2) and VLMs (GPT4V and Gemini Pro Vision) to estimate their verbalized uncertainty via prompting. We propose the new Japanese Uncertain Scenes (JUS) dataset, aimed at testing VLM capabilities via difficult queries and object counting, and the Net Calibration Error (NCE) to measure direction of miscalibration.Results show that both LLMs and VLMs have a high calibration error and are overconfident most of the time, indicating a poor capability for uncertainty estimation. Additionally we develop prompts for regression tasks, and we show that VLMs have poor calibration when producing mean/standard deviation and 95% confidence intervals.",
        "authors": [
            "Tobias Groot",
            "Matias Valdenegro-Toro"
        ],
        "citations": 3,
        "references": 18,
        "year": 2024
    },
    {
        "title": "Hawk: Learning to Understand Open-World Video Anomalies",
        "abstract": "Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios. In this paper, we introduce Hawk, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, Hawk explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that Hawk achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.",
        "authors": [
            "Jiaqi Tang",
            "Hao Lu",
            "Ruizheng Wu",
            "Xiaogang Xu",
            "Ke Ma",
            "Cheng Fang",
            "Bin Guo",
            "Jiangbo Lu",
            "Qifeng Chen",
            "Ying-Cong Chen"
        ],
        "citations": 3,
        "references": 48,
        "year": 2024
    },
    {
        "title": "AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description",
        "abstract": "Our objective is to generate Audio Descriptions (ADs) for both movies and TV series in a training-free manner. We use the power of off-the-shelf Visual-Language Models (VLMs) and Large Language Models (LLMs), and develop visual and text prompting strategies for this task. Our contributions are three-fold: (i) We demonstrate that a VLM can successfully name and refer to characters if directly prompted with character information through visual indications without requiring any fine-tuning; (ii) A two-stage process is developed to generate ADs, with the first stage asking the VLM to comprehensively describe the video, followed by a second stage utilising a LLM to summarise dense textual information into one succinct AD sentence; (iii) A new dataset for TV audio description is formulated. Our approach, named AutoAD-Zero, demonstrates outstanding performance (even competitive with some models fine-tuned on ground truth ADs) in AD generation for both movies and TV series, achieving state-of-the-art CRITIC scores.",
        "authors": [
            "Junyu Xie",
            "Tengda Han",
            "Max Bain",
            "Arsha Nagrani",
            "G√ºl Varol",
            "Weidi Xie",
            "Andrew Zisserman"
        ],
        "citations": 3,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Selective \"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning",
        "abstract": "Selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without increasing the error rate of the system's predictions. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables three VLMs (BLIP2, InstructBLIP, and LLaVA-1.5) to answer up to 20% more questions on the VQAv2 and A-OKVQA tasks without decreasing system accuracy, thus improving overall system reliability. Our code is available at https://github.com/tejas1995/ReCoVERR.",
        "authors": [
            "Tejas Srinivasan",
            "Jack Hessel",
            "Tanmay Gupta",
            "Bill Yuchen Lin",
            "Yejin Choi",
            "Jesse Thomason",
            "Khyathi Raghavi Chandu"
        ],
        "citations": 3,
        "references": 43,
        "year": 2024
    },
    {
        "title": "Unified Embedding Alignment for Open-Vocabulary Video Instance Segmentation",
        "abstract": "Open-Vocabulary Video Instance Segmentation (VIS) is attracting increasing attention due to its ability to segment and track arbitrary objects. However, the recent Open-Vocabulary VIS attempts obtained unsatisfactory results, especially in terms of generalization ability of novel categories. We discover that the domain gap between the VLM features (e.g., CLIP) and the instance queries and the underutilization of temporal consistency are two central causes. To mitigate these issues, we design and train a novel Open-Vocabulary VIS baseline called OVFormer. OVFormer utilizes a lightweight module for unified embedding alignment between query embeddings and CLIP image embeddings to remedy the domain gap. Unlike previous image-based training methods, we conduct video-based model training and deploy a semi-online inference scheme to fully mine the temporal consistency in the video. Without bells and whistles, OVFormer achieves 21.9 mAP with a ResNet-50 backbone on LV-VIS, exceeding the previous state-of-the-art performance by 7.7. Extensive experiments on some Close-Vocabulary VIS datasets also demonstrate the strong zero-shot generalization ability of OVFormer (+ 7.6 mAP on YouTube-VIS 2019, + 3.9 mAP on OVIS). Code is available at https://github.com/fanghaook/OVFormer.",
        "authors": [
            "Hao Fang",
            "Peng Wu",
            "Yawei Li",
            "Xinxin Zhang",
            "Xiankai Lu"
        ],
        "citations": 3,
        "references": 51,
        "year": 2024
    },
    {
        "title": "IQAGPT: computed tomography image quality assessment with vision-language and ChatGPT models",
        "abstract": null,
        "authors": [
            "Zhihao Chen",
            "Bin Hu",
            "Chuang Niu",
            "Tao Chen",
            "Yuxin Li",
            "Hongming Shan",
            "Ge Wang"
        ],
        "citations": 3,
        "references": 58,
        "year": 2024
    },
    {
        "title": "Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning",
        "abstract": "Few-shot recognition (FSR) aims to train a classification model with only a few labeled examples of each concept concerned by a downstream task, where data annotation cost can be prohibitively high. We develop methods to solve FSR by leveraging a pretrained Vision-Language Model (VLM). We particularly explore retrieval-augmented learning (RAL), which retrieves data from the VLM's pretraining set to learn better models for serving downstream tasks. RAL has been widely studied in zero-shot recognition but remains under-explored in FSR. Although applying RAL to FSR may seem straightforward, we observe interesting and novel challenges and opportunities. First, somewhat surprisingly, finetuning a VLM on a large amount of retrieved data underperforms state-of-the-art zero-shot methods. This is due to the imbalanced distribution of retrieved data and its domain gaps with the few-shot examples in the downstream task. Second, more surprisingly, we find that simply finetuning a VLM solely on few-shot examples significantly outperforms previous FSR methods, and finetuning on the mix of retrieved and few-shot data yields even better results. Third, to mitigate the imbalanced distribution and domain gap issues, we propose Stage-Wise retrieval-Augmented fineTuning (SWAT), which involves end-to-end finetuning on mixed data in the first stage and retraining the classifier on the few-shot data in the second stage. Extensive experiments on nine popular benchmarks demonstrate that SWAT significantly outperforms previous methods by $>$6% accuracy.",
        "authors": [
            "Tian Liu",
            "Huixin Zhang",
            "Shubham Parashar",
            "Shu Kong"
        ],
        "citations": 3,
        "references": 80,
        "year": 2024
    },
    {
        "title": "Smart Vision-Language Reasoners",
        "abstract": "In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \\cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to $48\\%$ gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at https://github.com/smarter-vlm/smarter.",
        "authors": [
            "Denisa Roberts",
            "Lucas Roberts"
        ],
        "citations": 2,
        "references": 60,
        "year": 2024
    },
    {
        "title": "DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for Task-Oriented Manipulation",
        "abstract": "We introduce DexGanGrasp, a dexterous grasp synthesis method that generates and evaluates grasps with a single view in real-time. DexGanGrasp comprises a Conditional Generative Adversarial Network (cGAN)-based DexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor to assess the stability of these grasps. Extensive simulation and real-world experiments showcase the effectiveness of our proposed method, outperforming the baseline FFHNet with an $18.57 \\%$ higher success rate in real-world evaluation. To further achieve task-oriented grasping, we extend DexGanGrasp to DexAfford-Prompt, an open-vocabulary affordance grounding pipeline for dexterous grasping leveraging Multimodal Large Language Models (MLLM) and Vision Language Models (VLM) with successful real-world deployments. For the code and data, visit our website.",
        "authors": [
            "Qian Feng",
            "David S. Martinez Lema",
            "M. Malmir",
            "Hang Li",
            "Jianxiang Feng",
            "Zhaopeng Chen",
            "Alois Knoll"
        ],
        "citations": 2,
        "references": 55,
        "year": 2024
    },
    {
        "title": "Building Vision-Language Models on Solid Foundations with Masked Distillation",
        "abstract": "Recent advancements in Vision-Language Models (VLMs) have marked a significant leap in bridging the gap between computer vision and natural language processing. However, traditional VLMs, trained through contrastive learning on limited and noisy image-text pairs, often lack the spatial and linguistic understanding to generalize well to dense vision tasks or less common languages. Our approach, Solid Foun-dation CLIP (SF-CLIP), circumvents this issue by implicitly building on the solid visual and language understanding of foundational models trained on vast amounts of unimodal data. SF-CLIP integrates contrastive image-text pretraining with a masked knowledge distillation from large foundational text and vision models. This methodology guides our VLM in developing robust text and image representations. As a result, SF-CLIP shows exceptional zero-shot classification accuracy and enhanced image and text retrieval capabilities, setting a new state of the art for ViT-B/16 trained on YFCC15M and CC12M.Moreover, the dense per-patch supervision enhances our zero-shot and linear probe performance in semantic segmentation tasks. A remarkable aspect of our model is its multilingual proficiency, evidenced by strong retrieval results in multiple languages despite being trained predominantly on English data. We achieve all of these improvements without sacrificing the training efficiency through our selective application of masked distillation and the inheritance of teacher word embeddings.",
        "authors": [
            "Sepehr Sameni",
            "Kushal Kafle",
            "Hao Tan",
            "Simon Jenni"
        ],
        "citations": 2,
        "references": 65,
        "year": 2024
    },
    {
        "title": "SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving",
        "abstract": "Many fields could benefit from the rapid development of the large language models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the typically fields facing new opportunities as the LLMs have supported more and more modalities. Here, by utilizing vision-language model (VLM), we proposed an e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided into four stages, which are perception, prediction, planning, and behavior. Each stage consists of several visual question answering (VQA) pairs and VQA pairs interconnect with each other constructing a graph called Graph VQA (GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our method could achieve e2e driving with language. In our method, vision transformers (ViT) models are employed to process nuScenes visual data, while VLM are utilized to interpret and reason about the information extracted from the visual inputs. In the perception stage, the system identifies and classifies objects from the driving environment. The prediction stage involves forecasting the potential movements of these objects. The planning stage utilizes the gathered information to develop a driving strategy, ensuring the safety and efficiency of the autonomous vehicle. Finally, the behavior stage translates the planned actions into executable commands for the vehicle. Our experiments demonstrate that SimpleLLM4AD achieves competitive performance in complex driving scenarios.",
        "authors": [
            "Peiru Zheng",
            "Yun Zhao",
            "Zhan Gong",
            "Hong Zhu",
            "Shaohua Wu"
        ],
        "citations": 2,
        "references": 37,
        "year": 2024
    },
    {
        "title": "Visual Narratives: Large-scale Hierarchical Classification of Art-historical Images",
        "abstract": "Iconography refers to the methodical study and interpretation of thematic content in the visual arts, distinguishing it, e.g., from purely formal or aesthetic considerations. In iconographic studies, Iconclass is a widely used taxonomy that encapsulates historical, biblical, and literary themes, among others. However, given the hierarchical nature and inherent complexity of such a taxonomy, it is highly desirable to use automated methods for (Iconclass-based) image classification. Previous studies either focused narrowly on certain subsets of narratives or failed to exploit Iconclass‚Äôs hierarchical structure. In this paper, we propose a novel approach for Hierarchical Multi-label Classification (HMC) of iconographic concepts in images. We present three strategies, including Language Models (LMs), for the generation of textual image descriptions using keywords extracted from Iconclass. These descriptions are utilized to pre-train a Vision-Language Model (VLM) based on a newly introduced data set of 477,569 images with more than 20,000 Iconclass concepts, far more than considered in previous studies. Furthermore, we present five approaches to multi-label classification, including a novel transformer decoder that leverages hierarchical information from the Iconclass taxonomy. Experimental results show the superiority of this approach over reasonable baselines.",
        "authors": [
            "Matthias Springstein",
            "Stefanie Schneider",
            "J. Rahnama",
            "Julian Stalter",
            "Maximilian Kristen",
            "Eric M√ºller-Budack",
            "Ralph Ewerth"
        ],
        "citations": 2,
        "references": 71,
        "year": 2024
    },
    {
        "title": "Medical Image Interpretation with Large Multimodal Models",
        "abstract": "This working note documents the participation of CS_Morgan in the ImageCLEFmedical 2024 Caption subtasks, focusing on Caption Prediction and Concept Detection challenges. The primary objectives included training, validating, and testing multimodal Artificial Intelligence (AI) models intended to automate the process of generating captions and identifying multi-concepts of radiology images. The dataset used is a subset of the Radiology Objects in COntext version 2 (ROCOv2) dataset and contains image-caption pairs and corresponding Unified Medical Language System (UMLS) concepts. To address the caption prediction challenge, different variants of the Large Language and Vision Assistant (LLaVA) models were experimented with, tailoring them for the medical domain. Additionally, a lightweight Large Multimodal Model (LMM), and MoonDream2, a small Vision Language Model (VLM), were explored. The former is the instruct variant of the Image-aware Decoder Enhanced √† la Flamingo with Interleaved Cross-attentionS (IDEFICS) 9B obtained through quantization. Besides LMMs, conventional encoder-decoder models like Vision Generative Pre-trained Transformer 2 (visionGPT2) and Convolutional Neural Network-Transformer (CNN-Transformer) architectures were considered. Consequently, this enabled 10 submissions for the caption prediction task, with the first submission of LLaVA 1.6 on the Mistral 7B weights securing the 2nd position among the participants. This model was adapted using 40.1M parameters and achieved the best performance on the test data across the performance metrics of BERTScore (0.628059), ROUGE (0.250801), BLEU-1 (0.209298), BLEURT (0.317385), METEOR (0.092682), CIDEr (0.245029), and RefCLIPScore (0.815534). For the concept detection task, our single submission based on the ConvMixer architecture‚Äîa hybrid approach leveraging CNN and Transformer advantages‚Äîranked 9th with an F1-score of 0.107645. Overall, the evaluations on the test data for the caption prediction task submissions suggest that LMMs, quantized LMMs, and small VLMs, when adapted and selectively fine-tuned using fewer parameters, have ample potential for understanding medical concepts present in images.",
        "authors": [
            "Mahmudul Hoque",
            "Md. Rakibul Hasan",
            "Md. Ismail Siddiqi Emon",
            "Fahmi Khalifa",
            "Md Mahmudur Rahman"
        ],
        "citations": 2,
        "references": 54,
        "year": 2024
    },
    {
        "title": "NEUI at MEDIQA-M3G 2024: Medical VQA through consensus",
        "abstract": "This document describes our solution to the MEDIQA-M3G: Multilingual & Multimodal Medical Answer Generation. To build our solution, we leveraged two pre-trained models, a Visual Language Model (VLM) and a Large Language Model (LLM). We fine-tuned both models using the MEDIQA-M3G and MEDIQA-CORR training datasets, respectively. In the first stage, the VLM provides singular responses for each pair of image & text inputs in a case. In the second stage, the LLM consolidates the VLM responses using it as context among the original text input. By changing the original English case content field in the context component of the second stage to the one in Spanish, we adapt the pipeline to generate submissions in English and Spanish. We performed an ablation study to explore the impact of the different models‚Äô capabilities, such as multimodality and reasoning, on the MEDIQA-M3G task. Our approach favored privacy and feasibility by adopting open-source and self-hosted small models and ranked 4th in English and 2nd in Spanish.",
        "authors": [
            "Ricardo Garc√≠a",
            "Oscar Lithgow-Serrano"
        ],
        "citations": 2,
        "references": 31,
        "year": 2024
    },
    {
        "title": "Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following",
        "abstract": "Contextual cues related to a person‚Äôs pose and interactions with objects and other people in the scene can provide valuable information for gaze following. While existing methods have focused on dedicated cue extraction methods, in this work we investigate the zero-shot capabilities of Vision-Language Models (VLMs) for extracting a wide array of contextual cues to improve gaze following performance. We first evaluate various VLMs, prompting strategies, and in-context learning (ICL) techniques for zero-shot cue recognition performance. We then use these insights to extract contextual cues for gaze following, and investigate their impact when incorporated into a state of the art model for the task. Our analysis indicates that BLIP-2 is the overall top performing VLM and that ICL can improve performance. We also observe that VLMs are sensitive to the choice of the text prompt although ensembling over multiple text prompts can provide more robust performance. Additionally, we discover that using the entire image along with an ellipse drawn around the target person is the most effective strategy for visual prompting. For gaze following, incorporating the extracted cues results in better generalization performance, especially when considering a larger set of cues, highlighting the potential of this approach.",
        "authors": [
            "Anshul Gupta",
            "Pierre Vuillecard",
            "Arya Farkhondeh",
            "J. Odobez"
        ],
        "citations": 2,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Learning Visual Grounding from Generative Vision and Language Model",
        "abstract": "Visual grounding tasks aim to localize image regions based on natural language references. In this work, we explore whether generative VLMs predominantly trained on image-text data could be leveraged to scale up the text annotation of visual grounding data. We find that grounding knowledge already exists in generative VLM and can be elicited by proper prompting. We thus prompt a VLM to generate object-level descriptions by feeding it object regions from existing object detection datasets. We further propose attribute modeling to explicitly capture the important object attributes, and spatial relation modeling to capture inter-object relationship, both of which are common linguistic pattern in referring expression. Our constructed dataset (500K images, 1M objects, 16M referring expressions) is one of the largest grounding datasets to date, and the first grounding dataset with purely model-generated queries and human-annotated objects. To verify the quality of this data, we conduct zero-shot transfer experiments to the popular RefCOCO benchmarks for both referring expression comprehension (REC) and segmentation (RES) tasks. On both tasks, our model significantly outperform the state-of-the-art approaches without using human annotated visual grounding data. Our results demonstrate the promise of generative VLM to scale up visual grounding in the real world. Code and models will be released.",
        "authors": [
            "Shijie Wang",
            "Dahun Kim",
            "A. Taalimi",
            "Chen Sun",
            "Weicheng Kuo"
        ],
        "citations": 2,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Sonic VisionLM: Playing Sound with Vision Language Models",
        "abstract": "There has been a growing interest in the task of generating sound for silent videos, primarily because of its prac-ticality in streamlining video post-production. However, existing methods for video-sound generation attempt to di-rectly create sound from visual representations, which can be challenging due to the difficulty of aligning visual rep-resentations with audio representations. In this paper, we present Sonic VisionLM, a novel framework aimed at gen-erating a wide range of sound effects by leveraging vision-language models(VLMs). Instead of generating audio di-rectly from video, we use the capabilities of powerful VLMs. When provided with a silent video, our approach first iden-tifies events within the video using a VLM to suggest pos-sible sounds that match the video content. This shift in approach transforms the challenging task of aligning image and audio into more well-studied sub-problems of aligning image-to-text and text-to-audio through the popular diffusion models. To improve the quality of audio recommen-dations with LLMs, we have collected an extensive dataset that maps text descriptions to specific sound effects and de-veloped a time-controlled audio adapter. Our approach surpasses current state-of-the-art methods for converting video to audio, enhancing synchronization with the visuals, and improving alignment between audio and video components. Project page: https://yusiissy.github.io/SonicVisionLM.github.io/",
        "authors": [
            "Zhifeng Xie",
            "Shengye Yu",
            "Qile He",
            "Mengtian Li"
        ],
        "citations": 2,
        "references": 41,
        "year": 2024
    },
    {
        "title": "SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors",
        "abstract": "Current state-of-the-art spatial reasoning-enhanced VLMs are trained to excel at spatial visual question answering (VQA). However, we believe that higher-level 3D-aware tasks, such as articulating dynamic scene changes and motion planning, require a fundamental and explicit 3D understanding beyond current spatial VQA datasets. In this work, we present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with priors from multiple 3D foundation models in a zero-shot, training-free manner. Extensive experiments demonstrate that our spatial reasoning-imbued VLM performs well on various forms of spatial VQA and can extend to help in various downstream robotics tasks such as pick and stack and trajectory planning.",
        "authors": [
            "Chenyang Ma",
            "Kai Lu",
            "Ta-Ying Cheng",
            "Niki Trigoni",
            "Andrew Markham"
        ],
        "citations": 2,
        "references": 73,
        "year": 2024
    },
    {
        "title": "Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension",
        "abstract": "Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM.",
        "authors": [
            "Wang Xiyao",
            "Zhengyuan Yang",
            "Linjie Li",
            "Hongjin Lu",
            "Yuancheng Xu",
            "Lin Chung-Ching Lin",
            "Lin Kevin",
            "Furong Huang",
            "Lijuan Wang"
        ],
        "citations": 2,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning",
        "abstract": "This paper proposes a novel framework for multi-label image recognition without any training data, called data-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt pretrained Vision-Language Model (VLM) like CLIP to multilabel classification. Through asking LLM by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning prompts. Then we propose a hierarchical prompt learning method by taking the multi-label dependency into consideration, wherein a subset of category-specific prompt tokens are shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical prompts learned from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that our method achieves better results than the state-of-the-art methods, especially outperforming the zero-shot multi-label recognition methods by 4.7% in mAP on MS-COCO.",
        "authors": [
            "Shuo Yang",
            "Zirui Shang",
            "Yongqi Wang",
            "Derong Deng",
            "Hongwei Chen",
            "Qiyuan Cheng",
            "Xinxiao Wu"
        ],
        "citations": 2,
        "references": 63,
        "year": 2024
    },
    {
        "title": "LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving",
        "abstract": "Existing Vision-Language Models (VLMs) produce long-term trajectory waypoints or directly control actions based on their perception input and language prompt. However, these VLMs are not explicitly aware of the constraints imposed by the scene or kinematics of the vehicle. As a result, the generated trajectories or control inputs are likely to be unsafe and/or infeasible. In this paper, we introduce LeGo-Drive‚Ä†, which aims to address these issues. Our key idea is to use the VLM to just predict a goal location based on the given language command and perception input, which is then fed to a downstream differentiable trajectory optimizer with learnable components. We train the VLM and the trajectory optimizer in an end-to-end fashion using a loss function that captures the ego-vehicle‚Äôs ability to reach the predicted goal while satisfying safety and kinematic constraints. The gradients during the back-propagation flow through the optimization layer and make the VLM aware of the planner‚Äôs capabilities, making more feasible goal predictions. We compare our end-to-end approach with a decoupled framework where the planner is just used at the inference time to drive to the VLM-predicted goal location and report a goal reaching Success Rate of 81%. We demonstrate the versatility of LeGo-Drive‚Ä† across various driving scenarios and navigation commands, highlighting its potential for practical deployment in autonomous vehicles.",
        "authors": [
            "Pranjal Paul",
            "Anant Garg",
            "Tushar Choudhary",
            "Arun Kumar Singh",
            "K. M. Krishna"
        ],
        "citations": 2,
        "references": 30,
        "year": 2024
    },
    {
        "title": "ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation",
        "abstract": "This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-contrastive BLIP ITM model is stronger (87%). We also find that finetuning on fewer than 2,000 examples yields significant performance gains on this out-of-distribution word-order understanding task. The dataset is here: https://github.com/Top34051/colorswap and here: https://huggingface.co/datasets/stanfordnlp/colorswap.",
        "authors": [
            "Jirayu Burapacheep",
            "Ishan Gaur",
            "Agam Bhatia",
            "Tristan Thrush"
        ],
        "citations": 2,
        "references": 21,
        "year": 2024
    },
    {
        "title": "VG4D: Vision-Language Model Goes 4D Video Recognition",
        "abstract": "Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems. However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information. Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks. However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem. In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pretrained models to a 4D point cloud network. Our approach involves aligning the 4D encoder‚Äôs representation with a VLM learning a shared visual and text space from training on large-scale image-text pairs. By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance. To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos. Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both NTU RGB+D 60 dataset and NTU RGB+D 120 dataset.",
        "authors": [
            "Zhichao Deng",
            "Xiangtai Li",
            "Xia Li",
            "Yunhai Tong",
            "Shen Zhao",
            "Mengyuan Liu"
        ],
        "citations": 2,
        "references": 56,
        "year": 2024
    },
    {
        "title": "ChangeChat: An Interactive Model for Remote Sensing Change Analysis via Multimodal Instruction Tuning",
        "abstract": "Remote sensing (RS) change analysis is vital for monitoring Earth's dynamic processes by detecting alterations in images over time. Traditional change detection excels at identifying pixel-level changes but lacks the ability to contextualize these alterations. While recent advancements in change captioning offer natural language descriptions of changes, they do not support interactive, user-specific queries. To address these limitations, we introduce ChangeChat, the first bitemporal vision-language model (VLM) designed specifically for RS change analysis. ChangeChat utilizes multimodal instruction tuning, allowing it to handle complex queries such as change captioning, category-specific quantification, and change localization. To enhance the model's performance, we developed the ChangeChat-87k dataset, which was generated using a combination of rule-based methods and GPT-assisted techniques. Experiments show that ChangeChat offers a comprehensive, interactive solution for RS change analysis, achieving performance comparable to or even better than state-of-the-art (SOTA) methods on specific tasks, and significantly surpassing the latest general-domain model, GPT-4. Code and pre-trained weights are available at https://github.com/hanlinwu/ChangeChat.",
        "authors": [
            "Pei Deng",
            "Wenqian Zhou",
            "Hanlin Wu"
        ],
        "citations": 2,
        "references": 20,
        "year": 2024
    },
    {
        "title": "Vision-language model-driven scene understanding and robotic object manipulation",
        "abstract": "Humans often use natural language instructions to control and interact with robots for task execution. This poses a big challenge to robots that need to not only parse and understand human instructions but also realise semantic understanding of an unknown environment and its constituent elements. To address this challenge, this study presents a vision-language model (VLM)-driven approach to scene understanding of an unknown environment to enable robotic object manipulation. Given language instructions, a pre-tained vision-language model built on open-sourced Llama2-chat (7B) as the language model backbone is adopted for image description and scene understanding, which translates visual information into text descriptions of the scene. Next, a zero-shot-based approach to fine-grained visual grounding and object detection is developed to extract and localise objects of interest from the scene task. Upon 3D reconstruction and pose estimate establishment of the object, a code-writing large language model (LLM) is adopted to generate high-level control codes and link language instructions with robot actions for downstream tasks. The performance of the developed approach is experimentally validated through table-top object manipulation by a robot.",
        "authors": [
            "Sichao Liu",
            "Jianjing Zhang",
            "Robert X. Gao",
            "X. Wang",
            "Lihui Wang"
        ],
        "citations": 2,
        "references": 31,
        "year": 2024
    },
    {
        "title": "Toward Open-Set Human Object Interaction Detection",
        "abstract": "This work is oriented toward the task of open-set Human Object Interaction (HOI) detection. The challenge lies in identifying completely new, out-of-domain relationships, as opposed to in-domain ones which have seen improvements in zero-shot HOI detection. To address this challenge, we introduce a simple Disentangled HOI Detection (DHD) model for detecting novel relationships by integrating an open-set object detector with a Visual Language Model (VLM). We utilize a disentangled image-text contrastive learning metric for training and connect the bottom-up visual features to text embeddings through lightweight unary and pair-wise adapters. Our model can benefit from the open-set object detector and the VLM to detect novel action categories and combine actions with novel object categories. We further present the VG-HOI dataset, a comprehensive benchmark with over 17k HOI relationships for open-set scenarios. Experimental results show that our model can detect unknown action classes and combine unknown object classes. Furthermore, it can generalize to over 17k HOI classes while being trained on just 600 HOI classes.",
        "authors": [
            "Ming-Kuan Wu",
            "Yuqi Liu",
            "Jiayi Ji",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "citations": 2,
        "references": 26,
        "year": 2024
    },
    {
        "title": "FlexCap: Generating Rich, Localized, and Flexible Captions in Images",
        "abstract": "We introduce a versatile $\\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications. First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQA datasets. We also demonstrate a $\\textit{localize-then-describe}$ approach with FlexCap can be better at open-ended object detection than a $\\textit{describe-then-localize}$ approach with other VLMs. We highlight a novel characteristic of FlexCap, which is its ability to extract diverse visual information through prefix conditioning. Finally, we qualitatively demonstrate FlexCap's broad applicability in tasks such as image labeling, object attribute recognition, and visual dialog. Project webpage: https://flex-cap.github.io .",
        "authors": [
            "Debidatta Dwibedi",
            "Vidhi Jain",
            "Jonathan Tompson",
            "A. Zisserman",
            "Y. Aytar"
        ],
        "citations": 2,
        "references": 53,
        "year": 2024
    },
    {
        "title": "HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision",
        "abstract": "Large Vision Language Models (VLMs) are now the de facto state-of-the-art for a number of tasks including visual question answering, recognising objects, and spatial referral. In this work, we propose the HOI-Ref task for egocentric images that aims to understand interactions between hands and objects using VLMs. To enable HOI-Ref, we curate the HOI-QA dataset that consists of 3.9M question-answer pairs for training and evaluating VLMs. HOI-QA includes questions relating to locating hands, objects, and critically their interactions (e.g. referring to the object being manipulated by the hand). We train the first VLM for HOI-Ref on this dataset and call it VLM4HOI. Our results demonstrate that VLMs trained for referral on third person images fail to recognise and refer hands and objects in egocentric images. When fine-tuned on our egocentric HOI-QA dataset, performance improves by 27.9% for referring hands and objects, and by 26.7% for referring interactions.",
        "authors": [
            "Siddhant Bansal",
            "Michael Wray",
            "D. Damen"
        ],
        "citations": 2,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Vertical Land Motion Due To Present‚ÄêDay Ice Loss From Greenland's and Canada's Peripheral Glaciers",
        "abstract": "Greenland's bedrock responds to ongoing ice loss with an elastic vertical land motion (VLM) that is measured by Greenland's Global Navigation Satellite System (GNSS) Network (GNET). The measured VLM also contains other contributions, including the long‚Äêterm viscoelastic response of the Earth to the deglaciation of the last glacial period. Greenland's ice sheet (GrIS) produces the most significant contribution to the total VLM. The contribution of peripheral glaciers (PGs) from both Greenland (GrPGs) and Arctic Canada (CanPGs) has not carefully been accounted for in previous GNSS analyses. This is a significant concern, since GNET stations are often closer to PGs than to the ice sheet. We find that, PGs produce significant elastic rebound, especially in North and East Greenland. Across these regions, the PGs produce up to 32% of the elastic rebound. For a few stations in the North, the VLM from PGs is larger than that due to the GrIS.",
        "authors": [
            "D. Berg",
            "V. Barletta",
            "J. Hassan",
            "E. Lippert",
            "W. Colgan",
            "M. Bevis",
            "R. Steffen",
            "S. A. Khan"
        ],
        "citations": 2,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Mammo-CLIP: A Vision Language Foundation Model to Enhance Data Efficiency and Robustness in Mammography",
        "abstract": "The lack of large and diverse training data on Computer-Aided Diagnosis (CAD) in breast cancer detection has been one of the concerns that impedes the adoption of the system. Recently, pre-training with large-scale image text datasets via Vision-Language models (VLM) (\\eg CLIP) partially addresses the issue of robustness and data efficiency in computer vision (CV). This paper proposes Mammo-CLIP, the first VLM pre-trained on a substantial amount of screening mammogram-report pairs, addressing the challenges of dataset diversity and size. Our experiments on two public datasets demonstrate strong performance in classifying and localizing various mammographic attributes crucial for breast cancer detection, showcasing data efficiency and robustness similar to CLIP in CV. We also propose Mammo-FActOR, a novel feature attribution method, to provide spatial interpretation of representation with sentence-level granularity within mammography reports. Code is available publicly: \\url{https://github.com/batmanlab/Mammo-CLIP}.",
        "authors": [
            "Shantanu Ghosh",
            "Clare B. Poynton",
            "Shyam Visweswaran",
            "K. Batmanghelich"
        ],
        "citations": 2,
        "references": 27,
        "year": 2024
    },
    {
        "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models",
        "abstract": "Despite recent advances demonstrating vision- language models‚Äô (VLMs) abilities to describe complex relationships among objects in images using natural language, their capability to quantitatively reason about object sizes and distances remains underexplored. In this work, we introduce a manually annotated benchmark of 241 questions across five categories specifically designed for quantitative spatial reasoning, and systematically investigate the performance of SoTA VLMs on this task. Our analysis reveals that questions involving reasoning about distances between objects are particularly challenging for SoTA VLMs; however, some VLMs perform significantly better at this task than others, with an almost 40 points gap between the two best performing models. We also make the surprising observation that the success rate of the top-performing VLM increases by 19 points when a reasoning path using a reference object emerges naturally in the response. Inspired by this observation, we develop a zero-shot prompting technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial questions using references objects as visual cues. Specifically, we demonstrate that instruct- ing VLMs to use reference objects in their reasoning paths significantly improves their quantitative spatial reasoning performance, bypassing the need for external data, architectural modifications, or fine-tuning. Remarkably, by solely using SpatialPrompt, Gemini 1.5 Pro, GPT-4V, and GPT-4o improve by 56.2, 28.5, and 6.7 points on average in Q-Spatial Bench without the need for more data, model architectural modifications, or fine-tuning.",
        "authors": [
            "Yuan-Hong Liao",
            "Rafid Mahmood",
            "Sanja Fidler",
            "David Acuna"
        ],
        "citations": 2,
        "references": 34,
        "year": 2024
    },
    {
        "title": "Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation",
        "abstract": "The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of large Vision-and-Language Models (VLMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to develop a VLM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. To this end, we developed a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. The dataset covers a range of tasks, from common ones like caption generation to specialized VQA tasks that require expert knowledge. Furthermore, using the dataset we created, we fine-tuned an existing VLM. This training enabled the models to generate questions and perform iterative reasoning during inference. The results demonstrated a stride toward a more robust, accurate, and interpretable VLM, capable of reasoning explicitly and seeking information proactively when confronted with ambiguous visual input.",
        "authors": [
            "Kohei Uehara",
            "Nabarun Goswami",
            "Hanqin Wang",
            "Toshiaki Baba",
            "Kohtaro Tanaka",
            "Tomohiro Hashimoto",
            "Kai Wang",
            "Rei Ito",
            "Takagi Naoya",
            "Ryo Umagami",
            "Yingyi Wen",
            "Tanachai Anakewat",
            "Tatsuya Harada"
        ],
        "citations": 2,
        "references": 114,
        "year": 2024
    },
    {
        "title": "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions",
        "abstract": "Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize textual features that are important for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate features that are important for the VLM. Then, we inspect the descriptions to identify features that contribute to VLM representations. Using EX2, we find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat (e.g., North America) to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.",
        "authors": [
            "Reza Esfandiarpoor",
            "Cristina Menghini",
            "Stephen H. Bach"
        ],
        "citations": 2,
        "references": 70,
        "year": 2024
    },
    {
        "title": "LLaVA-Ultra: Large Chinese Language and Vision Assistant for Ultrasound",
        "abstract": "Multimodal Large Language Model (MLLM) has recently garnered attention as a prominent research focus. By harnessing powerful LLM, it facilitates a transition of conversational generative AI from unimodal text to performing multimodal tasks. This boom begins to significantly impact medical field. However, general visual language model (VLM) lacks sophisticated comprehension for medical visual question answering (Med-VQA). Even models specifically tailored for medical domain tend to produce vague answers with weak visual relevance. In this paper, we propose a fine-grained adaptive VLM architecture for Chinese medical visual conversations through parameter-efficient tuning. Specifically, we devise a fusion module with fine-grained vision encoders to achieve enhancement for subtle medical visual semantics. Then we note data redundancy common to medical scenes is ignored in most prior works. In cases of a single text paired with multiple figures, we utilize weighted scoring with knowledge distillation to adaptively screen valid images mirroring text descriptions. For execution, we leverage a large-scale multimodal Chinese ultrasound dataset obtained from the hospital. We create instruction-following data based on text from professional doctors, which ensures effective tuning. With enhanced model and quality data, our Large Chinese Language and Vision Assistant for Ultrasound (LLaVA-Ultra) shows strong capability and robustness to medical scenarios. On three Med-VQA datasets, LLaVA-Ultra surpasses previous state-of-the-art models on various metrics.",
        "authors": [
            "Xuechen Guo",
            "Wenhao Chai",
            "Shiyan Li",
            "Gaoang Wang"
        ],
        "citations": 2,
        "references": 2,
        "year": 2024
    },
    {
        "title": "Interpretation and Attribution of Coastal Land Subsidence: An InSAR and Machine Learning Perspective",
        "abstract": "Subsidence, the downward vertical land motion (VLM), plays a pivotal role in contributing to the risk of coastal flooding. Accurately estimating VLM and identifying its potential features related to subsidence can provide crucial information for stakeholders to make better-informed decisions. This study aimed to estimate large-scale subsidence at the Texas Gulf Coast and identify potential subsidence features using explainable models. Nine potential features were considered for modeling the VLM, ranging from natural terrain variations to anthropogenic activities. These features were used to train a random forest (RF) machine learning model. Explainable artificial intelligence (XAI) techniques including SHapley Additive exPlanations (SHAP) and impurity- and permutation-based feature importance were used to identify the contributions to subsidence. The results demonstrated favorable performance of the RF model, achieving an $R^{2}$ value of 0.56 during validation. XAI results underscored the significance of the digital elevation model in explaining subsidence at the Texas Coast. Additionally, XAI analysis highlighted the overall contribution of subsidence from anthropogenic activities, such as hydrocarbon extraction and groundwater withdrawal. Furthermore, the sample-level SHAP map provided detailed and reasonable subsidence-attribution results across the study area, showing potential for automatic and data-driven explanations of the VLM.",
        "authors": [
            "Xiaojun Qiao",
            "T. Chu",
            "Evan Krell",
            "Philippe Tissot",
            "Seneca Holland",
            "Mohamed Ahmed",
            "Danielle Smilovsky"
        ],
        "citations": 2,
        "references": 75,
        "year": 2024
    },
    {
        "title": "PuzzleAvatar: Assembling 3D Avatars from Personal Albums",
        "abstract": "\n Generating\n personalized\n 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if users could just upload their personal \"OOTD\" (Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel \"\n Album2Human\n \" task by developing\n PuzzleAvatar\n , a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into separate learned tokens, instilling these cues into the VLM. In effect, we exploit the learned tokens as \"puzzle pieces\" from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we create a new dataset, called\n PuzzleIOI\n , with 41 subjects in a total of nearly 1k OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and demonstrating strong robustness. Our code and data are publicly available for research purpose at\n puzzleavatar.is.tue.mpg.de\n",
        "authors": [
            "Yuliang Xiu",
            "Yufei Ye",
            "Zhen Liu",
            "Dimitrios Tzionas",
            "Michael J. Black"
        ],
        "citations": 2,
        "references": 106,
        "year": 2024
    },
    {
        "title": "Are Bigger Encoders Always Better in Vision Large Models?",
        "abstract": "In recent years, multimodal large language models (MLLMs) have shown strong potential in real-world applications. They are developing rapidly due to their remarkable ability to comprehend multimodal information and their inherent powerful cognitive and reasoning capabilities. Among MLLMs, vision language models (VLM) stand out for their ability to understand vision information. However, the scaling trend of VLMs under the current mainstream paradigm has not been extensively studied. Whether we can achieve better performance by training even larger models is still unclear. To address this issue, we conducted experiments on the pretraining stage of MLLMs. We conduct our experiment using different encoder sizes and large language model (LLM) sizes. Our findings indicate that merely increasing the size of encoders does not necessarily enhance the performance of VLMs. Moreover, we analyzed the effects of LLM backbone parameter size and data quality on the pretraining outcomes. Additionally, we explored the differences in scaling laws between LLMs and VLMs.",
        "authors": [
            "Bozhou Li",
            "Hao Liang",
            "Zimo Meng",
            "Wentao Zhang"
        ],
        "citations": 2,
        "references": 44,
        "year": 2024
    },
    {
        "title": "Enhancing Presentation Slide Generation by LLMs with a Multi-Staged End-to-End Approach",
        "abstract": "Generating presentation slides from a long document with multimodal elements such as text and images is an important task. This is time consuming and needs domain expertise if done manually. Existing approaches for generating a rich presentation from a document are often semi-automatic or only put a flat summary into the slides ignoring the importance of a good narrative. In this paper, we address this research gap by proposing a multi-staged end-to-end model which uses a combination of LLM and VLM. We have experimentally shown that compared to applying LLMs directly with state-of-the-art prompting, our proposed multi-staged solution is better in terms of automated metrics and human evaluation.",
        "authors": [
            "Sambaran Bandyopadhyay",
            "Himanshu Maheshwari",
            "Anandhavelu Natarajan",
            "Apoorv Saxena"
        ],
        "citations": 2,
        "references": 30,
        "year": 2024
    },
    {
        "title": "Details Make a Difference: Object State-Sensitive Neurorobotic Task Planning",
        "abstract": "The state of an object reflects its current status or condition and is important for a robot's task planning and manipulation. However, detecting an object's state and generating a state-sensitive plan for robots is challenging. Recently, pre-trained Large Language Models (LLMs) and Vision-Language Models (VLMs) have shown impressive capabilities in generating plans. However, to the best of our knowledge, there is hardly any investigation on whether LLMs or VLMs can also generate object state-sensitive plans. To study this, we introduce an Object State-Sensitive Agent (OSSA), a task-planning agent empowered by pre-trained neural networks. We propose two methods for OSSA: (i) a modular model consisting of a pre-trained vision processing module (dense captioning model, DCM) and a natural language processing model (LLM), and (ii) a monolithic model consisting only of a VLM. To quantitatively evaluate the performances of the two methods, we use tabletop scenarios where the task is to clear the table. We contribute a multimodal benchmark dataset that takes object states into consideration. Our results show that both methods can be used for object state-sensitive tasks, but the monolithic approach outperforms the modular approach. The code for OSSA is available at https://github.com/Xiao-wen-Sun/OSSA",
        "authors": [
            "Xiaowen Sun",
            "Xufeng Zhao",
            "Jae Hee Lee",
            "Wenhao Lu",
            "Matthias Kerzel",
            "Stefan Wermter"
        ],
        "citations": 2,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image",
        "abstract": "Open-vocabulary 3D object detection (OV-3DDet) aims to localize and recognize both seen and previously unseen object categories within any new 3D scene. While language and vision foundation models have achieved success in handling various open-vocabulary tasks with abundant training data, OV-3DDet faces a significant challenge due to the limited availability of training data. Although some pioneering efforts have integrated vision-language models (VLM) knowledge into OV-3DDet learning, the full potential of these foundational models has yet to be fully exploited. In this paper, we unlock the textual and visual wisdom to tackle the open-vocabulary 3D detection task by leveraging the language and vision foundation models. We leverage a vision foundation model to provide image-wise guidance for discovering novel classes in 3D scenes. Specifically, we utilize a object detection vision foundation model to enable the zero-shot discovery of objects in images, which serves as the initial seeds and filtering guidance to identify novel 3D objects. Additionally, to align the 3D space with the powerful vision-language space, we introduce a hierarchical alignment approach, where the 3D feature space is aligned with the vision-language feature space using a pre-trained VLM at the instance, category, and scene levels. Through extensive experimentation, we demonstrate significant improvements in accuracy and generalization, highlighting the potential of foundation models in advancing open-vocabulary 3D object detection in real-world scenarios.",
        "authors": [
            "Pengkun Jiao",
            "Na Zhao",
            "Jingjing Chen",
            "Yugang Jiang"
        ],
        "citations": 2,
        "references": 31,
        "year": 2024
    },
    {
        "title": "KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data",
        "abstract": "Building generalist robotic systems involves effectively endowing robots with the capabilities to handle novel objects in an open-world setting. Inspired by the advances of large pre-trained models, we propose Keypoint Affordance Learning from Imagined Environments (KALIE), which adapts pre-trained Vision Language Models (VLMs) for robotic control in a scalable manner. Instead of directly producing motor commands, KALIE controls the robot by predicting point-based affordance representations based on natural language instructions and visual observations of the scene. The VLM is trained on 2D images with affordances labeled by humans, bypassing the need for training data collected on robotic systems. Through an affordance-aware data synthesis pipeline, KALIE automatically creates massive high-quality training data based on limited example data manually collected by humans. We demonstrate that KALIE can learn to robustly solve new manipulation tasks with unseen objects given only 50 example data points. Compared to baselines using pre-trained VLMs, our approach consistently achieves superior performance.",
        "authors": [
            "Grace Tang",
            "Swetha Rajkumar",
            "Yifei Zhou",
            "Homer Walke",
            "Sergey Levine",
            "Kuan Fang"
        ],
        "citations": 2,
        "references": 42,
        "year": 2024
    },
    {
        "title": "DST-Det: Open-Vocabulary Object Detection via Dynamic Self-Training",
        "abstract": "Open-vocabulary object detection (OVOD) aims to detect the objects beyond the set of classes observed during training. This work presents a simple yet effective strategy that leverages the zero-shot classification ability of pre-trained vision-language models (VLM), such as CLIP, to directly discover proposals of possible novel classes. Unlike previous works that ignore novel classes during training and rely solely on the region proposal network (RPN) for novel object detection, our method selectively filters proposals based on specific design criteria. The resulting sets of identified proposals serve as pseudo-labels of potential novel classes during the training phase. This self-training strategy improves the recall and accuracy of novel classes without requiring additional annotations or datasets. We further propose a simple offline pseudo-label generation strategy to refine the object detector. Empirical evaluations on three datasets, including LVIS, V3Det, and COCO, demonstrate significant improvements over the baseline performance without incurring additional parameters or computational costs during inference. In particular, compared with previous F-VLM, our method achieves a 1.7% improvement on the LVIS dataset. We also achieve over 6.5% improvement on the recent challenging V3Det dataset. When combined with the recent method CLIPSelf, our method also achieves 46.7 novel class AP on COCO without introducing extra data for pertaining.",
        "authors": [
            "Shilin Xu",
            "Xiangtai Li",
            "Size Wu",
            "Wenwei Zhang",
            "Yining Li",
            "Guangliang Cheng",
            "Yunhai Tong",
            "Chen Change Loy"
        ],
        "citations": 2,
        "references": 83,
        "year": 2024
    },
    {
        "title": "AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation",
        "abstract": "Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. While recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial reasoning and problem-solving abilities, they still struggle with failure recognition, limiting their real-world applicability. We introduce AHA, an open-source VLM designed to detect and reason about failures in robotic manipulation using natural language. By framing failure detection as a free-form reasoning task, AHA identifies failures and provides detailed, adaptable explanations across different robots, tasks, and environments. We fine-tuned AHA using FailGen, a scalable framework that generates the first large-scale dataset of robotic failure trajectories, the AHA dataset. FailGen achieves this by procedurally perturbing successful demonstrations from simulation. Despite being trained solely on the AHA dataset, AHA generalizes effectively to real-world failure datasets, robotic systems, and unseen tasks. It surpasses the second-best model (GPT-4o in-context learning) by 10.3% and exceeds the average performance of six compared models including five state-of-the-art VLMs by 35.3% across multiple metrics and datasets. We integrate AHA into three manipulation frameworks that utilize LLMs/VLMs for reinforcement learning, task and motion planning, and zero-shot trajectory generation. AHA's failure feedback enhances these policies' performances by refining dense reward functions, optimizing task planning, and improving sub-task verification, boosting task success rates by an average of 21.4% across all three tasks compared to GPT-4 models.",
        "authors": [
            "Jiafei Duan",
            "Wilbert Pumacay",
            "Nishanth Kumar",
            "Yi Ru Wang",
            "Shulin Tian",
            "Wentao Yuan",
            "Ranjay Krishna",
            "Dieter Fox",
            "Ajay Mandlekar",
            "Yijie Guo"
        ],
        "citations": 2,
        "references": 56,
        "year": 2024
    },
    {
        "title": "Navi2Gaze: Leveraging Foundation Models for Navigation and Target Gazing",
        "abstract": "Task-aware navigation continues to be a challenging area of research, especially in scenarios involving open vocabulary. Previous studies primarily focus on finding suitable locations for task completion, often overlooking the importance of the robot's pose. However, the robot's orientation is crucial for successfully completing tasks because of how objects are arranged (e.g., to open a refrigerator door). Humans intuitively navigate to objects with the right orientation using semantics and common sense. For instance, when opening a refrigerator, we naturally stand in front of it rather than to the side. Recent advances suggest that Vision-Language Models (VLMs) can provide robots with similar common sense. Therefore, we develop a VLM-driven method called Navigation-to-Gaze (Navi2Gaze) for efficient navigation and object gazing based on task descriptions. This method uses the VLM to score and select the best pose from numerous candidates automatically. In evaluations on multiple photorealistic simulation benchmarks, Navi2Gaze significantly outperforms existing approaches by precisely determining the optimal orientation relative to target objects, resulting in a 68.8% reduction in Distance to Goal (DTG). Real-world video demonstrations can be found on the supplementary website",
        "authors": [
            "Jun Zhu",
            "Zihao Du",
            "Haotian Xu",
            "Fengbo Lan",
            "Zilong Zheng",
            "Bo Ma",
            "Shengjie Wang",
            "Tao Zhang"
        ],
        "citations": 2,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Long-Term and Decadal Sea-Level Trends of the Baltic Sea Using Along-Track Satellite Altimetry",
        "abstract": "One of the main effects of climate change is rising sea levels, which presents challenges due to its geographically heterogenous nature. Often, contradictory results arise from examining different sources of measurement and time spans. This study addresses these issues by analysing both long-term (1995‚Äì2022) and decadal (2000‚Äì2009 and 2010‚Äì2019) sea-level trends in the Baltic Sea. Two independent sources of data, which consist of 13 tide gauge (TG) stations and multi-mission along-track satellite altimetry (SA), are utilized to calculate sea-level trends using the ordinary least-squares method. Given that the Baltic Sea is influenced by geographically varying vertical land motion (VLM), both relative sea level (RSL) and absolute sea level (ASL) trends were examined for the long-term assessment. The results for the long-term ASL show estimates for TG and SA to be 3.3 mm/yr and 3.9 mm/yr, respectively, indicating agreement between sources. Additionally, the comparison of long-term RSL ranges from ‚àí2 to 4.5 mm/yr, while ASL varies between 2 and 5.4 mm/yr, as expected due to the VLM. Spatial variation in long-term ASL trends is observed, with higher rates in the northern and eastern regions. Decadal sea-level trends show higher rates, particularly the decade 2000‚Äì2009. Comparison with other available sea-level datasets (gridded models) yields comparable results. Therefore, this study evaluates the ability of SA as a reliable source for determining reginal sea-level trends in comparison with TG data.",
        "authors": [
            "M. Mostafavi",
            "A. Ellmann",
            "N. Delpeche-Ellmann"
        ],
        "citations": 2,
        "references": 65,
        "year": 2024
    },
    {
        "title": "Towards Interpreting Visual Information Processing in Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) are powerful tools for processing and understanding text and images. We study the processing of visual tokens in the language model component of LLaVA, a prominent VLM. Our approach focuses on analyzing the localization of object information, the evolution of visual token representations across layers, and the mechanism of integrating visual information for predictions. Through ablation studies, we demonstrated that object identification accuracy drops by over 70\\% when object-specific tokens are removed. We observed that visual token representations become increasingly interpretable in the vocabulary space across layers, suggesting an alignment with textual tokens corresponding to image content. Finally, we found that the model extracts object information from these refined representations at the last token position for prediction, mirroring the process in text-only language models for factual association tasks. These findings provide crucial insights into how VLMs process and integrate visual information, bridging the gap between our understanding of language and vision models, and paving the way for more interpretable and controllable multimodal systems.",
        "authors": [
            "Clement Neo",
            "Luke Ong",
            "Philip Torr",
            "Mor Geva",
            "David Krueger",
            "Fazl Barez"
        ],
        "citations": 2,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Concept-based Analysis of Neural Networks via Vision-Language Models",
        "abstract": "The analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\\texttt{Con}_{\\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\\texttt{Con}_{\\texttt{spec}}$ specifications, we build a map between the internal representations of a given vision model and a VLM, leading to an efficient verification procedure of natural-language properties for vision models. We demonstrate our techniques on a ResNet-based classifier trained on the RIVAL-10 dataset using CLIP as the multimodal model.",
        "authors": [
            "Ravi Mangal",
            "Nina Narodytska",
            "Divya Gopinath",
            "Boyue Caroline Hu",
            "Anirban Roy",
            "Susmit Jha",
            "Corina S. Pasareanu"
        ],
        "citations": 2,
        "references": 38,
        "year": 2024
    },
    {
        "title": "RS-MoE: Mixture of Experts for Remote Sensing Image Captioning and Visual Question Answering",
        "abstract": "Remote Sensing Image Captioning (RSIC) presents unique challenges and plays a critical role in applications. Traditional RSIC methods often struggle to produce rich and diverse descriptions. Recently, with advancements in VLMs, efforts have emerged to integrate these models into the remote sensing domain and to introduce descriptive datasets specifically designed to enhance VLM training. This paper proposes RS-MoE, a first Mixture of Expert based VLM specifically customized for remote sensing domain. Unlike traditional MoE models, the core of RS-MoE is the MoE Block, which incorporates a novel Instruction Router and multiple lightweight Large Language Models (LLMs) as expert models. The Instruction Router is designed to generate specific prompts tailored for each corresponding LLM, guiding them to focus on distinct aspects of the RSIC task. This design not only allows each expert LLM to concentrate on a specific subset of the task, thereby enhancing the specificity and accuracy of the generated captions, but also improves the scalability of the model by facilitating parallel processing of sub-tasks. Additionally, we present a two-stage training strategy for tuning our RS-MoE model to prevent performance degradation due to sparsity. We fine-tuned our model on the RSICap dataset using our proposed training strategy. Experimental results on the RSICap dataset, along with evaluations on other traditional datasets where no additional fine-tuning was applied, demonstrate that our model achieves state-of-the-art performance in generating precise and contextually relevant captions. Notably, our RS-MoE-1B variant achieves performance comparable to 13B VLMs, demonstrating the efficiency of our model design. Moreover, our model demonstrates promising generalization capabilities by consistently achieving state-of-the-art performance on the Remote Sensing Visual Question Answering (RSVQA) task.",
        "authors": [
            "Hui Lin",
            "Danfeng Hong",
            "Shuhang Ge",
            "Chuyao Luo",
            "Kai Jiang",
            "Hao Jin",
            "Congcong Wen"
        ],
        "citations": 2,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Analyzing the Roles of Language and Vision in Learning from Limited Data",
        "abstract": "Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoning.",
        "authors": [
            "Allison Chen",
            "Ilia Sucholutsky",
            "Olga Russakovsky",
            "Thomas L. Griffiths"
        ],
        "citations": 2,
        "references": 46,
        "year": 2024
    },
    {
        "title": "AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation",
        "abstract": "Text-to-Image (T2I) diffusion models have achieved remarkable success in image generation. Despite their progress, challenges remain in both prompt-following ability, image quality and lack of high-quality datasets, which are essential for refining these models. As acquiring labeled data is costly, we introduce AGFSync, a framework that enhances T2I diffusion models through Direct Preference Optimization (DPO) in a fully AI-driven approach. AGFSync utilizes Vision-Language Models (VLM) to assess image quality across style, coherence, and aesthetics, generating feedback data within an AI-driven loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and SDXL-base, our extensive experiments on the TIFA dataset demonstrate notable improvements in VQA scores, aesthetic evaluations, and performance on the HPSv2 benchmark, consistently outperforming the base models. AGFSync's method of refining T2I diffusion models paves the way for scalable alignment techniques. Our code and dataset are publicly available at https://anjingkun.github.io/AGFSync.",
        "authors": [
            "Jingkun An",
            "Yinghao Zhu",
            "Zongjian Li",
            "Haoran Feng",
            "Bohua Chen",
            "Yemin Shi",
            "Chengwei Pan"
        ],
        "citations": 2,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Envisioning Medclip: A Deep Dive into Explainability for Medical Vision-Language Models",
        "abstract": "Explaining Deep Learning models is becoming increasingly important in the face of daily emerging multimodal models, particularly in safety-critical domains like medical imaging. However, the lack of detailed investigations into the performance of explainability methods on these models is widening the gap between their development and safe deployment. In this work, we analyze the performance of various explainable AI methods on a vision-language model, MedCLIP, to demystify its inner workings. We also provide a simple methodology to overcome the shortcomings of these methods. Our work offers a different new perspective on the explainability of a recent well-known VLM in the medical domain and our assessment method is generalizable to other current and possible future VLMs.",
        "authors": [
            "Anees Ur Rehman Hashmi",
            "Dwarikanath Mahapatra",
            "Mohammad Yaqub"
        ],
        "citations": 2,
        "references": 29,
        "year": 2024
    },
    {
        "title": "WATT: Weight Average Test-Time Adaptation of CLIP",
        "abstract": "Vision-Language Models (VLMs) such as CLIP have yielded unprecedented performance for zero-shot image classification, yet their generalization capability may still be seriously challenged when confronted to domain shifts. In response, we present Weight Average Test-Time Adaptation (WATT) of CLIP, a pioneering approach facilitating full test-time adaptation (TTA) of this VLM. Our method employs a diverse set of templates for text prompts, augmenting the existing framework of CLIP. Predictions are utilized as pseudo labels for model updates, followed by weight averaging to consolidate the learned information globally. Furthermore, we introduce a text ensemble strategy, enhancing overall test performance by aggregating diverse textual cues. Our findings underscore the efficacy of WATT in enhancing performance across diverse datasets, including CIFAR-10-C, CIFAR-10.1, CIFAR-100-C, VisDA-C, and several other challenging datasets, effectively covering a wide range of domain shifts. Notably, these enhancements are achieved without necessitating additional model transformations or trainable modules. Moreover, compared to other Test-Time Adaptation methods, our approach can operate effectively with just a single image. Highlighting the potential of innovative test-time strategies, this research emphasizes their role in fortifying the adaptability of VLMs. The implementation is available at: \\url{https://github.com/Mehrdad-Noori/WATT.git}.",
        "authors": [
            "David Osowiechi",
            "Mehrdad Noori",
            "G. A. V. Hakim",
            "Moslem Yazdanpanah",
            "Ali Bahri",
            "Milad Cheraghalikhani",
            "Sahar Dastani",
            "Farzad Beizaee",
            "Ismail Ben Ayed",
            "Christian Desrosiers"
        ],
        "citations": 2,
        "references": 28,
        "year": 2024
    },
    {
        "title": "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination",
        "abstract": "The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on average GSM8K: +3.1%; ASDIV: +3.2%; SVAMP: +6.9%) and the majority of the general-purpose reasoning tasks by 3.2% to 6.0% on average.",
        "authors": [
            "Syeda Nahida Akter",
            "Aman Madaan",
            "Sangwu Lee",
            "Yiming Yang",
            "Eric Nyberg"
        ],
        "citations": 2,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Test-Time Zero-Shot Temporal Action Localization",
        "abstract": "Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training. Existing ZS-TAL methods involve fine-tuning a model on a large amount of annotated training data. While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications. Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model's generalization ability to arbitrary videos. These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data. To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained Vision and Language Model (VLM). T3AL operates in three steps. First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video. Then, action localization is performed adopting a novel procedure inspired by self-supervised learning. Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals. We validate the effectiveness of T3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach.",
        "authors": [
            "Benedetta Liberatori",
            "Alessandro Conti",
            "P. Rota",
            "Yiming Wang",
            "Elisa Ricci"
        ],
        "citations": 2,
        "references": 34,
        "year": 2024
    },
    {
        "title": "Jailbreaking Text-to-Image Models with LLM-Based Agents",
        "abstract": "Recent advancements have significantly improved automated task-solving capabilities using autonomous agents powered by large language models (LLMs). However, most LLM-based agents focus on dialogue, programming, or specialized domains, leaving their potential for addressing generative AI safety tasks largely unexplored. In this paper, we propose Atlas, an advanced LLM-based multi-agent framework targeting generative AI models, specifically focusing on jailbreak attacks against text-to-image (T2I) models with built-in safety filters. Atlas consists of two agents, namely the mutation agent and the selection agent, each comprising four key modules: a vision-language model (VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses its VLM brain to determine whether a prompt triggers the T2I model's safety filter. It then collaborates iteratively with the LLM brain of the selection agent to generate new candidate jailbreak prompts with the highest potential to bypass the filter. In addition to multi-agent communication, we leverage in-context learning (ICL) memory mechanisms and the chain-of-thought (COT) approach to learn from past successes and failures, thereby enhancing Atlas's performance. Our evaluation demonstrates that Atlas successfully jailbreaks several state-of-the-art T2I models equipped with multi-modal safety filters in a black-box setting. Additionally, Atlas outperforms existing methods in both query efficiency and the quality of generated images. This work convincingly demonstrates the successful application of LLM-based agents in studying the safety vulnerabilities of popular text-to-image generation models. We urge the community to consider advanced techniques like ours in response to the rapidly evolving text-to-image generation field.",
        "authors": [
            "Yingkai Dong",
            "Zheng Li",
            "Xiangtao Meng",
            "Ning Yu",
            "Shanqing Guo"
        ],
        "citations": 2,
        "references": 77,
        "year": 2024
    },
    {
        "title": "VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions",
        "abstract": "Predicting future trajectories for other road agents is an essential task for autonomous vehicles. Established trajectory prediction methods primarily use agent tracks generated by a detection and tracking system and HD map as inputs. In this work, we propose a novel method that also incorporates visual input from surround-view cameras, allowing the model to utilize visual cues such as human gazes and gestures, road conditions, vehicle turn signals, etc, which are typically hidden from the model in prior methods. Furthermore, we use textual descriptions generated by a Vision-Language Model (VLM) and refined by a Large Language Model (LLM) as supervision during training to guide the model on what to learn from the input data. Despite using these extra inputs, our method achieves a latency of 53 ms, making it feasible for real-time processing, which is significantly faster than that of previous single-agent prediction methods with similar performance. Our experiments show that both the visual inputs and the textual descriptions contribute to improvements in trajectory prediction performance, and our qualitative analysis highlights how the model is able to exploit these additional inputs. Lastly, in this work we create and release the nuScenes-Text dataset, which augments the established nuScenes dataset with rich textual annotations for every scene, demonstrating the positive impact of utilizing VLM on trajectory prediction. Our project page is at https://moonseokha.github.io/VisionTrap/",
        "authors": [
            "Seokha Moon",
            "Hyun Woo",
            "Hongbeen Park",
            "Haeji Jung",
            "R. Mahjourian",
            "Hyung-Gun Chi",
            "Hyerin Lim",
            "Sangpil Kim",
            "Jinkyu Kim"
        ],
        "citations": 2,
        "references": 53,
        "year": 2024
    },
    {
        "title": "Decompose and Compare Consistency: Measuring VLMs‚Äô Answer Reliability via Task-Decomposition Consistency Comparison",
        "abstract": "Despite tremendous advancements, current state-of-the-art Vision-Language Models (VLMs) are still far from perfect. They tend to hallucinate and may generate biased responses. In such circumstances, having a way to assess the reliability of a given response generated by a VLM is quite useful. Existing methods, such as estimating uncertainty using answer likelihoods or prompt-based confidence generation, often suffer from overconfidence. Other methods use self-consistency comparison but are affected by confirmation biases. To alleviate these, we propose Decompose and Compare Consistency (DeCC) for reliability measurement. By comparing the consistency between the direct answer generated using the VLM‚Äôs internal reasoning process, and the indirect answers obtained by decomposing the question into sub-questions and reasoning over the sub-answers produced by the VLM, DeCC measures the reliability of VLM‚Äôs direct answer. Experiments across six vision-language tasks with three VLMs show DeCC‚Äôs reliability estimation achieves better correlation with task accuracy compared to the existing methods.",
        "authors": [
            "Qian Yang",
            "Weixiang Yan",
            "Aishwarya Agrawal"
        ],
        "citations": 2,
        "references": 33,
        "year": 2024
    },
    {
        "title": "NODE-Adapter: Neural Ordinary Differential Equations for Better Vision-Language Reasoning",
        "abstract": "In this paper, we consider the problem of prototype-based vision-language reasoning problem. We observe that existing methods encounter three major challenges: 1) escalating resource demands and prolonging training times, 2) contending with excessive learnable parameters, and 3) fine-tuning based only on a single modality. These challenges will hinder their capability to adapt Vision-Language Models (VLMs) to downstream tasks. Motivated by this critical observation, we propose a novel method called NODE-Adapter, which utilizes Neural Ordinary Differential Equations for better vision-language reasoning. To fully leverage both visual and textual modalities and estimate class prototypes more effectively and accurately, we divide our method into two stages: cross-modal prototype construction and cross-modal prototype optimization using neural ordinary differential equations. Specifically, we exploit VLM to encode hand-crafted prompts into textual features and few-shot support images into visual features. Then, we estimate the textual prototype and visual prototype by averaging the textual features and visual features, respectively, and adaptively combine the textual prototype and visual prototype to construct the cross-modal prototype. To alleviate the prototype bias, we then model the prototype optimization process as an initial value problem with Neural ODEs to estimate the continuous gradient flow. Our extensive experimental results, which cover few-shot classification, domain generalization, and visual reasoning on human-object interaction, demonstrate that the proposed method significantly outperforms existing state-of-the-art approaches.",
        "authors": [
            "Yi Zhang",
            "Chun-Wun Cheng",
            "Ke Yu",
            "Zhihai He",
            "C. Sch√∂nlieb",
            "Angelica I. Avil√©s-Rivero"
        ],
        "citations": 2,
        "references": 71,
        "year": 2024
    },
    {
        "title": "Fast and Lightweight Vision-Language Model for Adversarial Traffic Sign Detection",
        "abstract": "Several attacks have been proposed against autonomous vehicles and their subsystems that are powered by machine learning (ML). Road sign recognition models are especially heavily tested under various adversarial ML attack settings, and they have proven to be vulnerable. Despite the increasing research on adversarial ML attacks against road sign recognition models, there is little to no focus on defending against these attacks. In this paper, we propose the first defense method specifically designed for autonomous vehicles to detect adversarial ML attacks targeting road sign recognition models, which is called ViLAS (Vision-Language Model for Adversarial Traffic Sign Detection). The proposed defense method is based on a custom, fast, lightweight, and salable vision-language model (VLM) and is compatible with any existing traffic sign recognition system. Thanks to the orthogonal information coming from the class label text data through the language model, ViLAS leverages image context in addition to visual data for highly effective attack detection performance. In our extensive experiments, we show that our method consistently detects various attacks against different target models with high true positive rates while satisfying very low false positive rates. When tested against four state-of-the-art attacks targeting four popular action recognition models, our proposed detector achieves an average AUC of 0.94. This result achieves a 25.3% improvement over a state-of-the-art defense method proposed for generic image attack detection, which attains an average AUC of 0.75. We also show that our custom VLM is more suitable for an autonomous vehicle compared to the popular off-the-shelf VLM and CLIP in terms of speed (4.4 vs. 9.3 milliseconds), space complexity (0.36 vs. 1.6 GB), and performance (0.94 vs. 0.43 average AUC).",
        "authors": [
            "Furkan Mumcu",
            "Yasin Yƒ±lmaz"
        ],
        "citations": 2,
        "references": 15,
        "year": 2024
    },
    {
        "title": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor",
        "abstract": "Existing open-vocabulary image segmentation methods re-quire a fine-tuning step on mask labels and/or image-text datasets. Mask labels are labor-intensive, which limits the number of categories in segmentation datasets. Con-sequently, the vocabulary capacity of pre-trained VLMs is severely reduced after fine-tuning. However, without fine-tuning, VLMs trained under weak image-text supervision tend to make suboptimal mask predictions. To alleviate these issues, we introduce a novel recurrent framework that progressively filters out irrelevant texts and enhances mask quality without training efforts. The recurrent unit is a two-stage segmenter built upon a frozen VLM. Thus, our model retains the VLM's broad vocabulary space and equips it with segmentation ability. Experiments show that our method outperforms not only the training-free counter-parts, but also those fine-tuned with millions of data sam-ples, and sets the new state-of-the-art records for both zero-shot semantic and referring segmentation. Concretely, we improve the current record by 28.8, 16.0, and 6.9 mloU on Pascal VOC, COCO Object, and Pascal Context.",
        "authors": [
            "Shuyang Sun",
            "Runjia Li",
            "Philip Torr",
            "Xiuye Gu",
            "Siyang Li"
        ],
        "citations": 20,
        "references": 84,
        "year": 2023
    },
    {
        "title": "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models",
        "abstract": "We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a preexisting vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as \"ideal words\" for generating concepts directly within embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP‚Äôs embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic operations on embedding vectors can be used as compositional and interpretable methods for regulating the behavior of VLMs.",
        "authors": [
            "Matthew Trager",
            "Pramuditha Perera",
            "L. Zancato",
            "A. Achille",
            "Parminder Bhatia",
            "S. Soatto"
        ],
        "citations": 21,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Measuring Progress in Fine-grained Vision-and-Language Understanding",
        "abstract": "While pretraining on large-scale image‚Äìtext data from the Web has facilitated rapid progress on many vision-and-language (V&L) tasks, recent work has demonstrated that pretrained models lack ‚Äúfine-grained‚Äù understanding, such as the ability to recognise relationships, verbs, and numbers in images. This has resulted in an increased interest in the community to either develop new benchmarks or models for such capabilities. To better understand and quantify progress in this direction, we investigate four competitive V&L models on four fine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al., 2022) consistently outperforms other baselines, and that modelling innovations can impact performance more than scaling Web data, which even degrades performance sometimes. Through a deeper investigation of X-VLM, we highlight the importance of both novel losses and rich data sources for learning fine-grained skills. Finally, we inspect training dynamics, and discover that for some tasks, performance peaks early in training or significantly fluctuates, never converging.",
        "authors": [
            "Emanuele Bugliarello",
            "Laurent Sartran",
            "Aishwarya Agrawal",
            "Lisa Anne Hendricks",
            "Aida Nematzadeh"
        ],
        "citations": 21,
        "references": 70,
        "year": 2023
    },
    {
        "title": "Distilling Internet-Scale Vision-Language Models into Embodied Agents",
        "abstract": "Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.",
        "authors": [
            "T. Sumers",
            "Kenneth Marino",
            "Arun Ahuja",
            "R. Fergus",
            "Ishita Dasgupta"
        ],
        "citations": 20,
        "references": 81,
        "year": 2023
    },
    {
        "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
        "abstract": "While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often chal-lenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world. Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dy-namics. Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark's diverse tasks highlight EMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.",
        "authors": [
            "Yijun Yang",
            "Tianyi Zhou",
            "Kanxue Li",
            "Dapeng Tao",
            "Lusong Li",
            "Li Shen",
            "Xiaodong He",
            "Jing Jiang",
            "Yuhui Shi"
        ],
        "citations": 20,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
        "abstract": "Solving complex visual tasks such as ‚ÄúWho invented the musical instrument on the right?‚Äù involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incor-rect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) ca-pable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Exten-sive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data.",
        "authors": [
            "Yushi Hu",
            "Otilia Stretcu",
            "Chun-Ta Lu",
            "Krishnamurthy Viswanathan",
            "K. Hata",
            "Enming Luo",
            "Ranjay Krishna",
            "Ariel Fuxman"
        ],
        "citations": 17,
        "references": 67,
        "year": 2023
    },
    {
        "title": "ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation",
        "abstract": "Large-scale pre-trained vision-language models (VLM) such as CLIP [32] have demonstrated noteworthy zero-shot classification capability, achieving 76.3% top-1 accuracy on ImageNet without seeing any examples. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, a novel source-free domain adaptation method for VLMs, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels. Then, it deploys cross-modality self-training with the pseudo labels to update visual and text encoders, refine labels and reduce domain gaps and misalignment iteratively. With extensive experiments, we show that ReCLIP outperforms all the baselines significantly and improves the average accuracy of CLIP from 69.83% to 74.94% on 22 image classification benchmarks.",
        "authors": [
            "Xuefeng Hu",
            "Ke Zhang",
            "Lu Xia",
            "Albert Y. C. Chen",
            "Jiajia Luo",
            "Yuyin Sun",
            "Ke Min Wang",
            "Nan Qiao",
            "Xiao Zeng",
            "Min Sun",
            "Cheng-Hao Kuo",
            "R. Nevatia"
        ],
        "citations": 15,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory",
        "abstract": "Human Object Interaction (HOI) detection aims to localize and infer the relationships between a human and an object. Arguably, training supervised models for this task from scratch presents challenges due to the performance drop over rare classes and the high computational cost and time required to handle long-tailed distributions of HOIs in complex HOI scenes in realistic settings. This observation motivates us to design an HOI detector that can be trained even with long-tailed labeled data and can leverage existing knowledge from pre-trained models. Inspired by the powerful generalization ability of the large Vision-Language Models (VLM) on classification and retrieval tasks, we propose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM). ADA-CM has two operating modes. The first mode makes it tunable without learning new parameters in a training-free paradigm. Its second mode incorporates an instance-aware adapter mechanism that can further efficiently boost performance if updating a lightweight set of parameters can be afforded. Our proposed method achieves competitive results with state-of-the-art on the HICO-DET and V-COCO datasets with much less training time. Code can be found at https://github.com/ltttpku/ADA-CM.",
        "authors": [
            "Ting Lei",
            "Fabian Caba",
            "Qingchao Chen",
            "Hailin Jin",
            "Yuxin Peng",
            "Yang Liu"
        ],
        "citations": 14,
        "references": 53,
        "year": 2023
    },
    {
        "title": "MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices",
        "abstract": "We present MobileVLM, a competent multimodal vision language model (MMVLM) targeted to run on mobile devices. It is an amalgamation of a myriad of architectural designs and techniques that are mobile-oriented, which comprises a set of language models at the scale of 1.4B and 2.7B parameters, trained from scratch, a multimodal vision model that is pre-trained in the CLIP fashion, cross-modality interaction via an efficient projector. We evaluate MobileVLM on several typical VLM benchmarks. Our models demonstrate on par performance compared with a few much larger models. More importantly, we measure the inference speed on both a Qualcomm Snapdragon 888 CPU and an NVIDIA Jeston Orin GPU, and we obtain state-of-the-art performance of 21.5 tokens and 65.3 tokens per second, respectively. Our code will be made available at: https://github.com/Meituan-AutoML/MobileVLM.",
        "authors": [
            "Xiangxiang Chu",
            "Limeng Qiao",
            "Xinyang Lin",
            "Shuang Xu",
            "Yang Yang",
            "Yiming Hu",
            "Fei Wei",
            "Xinyu Zhang",
            "Bo Zhang",
            "Xiaolin Wei",
            "Chunhua Shen"
        ],
        "citations": 16,
        "references": 125,
        "year": 2023
    },
    {
        "title": "Rethinking Benchmarks for Cross-modal Image-text Retrieval",
        "abstract": "Image-text retrieval, as a fundamental and important branch of information retrieval, has attracted extensive research attentions. The main challenge of this task is cross-modal semantic understanding and matching. Some recent works focus more on fine-grained cross-modal semantic matching. With the prevalence of large scale multimodal pretraining models, several state-of-the-art models (e.g. X-VLM) have achieved near-perfect performance on widely-used image-text retrieval benchmarks, i.e. MSCOCO-Test-5K and Flickr30K-Test-1K. In this paper, we review the two common benchmarks and observe that they are insufficient to assess the true capability of models on fine-grained cross-modal semantic matching. The reason is that a large amount of images and texts in the benchmarks are coarse-grained. Based on the observation, we renovate the coarse-grained images and texts in the old benchmarks and establish the improved benchmarks called MSCOCO-FG and Flickr30K-FG. Specifically, on the image side, we enlarge the original image pool by adopting more similar images. On the text side, we propose a novel semi-automatic renovation approach to refine coarse-grained sentences into finer-grained ones with little human effort. Furthermore, we evaluate representative image-text retrieval models on our new benchmarks to demonstrate the effectiveness of our method. We also analyze the capability of models on fine-grained semantic comprehension through extensive experiments. The results show that even the state-of-the-art models have much room for improvement in fine-grained semantic understanding, especially in distinguishing attributes of close objects in images. Our code and improved benchmark datasets are publicly available1 which we hope will inspire further in-depth research on cross-modal retrieval.",
        "authors": [
            "Wei Chen",
            "Linli Yao",
            "Qin Jin"
        ],
        "citations": 16,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Hierarchical Prompt Learning for Multi-Task Learning",
        "abstract": "Vision-language models (VLMs) can effectively transfer to various vision tasks via prompt learning. Real-world scenarios often require adapting a model to multiple similar yet distinct tasks. Existing methods focus on learning a specific prompt for each task, limiting the ability to exploit potentially shared information from other tasks. Naively training a task-shared prompt using a combination of all tasks ignores fine-grained task correlations. Significant discrepancies across tasks could cause negative transferring. Considering this, we present Hierarchical Prompt (HiPro) learning, a simple and effective method for jointly adapting a pre-trained VLM to multiple downstream tasks. Our method quantifies inter-task affinity and subsequently constructs a hierarchical task tree. Task-shared prompts learned by internal nodes explore the information within the corresponding task group, while task-individual prompts learned by leaf nodes obtain fine-grained information targeted at each task. The combination of hierarchical prompts provides high-quality content of different granularity. We evaluate HiPro on four multi-task learning datasets. The results demonstrate the effectiveness of our method.",
        "authors": [
            "Yajing Liu",
            "Yuning Lu",
            "Hao Liu",
            "Yaozu An",
            "Zhuoran Xu",
            "Zhuokun Yao",
            "B. Zhang",
            "Zhiwei Xiong",
            "Chenguang Gui"
        ],
        "citations": 16,
        "references": 81,
        "year": 2023
    },
    {
        "title": "LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models",
        "abstract": "Recent advances in large vision-language models (VLMs) typically employ vision encoders based on the Vision Transformer (ViT) architecture. The division of the images into patches by ViT results in a fragmented perception, thereby hindering the visual understanding capabilities of VLMs. In this paper, we propose an innovative enhancement to address this limitation by introducing a Scene Graph Expression (SGE) module in VLMs. This module extracts and structurally expresses the complex semantic information within images, thereby improving the foundational perception and understanding abilities of VLMs. Extensive experiments demonstrate that integrating our SGE module significantly enhances the VLM's performance in vision-language tasks, indicating its effectiveness in preserving intricate semantic details and facilitating better visual understanding.",
        "authors": [
            "Jingyi Wang",
            "Jianzhong Ju",
            "Jian Luan",
            "Zhidong Deng"
        ],
        "citations": 1,
        "references": 29,
        "year": 2024
    },
    {
        "title": "Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps",
        "abstract": "Real-time detection and prediction of extreme weather protect human lives and infrastructure. Traditional methods rely on numerical threshold setting and manual interpretation of weather heatmaps with Geographic Information Systems (GIS), which can be slow and error-prone. Our research redefines Extreme Weather Events Detection (EWED) by framing it as a Visual Question Answering (VQA) problem, thereby introducing a more precise and automated solution. Leveraging Vision-Language Models (VLM) to simultaneously process visual and textual data, we offer an effective aid to enhance the analysis process of weather heatmaps. Our initial assessment of general-purpose VLMs (e.g., GPT-4-Vision) on EWED revealed poor performance, characterized by low accuracy and frequent hallucinations due to inadequate color differentiation and insufficient meteorological knowledge. To address these challenges, we introduce ClimateIQA, the first meteorological VQA dataset, which includes 8,760 wind gust heatmaps and 254,040 question-answer pairs covering four question types, both generated from the latest climate reanalysis data. We also propose Sparse Position and Outline Tracking (SPOT), an innovative technique that leverages OpenCV and K-Means clustering to capture and depict color contours in heatmaps, providing ClimateIQA with more accurate color spatial location information. Finally, we present Climate-Zoo, the first meteorological VLM collection, which adapts VLMs to meteorological applications using the ClimateIQA dataset. Experiment results demonstrate that models from Climate-Zoo substantially outperform state-of-the-art general VLMs, achieving an accuracy increase from 0% to over 90% in EWED verification. The datasets and models in this study are publicly available for future climate science research: https://github.com/AlexJJJChen/Climate-Zoo.",
        "authors": [
            "Jian Chen",
            "Peilin Zhou",
            "Y. Hua",
            "Dading Chong",
            "Meng Cao",
            "Yaowei Li",
            "Zixuan Yuan",
            "Bing Zhu",
            "Junwei Liang"
        ],
        "citations": 1,
        "references": 41,
        "year": 2024
    },
    {
        "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models",
        "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.",
        "authors": [
            "M. J. Mirza",
            "Mengjie Zhao",
            "Zhuoyuan Mao",
            "Sivan Doveh",
            "Wei Lin",
            "Paul Gavrikov",
            "Michael Dorkenwald",
            "Shiqi Yang",
            "Saurav Jha",
            "Hiromi Wakaki",
            "Yuki Mitsufuji",
            "Horst Possegger",
            "Rog√©rio Feris",
            "Leonid Karlinsky",
            "James Glass"
        ],
        "citations": 1,
        "references": 75,
        "year": 2024
    },
    {
        "title": "EvoChart: A Benchmark and a Self-Training Approach Towards Real-World Chart Understanding",
        "abstract": "Chart understanding enables automated data analysis for humans, which requires models to achieve highly accurate visual comprehension. While existing Visual Language Models (VLMs) have shown progress in chart understanding, the lack of high-quality training data and comprehensive evaluation benchmarks hinders VLM chart comprehension. In this paper, we introduce EvoChart, a novel self-training method for generating synthetic chart data to enhance VLMs' capabilities in real-world chart comprehension. We also propose EvoChart-QA, a noval benchmark for measuring models' chart comprehension abilities in real-world scenarios. Specifically, EvoChart is a unique self-training data synthesis approach that simultaneously produces high-quality training corpus and a high-performance chart understanding model. EvoChart-QA consists of 650 distinct real-world charts collected from 140 different websites and 1,250 expert-curated questions that focus on chart understanding. Experimental results on various open-source and proprietary VLMs tested on EvoChart-QA demonstrate that even the best proprietary model, GPT-4o, achieves only 49.8% accuracy. Moreover, the EvoChart method significantly boosts the performance of open-source VLMs on real-world chart understanding tasks, achieving 54.2% accuracy on EvoChart-QA.",
        "authors": [
            "Muye Huang",
            "Lai Han",
            "Xinyu Zhang",
            "Wenjun Wu",
            "Jie Ma",
            "Lingling Zhang",
            "Jun Liu"
        ],
        "citations": 1,
        "references": 36,
        "year": 2024
    },
    {
        "title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models",
        "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as ''safety alignment degradation'' in this paper, and show that the challenge arises from the representation gap that emerges when introducing vision modality to VLMs. In particular, we show that the representations of multi-modal inputs shift away from that of text-only inputs which represent the distribution that the LLM backbone is optimized for. At the same time, the safety alignment capabilities, initially developed within the textual embedding space, do not successfully transfer to this new multi-modal representation space. To reduce safety alignment degradation, we introduce Cross-Modality Representation Manipulation (CMRM), an inference time representation intervention method for recovering the safety alignment ability that is inherent in the LLM backbone of VLMs, while simultaneously preserving the functional capabilities of VLMs. The empirical results show that our framework significantly recovers the alignment ability that is inherited from the LLM backbone with minimal impact on the fluency and linguistic capabilities of pre-trained VLMs even without additional training. Specifically, the unsafe rate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as 3.15% with only inference-time intervention. WARNING: This paper contains examples of toxic or harmful language.",
        "authors": [
            "Qin Liu",
            "Chao Shang",
            "Ling Liu",
            "Nikolaos Pappas",
            "Jie Ma",
            "Neha Ann John",
            "Srikanth Doss",
            "Llu√≠s M√†rquez i Villodre",
            "Miguel Ballesteros",
            "Yassine Benajiba"
        ],
        "citations": 1,
        "references": 39,
        "year": 2024
    },
    {
        "title": "Visual Landmark Map-Based Spatial Recognition Using a Monocular Camera",
        "abstract": "In this paper, we propose a computer vision-based indoor spatial recognition method. The proposed method recognizes objects in a 360-degree space and eliminates errors in the process in which the space‚Äôs visual landmark map (VLM) is constructed by considering the 360-degree spatial arrangement and relative coordinates between the recognized objects. The method divides the target space into subspaces, and individual VLMs are applied to each subspace. The VLMs for these subspaces are activated based on their arrangement. When matching images from a camera with VLMs, the system identifies the user‚Äôs location within the space. An experiment demonstrated the effectiveness of the proposed method in accurately recognizing the user‚Äôs indoor position, and the data size for maintaining the VLM is highly efficient.",
        "authors": [
            "Haichuan Chen",
            "Gaoyang Shan",
            "B. Roh",
            "Sj Kim",
            "Junghyun Lim",
            "Geunkyung Choi"
        ],
        "citations": 1,
        "references": 12,
        "year": 2024
    },
    {
        "title": "Response Wide Shut: Surprising Observations in Basic Vision Language Model Capabilities",
        "abstract": "Vision-Language Models (VLMs) have emerged as general purpose tools for addressing a variety of complex computer vision problems. Such models have been shown to be highly capable, but, at the same time, also lacking some basic visual understanding skills. In this paper, we set out to understand the limitations of SoTA VLMs on fundamental visual tasks: object classification, understanding spatial arrangement, and ability to delineate individual object instances (through counting), by constructing a series of tests that probe which components of design, specifically, maybe lacking. Importantly, we go significantly beyond the current benchmarks, that simply measure final performance of VLM, by also comparing and contrasting it to performance of probes trained directly on features obtained from visual encoder (image embeddings), as well as intermediate vision-language projection used to bridge image-encoder and LLM-decoder ouput in many SoTA models (e.g., LLaVA, BLIP, InstructBLIP). In doing so, we uncover nascent shortcomings in VLMs response and make a number of important observations which could help train and develop more effective VLM models in future.",
        "authors": [
            "Shivam Chandhok",
            "Wan-Cyuan Fan",
            "Leonid Sigal"
        ],
        "citations": 1,
        "references": 19,
        "year": 2024
    },
    {
        "title": "SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining",
        "abstract": "Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accomplished by harnessing cross-attentional features obtained from a momentum model, ensuring a precise alignment between the two modalities. Additionally, we enhance grouped batch sampling with semi-hard negatives, effectively mitigating false negative issues in Image-Text Matching and Image-Text Contrastive learning objectives within fashion datasets. Our experiments demonstrate the effectiveness of the proposed approach, outper-forming existing methods in three downstream tasks.",
        "authors": [
            "Chull Hwan Song",
            "Taebaek Hwang",
            "Jooyoung Yoon",
            "Shunghyun Choi",
            "Yeong Hyeon Gu"
        ],
        "citations": 1,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Evaluating Semantic Relations in Predicting Textual Labels for Images of Abstract and Concrete Concepts",
        "abstract": "This study investigates the performance of SigLIP, a state-of-the-art Vision-Language Model (VLM), in predicting labels for images depicting 1,278 concepts. Our analysis across 300 images per concept shows that the model frequently predicts the exact user-tagged labels, but similarly, it often predicts labels that are semantically related to the exact labels in various ways: synonyms, hypernyms, co-hyponyms, and associated words, particularly for abstract concepts. We then zoom into the diversity of the user tags of images and word associations for abstract versus concrete concepts. Surprisingly, not only abstract but also concrete concepts exhibit significant variability, thus challenging the traditional view that representations of concrete concepts are less diverse.",
        "authors": [
            "Tarun Tater",
            "S. Schulte im Walde",
            "Diego Frassinelli"
        ],
        "citations": 1,
        "references": 22,
        "year": 2024
    },
    {
        "title": "On AI-Inspired UI-Design",
        "abstract": "Graphical User Interface (or simply UI) is a primary mean of interaction between users and their devices. In this paper, we discuss three complementary Artificial Intelligence (AI) approaches for triggering the creativity of app designers and inspiring them create better and more diverse UI designs. First, designers can prompt a Large Language Model (LLM) to directly generate and adjust UIs. Second, a Vision-Language Model (VLM) enables designers to effectively search a large screenshot dataset, e.g. from apps published in app stores. Third, a Diffusion Model (DM) can be trained to specifically generate UIs as inspirational images. We present an AI-inspired design process and discuss the implications and limitations of the approaches.",
        "authors": [
            "Jialiang Wei",
            "A. Courbis",
            "Thomas Lambolais",
            "G√©rard Dray",
            "Walid Maalej"
        ],
        "citations": 1,
        "references": 18,
        "year": 2024
    },
    {
        "title": "BendVLM: Test-Time Debiasing of Vision-Language Embeddings",
        "abstract": "Vision-language model (VLM) embeddings have been shown to encode biases present in their training data, such as societal biases that prescribe negative characteristics to members of various racial and gender identities. VLMs are being quickly adopted for a variety of tasks ranging from few-shot classification to text-guided image generation, making debiasing VLM embeddings crucial. Debiasing approaches that fine-tune the VLM often suffer from catastrophic forgetting. On the other hand, fine-tuning-free methods typically utilize a\"one-size-fits-all\"approach that assumes that correlation with the spurious attribute can be explained using a single linear direction across all possible inputs. In this work, we propose Bend-VLM, a nonlinear, fine-tuning-free approach for VLM embedding debiasing that tailors the debiasing operation to each unique input. This allows for a more flexible debiasing approach. Additionally, we do not require knowledge of the set of inputs a priori to inference time, making our method more appropriate for online, open-set tasks such as retrieval and text guided image generation.",
        "authors": [
            "Walter Gerych",
            "Haoran Zhang",
            "Kimia Hamidieh",
            "Eileen Pan",
            "Maanas Sharma",
            "Tom Hartvigsen",
            "Marzyeh Ghassemi"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "OLiVia-Nav: An Online Lifelong Vision Language Approach for Mobile Robot Social Navigation",
        "abstract": "Service robots in human-centered environments such as hospitals, office buildings, and long-term care homes need to navigate while adhering to social norms to ensure the safety and comfortability of the people they are sharing the space with. Furthermore, they need to adapt to new social scenarios that can arise during robot navigation. In this paper, we present a novel Online Lifelong Vision Language architecture, OLiVia-Nav, which uniquely integrates vision-language models (VLMs) with an online lifelong learning framework for robot social navigation. We introduce a unique distillation approach, Social Context Contrastive Language Image Pre-training (SC-CLIP), to transfer the social reasoning capabilities of large VLMs to a lightweight VLM, in order for OLiVia-Nav to directly encode social and environment context during robot navigation. These encoded embeddings are used to generate and select robot social compliant trajectories. The lifelong learning capabilities of SC-CLIP enable OLiVia-Nav to update the lightweight VLM with robot trajectory predictions overtime as new social scenarios are encountered. We conducted extensive real-world experiments in diverse social navigation scenarios. The results showed that OLiVia-Nav outperformed existing state-of-the-art DRL and VLM methods in terms of mean squared error, Hausdorff loss, and personal space violation duration. Ablation studies also verified the design choices for OLiVia-Nav.",
        "authors": [
            "Siddarth Narasimhan",
            "Aaron Hao Tan",
            "Daniel Choi",
            "G. Nejat"
        ],
        "citations": 1,
        "references": 46,
        "year": 2024
    },
    {
        "title": "How Well Can Vision Language Models See Image Details?",
        "abstract": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level remains unclear. In our study, we introduce a pixel value prediction task (PVP) to explore\"How Well Can Vision Language Models See Image Details?\"and to assist VLMs in perceiving more details. Typically, these models comprise a frozen CLIP visual encoder, a large language model, and a connecting module. After fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to predict precise pixel values by only fine-tuning the connection module and LLM; and 2) prediction precision is significantly improved when the vision encoder is also adapted. Additionally, our research reveals that incorporating pixel value prediction as one of the VLM pre-training tasks and vision encoder adaptation markedly boosts VLM performance on downstream image-language understanding tasks requiring detailed image perception, such as referring image segmentation (with an average +10.19 cIoU improvement) and video game decision making (with average score improvements of +80.34 and +70.54 on two games, respectively).",
        "authors": [
            "Chenhui Gou",
            "Abdulwahab Felemban",
            "Faizan Farooq Khan",
            "Deyao Zhu",
            "Jianfei Cai",
            "Hamid Rezatofighi",
            "Mohamed Elhoseiny"
        ],
        "citations": 1,
        "references": 60,
        "year": 2024
    },
    {
        "title": "GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling",
        "abstract": "Multimodal event argument role labeling (EARL), a task that assigns a role for each event participant (object) in an image is a complex challenge. It requires reasoning over the entire image, the depicted event, and the interactions between various objects participating in the event. Existing models heavily rely on high-quality event-annotated training data to understand the event semantics and structures, and they fail to generalize to new event types and domains. In this paper, we propose GenEARL, a training-free generative framework that harness the power of the modern generative models to understand event task descriptions given image contexts to perform the EARL task. Specifically, GenEARL comprises two stages of generative prompting with a frozen vision-language model (VLM) and a frozen large language model (LLM). First, a generative VLM learns the semantics of the event argument roles and generates event-centric object descriptions based on the image. Subsequently, a LLM is prompted with the generated object descriptions with a predefined template for EARL (i.e., assign an object with an event argument role). We show that GenEARL outperforms the contrastive pretraining (CLIP) baseline by 9.4% and 14.2% accuracy for zero-shot EARL on the M2E2 and SwiG datasets, respectively. In addition, we outperform CLIP-Event by 22% precision on M2E2 dataset. The framework also allows flexible adaptation and generalization to unseen domains.",
        "authors": [
            "Hritik Bansal",
            "Po-Nien Kung",
            "P. Brantingham",
            "Kai-Wei Chang",
            "Nanyun Peng",
            "Jean-Baptiste Alayrac",
            "Jeff Donahue",
            "Pauline Luc",
            "Antoine Miech",
            "Iain Barr",
            "Yana Hasson",
            "Karel Lenc",
            "A. Mensch",
            "Katie Millican",
            "Malcolm Reynolds",
            "Roman Ring",
            "Eliza Rutherford",
            "Serkan Cabi",
            "Tengda Han",
            "Zhitao Gong",
            "Sina Samangooei",
            "Marianne Monteiro",
            "Jacob Menick",
            "Sebastian Borgeaud",
            "Andy Brock",
            "Aida Nematzadeh",
            "Sahand Sharifzadeh",
            "M. Binkowski",
            "Ricardo Barreira",
            "O. Vinyals",
            "Andrew Zisserman",
            "Karen Simonyan. 2022",
            "Anas Awadalla",
            "Irena Gao",
            "Josh Gardner",
            "Jack Hes-sel",
            "Yusuf Hanafy",
            "Wanrong Zhu",
            "K. Marathe",
            "Yonatan Bitton",
            "S. Gadre",
            "J. Jitsev",
            "Pang Simon Kornblith",
            "Wei Koh",
            "Gabriel Ilharco",
            "Mitchell Wortsman",
            "Ludwig Schmidt. 2023",
            "Open-flamingo Hritik",
            "Da Bansal",
            "Masoud Yin",
            "Monajatipoor Kai-Wei",
            "Chang",
            "Philipp Blandfort",
            "D. U. Patton",
            "William R. Frey",
            "Svebor Karaman",
            "Surabhi Bhargava",
            "Tom Brown",
            "Benjamin Mann",
            "Nick Ryder",
            "Melanie Subbiah",
            "Jared Kaplan",
            "Prafulla Dhariwal",
            "Arvind Neelakantan",
            "Pranav Shyam",
            "Girish Sastry",
            "Wenliang Dai",
            "Junnan Li",
            "Dongxu Li",
            "Anthony Meng",
            "Huat Tiong",
            "Junqi Zhao",
            "Weisheng Wang",
            "Boyang Li",
            "Pascale Fung",
            "Steven Hoi. 2023",
            "George R Doddington",
            "Alexis Mitchell",
            "Mark A Przy-bocki",
            "lan A. Ramshaw",
            "Stephanie Strassel",
            "Ralph M Weischedel. 2004",
            "Salvatore Giorgi",
            "Sharath Chandra Guntuku",
            "McKenzie Himelein-Wachowiak",
            "Amy Kwarteng",
            "Sy Hwang",
            "Muhammad Rahman",
            "Brenda L. Curtis",
            "Tao Gong",
            "Chengqi Lyu",
            "Shilong Zhang",
            "Yudong Wang",
            "Miao Zheng",
            "Qian Zhao",
            "Kuikun Liu",
            "Wenwei Zhang",
            "Ping Luo",
            "Kai Chen",
            "Multimodal-gpt",
            "Jordan Hoffmann",
            "Arthur Men-sch",
            "Elena Buchatskaya",
            "Trevor Cai",
            "Eliza Ruther-ford",
            "Diego de",
            "Las Casas",
            "Lisa Anne Hendricks",
            "Johannes Welbl",
            "Aidan Clark",
            "I-Hung Hsu",
            "Kuan-Hao Huang",
            "Elizabeth Boschee",
            "Scott Miller",
            "Premkumar Natarajan",
            "Mike Lewis",
            "Yinhan Liu",
            "Naman Goyal",
            "Marjan Ghazvininejad",
            "Abdelrahman Mohamed",
            "Omer Levy",
            "Ves Stoyanov",
            "Luke Zettlemoyer. 2019",
            "Bart"
        ],
        "citations": 1,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Sensitivity of GNSS to vertical land motion over Europe: effects of geophysical loadings and common-mode errors",
        "abstract": null,
        "authors": [
            "Roland Hohensinn",
            "Pia Ruttner",
            "Yehuda Bock"
        ],
        "citations": 1,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Rugby Scene Classification Enhanced by Vision Language Model",
        "abstract": "This study investigates the integration of vision language models (VLM) to enhance the classification of situations within rugby match broadcasts. The importance of accurately identifying situations in sports videos is emphasized for understanding game dynamics and facilitating downstream tasks like performance evaluation and injury prevention. Utilizing a dataset comprising 18, 000 labeled images extracted at 0.2-second intervals from 100 minutes of rugby match broadcasts, scene classification tasks including contact plays (scrums, mauls, rucks, tackles, lineouts), rucks, tackles, lineouts, and multiclass classification were performed. The study aims to validate the utility of VLM outputs in improving classification performance compared to using solely image data. Experimental results demonstrate substantial performance improvements across all tasks with the incorporation of VLM outputs. Our analysis of prompts suggests that, when provided with appropriate contextual information through natural language, VLMs can effectively capture the context of a given image. The findings of our study indicate that leveraging VLMs in the domain of sports analysis holds promise for developing image processing models capable of incorpolating the tacit knowledge encoded within language models, as well as information conveyed through natural language descriptions.",
        "authors": [
            "Naoki Nonaka",
            "Ryo Fujihira",
            "Toshiki Koshiba",
            "Akira Maeda",
            "Jun Seita"
        ],
        "citations": 1,
        "references": 64,
        "year": 2024
    },
    {
        "title": "UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark",
        "abstract": "Localizing unusual activities, such as human errors or surveillance incidents, in videos holds practical significance. However, current video understanding models struggle with localizing these unusual events likely because of their insufficient representation in models' pretraining datasets. To explore foundation models' capability in localizing unusual activity, we introduce UAL-Bench, a comprehensive benchmark for unusual activity localization, featuring three video datasets: UAG-OOPS, UAG-SSBD, UAG-FunQA, and an instruction-tune dataset: OOPS-UAG-Instruct, to improve model capabilities. UAL-Bench evaluates three approaches: Video-Language Models (Vid-LLMs), instruction-tuned Vid-LLMs, and a novel integration of Vision-Language Models and Large Language Models (VLM-LLM). Our results show the VLM-LLM approach excels in localizing short-span unusual events and predicting their onset (start time) more accurately than Vid-LLMs. We also propose a new metric, R@1, TD<= p, to address limitations in existing evaluation methods. Our findings highlight the challenges posed by long-duration videos, particularly in autism diagnosis scenarios, and the need for further advancements in localization techniques. Our work not only provides a benchmark for unusual activity localization but also outlines the key challenges for existing foundation models, suggesting future research directions on this important task.",
        "authors": [
            "Hasnat Md. Abdullah",
            "Tian Liu",
            "Kangda Wei",
            "Shu Kong",
            "Ruihong Huang"
        ],
        "citations": 1,
        "references": 67,
        "year": 2024
    },
    {
        "title": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks",
        "abstract": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks, which are inference-time attacks that induce the model to output harmful responses with tricky prompts. It is thus essential to defend VLMs against potential jailbreaks for their trustworthy deployment in real-world applications. In this work, we focus on black-box defense for VLMs against jailbreak attacks. Existing black-box defense methods are either unimodal or bimodal. Unimodal methods enhance either the vision or language module of the VLM, while bimodal methods robustify the model through text-image representation realignment. However, these methods suffer from two limitations: 1) they fail to fully exploit the cross-modal information, or 2) they degrade the model performance on benign inputs. To address these limitations, we propose a novel blue-team method BlueSuffix that defends the black-box target VLM against jailbreak attacks without compromising its performance. BlueSuffix includes three key components: 1) a visual purifier against jailbreak images, 2) a textual purifier against jailbreak texts, and 3) a blue-team suffix generator fine-tuned via reinforcement learning for enhancing cross-modal robustness. We empirically show on three VLMs (LLaVA, MiniGPT-4, and Gemini) and two safety benchmarks (MM-SafetyBench and RedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant margin. Our BlueSuffix opens up a promising direction for defending VLMs against jailbreak attacks.",
        "authors": [
            "Yunhan Zhao",
            "Xiang Zheng",
            "Lin Luo",
            "Yige Li",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "citations": 1,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Leveraging Multi-Primary PS-InSAR Configurations for the Robust Estimation of Coastal Subsidence",
        "abstract": "Interferometric synthetic aperture radar (InSAR) is a key technique used to constrain contributions of diverse processes to coastal subsidence, also known as vertical land motion (VLM). However, coastal environments can pose major challenges for InSAR due to natural disturbances that degrade interferogram quality. We describe a new multi-primary pairing strategy for persistent scatterer InSAR (PS-InSAR) to estimate subsidence in challenging coastal environments. Our method retains only consistent PS candidates across multi-primary substacks and solves for redundant velocity observations using SVD-based inversion, similar to the conventional small baseline subset (SBAS) method. Through simulations and a case study comparing with single-primary PS-InSAR and conventional SBAS techniques, we show that our pairing strategy reduces temporal and spatial uncertainty in subsidence estimates in the presence of strong but temporary decorrelation loss, even with increased distance from the reference point. Moreover, our method visibly dampens time-series variation and decreases standard error in our time-series fit by nearly 2x in our case study. Thus, we find that implementing a multi-primary PS-InSAR configuration is a simple method of increasing the robustness of VLM estimates in challenging coastal environments.",
        "authors": [
            "Stacey A. Huang",
            "J. Sauber"
        ],
        "citations": 1,
        "references": 19,
        "year": 2024
    },
    {
        "title": "PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning",
        "abstract": "Medical vision-language models (Med-VLMs) trained on large datasets of medical image-text pairs and later fine-tuned for specific tasks have emerged as a mainstream paradigm in medical image analysis. However, recent studies have highlighted the susceptibility of these Med-VLMs to adversarial attacks, raising concerns about their safety and robustness. Randomized smoothing is a well-known technique for turning any classifier into a model that is certifiably robust to adversarial perturbations. However, this approach requires retraining the Med-VLM-based classifier so that it classifies well under Gaussian noise, which is often infeasible in practice. In this paper, we propose a novel framework called PromptSmooth to achieve efficient certified robustness of Med-VLMs by leveraging the concept of prompt learning. Given any pre-trained Med-VLM, PromptSmooth adapts it to handle Gaussian noise by learning textual prompts in a zero-shot or few-shot manner, achieving a delicate balance between accuracy and robustness, while minimizing the computational overhead. Moreover, PromptSmooth requires only a single model to handle multiple noise levels, which substantially reduces the computational cost compared to traditional methods that rely on training a separate model for each noise level. Comprehensive experiments based on three Med-VLMs and across six downstream datasets of various imaging modalities demonstrate the efficacy of PromptSmooth. Our code and models are available at https://github.com/nhussein/promptsmooth.",
        "authors": [
            "Noor Hussein",
            "Fahad Shamshad",
            "Muzammal Naseer",
            "Karthik Nandakumar"
        ],
        "citations": 1,
        "references": 31,
        "year": 2024
    },
    {
        "title": "Open-Vocabulary Action Localization with Iterative Visual Prompting",
        "abstract": "Video action localization aims to find the timings of specific actions from a long video. Although existing learning-based approaches have been successful, they require annotating videos, which comes with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLMs). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames and create a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start and end of the action. Iterating this process by narrowing a sampling time window results in finding the specific frames corresponding to the start and end of an action. We demonstrate that this technique yields reasonable performance, achieving results comparable to state-of-the-art zero-shot action localization. These results illustrate a practical extension of VLMs for understanding videos. A sample code is available at https://microsoft.github.io/VLM-Video-Action-Localization/.",
        "authors": [
            "Naoki Wake",
            "Atsushi Kanehira",
            "Kazuhiro Sasabuchi",
            "Jun Takamatsu",
            "Katsushi Ikeuchi"
        ],
        "citations": 1,
        "references": 61,
        "year": 2024
    },
    {
        "title": "DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation",
        "abstract": "Reliably detecting when a deployed machine learning model is likely to fail on a given input is crucial for ensuring safe operation. In this work, we propose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel approach that leverages priors from large language models (LLMs) and vision-language models (VLMs) to detect failures in image classification models. DECIDER utilizes LLMs to specify task-relevant core attributes and constructs a ``debiased'' version of the classifier by aligning its visual features to these core attributes using a VLM, and detects potential failure by measuring disagreement between the original and debiased models. In addition to proactively identifying samples on which the model would fail, DECIDER also provides human-interpretable explanations for failure through a novel attribute-ablation strategy. Through extensive experiments across diverse benchmarks spanning subpopulation shifts (spurious correlations, class imbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER consistently achieves state-of-the-art failure detection performance, significantly outperforming baselines in terms of the overall Matthews correlation coefficient as well as failure and success recall. Our codes can be accessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}",
        "authors": [
            "Rakshith Subramanyam",
            "Kowshik Thopalli",
            "V. Narayanaswamy",
            "J. Thiagarajan"
        ],
        "citations": 1,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models",
        "abstract": "In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction. Our code and data are available at www.github.com/JChiyah/blockworld-repairs",
        "authors": [
            "Javier Chiyah-Garcia",
            "Alessandro Suglia",
            "Arash Eshghi"
        ],
        "citations": 1,
        "references": 81,
        "year": 2024
    },
    {
        "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
        "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question answering, and cross-modal retrieval. Despite their superior capabilities, VLMs struggle with fine-grained image regional composition information perception. Specifically, they have difficulty accurately aligning the segmentation masks with the corresponding semantics and precisely describing the compositional aspects of the referred regions. However, compositionality - the ability to understand and generate novel combinations of known visual and textual components - is critical for facilitating coherent reasoning and understanding across modalities by VLMs. To address this issue, we propose FINECAPTION, a novel VLM that can recognize arbitrary masks as referential inputs and process high-resolution images for compositional image captioning at different granularity levels. To support this endeavor, we introduce COMPOSITIONCAP, a new dataset for multi-grained region compositional image captioning, which introduces the task of compositional attribute-aware regional image captioning. Empirical results demonstrate the effectiveness of our proposed model compared to other state-of-the-art VLMs. Additionally, we analyze the capabilities of current VLMs in recognizing various visual prompts for compositional region image captioning, highlighting areas for improvement in VLM design and training.",
        "authors": [
            "Hang Hua",
            "Qing Liu",
            "Lingzhi Zhang",
            "Jing Shi",
            "Zhifei Zhang",
            "Yilin Wang",
            "Jianming Zhang",
            "Jiebo Luo"
        ],
        "citations": 1,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Training a Vision Language Model as Smartphone Assistant",
        "abstract": "Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in large language models (LLMs) and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on vision-language sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild benchmark demonstrates its promising efficacy and potential.",
        "authors": [
            "Nicolai Dorka",
            "Janusz Marecki",
            "Ammar Anwar"
        ],
        "citations": 1,
        "references": 29,
        "year": 2024
    },
    {
        "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration",
        "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression methods are effective for Large Language Models (LLMs), directly migrating them to VLMs yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a novel KV cache compression recipe tailored for accelerating VLM inference. In this paper, we first investigate the unique sparsity pattern of VLM attention by distinguishing visual and text tokens in prefill and decoding phases. Based on these observations, we introduce a layer-adaptive sparsity-aware cache budget allocation method that effectively distributes the limited cache budget across different layers, further reducing KV cache size without compromising accuracy. Additionally, we develop a modality-aware token scoring policy to better evaluate the token importance. Empirical results on multiple benchmark datasets demonstrate that retaining only 10% of KV cache achieves accuracy comparable to that with full cache. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x, while reducing the memory footprint of KV cache in GPU by 90%.",
        "authors": [
            "Dezhan Tu",
            "Danylo Vashchilenko",
            "Yuzhe Lu",
            "Panpan Xu"
        ],
        "citations": 1,
        "references": 35,
        "year": 2024
    },
    {
        "title": "SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework with Interactive Vision-Language Models",
        "abstract": "With the continuous advancement of vision language models (VLMs) technology, remarkable research achievements have emerged in the dermatology field, the fourth most prevalent human disease category. However, despite these advancements, VLM still faces\"hallucination\"in dermatological diagnosis, and due to the inherent complexity of dermatological conditions, existing tools offer relatively limited support for user comprehension. We propose SkinGEN, a diagnosis-to-generation framework that leverages the stable diffusion (SD) method to generate reference demonstrations from diagnosis results provided by VLM, thereby enhancing the visual explainability for users. Through extensive experiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for skin condition image generation. We conduct a user study with 32 participants evaluating both the system performance and explainability. Results demonstrate that SkinGEN significantly improves users' comprehension of VLM predictions and fosters increased trust in the diagnostic process. This work paves the way for more transparent and user-centric VLM applications in dermatology and beyond.",
        "authors": [
            "Bo Lin",
            "Yingjing Xu",
            "Xuanwen Bao",
            "Zhou Zhao",
            "Zuyong Zhang",
            "Zhouyang Wang",
            "Jie Zhang",
            "Shuiguang Deng",
            "Jianwei Yin"
        ],
        "citations": 1,
        "references": 53,
        "year": 2024
    },
    {
        "title": "Intelligent smelting process, management system: Efficient and intelligent management strategy by incorporating large language model",
        "abstract": null,
        "authors": [
            "Tianjie Fu",
            "Shimin Liu",
            "Peiyu Li"
        ],
        "citations": 1,
        "references": 27,
        "year": 2024
    },
    {
        "title": "Visual Landmark Map-Based Spatial Recognition Using a Monocular Camera",
        "abstract": "In this paper, we propose a computer vision-based indoor spatial recognition method. The proposed method recognizes objects in a 360-degree space and eliminates errors in the process in which the space‚Äôs visual landmark map (VLM) is constructed by considering the 360-degree spatial arrangement and relative coordinates between the recognized objects. The method divides the target space into subspaces, and individual VLMs are applied to each subspace. The VLMs for these subspaces are activated based on their arrangement. When matching images from a camera with VLMs, the system identifies the user‚Äôs location within the space. An experiment demonstrated the effectiveness of the proposed method in accurately recognizing the user‚Äôs indoor position, and the data size for maintaining the VLM is highly efficient.",
        "authors": [
            "Haichuan Chen",
            "Gaoyang Shan",
            "B. Roh",
            "Sj Kim",
            "Junghyun Lim",
            "Geunkyung Choi"
        ],
        "citations": 1,
        "references": 12,
        "year": 2024
    },
    {
        "title": "Training a Vision Language Model as Smartphone Assistant",
        "abstract": "Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in large language models (LLMs) and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on vision-language sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild benchmark demonstrates its promising efficacy and potential.",
        "authors": [
            "Nicolai Dorka",
            "Janusz Marecki",
            "Ammar Anwar"
        ],
        "citations": 1,
        "references": 29,
        "year": 2024
    },
    {
        "title": "Vision-Language Models for Robot Success Detection",
        "abstract": "In this work, we use Vision-Language Models (VLMs) as a binary success detector given a robot observation and task description, formulated as a Visual Question Answering (VQA) problem. We fine-tune the open-source MiniGPT-4 VLM to detect success on robot trajectories from the Berkeley Bridge and Berkeley AUTOLab UR5 datasets. We find that while a handful of test distribution trajectories can train an accurate detector, transferring learning between different environments is challenging due to distribution shift. In addition, while our VLM is robust to language variations, it is less robust to visual variations. In the future, more powerful VLMs such as Gemini and GPT-4 have the potential to be more accurate and robust success detectors, and success detectors can provide a sparse binary reward to improve existing policies.",
        "authors": [
            "Fiona Luo"
        ],
        "citations": 1,
        "references": 7,
        "year": 2024
    },
    {
        "title": "How Well Can Vision Language Models See Image Details?",
        "abstract": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level remains unclear. In our study, we introduce a pixel value prediction task (PVP) to explore\"How Well Can Vision Language Models See Image Details?\"and to assist VLMs in perceiving more details. Typically, these models comprise a frozen CLIP visual encoder, a large language model, and a connecting module. After fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to predict precise pixel values by only fine-tuning the connection module and LLM; and 2) prediction precision is significantly improved when the vision encoder is also adapted. Additionally, our research reveals that incorporating pixel value prediction as one of the VLM pre-training tasks and vision encoder adaptation markedly boosts VLM performance on downstream image-language understanding tasks requiring detailed image perception, such as referring image segmentation (with an average +10.19 cIoU improvement) and video game decision making (with average score improvements of +80.34 and +70.54 on two games, respectively).",
        "authors": [
            "Chenhui Gou",
            "Abdulwahab Felemban",
            "Faizan Farooq Khan",
            "Deyao Zhu",
            "Jianfei Cai",
            "Hamid Rezatofighi",
            "Mohamed Elhoseiny"
        ],
        "citations": 1,
        "references": 60,
        "year": 2024
    },
    {
        "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
        "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.",
        "authors": [
            "Kevin Y. Li",
            "Sachin Goyal",
            "Jo√£o Dias Semedo",
            "J. Kolter"
        ],
        "citations": 1,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Visual Grounding for Object-Level Generalization in Reinforcement Learning",
        "abstract": "Generalization is a pivotal challenge for agents following natural language instructions. To approach this goal, we leverage a vision-language model (VLM) for visual grounding and transfer its vision-language knowledge into reinforcement learning (RL) for object-centric tasks, which makes the agent capable of zero-shot generalization to unseen objects and instructions. By visual grounding, we obtain an object-grounded confidence map for the target object indicated in the instruction. Based on this map, we introduce two routes to transfer VLM knowledge into RL. Firstly, we propose an object-grounded intrinsic reward function derived from the confidence map to more effectively guide the agent towards the target object. Secondly, the confidence map offers a more unified, accessible task representation for the agent's policy, compared to language embeddings. This enables the agent to process unseen objects and instructions through comprehensible visual confidence maps, facilitating zero-shot object-level generalization. Single-task experiments prove that our intrinsic reward significantly improves performance on challenging skill learning. In multi-task experiments, through testing on tasks beyond the training set, we show that the agent, when provided with the confidence map as the task representation, possesses better generalization capabilities than language-based conditioning. The code is available at https://github.com/PKU-RL/COPL.",
        "authors": [
            "Haobin Jiang",
            "Zongqing Lu"
        ],
        "citations": 1,
        "references": 68,
        "year": 2024
    },
    {
        "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models",
        "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.",
        "authors": [
            "M. J. Mirza",
            "Mengjie Zhao",
            "Zhuoyuan Mao",
            "Sivan Doveh",
            "Wei Lin",
            "Paul Gavrikov",
            "Michael Dorkenwald",
            "Shiqi Yang",
            "Saurav Jha",
            "Hiromi Wakaki",
            "Yuki Mitsufuji",
            "Horst Possegger",
            "Rog√©rio Feris",
            "Leonid Karlinsky",
            "James Glass"
        ],
        "citations": 1,
        "references": 75,
        "year": 2024
    },
    {
        "title": "Lesion of NPY Receptor-expressing Neurons in Perifornical Lateral Hypothalamus Attenuates Glucoprivic Feeding.",
        "abstract": "Glucoprivic feeding is one of several counter-regulatory responses (CRRs) that facilitates restoration of euglycemia following acute glucose deficit (glucoprivation). Our previous work established that glucoprivic feeding requires ventrolateral medullary (VLM) catecholamine (CA) neurons that coexpress neuropeptide Y (NPY). However, the connections by which VLM CA/NPY neurons trigger increased feeding are uncertain. We have previously shown that glucoprivation, induced by an anti-glycolygic agent 2-Deoxy-D-glucose (2DG), activates perifornical lateral hypothalamus (PeFLH) neurons and that expression of NPY in the VLM CA/NPY neurons is required for glucoprivic feeding. We therefore hypothesized that glucoprivic feeding and possibly other CRRs require NPY-sensitive PeFLH neurons. To test this, we used the ribosomal toxin conjugate, NPY-saporin (NPY-SAP), to selectively lesion NPY receptor-expressing neurons in the PeFLH of male rats. We found that NPY-SAP destroyed a significant number of PeFLH neurons, including those expressing orexin, but not those expressing melanin-concentrating hormone. The PeFLH NPY-SAP lesions attenuated 2DG-induced feeding but did not affect 2DG-induced increase in locomotor activity, sympathoadrenal hyperglycemia, or corticosterone release. The 2DG-induced feeding response was also significantly attenuated in NPY-SAP-treated female rats. Interestingly, PeFLH NPY-SAP lesioned male rats had reduced body weights and decreased dark cycle feeding, but this effect was not seen in female rats. We conclude that a NPY projection to the PeFLH is necessary for glucoprivic feeding, but not locomotor activity, hyperglycemia, or corticosterone release, in both male and female rats.",
        "authors": [
            "Pique P Choi",
            "Qing Wang",
            "L. Brenner",
            "Ai-Jun Li",
            "Robert C Ritter",
            "S. Appleyard"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "On AI-Inspired UI-Design",
        "abstract": "Graphical User Interface (or simply UI) is a primary mean of interaction between users and their devices. In this paper, we discuss three complementary Artificial Intelligence (AI) approaches for triggering the creativity of app designers and inspiring them create better and more diverse UI designs. First, designers can prompt a Large Language Model (LLM) to directly generate and adjust UIs. Second, a Vision-Language Model (VLM) enables designers to effectively search a large screenshot dataset, e.g. from apps published in app stores. Third, a Diffusion Model (DM) can be trained to specifically generate UIs as inspirational images. We present an AI-inspired design process and discuss the implications and limitations of the approaches.",
        "authors": [
            "Jialiang Wei",
            "A. Courbis",
            "Thomas Lambolais",
            "G√©rard Dray",
            "Walid Maalej"
        ],
        "citations": 1,
        "references": 18,
        "year": 2024
    },
    {
        "title": "Generating CAD Code with Vision-Language Models for 3D Designs",
        "abstract": "Generative AI has transformed the fields of Design and Manufacturing by providing efficient and automated methods for generating and modifying 3D objects. One approach involves using Large Language Models (LLMs) to generate Computer- Aided Design (CAD) scripting code, which can then be executed to render a 3D object; however, the resulting 3D object may not meet the specified requirements. Testing the correctness of CAD generated code is challenging due to the complexity and structure of 3D objects (e.g., shapes, surfaces, and dimensions) that are not feasible in code. In this paper, we introduce CADCodeVerify, a novel approach to iteratively verify and improve 3D objects generated from CAD code. Our approach works by producing ameliorative feedback by prompting a Vision-Language Model (VLM) to generate and answer a set of validation questions to verify the generated object and prompt the VLM to correct deviations. To evaluate CADCodeVerify, we introduce, CADPrompt, the first benchmark for CAD code generation, consisting of 200 natural language prompts paired with expert-annotated scripting code for 3D objects to benchmark progress. Our findings show that CADCodeVerify improves VLM performance by providing visual feedback, enhancing the structure of the 3D objects, and increasing the success rate of the compiled program. When applied to GPT-4, CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a 5.0% improvement in success rate compared to prior work",
        "authors": [
            "Kamel Alrashedy",
            "Pradyumna Tambwekar",
            "Z. Zaidi",
            "Megan Langwasser",
            "Wei Xu",
            "Matthew C. Gombolay"
        ],
        "citations": 1,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Evaluating Semantic Relations in Predicting Textual Labels for Images of Abstract and Concrete Concepts",
        "abstract": "This study investigates the performance of SigLIP, a state-of-the-art Vision-Language Model (VLM), in predicting labels for images depicting 1,278 concepts. Our analysis across 300 images per concept shows that the model frequently predicts the exact user-tagged labels, but similarly, it often predicts labels that are semantically related to the exact labels in various ways: synonyms, hypernyms, co-hyponyms, and associated words, particularly for abstract concepts. We then zoom into the diversity of the user tags of images and word associations for abstract versus concrete concepts. Surprisingly, not only abstract but also concrete concepts exhibit significant variability, thus challenging the traditional view that representations of concrete concepts are less diverse.",
        "authors": [
            "Tarun Tater",
            "S. Schulte im Walde",
            "Diego Frassinelli"
        ],
        "citations": 1,
        "references": 22,
        "year": 2024
    },
    {
        "title": "Integrating Vision‚ÄêLanguage Models for Accelerated High‚ÄêThroughput Nutrition Screening",
        "abstract": "Abstract Addressing the critical need for swift and precise nutritional profiling in healthcare and in food industry, this study pioneers the integration of vision‚Äêlanguage models (VLMs) with chemical analysis techniques. A cutting‚Äêedge VLM is unveiled, utilizing the expansive UMDFood‚Äê90k database, to significantly improve the speed and accuracy of nutrient estimation processes. Demonstrating a macro‚ÄêAUCROC of 0.921 for lipid quantification, the model exhibits less than 10% variance compared to traditional chemical analyses for over 82% of the analyzed food items. This innovative approach not only accelerates nutritional screening by 36.9% when tested amongst students but also sets a new benchmark in the precision of nutritional data compilation. This research marks a substantial leap forward in food science, employing a blend of advanced computational models and chemical validation to offer a rapid, high‚Äêthroughput solution for nutritional analysis.",
        "authors": [
            "Peihua Ma",
            "Yixin Wu",
            "Ning Yu",
            "Xiaoxue Jia",
            "Yiyang He",
            "Yang Zhang",
            "Michael Backes",
            "Qin Wang",
            "Cheng-I Wei"
        ],
        "citations": 1,
        "references": 34,
        "year": 2024
    },
    {
        "title": "Discovering Syntactic Interaction Clues for Human-Object Interaction Detection",
        "abstract": "Recently, Vision-Language Model (VLM) has greatly ad-vanced the Human-Object Interaction (HOI) detection. The existing VLM-based HOI detectors typically adopt a hand-crafted template (e.g., a photo of a person [action] a/an [object]) to acquire text knowledge through the VLM text encoder. However, such approaches, only encoding the action-specific text prompts in vocabulary level, may suffer from learning ambiguity without exploring the fine-grained clues from the perspective of interaction context. In this paper, we propose a novel method to discover Syntactic Interaction Clues for HOI detection (SICHOI) by using VLM. Specifically, we first investigate what are the essen-tial elements for an interaction context, and then establish a syntactic interaction bank from three levels: spatial relationship, action-oriented posture and situational condition. Further, to align visual features with the syntactic interaction bank, we adopt a multi-view extractor to jointly aggre-gate visual features from instance, interaction, and image levels accordingly. In addition, we also introduce a dual cross-attention decoder to perform context propagation be-tween text knowledge and visual features, thereby enhancing the HOI detection. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on HICO-DET and V-COCO.",
        "authors": [
            "Jinguo Luo",
            "Weihong Ren",
            "Weibo Jiang",
            "Xi‚Äôai Chen",
            "Qiang Wang",
            "Zhi Han",
            "Honghai Liu"
        ],
        "citations": 1,
        "references": 65,
        "year": 2024
    },
    {
        "title": "VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with Lightweight Blocks",
        "abstract": "Foundation Vision-Language Models (VLMs) trained using large-scale open-domain images and text pairs have recently been adapted to develop Vision-Language Segmentation Models (VLSMs) that allow providing text prompts during inference to guide image segmentation. If robust and powerful VLSMs can be built for medical images, it could aid medical professionals in many clinical tasks where they must spend substantial time delineating the target structure of interest. VLSMs for medical images resort to fine-tuning base VLM or VLSM pretrained on open-domain natural image datasets due to fewer annotated medical image datasets; this fine-tuning is resource-consuming and expensive as it usually requires updating all or a significant fraction of the pretrained parameters. Recently, lightweight blocks called adapters have been proposed in VLMs that keep the pretrained model frozen and only train adapters during fine-tuning, substantially reducing the computing resources required. We introduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained vision-language segmentation models using transformer encoders. Our experiments in widely used CLIP-based segmentation models show that with only 3 million trainable parameters, the VLSM-Adapter outperforms state-of-the-art and is comparable to the upper bound end-to-end fine-tuning. The source code is available at: https://github.com/naamiinepal/vlsm-adapter.",
        "authors": [
            "Manish Dhakal",
            "Rabin Adhikari",
            "Safal Thapaliya",
            "Bishesh Khanal"
        ],
        "citations": 1,
        "references": 32,
        "year": 2024
    },
    {
        "title": "LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models",
        "abstract": "Recent advances in large vision-language models (VLMs) typically employ vision encoders based on the Vision Transformer (ViT) architecture. The division of the images into patches by ViT results in a fragmented perception, thereby hindering the visual understanding capabilities of VLMs. In this paper, we propose an innovative enhancement to address this limitation by introducing a Scene Graph Expression (SGE) module in VLMs. This module extracts and structurally expresses the complex semantic information within images, thereby improving the foundational perception and understanding abilities of VLMs. Extensive experiments demonstrate that integrating our SGE module significantly enhances the VLM's performance in vision-language tasks, indicating its effectiveness in preserving intricate semantic details and facilitating better visual understanding.",
        "authors": [
            "Jingyi Wang",
            "Jianzhong Ju",
            "Jian Luan",
            "Zhidong Deng"
        ],
        "citations": 1,
        "references": 29,
        "year": 2024
    },
    {
        "title": "Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection",
        "abstract": "The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via input perturbations, our method can reprogram a pre-trained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. First, learnable visual perturbations are used to refine feature extraction for deepfake detection. Then, we exploit information of face embedding to create sample-level adaptative text prompts, improving the performance. Extensive experiments on several popular benchmark datasets demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88\\% AUC in cross-dataset setting from FF++ to WildDeepfake); (2) the superior performances are achieved with fewer trainable parameters, making it a promising approach for real-world applications.",
        "authors": [
            "Kaiqing Lin",
            "Yuzhen Lin",
            "Weixiang Li",
            "Taiping Yao",
            "Bin Li"
        ],
        "citations": 1,
        "references": 43,
        "year": 2024
    },
    {
        "title": "Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding",
        "abstract": "Deep multimodal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three experts of soft prompts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction. Additionally, we reorganize Transformer layers into several blocks and introduce cross-modal prompt attention between adjacent blocks, which smoothens the transition from single-modal representation to multi-modal fusion. On both MSD and MSA datasets in few-shot setting, our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M), but also significantly outperforms other widely-used prompt methods on VLMs or task-specific methods.",
        "authors": [
            "Zichen Wu",
            "Hsiu-Yuan Huang",
            "Fanyi Qu",
            "Yunfang Wu"
        ],
        "citations": 1,
        "references": 51,
        "year": 2024
    },
    {
        "title": "ATTIQA: Generalizable Image Quality Feature Extractor Using Attribute-Aware Pretraining",
        "abstract": null,
        "authors": [
            "Daekyu Kwon",
            "Dongyoung Kim",
            "Sehwan Ki",
            "Younghyun Jo",
            "Hyong-Euk Lee",
            "Seon Joo Kim"
        ],
        "citations": 1,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Leveraging Multi-Primary PS-InSAR Configurations for the Robust Estimation of Coastal Subsidence",
        "abstract": "Interferometric synthetic aperture radar (InSAR) is a key technique used to constrain contributions of diverse processes to coastal subsidence, also known as vertical land motion (VLM). However, coastal environments can pose major challenges for InSAR due to natural disturbances that degrade interferogram quality. We describe a new multi-primary pairing strategy for persistent scatterer InSAR (PS-InSAR) to estimate subsidence in challenging coastal environments. Our method retains only consistent PS candidates across multi-primary substacks and solves for redundant velocity observations using SVD-based inversion, similar to the conventional small baseline subset (SBAS) method. Through simulations and a case study comparing with single-primary PS-InSAR and conventional SBAS techniques, we show that our pairing strategy reduces temporal and spatial uncertainty in subsidence estimates in the presence of strong but temporary decorrelation loss, even with increased distance from the reference point. Moreover, our method visibly dampens time-series variation and decreases standard error in our time-series fit by nearly 2x in our case study. Thus, we find that implementing a multi-primary PS-InSAR configuration is a simple method of increasing the robustness of VLM estimates in challenging coastal environments.",
        "authors": [
            "Stacey A. Huang",
            "J. Sauber"
        ],
        "citations": 1,
        "references": 19,
        "year": 2024
    },
    {
        "title": "MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning",
        "abstract": "Multiple instance learning (MIL) has become a standard paradigm for weakly supervised classification of whole slide images (WSI). However, this paradigm relies on the use of a large number of labelled WSIs for training. The lack of training data and the presence of rare diseases present significant challenges for these methods. Prompt tuning combined with the pre-trained Vision-Language models (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI classification (FSWC) tasks. Nevertheless, applying prompt tuning methods designed for natural images to WSIs presents three significant challenges: 1) These methods fail to fully leverage the prior knowledge from the VLM's text modality; 2) They overlook the essential multi-scale and contextual information in WSIs, leading to suboptimal results; and 3) They lack exploration of instance aggregation methods. To address these problems, we propose a Multi-Scale and Context-focused Prompt Tuning (MSCPT) method for FSWC tasks. Specifically, MSCPT employs the frozen large language model to generate pathological visual language prior knowledge at multi-scale, guiding hierarchical prompt tuning. Additionally, we design a graph prompt tuning module to learn essential contextual information within WSI, and finally, a non-parametric cross-guided instance aggregation module has been introduced to get the WSI-level features. Based on two VLMs, extensive experiments and visualizations on three datasets demonstrated the powerful performance of our MSCPT.",
        "authors": [
            "Minghao Han",
            "Linhao Qu",
            "Dingkang Yang",
            "Xukun Zhang",
            "Xiaoying Wang",
            "Lihua Zhang"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Leveraging vision-language models for fair facial attribute classification",
        "abstract": "Performance disparities of image recognition across different demographic populations are known to exist in deep learning-based models, but previous work has largely addressed such fairness problems assuming knowledge of sensitive attribute labels. To overcome this reliance, previous strategies have involved separate learning structures to expose and adjust for disparities. In this work, we explore a new paradigm that does not require sensitive attribute labels, and evades the need for extra training by leveraging general-purpose vision-language model (VLM), as a rich knowledge source for common sensitive attributes. We analyze the correspondence between VLM predicted and human defined sensitive attribute distribution. We find that VLMs can recognize samples with clear attribute information encoded in image representations, thus capture under-performed samples conflicting with attribute-related bias. We train downstream target classifiers by re-sampling and augmenting under-performed attribute groups. Extensive experiments on multiple benchmark facial attribute classification datasets show fairness gains of the model over existing unsupervised baselines that tackle with arbitrary bias. The work indicates that vision-language models can extract discriminative sensitive information prompted by language, and be used to promote model fairness.",
        "authors": [
            "Miao Zhang",
            "R. Chunara"
        ],
        "citations": 1,
        "references": 61,
        "year": 2024
    },
    {
        "title": "UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark",
        "abstract": "Localizing unusual activities, such as human errors or surveillance incidents, in videos holds practical significance. However, current video understanding models struggle with localizing these unusual events likely because of their insufficient representation in models' pretraining datasets. To explore foundation models' capability in localizing unusual activity, we introduce UAL-Bench, a comprehensive benchmark for unusual activity localization, featuring three video datasets: UAG-OOPS, UAG-SSBD, UAG-FunQA, and an instruction-tune dataset: OOPS-UAG-Instruct, to improve model capabilities. UAL-Bench evaluates three approaches: Video-Language Models (Vid-LLMs), instruction-tuned Vid-LLMs, and a novel integration of Vision-Language Models and Large Language Models (VLM-LLM). Our results show the VLM-LLM approach excels in localizing short-span unusual events and predicting their onset (start time) more accurately than Vid-LLMs. We also propose a new metric, R@1, TD<= p, to address limitations in existing evaluation methods. Our findings highlight the challenges posed by long-duration videos, particularly in autism diagnosis scenarios, and the need for further advancements in localization techniques. Our work not only provides a benchmark for unusual activity localization but also outlines the key challenges for existing foundation models, suggesting future research directions on this important task.",
        "authors": [
            "Hasnat Md. Abdullah",
            "Tian Liu",
            "Kangda Wei",
            "Shu Kong",
            "Ruihong Huang"
        ],
        "citations": 1,
        "references": 67,
        "year": 2024
    },
    {
        "title": "DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation",
        "abstract": "Reliably detecting when a deployed machine learning model is likely to fail on a given input is crucial for ensuring safe operation. In this work, we propose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel approach that leverages priors from large language models (LLMs) and vision-language models (VLMs) to detect failures in image classification models. DECIDER utilizes LLMs to specify task-relevant core attributes and constructs a ``debiased'' version of the classifier by aligning its visual features to these core attributes using a VLM, and detects potential failure by measuring disagreement between the original and debiased models. In addition to proactively identifying samples on which the model would fail, DECIDER also provides human-interpretable explanations for failure through a novel attribute-ablation strategy. Through extensive experiments across diverse benchmarks spanning subpopulation shifts (spurious correlations, class imbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER consistently achieves state-of-the-art failure detection performance, significantly outperforming baselines in terms of the overall Matthews correlation coefficient as well as failure and success recall. Our codes can be accessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}",
        "authors": [
            "Rakshith Subramanyam",
            "Kowshik Thopalli",
            "V. Narayanaswamy",
            "J. Thiagarajan"
        ],
        "citations": 1,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models",
        "abstract": "In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction. Our code and data are available at www.github.com/JChiyah/blockworld-repairs",
        "authors": [
            "Javier Chiyah-Garcia",
            "Alessandro Suglia",
            "Arash Eshghi"
        ],
        "citations": 1,
        "references": 81,
        "year": 2024
    },
    {
        "title": "Sensitivity of GNSS to vertical land motion over Europe: effects of geophysical loadings and common-mode errors",
        "abstract": null,
        "authors": [
            "Roland Hohensinn",
            "Pia Ruttner",
            "Yehuda Bock"
        ],
        "citations": 1,
        "references": 42,
        "year": 2024
    },
    {
        "title": "SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model",
        "abstract": "The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks. However, there are few explorations for these foundation models used in quadruped robot navigation through terrains in 3D environments. In this work, we introduce SARO (Space Aware Robot System for Terrain Crossing), an innovative system composed of a high-level reasoning module, a closed-loop sub-task execution module, and a low-level control policy. It enables the robot to navigate across 3D terrains and reach the goal position. For high-level reasoning and execution, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to effectively train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across several 3D terrains, and its generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: https://saro-vlm.github.io/",
        "authors": [
            "Shaoting Zhu",
            "Derun Li",
            "Linzhan Mou",
            "Yong Liu",
            "Ningyi Xu",
            "Hang Zhao"
        ],
        "citations": 1,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Non‚ÄêLinear Vertical Land Motion of Coastal Chile and the Antarctic Peninsula Inferred From Combining Satellite Altimetry, Tide Gauge and GPS Data",
        "abstract": "We developed an enhanced Kalman‚Äêbased approach to quantify abrupt changes and significant non‚Äêlinearity in vertical land motion (VLM) along the coast of Chile and the Antarctic Peninsula using a combination of multi‚Äêmission satellite altimetry (ALT), tide gauge (TG), and GPS data starting from the early 1990s. The data reveal the spatial variability of co‚Äêseismic and post‚Äêseismic subsidence at TGs along the Chilean subduction zone in response to the Mw8.8 Maule 2010, Mw8.1 Iquique 2014, and Mw8.3 Illapel 2015 earthquakes that are not retrievable from the interpolation of sparse GPS observations across space and time. In the Antarctic Peninsula, where continuous GPS data do not commence until ‚àº1998, the approach provides new insight into the ‚àº2002 change in VLM at the TGs of +5.3 ¬± 2.2 mm/yr (Palmer) and +3.5 ¬± 2.8 mm/yr (Vernadsky) due to the onset of ice‚Äêmass loss following the Larsen‚ÄêB Ice Shelf breakup. We used these data to constrain viscoelastic Earth model parameters for the northern Antarctic Peninsula, obtaining a preferred lithosphere thickness of 115 km and upper mantle viscosity of 0.9 √ó 1018 Pa s. Our estimates of regionally‚Äêcorrelated ALT systematic errors are small, typically between ‚àº¬±0.5‚Äì2.5 mm/yr over single‚Äêmission time scales. These are consistent with competing orbit differences and the relative errors apparent in ALT crossovers. This study demonstrates that, with careful tuning, the ALT‚ÄêTG technique can provide improved temporal and spatial sampling of VLM, yielding new constraints on geodynamic models and assisting sea‚Äêlevel change studies in otherwise data sparse regions and periods.",
        "authors": [
            "Mohammad‚ÄêHadi Rezvani",
            "Christopher S. Watson",
            "Matt A. King"
        ],
        "citations": 1,
        "references": 130,
        "year": 2024
    },
    {
        "title": "ActPrompt: In-Domain Feature Adaptation via Action Cues for Video Temporal Grounding",
        "abstract": "Video temporal grounding is an emerging topic aiming to identify specific clips within videos. In addition to pre-trained video models, contemporary methods utilize pre-trained vision-language models (VLM) to capture detailed characteristics of diverse scenes and objects from video frames. However, as pre-trained on images, VLM may struggle to distinguish action-sensitive patterns from static objects, making it necessary to adapt them to specific data domains for effective feature representation over temporal grounding. We address two primary challenges to achieve this goal. Specifically, to mitigate high adaptation costs, we propose an efficient preliminary in-domain fine-tuning paradigm for feature adaptation, where downstream-adaptive features are learned through several pretext tasks. Furthermore, to integrate action-sensitive information into VLM, we introduce Action-Cue-Injected Temporal Prompt Learning (ActPrompt), which injects action cues into the image encoder of VLM for better discovering action-sensitive patterns. Extensive experiments demonstrate that ActPrompt is an off-the-shelf training framework that can be effectively applied to various SOTA methods, resulting in notable improvements. The complete code used in this study is provided in the supplementary materials.",
        "authors": [
            "Yubin Wang",
            "Xinyang Jiang",
            "De Cheng",
            "Dongsheng Li",
            "Cairong Zhao"
        ],
        "citations": 1,
        "references": 40,
        "year": 2024
    },
    {
        "title": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight",
        "abstract": "Video anomaly detection (VAD) has witnessed significant advancements through the integration of large language models (LLMs) and vision-language models (VLMs), addressing critical challenges such as interpretability, temporal reasoning, and generalization in dynamic, open-world scenarios. This paper presents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024, focusing on four key aspects: (i) enhancing interpretability through semantic insights and textual explanations, making visual anomalies more understandable; (ii) capturing intricate temporal relationships to detect and localize dynamic anomalies across video frames; (iii) enabling few-shot and zero-shot detection to minimize reliance on large, annotated datasets; and (iv) addressing open-world and class-agnostic anomalies by using semantic understanding and motion features for spatiotemporal coherence. We highlight their potential to redefine the landscape of VAD. Additionally, we explore the synergy between visual and textual modalities offered by LLMs and VLMs, highlighting their combined strengths and proposing future directions to fully exploit the potential in enhancing video anomaly detection.",
        "authors": [
            "Xi Ding",
            "Lei Wang"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Exploring the Spectrum of Visio-Linguistic Compositionality and Recognition",
        "abstract": "Vision and language models (VLMs) such as CLIP have showcased remarkable zero-shot recognition abilities yet face challenges in visio-linguistic compositionality, particularly in linguistic comprehension and fine-grained image-text alignment. This paper explores the intricate relationship between compositionality and recognition -- two pivotal aspects of VLM capability. We conduct a comprehensive evaluation of existing VLMs, covering both pre-training approaches aimed at recognition and the fine-tuning methods designed to improve compositionality. Our evaluation employs 12 benchmarks for compositionality, along with 21 zero-shot classification and two retrieval benchmarks for recognition. In our analysis from 274 CLIP model checkpoints, we reveal patterns and trade-offs that emerge between compositional understanding and recognition accuracy. Ultimately, this necessitates strategic efforts towards developing models that improve both capabilities, as well as the meticulous formulation of benchmarks for compositionality. We open our evaluation framework at https://github.com/ytaek-oh/vl_compo.",
        "authors": [
            "Youngtaek Oh",
            "Pyunghwan Ahn",
            "Jinhyung Kim",
            "Gwangmo Song",
            "Soonyoung Lee",
            "I. Kweon",
            "Junmo Kim"
        ],
        "citations": 1,
        "references": 42,
        "year": 2024
    },
    {
        "title": "LIT: Large Language Model Driven Intention Tracking for Proactive Human-Robot Collaboration - A Robot Sous-Chef Application",
        "abstract": "Large Language Models (LLM) and Vision Language Models (VLM) enable robots to ground natural language prompts into control actions to achieve tasks in an open world. However, when applied to a long-horizon collaborative task, this formulation results in excessive prompting for initiating or clarifying robot actions at every step of the task. We propose Language-driven Intention Tracking (LIT), leveraging LLMs and VLMs to model the human user's long-term behavior and to predict the next human intention to guide the robot for proactive collaboration. We demonstrate smooth coordination between a LIT-based collaborative robot and the human user in collaborative cooking tasks.",
        "authors": [
            "Zhe Huang",
            "John Pohovey",
            "Ananya Yammanuru",
            "K. Driggs-Campbell"
        ],
        "citations": 1,
        "references": 39,
        "year": 2024
    },
    {
        "title": "GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling",
        "abstract": "Multimodal event argument role labeling (EARL), a task that assigns a role for each event participant (object) in an image is a complex challenge. It requires reasoning over the entire image, the depicted event, and the interactions between various objects participating in the event. Existing models heavily rely on high-quality event-annotated training data to understand the event semantics and structures, and they fail to generalize to new event types and domains. In this paper, we propose GenEARL, a training-free generative framework that harness the power of the modern generative models to understand event task descriptions given image contexts to perform the EARL task. Specifically, GenEARL comprises two stages of generative prompting with a frozen vision-language model (VLM) and a frozen large language model (LLM). First, a generative VLM learns the semantics of the event argument roles and generates event-centric object descriptions based on the image. Subsequently, a LLM is prompted with the generated object descriptions with a predefined template for EARL (i.e., assign an object with an event argument role). We show that GenEARL outperforms the contrastive pretraining (CLIP) baseline by 9.4% and 14.2% accuracy for zero-shot EARL on the M2E2 and SwiG datasets, respectively. In addition, we outperform CLIP-Event by 22% precision on M2E2 dataset. The framework also allows flexible adaptation and generalization to unseen domains.",
        "authors": [
            "Hritik Bansal",
            "Po-Nien Kung",
            "P. Brantingham",
            "Kai-Wei Chang",
            "Nanyun Peng",
            "Jean-Baptiste Alayrac",
            "Jeff Donahue",
            "Pauline Luc",
            "Antoine Miech",
            "Iain Barr",
            "Yana Hasson",
            "Karel Lenc",
            "A. Mensch",
            "Katie Millican",
            "Malcolm Reynolds",
            "Roman Ring",
            "Eliza Rutherford",
            "Serkan Cabi",
            "Tengda Han",
            "Zhitao Gong",
            "Sina Samangooei",
            "Marianne Monteiro",
            "Jacob Menick",
            "Sebastian Borgeaud",
            "Andy Brock",
            "Aida Nematzadeh",
            "Sahand Sharifzadeh",
            "M. Binkowski",
            "Ricardo Barreira",
            "O. Vinyals",
            "Andrew Zisserman",
            "Karen Simonyan. 2022",
            "Anas Awadalla",
            "Irena Gao",
            "Josh Gardner",
            "Jack Hes-sel",
            "Yusuf Hanafy",
            "Wanrong Zhu",
            "K. Marathe",
            "Yonatan Bitton",
            "S. Gadre",
            "J. Jitsev",
            "Pang Simon Kornblith",
            "Wei Koh",
            "Gabriel Ilharco",
            "Mitchell Wortsman",
            "Ludwig Schmidt. 2023",
            "Open-flamingo Hritik",
            "Da Bansal",
            "Masoud Yin",
            "Monajatipoor Kai-Wei",
            "Chang",
            "Philipp Blandfort",
            "D. U. Patton",
            "William R. Frey",
            "Svebor Karaman",
            "Surabhi Bhargava",
            "Tom Brown",
            "Benjamin Mann",
            "Nick Ryder",
            "Melanie Subbiah",
            "Jared Kaplan",
            "Prafulla Dhariwal",
            "Arvind Neelakantan",
            "Pranav Shyam",
            "Girish Sastry",
            "Wenliang Dai",
            "Junnan Li",
            "Dongxu Li",
            "Anthony Meng",
            "Huat Tiong",
            "Junqi Zhao",
            "Weisheng Wang",
            "Boyang Li",
            "Pascale Fung",
            "Steven Hoi. 2023",
            "George R Doddington",
            "Alexis Mitchell",
            "Mark A Przy-bocki",
            "lan A. Ramshaw",
            "Stephanie Strassel",
            "Ralph M Weischedel. 2004",
            "Salvatore Giorgi",
            "Sharath Chandra Guntuku",
            "McKenzie Himelein-Wachowiak",
            "Amy Kwarteng",
            "Sy Hwang",
            "Muhammad Rahman",
            "Brenda L. Curtis",
            "Tao Gong",
            "Chengqi Lyu",
            "Shilong Zhang",
            "Yudong Wang",
            "Miao Zheng",
            "Qian Zhao",
            "Kuikun Liu",
            "Wenwei Zhang",
            "Ping Luo",
            "Kai Chen",
            "Multimodal-gpt",
            "Jordan Hoffmann",
            "Arthur Men-sch",
            "Elena Buchatskaya",
            "Trevor Cai",
            "Eliza Ruther-ford",
            "Diego de",
            "Las Casas",
            "Lisa Anne Hendricks",
            "Johannes Welbl",
            "Aidan Clark",
            "I-Hung Hsu",
            "Kuan-Hao Huang",
            "Elizabeth Boschee",
            "Scott Miller",
            "Premkumar Natarajan",
            "Mike Lewis",
            "Yinhan Liu",
            "Naman Goyal",
            "Marjan Ghazvininejad",
            "Abdelrahman Mohamed",
            "Omer Levy",
            "Ves Stoyanov",
            "Luke Zettlemoyer. 2019",
            "Bart"
        ],
        "citations": 1,
        "references": 45,
        "year": 2024
    },
    {
        "title": "A top-down slow breathing circuit that alleviates negative affect in mice.",
        "abstract": null,
        "authors": [
            "Jinho Jhang",
            "Seahyung Park",
            "Shijia Liu",
            "David D O'Keefe",
            "Sung Han"
        ],
        "citations": 1,
        "references": 46,
        "year": 2024
    },
    {
        "title": "OVExp: Open Vocabulary Exploration for Object-Oriented Navigation",
        "abstract": "Object-oriented embodied navigation aims to locate specific objects, defined by category or depicted in images. Existing methods often struggle to generalize to open vocabulary goals without extensive training data. While recent advances in Vision-Language Models (VLMs) offer a promising solution by extending object recognition beyond predefined categories, efficient goal-oriented exploration becomes more challenging in an open vocabulary setting. We introduce OVExp, a learning-based framework that integrates VLMs for Open-Vocabulary Exploration. OVExp constructs scene representations by encoding observations with VLMs and projecting them onto top-down maps for goal-conditioned exploration. Goals are encoded in the same VLM feature space, and a lightweight transformer-based decoder predicts target locations while maintaining versatile representation abilities. To address the impracticality of fusing dense pixel embeddings with full 3D scene reconstruction for training, we propose constructing maps using low-cost semantic categories and transforming them into CLIP's embedding space via the text encoder. The simple but effective design of OVExp significantly reduces computational costs and demonstrates strong generalization abilities to various navigation settings. Experiments on established benchmarks show OVExp outperforms previous zero-shot methods, can generalize to diverse scenes, and handle different goal modalities.",
        "authors": [
            "Meng Wei",
            "Tai Wang",
            "Yilun Chen",
            "Hanqing Wang",
            "Jiangmiao Pang",
            "Xihui Liu"
        ],
        "citations": 1,
        "references": 33,
        "year": 2024
    },
    {
        "title": "Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models",
        "abstract": "We introduce Dream2Real, a robotics framework which integrates vision-language models (VLMs) trained on 2D data into a 3D object rearrangement pipeline. This is achieved by the robot autonomously constructing a 3D representation of the scene, where objects can be rearranged virtually and an image of the resulting arrangement rendered. These renders are evaluated by a VLM, so that the arrangement which best satisfies the user instruction is selected and recreated in the real world with pick-and-place. This enables language-conditioned rearrangement to be performed zero-shot, without needing to collect a training dataset of example arrangements. Results on a series of real-world tasks show that this framework is robust to distractors, controllable by language, capable of understanding complex multi-object relations, and readily applicable to both tabletop and 6-DoF rearrangement tasks. Videos are available on our webpage at: https://www.robot-learning.uk/dream2real.",
        "authors": [
            "Ivan Kapelyukh",
            "Yifei Ren",
            "Ignacio Alzugaray",
            "Edward Johns"
        ],
        "citations": 14,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!",
        "abstract": "Finetuning a large vision language model (VLM) on a target dataset after large scale pretraining is a dominant paradigm in visual question answering (VQA). Datasets for specialized tasks such as knowledge-based VQA or VQA in non natural-image domains are orders of magnitude smaller than those for general-purpose VQA. While collecting additional labels for specialized tasks or domains can be challenging, unlabeled images are often available. We introduce SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA uses the VLM and target dataset to build a teacher model that can generate question-answer pseudolabels directly conditioned on an image alone, allowing us to pseudolabel unlabeled images. SelTDA then finetunes the initial VLM on the original dataset augmented with freshly pseudolabeled images. We describe a series of experiments showing that our self-taught data augmentation increases robustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills. The proposed strategy requires no additional annotations or architectural modifications, and is compatible with any modern encoder-decoder multimodal transformer. Code available at https://github.com/codezakh/SelTDA.",
        "authors": [
            "Zaid Khan",
            "B. Vijaykumar",
            "S. Schulter",
            "Xiang Yu",
            "Y. Fu",
            "Manmohan Chandraker"
        ],
        "citations": 14,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
        "abstract": "Pre-trained and frozen large language models (LLMs) can effectively map simple scene rearrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for TfD. Our models, code, and video results can be found in our project's website: https://helper-agent-llm.github.io.",
        "authors": [
            "Gabriel Sarch",
            "Yue Wu",
            "Michael J. Tarr",
            "Katerina Fragkiadaki"
        ],
        "citations": 14,
        "references": 98,
        "year": 2023
    },
    {
        "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
        "abstract": "The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate\"verbalizers\"that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.",
        "authors": [
            "Byung-Kwan Lee",
            "Ryo Hachiuma",
            "Yu-Chiang Frank Wang",
            "Yonghyun Ro",
            "Yueh-Hua Wu"
        ],
        "citations": 0,
        "references": 102,
        "year": 2024
    },
    {
        "title": "Molecular organization of autonomic, respiratory, and spinally-projecting neurons in the mouse ventrolateral medulla.",
        "abstract": "The ventrolateral medulla (VLM) is a crucial region in the brain for visceral and somatic control, serving as a significant source of synaptic input to the spinal cord. Experimental studies have shown that gene expression in individual VLM neurons is predictive of their function. However, the molecular and cellular organization of the VLM has remained uncertain. This study aimed to create a comprehensive dataset of VLM cells using single-cell RNA sequencing in male and female mice. The dataset was enriched with targeted sequencing of spinally-projecting and adrenergic/noradrenergic VLM neurons. Based on differentially expressed genes, the resulting dataset of 114,805 VLM cells identifies 23 subtypes of neurons, excluding those in the inferior olive, and 5 subtypes of astrocytes. Spinally-projecting neurons were found to be abundant in 7 subtypes of neurons, which were validated through in-situ hybridization. These subtypes included adrenergic/noradrenergic neurons, serotonergic neurons, and neurons expressing gene markers associated with pre-motor neurons in the ventromedial medulla. Further analysis of adrenergic/noradrenergic neurons and serotonergic neurons identified 9 and 6 subtypes, respectively, within each class of monoaminergic neurons. Marker genes that identify the neural network responsible for breathing were concentrated in 2 subtypes of neurons, delineated from each other by markers for excitatory and inhibitory neurons. These datasets are available for public download and for analysis with a user-friendly interface. Collectively, this study provides a fine-scale molecular identification of cells in the VLM, forming the foundation for a better understanding of the VLM's role in vital functions and motor control.Significance statement The ventrolateral medulla (VLM) is an anatomically complex region of the brain that plays a crucial role in regulating vital functions, including autonomic and respiratory control, sleep-wake behaviors, cranial motor functions, and locomotion. This study comprehensively classifies VLM cell types and neuronal subtypes based on their molecular and anatomical features, by leveraging single-nuclei RNA sequencing, RNA fluorescence in situ hybridization, and axonal tract tracing. We present a dataset comprising 114,805 single-nuclei transcriptomes that identifies and validates the precise molecular characteristics of neurons involved in autonomic and motor systems functions. This publicly-available dataset offers new opportunities for comprehensive experimental studies to dissect the central organization of vital homeostatic functions and body movement.",
        "authors": [
            "Dana C Schwalbe",
            "Daniel S. Stornetta",
            "Ruei-Jen Abraham-Fan",
            "George Souza",
            "Maira Jalil",
            "Maisie E. Crook",
            "John N. Campbell",
            "Stephen B. G. Abbott"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Virtual-Stator Loss Model for Synchronous Generators With Fractional-Slot Winding",
        "abstract": "In synchronous generators, the end-region field can cause additional losses at clamping structures and stator end core. It can be a computational challenge to solve the end-region field of synchronous generators featuring fractional-slot winding. With a minor modification of the number of stator slots, the winding scheme can be changed to the integral slot and, consequently, a virtual stator is derived. The model size of a three-dimensional (3D) Virtual-stator Loss Model (VLM) is much smaller than the Real-stator Loss Model (RLM), and the computational burden is substantially alleviated. This paper delves into the theory underpinning VLM and its practical implementation. From the perspective of two-dimensional (2D) analysis, 2D VLM demonstrates close results in field analysis and loss computation when compared to 2D RLM, indicating the potential of the 3D VLM in accurately computing end-region losses. The 3D VLM is used to compute the end-region losses at different operating points. It concludes that the 3D VLM can keep the essential nature of the loss-producing mechanism while dramatically mitigating the computation hurdles. Lab tests at a 100 kVA salient-pole synchronous generator prove that the investigation approach employed in the paper is trustworthy.",
        "authors": [
            "Zhaoqiang Zhang",
            "A. Nysveen",
            "B√∏rge Johannes Fagermyr",
            "R. Nilssen",
            "H. Ehya",
            "Satish Belkhode"
        ],
        "citations": 0,
        "references": 23,
        "year": 2024
    },
    {
        "title": "Harnessing LLMs for VQA: A Prompted Benchmark with Animate/Inanimate Keywords",
        "abstract": "In the field of NLP, Large Language Models (LLMs) have recently achieved significant advancements, leading to the development of various benchmarks for their evaluation. Along-side NLP, Vision Language Models (VLMs) have also VLM have also significantly progressed, similar to LLMs. However, benchmarks for VLMs are still relatively underdeveloped compared to those for NLP, and their construction is often costly. In this work, we propose an automatically generated benchmark for evaluating VLMs based on LLMs and conduct a visual question answering task to assess this benchmark. The benchmark includes multiple-choice questions that not only distinguish between animate and inanimate objects but also generate these distinctions automatically, along with entity and object information within images. We evaluate the performance of open VLM using the generated multiple-choice questions, demonstrating the model's capabilities and the significance of the automatically generated benchmark. Finally, we discuss the necessity and future directions for benchmark research in this area.",
        "authors": [
            "Chanwoo Lee",
            "Hyunjeong Lee",
            "Minsang Kim",
            "Hyun Kim",
            "Haneol Jang",
            "Cheoneum Park"
        ],
        "citations": 0,
        "references": 21,
        "year": 2024
    },
    {
        "title": "Vision Language Models Can Parse Floor Plan Maps",
        "abstract": "Vision language models (VLMs) can simultaneously reason about images and texts to tackle many tasks, from visual question answering to image captioning. This paper focuses on map parsing, a novel task that is unexplored within the VLM context and particularly useful to mobile robots. Map parsing requires understanding not only the labels but also the geometric configurations of a map, i.e., what areas are like and how they are connected. To evaluate the performance of VLMs on map parsing, we prompt VLMs with floorplan maps to generate task plans for complex indoor navigation. Our results demonstrate the remarkable capability of VLMs in map parsing, with a success rate of 0.96 in tasks requiring a sequence of nine navigation actions, e.g., approaching and going through doors. Other than intuitive observations, e.g., VLMs do better in smaller maps and simpler navigation tasks, there was a very interesting observation that its performance drops in large open areas. We provide practical suggestions to address such challenges as validated by our experimental results. Webpage: https://shorturl.at/OUkEY",
        "authors": [
            "David DeFazio",
            "Hrudayangam Mehta",
            "Jeremy Blackburn",
            "Shiqi Zhang"
        ],
        "citations": 0,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical Imaging Interpretation",
        "abstract": "Recent medical vision-language models (VLMs) have shown promise in 2D medical image interpretation. However extending them to 3D medical imaging has been challenging due to computational complexities and data scarcity. Although a few recent VLMs specified for 3D medical imaging have emerged, all are limited to learning volumetric representation of a 3D medical image as a set of sub-volumetric features. Such process introduces overly correlated representations along the z-axis that neglect slice-specific clinical details, particularly for 3D medical images where adjacent slices have low redundancy. To address this limitation, we introduce MS-VLM that mimic radiologists' workflow in 3D medical image interpretation. Specifically, radiologists analyze 3D medical images by examining individual slices sequentially and synthesizing information across slices and views. Likewise, MS-VLM leverages self-supervised 2D transformer encoders to learn a volumetric representation that capture inter-slice dependencies from a sequence of slice-specific features. Unbound by sub-volumetric patchification, MS-VLM is capable of obtaining useful volumetric representations from 3D medical images with any slice length and from multiple images acquired from different planes and phases. We evaluate MS-VLM on publicly available chest CT dataset CT-RATE and in-house rectal MRI dataset. In both scenarios, MS-VLM surpasses existing methods in radiology report generation, producing more coherent and clinically relevant reports. These findings highlight the potential of MS-VLM to advance 3D medical image interpretation and improve the robustness of medical VLMs.",
        "authors": [
            "Changsun Lee",
            "Sangjoon Park",
            "Cheong-Il Shin",
            "Woo Hee Choi",
            "HyunWook Park",
            "Jeonghyeon Lee",
            "Jong Chul Ye"
        ],
        "citations": 0,
        "references": 42,
        "year": 2024
    },
    {
        "title": "PolySmart @ TRECVid 2024 Video Captioning (VTT)",
        "abstract": "In this paper, we present our methods and results for the Video-To-Text (VTT) task at TRECVid 2024, exploring the capabilities of Vision-Language Models (VLMs) like LLaVA and LLaVA-NeXT-Video in generating natural language descriptions for video content. We investigate the impact of fine-tuning VLMs on VTT datasets to enhance description accuracy, contextual relevance, and linguistic consistency. Our analysis reveals that fine-tuning substantially improves the model's ability to produce more detailed and domain-aligned text, bridging the gap between generic VLM tasks and the specialized needs of VTT. Experimental results demonstrate that our fine-tuned model outperforms baseline VLMs across various evaluation metrics, underscoring the importance of domain-specific tuning for complex VTT tasks.",
        "authors": [
            "Jiaxin Wu",
            "Wengyu Zhang",
            "Xiao Wei",
            "Qing Li"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Seeing Syntax: Uncovering Syntactic Learning Limitations in Vision-Language Models",
        "abstract": "Vision-language models (VLMs), serve as foundation models for multi-modal applications such as image captioning and text-to-image generation. Recent studies have highlighted limitations in VLM text encoders, particularly in areas like compositionality and semantic understanding, though the underlying reasons for these limitations remain unclear. In this work, we aim to address this gap by analyzing the syntactic information, one of the fundamental linguistic properties, encoded by the text encoders of VLMs. We perform a thorough analysis comparing VLMs with different objective functions, parameter size and training data size, and with uni-modal language models (ULMs) in their ability to encode syntactic knowledge. Our findings suggest that ULM text encoders acquire syntactic information more effectively than those in VLMs. The syntactic information learned by VLM text encoders is shaped primarily by the pre-training objective, which plays a more crucial role than other factors such as model architecture, model size, or the volume of pre-training data. Models exhibit different layer-wise trends where CLIP performance dropped across layers while for other models, middle layers are rich in encoding syntactic knowledge.",
        "authors": [
            "Sri Harsha Dumpala",
            "David Arps",
            "Sageev Oore",
            "Laura Kallmeyer",
            "Hassan Sajjad"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Semi-Supervised Implicit Augmentation for Data-Scarce VQA",
        "abstract": ": Vision-language models (VLMs) have demonstrated increasing potency in solving complex vision-language tasks in the recent past. Visual question answering (VQA) is one of the primary downstream tasks for assessing the capability of VLMs, as it helps in gauging the multimodal understanding of a VLM in answering open-ended questions. The vast contextual information learned during the pretraining stage in VLMs can be utilised effectively to finetune the VQA model for specific datasets. In particular, special types of VQA datasets, such as OK-VQA, A-OKVQA (outside knowledge-based), and ArtVQA (domain-specific), have a relatively smaller number of images and corresponding question-answer annotations in the training set. Such datasets can be categorised as data-scarce. This hinders the effective learning of VLMs due to the low information availability. We introduce SemIAug ( Sem i-Supervised I mplicit Aug mentation), a model and dataset agnostic strategy specially designed to address the challenges faced by limited data availability in the domain-specific VQA datasets. SemIAug uses the annotated image-question data present within the chosen dataset and augments it with meaningful new image-question associations. We show that SemIAug improves the VQA performance on data-scarce datasets without the need for additional data or labels.",
        "authors": [
            "Kuan-Chuan Peng",
            "Abhishek Aich",
            "Ziyan Wu",
            "Bhargav Dodla",
            "Kartik Hegde",
            "A. N. Rajagopalan"
        ],
        "citations": 0,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset",
        "abstract": "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms.",
        "authors": [
            "Yingzi Ma",
            "Jiong Wang",
            "Fei Wang",
            "Siyuan Ma",
            "Jiazhao Li",
            "Xiujun Li",
            "Furong Huang",
            "Lichao Sun",
            "Bo Li",
            "Yejin Choi",
            "Muhao Chen",
            "Chaowei Xiao"
        ],
        "citations": 0,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Dual-Branch Knowledge Enhancement Network with Vision-Language Model for Human-Object Interaction Detection",
        "abstract": "Human-Object Interaction (HOI) detection aims to localize human-object pairs and comprehend their interactions. Recently, pre-trained Vision-Language Models (VLM) have shown their great recognition ability in HOI detection task. However, these VLM based methods are struggle to transfer knowledge to achieve desired performance. To this end, we propose a Dual-Branch Knowledge Enhancement Network with VLM (DBKEN-VLM) within the two-stage paradigm to enhance the effectiveness of VLM. Specifically, we propose a semantic mining decoder to supplement contextual and action-related semantic information into our model. It forms a dual-branch knowledge enhancement network with spatial guided decoder. Furthermore, we propose a two-level fusion strategy for the dualbranch network to facilitate better knowledge transfer of VLM. One is feature-level fusion, producing more instructive interaction features; another is decision-level fusion, further enhancing the capability of VLM for HOI detection. The proposed method achieves competitive performance compared to recent methods on two benchmark datasets, HICO-DET and V-COCO.",
        "authors": [
            "Guangpu Zhou",
            "Dehui Kong",
            "Jinghua Li",
            "Dongpan Chen",
            "Zhuowei Bai",
            "Baocai Yin"
        ],
        "citations": 0,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Multimodal Fact-Checking with Vision Language Models: A Probing Classifier based Solution with Embedding Strategies",
        "abstract": "This study evaluates the effectiveness of Vision Language Models (VLMs) in representing and utilizing multimodal content for fact-checking. To be more specific, we investigate whether incorporating multimodal content improves performance compared to text-only models and how well VLMs utilize text and image information to enhance misinformation detection. Furthermore we propose a probing classifier based solution using VLMs. Our approach extracts embeddings from the last hidden layer of selected VLMs and inputs them into a neural probing classifier for multi-class veracity classification. Through a series of experiments on two fact-checking datasets, we demonstrate that while multimodality can enhance performance, fusing separate embeddings from text and image encoders yielded superior results compared to using VLM embeddings. Furthermore, the proposed neural classifier significantly outperformed KNN and SVM baselines in leveraging extracted embeddings, highlighting its effectiveness for multimodal fact-checking.",
        "authors": [
            "R. √áekinel",
            "Pinar Senkul",
            "√áaƒürƒ± √á√∂ltekin"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
        "abstract": "The recent progress in Vision-Language Models (VLMs) has broadened the scope of multimodal applications. However, evaluations often remain limited to functional tasks, neglecting abstract dimensions such as personality traits and human values. To address this gap, we introduce Value-Spectrum, a novel Visual Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's value dimensions that capture core values guiding people's preferences and actions. We designed a VLM agent pipeline to simulate video browsing and constructed a vector database comprising over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels. These videos span multiple months and cover diverse topics, including family, health, hobbies, society, technology, etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs handle value-oriented content. Beyond identifying VLMs' intrinsic preferences, we also explored the ability of VLM agents to adopt specific personas when explicitly prompted, revealing insights into the adaptability of the model in role-playing scenarios. These findings highlight the potential of Value-Spectrum as a comprehensive evaluation set for tracking VLM alignments in value-based tasks and abilities to simulate diverse personas.",
        "authors": [
            "Jingxuan Li",
            "Yuning Yang",
            "Shengqi Yang",
            "Linfan Zhang",
            "Ying Nian Wu"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Retention Score: Quantifying Jailbreak Risks for Vision Language Models",
        "abstract": "The emergence of Vision-Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to enhance multi-modal machine learning capabilities. However, this progress has also made VLMs vulnerable to sophisticated adversarial attacks, raising concerns about their reliability. The objective of this paper is to assess the resilience of VLMs against jailbreak attacks that can compromise model safety compliance and result in harmful outputs. To evaluate a VLM's ability to maintain its robustness against adversarial input perturbations, we propose a novel metric called the \\textbf{Retention Score}. Retention Score is a multi-modal evaluation metric that includes Retention-I and Retention-T scores for quantifying jailbreak risks in visual and textual components of VLMs. Our process involves generating synthetic image-text pairs using a conditional diffusion model. These pairs are then predicted for toxicity score by a VLM alongside a toxicity judgment classifier. By calculating the margin in toxicity scores, we can quantify the robustness of the VLM in an attack-agnostic manner. Our work has four main contributions. First, we prove that Retention Score can serve as a certified robustness metric. Second, we demonstrate that most VLMs with visual components are less robust against jailbreak attacks than the corresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find that the security settings in Google Gemini significantly affect the score and robustness. Moreover, the robustness of GPT4V is similar to the medium settings of Gemini. Finally, our approach offers a time-efficient alternative to existing adversarial attack methods and provides consistent model robustness rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA.",
        "authors": [
            "Zaitang Li",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "citations": 0,
        "references": 44,
        "year": 2024
    },
    {
        "title": "NL-Eye: Abductive NLI for Images",
        "abstract": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps - writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-Eye, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.",
        "authors": [
            "Mor Ventura",
            "Michael Toker",
            "Nitay Calderon",
            "Zorik Gekhman",
            "Yonatan Bitton",
            "Roi Reichart"
        ],
        "citations": 0,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Q-GroundCAM: Quantifying Grounding in Vision Language Models via GradCAM",
        "abstract": "Vision and Language Models (VLMs) continue to demonstrate remarkable zero-shot (ZS) performance across various tasks. However, many probing studies have revealed that even the best-performing VLMs struggle to capture aspects of compositional scene understanding, lacking the ability to properly ground and localize linguistic phrases in images. Recent VLM advancements include scaling up both model and dataset sizes, additional training objectives and levels of supervision, and variations in the model architectures. To characterize the grounding ability of VLMs, such as phrase grounding, referring expressions comprehension, and relationship understanding, Pointing Game has been used as an evaluation metric for datasets with bounding box annotations. In this paper, we introduce a novel suite of quantitative metrics that utilize GradCAM activations to rigorously evaluate the grounding capabilities of pre-trained VLMs like CLIP, BLIP, and ALBEF. These metrics offer an explainable and quantifiable approach for a more detailed comparison of the zero-shot capabilities of VLMs and enable measuring models' grounding uncertainty. This characterization reveals interesting tradeoffs between the size of the model, the dataset size, and their performance.",
        "authors": [
            "Navid Rajabi",
            "J. Kosecka"
        ],
        "citations": 0,
        "references": 34,
        "year": 2024
    },
    {
        "title": "Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding",
        "abstract": "Vision-Language Models (VLM) can support clinicians by analyzing medical images and engaging in natural language interactions to assist in diagnostic and treatment tasks. However, VLMs often exhibit\"hallucinogenic\"behavior, generating textual outputs not grounded in contextual multimodal information. This challenge is particularly pronounced in the medical domain, where we do not only require VLM outputs to be accurate in single interactions but also to be consistent with clinical reasoning and diagnostic pathways throughout multi-turn conversations. For this purpose, we propose a new alignment algorithm that uses symbolic representations of clinical reasoning to ground VLMs in medical knowledge. These representations are utilized to (i) generate GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM conversations with demonstrations of clinical reasoning, and (ii) create an automatic reward function that evaluates the clinical validity of VLM generations throughout clinician-VLM interactions. Our algorithm eliminates the need for human involvement in training data generation or reward model construction, reducing costs compared to standard reinforcement learning with human feedback (RLHF). We apply our alignment algorithm to develop Dr-LLaVA, a conversational VLM finetuned for analyzing bone marrow pathology slides, demonstrating strong performance in multi-turn medical conversations.",
        "authors": [
            "Shenghuan Sun",
            "Gregory M. Goldgof",
            "Alexander Schubert",
            "Zhiqing Sun",
            "Thomas Hartvigsen",
            "A. Butte",
            "Ahmed Alaa"
        ],
        "citations": 0,
        "references": 56,
        "year": 2024
    },
    {
        "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension",
        "abstract": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in various open-vocabulary tasks, yet their zero-shot performance lags behind task-specific finetuned models, particularly in complex tasks like Referring Expression Comprehension (REC). Fine-tuning usually requires 'white-box' access to the model's architecture and weights, which is not always feasible due to proprietary or privacy concerns. In this work, we propose LLM-wrapper, a method for 'black-box' adaptation of VLMs for the REC task using Large Language Models (LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved with a light fine-tuning, to select the most relevant bounding box matching the referring expression, from candidates generated by a zero-shot black-box VLM. Our approach offers several advantages: it enables the adaptation of closed-source models without needing access to their internal workings, it is versatile as it works with any VLM, it transfers to new VLMs, and it allows for the adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on multiple datasets using different VLMs and LLMs, demonstrating significant performance improvements and highlighting the versatility of our method. While LLM-wrapper is not meant to directly compete with standard white-box fine-tuning, it offers a practical and effective alternative for black-box VLM adaptation. The code will be open-sourced.",
        "authors": [
            "Amaia Cardiel",
            "√âloi Zablocki",
            "Elias Ramzi",
            "Oriane Sim'eoni",
            "Matthieu Cord"
        ],
        "citations": 0,
        "references": 49,
        "year": 2024
    },
    {
        "title": "Understanding Graphical Perception in Data Visualization through Zero-shot Prompting of Vision-Language Models",
        "abstract": "Vision Language Models (VLMs) have been successful at many chart comprehension tasks that require attending to both the images of charts and their accompanying textual descriptions. However, it is not well established how VLM performance profiles map to human-like behaviors. If VLMs can be shown to have human-like chart comprehension abilities, they can then be applied to a broader range of tasks, such as designing and evaluating visualizations for human readers. This paper lays the foundations for such applications by evaluating the accuracy of zero-shot prompting of VLMs on graphical perception tasks with established human performance profiles. Our findings reveal that VLMs perform similarly to humans under specific task and style combinations, suggesting that they have the potential to be used for modeling human performance. Additionally, variations to the input stimuli show that VLM accuracy is sensitive to stylistic changes such as fill color and chart contiguity, even when the underlying data and data mappings are the same.",
        "authors": [
            "Grace Guo",
            "Jenna Jiayi Kang",
            "Raj Sanjay Shah",
            "Hanspeter Pfister",
            "Sashank Varma"
        ],
        "citations": 0,
        "references": 24,
        "year": 2024
    },
    {
        "title": "Improving Medical Diagnostics with Vision-Language Models: Convex Hull-Based Uncertainty Analysis",
        "abstract": "In recent years, vision-language models (VLMs) have been applied to various fields, including healthcare, education, finance, and manufacturing, with remarkable performance. However, concerns remain regarding VLMs' consistency and uncertainty, particularly in critical applications such as healthcare, which demand a high level of trust and reliability. This paper proposes a novel approach to evaluate uncertainty in VLMs' responses using a convex hull approach on a healthcare application for Visual Question Answering (VQA). LLM-CXR model is selected as the medical VLM utilized to generate responses for a given prompt at different temperature settings, i.e., 0.001, 0.25, 0.50, 0.75, and 1.00. According to the results, the LLM-CXR VLM shows a high uncertainty at higher temperature settings. Experimental outcomes emphasize the importance of uncertainty in VLMs' responses, especially in healthcare applications.",
        "authors": [
            "Ferhat Ozgur Catak",
            "M. Kuzlu",
            "Taylor Patrick"
        ],
        "citations": 0,
        "references": 40,
        "year": 2024
    },
    {
        "title": "Open-vocabulary Temporal Action Localization using VLMs",
        "abstract": "‚ÄîVideo action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. The code will be available shortly.",
        "authors": [
            "Naoki Wake",
            "Atsushi Kanehira",
            "Kazuhiro Sasabuchi",
            "Jun Takamatsu",
            "Katsushi Ikeuchi"
        ],
        "citations": 0,
        "references": 49,
        "year": 2024
    },
    {
        "title": "The potential promise and pitfalls of point-of-care viral load monitoring to expedite HIV treatment decision-making in rural Uganda: a qualitative study",
        "abstract": null,
        "authors": [
            "Joseph G Rosen",
            "W. Ddaaki",
            "N. Nakyanjo",
            "L. Chang",
            "Anh Van Vo",
            "Tongying Zhao",
            "G. Nakigozi",
            "Fred Nalugoda",
            "G. Kigozi",
            "J. Kagaayi",
            "Thomas C. Quinn",
            "M. K. Grabowski",
            "S. Reynolds",
            "Caitlin E Kennedy",
            "R. Galiwango"
        ],
        "citations": 0,
        "references": 45,
        "year": 2024
    },
    {
        "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
        "abstract": "Recent advances in Vision-Language Models (VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numerous researches on synthetic VLM data generation. The conventional norm in VLM data construction uses a mixture of specialists in caption and OCR, or stronger VLM APIs and expensive human annotation.In this paper, we present World to Code (W2C), a meticulously curated multi-modal data construction pipeline that organizes the final generation output into a Python code format. The pipeline leverages the VLM itself to extract cross-modal information via different prompts and filter the generated outputs again via a consistency filtering strategy. Experiments have demonstrated the high quality of W2C by improving various existing visual question answering and visual grounding benchmarks across different VLMs. Further analysis also demonstrates that the new code parsing ability of VLMs presents better cross-modal equivalence than the commonly used detail caption ability. Our code is available at https://github.com/foundation-multimodal-models/World2Code.",
        "authors": [
            "Jiacong Wang",
            "Bohong Wu",
            "Haiyong Jiang",
            "Xun Zhou",
            "Xin Xiao",
            "Haoyuan Guo",
            "Jun Xiao"
        ],
        "citations": 0,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Ventrolateral medullary compression by vascular contact in primary hemifacial spasm: a radiological analysis.",
        "abstract": null,
        "authors": [
            "D. Anudeep",
            "K. Karthik",
            "V. Holla",
            "N. Kamble",
            "Ravi Yadav",
            "P. Pal",
            "R. Mahale"
        ],
        "citations": 0,
        "references": 9,
        "year": 2024
    },
    {
        "title": "More Distinctively Black and Feminine Faces Lead to Increased Stereotyping in Vision-Language Models",
        "abstract": "Vision Language Models (VLMs), exemplified by GPT-4V, adeptly integrate text and vision modalities. This integration enhances Large Language Models' ability to mimic human perception, allowing them to process image inputs. Despite VLMs' advanced capabilities, however, there is a concern that VLMs inherit biases of both modalities in ways that make biases more pervasive and difficult to mitigate. Our study explores how VLMs perpetuate homogeneity bias and trait associations with regards to race and gender. When prompted to write stories based on images of human faces, GPT-4V describes subordinate racial and gender groups with greater homogeneity than dominant groups and relies on distinct, yet generally positive, stereotypes. Importantly, VLM stereotyping is driven by visual cues rather than group membership alone such that faces that are rated as more prototypically Black and feminine are subject to greater stereotyping. These findings suggest that VLMs may associate subtle visual cues related to racial and gender groups with stereotypes in ways that could be challenging to mitigate. We explore the underlying reasons behind this behavior and discuss its implications and emphasize the importance of addressing these biases as VLMs come to mirror human perception.",
        "authors": [
            "Messi H.J. Lee",
            "Jacob M. Montgomery",
            "Calvin K Lai"
        ],
        "citations": 0,
        "references": 58,
        "year": 2024
    },
    {
        "title": "Knowledge-grounded Adaptation Strategy for Vision-language Models: Building Unique Case-set for Screening Mammograms for Residents Training",
        "abstract": "A visual-language model (VLM) pre-trained on natural images and text pairs poses a significant barrier when applied to medical contexts due to domain shift. Yet, adapting or fine-tuning these VLMs for medical use presents considerable hurdles, including domain misalignment, limited access to extensive datasets, and high-class imbalances. Hence, there is a pressing need for strategies to effectively adapt these VLMs to the medical domain, as such adaptations would prove immensely valuable in healthcare applications. In this study, we propose a framework designed to adeptly tailor VLMs to the medical domain, employing selective sampling and hard-negative mining techniques for enhanced performance in retrieval tasks. We validate the efficacy of our proposed approach by implementing it across two distinct VLMs: the in-domain VLM (MedCLIP) and out-of-domain VLMs (ALBEF). We assess the performance of these models both in their original off-the-shelf state and after undergoing our proposed training strategies, using two extensive datasets containing mammograms and their corresponding reports. Our evaluation spans zero-shot, few-shot, and supervised scenarios. Through our approach, we observe a notable enhancement in Recall@K performance for the image-text retrieval task.",
        "authors": [
            "Aisha Urooj Khan",
            "John Garrett",
            "Tyler Bradshaw",
            "Lonie R. Salkowski",
            "Jiwoong Jeong",
            "Amara Tariq",
            "Imon Banerjee"
        ],
        "citations": 0,
        "references": 22,
        "year": 2024
    },
    {
        "title": "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models",
        "abstract": "Vision-language models (VLMs) have gained widespread adoption in both industry and academia. In this study, we propose a unified framework for systematically evaluating gender, race, and age biases in VLMs with respect to professions. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. Additionally, we propose an automated pipeline to generate high-quality synthetic datasets that intentionally conceal gender, race, and age information across different professional domains, both in generated text and images. The dataset includes action-based descriptions of each profession and serves as a benchmark for evaluating societal biases in vision-language models (VLMs). In our comparative analysis of widely used VLMs, we have identified that varying input-output modalities lead to discernible differences in bias magnitudes and directions. Additionally, we find that VLM models exhibit distinct biases across different bias attributes we investigated. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.",
        "authors": [
            "Ashutosh Sathe",
            "Prachi Jain",
            "Sunayana Sitaram"
        ],
        "citations": 0,
        "references": 46,
        "year": 2024
    },
    {
        "title": "The Solution for CVPR2024 Foundational Few-Shot Object Detection Challenge",
        "abstract": "This report introduces an enhanced method for the Foundational Few-Shot Object Detection (FSOD) task, leveraging the vision-language model (VLM) for object detection. However, on specific datasets, VLM may encounter the problem where the detected targets are misaligned with the target concepts of interest. This misalignment hinders the zero-shot performance of VLM and the application of fine-tuning methods based on pseudo-labels. To address this issue, we propose the VLM+ framework, which integrates the multimodal large language model (MM-LLM). Specifically, we use MM-LLM to generate a series of referential expressions for each category. Based on the VLM predictions and the given annotations, we select the best referential expression for each category by matching the maximum IoU. Subsequently, we use these referential expressions to generate pseudo-labels for all images in the training set and then combine them with the original labeled data to fine-tune the VLM. Additionally, we employ iterative pseudo-label generation and optimization to further enhance the performance of the VLM. Our approach achieve 32.56 mAP in the final test.",
        "authors": [
            "Hongpeng Pan",
            "Shifeng Yi",
            "Shouwei Yang",
            "Lei Qi",
            "Bing Hu",
            "Yi Xu",
            "Yang Yang"
        ],
        "citations": 0,
        "references": 13,
        "year": 2024
    },
    {
        "title": "Multi-Method Modeling and Simulation of a Vertical Lift Module With an Integrated Buffer System Using Anylogic",
        "abstract": "As industry trend continues to accelerate sellers to begin transitioning the sale of their products exclusively through e-commerce platforms, companies must remain vigilant and recognize the requirement for their products to be safely stored and quickly retrieved. This research presents a comprehensive model and simulation study of a Vertical Lift Module (VLM) with an integrated shuttle-based storage and retrieval system (SBS/RS) or buffer system. This work evaluates a proposed solution to the ever-increasing emergent storage and retrieval challenges faced by warehouses worldwide. The VLM system was modeled using AnyLogic software to evaluate system capacity, travel distance, velocity profiles, and other user-defined operational constraints. The VLM performance is modeled under various conditions and compared to the performance of a traditional stand-alone VLM in terms of throughput and cycle-time to identify potential VLM-Buffer system integration drawbacks or limitations.",
        "authors": [
            "Noe Tavira",
            "Abhimanyu Sharotry",
            "Jesus A. Jimenez",
            "Jakob Marolt",
            "T. Lerher"
        ],
        "citations": 0,
        "references": 22,
        "year": 2024
    },
    {
        "title": "A novel interpretation of Nesterov's acceleration via variable step-size linear multistep methods",
        "abstract": "Nesterov's acceleration in continuous optimization can be understood in a novel way when Nesterov's accelerated gradient (NAG) method is considered as a linear multistep (LM) method for gradient flow. Although the NAG method for strongly convex functions (NAG-sc) has been fully discussed, the NAG method for $L$-smooth convex functions (NAG-c) has not. To fill this gap, we show that the existing NAG-c method can be interpreted as a variable step size LM (VLM) for the gradient flow. Surprisingly, the VLM allows linearly increasing step sizes, which explains the acceleration in the convex case. Here, we introduce a novel technique for analyzing the absolute stability of VLMs. Subsequently, we prove that NAG-c is optimal in a certain natural class of VLMs. Finally, we construct a new broader class of VLMs by optimizing the parameters in the VLM for ill-conditioned problems. According to numerical experiments, the proposed method outperforms the NAG-c method in ill-conditioned cases. These results imply that the numerical analysis perspective of the NAG is a promising working environment, and considering a broader class of VLMs could further reveal novel methods.",
        "authors": [
            "Ryota Nozawa",
            "Shun Sato",
            "Takayasu Matsuo"
        ],
        "citations": 0,
        "references": 23,
        "year": 2024
    },
    {
        "title": "Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks",
        "abstract": "Vision language models (VLMs) like CLIP show stellar zero-shot capability on classification benchmarks. However, selecting the VLM with the highest performance on the unlabeled downstream task is non-trivial. Existing VLM selection methods focus on the class-name-only setting, relying on a supervised large-scale dataset and large language models, which may not be accessible or feasible during deployment. This paper introduces the problem of \\textbf{unsupervised vision-language model selection}, where only unsupervised downstream datasets are available, with no additional information provided. To solve this problem, we propose a method termed Visual-tExtual Graph Alignment (VEGA), to select VLMs without any annotations by measuring the alignment of the VLM between the two modalities on the downstream task. VEGA is motivated by the pretraining paradigm of VLMs, which aligns features with the same semantics from the visual and textual modalities, thereby mapping both modalities into a shared representation space. Specifically, we first construct two graphs on the vision and textual features, respectively. VEGA is then defined as the overall similarity between the visual and textual graphs at both node and edge levels. Extensive experiments across three different benchmarks, covering a variety of application scenarios and downstream datasets, demonstrate that VEGA consistently provides reliable and accurate estimates of VLMs' performance on unlabeled downstream tasks.",
        "authors": [
            "Yuhe Ding",
            "Bo Jiang",
            "Aihua Zheng",
            "Qin Xu",
            "Jian Liang"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "The vastus medialis oblique compensates in current patellar dislocation patients with the increased femoral anteversion",
        "abstract": null,
        "authors": [
            "C. Dong",
            "Zhenhui Huo",
            "Y. Niu",
            "Hui-jun Kang",
            "Fei Wang"
        ],
        "citations": 0,
        "references": 44,
        "year": 2024
    },
    {
        "title": "Advancements in Visual Language Models for Remote Sensing: Datasets, Capabilities, and Enhancement Techniques",
        "abstract": "Recently, the remarkable success of ChatGPT has sparked a renewed wave of interest in artificial intelligence (AI), and the advancements in visual language models (VLMs) have pushed this enthusiasm to new heights. Differring from previous AI approaches that generally formulated different tasks as discriminative models, VLMs frame tasks as generative models and align language with visual information, enabling the handling of more challenging problems. The remote sensing (RS) field, a highly practical domain, has also embraced this new trend and introduced several VLM-based RS methods that have demonstrated promising performance and enormous potential. In this paper, we first review the fundamental theories related to VLM, then summarize the datasets constructed for VLMs in remote sensing and the various tasks they addressed. Finally, we categorize the improvement methods into three main parts according to the core components of VLMs and provide a detailed introduction and comparison of these methods. A project associated with this review has been created at https://github.com/taolijie11111/VLMs-in-RS-review.",
        "authors": [
            "Lijie Tao",
            "Haokui Zhang",
            "Haizhao Jing",
            "Yu Liu",
            "Kelu Yao",
            "Chao Li",
            "Xizhe Xue"
        ],
        "citations": 0,
        "references": 98,
        "year": 2024
    },
    {
        "title": "EdgeCloudAI: Edge-Cloud Distributed Video Analytics",
        "abstract": "Recent advances in Visual Language Models (VLMs) have significantly enhanced video analytics. VLMs capture complex visual and textual connections. While Convolutional Neural Networks (CNNs) excel in spatial pattern recognition, VLMs provide a global context, making them ideal for tasks like complex incidents and anomaly detection. However, VLMs are much more computationally intensive, posing challenges for large-scale and real-time applications. This paper introduces EdgeCloudAI, a scalable system integrating VLMs and CNNs through edge-cloud computing. Edge-CloudAI performs initial video processing (e.g., CNN) on edge devices and offloads deeper analysis (e.g., VLM) to the cloud, optimizing resource use and reducing latency. We have deployed EdgeCloudAI on the NSF COSMOS testbed in NYC. In this demo, we will demonstrate EdgeCloudAI‚Äôs performance in detecting user-defined incidents in real-time.",
        "authors": [
            "Mahshid Ghasemi",
            "Z. Kostiƒá",
            "Javad Ghaderi",
            "Gil Zussman"
        ],
        "citations": 0,
        "references": 30,
        "year": 2024
    },
    {
        "title": "Evaluating Vision-Language Models as Evaluators in Path Planning",
        "abstract": "Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs augmented with visual understanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a novel benchmark evaluating VLMs as plan evaluators in complex path-planning scenarios. Succeeding in the benchmark requires a VLM to be able to abstract traits of optimal paths from the scenario description, demonstrate precise low-level perception on each path, and integrate this information to decide the better path. Our analysis of state-of-the-art VLMs reveals that these models face significant challenges on the benchmark. We observe that the VLMs can precisely abstract given scenarios to identify the desired traits and exhibit mixed performance in integrating the provided information. Yet, their vision component presents a critical bottleneck, with models struggling to perceive low-level details about a path. Our experimental results show that this issue cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific discriminative adaptation of these vision encoders is needed for these VLMs to become effective path evaluators.",
        "authors": [
            "Mohamed Aghzal",
            "Xiang Yue",
            "E. Plaku",
            "Ziyu Yao"
        ],
        "citations": 0,
        "references": 78,
        "year": 2024
    },
    {
        "title": "Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study",
        "abstract": "The large-scale pre-trained vision language models (VLM) have shown remarkable domain transfer capability on natural images. However, it remains unknown whether this capability can also apply to the medical image domain. This paper thoroughly studies the knowledge transferability of pre-trained VLMs to the medical domain, where we show that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs. We demonstrate that by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization. This mechanism empowers VLMs to recognize novel objects with fewer or without image samples. Furthermore, to avoid the laborious manual designing process, we develop three approaches for automatic generation of medical prompts, which can inject expert-level medical knowledge and image-specific information into the prompts for fine-grained grounding. We conduct extensive experiments on thirteen different medical datasets across various modalities, showing that our well-designed prompts greatly improve the zero-shot performance compared to the default prompts, and our fine-tuned models surpass the supervised models by a significant margin.",
        "authors": [
            "Ziyuan Qin",
            "Huahui Yi",
            "Qicheng Lao",
            "Kang Li"
        ],
        "citations": 51,
        "references": 53,
        "year": 2022
    },
    {
        "title": "Towards Evaluating Generalist Agents: An Automated Benchmark in Open World",
        "abstract": "Evaluating generalist agents presents significant challenges due to their wide-ranging abilities and the limitations of current benchmarks in assessing true generalization. We introduce the Minecraft Universe (MCU), a fully automated benchmarking framework set within the open-world game Minecraft. MCU dynamically generates and evaluates a broad spectrum of tasks, offering three core components: 1) a task generation mechanism that provides high degrees of freedom and variability, 2) an ever-expanding set of over 3K composable atomic tasks, and 3) a general evaluation framework that supports open-ended task assessment. By integrating large language models (LLMs), MCU dynamically creates diverse environments for each evaluation, fostering agent generalization. The framework uses a vision-language model (VLM) to automatically generate evaluation criteria, achieving over 90% agreement with human ratings across multi-dimensional assessments, which demonstrates that MCU is a scalable and explainable solution for evaluating generalist agents. Additionally, we show that while state-of-the-art foundational models perform well on specific tasks, they often struggle with increased task diversity and difficulty.",
        "authors": [
            "Haowei Lin",
            "Zihao Wang",
            "Jianzhu Ma",
            "Yitao Liang"
        ],
        "citations": 13,
        "references": 68,
        "year": 2023
    },
    {
        "title": "DST-Det: Simple Dynamic Self-Training for Open-Vocabulary Object Detection",
        "abstract": "Open-vocabulary object detection (OVOD) aims to detect the objects beyond the set of classes observed during training. This work introduces a straightforward and efficient strategy that utilizes pre-trained vision-language models (VLM), like CLIP, to identify potential novel classes through zero-shot classification. Previous methods use a class-agnostic region proposal network to detect object proposals and consider the proposals that do not match the ground truth as background. Unlike these methods, our method will select a subset of proposals that will be considered as background during the training. Then, we treat them as novel classes during training. We refer to this approach as the self-training strategy, which enhances recall and accuracy for novel classes without requiring extra annotations, datasets, and re-training. Compared to previous pseudo methods, our approach does not require re-training and offline labeling processing, which is more efficient and effective in one-shot training. Empirical evaluations on three datasets, including LVIS, V3Det, and COCO, demonstrate significant improvements over the baseline performance without incurring additional parameters or computational costs during inference. In addition, we also apply our method to various baselines. In particular, compared with the previous method, F-VLM, our method achieves a 1.7% improvement on the LVIS dataset. Combined with the recent method CLIPSelf, our method also achieves 46.7 novel class AP on COCO without introducing extra data for pertaining. We also achieve over 6.5% improvement over the F-VLM baseline in the recent challenging V3Det dataset. We release our code and models at https://github.com/xushilin1/dst-det.",
        "authors": [
            "Shilin Xu",
            "Xiangtai Li",
            "Size Wu",
            "Wenwei Zhang",
            "Yining Li",
            "Guangliang Cheng",
            "Yunhai Tong",
            "Kai Chen",
            "Chen Change Loy"
        ],
        "citations": 12,
        "references": 86,
        "year": 2023
    },
    {
        "title": "LOVM: Language-Only Vision Model Selection",
        "abstract": "Pre-trained multi-modal vision-language models (VLMs) are becoming increasingly popular due to their exceptional performance on downstream vision applications, particularly in the few- and zero-shot settings. However, selecting the best-performing VLM for some downstream applications is non-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive evaluation of all available VLMs on a novel application is not only time and computationally demanding but also necessitates the collection of a labeled dataset for evaluation. As the number of open-source VLM variants increases, there is a need for an efficient model selection strategy that does not require access to a curated evaluation dataset. This paper proposes a novel task and benchmark for efficiently evaluating VLMs' zero-shot performance on downstream applications without access to the downstream task dataset. Specifically, we introduce a new task LOVM: Language-Only Vision Model Selection, where methods are expected to perform both model selection and performance prediction based solely on a text description of the desired downstream application. We then introduced an extensive LOVM benchmark consisting of ground-truth evaluations of 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the pre-trained VLMs and predict their zero-shot performance.",
        "authors": [
            "O. Zohar",
            "Shih-Cheng Huang",
            "Kuan Wang",
            "Serena Yeung"
        ],
        "citations": 10,
        "references": 73,
        "year": 2023
    },
    {
        "title": "SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance",
        "abstract": "In semi-supervised semantic segmentation, a model is trained with a limited number of labeled images along with a large corpus of unlabeled images to reduce the high annotation effort. While previous methods are able to learn good segmentation boundaries, they are prone to confuse classes with similar visual appearance due to the limited supervision. On the other hand, vision-language models (VLMs) are able to learn diverse semantic knowledge from image-caption datasets but produce noisy segmentation due to the image-level training. In SemiVL, we propose to integrate rich priors from VLM pre-training into semi-supervised semantic segmentation to learn better semantic decision boundaries. To adapt the VLM from global to local reasoning, we introduce a spatial fine-tuning strategy for label-efficient learning. Further, we design a language-guided decoder to jointly reason over vision and language. Finally, we propose to handle inherent ambiguities in class labels by providing the model with language guidance in the form of class definitions. We evaluate SemiVL on 4 semantic segmentation datasets, where it significantly outperforms previous semi-supervised methods. For instance, SemiVL improves the state-of-the-art by +13.5 mIoU on COCO with 232 annotated images and by +6.1 mIoU on Pascal VOC with 92 labels. Project page: https://github.com/google-research/semivl",
        "authors": [
            "Lukas Hoyer",
            "D. Tan",
            "Muhammad Ferjad Naeem",
            "L. V. Gool",
            "F. Tombari"
        ],
        "citations": 11,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models",
        "abstract": "One fascinating aspect of pre-trained vision-language models~(VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation~(TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions. In this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident. Specifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM. Given a single test sample, the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution. The proposed \\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly flexible and universal. Beyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning, improving the zero-shot generalization capacity of VLMs. According to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs. Extensive experiments along with promising empirical results demonstrate the effectiveness of RLCF. The code is available at https://github.com/mzhaoshuai/RLCF.",
        "authors": [
            "Shuai Zhao",
            "Xiaohan Wang",
            "Linchao Zhu",
            "Yezhou Yang"
        ],
        "citations": 11,
        "references": 65,
        "year": 2023
    },
    {
        "title": "ASCO-OFDM based VLC system throughput improvement using PAPR precoding reduction techniques",
        "abstract": null,
        "authors": [
            "Sara M. Farid",
            "Mona Z. Saleh",
            "Hesham M. Elbadawy",
            "Salwa H. Elramly"
        ],
        "citations": 10,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection",
        "abstract": "Current methods for open-vocabulary object detection (OVOD) rely on a pre-trained vision-language model (VLM) to acquire the recognition ability. In this paper, we propose a simple yet effective framework to Distill the Knowledge from the VLM to a DETR-like detector, termed DK-DETR. Specifically, we present two ingenious distillation schemes named semantic knowledge distillation (SKD) and relational knowledge distillation (RKD). To utilize the rich knowledge from the VLM systematically, SKD transfers the semantic knowledge explicitly, while RKD exploits implicit relationship information between objects. Furthermore, a distillation branch including a group of auxiliary queries is added to the detector to mitigate the negative effect on base categories. Equipped with SKD and RKD on the distillation branch, DK-DETR improves the detection performance of novel categories significantly and avoids disturbing the detection of base categories. Extensive experiments on LVIS and COCO datasets show that DK-DETR surpasses existing OVOD methods under the setting that the base-category supervision is solely available. The code and models are available at https://github.com/hikvision-research/opera.",
        "authors": [
            "Liangqi Li",
            "Jiaxu Miao",
            "Dahu Shi",
            "Wenming Tan",
            "Ye Ren",
            "Yi Yang",
            "Shiliang Pu"
        ],
        "citations": 10,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Vision-Language Models for Zero-Shot Classification of Remote Sensing Images",
        "abstract": "Zero-shot classification presents a challenge since it necessitates a model to categorize images belonging to classes it has not encountered during its training phase. Previous research in the field of remote sensing (RS) has explored this task by training image-based models on known RS classes and then attempting to predict the outcomes for unfamiliar classes. Despite these endeavors, the outcomes have proven to be less than satisfactory. In this paper, we propose an alternative approach that leverages vision-language models (VLMs), which have undergone pre-training to grasp the associations between general computer vision image-text pairs in diverse datasets. Specifically, our investigation focuses on thirteen VLMs derived from Contrastive Language-Image Pre-Training (CLIP/Open-CLIP) with varying levels of parameter complexity. In our experiments, we ascertain the most suitable prompt for RS images to query the language capabilities of the VLM. Furthermore, we demonstrate that the accuracy of zero-shot classification, particularly when using large CLIP models, on three widely recognized RS scene datasets yields superior results compared to existing RS solutions.",
        "authors": [
            "Mohamad Mahmoud Al Rahhal",
            "Y. Bazi",
            "Hebah Elgibreen",
            "M. Zuair"
        ],
        "citations": 10,
        "references": 24,
        "year": 2023
    },
    {
        "title": "ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations",
        "abstract": "Multimodal Vision-Language Models (VLMs) enable powerful applications from their fused understanding of images and language, but many perform poorly on UI tasks due to the lack of UI training data. In this paper, we adapt a recipe for generating paired text-image training data for VLMs to the UI domain by combining existing pixel-based methods with a Large Language Model (LLM). Unlike prior art, our method requires no human-provided annotations, and it can be applied to any dataset of UI screenshots. We generate a dataset of 335K conversational examples paired with UIs that cover Q&A, UI descriptions, and planning, and use it to fine-tune a conversational VLM for UI tasks. To assess the performance of our model, we benchmark it on UI element detection tasks, evaluate response quality, and showcase its applicability to multi-step UI navigation and planning.",
        "authors": [
            "Yue Jiang",
            "E. Schoop",
            "Amanda Swearngin",
            "Jeffrey Nichols"
        ],
        "citations": 10,
        "references": 73,
        "year": 2023
    },
    {
        "title": "TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model",
        "abstract": "Prompt tuning represents a valuable technique for adapting pre-trained visual-language models (VLM) to various downstream tasks. Recent advancements in CoOp-based methods propose a set of learnable domain-shared or image-conditional textual tokens to facilitate the generation of task-specific textual classifiers. However, those textual tokens have a limited generalization ability regarding unseen domains, as they cannot dynamically adjust to the distribution of testing classes. To tackle this issue, we present a novel Textual-based Class-aware Prompt tuning(TCP) that explicitly incorporates prior knowledge about classes to enhance their discriminability. The critical concept of TCP involves leveraging Textual Knowledge Embedding (TKE) to map the high generalizability of class-level textual knowledge into class-aware textual tokens. By seamlessly integrating these class-aware prompts into the Text Encoder, a dynamic class-aware classifier is generated to enhance discriminability for unseen domains. During inference, TKE dynamically generates class-aware prompts related to the unseen classes. Comprehensive evaluations demonstrate that TKE serves as a plug-and-play module effortlessly combinable with existing methods. Furthermore, TCP consistently achieves superior performance while demanding less training time. Code:https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/",
        "authors": [
            "Hantao Yao",
            "Rui Zhang",
            "Changsheng Xu"
        ],
        "citations": 9,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Vision-Language Modelling For Radiological Imaging and Reports In The Low Data Regime",
        "abstract": "This paper explores training medical vision-language models (VLMs) -- where the visual and language inputs are embedded into a common space -- with a particular focus on scenarios where training data is limited, as is often the case in clinical datasets. We explore several candidate methods to improve low-data performance, including: (i) adapting generic pre-trained models to novel image and text domains (i.e. medical imaging and reports) via unimodal self-supervision; (ii) using local (e.g. GLoRIA)&global (e.g. InfoNCE) contrastive loss functions as well as a combination of the two; (iii) extra supervision during VLM training, via: (a) image- and text-only self-supervision, and (b) creating additional positive image-text pairs for training through augmentation and nearest-neighbour search. Using text-to-image retrieval as a benchmark, we evaluate the performance of these methods with variable sized training datasets of paired chest X-rays and radiological reports. Combined, they significantly improve retrieval compared to fine-tuning CLIP, roughly equivalent to training with the data. A similar pattern is found in the downstream task classification of CXR-related conditions with our method outperforming CLIP and also BioVIL, a strong CXR VLM benchmark, in the zero-shot and linear probing settings. We conclude with a set of recommendations for researchers aiming to train vision-language models on other medical imaging modalities when training data is scarce. To facilitate further research, we will make our code and models publicly available.",
        "authors": [
            "Rhydian Windsor",
            "A. Jamaludin",
            "T. Kadir",
            "Andrew Zisserman"
        ],
        "citations": 9,
        "references": 29,
        "year": 2023
    },
    {
        "title": "InstructDET: Diversifying Referring Object Detection with Generalized Instructions",
        "abstract": "We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are from foundation models. Our InDET is developed from existing REC datasets and object detection datasets, with the expanding potential that any image with object bbxs can be incorporated through using our InstructDET method. By using our InDET dataset, we show that a conventional ROD model surpasses existing methods on standard REC datasets and our InDET test set. Our data-centric method InstructDET, with automatic data expansion by leveraging foundation models, directs a promising field that ROD can be greatly diversified to execute common object detection instructions.",
        "authors": [
            "Ronghao Dang",
            "Jiangyan Feng",
            "Haodong Zhang",
            "Chongjian Ge",
            "Lin Song",
            "Lijun Gong",
            "Chengju Liu",
            "Qi Chen",
            "Feng Zhu",
            "Rui Zhao",
            "Yibing Song"
        ],
        "citations": 9,
        "references": 65,
        "year": 2023
    },
    {
        "title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning",
        "abstract": "Pre-trained vision-language models (VLMs) have achieved impressive results in a range of vision-language tasks. However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and latency constraints. In this work, we introduce a distilling then pruning framework to compress large vision-language models into smaller, faster, and more accurate ones. We first shrink the size of a pre-trained large VLM and apply knowledge distillation in the vision-language pre-training stage to obtain a task-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm to automatically infer the importance of vision and language modalities for different downstream tasks and adaptively remove redundant structures and neurons in different encoders with controllable target sparsity. We apply our framework to train EfficientVLM, a fast and accurate vision-language model consisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers, accounting for only 93 million parameters in total, which is 44.3% of the teacher model. EfficientVLM retains 98.4% performance of the teacher model and accelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute improvement over previous SoTA efficient VLMs of similar sizes by a large margin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2 (+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation (CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.",
        "authors": [
            "Tiannan Wang",
            "Wangchunshu Zhou",
            "Yan Zeng",
            "Xinsong Zhang"
        ],
        "citations": 28,
        "references": 83,
        "year": 2022
    },
    {
        "title": "Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models",
        "abstract": "Accurate video moment retrieval (VMR) requires universal visual-textual correlations that can handle unknown vocabulary and unseen scenes. However, the learned correlations are likely either biased when derived from a limited amount of moment-text data which is hard to scale up because of the prohibitive annotation cost (fully-supervised), or unreliable when only the video-text pairwise relationships are available without fine-grained temporal annotations (weakly-supervised). Recently, the vision-language models (VLM) demonstrate a new transfer learning paradigm to benefit different vision tasks through the universal visual-textual correlations derived from large-scale vision-language pairwise web data, which has also shown benefits to VMR by fine-tuning in the target domains.In this work, we propose a zero-shot method for adapting generalisable visual-textual priors from arbitrary VLM to facilitate moment-text alignment, without the need for accessing the VMR data. To this end, we devise a conditional feature refinement module to generate boundary-aware visual features conditioned on text queries to enable better moment boundary understanding. Additionally, we design a bottom-up proposal generation strategy that mitigates the impact of domain discrepancies and breaks down complex-query retrieval tasks into individual action retrievals, thereby maximizing the benefits of VLM. Extensive experiments conducted on three VMR benchmark datasets demonstrate the notable performance advantages of our zero-shot algorithm, especially in the novel-word and novel-location out-of-distribution setups.",
        "authors": [
            "Dezhao Luo",
            "Jiabo Huang",
            "Shaogang Gong",
            "Hailin Jin",
            "Yang Liu"
        ],
        "citations": 8,
        "references": 45,
        "year": 2023
    },
    {
        "title": "LONG DISTANCE LAB AFFAIRS: PHYSICS ACHIEVEMENT AND METACOGNITION EFFECTS OF DISTANCE LABORATORIES IN A SENIOR HIGH SCHOOL IN THE PHILIPPINES",
        "abstract": "Due to the necessity to continue learning even during the pandemic, schools opened utilizing distance learning modalities. However, there is a dearth of evidence on the effectivity of this modalities in physics. In this study, we investigated the effects of three physics distance learning modes; the module-only (MO), virtual lab plus module (VLM), and the physical lab plus module (PLM) classes in physics achievement and metacognition employing the pretest-posttest and repeated measures research designs. All learning modules used were in digital formats sent through free messaging platforms. Analysis of data includes paired samples t-test, one-way ANOVA, repeated measures ANOVA, and independent samples t-test. Results revealed that all three distance learning modes have significantly higher post-test than pre-test scores. Further analysis showed, however, that only VLM had significantly higher gain scores than MO. Initially, at pre-MO and post-MO administrations, male students had significantly higher metacognition but this diminished after they perform both virtual and physical labs. It was in post-PLM where students have significantly better metacognition than pre-MO and post-MO. This study showed that not only do physical and virtual labs supplement distance modular learning, they are also complementary that both must be used in distance learning.",
        "authors": [],
        "citations": 2,
        "references": 29,
        "year": 2023
    },
    {
        "title": "Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding",
        "abstract": "Vision language models (VLM) have demonstrated re-markable performance across various downstream tasks. However, understanding fine-grained visual-linguistic con-cepts, such as attributes and inter-object relationships, re-mains a significant challenge. While several benchmarks aim to evaluate VLMs in finer granularity, their primary fo-cus remains on the linguistic aspect, neglecting the visual dimension. Here, we highlight the importance of evaluating VLMs from both a textual and visual perspective. We intro-duce a progressive pipeline to synthesize images that vary in a specific attribute while ensuring consistency in all other aspects. Utilizing this data engine, we carefully design a benchmark, SPEC, to diagnose the comprehension of object size, position, existence, and count. Subsequently, we con-duct a thorough evaluation offour leading VLMs on SPEC. Surprisingly, their performance is close to random guess, revealing significant limitations. With this in mind, we pro-pose a simple yet effective approach to optimize VLMs in fine- grained understanding, achieving significant improve-ments on SPEC without compromising the zero-shot performance. Results on two additional fine-grained benchmarks also show consistent improvements, further validating the transferability of our approach. Code and data are available at https://github.com/wjpoom/SPEC.",
        "authors": [
            "Wujian Peng",
            "Sicheng Xie",
            "Zuyao You",
            "Shiyi Lan",
            "Zuxuan Wu"
        ],
        "citations": 8,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Retrieval-based Video Language Model for Efficient Long Video Question Answering",
        "abstract": "The remarkable natural language understanding, reasoning, and generation capabilities of large language models (LLMs) have made them attractive for application to video question answering (Video QA) tasks, utilizing video tokens as contextual input. However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video QA process. To address these issues, we introduce a simple yet effective retrieval-based video language model (R-VLM) for efficient and interpretable long video QA. Specifically, given a question (query) and a long video, our model identifies and selects the most relevant $K$ video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, eliminates noise interference, and enhances system performance. Our experimental results validate the effectiveness of our framework for comprehending long videos. Furthermore, based on the retrieved chunks, our model is interpretable that provides the justifications on where we get the answers.",
        "authors": [
            "Jiaqi Xu",
            "Cuiling Lan",
            "Wenxuan Xie",
            "Xuejin Chen",
            "Yan Lu"
        ],
        "citations": 6,
        "references": 24,
        "year": 2023
    },
    {
        "title": "Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Fine-grained Understanding",
        "abstract": "Current Vision and Language Models (VLMs) demonstrate strong performance across various vision-language tasks, yet they struggle with Ô¨Åne-grained understanding. This issue stems from weak image-caption alignment in pretraining datasets and a simpliÔ¨Åed contrastive objective that fails to distinguish nuanced grounding elements such as relations, actions, and attributes. As a result, the models tend to learn bag-of-words representations. To mitigate these challenges, we introduce an intra-modal contrastive loss and a unique cross-modal rank loss with an adaptive threshold that serves as curriculum learning, utilizing our automatically generated hard negatives to augment the model‚Äôs capacity. Our strategy, which does not necessitate additional annotations or parameters, can be incorporated into any VLM trained with an image-text contrastive loss. Upon application to CLIP, our method leads to signiÔ¨Åcant improvements on three Ô¨Åne-grained benchmarks, and it also enhances the performance of X-VLM, which is the state-of-art moodel on Ô¨Åne-grained reasoning. 1",
        "authors": [
            "Le Zhang",
            "Rabiul Awal",
            "Aishwarya Agrawal"
        ],
        "citations": 6,
        "references": 41,
        "year": 2023
    },
    {
        "title": "PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation",
        "abstract": "This paper proposes a cross-modal distillation frame-work, PartDistill, which transfers 2D knowledge from vision-language models (VLMs) to facilitate 3D shape part segmentation. PartDistill addresses three major challenges in this task: the lack of 3D segmentation in invisible or undetected regions in the 2D projections, inconsistent 2D predictions by VLMs, and the lack of knowledge accumu-lation across different 3D shapes. PartDistill consists of a teacher network that uses a VLM to make 2D predictions and a student network that learns from the 2D pre-dictions while extracting geometrical features from multi-ple 3D shapes to carry out 3D part segmentation. A bi-directional distillation, including forward and backward distillations, is carried out within the framework, where the former forward distills the 2D predictions to the student net-work, and the latter improves the quality of the 2D predictions, which subsequently enhances the final 3D segmen-tation. Moreover, PartDistill can exploit generative mod-els that facilitate effortless 3D shape creation for generating knowledge sources to be distilled. Through extensive experiments, PartDistill boosts the existing methods with substantial margins on widely used ShapeNetPart and Part-NetE datasets, by more than 15% and 12% higher mIoU scores, respectively. The code for this work is available at https://github.com/ardianumam/PartDistill.",
        "authors": [
            "Ardian Umam",
            "Cheng-Kun Yang",
            "Min-Hung Chen",
            "Jen-Hui Chuang",
            "Yen-Yu Lin"
        ],
        "citations": 6,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Sea level rise projections up to 2150 in the northern Mediterranean coasts",
        "abstract": "Vertical land movements (VLM) play a crucial role in affecting the sea level rise along the coasts. They need to be estimated and included in the analysis for more accurate Sea Level (SL) projections. Here we focus on the Mediterranean basin characterized by spatially variable rates of VLM that affect the future SL along the coasts. To estimate the VLM rates we used geodetic data from continuous global navigation satellite system stations with time series longer than 4.5 years in the 1996‚Äì2023 interval, belonging to Euro-Mediterranean networks and located within 5 km from the coast. Revised SL projections up to the year 2150 are provided at 265 points on a geographical grid and at the locations of 51 tide gauges of the Permanent Service for Mean Sea Level, by including the estimated VLM in the SL projections released by the Intergovernmental Panel on Climate Change (IPCC) in the AR6 Report. Results show that the IPCC projections underestimate future SL along the coasts of the Mediterranean Sea since the effects of tectonics and other local factors were not properly considered. Here we show that revised SL projections at 2100, when compared to the IPCC, show a maximum and minimum differences of 1094 ¬± 103 mm and ‚àí773 ¬± 106 mm, respectively, with an average value that exceeds by about 80 mm that of the IPCC in the reference Shared Socio-economic Pathways and different global warming levels. Finally, the projections indicate that about 19.000 km2 of the considered Mediterranean coasts will be more exposed to risk of inundation for the next decades, leading to enhanced impacts on the environment, human activities and infrastructures, thus suggesting the need for concrete actions to support vulnerable populations to adapt to the expected SL rise and coastal hazards by the end of this century.",
        "authors": [
            "A. Vecchio",
            "M. Anzidei",
            "E. Serpelloni"
        ],
        "citations": 8,
        "references": 78,
        "year": 2023
    },
    {
        "title": "Learning Domain-Aware Detection Head with Prompt Tuning",
        "abstract": "Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. However, existing methods focus on reducing the domain bias of the detection backbone by inferring a discriminative visual encoder, while ignoring the domain bias in the detection head. Inspired by the high generalization of vision-language models (VLMs), applying a VLM as the robust detection backbone following a domain-aware detection head is a reasonable way to learn the discriminative detector for each domain, rather than reducing the domain bias in traditional methods. To achieve the above issue, we thus propose a novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain. Formally, the domain-adaptive prompt consists of the domain-invariant tokens, domain-specific tokens, and the domain-related textual description along with the class label. Furthermore, two constraints between the source and target domains are applied to ensure that the domain-adaptive prompt can capture the domains-shared and domain-specific knowledge. A prompt ensemble strategy is also proposed to reduce the effect of prompt disturbance. Comprehensive experiments over multiple cross-domain adaptation tasks demonstrate that using the domain-adaptive prompt can produce an effectively domain-related detection head for boosting domain-adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Pro.",
        "authors": [
            "Haochen Li",
            "Rui Zhang",
            "Hantao Yao",
            "Xinkai Song",
            "Yifan Hao",
            "Yongwei Zhao",
            "Ling Li",
            "Yunji Chen"
        ],
        "citations": 8,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Video Action Recognition with Attentive Semantic Units",
        "abstract": "Visual-Language Models (VLMs) have significantly advanced video action recognition. Supervised by the semantics of action labels, recent works adapt the visual branch of VLMs to learn video representations. Despite the effectiveness proved by these works, we believe that the potential of VLMs has yet to be fully harnessed. In light of this, we exploit the semantic units (SU) hiding behind the action labels and leverage their correlations with fine-grained items in frames for more accurate action recognition. SUs are entities extracted from the language descriptions of the entire action set, including body parts, objects, scenes, and motions. To further enhance the alignments between visual contents and the SUs, we introduce a multi-region attention module (MRA) to the visual branch of the VLM. The MRA allows the perception of region-aware visual features beyond the original global feature. Our method adaptively attends to and selects relevant SUs with visual features of frames. With a cross-modal decoder, the selected SUs serve to decode spatiotemporal video representations. In summary, the SUs as the medium can boost discriminative ability and transferability. Specifically, in fully-supervised learning, our method achieved 87.8% top-1 accuracy on Kinetics-400. In K=2 few-shot experiments, our method surpassed the previous state-of-the-art by +7.1% and +15.0% on HMDB-51 and UCF-101, respectively.",
        "authors": [
            "Yifei Chen",
            "Dapeng Chen",
            "Ruijin Liu",
            "Hao Li",
            "Wei Peng"
        ],
        "citations": 8,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Reinforcement Learning Friendly Vision-Language Model for Minecraft",
        "abstract": null,
        "authors": [
            "Ziluo Ding",
            "Hao Luo",
            "Ke Li",
            "Junpeng Yue",
            "Tiejun Huang",
            "Zongqing Lu"
        ],
        "citations": 8,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation",
        "abstract": "Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in the medical analysis, lesions exhibit varying sizes and positions, and the fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explain-ability by using heatmaps to show the general potential image areas associated with texts rather than specific regions, making their explanations not explicit and specific enough. To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainabil-ity for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce the Adaptive Patch extraction (AdaPatch) module to acquire the adaptive patches for these regions adaptively. In order to provide explicit explainability for CXR-report generation task, we propose an AdaMatch-based bidirectional large language model for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords for CXR images and ‚Äòkeypatches‚Äô for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets prove the effectiveness of our method and its superior performance to existing methods.",
        "authors": [
            "Wenting Chen",
            "Linlin Shen",
            "Xiang Li",
            "Yixuan Yuan"
        ],
        "citations": 6,
        "references": 53,
        "year": 2023
    },
    {
        "title": "TAP: Targeted Prompting for Task Adaptive Generation of Textual Training Instances for Visual Classification",
        "abstract": "Vision and Language Models (VLMs), such as CLIP, have enabled visual recognition of a potentially unlimited set of categories described by text prompts. However, for the best visual recognition performance, these models still require tuning to better fit the data distributions of the downstream tasks, in order to overcome the domain shift from the web-based pre-training data. Recently, it has been shown that it is possible to effectively tune VLMs without any paired data, and in particular to effectively improve VLMs visual recognition performance using text-only training data generated by Large Language Models (LLMs). In this paper, we dive deeper into this exciting text-only VLM training approach and explore ways it can be significantly further improved taking the specifics of the downstream task into account when sampling text data from LLMs. In particular, compared to the SOTA text-only VLM training approach, we demonstrate up to 8.4% performance improvement in (cross) domain-specific adaptation, up to 8.7% improvement in fine-grained recognition, and 3.1% overall average improvement in zero-shot classification compared to strong baselines.",
        "authors": [
            "M. J. Mirza",
            "Leonid Karlinsky",
            "Wei Lin",
            "Horst Possegger",
            "Rog√©rio Feris",
            "Horst Bischof"
        ],
        "citations": 6,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Localized uplift, widespread subsidence, and implications for sea level rise in the New York City metropolitan area",
        "abstract": "Regional relative sea level rise is exacerbating flooding hazards in the coastal zone. In addition to changes in the ocean, vertical land motion (VLM) is a driver of spatial variation in sea level change that can either diminish or enhance flood risk. Here, we apply state-of-the-art interferometric synthetic aperture radar and global navigation satellite system time series analysis to estimate velocities and corresponding uncertainties at 30-m resolution in the New York City metropolitan area, revealing VLM with unprecedented detail. We find broad subsidence of 1.6 mm/year, consistent with glacial isostatic adjustment to the melting of the former ice sheets, and previously undocumented hot spots of both subsidence and uplift that can be physically explained in some locations. Our results inform ongoing efforts to adapt to sea level rise and reveal points of VLM that motivate both future scientific investigations into surface geology and assessments of engineering projects.",
        "authors": [
            "B. Buzzanga",
            "D. Bekaert",
            "B. Hamlington",
            "Robert E. Kopp",
            "Marin Govorcin",
            "Kenneth G. Miller"
        ],
        "citations": 6,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation",
        "abstract": "Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in the medical analysis, lesions exhibit varying sizes and positions, and the fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explain-ability by using heatmaps to show the general potential image areas associated with texts rather than specific regions, making their explanations not explicit and specific enough. To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainabil-ity for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce the Adaptive Patch extraction (AdaPatch) module to acquire the adaptive patches for these regions adaptively. In order to provide explicit explainability for CXR-report generation task, we propose an AdaMatch-based bidirectional large language model for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords for CXR images and ‚Äòkeypatches‚Äô for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets prove the effectiveness of our method and its superior performance to existing methods.",
        "authors": [
            "Wenting Chen",
            "Linlin Shen",
            "Xiang Li",
            "Yixuan Yuan"
        ],
        "citations": 6,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Meta-Personalizing Vision-Language Models to Find Named Instances in Video",
        "abstract": "Large-scale vision-language models (VLM) have shown impressive results for language-guided search applications. While these models allow category-level queries, they currently struggle with personalized searches for moments in a video where a specific object instance such as ‚ÄúMy dog Biscuit‚Äù appears. We present the following three contributions to address this problem. First, we describe a method to meta-personalize a pre-trained VLM, i.e., learning how to learn to personalize a VLM at test time to search in video. Our method extends the VLM's token vocabulary by learning novel word embeddings specific to each instance. To capture only instance-specific features, we represent each instance embedding as a combination of shared and learned global category features. Second, we propose to learn such personalization without explicit human supervision. Our approach automatically identifies moments of named visual instances in video using transcripts and vision-language similarity in the VLM's embedding space. Finally, we introduce This-Is-My, a personal video instance retrieval benchmark. We evaluate our approach on This-Is-My and Deep-Fashion2 and show that we obtain a 15% relative improvement over the state of the art on the latter dataset.",
        "authors": [
            "Chun-Hsiao Yeh",
            "Bryan C. Russell",
            "Josef Sivic",
            "Fabian Caba Heilbron",
            "S. Jenni"
        ],
        "citations": 6,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Vision Language Models in Autonomous Driving: A Survey and Outlook",
        "abstract": "The applications of Vision-Language Models (VLMs) in the field of Autonomous Driving (AD) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By incorporating language data, driving systems can gain a better understanding of real-world environments, thereby enhancing driving safety and efficiency. In this work, we present a comprehensive and systematic survey of the advances in vision language models in this domain, encompassing perception and understanding, navigation and planning, decision-making and control, end-to-end autonomous driving, and data generation. We introduce the mainstream VLM tasks in AD and the commonly utilized metrics. Additionally, we review current studies and applications in various areas and summarize the existing language-enhanced autonomous driving datasets thoroughly. Lastly, we discuss the benefits and challenges of VLMs in AD and provide researchers with the current research gaps and future trends.",
        "authors": [
            "Xingcheng Zhou",
            "Mingyu Liu",
            "Ekim Yurtsever",
            "B. L. ≈Ωagar",
            "Walter Zimmer",
            "Hu Cao",
            "Alois C. Knoll"
        ],
        "citations": 7,
        "references": 173,
        "year": 2023
    },
    {
        "title": "Disruptive Role of Vertical Land Motion in Future Assessments of Climate Change‚ÄêDriven Sea‚ÄêLevel Rise and Coastal Flooding Hazards in the Chesapeake Bay",
        "abstract": "Future projections of sea‚Äêlevel rise (SLR) used to assess coastal flooding hazards and exposure throughout the 21st century and devise risk mitigation efforts often lack an accurate estimate of coastal vertical land motion (VLM) rate, driven by anthropogenic or non‚Äêclimate factors in addition to climatic factors. The Chesapeake Bay (CB) region of the United States is experiencing one of the fastest rates of relative sea‚Äêlevel rise on the Atlantic coast of the United States. This study uses a combination of space‚Äêborne Interferometric Synthetic Aperture Radar (InSAR), Global Navigation Satellite System (GNSS), Light Detecting and Ranging (LiDAR) data sets, available National Oceanic and Atmospheric Administration (NOAA) long‚Äêterm tide gauge data, and SLR projections from the Intergovernmental Panel on Climate Change (IPCC), AR6 WG1 to quantify the regional rate of relative SLR and future flooding hazards for the years 2030, 2050, and 2100. By the year 2100, the total inundated areas from SLR and subsidence are projected to be 454(316‚Äì549)‚Äì600(535‚Äì690) km2 ${\\mathrm{k}\\mathrm{m}}^{2}$ for Shared Socioeconomic Pathways (SSPs) 1‚Äì1.9 to 5‚Äì8.5, respectively, and 342(132‚Äì552)‚Äì627(526‚Äì735) km2 ${\\mathrm{k}\\mathrm{m}}^{2}$ only from SLR. The effect of storm surges based on Hurricane Isabel can increase the inundated area to 849(832‚Äì867)‚Äì1,117(1,054‚Äì1,205) km2 under different VLM and SLR scenarios. We suggest that accurate estimates of VLM rate, such as those obtained here, are essential to revise IPCC projections and obtain accurate maps of coastal flooding and inundation hazards. The results provided here inform policymakers when assessing hazards associated with global climate changes and local factors in CB, required for developing risk management and disaster resilience plans.",
        "authors": [
            "S. F. Sherpa",
            "M. Shirzaei",
            "Chandrakanta Ojha"
        ],
        "citations": 7,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models",
        "abstract": "Utilizing vision and language models (VLMs) pre-trained on large-scale image-text pairs is becoming a promising paradigm for open-vocabulary visual recognition. In this work, we extend this paradigm by leveraging motion and audio that naturally exist in video. We present \\textbf{MOV}, a simple yet effective method for \\textbf{M}ultimodal \\textbf{O}pen-\\textbf{V}ocabulary video classification. In MOV, we directly use the vision encoder from pre-trained VLMs with minimal modifications to encode video, optical flow and audio spectrogram. We design a cross-modal fusion mechanism to aggregate complimentary multimodal information. Experiments on Kinetics-700 and VGGSound show that introducing flow or audio modality brings large performance gains over the pre-trained VLM and existing methods. Specifically, MOV greatly improves the accuracy on base classes, while generalizes better on novel classes. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video classification benchmarks, significantly outperforming both traditional zero-shot methods and recent methods based on VLMs. Code and models will be released.",
        "authors": [
            "Rui Qian",
            "Yeqing Li",
            "Zheng Xu",
            "Ming Yang",
            "Serge J. Belongie",
            "Yin Cui"
        ],
        "citations": 22,
        "references": 82,
        "year": 2022
    },
    {
        "title": "Performances of Dried Blood Spots and Point-of-Care Devices to Identify Virological Failure in HIV-Infected Patients: A Systematic Review and Meta-Analysis.",
        "abstract": "To broaden access to HIV viral load monitoring (VLM), the use of blood samples from dried blood spots (DBS) or point-of-care (POC) devices, could be of great help in settings where plasma is not easily accessible. The variety of assays available makes the choice complex. This systematic review and meta-analysis aims to estimate the sensitivity and specificity of DBS and POC devices to identify patients in virological failure using World Health Organization (WHO) recommendations (viral load ‚â•1000 copies/mL), compared with plasma, for the assays currently available. Four databases were searched for articles, and two reviewers independently identified articles reporting sensitivity and specificity of DBS and/or POC to identify patients in virological failure. We excluded articles that used other thresholds as well as articles with a total number of participants below 50 to avoid reporting bias. Heterogeneity and factors associated with assays' performances were assessed by I2 statistics and metaregression. The protocol of this review follows the PRISMA guidelines. Out of 941 articles, 47 were included: 32 DBS evaluations and 16 POC evaluations. Overall, when using DBS, the Abbott RT HIV-1, Roche CAP-CTM, NucliSENS BioMerieux and Aptima assays presented sensitivity and specificity exceeding 85%, but reported results were highly heterogeneous. Factors associated with better performances were high volume of blood and the use of the same assay for DBS and plasma VLM. Regarding the POC devices, SAMBA I, SAMBA II, and GeneXpert devices presented high sensitivity and specificity exceeding 90%, with less heterogeneity. DBS is suitable VLM, but performances can vary greatly depending on the protocols, and should be performed in trained centers. POC is suitable for VLM with less risk of heterogeneity but is more intensive in costs and logistics.",
        "authors": [
            "Liem Binh Luong Nguyen",
            "Abou Aissata Soumah",
            "V. Hoang",
            "Anh Tuan Nguyen",
            "T. H. Pham",
            "Sandrine Royer-Devaux",
            "Y. Madec"
        ],
        "citations": 5,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Suprapontine Structures Modulate Brainstem and Spinal Networks",
        "abstract": null,
        "authors": [
            "Atiyeh Mohammadshirazi",
            "Rosamaria Apicella",
            "Benjam√≠n A. Zylberberg",
            "G. Mazzone",
            "G. Taccola"
        ],
        "citations": 4,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Geometrically-Driven Aggregation for Zero-Shot 3D Point Cloud Understanding",
        "abstract": "Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language Models (VLMs). Existing strategies directly map VLM representations from 2D pixels of rendered or captured views to 3D points, overlooking the inherent and expressible point cloud geometric structure. Geometrically similar or close regions can be exploited for bolstering point cloud understanding as they are likely to share semantic information. To this end, we introduce the first training-free aggregation technique that leverages the point cloud's 3D geometric structure to improve the quality of the transferred VLM representations. Our approach operates iteratively, performing local-to-global aggregation based on geometric and semantic point-level reasoning. We benchmark our approach on three downstream tasks, including classification, part segmentation, and semantic segmentation, with a variety of datasets representing both synthetic/real-world, and indoor/outdoor scenarios. Our approach achieves new state-of-the-art results in all benchmarks. Code and dataset are available at https://luigiriz.github.io/geoze-website/",
        "authors": [
            "Guofeng Mei",
            "Luigi Riz",
            "Yiming Wang",
            "Fabio Poiesi"
        ],
        "citations": 5,
        "references": 40,
        "year": 2023
    },
    {
        "title": "A 3-D Printed Ultra-Wideband Achromatic Metalens Antenna",
        "abstract": "Lens antennas, which can transform the incident spherical wavefronts to planar above the radiating aperture, have attracted increasing attention due to their simple structure and high gain performance. Nevertheless, conventional lens antennas face with the problem of dispersion effects, which hinders their applications in large bandwidth and multi-channel communications. In this article, we propose a millimeter-wave achromatic metalens antenna using three-dimensional (3D) printing technology to reduce the dispersion effect and enlarge its bandwidth. The proposed ultra-wideband achromatic metalens antenna consists of a convex-liked metalens (VLM) and a concave-liked metalens (CLM) integrated as a metalens group. The VLM is designed on the basis of dielectric posts with different heights. The calculated transmission phase (from 0 to $2\\pi$ ) of VLM can be realized by changing the height of dielectric posts. The CLM consists of discrete variable-width dielectric posts with two different heights to achieve desired transmission phase. Measured results demonstrate that the maximum realized gain is 23.27 dBi, and the return loss is smaller than ‚àí15 dB within the whole operating bandwidth. More importantly, a broad 3-dB gain bandwidth of more than 68.4% has been achieved to cover nearly the entire V and W bands, ranging from 50 to 102 GHz.",
        "authors": [
            "Yu-Xuan Xie",
            "Gengbo Wu",
            "Wenhui Deng",
            "Shuyan Zhu",
            "C. Chan"
        ],
        "citations": 5,
        "references": 30,
        "year": 2023
    },
    {
        "title": "The SAVEMEDCOASTS-2 webGIS: The Online Platform for Relative Sea Level Rise and Storm Surge Scenarios up to 2100 for the Mediterranean Coasts",
        "abstract": "Here we show the SAVEMEDCOASTS-2 web-based geographic information system (webGIS) that supports land planners and decision makers in considering the ongoing impacts of Relative Sea Level Rise (RSLR) when formulating and prioritizing climate-resilient adaptive pathways for the Mediterranean coasts. The webGIS was developed within the framework of the SAVEMEDCOASTS and SAVEMEDCOASTS-2 projects, funded by the European Union, which respond to the need to protect people and assets from natural disasters along the Mediterranean coasts that are vulnerable to the combined effects of Sea Level Rise (SLR) and Vertical Land Movements (VLM). The geospatial data include available or new high-resolution Digital Terrain Models (DTM), bathymetric data, rates of VLM, and multi-temporal coastal flooding scenarios for 2030, 2050, and 2100 with respect to 2021, as a consequence of RSLR. The scenarios are derived from the 5th Assessment Report (AR5) provided by the Intergovernmental Panel on Climate Change (IPCC) and encompass different Representative Concentration Pathways (RCP2.6 and RCP8.5) for climate projections. The webGIS reports RSLR scenarios that incorporate the temporary contribution of both the highest astronomical tides (HAT) and storm surges (SS), which intensify risks to the coastal infrastructure, local community, and environment.",
        "authors": [
            "Antonio Falciano",
            "M. Anzidei",
            "Michele Greco",
            "M. Trivigno",
            "A. Vecchio",
            "C. Georgiadis",
            "P. Patias",
            "M. Crosetto",
            "J. Navarro",
            "E. Serpelloni",
            "C. Tolomei",
            "G. Martino",
            "Giuseppe Mancino",
            "F. Arbia",
            "C. Bignami",
            "F. Doumaz"
        ],
        "citations": 5,
        "references": 95,
        "year": 2023
    },
    {
        "title": "Review of vortex lattice method for supersonic aircraft design",
        "abstract": "\n There has been a renewed interest in developing environmentally friendly, economically viable, and technologically feasible supersonic transport aircraft and reduced order modeling methods can play an important contribution in accelerating the design process of these future aircraft. This paper reviews the use of the vortex lattice method (VLM) in modeling the general aerodynamics of subsonic and supersonic aircraft. The historical overview of the vortex lattice method is reviewed which indicates the use of this method for over a century for development and advancements in the aerodynamic analysis of subsonic and supersonic aircraft. The preference of VLM over other potential flow-solvers is because of its low order highly efficient computational analysis which is quick and efficient. Developments in VLM covering steady, unsteady state, linear and non-linear aerodynamic characteristics for different wing planform for the purpose of several different types of design optimisation is reviewed. For over a decade classical vortex lattice method has been used for multi-objective optimisation studies for commercial aircraft and unmanned aerial vehicle‚Äôs aerodynamic performance optimisation. VLM was one of the major potential flow solvers for studying the aerodynamic and aeroelastic characteristics of many wings and aircraft for NASA‚Äôs supersonic transport mission (SST). VLM is a preferred means for solving large numbers of computational design parameters in less time, more efficiently, and cheaper when compared to conventional CFD analysis which lends itself more to detailed study and solving the more challenging configuration and aerodynamic features of civil supersonic transport.",
        "authors": [
            "H. Joshi",
            "P. Thomas"
        ],
        "citations": 4,
        "references": 153,
        "year": 2023
    },
    {
        "title": "Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study",
        "abstract": "Most approaches to cross-modal retrieval (CMR) focus either on object-centric datasets, meaning that each document depicts or describes a single object, or on scene-centric datasets, meaning that each image depicts or describes a complex scene that involves multiple objects and relations between them. We posit that a robust CMR model should generalize well across both dataset types. Despite recent advances in CMR, the reproducibility of the results and their generalizability across different dataset types has not been studied before. We address this gap and focus on the reproducibility of the state-of-the-art CMR results when evaluated on object-centric and scene-centric datasets. We select two state-of-the-art CMR models with different architectures: (i) CLIP; and (ii) X-VLM. Additionally, we select two scene-centric datasets, and three object-centric datasets, and determine the relative performance of the selected models on these datasets. We focus on reproducibility, replicability, and generalizability of the outcomes of previously published CMR experiments. We discover that the experiments are not fully reproducible and replicable. Besides, the relative performance results partially generalize across object-centric and scene-centric datasets. On top of that, the scores obtained on object-centric datasets are much lower than the scores obtained on scene-centric datasets. For reproducibility and transparency we make our source code and the trained models publicly available.",
        "authors": [
            "Mariya Hendriksen",
            "Svitlana Vakulenko",
            "E. Kuiper",
            "M. de Rijke"
        ],
        "citations": 5,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Relative Sea Level Trends for the Coastal Areas of Peninsular and East Malaysia Based on Remote and In Situ Observations",
        "abstract": "Absolute sea-level rise has become an important topic globally due to climate change. In addition, relative sea-level rise due to the vertical land motion in coastal areas can have a big societal impact. Vertical land motion (VLM) in Southeast Asia includes a tectonically induced component: uplift and subsidence in plate boundary zones where both Peninsular and East Malaysia are located. In this paper, the relative sea-level trends and (seismic cycle-induced) temporal changes across Malaysia were investigated. To do so, the data (1984‚Äì2019) from 21 tide gauges were analyzed, along with a subset (1994‚Äì2021) of nearby Malaysian GNSS stations. Changes in absolute sea level (ASL) at these locations (1992‚Äì2021) were also estimated from satellite altimetry data. As a first for Peninsular and East Malaysia, the combination ASL minus VLM was robustly used to validate relative sea-level rise from tide-gauge data and provide relative sea-level trend estimates based on a common data period of 25+ years. A good match between both the remote and in situ sea-level rise estimations was observed, especially for Peninsular Malaysia (differences < 1 mm/year), when split trends were estimated from the tide gauges and GNSS time series to distinguish between the different VLM regimes that exist due to the 2004 Sumatra‚ÄìAndaman megathrust earthquake. As in the south of Thailand, post-seismic-induced negative VLM has increased relative sea-level rise by 2‚Äì3 mm/year along the Andaman Sea and Malacca Strait coastlines since 2005. For East Malaysia, the validation shows higher differences (bias of 2‚Äì3 mm/year), but this poorer match is significantly improved by either not including data after 1 January 2014 or applying a generic jump to all East Malay tide gauges from that date onwards. Overall, the present relative sea-level trends range from 4 to 6 mm/year for Malaysia with a few regions showing up to 9 mm/year due to human-induced land subsidence.",
        "authors": [
            "W. Simons",
            "M. Naeije",
            "Zaki Ghazali",
            "Wan Darani Rahman",
            "Sanusi Cob",
            "M. Kadir",
            "Asrul Mustafar",
            "Ami Hassan Md Din",
            "J. Efendi",
            "P. Noppradit"
        ],
        "citations": 4,
        "references": 0,
        "year": 2023
    },
    {
        "title": "20thto 21stCentury Relative Sea and Land LevelChanges in Northern California:Tectonic Land Level Changes and theirContribution to Sea-Level Rise, Humboldt BayRegion, Northern California",
        "abstract": "Sea-level changes are modulated in coastal northern California by land-level changes due to the earthquake cycle along the Cascadia subduction zone, the San Andreas plate boundary fault system, and crustal faults. Sea-level rise (SLR) subjects ecological and anthropogenic infrastructure to increased vulnerability to changes in habitat and increased risk for physical damage. The degree to which each of these forcing factors drives this modulation is poorly resolved. We use NOAA tide gage data and ‚Äòcampaign‚Äô tide gage deployments, Global Navigation Satellite System (GNSS) data, and National Geodetic Survey (NGS) first-order levelling data to calculate vertical land motion (VLM) rates in coastal northern California. Sea-level observations, highway level surveys, and GNSS data all confirm that land is subsiding in Humboldt Bay, in contrast to Crescent City where the land is rising. Subtracting absolute sea-level rate (~1.99 mm/year) from Crescent City (CC) and North Spit (NS) gage relative sea-level rates reveals that CC is uplifting at ~2.83 mm/year and NS is subsiding at ~3.21mm/year. GNSS vertical deformation reveals similar rates of ~2.60 mm/year of uplift at Crescent City. In coastal northern California, there is an E-W trending variation in vertical land motion that is primarily due to Cascadia megathrust fault seismogenic coupling. This interseismic subsidence also dominates the N-S variation in vertical land motion in most of the study region. There exists a second-order heterogeneous N-S trend in vertical land motion that we associate to crustal fault-related strain. There may be non-tectonic contributions to the observed VLM rates.",
        "authors": [
            "J. Patton",
            "T. Williams",
            "Jeffrey L. Anderson",
            "M. Hemphill-Haley",
            "R. Burgette",
            "Ray Weldon II",
            "R. McPherson",
            "T. Leroy"
        ],
        "citations": 4,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors",
        "abstract": "Human-object interaction (HOI) detection aims to extract interacting human-object pairs and their interaction categories from a given natural image. Even though the labeling effort required for building HOI detection datasets is inherently more extensive than for many other computer vision tasks, weakly-supervised directions in this area have not been sufficiently explored due to the difficulty of learning human-object interactions with weak supervision, rooted in the combinatorial nature of interactions over the object and predicate space. In this paper, we tackle HOI detection with the weakest supervision setting in the literature, using only image-level interaction labels, with the help of a pretrained vision-language model (VLM) and a large language model (LLM). We first propose an approach to prune non-interacting human and object proposals to increase the quality of positive pairs within the bag, exploiting the grounding capability of the vision-language model. Second, we use a large language model to query which interactions are possible between a human and a given object category, in order to force the model not to put emphasis on unlikely interactions. Lastly, we use an auxiliary weakly-supervised preposition prediction task to make our model explicitly reason about space. Extensive experiments and ablations show that all of our contributions increase HOI detection performance.",
        "authors": [
            "Mesut Erhan Unal",
            "Adriana Kovashka"
        ],
        "citations": 4,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Toward Grounded Commonsense Reasoning",
        "abstract": "Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the \"tidying.\" How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded commonsense reasoning. To evaluate our framework at scale, we release the MessySurfaces dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MessySurfaces benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/grounded_commonsense_reasoning/.",
        "authors": [
            "Minae Kwon",
            "Hengyuan Hu",
            "Vivek Myers",
            "Siddharth Karamcheti",
            "A. Dragan",
            "Dorsa Sadigh"
        ],
        "citations": 5,
        "references": 54,
        "year": 2023
    },
    {
        "title": "High-Fat Diet Modulates the Excitability of Neurons within the Brain‚ÄìLiver Pathway",
        "abstract": "Stimulation of hepatic sympathetic nerves increases glucose production and glycogenolysis. Activity of pre-sympathetic neurons in the paraventricular nucleus (PVN) of the hypothalamus and in the ventrolateral and ventromedial medulla (VLM/VMM) largely influence the sympathetic output. Increased activity of the sympathetic nervous system (SNS) plays a role in the development and progression of metabolic diseases; however, despite the importance of the central circuits, the excitability of pre-sympathetic liver-related neurons remains to be determined. Here, we tested the hypothesis that the activity of liver-related neurons in the PVN and VLM/VMM is altered in diet-induced obese mice, as well as their response to insulin. Patch-clamp recordings were conducted from liver-related PVN neurons, VLM-projecting PVN neurons, and pre-sympathetic liver-related neurons in the ventral brainstem. Our data demonstrate that the excitability of liver-related PVN neurons increased in high-fat diet (HFD)-fed mice compared to mice fed with control diet. Insulin receptor expression was detected in a population of liver-related neurons, and insulin suppressed the firing activity of liver-related PVN and pre-sympathetic VLM/VMM neurons in HFD mice; however, it did not affect VLM-projecting liver-related PVN neurons. These findings further suggest that HFD alters the excitability of pre-autonomic neurons as well as their response to insulin.",
        "authors": [
            "Adrien J R Molinas",
            "Lucie D. Desmoulins",
            "Roslyn Davis",
            "Hong Gao",
            "Ryousuke Satou",
            "A. Derbenev",
            "A. Zsombok"
        ],
        "citations": 4,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Zero-Shot Image Harmonization with Generative Model Prior",
        "abstract": "We propose a zero-shot approach to image harmonization, aiming to overcome the reliance on large amounts of synthetic composite images in existing methods. These methods, while showing promising results, involve significant training expenses and often struggle with generalization to unseen images. To this end, we introduce a fully modularized framework inspired by human behavior. Leveraging the reasoning capabilities of recent foundation models in language and vision, our approach comprises three main stages. Initially, we employ a pretrained vision-language model (VLM) to generate descriptions for the composite image. Subsequently, these descriptions guide the foreground harmonization direction of a text-to-image generative model (T2I). We refine text embeddings for enhanced representation of imaging conditions and employ self-attention and edge maps for structure preservation. Following each harmonization iteration, an evaluator determines whether to conclude or modify the harmonization direction. The resulting framework, mirroring human behavior, achieves harmonious results without the need for extensive training. We present compelling visual results across diverse scenes and objects, along with a user study validating the effectiveness of our approach.",
        "authors": [
            "Jianqi Chen",
            "Zhengxia Zou",
            "Yilan Zhang",
            "Keyan Chen",
            "Z. Shi"
        ],
        "citations": 4,
        "references": 76,
        "year": 2023
    },
    {
        "title": "ANSEL Photobot: A Robot Event Photographer with Semantic Intelligence",
        "abstract": "Our work examines the way in which large language models can be used for robotic planning and sampling in the context of automated photographic documentation. Specifically, we illustrate how to produce a photo-taking robot with an exceptional level of semantic awareness by leveraging recent advances in general purpose language (LM) and vision-language (VLM) models. Given a high-level description of an event we use an LM to generate a natural-language list of photo descriptions that one would expect a photographer to capture at the event. We then use a VLM to identify the best matches to these descriptions in the robot's video stream. The photo portfolios generated by our method are consistently rated as more appropriate to the event by human evaluators than those generated by existing methods.",
        "authors": [
            "D. Rivkin",
            "Gregory Dudek",
            "Nikhil Kakodkar",
            "D. Meger",
            "Oliver Limoyo",
            "Xue Liu",
            "F. Hogan"
        ],
        "citations": 5,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Interannual variability of vertical land motion over High Mountain Central Asia from GPS and GRACE/GRACE-FO observations",
        "abstract": null,
        "authors": [
            "Yuanjin Pan",
            "Weiping Jiang",
            "H. Ding",
            "C. Shum",
            "J. Jiao",
            "Yixin Xiao",
            "Qiwen Wu"
        ],
        "citations": 4,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Text Descriptions are Compressive and Invariant Representations for Visual Learning",
        "abstract": "Modern image classification is based upon directly predicting classes via large discriminative networks, which do not directly contain information about the intuitive visual features that may constitute a classification decision. Recently, work in vision-language models (VLM) such as CLIP has provided ways to specify natural language descriptions of image classes, but typically focuses on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, in line with humans' understanding of multiple visual features per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we introduce a novel method, \\textit{SLR-AVD (Sparse Logistic Regression using Augmented Visual Descriptors)}. This method first automatically generates multiple visual descriptions of each class via a large language model (LLM), then uses a VLM to translate these descriptions to a set of visual feature embeddings of each image, and finally uses sparse logistic regression to select a relevant subset of these features to classify each image. Core to our approach is the fact that, information-theoretically, these descriptive features are more invariant to domain shift than traditional image embeddings, even though the VLM training process is not explicitly designed for invariant representation learning. These invariant descriptive features also compose a better input compression scheme. When combined with finetuning, we show that SLR-AVD is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.",
        "authors": [
            "Zhili Feng",
            "Anna Bair",
            "J. Z. Kolter"
        ],
        "citations": 5,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Prompt Algebra for Task Composition",
        "abstract": "We investigate whether prompts learned independently for different tasks can be later combined through prompt algebra to obtain a model that supports composition of tasks. We consider Visual Language Models (VLM) with prompt tuning as our base classifier and formally define the notion of prompt algebra. We propose constrained prompt tuning to improve performance of the composite classifier. In the proposed scheme, prompts are constrained to appear in the lower dimensional subspace spanned by the basis vectors of the pre-trained vocabulary. Further regularization is added to ensure that the learned prompt is grounded correctly to the existing pre-trained vocabulary. We demonstrate the effectiveness of our method on object classification and object-attribute classification datasets. On average, our composite model obtains classification accuracy within 2.5% of the best base model. On UTZappos it improves classification accuracy over the best base model by 8.45% on average.",
        "authors": [
            "Pramuditha Perera",
            "Matthew Trager",
            "L. Zancato",
            "A. Achille",
            "S. Soatto"
        ],
        "citations": 5,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification",
        "abstract": "Vision-Language Models (VLMs) such as CLIP are trained on large amounts of image-text pairs, resulting in remarkable generalization across several data distributions. However, in several cases, their expensive training and data collection/curation costs do not justify the end application. This motivates a vendor-client paradigm, where a vendor trains a large-scale VLM and grants only input-output access to clients on a pay-per-query basis in a black-box setting. The client aims to minimize inference cost by distilling the VLM to a student model using the limited available task-specific data, and further deploying this student model in the downstream application. While naive distillation largely improves the In-Domain (ID) accuracy of the student, it fails to transfer the superior out-of-distribution (OOD) generalization of the VLM teacher using the limited available labeled images. To mitigate this, we propose Vision-Language to Vision - Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and language modalities of the teacher model with the vision modality of a pre-trained student model, and further distills the aligned VLM representations to the student. This maximally retains the pre-trained features of the student, while also incorporating the rich representations of the VLM image encoder and the superior generalization of the text embeddings. The proposed approach achieves state-of-the-art results on the standard Domain Generalization benchmarks in a black-box teacher setting as well as a white-box setting where the weights of the VLM are accessible. Project page: http://val.cds.iisc.ac.in/VL2V-ADiP/",
        "authors": [
            "Sravanti Addepalli",
            "Ashish Ramayee Asokan",
            "Lakshay Sharma",
            "R. Babu"
        ],
        "citations": 3,
        "references": 66,
        "year": 2023
    },
    {
        "title": "New vision-incorporated third-generation video laryngeal mask airways for intubation of patients in prone position",
        "abstract": null,
        "authors": [
            "T. Gaszy≈Ñski"
        ],
        "citations": 3,
        "references": 7,
        "year": 2023
    },
    {
        "title": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering",
        "abstract": "Recently, to comprehensively improve Vision Language Models (VLMs) for Visual Question Answering (VQA), several methods have been proposed to further reinforce the inference capabilities of VLMs to independently tackle VQA tasks rather than some methods that only utilize VLMs as aids to Large Language Models (LLMs). However, these methods ignore the rich common-sense knowledge inside the given VQA image sampled from the real world. Thus, they cannot fully use the powerful VLM for the given VQA question to achieve optimal performance. Attempt to overcome this limitation and inspired by the human top-down reasoning process, i.e., systematically exploring relevant issues to derive a comprehensive answer, this work introduces a novel, explainable multi-agent collaboration framework by leveraging the expansive knowledge of Large Language Models (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our framework comprises three agents, i.e., Responder, Seeker, and Integrator, to collaboratively answer the given VQA question by seeking its relevant issues and generating the final answer in such a top-down reasoning process. The VLM-based Responder agent generates the answer candidates for the question and responds to other relevant issues. The Seeker agent, primarily based on LLM, identifies relevant issues related to the question to inform the Responder agent and constructs a Multi-View Knowledge Base (MVKB) for the given visual scene by leveraging the build-in world knowledge of LLM. The Integrator agent combines knowledge from the Seeker agent and the Responder agent to produce the final VQA answer. Extensive and comprehensive evaluations on diverse VQA datasets with a variety of VLMs demonstrate the superior performance and interpretability of our framework over the baseline method in the zero-shot setting without extra training cost.",
        "authors": [
            "Zeqing Wang",
            "Wentao Wan",
            "Runmeng Chen",
            "Qiqing Lao",
            "Minjie Lang",
            "Keze Wang"
        ],
        "citations": 3,
        "references": 93,
        "year": 2023
    },
    {
        "title": "Hepatic Innervations and Nonalcoholic Fatty Liver Disease",
        "abstract": "Abbreviations graphical abstract: VMN/PVN, hypothalamic ventromedial nucleus/paraventricular nucleus; VLM/VMM, ventrolateral medulla/ventromedial medulla; SMG/CG, superior mesenteric ganglion/caeliac ganglia; NTS, nucleus of the solitary tract; NG, nodose ganglion. Nonalcoholic fatty liver disease (NAFLD) is the most common chronic liver disorder. Increased sympathetic (noradrenergic) nerve tone has a complex role in the etiopathomechanism of NAFLD, affecting the development/progression of steatosis, inflammation, fibrosis, and liver hemodynamical alterations. Also, lipid sensing by vagal afferent fibers is an important player in the development of hepatic steatosis. Moreover, disorganization and progressive degeneration of liver sympathetic nerves were recently described in human and experimental NAFLD. These structural alterations likely come along with impaired liver sympathetic nerve functionality and lack of adequate hepatic noradrenergic signaling. Here, we first overview the anatomy and physiology of liver nerves. Then, we discuss the nerve impairments in NAFLD and their pathophysiological consequences in hepatic metabolism, inflammation, fibrosis, and hemodynamics. We conclude that further studies considering the spatial-temporal dynamics of structural and functional changes in the hepatic nervous system may lead to more targeted pharmacotherapeutic advances in NAFLD.",
        "authors": [
            "M. √Ådori",
            "S. Bhat",
            "R. Gramignoli",
            "Ismael Valladolid-Acebes",
            "T. Bengtsson",
            "Mathias Uhl√©n",
            "C. Adori"
        ],
        "citations": 3,
        "references": 158,
        "year": 2023
    },
    {
        "title": "C-SAW: Self-Supervised Prompt Learning for Image Generalization in Remote Sensing",
        "abstract": "We focus on domain and class generalization problems in analyzing optical remote sensing images, using the large-scale pre-trained vision-language model (VLM), CLIP. While contrastively trained VLMs show impressive zero-shot generalization performance, their effectiveness is limited when dealing with diverse domains during training and testing. Existing prompt learning techniques overlook the importance of incorporating domain and content information into the prompts, which results in a drop in performance while dealing with such multi-domain data. To address these challenges, we propose a solution that ensures domain-invariant prompt learning while enhancing the expressiveness of visual features. We observe that CLIP‚Äôs vision encoder struggles to identify contextual image information, particularly when image patches are jumbled up. This issue is especially severe in optical remote sensing images, where land-cover classes exhibit well-defined contextual appearances. To this end, we introduce C-SAW, a method that complements CLIP with a self-supervised loss in the visual space and a novel prompt learning technique that emphasizes both visual domain and content-specific features. We keep the CLIP backbone frozen and introduce a small set of projectors for both the CLIP encoders to train C-SAW contrastively. Experimental results demonstrate the superiority of C-SAW across multiple remote sensing benchmarks and different generalization tasks.",
        "authors": [
            "Avigyan Bhattacharya",
            "M. Singha",
            "Ankit Jha",
            "Biplab Banerjee"
        ],
        "citations": 3,
        "references": 91,
        "year": 2023
    },
    {
        "title": "Exploiting CLIP for Zero-shot HOI Detection Requires Knowledge Distillation at Multiple Levels",
        "abstract": "In this paper, we investigate the task of zero-shot human-object interaction (HOI) detection, a novel paradigm for identifying HOIs without the need for task-specific annotations. To address this challenging task, we employ CLIP, a large-scale pre-trained vision-language model (VLM), for knowledge distillation on multiple levels. Specifically, we design a multi-branch neural network that leverages CLIP for learning HOI representations at various levels, including global images, local union regions encompassing human-object pairs, and individual instances of humans or objects. To train our model, CLIP is utilized to generate HOI scores for both global images and local union regions that serve as supervision signals. The extensive experiments demonstrate the effectiveness of our novel multi-level CLIP knowledge integration strategy. Notably, the model achieves strong performance, which is even comparable with some fully-supervised and weakly-supervised methods on the public HICO-DET benchmark. Code is available at https://github.com/bobwan1995/Zeroshot-HOI-with-CLIP.",
        "authors": [
            "Bo Wan",
            "T. Tuytelaars"
        ],
        "citations": 3,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Virtual Network Embedding Over Multi-Band Elastic Optical Network Based on Cross-Matching Mechanism and Hypergraph Theory",
        "abstract": "The commercialization of 5G and the explosive emergence of new applications stimulate the exponential growth of network traffic and diversification of services. It is promising to integrate multi-band elastic optical network (MBEON) and network virtualization for large volume traffic transmission and highly diverse services. However, performing virtual network embedding (VNE) for network virtualization over MBEON faces the challenge of severe inter-channel stimulated Raman scattering (ISRS) effect, which complicates the underlying physical layer effect in the substrate network. In this paper, we investigate the ISRS-aware VNE over MBEON, where a cross-matching mechanism is proposed for virtual node mapping (VNM) and a hypergraph is introduced for parallel virtual link mapping (VLM). A lightpath-level integer linear programming model is first formulated. To integrate the cost and availability of VLM, which significantly affect the performance of the VNE under the ISRS effect, into the VNM process, the ‚Äúvirtual node-substrate node‚Äù mapping pairs are specifically evaluated through the cross-matching mechanism. Moreover, to tackle the couplings among multiple lightpaths induced by the wide spectrum ISRS effect, hypergraphs are used to model the ISRS effect-aware quality of transmission (QoT) constraints among multiple lightpaths. A hypergraph maximal weight independent set heuristic is presented for lightpath selection, which guarantees the obedience of basic constraints and generates near-optimal solutions. Experimental results show that the proposed methods decrease blocking ratio by more than 30% compared with the benchmarks with similar computational complexity.",
        "authors": [
            "Zeyuan Yang",
            "Rentao Gu",
            "Yuefeng Ji"
        ],
        "citations": 3,
        "references": 61,
        "year": 2023
    },
    {
        "title": "ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition",
        "abstract": "Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even without any video data pre-training, and an accuracy of 96.11% and 75.75% after kinetics pre-training.",
        "authors": [
            "S. Chaudhuri",
            "Saumik Bhattacharya"
        ],
        "citations": 3,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Open-Vocabulary Camouflaged Object Segmentation",
        "abstract": "Recently, the emergence of the large-scale vision-language model (VLM), such as CLIP, has opened the way towards open-world object perception. Many works have explored the utilization of pre-trained VLM for the challenging open-vocabulary dense prediction task that requires perceiving diverse objects with novel classes at inference time. Existing methods construct experiments based on the public datasets of related tasks, which are not tailored for open vocabulary and rarely involve imperceptible objects camouflaged in complex scenes due to data collection bias and annotation costs. To fill in the gaps, we introduce a new task, open-vocabulary camouflaged object segmentation (OVCOS), and construct a large-scale complex scene dataset (\\textbf{OVCamo}) containing 11,483 hand-selected images with fine annotations and corresponding object classes. Further, we build a strong single-stage open-vocabulary \\underline{c}amouflaged \\underline{o}bject \\underline{s}egmentation transform\\underline{er} baseline \\textbf{OVCoser} attached to the parameter-fixed CLIP with iterative semantic guidance and structure enhancement. By integrating the guidance of class semantic knowledge and the supplement of visual structure cues from the edge and depth information, the proposed method can efficiently capture camouflaged objects. Moreover, this effective framework also surpasses previous state-of-the-arts of open-vocabulary semantic image segmentation by a large margin on our OVCamo dataset. With the proposed dataset and baseline, we hope that this new task with more practical value can further expand the research on open-vocabulary dense prediction tasks. Our code and data can be found in the \\href{https://github.com/lartpang/OVCamo}{link}.",
        "authors": [
            "Youwei Pang",
            "Xiaoqi Zhao",
            "Jiaming Zuo",
            "Lihe Zhang",
            "Huchuan Lu"
        ],
        "citations": 2,
        "references": 90,
        "year": 2023
    },
    {
        "title": "Emergent Open-Vocabulary Semantic Segmentation from Off-the-Shelf Vision-Language Models",
        "abstract": "From image-text pairs, large-scale vision-language models (VLMs) learn to implicitly associate image regions with words, which prove effective for tasks like visual question answering. However, leveraging the learned association for open-vocabulary semantic segmentation remains a challenge. In this paper, we propose a simple, yet extremely effective, training-free technique, Plug-and-Play Open- Vocabulary Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with direct text-to-image cross-attention and an image-text matching loss. To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask. PnP-OVSS does not require any neural net-work training and performs hyperparameter tuning without the need for any segmentation annotations, even for a validation set. PnP-OVSS demonstrates substantial improvements over comparable baselines (+29.4% mIoU on Pascal VOC, +13.2% mIoU on Pascal Context, +14.0% mIoU on MS COCO, +2.4% mIoU on COCO Stuff) and even outper-forms most baselines that conduct additional network training on top of pretrained VLMs. Our codebase is at https://github.com/letitiabanana/PnP-OVSS.",
        "authors": [
            "Jiayun Luo",
            "Siddhesh Khandelwal",
            "Leonid Sigal",
            "Boyang Albert Li"
        ],
        "citations": 2,
        "references": 89,
        "year": 2023
    },
    {
        "title": "Resolving References in Visually-Grounded Dialogue via Text Generation",
        "abstract": "Vision-language models (VLMs) have shown to be effective at image retrieval based on simple text queries, but text-image retrieval based on conversational input remains a challenge. Consequently, if we want to use VLMs for reference resolution in visually-grounded dialogue, the discourse processing capabilities of these models need to be augmented. To address this issue, we propose fine-tuning a causal large language model (LLM) to generate definite descriptions that summarize coreferential information found in the linguistic context of references. We then use a pretrained VLM to identify referents based on the generated descriptions, zero-shot. We evaluate our approach on a manually annotated dataset of visually-grounded dialogues and achieve results that, on average, exceed the performance of the baselines we compare against. Furthermore, we find that using referent descriptions based on larger context windows has the potential to yield higher returns.",
        "authors": [
            "Bram Willemsen",
            "Livia Qian",
            "Gabriel Skantze"
        ],
        "citations": 2,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Twenty-minute harvesting of flow-through type vastus lateralis muscle flap significantly reduces the need for a temporary intravascular shunt in the treatment of severe upper extremity trauma in civilian patients",
        "abstract": "For the reconstruction of severe upper extremity trauma involving arterial injury in civilian patients, it is generally recommended that the revascularization time be shortened using a temporary intravascular shunt (TIVS). However, if a flow-through type vastus lateralis muscle (VLm) flap can be harvested in 20 minutes and bypassed at the obstructed ischemic zone within 30 minutes, blood flow can be restored as quickly or more quickly than when using a TIVS, eliminating the need for a TIVS. This procedure was applied in the reconstruction of 3 cases of severe extremity trauma with vascular injury. The mean age was 69.7 years. Surgery was started an average of 2.93 hours from the onset. The average flap harvest time was 0.33 hours. The average time to revascularization from flap harvest was 1.33 hours, the average total operation time was 6.43 hours, and all upper extremities were salvaged. No cases showed ischemia-reperfusion injury or severe muscle contracture. The flow-through-type VLm flap can be applicable as a bypass graft for a 20‚Äâcm defect at any region distal to the elbow. In addition, harvesting the flap attached to blood-rich muscle not only controls the infection of contaminated wounds through the filling of dead space, but also has the potential to replace damaged muscle or tendon tissue. Even though TIVS placement is currently used extensively in this field of treatment, its role could be significantly reduced if a flow-through-type VLm flap can be harvested within 20 minutes.",
        "authors": [
            "Masakatsu Hihara",
            "A. Kuro",
            "Toshihito Mitsui",
            "N. Kakudo"
        ],
        "citations": 2,
        "references": 16,
        "year": 2023
    },
    {
        "title": "Simultaneous Microendoscopic Calcium Imaging and EEG Recording of Mouse Brain during Sleep",
        "abstract": "Sleep is a conserved biological process in the animal kingdom. Understanding the neural mechanisms underlying sleep state transitions is a fundamental goal of neurobiology, important for the development of new treatments for insomnia and other sleep-related disorders. Yet, brain circuits controlling this process remain poorly understood. A key technique in sleep research is to monitor in vivo neuronal activity in sleep-related brain regions across different sleep states. These sleep-related regions are usually located deeply in the brain. Here, we describe technical details and protocols for in vivo calcium imaging in the brainstem of sleeping mice. In this system, sleep-related neuronal activity in the ventrolateral medulla (VLM) is measured using simultaneous microendoscopic calcium imaging and electroencephalogram (EEG) recording. By aligning calcium and EEG signals, we demonstrate that VLM glutamatergic neurons display increased activity during the transition from wakefulness to non-rapid eye movement (NREM) sleep. The protocol described here can be applied to study neuronal activity in other deep brain regions involved in REM or NREM sleep.",
        "authors": [
            "Sasa Teng",
            "Yueqing Peng"
        ],
        "citations": 2,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Manipulating the Label Space for In-Context Classification",
        "abstract": "After pre-training by generating the next word conditional on previous words, the Language Model (LM) acquires the ability of In-Context Learning (ICL) that can learn a new task conditional on the context of the given in-context examples (ICEs). Similarly, visually-conditioned Language Modelling is also used to train Vision-Language Models (VLMs) with ICL ability. However, such VLMs typically exhibit weaker classification abilities compared to contrastive learning-based models like CLIP, since the Language Modelling objective does not directly contrast whether an object is paired with a text. To improve the ICL of classification, using more ICEs to provide more knowledge is a straightforward way. However, this may largely increase the selection time, and more importantly, the inclusion of additional in-context images tends to extend the length of the in-context sequence beyond the processing capacity of a VLM. To alleviate these limitations, we propose to manipulate the label space of each ICE to increase its knowledge density, allowing for fewer ICEs to convey as much information as a larger set would. Specifically, we propose two strategies which are Label Distribution Enhancement and Visual Descriptions Enhancement to improve In-context classification performance on diverse datasets, including the classic ImageNet and more fine-grained datasets like CUB-200. Specifically, using our approach on ImageNet, we increase accuracy from 74.70\\% in a 4-shot setting to 76.21\\% with just 2 shots. surpassing CLIP by 0.67\\%. On CUB-200, our method raises 1-shot accuracy from 48.86\\% to 69.05\\%, 12.15\\% higher than CLIP. The code is given in https://anonymous.4open.science/r/MLS_ICC.",
        "authors": [
            "Haokun Chen",
            "Xu Yang",
            "Yuhang Huang",
            "Zihan Wu",
            "Jing Wang",
            "Xin Geng"
        ],
        "citations": 2,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Gradient constrained sharpness-aware prompt learning for vision-language models",
        "abstract": "This paper targets a novel trade-off problem in generalizable prompt learning for vision-language models (VLM), i.e., improving the performance on unseen classes while maintaining the performance on seen classes. Comparing with existing generalizable methods that neglect the seen classes degradation, the setting of this problem is more strict and fits more closely with practical applications. To solve this problem, we start from the optimization perspective, and leverage the relationship between loss landscape geometry and model generalization ability. By analyzing the loss landscapes of the state-of-the-art method and vanilla Sharpness-aware Minimization (SAM) based method, we conclude that the trade-off performance correlates to both loss value and loss sharpness, while each of them is indispensable. However, we find the optimizing gradient of existing methods cannot maintain high relevance to both loss value and loss sharpness during optimization, which severely affects their trade-off performance. To this end, we propose a novel SAM-based method for prompt learning, denoted as Gradient Constrained Sharpness-aware Context Optimization (GCSCoOp), to dynamically constrain the optimizing gradient, thus achieving above two-fold optimization objective simultaneously. Extensive experiments verify the effectiveness of GCSCoOp in the trade-off problem.",
        "authors": [
            "Liangchen Liu",
            "Nannan Wang",
            "Dawei Zhou",
            "Xinbo Gao",
            "Decheng Liu",
            "Xi Yang",
            "Tongliang Liu"
        ],
        "citations": 2,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity",
        "abstract": "This paper presents novel benchmarks for evaluating vision-language models (VLMs) in zero-shot recognition, focusing on granularity and specificity. Although VLMs excel in tasks like image captioning, they face challenges in open-world settings. Our benchmarks test VLMs‚Äô consistency in understanding concepts across semantic granularity levels and their response to varying text specificity. Findings show that VLMs favor moderately fine-grained concepts and struggle with specificity, often misjudging texts that differ from their training data. Extensive evaluations reveal limitations in current VLMs, particularly in distinguishing between correct and subtly incorrect descriptions. While fine-tuning offers some improvements, it doesn‚Äôt fully address these issues, highlighting the need for VLMs with enhanced generalization capabilities for real-world applications. This study provides insights into VLM limitations and suggests directions for developing more robust models.",
        "authors": [
            "Zhenlin Xu",
            "Yi Zhu",
            "Tiffany Deng",
            "Abhay Mittal",
            "Yanbei Chen",
            "Manchen Wang",
            "P. Favaro",
            "Joseph Tighe",
            "Davide Modolo"
        ],
        "citations": 2,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Contrastive Vision-Language Alignment Makes Efficient Instruction Learner",
        "abstract": "We study the task of extending the large language model (LLM) into a vision-language instruction-following model. This task is crucial but challenging since the LLM is trained on text modality only, making it hard to effectively digest the visual modality. To address this, existing methods typically train a visual adapter to align the representation between a pre-trained vision transformer (ViT) and the LLM by a generative image captioning loss. However, we find that the generative objective can only produce weak alignment for vision and language, making the aligned vision-language model very hungry for the instruction fine-tuning data. In this paper, we propose CG-VLM that applies both Contrastive and Generative alignment objectives to effectively align the representation of ViT and LLM. Different from image level and sentence level alignment in common contrastive learning settings, CG-VLM aligns the image-patch level features and text-token level embeddings, which, however, is very hard to achieve as no explicit grounding patch-token relation provided in standard image captioning datasets. To address this issue, we propose to maximize the averaged similarity between pooled image-patch features and text-token embeddings. Extensive experiments demonstrate that the proposed CG-VLM produces strong vision-language alignment and is an efficient instruction learner. For example, using only 10% instruction tuning data, we reach 95% performance of state-of-the-art method LLaVA [29] on the zero-shot ScienceQA-Image benchmark.",
        "authors": [
            "Lizhao Liu",
            "Xinyu Sun",
            "Tianhang Xiang",
            "Zhuangwei Zhuang",
            "Liuren Yin",
            "Mingkui Tan"
        ],
        "citations": 2,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Bi-VLGM : Bi-Level Class-Severity-Aware Vision-Language Graph Matching for Text Guided Medical Image Segmentation",
        "abstract": "Medical reports containing specific diagnostic results and additional information not present in medical images can be effectively employed to assist image understanding tasks, and the modality gap between vision and language can be bridged by vision-language matching (VLM). However, current vision-language models distort the intra-model relation and only include class information in reports that is insufficient for segmentation task. In this paper, we introduce a novel Bi-level class-severity-aware Vision-Language Graph Matching (Bi-VLGM) for text guided medical image segmentation, composed of a word-level VLGM module and a sentence-level VLGM module, to exploit the class-severity-aware relation among visual-textual features. In word-level VLGM, to mitigate the distorted intra-modal relation during VLM, we reformulate VLM as graph matching problem and introduce a vision-language graph matching (VLGM) to exploit the high-order relation among visual-textual features. Then, we perform VLGM between the local features for each class region and class-aware prompts to bridge their gap. In sentence-level VLGM, to provide disease severity information for segmentation task, we introduce a severity-aware prompting to quantify the severity level of disease lesion, and perform VLGM between the global features and the severity-aware prompts. By exploiting the relation between the local (global) and class (severity) features, the segmentation model can include the class-aware and severity-aware information to promote segmentation performance. Extensive experiments proved the effectiveness of our method and its superiority to existing methods. The source code will be released.",
        "authors": [
            "Wenting Chen",
            "Jie Liu",
            "Yixuan Yuan"
        ],
        "citations": 2,
        "references": 53,
        "year": 2023
    },
    {
        "title": "IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models",
        "abstract": "Large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in various tasks and attracted an increasing interest as a natural language interface across many domains. Recently, large vision-language models (VLMs) like BLIP-2 and GPT-4 have been intensively investigated, which learn rich vision-language correlation from image-text pairs. However, despite these developments, the application of LLMs and VLMs in image quality assessment (IQA), particularly in medical imaging, remains to be explored, which is valuable for objective performance evaluation and potential supplement or even replacement of radiologists' opinions. To this end, this paper introduces IQAGPT, an innovative image quality assessment system integrating an image quality captioning VLM with ChatGPT for generating quality scores and textual reports. First, we build a CT-IQA dataset for training and evaluation, comprising 1,000 CT slices with diverse quality levels professionally annotated. To better leverage the capabilities of LLMs, we convert annotated quality scores into semantically rich text descriptions using a prompt template. Second, we fine-tune the image quality captioning VLM on the CT-IQA dataset to generate quality descriptions. The captioning model fuses the image and text features through cross-modal attention. Third, based on the quality descriptions, users can talk with ChatGPT to rate image quality scores or produce a radiological quality report. Our preliminary results demonstrate the feasibility of assessing image quality with large models. Remarkably, our IQAGPT outperforms GPT-4 and CLIP-IQA, as well as the multi-task classification and regression models that solely rely on images.",
        "authors": [
            "Zhihao Chen",
            "Bin Hu",
            "Chuang Niu",
            "Tao Chen",
            "Yuxin Li",
            "Hongming Shan",
            "Ge Wang"
        ],
        "citations": 2,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Parameterization investigation on distributed electric propulsion aircraft aerodynamic characteristics",
        "abstract": "Revealing the principle of distributed electric propulsion (DEP) plane aerodynamic design under the effect of strong aerodynamic coupling of distributed power-wing and the change law of total gain is the key to carrying out the design of distributed electric propulsion plane. For the distributed electric propulsion plane aerodynamic layout, the aerodynamic performance of the DEP configuration is studied based on the VLM-ADT aerodynamic characteristics fast solver method. This paper gives a set of fast parametric methods to research the aerodynamic performance of DEP layout, which mainly includes three comprehensive indexes such as lift-to-drag ratio L/D, power loading T/P, and lift loading L/P‚Äîfocused on; the factor of propeller number, which affects the aerodynamic performance of DEP aerodynamic layout. As the propeller number increases, the lift-to-drag ratio decreases, but the wing‚Äôs lift efficiency increases.",
        "authors": [
            "Z. Cheng",
            "Youxu Yang",
            "Bo Ye"
        ],
        "citations": 2,
        "references": 6,
        "year": 2023
    },
    {
        "title": "A Simple Knowledge Distillation Framework for Open-world Object Detection",
        "abstract": "Open World Object Detection (OWOD) is a novel computer vision task with a considerable challenge, bridging the gap between classic object detection (OD) benchmarks and real-world object detection. In addition to detecting and classifying seen/known objects, OWOD algorithms are expected to localize all potential unseen/unknown objects and incrementally learn them. The large pre-trained vision-language grounding models (VLM, e.g. , GLIP) have rich knowledge about the open world, but are limited by text prompts and cannot localize indescribable objects. However, there are many detection scenarios which pre-defined language descriptions are unavailable during inference. In this paper, we attempt to specialize the VLM model for OWOD task by distilling its open-world knowledge into a language-agnostic detector. Surprisingly, we observe that the combination of a simple knowledge distillation approach and the automatic pseudo-labeling mechanism in OWOD can achieve better performance for unknown object detection, even with a small amount of data. Unfortunately, knowledge distillation for unknown objects severely affects the learning of detectors with conventional structures for known objects, leading to catastrophic forgetting. To alleviate these problems, we pro-pose the down-weight loss function for knowledge distillation from vision-language to single vision modality. Meanwhile, we decouple the learning of localization and recognition to reduce the impact of category interactions of known and unknown objects on the localization learning process. Comprehensive experiments performed on MS-COCO and PASCAL VOC demonstrate the effectiveness of our methods.",
        "authors": [
            "Shuailei Ma",
            "Yuefeng Wang",
            "Ying Wei",
            "Jiaqi Fan",
            "Xinyu Sun",
            "Peihao Chen",
            "Enming Zhang"
        ],
        "citations": 2,
        "references": 27,
        "year": 2023
    },
    {
        "title": "LiFT: Transfer Learning in Vision-Language Models for Downstream Adaptation and Generalization",
        "abstract": "Pre-trained Vision-Language Models (VLMs) on large-scale image-text pairs, e.g., CLIP, have shown promising performance on zero-shot knowledge transfer. Recently, fine-tuning pre-trained VLMs to downstream few-shot classification with limited image annotation data yields significant gains. However, there are two limitations. First, most of the methods for fine-tuning VLMs only update newly added parameters while keeping the whole VLM frozen. Thus, it remains unclear how to directly update the VLM itself. Second, fine-tuning VLMs to a specific set of base classes would deteriorate the well-learned representation space such that the VLMs generalize poorly on novel classes. To address these issues, we first propose Layer-wise Fine-Tuning (LiFT) which achieves average gains of 3.9%, 4.3%, 4.2% and 4.5% on base classes under 2-, 4-, 8- and 16-shot respectively compared to the baseline CoOp over 11 datasets. Alternatively, we provide a parameter-efficient LiFT-Adapter exhibiting favorable performance while updating only 1.66% of total parameters. Further, we design scalable LiFT-NCD to identify both base classes and novel classes, which boosts the accuracy by an average of 5.01% over zero-shot generalization of CLIP, exploring the potential of VLMs in discovering novel classes.",
        "authors": [
            "Jingzheng Li",
            "Hailong Sun"
        ],
        "citations": 2,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Questionnaire survey of virtual reality experiences of digestive surgery at a rural academic institute: A pilot study for pre-surgical education.",
        "abstract": "We developed a prototype VR platform, VECTORS L&M (VLM), aiming to enhance the understanding of digestive surgery for students, interns, and young surgeons by limiting costs. Its efficacy was assessed via questionnaires before implementation in surgical education. The VLM provides nine-minute VR views of surgeries, from both 180- and 360-degree angles. It was created with L.A.B. Co., Ltd. and incorporates surgery videos from biliary malignancy patients. Following VLM development, a survey was conducted among surgeons who had experienced it. Twenty-eight participants (32% of observers) responded to the survey. A majority (81%) reported positive experiences with the VR content and showed interest in VR video production, though some reported sickness. Most respondents were experienced surgeons, and nearly all believed VR was important for medical education with a mean score of 4.14 on a scale of up to 5. VR was preferred over 3D printed models due to its application versatility. Participants expressed the desire for future VR improvements, such as increased mobility, cloud connectivity, cost reduction, and better resolution. The VLM platform, coupled with this innovative teaching approach, offers experiential learning in intraabdominal surgery, effectively enriching the knowledge of students and surgeons ahead of surgical education and training.",
        "authors": [
            "Atsushi Nanashima",
            "K. Kai",
            "T. Hamada",
            "Shun Munakata",
            "N. Imamura",
            "M. Hiyoshi",
            "Kiyoaki Hamada",
            "Ikko Shimizu",
            "Yuki Tsuchimochi",
            "Isao Tsuneyoshi"
        ],
        "citations": 2,
        "references": 24,
        "year": 2023
    },
    {
        "title": "Offsets in tide-gauge reference levels detected by satellite altimetry: ten case studies",
        "abstract": null,
        "authors": [
            "R. Ray",
            "M. Widlansky",
            "A. Genz",
            "P. Thompson"
        ],
        "citations": 2,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Enabling Vision-and-Language Navigation for Intelligent Connected Vehicles Using Large Pre-Trained Models",
        "abstract": "In the field of autonomous driving, Visual-and-Language Navigation (VLN) is a typical multimodal task. In the VLN task, an intelligent vehicle needs to find the target location based on user-provided navigation instructions. However, conventional VLN models generally face the problem of limited generalization ability when dealing with a large number of real-world environmental objects and language instructions. This paper proposed a novel VLN system based on large-scale pre-trained models and applied it to intelligent vehicles. The method consists of an Instruction Extraction System, a Vision-Language Association System, and a Navigational Decisions System. Specifically, a pre-trained Large Language Model (LLM) is first used to extract a series of landmark names from the user‚Äôs natural language instructions. Then, the landmark name list is inputted into a pre-trained Visual-Language Model (VLM) to infer the joint probability with environmental objects. Therefore, the image nodes that match the landmarks have been selected. Additionally, the selected image nodes are inputted into another VLM to obtain descriptions of the image nodes. Finally, LLM is used to reason navigation actions for the intelligent vehicle. With the reasoning ability of LLM, the intelligent vehicle takes navigation knowledge, visual environment descriptions, and navigation history as inputs to output navigation actions. The simulation results demonstrate that compared to other fully supervised learning methods, this approach exhibits better generalization ability in unknown environments. Based on Google Map‚Äôs Street View data, it achieves a 14.6% higher task success rate compared to the baseline model VLN Transformer.",
        "authors": [
            "Yaqi Hu",
            "Dongyuan Ou",
            "Xiaoxu Wang",
            "Rong Yu"
        ],
        "citations": 2,
        "references": 19,
        "year": 2023
    },
    {
        "title": "Fine-Grained Visual‚ÄìText Prompt-Driven Self-Training for Open-Vocabulary Object Detection",
        "abstract": "Inspired by the success of vision‚Äìlanguage methods (VLMs) in zero-shot classification, recent works attempt to extend this line of work into object detection by leveraging the localization ability of pretrained VLMs and generating pseudolabels for unseen classes in a self-training manner. However, since the current VLMs are usually pretrained with aligning sentence embedding with global image embedding, the direct use of them lacks fine-grained alignment for object instances, which is the core of detection. In this article, we propose a simple but effective fine-grained visual-text prompt-driven self-training paradigm for open-vocabulary detection (VTP-OVD) that introduces a fine-grained visual‚Äìtext prompt adapting stage to enhance the current self-training paradigm with a more powerful fine-grained alignment. During the adapting stage, we enable VLM to obtain fine-grained alignment using learnable text prompts to resolve an auxiliary dense pixelwise prediction task. Furthermore, we propose a visual prompt module to provide the prior task information (i.e., the categories need to be predicted) for the vision branch to better adapt the pretrained VLM to the downstream tasks. Experiments show that our method achieves the state-of-the-art performance for open-vocabulary object detection, e.g., 31.5% mAP on unseen classes of COCO.",
        "authors": [
            "Yanxin Long",
            "Jianhua Han",
            "Runhu Huang",
            "Hang Xu",
            "Yi Zhu",
            "Chunjing Xu",
            "Xiaodan Liang"
        ],
        "citations": 15,
        "references": 88,
        "year": 2022
    },
    {
        "title": "Sea Level Rise Estimation on the Pacific Coast from Southern California to Vancouver Island",
        "abstract": "Previous studies have estimated the sea level rise (SLR) at various locations on the west coast of the USA and Vancouver Island in Canada. Here, we construct an entire SLR profile from Vancouver Island in the Pacific Northwest to San Diego in Southern California. First, we process global navigation satellite system (GNSS) measurements at 405 stations blanketing the whole coast to generate a profile of vertical land motion (VLM) known to bias century-long tide gauge (TG) measurements recording relative SLR (RSLR). We are then able to estimate the absolute SLR (ASLR) by correcting the SLR with the VLM. Our study emphasizes the relationship between the various tectonic movements (i.e., the Cascadia subduction zone, the San Andreas strike-slip fault system) along the Pacific coast which renders it difficult to accurately estimate the SLR. That is why we precisely model the stochastic noise of both GNSS and tide gauge time series using a combination of various models and information criterions (ICs). We also use the latest altimetry products and sea surface height (SSH) to compare it with ASLR at the same location as the TGs. This study supports previous analysis that the power law + white noise and generalized Gauss‚ÄìMarkov + white noise models are the best stochastic noise models for the GNSS time series. The new coastal profile confirms the large variability of VLM estimates in the Pacific Northwest around the Cascadia subduction zone in agreement with previous studies, and a similar result when the San Andreas fault comes onshore in Central California (San Francisco Bay). Negative RSLR values are mostly located in the Pacific Northwest (Vancouver Island and Olympic Peninsula). We also observe a much bigger variation (about 90%‚Äì150%) of the ASLR in the Pacific Northwest which is predominantly due to glacial isostatic adjustment (GIA). Moreover, the comparison between the ASLR and the SSH estimates shows similarities in the center of the studied area (South Washington, Oregon planes, and some parts of Southern California) where the tectonic activity does not significantly influence the TG measurements. Finally, the twentieth-century satellite geocentric ocean height rates show a global mean of 1.5 to 1.9 mm/yr. Our estimates based on ASLR and SSH are within this interval.",
        "authors": [
            "Xiaoxing He",
            "J. Montillet",
            "Rui Manuel da Silva Fernandes",
            "T. Melbourne",
            "Weiping Jiang",
            "Zhengkai Huang"
        ],
        "citations": 16,
        "references": 0,
        "year": 2022
    },
    {
        "title": "A reduced PAPR hybrid OFDM visible light communication system",
        "abstract": null,
        "authors": [
            "Basma Taha",
            "H. Fayed",
            "M. Aly",
            "M. Mahmoud"
        ],
        "citations": 15,
        "references": 15,
        "year": 2022
    },
    {
        "title": "How to Use Language Expert to Assist Inference for Visual Commonsense Reasoning",
        "abstract": "Visual Commonsense Reasoning (VCR) task requires Vision and Language Model (VLM) to capture cognitive level clues from the visual-language input and give the right answers to questions and their rationales. Recently, although Pretrained Language Model (PLM) has been taken as a powerful in-domain knowledge base to the various tasks like image segmentation and visual question answering, PLM remains unexplored to generalize to the unseen multi-modal data in an out-domain way. In this paper, we explore how to use PLM to assist VLM for the challenging VCR task and propose a framework called Vision and Language Assisted with Expert Language Model (VLAELM). The VLAELM aims to employ a PLM with expert level of commonsense knowledge to assist reasoning, which is difficult for the VLM learning just from scarce multi-modal data. The experiments show that VLAELM achieves significant improvements against the strong baselines. Moreover, we validate credibility for language expert as knowledge base and measure application value between generalization and specialty in PLM.",
        "authors": [
            "Zijie Song",
            "Wenbo Hu",
            "Hao Ye",
            "Richang Hong"
        ],
        "citations": 1,
        "references": 32,
        "year": 2023
    },
    {
        "title": "KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization",
        "abstract": "In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema.It enables the model to effectively learn the required knowledge and skills from limited resources in the domain.Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task (Delbrouck et al., 2023), our model benefits from its training across multiple tasks and domains.With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.",
        "authors": [
            "Gangwoo Kim",
            "Hajung Kim",
            "Lei Ji",
            "Seongsu Bae",
            "Chanhwi Kim",
            "Mujeen Sung",
            "Hyunjae Kim",
            "Kun Yan",
            "E. Chang",
            "Jaewoo Kang"
        ],
        "citations": 1,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Semantically Enhanced Scene Captions with Physical and Weather Condition Changes",
        "abstract": "Vision-Language models (VLMs), i.e., image-text pairs of CLIP, have boosted image-based Deep Learning (DL). Moreover, Visual-Question-Answer (VQA) tools and open-vocabulary semantic segmentation provide us with more detailed scene descriptions, i.e., qualitative texts, in captions. Images from surveillance, auto-drive, and mobile phone cameras have been used with segmentation and captions. However, unlike indoor scenes, outdoor scenes with uncontrolled illumination and noise can degrade the accuracy of segmented objects. Moreover, unpredictable events such as natural phenomena and accidents can cause dynamic and adverse scene changes over time. This greatly increases unseen objects due to sudden changes. Therefore, only a single state-of-the-art (SOTA) VLM and DL model cannot sufficiently generate and enhance captions. Even one time VQA is limited to generate a good answer. This paper proposes RoadCAP for refined and enriched qualitative and quantitative captions by DL models and VLMs with different tasks in a complementary manner. In particular, 2D-Contrastive Physical-Scale Pretraining (CPP) is also proposed for captions with physical scales. An iterative VQA model is proposed to further refine incomplete segmented images with the prompts. Experimental results outperform SOTA DL models and VLMs using images with adverse conditions. A higher semantic level in captions for real-world scene descriptions is shown as compared with SOTA VLMs.",
        "authors": [
            "Hidetomo Sakaino"
        ],
        "citations": 1,
        "references": 105,
        "year": 2023
    },
    {
        "title": "Scalable Performance Analysis for Vision-Language Models",
        "abstract": "Joint vision-language models have shown great performance over a diverse set of tasks. However, little is known about their limitations, as the high dimensional space learned by these models makes it difficult to identify semantic errors. Recent work has addressed this problem by designing highly controlled probing task benchmarks. Our paper introduces a more scalable solution that relies on already annotated benchmarks. Our method consists of extracting a large set of diverse features from a vision-language benchmark and measuring their correlation with the output of the target model. We confirm previous findings that CLIP behaves like a bag of words model and performs better with nouns and verbs; we also uncover novel insights such as CLIP getting confused by concrete words. Our framework is available at https://github.com/MichiganNLP/Scalable-VLM-Probing and can be used with other multimodal models and benchmarks.",
        "authors": [
            "Santiago Castro",
            "Oana Ignat",
            "Rada Mihalcea"
        ],
        "citations": 1,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Molecular organization of autonomic, respiratory, and spinally-projecting neurons in the mouse ventrolateral medulla",
        "abstract": "The ventrolateral medulla (VLM) is a crucial region in the brain for visceral and somatic control. It also serves as a significant source of synaptic input to the spinal cord. Experimental studies have shown that gene expression in individual VLM neurons is predictive of their function. However, the organizing principles of the VLM have remained uncertain. This study aimed to create a comprehensive dataset of VLM cells using single-cell RNA sequencing. The dataset was enriched with targeted sequencing of spinally-projecting and adrenergic/noradrenergic VLM neurons. Based on differentially expressed genes, the resulting dataset of 114,805 VLM cells identifies 23 subtypes of neurons, excluding those in the inferior olive, and 5 subtypes of astrocytes. Spinally-projecting neurons were found to be abundant in 7 subtypes of neurons, which were validated through in-situ hybridization. These subtypes included adrenergic/noradrenergic neurons, serotonergic neurons, and neurons expressing gene markers associated with pre-motor neurons in the ventromedial medulla. Further analysis of adrenergic/noradrenergic neurons and serotonergic neurons identified 9 and 6 subtypes, respectively, within each class of monoaminergic neurons. Marker genes that identify the neural network responsible for breathing were concentrated in 2 subtypes of neurons, delineated from each other by markers for excitatory and inhibitory neurons. These datasets are available for public download and for analysis with a user-friendly interface. Collectively, this study provides a fine-scale molecular identification of cells in the VLM, forming the foundation for a better understanding of the VLM‚Äôs role in vital functions and motor control.",
        "authors": [
            "Dana C Schwalbe",
            "Daniel S. Stornetta",
            "Ruei-Jen Abraham-Fan",
            "George Souza",
            "Maira Jalil",
            "Maisie E. Crook",
            "John N. Campbell",
            "Stephen B. G. Abbott"
        ],
        "citations": 1,
        "references": 0,
        "year": 2023
    },
    {
        "title": "Enhance Reasoning Ability of Visual-Language Models via Large Language Models",
        "abstract": "Pre-trained visual language models (VLM) have shown excellent performance in image caption tasks. However, it sometimes shows insufficient reasoning ability. In contrast, large language models (LLMs) emerge with powerful reasoning capabilities. Therefore, we propose a method called TReE, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios. TReE contains three stages: observation, thinking, and re-thinking. Observation stage indicates that VLM obtains the overall information of the relative image. Thinking stage combines the image information and task description as the prompt of the LLM, inference with the rationals. Re-Thinking stage learns from rationale and then inference the final result through VLM.",
        "authors": [
            "Yueting Yang",
            "Xintong Zhang",
            "Wenjuan Han"
        ],
        "citations": 1,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Core content for venous and Lymphatic Medicine: 2022 revision.",
        "abstract": "The core content for a medical specialty outlines the scope of the discipline as well as the categories of knowledge considered essential to practice in the field. It provides a template for the development of curricula for medical school, graduate, and postgraduate education, as well as for creating certification standards. Venous and Lymphatic Medicine (VLM) is a specialty that has benefitted from contributions from specialists from several medical disciplines. Optimally, the societies, boards, and residency review committees representing these disciplines would uniformly recognize the scope of VLM to develop education and assessment standards to allow training and identification of qualified practitioners. In order to inform the standard setting bodies and other stakeholders of the current scope of VLM, a task force of VLM experts from cardiology, dermatology, emergency medicine, general surgery, interventional radiology, vascular medicine, and vascular surgery was formed to revise a 2014 consensus document defining the core content of the specialty of VLM.",
        "authors": [
            "NM Khilnani",
            "SM Wasan",
            "P. Pappas",
            "Z. Deol",
            "JP Schoonover",
            "SF Daugherty",
            "RR Attaran",
            "TV Cartee",
            "TM Straight",
            "J. Fish",
            "JW Granzow",
            "RS Winokur",
            "KR Desai",
            "G. Salazar",
            "J. Stoughton",
            "K. Gibson",
            "AD Jones",
            "JM Lohr",
            "S. Vayuvegula",
            "MH Meissner"
        ],
        "citations": 1,
        "references": 11,
        "year": 2023
    },
    {
        "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models",
        "abstract": "In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) on visual commonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decision pipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibit strong performance for problems involving understanding the literal visual content, which we noted as visual commonsense understanding (VCU). For problems where the goal is to infer conclusions beyond image content, which we noted as visual commonsense inference (VCI), VLMs face difficulties, while LLMs, given sufficient visual evidence, can use commonsense to infer the answer well. We empirically validate this by letting LLMs classify VCR problems into these two categories and show the significant difference between VLM and LLM with image caption decision pipelines on two subproblems. Moreover, we identify a challenge with VLMs' passive perception, which may miss crucial context information, leading to incorrect reasoning by LLMs. Based on these, we suggest a collaborative approach, named ViCor, where pre-trained LLMs serve as problem classifiers to analyze the problem category, then either use VLMs to answer the question directly or actively instruct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences. We evaluate our framework on two VCR benchmark datasets and outperform all other methods that do not require in-domain fine-tuning.",
        "authors": [
            "KAI-QING Zhou",
            "Kwonjoon Lee",
            "Teruhisa Misu",
            "X. Wang"
        ],
        "citations": 1,
        "references": 35,
        "year": 2023
    },
    {
        "title": "COCA: Classifier-Oriented Calibration via Textual Prototype for Source-Free Universal Domain Adaptation",
        "abstract": "Universal domain adaptation (UniDA) aims to address domain and category shifts across data sources. Recently, due to more stringent data restrictions, researchers have introduced source-free UniDA (SF-UniDA). SF-UniDA methods eliminate the need for direct access to source samples when performing adaptation to the target domain. However, existing SF-UniDA methods still require an extensive quantity of labeled source samples to train a source model, resulting in significant labeling costs. To tackle this issue, we present a novel plug-and-play classifier-oriented calibration (COCA) method. COCA, which exploits textual prototypes, is designed for the source models based on few-shot learning with vision-language models (VLMs). It endows the VLM-powered few-shot learners, which are built for closed-set classification, with the unknown-aware ability to distinguish common and unknown classes in the SF-UniDA scenario. Crucially, COCA is a new paradigm to tackle SF-UniDA challenges based on VLMs, which focuses on classifier instead of image encoder optimization. Experiments show that COCA outperforms state-of-the-art UniDA and SF-UniDA models.",
        "authors": [
            "Xingxian Liu",
            "Yi Zhou",
            "Tao Zhou",
            "Chun-Mei Feng",
            "Ling Shao"
        ],
        "citations": 1,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception",
        "abstract": "Vision-language models (VLMs) have shown powerful capabilities in visual question answering and reasoning tasks by combining visual representations with the abstract skill set large language models (LLMs) learn during pretraining. Vision, while the most popular modality to augment LLMs with, is only one representation of a scene. In human-robot interaction scenarios, robot perception requires accurate scene understanding by the robot. In this paper, we define and demonstrate a method of aligning the embedding spaces of different modalities (in this case, inertial measurement unit (IMU) data) to the vision embedding space through a combination of supervised and contrastive training, enabling the VLM to understand and reason about these additional modalities without retraining. We opt to give the model IMU embeddings directly over using a separate human activity recognition model that feeds directly into the prompt to allow for any nonlinear interactions between the query, image, and IMU signal that would be lost by mapping the IMU data to a discrete activity label. Further, we demonstrate our methodology's efficacy through experiments involving human activity recognition using IMU data and visual inputs. Our results show that using multiple modalities as input improves the VLM's scene understanding and enhances its overall performance in various tasks, thus paving the way for more versatile and capable language models in multi-modal contexts.",
        "authors": [
            "Riley Tavassoli",
            "Mani Amani",
            "Reza Akhavian"
        ],
        "citations": 1,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Efficiency Enhancement of Marine Propellers via Reformation of Blade Tip-Rake Distribution",
        "abstract": "This work addresses the effects of blade tip-rake reformation on the performance of marine propellers using a low-cost potential-based vortex-lattice method (VLM) and the high fidelity artificial compressibility CFD-RANS solver MaPFlow. The primary focus lies on determining whether the low-cost VLM, in conjunction with a multidimensional parametric model for the tip-rake and pitch/camber distributions, can produce a propeller geometry with improved efficiency. Due to the availability of experimental and numerical data, the NSRDC 4381-82 propellers were selected as reference geometries. Torque minimization serves as the objective function in the gradient-based optimization procedure under a thrust constraint, which translates into efficiency enhancement at the selected design advance ratio. The optimized 4381 propeller yields a +1.1% improvement in efficiency based on CFD-RANS, whereas for the modified skewed 4382 propeller, the efficiency gain is +0.5%. The performance enhancement is also evident at a region near the design advance ratio. The results suggest that the exploitation of low-cost VLM solvers can significantly reduce the CFD simulations required in the optimization process and thus can be effectively used for the design of propellers with tip-rake reformation.",
        "authors": [
            "D. Anevlavi",
            "Spiros Zafeiris",
            "George Papadakis",
            "K. Belibassakis"
        ],
        "citations": 1,
        "references": 22,
        "year": 2023
    },
    {
        "title": "Successes and Challenges of MT-InSAR Methods in Coastal Regions: A Case Study on the Island of Tutuila, American Samoa",
        "abstract": "Multi-temporal InSAR (MT-InSAR) techniques are powerful time-series analysis methods that enable all-weather measurements of surface deformation. However, even advanced MT-InSAR techniques can produce inaccurate results in challenging environments, including many coastal regions. Here, we describe successes and challenges of applying MT-InSAR techniques for imaging post-seismic vertical land motion (VLM) on Tutuila Island in American Samoa. We tested traditional persistent scatterer (PS) and small baseline subset (SBAS) methods and also designed a customized redundant PS method to address observed inconsistencies in both PS and SBAS analysis. The redundant PS method yielded results with lower noise and more realistic spatial variation compared to PS and SBAS workflows. Our case study emphasizes the importance of careful application of MT-InSAR for measuring VLM along coastal regions, where subsidence and uplift rates may be moderate in the presence of numerous noise sources. In particularly challenging areas, customized approaches may be needed to produce useful solutions.",
        "authors": [
            "Stacey A. Huang",
            "J. Sauber"
        ],
        "citations": 1,
        "references": 15,
        "year": 2023
    },
    {
        "title": "Treatment of Tide Gauge Time Series and Marine GNSS Measurements for Vertical Land Motion with Relevance to the Implementation of the Baltic Sea Chart Datum 2000",
        "abstract": "Tide gauge (TG) time series and GNSS measurements have become standard datasets for various scientific and practical applications. However, the TG and geodetic networks in the Baltic Sea region are deforming due to vertical land motion (VLM), the primary cause of which is the glacial isostatic adjustment. Consequently, a correction for VLM, either obtained from a suitable VLM model or by utilizing space-geodetic techniques, must be applied to ensure compatibility of various data sources. It is common to consider the VLM rate relative to an arbitrary reference epoch, but this also yields that the resulting datasets may not be directly comparable. The common height reference, Baltic Sea Chart Datum 2000 (BSCD2000), has been initiated to facilitate the effective use of GNSS methods for accurate navigation and offshore surveying. The BSCD2000 agrees with the current national height realizations of the Baltic Sea countries. As TGs managed by national authorities are rigorously connected to the national height systems, the TG data can also be used in a common system. Hence, this contribution aims to review the treatment of TG time series for VLM and outline potential error sources for utilizing TG data relative to a common reference. Similar consideration is given for marine GNSS measurements that likewise require VLM correction for some marine applications (such as validating marine geoid models). The described principles are illustrated by analyzing and discussing numerical examples. These include investigations of TG time series and validation of shipborne GNSS determined sea surface heights. The latter employs a high-resolution geoid model and hydrodynamic model-based dynamic topography, which is linked to the height reference using VLM corrected TG data. Validation of the presented VLM corrected marine GNSS measurements yields a 1.7 cm standard deviation and ‚Äì2.7 cm mean residual. The estimates are 1.9 cm and ‚Äì10.2 cm, respectively, by neglecting VLM correction. The inclusion of VLM correction thus demonstrates significant improvement toward data consistency. Although the focus is on the Baltic Sea region, the principles described here are also applicable elsewhere.",
        "authors": [
            "Sander Varbla",
            "J. √Ögren",
            "A. Ellmann",
            "M. Poutanen"
        ],
        "citations": 12,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Control of non-REM sleep by ventrolateral medulla glutamatergic neurons projecting to the preoptic area",
        "abstract": null,
        "authors": [
            "Sasa Teng",
            "Fenghua Zhen",
            "Li Wang",
            "Jose Canovas Schalchli",
            "Jane Simko",
            "Xinyue Chen",
            "Hao Jin",
            "Christopher D. Makinson",
            "Yueqing Peng"
        ],
        "citations": 11,
        "references": 59,
        "year": 2022
    },
    {
        "title": "Generalizing Multiple Object Tracking to Unseen Domains by Introducing Natural Language Representation",
        "abstract": "Although existing multi-object tracking (MOT) algorithms have obtained competitive performance on various benchmarks, almost all of them train and validate models on the same domain. The domain generalization problem of MOT is hardly studied. To bridge this gap, we first draw the observation that the high-level information contained in natural language is domain invariant to different tracking domains. Based on this observation, we propose to introduce natural language representation into visual MOT models for boosting the domain generalization ability. However, it is infeasible to label every tracking target with a textual description. To tackle this problem, we design two modules, namely visual context prompting (VCP) and visual-language mixing (VLM). Specifically, VCP generates visual prompts based on the input frames. VLM joints the information in the generated visual prompts and the textual prompts from a pre-defined Trackbook to obtain instance-level pseudo textual description, which is domain invariant to different tracking scenes. Through training models on MOT17 and validating them on MOT20, we observe that the pseudo textual descriptions generated by our proposed modules improve the generalization performance of query-based trackers by large margins.",
        "authors": [
            "En Yu",
            "Songtao Liu",
            "Zhuoling Li",
            "Jinrong Yang",
            "Zeming Li",
            "Shoudong Han",
            "Wenbing Tao"
        ],
        "citations": 10,
        "references": 53,
        "year": 2022
    },
    {
        "title": "A giant planet shaping the disk around the very low-mass star CIDA 1",
        "abstract": "Context. Exoplanetary research has provided us with exciting discoveries of planets around very low mass (VLM) stars (0 . 08M M (cid:46) 0 . 3M ; e.g., TRAPPIST-1 and Proxima Centauri). However, current theoretical models still strive to explain planet formation in these conditions and do not predict the development of giant planets. Recent high-resolution observations from the Atacama Large Millimeter / submillimeter Array (ALMA) of the disk around CIDA 1, a VLM star in Taurus, show substructures hinting at the presence of a massive planet. Aims. We aim to reproduce the dust ring of CIDA 1, observed in the dust continuum emission in ALMA Band 7 (0 . 9mm) and Band 4 (2 . 1mm), along with its 12 CO ( J = 3 ‚àí 2) and 13 CO ( J = 3 ‚àí 2) channel maps, assuming the structures are shaped by the interaction of the disk with a massive planet. We seek to retrieve the mass and position of the putative planet, through a global simulation assessing planet-disk interaction to quantitatively reproduce protoplanetary disk observations of both dust and gas emission in a self-consistent way. Methods. We model the protoplanetary disk with a set of hydrodynamical simulations, hosting an embedded planet with a starting mass between 0 . 1 and 4 . 0M Jup initially located at a distance between 9 and 11au from the central star. We compute the dust and gas emission using radiative transfer simulations, and, Ô¨Ånally, we obtain the synthetic observations treating the images as the actual ALMA observations. Results. Our models indicate that a planet with a minimum mass of ‚àº 1 . 4M Jup orbiting at a distance of ‚àº 9 ‚àí 10au can explain the morphology and location of the observed dust ring at Band 7 and Band 4. We match the Ô¨Çux of the dust emission observation with a dust-to-gas mass ratio in the disk of ‚àº 10 ‚àí 2 . We are able to reproduce the low spectral index ( ‚àº 2) observed where the dust ring is detected, with a ‚àº 40 ‚àí 50% fraction of optically thick emission. Assuming a 12 CO abundance of 5 √ó 10 ‚àí 5 and a 13 CO abundance 70 times lower, our synthetic images reproduce the morphology of the 12 CO ( J = 3 ‚àí 2) and 13 CO ( J = 3 ‚àí 2) observed channel maps where the cloud absorption allowed a detection. From our simulations, we estimate that a stellar mass M (cid:63) = 0 . 2M (cid:12) and a systemic velocity (cid:51) sys = 6 . 25kms ‚àí 1 are needed to reproduce the gas rotation as retrieved from molecular line observations. Applying an empirical relation between planet mass and gap width in the dust, we predict a maximum planet mass of ‚àº 4 ‚àí 8M Jup . Conclusions. Our results suggest the presence of a massive planet orbiting CIDA 1, thus challenging our understanding of planet formation around VLM stars.",
        "authors": [
            "P. Curone",
            "A. Izquierdo",
            "L. Testi",
            "G. Lodato",
            "S. Facchini",
            "A. Natta",
            "P. Pinilla",
            "N. Kurtovic",
            "C. Toci",
            "M. Benisty",
            "M. Tazzari",
            "F. Borsa",
            "M. Lombardi",
            "C. Manara",
            "E. Sanchis",
            "L. Ricci"
        ],
        "citations": 7,
        "references": 6,
        "year": 2022
    },
    {
        "title": "Survey of Multiple Populations in Globular Clusters among Very-low-mass Stars",
        "abstract": "Recent work has shown that near-infrared (NIR) Hubble Space Telescope (HST) photometry allows us to disentangle multiple populations (MPs) among M dwarfs of globular clusters (GCs) and to investigate this phenomenon in very-low-mass (VLM) stars. Here, we present the color‚Äìmagnitude diagrams of nine GCs and the open cluster NGC 6791 in the F110W and F160W bands of HST, showing that the main sequences (MSs) below the knee are either broadened or split, thus providing evidence of MPs among VLM stars. In contrast, the MS of NGC 6791 is consistent with a single population. The color distribution of M dwarfs dramatically changes between different GCs, and the color width correlates with the cluster mass. We conclude that the MP ubiquity, variety, and dependence on GC mass are properties common to VLM and more-massive stars. We combined UV, optical, and NIR observations of NGC 2808 and NGC 6121 (M4) to identify MPs along with a wide range of stellar masses (‚àº0.2‚Äì0.8 Óàπ‚äô ), from the MS turnoff to the VLM regime, and measured, for the first time, their mass functions (MFs). We find that the fraction of MPs does not depend on the stellar mass and that their MFs have similar slopes. These findings indicate that the properties of MPs do not depend on stellar mass. In a scenario where the second generations formed in higher-density environments than the first generations, the possibility that the MPs formed with the same initial MF would suggest that it does not depend on the environment.",
        "authors": [
            "E. Dondoglio",
            "A. Milone",
            "A. Renzini",
            "E. Vesperini",
            "E. Lagioia",
            "A. Marino",
            "A. Bellini",
            "M. Carlos",
            "G. Cordoni",
            "S. Jang",
            "M. V. Legnardi",
            "M. Libralato",
            "A. Mohandasan",
            "F. D‚ÄôAntona",
            "M. Martorano",
            "F. Muratore",
            "M. Tailo"
        ],
        "citations": 6,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Comparison of SaCoVLM‚Ñ¢ video laryngeal mask-guided intubation and i-gel combined with flexible bronchoscopy-guided intubation in airway management during general anesthesia: a non-inferiority study",
        "abstract": null,
        "authors": [
            "Chun-ling Yan",
            "Yi-qi-yuan Zhang",
            "Ying-An Chen",
            "Zong-yang Qv",
            "M. Zuo"
        ],
        "citations": 6,
        "references": 28,
        "year": 2022
    },
    {
        "title": "Epoch-Based Height Reference System for Sea Level Rise Impact Assessment on the Coast of Peninsular Malaysia",
        "abstract": "The Peninsular Malaysia Geodetic Vertical Datum 2000 (PMGVD2000) inherited several deficiencies due to offsets between local datums used, levelling error propagations, land subsidence, sea level rise, and sea level slopes along the southern half of the Malacca Strait on the west coast and the South China Sea in the east coast of the Peninsular relative to the Port Klang (PTK) datum point. To cater for a more reliable elevation-based assessment of both sea level rise and coastal flooding exposure, a new epoch-based height reference system PMGVD2022 has been developed. We have undertaken the processing of more than 30 years of sea level data from twelve tide gauge (TG) stations along the Peninsular Malaysia coast for the determination of the relative mean sea level (RMSL) at epoch 2022.0 with their respective trends and incorporates the quantification of the local vertical land motion (VLM) impact. PMGVD2022 is based on a new gravimetric geoid (PMGeoid2022) fitted to the RMSL at PTK. The orthometric height is realised through the GNSS levelling concept H = hGNSS‚ÄìNfit_PTK‚ÄìNRMDT, where NRMDT is a constant offset due to the relative mean dynamic ocean topography (RMDT) between the fitted geoid at PTK and the local MSL datums along the Peninsular Malaysia coast. PMGVD2022 will become a single height reference system with absolute accuracies of better than ¬±3 cm and ¬±10 cm across most of the land/coastal area and the continental shelf of Peninsular Malaysia, respectively.",
        "authors": [
            "Sanusi Cob",
            "M. Kadir",
            "R. Forsberg",
            "W. Simons",
            "M. Naeije",
            "Ami Hassan Md Din",
            "Husaini Yacob",
            "A. Amat",
            "Daud Mahdzur",
            "Zuhairy Ibrahim",
            "Kenidi Aziz",
            "Norehan Yaacob",
            "F. Johann",
            "T. Jensen",
            "Hergeir Teitsson",
            "S. Ses",
            "Anim Yahaya",
            "Soeb Nordin",
            "Fadhil Majid"
        ],
        "citations": 7,
        "references": 67,
        "year": 2022
    },
    {
        "title": "Separating GIA signal from surface mass change using GPS and GRACE data",
        "abstract": "\n The visco-elastic response of the solid Earth to the past glacial cycles and the present day surface mass change (PDSMC) are detected by the geodetic observation systems such as global navigation satellite system (GNSS) and satellite gravimetry. Majority of the contemporary PDSMC is driven by climate change and in order to better understand them using the aforementioned geodetic observations, glacial isostatic adjustment (GIA) signal should be accounted first. The default approach is to use forward GIA models that use uncertain ice-load history and approximate Earth rheology to predict GIA, yielding large uncertainties. The proliferation of contemporary, global, geodetic observations and their coverage have therefore enabled estimation of data-driven GIA solutions. A novel framework is presented that uses geophysical relations between the vertical land motion (VLM) and geopotential anomaly due to GIA and PDSMC to express GPS VLM trends and GRACE geopotential trends as a function of either GIA or PDSMC, which can be easily solved using least-squares regression. The GIA estimates are data-driven and differ significantly from forward models over Alaska and Greenland.",
        "authors": [
            "B. Vishwakarma",
            "Y. Ziegler",
            "J. Bamber",
            "S. Royston"
        ],
        "citations": 5,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Black-box Online Aerodynamic Performance Optimization for a Seamless Wing with Distributed Morphing",
        "abstract": "Morphing is a promising bio-inspired technology, with the potential to make aircraft more economical and sustainable through adaptation of the wing shape for best efficiency at any flight condition. This paper proposes an online black-box performance optimization strategy for a seamless wing with distributed morphing control. Pursuing global performance, the presented method integrates a global radial basis function neural network (RBFNN) surrogate model with a derivative-free evolutionary optimization algorithm. The effectiveness of the optimization strategy was validated on a vortex lattice method (VLM) aerodynamic model of an over-actuated morphing wing augmented by wind tunnel experiment data. Simulations show that the proposed method is able to control the morphing shape and angle of attack to achieve various target lift coefficients with better aerodynamic efficiency than the unmorphed wing shape. The global nature of the on-board model allows the presented method to find shape solutions for a wide range of target lift coefficients without the need for additional model excitation maneuvers. Compared to the unmorphed shape, up to 14 . 6 % of lift-to-drag ratio increase is achieved",
        "authors": [
            "Oscar Ruland",
            "T. Mkhoyan",
            "R. De Breuker",
            "Xuerui Wang"
        ],
        "citations": 4,
        "references": 25,
        "year": 2022
    },
    {
        "title": "Grain Growth in the Dust Ring with a Crescent around the Very Low-mass Star ZZ Tau IRS with JVLA",
        "abstract": "The azimuthal asymmetries of dust rings in protoplanetary disks such as a crescent around young stars are often interpreted as dust traps, and thus as ideal locations for planetesimal and planet formations. Whether such dust traps effectively promote planetesimal formation in disks around very low-mass stars (VLM; a mass of ‚â≤0.2 M ‚òâ) is debatable, as the dynamical and grain growth timescales in such systems are long. To investigate grain growth in such systems, we studied the dust ring with a crescent around the VLM star ZZ Tau IRS using the Karl G. Jansky Very Large Array at centimeter wavelengths. Significant signals were detected around ZZ Tau IRS. To estimate the maximum grain size ( amax ) in the crescent, we compared the observed spectral energy distribution (SED) with SEDs for various amax values predicted by radiative transfer calculations. We found amax‚â≥ 1 mm and ‚â≤60 Œºm in the crescent and ring, respectively, though our modeling efforts rely on uncertain dust properties. Our results suggest that grain growth occurred in the ZZ Tau IRS disk, relative to the sub-micron-sized interstellar medium. Planet formation in a crescent with millimeter-sized pebbles might proceed more efficiently than in other regions with submillimeter-sized pebbles via pebble accretion scenarios.",
        "authors": [
            "J. Hashimoto",
            "H. Liu",
            "R. Dong",
            "Beibei Liu",
            "T. Muto"
        ],
        "citations": 5,
        "references": 61,
        "year": 2022
    },
    {
        "title": "Land Subsidence Estimation With Tide Gauge and Satellite Radar Altimetry Measurements Along the Texas Gulf Coast, USA",
        "abstract": "A double-difference (DD) method was used to estimate vertical land motion (VLM) at 26 tide gauge (TG) sites with record lengths of at least ten years across the Texas Gulf Coast, USA, between 1993 and 2020. In the method, the first difference was conducted by coupling nearby correlated TG stations to remove sea-level variability for both TG and satellite radar altimetry (SRA) data. Upon completion of the first difference, a second difference was performed by subtracting between TG and SRA data. The results obtained from the DD method were compared against that of: 1) a single-difference (SD) method through subtraction between measurements from TG and SRA and 2) a global navigation satellite system (GNSS) precise point positioning (PPP) method. The results showed that the DD method improved the performance of VLM estimation with an uncertainty below 1.0 mm/yr at most TG stations. Meanwhile, the estimated VLM trends acquired from the DD method correlated better to that of the ground-truth GNSS PPP solutions than the SD method. The DD method possesses great potential to discover VLM knowledge, particularly along coastal regions where other techniques such as GNSS and interferometric synthetic aperture radar (InSAR) are of impaired estimation capability.",
        "authors": [
            "Xiaojun Qiao",
            "Tianxing Chu",
            "P. Tissot",
            "Jason Louis",
            "Ibraheem Ali"
        ],
        "citations": 5,
        "references": 0,
        "year": 2022
    },
    {
        "title": "The significance of vertical land movements at convergent plate boundaries in probabilistic sea-level projections for AR6 scenarios: The New Zealand case",
        "abstract": "Anticipating and managing the impacts of sea-level rise for nations astride active tectonic margins requires rates of sea surface elevation change in relation to coastal land elevation to be understood. Vertical land motion (VLM) can either exacerbate or reduce sea-level changes with impacts varying significantly along a coastline. Determining rate, pattern, and variability of VLM near coasts leads to a direct improvement of location-specific relative sea level (RSL) estimates. Here, we utilise vertical velocity field from interferometric synthetic aperture radar (InSAR) data, calibrated with campaign and continuous Global Navigation Satellite System (GNSS), to determine the VLM for the entire coastline of New Zealand. Guided by existing knowledge of the seismic cycle, the VLM data infer long-term, interseismic rates of land surface deformation. We build probabilistic RSL projections using the Framework for Assessing Changes to Sea-level (FACTS) from IPCC Assessment Report 6 and ingest local VLM data to produce RSL projections at 7435 sites, thereby enhancing spatial coverage that was previously limited to tide gauges. We present ensembles of probability distributions of RSL for medium confidence climatic processes for each scenario to 2150 and low confidence processes to 2300. For regions where land subsidence is occurring at rates >2mm yr-1 VLM makes a significant contribution to RSL projections for all scenarios out 2150. Beyond 2150, for higher emissions scenarios, the land ice contribution to global sea level dominates. We discuss the planning implications of RSL projections, where timing of threshold exceedance for coastal inundation can be brought forward by decades.",
        "authors": [
            "T. Naish",
            "R. Levy",
            "I. Hamling",
            "G. Garner",
            "Sigr√∫n",
            "Hreinsd√≥ttir",
            "Robert E Kopp",
            "N. Golledge",
            "R. Bell",
            "R. Paulik",
            "Judy",
            "Lawrence",
            "P. Denys",
            "T. Gillies",
            "Shannon Bengston",
            "K. Clark",
            "Daniel",
            "King",
            "N. Litchfield",
            "L. Wallace",
            "R. Newnham"
        ],
        "citations": 5,
        "references": 139,
        "year": 2022
    },
    {
        "title": "Prediction of Sea Level with Vertical Land Movement Correction Using Deep Learning",
        "abstract": "Sea level rise (SLR) in small island countries such as Kiribati and Tuvalu have been a significant issue for decades. There is an urgent need for more accurate and reliable scientific information regarding SLR and its trend and for more informed decision making. This study uses the tide gauge (TG) dataset obtained from locations in Betio, Kiribati and Funafuti, Tuvalu with sea level corrections for vertical land movement (VLM) at these locations from the data obtained by the Global Navigation Satellite System (GNSS) before the sea level trend and rise predictions. The oceanic feature inputs of water temperature, barometric pressure, wind speed, wind gust, wind direction, air temperature, and three significant lags of sea level are considered in this study for data modeling. A new data decomposition method, namely, successive variational mode decomposition (SVMD), is employed to extract intrinsic modes of each feature that are processed for selection by the Boruta random optimizer (BRO). The study develops a deep learning model, namely, stacked bidirectional long short-term memory (BiLSTM), to make sea level (target variable) predictions that are benchmarked by three other AI models adaptive boosting regressor (AdaBoost), support vector regression (SVR), and multilinear regression (MLR). With a comprehensive evaluation of performance metrics, stacked BiLSTM attains superior results of 0.994207, 0.994079, 0.988219, and 0.899868 for correlation coefficient, Wilmott‚Äôs Index, the Nash‚ÄìSutcliffe Index, and the Legates‚ÄìMcCabe Index, respectively, for Kiribati, and with values of 0.996806, 0.996272, 0.992316, and 0.919732 for correlation coefficient, Wilmott‚Äôs Index, the Nash‚ÄìSutcliffe Index, and the Legates‚ÄìMcCabe Index, respectively, for the case of Tuvalu. It also shows the lowest error metrics in prediction for both study locations. Finally, trend analysis and linear projection are provided with the GNSS-VLM-corrected sea level average for the period 2001 to 2040. The analysis shows an average sea level rate rise of 2.1 mm/yr for Kiribati and 3.9 mm/yr for Tuvalu. It is estimated that Kiribati and Tuvalu will have a rise of 80 mm and 150 mm, respectively, by the year 2040 if estimated from year 2001 with the current trend.",
        "authors": [
            "N. Raj"
        ],
        "citations": 5,
        "references": 91,
        "year": 2022
    },
    {
        "title": "Aeroelastic method to investigate nonlinear elastic wing structures",
        "abstract": null,
        "authors": [
            "K. Bramsiepe",
            "T. Klimmek",
            "W. Kr√ºger",
            "Lorenz Tichy"
        ],
        "citations": 4,
        "references": 16,
        "year": 2022
    },
    {
        "title": "Golden Retriever: A Real-Time Multi-Modal Text-Image Retrieval System with the Ability to Focus",
        "abstract": "In this work, we present the Golden Retriever, a system leveraging state-of-the-art visio-linguistic models (VLMs) for real-time text-image retrieval. The unique feature of our system is that it can focus on words contained in the textual query, i.e., locate and high-light them within retrieved images. An efficient two-stage process implements real-time capability and the ability to focus. Therefore, we first drastically reduce the number of images processed by a VLM. Then, in the second stage, we rank the images and highlight the focussed word using the outputs of a VLM. Further, we introduce a new and efficient algorithm based on the idea of TF-IDF to retrieve images for short textual queries. One of multiple use cases where we employ the Golden Retriever is a language learner scenario, where visual cues for \"difficult\" words within sentences are provided to improve a user's reading comprehension. However, since the backend is completely decoupled from the frontend, the system can be integrated into any other application where images must be retrieved fast. We demonstrate the Golden Retriever with screenshots of a minimalistic user interface.",
        "authors": [
            "Florian Schneider",
            "Chris Biemann"
        ],
        "citations": 3,
        "references": 23,
        "year": 2022
    },
    {
        "title": "A Study on the Scale Effect According to the Reynolds Number in Propeller Flow Analysis and a Model Experiment",
        "abstract": "The demand for new propeller designs has increased alongside the development of new technology, such as urban aircraft and large unmanned aerial vehicles. In order to experimentally identify the performance of a propeller, a wind tunnel that provides the operating flow is essential. However, in the case of a meter class or larger propeller, a large wind tunnel is required and the related equipment becomes heavy; therefore, it is difficult to implement in reality. For this reason, propeller studies have been conducted via reduced models. In this case, it is necessary to investigate the different performance outputs between the full- and model-scale propellers due to the size difference. In the current study, a method is proposed to investigate the difference in the aerodynamic performance caused by the difference in propeller scale using VLM and RANS calculations, and the differences are analyzed. The wind tunnel test also verified the propeller performance prediction method. The boundary of aerodynamic performance independent of the Reynolds number could be predicted through the VLM based on the ideal fluid assumption. From the RANS calculations, it was possible to present the difference in the aerodynamic performance when propellers of the same geometry with different ratios were operated using different Reynolds numbers. It was confirmed that each numerical method matched well with the wind tunnel test results in the range of the advance ratio that produced the maximum efficiency, and from the results, it was possible to observe the change in aerodynamic performance that differed according to the scale change.",
        "authors": [
            "Yeong-Ju Go",
            "J. Bae",
            "Jaeha Ryi",
            "J. Choi",
            "Chung-Ryeol Lee"
        ],
        "citations": 3,
        "references": 24,
        "year": 2022
    },
    {
        "title": "Multidisciplinary Optimisation of an eVTOL UAV With a Hydrogen Fuel Cell",
        "abstract": "To explore the use of hydrogen fuel cells as a feasible alternative on Unmanned Aerial Vehicles (UAVs), a class I concept was designed at the Portuguese Air Force Research Centre (CIAFA). This work focuses on the Multidisciplinary Design Optimisation (MDO) methodology that was used to improve the 3h endurance of the baseline concept that had a Maximum Take-Off Weight of 21.6 kg, using 148 g of hydrogen and a 800 W fuel cell to power conventional flight operations. Another propulsive system comprised of batteries and rotors is used for Vertical Take-Off and Landing (VTOL). MDO was performed with the aid of OpenAeroStruct, a low fidelity software that combines Finite Element Analysis (FEA) and Vortex Lattice Method (VLM) to model lifting surfaces. Initially, a cruise and a load flight conditions were used with structural parameters and geometric twist as design variables. In a second approach, complexity was increased by including taper, wing chord and span as design variables in the problem formulation. Lastly, a third flight condition was introduced to ensure stall requirements were met. The use of MDO led to a 21% increase in endurance with a smaller wing, while satisfying all imposed constraints. This work marks an important milestone in the development of a future prototype at the CIAFA.",
        "authors": [
            "Bernardo Alves",
            "A. Marta",
            "Lu√≠s F√©lix"
        ],
        "citations": 3,
        "references": 18,
        "year": 2022
    },
    {
        "title": "ILLUME: Rationalizing Vision-Language Models through Human Interactions",
        "abstract": "Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.",
        "authors": [
            "Manuel Brack",
            "P. Schramowski",
            "Bjorn Deiseroth",
            "K. Kersting"
        ],
        "citations": 3,
        "references": 46,
        "year": 2022
    },
    {
        "title": "Aligning MAGMA by Few-Shot Learning and Finetuning",
        "abstract": "The goal of vision-language modeling is to allow models to tie language understanding with visual inputs. The aim of this paper is to evaluate and align the Visual Language Model (VLM) called Multimodal Augmentation of Generative Models through Adapter-based finetuning (MAGMA) with human values. MAGMA is a VLM that is capable of image captioning and visual question-answering. We will evaluate its alignment in three different scenarios. To begin, we assess MAGMA's out-of-the-box alignment through the checkpoint provided by Hugging Face. Then, we measure if few-shot learning manages to improve the results. Finally, we finetune the model on aligned examples and evaluate its behavior.",
        "authors": [
            "Jean-Charles Layoun",
            "Alexis Roger",
            "I. Rish"
        ],
        "citations": 2,
        "references": 8,
        "year": 2022
    },
    {
        "title": "Benchmark of different aerodynamic solvers on wing aero-propulsive interactions",
        "abstract": "Distributed electric propulsion is a fertile research topic aiming to increase the wing aerodynamic efficiency by distributing the thrust over the wing span. The blowing due to distributed propulsors shall increase the wing lift coefficient for a given planform area and flight speed. This should bring several advantages as wing area, drag, and structural weight reduction, which in turn reduce fuel consumption, allowing airplanes to fly more efficiently. However, there are no consolidated preliminary design methods to size a distributed propulsion system. Numerical analysis is then performed at early stage, where many design variables have not been fixed yet. Therefore, the design space is vast and exploring all the possible combinations is unfeasible. For instance, low-fidelity methods (VLM, panel codes) have a low computational time, but usually they do not account for flow separation and hence they are unable to predict the wing maximum lift. Conversely, high-fidelity codes (CFD) provide more realistic results, but a single drag polar sweep can last days. This work provides a benchmark of different aerodynamic solvers for a typical regional turboprop wing with flaps and distributed propulsion, to better understand the limits of each software in the prediction of aero-propulsive effects.",
        "authors": [
            "D. Ciliberti",
            "E. B√©nard",
            "F. Nicolosi"
        ],
        "citations": 2,
        "references": 22,
        "year": 2022
    },
    {
        "title": "In vivo absolute quantification of carnosine in the vastus lateralis muscle with 1H MRS using a surface coil and water as internal reference",
        "abstract": null,
        "authors": [
            "Gloria Vega",
            "G. Ricaurte",
            "Mauricio Estrada-Castrill√≥n",
            "H. Reyngoudt",
            "Oscar Cardona",
            "J. Gallo-Villegas",
            "R. Narvaez-Sanchez",
            "J. Calder√≥n"
        ],
        "citations": 2,
        "references": 47,
        "year": 2022
    },
    {
        "title": "A Vortex Lattice Method for the Hydrodynamic Solution of Lifting Bodies Traveling Close and Across a Free Surface",
        "abstract": "The hydrodynamics performance of submerged and surface-piercing lifting bodies is analyzed by a potential flow model based on a Vortex Lattice Method (VLM). Such a numerical scheme, widely applied in aerodynamics, is particularly suitable to model the lifting effects thanks to the vortex distribution used to discretize the boundaries of the lifting bodies. The method has been developed with specific boundary conditions to account for the development of steady free surface wave patterns. Both submerged bodies, such as flat plates and hydrofoils, as well as planing hulls can be studied. The method is validated by comparison against available experimental data and other Computational Fluid Dynamic (CFD) results from Reynolds Averaged Navier Stokes (RANS) approaches. In all the analyzed cases, namely 2D and 3D flat plates, a NACA hydrofoil, planning flat plates and prismatic planing hulls, results have been found to be consistent with those taken as reference. The obtained hydrodynamic predictionsare discussed highlighting the advantages and the possible improvements of the developed approach.",
        "authors": [
            "Raffaele Solari",
            "P. Bagnerini",
            "G. Vernengo"
        ],
        "citations": 2,
        "references": 21,
        "year": 2022
    },
    {
        "title": "Intermediate Points for Missions to Interstellar Objects Using Optimum Interplanetary Trajectory Software",
        "abstract": "This paper explicates the concept of an ‚ÄòIntermediate Point‚Äô (IP), its incorporation as a node along an interplanetary trajectory, and how it permits the determination and optimization of trajectories to interstellar objects (ISOs). IPs can be used to model Solar Oberth Manoeuvres (SOM) as well as V ‚àû Leveraging Manoeuvres (VLM). The SOM has been established theoretically as an important mechanism by which rapid exit from the solar system and intercept of ISOs can both be achieved; the VLM has been demonstrated practically as a method of reducing overall mission ŒîV as well as the Characteristic Energy, C 3 , at launch. Thus via these two applications, the feasibility of missions to interstellar objects (ISOs) such as 1I/‚ÄôOumuamua can be analysed. The interplanetary trajectory optimizer tool exploited for this analysis, OITS, permits IP selection as an encounter option along the interplanetary trajectory in question. OITS adopts the assumption of impulsive thrust at discrete points along the trajectory, an assumption which is generally valid for high thrust propulsion scenarios, like chemical or nuclear thermal for instance.",
        "authors": [
            "A. Hibberd"
        ],
        "citations": 2,
        "references": 20,
        "year": 2022
    },
    {
        "title": "Orbital Venolymphatic Malformation Treated With Sodium Tetradecyl Sulfate: A Case Report",
        "abstract": "Orbital and periorbital venolymphatic malformations (VLMs) are benign congenital vascular lesions and constitute 1%-3% of all orbital masses. Widespread facial venous malformations have a high incidence of associated intracranial developmental venous anomalies (DVAs). In such cases, there can be a sudden increase in proptosis following upper respiratory infection or minor trauma. Numerous percutaneous intralesional sclerosing agents like sodium tetradecyl sulfate (STS), bleomycin, doxycycline, ethanol, and OK-432 (Picibanil) have been used for treating VLMs. We hereby report a rare case of retro-orbital VLM treated successfully with STS injection and an isolated dural arterio-venous (AV) fistula in the same patient.",
        "authors": [
            "Sucharita Das",
            "A. Agrawal",
            "S. Burathoki",
            "K. Nandolia",
            "A. Juneja",
            "R. Samanta"
        ],
        "citations": 2,
        "references": 9,
        "year": 2022
    },
    {
        "title": "GPS data from 2019 and 2020 campaigns in the Chesapeake Bay region towards quantifying vertical land motions",
        "abstract": null,
        "authors": [
            "G. Troia",
            "D. S. Stamps",
            "R. Lotspeich",
            "James Duda",
            "K. J. McCoy",
            "W. Moore",
            "Philippe F. Hensel",
            "R. Hippenstiel",
            "T. McKenna",
            "D. Andreasen",
            "C. Geoghegan",
            "T. Ulizio",
            "Madeline Kronebusch",
            "J. Carr",
            "D. Walters",
            "Neil Winn"
        ],
        "citations": 2,
        "references": 23,
        "year": 2022
    },
    {
        "title": "Mapping Vertical Land Motion in Challenging Terrain: Six‚ÄêYear Trends on Tutuila Island, American Samoa, With PS‚ÄêInSAR, GPS, Tide Gauge, and Satellite Altimetry Data",
        "abstract": "Sea level rise is a major challenge facing coastlines worldwide and can be strongly exacerbated by land subsidence. However, detailed characterization of vertical land motion (VLM) is limited for many tectonically active islands, as many remote sensing methods are hindered by dense vegetation and thick cloud cover. In American Samoa, strong post‚Äêseismic deformation from the 2009 Samoa‚ÄêTonga earthquake has increased flooding, but large uncertainties remain in hazard forecasting as only point measurements of VLM have been available. Here, we present novel VLM results over Tutuila, the largest and most populated island in American Samoa, using interferometric synthetic aperture radar, GPS, tide gauge, and satellite altimetry data. Measurements cover populated areas, with subsidence rates of 6‚Äì9 mm/yr and uncertainties of <1 mm/yr; the highest rates lie along the coastlines. We find differences in rate changes across the island, suggesting that local processes need to be well‚Äêconstrained for effective flood forecasting efforts.",
        "authors": [
            "Stacey A. Huang",
            "J. Sauber",
            "R. Ray"
        ],
        "citations": 2,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Application of a Comprehensive Design Method to Counter-Rotating Propellers",
        "abstract": "A comprehensive design method is applied to design of counter-rotating propellers (CRP). A numerical nonlinear optimization algorithm is first used for design of each propeller. This approach represents a blade by B-spline geometry and the design variables at the location of the vertices of the B-spline polygon determine the optimal blade shape. The nonlinear optimization method aims at minimizing the torque for a given target thrust with constraints, e.g. the minimum pressure constraint for a fully wetted propeller or maximum allowed cavity area for a cavitating propeller. Then the interaction of the designed propellers and a given pod including the viscous flow field around the two propellers is analyzed by coupling a vortex-lattice method (VLM) with a Reynolds-Averaged Navier-Stokes (RANS) solver. The analysis determines a new inflow for a new design of propellers. The procedure of the design and interaction analysis finishes when the propeller thrust converges within a certain tolerance. Finally, the designed propellers are compared with the original propellers.",
        "authors": [
            "K. Cha",
            "S. Kinnas"
        ],
        "citations": 2,
        "references": 0,
        "year": 2022
    },
    {
        "title": "Simplified vortex methods to model wake vortex roll-up in real-time simulations for fuel-saving formation flight",
        "abstract": null,
        "authors": [
            "Henrik Spark",
            "R. Luckner"
        ],
        "citations": 1,
        "references": 33,
        "year": 2022
    },
    {
        "title": "Monitoring Megathrust-Earthquake-Cycle-Induced Relative Sea-Level Changes near Phuket, South Thailand, Using (Space) Geodetic Techniques",
        "abstract": "Temporal changes in vertical land motion (VLM) in and around Phuket Island in southern Thailand following the great 2004 Sumatra‚ÄìAndaman megathrust earthquake have impacted the relative sea-level change estimates based on tide-gauge (TG) records. To better monitor the VLM, two new continuous global navigation satellite system (GNSS) stations have been installed in the past 5 years, situated on bedrock both near and at the Koh Taphao Noi Island TG in Phuket, which together with older global positioning system (GPS) data provides a clear insight in the VLM of Phuket Island from 1994 onward. In addition, satellite altimetry (SALT) data has been analyzed since 1992. The VLM (GPS) position and relative (TG) and absolute (SALT) sea-level change time series were successfully combined in pairs to validate each independent result (e.g., SALT ‚àí GNSS = TG): prior to the 2004 earthquake, the relative sea-level rise in Phuket was 1.0 ¬± 0.7 mm/yr, lower by 2.4 ¬± 0.2 mm/yr than the absolute sea-level rise caused by VLM. After the earthquake, nonlinear post-seismic subsidence has caused the VLM to drop by 10 cm in the past 17 years, resulting, by the end of 2020, in a relative sea-level rise by up to 16 cm. During the same period, other TG stations in south Thailand recorded similar sea-level increases. Combination with SALT further suggests that, prior to 2005, uplift (5.3 ¬± 1.4 mm/yr) of the coastal region of Ranong (north of Phuket) resulted in a relative sea-level fall, but since then, post-seismic-induced negative VLM may have significantly increased coastal erosion along the entire Andaman Sea coastline.",
        "authors": [
            "M. Naeije",
            "W. Simons",
            "Siriporn Pradit",
            "Sommart Niemnil",
            "N. Thongtham",
            "M. A. Mustafar",
            "P. Noppradit"
        ],
        "citations": 1,
        "references": 21,
        "year": 2022
    },
    {
        "title": "ILLUME: Rationalizing Vision-Language Models by Interacting with their Jabber",
        "abstract": "Bootstrapping from pre-trained language models has been proven to be an efÔ¨Åcient approach for building foundation vision-language models (VLM) for tasks such as im- age captioning or visual question answering. However, it is difÔ¨Åcult‚Äîif not impossible‚Äîto utilize it to make the model conform with user‚Äôs rationales for speciÔ¨Åc answers. To elicit and reinforce commonsense reasons, we propose an iterative sampling and tuning paradigm, called I LLUME , that executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides minimal feedback via preference selec- tion, used for Ô¨Åne-tuning. This loop increases the training data and gradually carves out the VLM‚Äôs rationalization capabili- ties. Our exhaustive experiments demonstrate that I LLUME is competitive with standard supervised Ô¨Åne-tuning while using signiÔ¨Åcantly fewer training data and only requiring minimal feedback.",
        "authors": [
            "Manuel Brack",
            "P. Schramowski",
            "Bjorn Deiseroth",
            "K. Kersting"
        ],
        "citations": 1,
        "references": 33,
        "year": 2022
    },
    {
        "title": "Vertical Land Motion Monitoring at Tide Gauges in Korean Peninsula using Sequential Sbas-Insar",
        "abstract": "Vertical land motion (VLM) is an essential information for relative sea level changes measured by tide gauges. Corrections to tide gauge records for VLM will provide information to understand the regional and global sea level changes. In this study, time-series SBAS-InSAR algorithm was implemented to measure the ground subsidence/uplift at tide gauges located in the Korean peninsula using Sentinel-1 A/B SAR data. We processed about 10 tide gauges in Korea and observed VLM at Pohang, Incheon and Jeju tide stations.",
        "authors": [
            "S. K. P. Vadivel",
            "Duk‚Äêjin Kim",
            "Jungkyo Jung",
            "Yang‚ÄêKi Cho"
        ],
        "citations": 1,
        "references": 9,
        "year": 2022
    },
    {
        "title": "Virtual-stator Loss Model for Synchronous Generators",
        "abstract": "In synchronous generators, end region field can cause additional losses at clamping structures and stator end core. It can be a computational challenge to solve the end region field of synchronous generators with fractional-slot winding. With a minor change of the number of stator slots, three-dimensional (3D) Virtual-stator Loss Model (VLM) can largely reduce computational burden of end region field computation. This paper investigates VLM theory and its implementation. Compared to Real-stator Loss Model, VLM demonstrates excellent accuracy when implemented with two-dimensional finite element method. This indicates that 3D VLM can be used for the accurate computation of losses induced by end region field, while the computational challenge is dramatically reduced. Lab test at a 100kVA salient pole synchronous generator proves that the analysis approach is trustful.",
        "authors": [
            "Zhaoqiang Zhang",
            "A. Nysveen",
            "B√∏rge Johannes Fagermyr",
            "R. Nilssen",
            "H. Ehya"
        ],
        "citations": 1,
        "references": 19,
        "year": 2022
    },
    {
        "title": "Quantifying vertical land motion at tide gauge sites using permanent scatterer interferometric synthetic aperture radar and global navigation satellite system solutions",
        "abstract": null,
        "authors": [
            "R. Reyes",
            "M. D. A. Bauzon",
            "N. Pasaje",
            "Rey Mark Alfante",
            "Pocholo Miguel A. De Lara",
            "Marion Ordillano",
            "Paul Caesar M. Flores",
            "A. Rediang",
            "Patrick Anthony Nota",
            "F. Siringan",
            "A. Blanco",
            "D. Bringas"
        ],
        "citations": 1,
        "references": 33,
        "year": 2022
    },
    {
        "title": "Tracking Coastal Change in American Samoa by Mapping Local Vertical Land Motion with PS-InSAR",
        "abstract": "Characterizing diverse contributions to coastal land change is a key step to mitigating the effects of rising sea levels that threaten coastal communities. This task is particularly critical for small island communities in tectonically active regions, which are highly vulnerable to the effects of sea level rise. We highlight here a case study to extract regional estimates of vertical land motion (VLM) over American Samoa, which in recent years has observed increased nuisance flooding. We used persistent scatterer Interferometric Synthetic Aperture Radar (PS-InSAR) to derive high-resolution estimates of crustal deformation in populated regions over Tutuila Island from 2016 to 2021. While the area is small and highly vegetated and poses challenges for InSAR, we were able to construct a regional map of the estimated deformation rate in these areas and validate the time-series with a local GPS station. Our preliminary results suggest that PS-InSAR has the ability to capture local and regional deformation patterns. Further work to refine our VLM estimate will include models to account for more complex atmospheric effects and estimates of error margins, as well as integration of our results into models of sealevel change that can be used by local stakeholders.",
        "authors": [
            "Stacey A. Huang",
            "J. Sauber"
        ],
        "citations": 1,
        "references": 13,
        "year": 2022
    },
    {
        "title": "Effect of Vernonia amygdalina leaf meal on the reproductive indices of male rabbits",
        "abstract": "Abstract The study evaluated the effect of Vernonia amygdalina leaf meal on semen indices, serum testosterone and sperm reserve of male rabbits. Forty rabbit bucks were randomly assigned into four groups and fed the experimental diets containing Vernonia amygdalina (VLM) at 0, 5, 10 and 15 % levels for 84days. Reproductive indices were evaluated using standard procedures. Data obtained were subjected to analysis of variance at p<0.05. All semen in rabbits fed 0, 5 and 10 % VLM had milky colour while 14.8% light green and 85.2% milky semen colour was observed in bucks fed 15%VLM. Libido score reduced in rabbits fed VLM diets. Bucks fed 15%VLM had significantly higher semen volume (0.47ml). VLM had no significant effect on spermatozoa mass motility, progressive motility and sperm concentration. Live sperm cells significantly increased in bucks fed 5 and 10% VLM diets. VLM had significant (p<0.05) effect on spermatozoa morphology. Vernonia amygdalina leaf meal had no significant (p<0.05) impact on testosterone, testicular and epididymal indices. In conclusion, up to 10%VLM can be adopted as feed ingredient for male rabbit breeder stock without deleterious effect on reproductive indices.",
        "authors": [
            "A. Adeyemi",
            "Christiana Oloyede",
            "Adedamola Adedotun"
        ],
        "citations": 1,
        "references": 10,
        "year": 2022
    },
    {
        "title": "Vision Encoders in Visual Question Answering",
        "abstract": "Most existing methods that apply pretrained Visual Language Models (VLMs) to vision and language tasks do not sufficiently explore the effect of the format of their inputs on downstream performance. We show that utilising appropriate prompt formatting is a simple yet effective approach to improving the few-shot performance of VLMs that use relatively small language models on the Visual Question Answering (VQA) task. We format the inputs used to prompt a VLM using a modified text-only template from a closed-book question answering task that the language-model component of the VLM was pretrained on. By doing this, we explicitly align the VQA task with a task that this language model has already seen, enabling the VLM to leverage the similarities between the tasks, such as the answer-length distribution, when generating answers to the visual questions. In order to test our claims, we implement a simple architecture based on Frozen (Tsimpoukelli et al., 2021) and ClipCap (Mokady et al., 2021), whereby, through image captioning, the VLM learns to integrate powerful pretrained vision-only and language-only models via a relatively simple learnt mapping network. Furthermore, we contextualise our approach relative to existing work by presenting a unified view of VLMs. Our results show that explicit alignment enables our VLMs to achieve a significantly higher zero-shot (34.49% vs 20.89%) and best overall (40.39% vs 30.83%) VQA score on the VQA2.0 dataset (Goyal et al., 2017) than when the prompt template from Frozen (Tsimpoukelli et al., 2021) and Flamingo (Alayrac et al., 2022) is used. Furthermore, our zero-shot and best overall performance is better than Frozen‚Äôs (34.49% vs 29.5% and 40.39% vs 38.2%, respectively) despite Frozen using a language model with more than double the number of parameters. Our code is available here.",
        "authors": [
            "Ryan R. Anderson"
        ],
        "citations": 1,
        "references": 71,
        "year": 2022
    },
    {
        "title": "Simplified vortex methods to model wake vortex roll-up in real-time simulations for fuel-saving formation flight",
        "abstract": null,
        "authors": [
            "Henrik Spark",
            "R. Luckner"
        ],
        "citations": 1,
        "references": 33,
        "year": 2022
    },
    {
        "title": "DAILY AND SEASONAL VARIATION OF SOIL RESPIRATION IN A SEASONAL SEMIDECIDUAL ATLANTIC FOREST FRAGMENT AND A RESTORATION SITE IN SOUTHERN BRAZIL",
        "abstract": null,
        "authors": [
            "Jvc Souza",
            "G. Souza-Gonzaga",
            "J. Melo-Tambani",
            "MF Hertel",
            "Vlm de Paula",
            "Jmd Torezan"
        ],
        "citations": 1,
        "references": 0,
        "year": 2022
    },
    {
        "title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
        "abstract": "Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.",
        "authors": [
            "Pranab Sahoo",
            "Ayush Kumar Singh",
            "Sriparna Saha",
            "Vinija Jain",
            "S. Mondal",
            "Aman Chadha"
        ],
        "citations": 143,
        "references": 35,
        "year": 2024
    },
    {
        "title": "ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models",
        "abstract": "Large vision-language models (LVLMs) have shown premise in a broad range of vision-language tasks with their strong reasoning and generalization capabilities. However, they require considerable computational resources for training and deployment. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To this end, we propose a comprehensive pipeline for generating a synthetic dataset. The key idea is to leverage strong proprietary models to generate (i) fine-grained image annotations for vision-language alignment and (ii) complex reasoning visual question-answering pairs for visual instruction fine-tuning, yielding 1.3M samples in total. We train a series of lite VLMs on the synthetic dataset and experimental results demonstrate the effectiveness of the proposed scheme, where they achieve competitive performance on 17 benchmarks among 4B LVLMs, and even perform on par with 7B/13B-scale models on various benchmarks. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. We name our dataset \\textit{ALLaVA}, and open-source it to research community for developing better resource-efficient LVLMs for wider usage.",
        "authors": [
            "Guiming Hardy Chen",
            "Shunian Chen",
            "Ruifei Zhang",
            "Junying Chen",
            "Xiangbo Wu",
            "Zhiyi Zhang",
            "Zhihong Chen",
            "Jianquan Li",
            "Xiang Wan",
            "Benyou Wang"
        ],
        "citations": 94,
        "references": 44,
        "year": 2024
    },
    {
        "title": "PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning",
        "abstract": "Vision-language pre-training has significantly elevated performance across a wide range of image-language applications. Yet, the pre-training process for video-related tasks demands exceptionally large computational and data resources, which hinders the progress of video-language models. This paper investigates a straight-forward, highly efficient, and resource-light approach to adapting an existing image-language pre-trained model for dense video understanding. Our preliminary experiments reveal that directly fine-tuning pre-trained image-language models with multiple frames as inputs on video datasets leads to performance saturation or even a drop. Our further investigation reveals that it is largely attributed to the bias of learned high-norm visual features. Motivated by this finding, we propose a simple but effective pooling strategy to smooth the feature distribution along the temporal dimension and thus reduce the dominant impacts from the extreme features. The new model is termed Pooling LLaVA, or PLLaVA in short. PLLaVA achieves new state-of-the-art performance on modern benchmark datasets for both video question-answer and captioning tasks. Notably, on the recent popular VideoChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average of five evaluated dimensions, exceeding the previous SOTA results from GPT4V (IG-VLM) by 9%. On the latest multi-choice benchmark MVBench, PLLaVA achieves 58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V (IG-VLM). Code is available at https://pllava.github.io/",
        "authors": [
            "Lin Xu",
            "Yilin Zhao",
            "Daquan Zhou",
            "Zhijie Lin",
            "See Kiong Ng",
            "Jiashi Feng"
        ],
        "citations": 85,
        "references": 50,
        "year": 2024
    },
    {
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
        "abstract": "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",
        "authors": [
            "Yuanzhan Liu",
            "Haodong Duan",
            "Yuanhan Zhang",
            "Bo Li",
            "Songyang Zhang",
            "Wangbo Zhao",
            "Yike Yuan",
            "Jiaqi Wang",
            "Conghui He",
            "Ziwei Liu",
            "Kai Chen",
            "Dahua Lin"
        ],
        "citations": 614,
        "references": 71,
        "year": 2023
    },
    {
        "title": "DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models",
        "abstract": "A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of reasoning modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and unpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a production vehicle, verifying it is effective in real-world autonomous driving environments.",
        "authors": [
            "Xiaoyu Tian",
            "Junru Gu",
            "Bailin Li",
            "Yicheng Liu",
            "Chenxu Hu",
            "Yang Wang",
            "Kun Zhan",
            "Peng Jia",
            "Xianpeng Lang",
            "Hang Zhao"
        ],
        "citations": 62,
        "references": 78,
        "year": 2024
    },
    {
        "title": "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
        "abstract": "Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at https://os-world.github.io.",
        "authors": [
            "Tianbao Xie",
            "Danyang Zhang",
            "Jixuan Chen",
            "Xiaochuan Li",
            "Siheng Zhao",
            "Ruisheng Cao",
            "Toh Jing Hua",
            "Zhoujun Cheng",
            "Dongchan Shin",
            "Fangyu Lei",
            "Yitao Liu",
            "Yiheng Xu",
            "Shuyan Zhou",
            "Silvio Savarese",
            "Caiming Xiong",
            "Victor Zhong",
            "Tao Yu"
        ],
        "citations": 54,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Demonstrating OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics",
        "abstract": "Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments and code are available on our website: https://ok-robot.github.io",
        "authors": [
            "Peiqi Liu",
            "Yaswanth Orru",
            "Jay Vakil",
            "Chris Paxton",
            "N. Shafiullah",
            "Lerrel Pinto"
        ],
        "citations": 42,
        "references": 70,
        "year": 2024
    },
    {
        "title": "CogVLM2: Visual Language Models for Image and Video Understanding",
        "abstract": "Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in pursuit of enhanced vision-language fusion, efficient higher-resolution architecture, and broader modalities and applications. Here we propose the CogVLM2 family, a new generation of visual language models for image and video understanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image understanding model, CogVLM2 inherits the visual expert architecture with improved training recipes in both pre-training and post-training stages, supporting input resolution up to $1344 \\times 1344$ pixels. As a video understanding model, CogVLM2-Video integrates multi-frame input with timestamps and proposes automated temporal grounding data construction. Notably, CogVLM2 family has achieved state-of-the-art results on benchmarks like MMBench, MM-Vet, TextVQA, MVBench and VCGBench. All models are open-sourced in https://github.com/THUDM/CogVLM2 and https://github.com/THUDM/GLM-4, contributing to the advancement of the field.",
        "authors": [
            "Wenyi Hong",
            "Weihan Wang",
            "Ming Ding",
            "Wenmeng Yu",
            "Qingsong Lv",
            "Yan Wang",
            "Yean Cheng",
            "Shiyu Huang",
            "Junhui Ji",
            "Zhao Xue",
            "Lei Zhao",
            "Zhuoyi Yang",
            "Xiaotao Gu",
            "Xiaohan Zhang",
            "Guanyu Feng",
            "Da Yin",
            "Zihan Wang",
            "Ji Qi",
            "Xixuan Song",
            "Peng Zhang",
            "De-Feng Liu",
            "Bin Xu",
            "Juanzi Li",
            "Yu-Chen Dong",
            "Jie Tang"
        ],
        "citations": 38,
        "references": 76,
        "year": 2024
    },
    {
        "title": "Yell At Your Robot: Improving On-the-Fly from Language Corrections",
        "abstract": "Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.",
        "authors": [
            "Lucy Xiaoyang Shi",
            "Zheyuan Hu",
            "Tony Zhao",
            "Archit Sharma",
            "Karl Pertsch",
            "Jianlan Luo",
            "Sergey Levine",
            "Chelsea Finn"
        ],
        "citations": 35,
        "references": 0,
        "year": 2024
    },
    {
        "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
        "abstract": "Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such\"in-the-wild\"data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.",
        "authors": [
            "Michael Ahn",
            "Debidatta Dwibedi",
            "Chelsea Finn",
            "Montse Gonzalez Arenas",
            "K. Gopalakrishnan",
            "Karol Hausman",
            "Brian Ichter",
            "A. Irpan",
            "Nikhil J. Joshi",
            "Ryan C. Julian",
            "Sean Kirmani",
            "Isabel Leal",
            "T. Lee",
            "Sergey Levine",
            "Yao Lu",
            "Sharath Maddineni",
            "Kanishka Rao",
            "Dorsa Sadigh",
            "Pannag R. Sanketi",
            "P. Sermanet",
            "Q. Vuong",
            "Stefan Welker",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Steve Xu",
            "Zhuo Xu"
        ],
        "citations": 33,
        "references": 39,
        "year": 2024
    },
    {
        "title": "Real-World Robot Applications of Foundation Models: A Review",
        "abstract": "Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.",
        "authors": [
            "Kento Kawaharazuka",
            "T. Matsushima",
            "Andrew Gambardella",
            "Jiaxian Guo",
            "Chris Paxton",
            "Andy Zeng"
        ],
        "citations": 30,
        "references": 237,
        "year": 2024
    },
    {
        "title": "CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models",
        "abstract": "Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object‚Äôs grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: copa-2024.github.io",
        "authors": [
            "Haoxu Huang",
            "Fanqi Lin",
            "Yingdong Hu",
            "Shengjie Wang",
            "Yang Gao"
        ],
        "citations": 26,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
        "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87 times and 8.56 times compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications. Our code is available at https://github.com/KuofengGao/Verbose_Images.",
        "authors": [
            "Kuofeng Gao",
            "Yang Bai",
            "Jindong Gu",
            "Shu-Tao Xia",
            "Philip Torr",
            "Zhifeng Li",
            "Wei Liu"
        ],
        "citations": 26,
        "references": 74,
        "year": 2024
    },
    {
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
        "abstract": "Today‚Äôs most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like",
        "authors": [
            "Matt Deitke",
            "Christopher Clark",
            "Sangho Lee",
            "Rohun Tripathi",
            "Yue Yang",
            "Jae Sung Park",
            "Mohammadreza Salehi",
            "Niklas Muennighoff",
            "Kyle Lo",
            "Luca Soldaini",
            "Jiasen Lu",
            "Taira Anderson",
            "Erin Bransom",
            "Kiana Ehsani",
            "Huong Ngo",
            "YenSung Chen",
            "Ajay Patel",
            "Mark Yatskar",
            "Christopher Callison-Burch",
            "Andrew Head",
            "Rose Hendrix",
            "F. Bastani",
            "Eli VanderBilt",
            "Nathan Lambert",
            "Yvonne Chou",
            "Arnavi Chheda",
            "Jenna Sparks",
            "Sam Skjonsberg",
            "Michael Schmitz",
            "Aaron Sarnat",
            "Byron Bischoff",
            "Pete Walsh",
            "Christopher Newell",
            "Piper Wolters",
            "Tanmay Gupta",
            "Kuo-Hao Zeng",
            "Jon Borchardt",
            "Dirk Groeneveld",
            "Jennifer Dumas",
            "Crystal Nam",
            "Sophie Lebrecht",
            "Caitlin Marie Wittlif",
            "Carissa Schoenick",
            "Oscar Michel",
            "Ranjay Krishna",
            "Luca Weihs",
            "Noah A. Smith",
            "Hanna Hajishirzi",
            "Ross Girshick",
            "Ali Farhadi",
            "Aniruddha Kembhavi"
        ],
        "citations": 27,
        "references": 33,
        "year": 2024
    },
    {
        "title": "RegionGPT: Towards Region Understanding Vision Language Model",
        "abstract": "Vision language models (VLMs) have experienced rapid advancements through the integration of large language models (LLMs) with image-text pairs, yet they struggle with detailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse-grained training data that lacks detailed, region-specific captions. To address this, we introduce RegionGPT (short as RGPT), a novel framework designed for complex region-level captioning and understanding. RGPT enhances the spatial awareness of regional representation with simple yet effective modifications to existing visual encoders in VLMs. We further improve performance on tasks requiring a specific output scope by integrating task-guided instruction prompts during both training and inference phases, while maintaining the model's versatility for general-purpose tasks. Additionally, we develop an automated region caption data generation pipeline, enriching the training set with detailed region-level captions. We demonstrate that a universal RGPT model can be effectively applied and significantly enhancing performance across a range of region-level tasks, including but not limited to complex region descriptions, reasoning, object classification, and referring expressions comprehension. Code will be released at the project page.",
        "authors": [
            "Qiushan Guo",
            "Shalini De Mello",
            "Hongxu Yin",
            "Wonmin Byeon",
            "Ka Chun Cheung",
            "Yizhou Yu",
            "Ping Luo",
            "Sifei Liu"
        ],
        "citations": 25,
        "references": 60,
        "year": 2024
    },
    {
        "title": "3D-LLM: Injecting the 3D World into Large Language Models",
        "abstract": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 1M 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on held-out evaluation dataset, ScanQA, SQA3D and 3DMV-VQA, outperform state-of-the-art baselines. In particular, experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ( e.g. , the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/ .",
        "authors": [
            "Yining Hong",
            "Haoyu Zhen",
            "Peihao Chen",
            "Shuhong Zheng",
            "Yilun Du",
            "Zhenfang Chen",
            "Chuang Gan"
        ],
        "citations": 181,
        "references": 52,
        "year": 2023
    },
    {
        "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning",
        "abstract": "High-quality instructions and responses are essential for the zero-shot performance of large language models on interactive natural language tasks. For interactive vision-language tasks involving intricate visual scenes, a large quantity of diverse and creative instruction-response pairs should be imperative to tune vision-language models (VLMs). Nevertheless, the current availability of vision-language instruction-response pairs in terms of quantity, diversity, and creativity remains limited, posing challenges to the generalization of interactive VLMs. Here we present MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos. Each pair is accompanied by multi-modal in-context information, forming conversational contexts aimed at empowering VLMs in perception, reasoning, and planning. The instruction-response collection process, dubbed as Syphus, is scaled using an automatic annotation pipeline that combines human expertise with GPT's capabilities. Using the MIMIC-IT dataset, we train a large VLM named Otter. Based on extensive evaluations conducted on vision-language benchmarks, it has been observed that Otter demonstrates remarkable proficiency in multi-modal perception, reasoning, and in-context learning. Human evaluation reveals it effectively aligns with the user's intentions. We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model.",
        "authors": [
            "Bo Li",
            "Yuanhan Zhang",
            "Liangyu Chen",
            "Jinghao Wang",
            "Fanyi Pu",
            "Jingkang Yang",
            "C. Li",
            "Ziwei Liu"
        ],
        "citations": 190,
        "references": 55,
        "year": 2023
    },
    {
        "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
        "abstract": "Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack. LongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g. 65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers.",
        "authors": [
            "Fuzhao Xue",
            "Yukang Chen",
            "Dacheng Li",
            "Qinghao Hu",
            "Ligeng Zhu",
            "Xiuyu Li",
            "Yunhao Fang",
            "Haotian Tang",
            "Shang Yang",
            "Zhijian Liu",
            "Ethan He",
            "Hongxu Yin",
            "Pavlo Molchanov",
            "Jan Kautz",
            "Linxi Fan",
            "Yuke Zhu",
            "Yao Lu",
            "Song Han"
        ],
        "citations": 22,
        "references": 75,
        "year": 2024
    },
    {
        "title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
        "abstract": "Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs' spatial perception and reasoning capabilities. SpatialRGPT advances VLMs' spatial understanding through two key innovations: (1) a data curation pipeline that enables effective learning of regional representation from 3D scene graphs, and (2) a flexible plugin module for integrating depth information into the visual encoder of existing VLMs. During inference, when provided with user-specified region proposals, SpatialRGPT can accurately perceive their relative directions and distances. Additionally, we propose SpatialRGBT-Bench, a benchmark with ground-truth 3D annotations encompassing indoor, outdoor, and simulated environments, for evaluating 3D spatial cognition in VLMs. Our results demonstrate that SpatialRGPT significantly enhances performance in spatial reasoning tasks, both with and without local region prompts. The model also exhibits strong generalization capabilities, effectively reasoning about complex spatial relations and functioning as a region-aware dense reward annotator for robotic tasks. Code, dataset, and benchmark are released at https://www.anjiecheng.me/SpatialRGPT",
        "authors": [
            "An-Chieh Cheng",
            "Hongxu Yin",
            "Yang Fu",
            "Qiushan Guo",
            "Ruihan Yang",
            "Jan Kautz",
            "Xiaolong Wang",
            "Sifei Liu"
        ],
        "citations": 22,
        "references": 96,
        "year": 2024
    },
    {
        "title": "Toward Generalist Anomaly Detection via In-Context Residual Learning with Few-Shot Sample Prompts",
        "abstract": "This paper explores the problem of Generalist Anomaly Detection (GAD), aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data. Some recent studies have showed that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text prompts about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images. In this work, we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly. To this end, we introduce a novel approach that learns an in-context residual learning model for GAD, termed InCTRL. It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts. Regardless of the datasets, per definition of anomaly, larger residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training. Comprehensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulate the detection of industrial defect anomalies, medical anomalies, and semantic anomalies in both one-vs-all and multi-class setting, on which InCTRL is the best performer and significantly outperforms state-of-the-art competing methods. Code is available at https://github.com/mala-lab/InCTRL.",
        "authors": [
            "Jiawen Zhu",
            "Guansong Pang"
        ],
        "citations": 20,
        "references": 76,
        "year": 2024
    },
    {
        "title": "Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training",
        "abstract": "Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a\"visual prompt\", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior). CRG achieves substantial improvements in a wide variety of VL tasks: When region annotations are provided, CRG increases absolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse region-based tasks such as recognition, math, and object relationship reasoning. We also show CRG's applicability to spatial reasoning, with 10% improvement on What'sUp, as well as to compositional generalization -- improving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe -- and to image-text alignment for generated images, where we improve by up to 8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG allows us to re-rank proposed regions in referring expression comprehension and phrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an average gain of 3.2% in accuracy. Our analysis explores alternative masking strategies for CRG, quantifies CRG's probability shift, and evaluates the role of region guidance strength, empirically validating CRG's design choices.",
        "authors": [
            "David Wan",
            "Jaemin Cho",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "citations": 20,
        "references": 59,
        "year": 2024
    },
    {
        "title": "The Neglected Tails in Vision-Language Models",
        "abstract": "Vision-language models (VLMs) excel in zero-shot recognition but their performance varies greatly across different visual concepts. For example, although CLIP achieves impressive accuracy on ImageNet (60-80%), its performance drops below 10% for more than ten concepts like night snake, presumably due to their limited presence in the pretraining data. However, measuring the frequency of concepts in VLMs' large-scale datasets is challenging. We address this by using large language models (LLMs) to count the number of pretraining texts that con-tain synonyms of these concepts. Our analysis confirms that popular datasets, such as LAION, exhibit a long-tailed concept distribution, yielding biased performance in VLMs. We also find that downstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and text-to-image models (e.g., Stable Diffusion), often fail to recognize or generate images of rare concepts identified by our method. To mit-igate the imbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented Learning (REAL). First, in-stead of prompting VLMs using the original class names, REAL uses their most frequent synonyms found in pretraining texts. This simple change already outperforms costly human-engineered and LLM-enriched prompts over nine benchmark datasets. Second, REAL trains a linear classifier on a small yet balanced set of pretraining data re-trieved using concept synonyms. REAL surpasses the previous zero-shot SOTA, using 400√ó less storage and 10,000√ó less training time!",
        "authors": [
            "Shubham Parashar",
            "Zhiqiu Lin",
            "Tian Liu",
            "Xiangjue Dong",
            "Yanan Li",
            "Deva Ramanan",
            "James Caverlee",
            "Shu Kong"
        ],
        "citations": 20,
        "references": 50,
        "year": 2024
    },
    {
        "title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation",
        "abstract": "VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.",
        "authors": [
            "Yecheng Wu",
            "Zhuoyang Zhang",
            "Junyu Chen",
            "Haotian Tang",
            "Dacheng Li",
            "Yunhao Fang",
            "Ligeng Zhu",
            "Enze Xie",
            "Hongxu Yin",
            "Li Yi",
            "Song Han",
            "Yao Lu"
        ],
        "citations": 20,
        "references": 67,
        "year": 2024
    },
    {
        "title": "Scaling Laws for Data Filtering‚ÄîData Curation Cannot be Compute Agnostic",
        "abstract": "Vision-language models (VLMs) are trained for thousands of GPU hours on carefully selected subsets of massive web scrapes. For instance, the LAION public dataset retained only about 10% of the total crawled data. In recent times, data curation has gained prominence with several works developing strategies to retain ‚Äòhigh-quality‚Äô subsets of ‚Äòraw‚Äô scraped data. However, these strategies are typically developed agnostic to the available compute for training. In this paper, we demonstrate that making filtering decisions independent of training compute is often suboptimal‚Äîwell-curated data rapidly loses its utility when repeated, eventually decreasing below the utility of ‚Äòunseen‚Äô but ‚Äòlower-quality‚Äô data. While past research in neural scaling laws has considered web data to be homogenous, real data is not. Our work bridges this important gap in the literature by developing scaling laws that characterize the differing ‚Äòutility‚Äô of various data subsets, and accounting for how this diminishes for a data point at its ‚Äònth‚Äô repetition. Our key message is that data curation can not be agnostic of the total compute a model will be trained for. Even without ever jointly training on multiple data buckets, our scaling laws enable us to estimate model performance under this dynamic trade-off between quality and repetition. This allows us to curate the best possible pool for achieving top performance on Datacomp at various compute budgets, carving out a pareto-frontier for data curation.",
        "authors": [
            "Sachin Goyal",
            "Pratyush Maini",
            "Zachary Chase Lipton",
            "Aditi Raghunathan",
            "J. Kolter"
        ],
        "citations": 22,
        "references": 52,
        "year": 2024
    },
    {
        "title": "VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models",
        "abstract": "Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.",
        "authors": [
            "Jiawei Liang",
            "Siyuan Liang",
            "Man Luo",
            "Aishan Liu",
            "Dongchen Han",
            "Ee-Chien Chang",
            "Xiaochun Cao"
        ],
        "citations": 22,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Enhancing Video-Language Representations With Structural Spatio-Temporal Alignment",
        "abstract": "While pre-training large-scale video-language models (VLMs) has shown remarkable potential for various downstream video-language tasks, existing VLMs can still suffer from certain commonly seen limitations, e.g., coarse-grained cross-modal aligning, under-modeling of temporal dynamics, detached video-language view. In this work, we target enhancing VLMs with a fine-grained structural spatio-temporal alignment learning method (namely Finsta). First of all, we represent the input texts and videos with fine-grained scene graph (SG) structures, both of which are further unified into a holistic SG (HSG) for bridging two modalities. Then, an SG-based framework is built, where the textual SG (TSG) is encoded with a graph Transformer, while the video dynamic SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for spatial and temporal feature propagation. A spatial-temporal Gaussian differential graph Transformer is further devised to strengthen the sense of the changes in objects across spatial and temporal dimensions. Next, based on the fine-grained structural features of TSG and DSG, we perform object-centered spatial alignment and predicate-centered temporal alignment respectively, enhancing the video-language grounding in both the spatiality and temporality. We design our method as a plug&play system, which can be integrated into existing well-trained VLMs for further representation augmentation, without training from scratch or relying on SG annotations in downstream applications. On 6 representative VL modeling tasks over 12 datasets in both standard and long-form video scenarios, Finsta consistently improves the existing 13 strong-performing VLMs persistently, and refreshes the current state-of-the-art end task performance significantly in both the fine-tuning and zero-shot settings.",
        "authors": [
            "Hao Fei",
            "Shengqiong Wu",
            "Meishan Zhang",
            "Min Zhang",
            "Tat-Seng Chua",
            "Shuicheng Yan"
        ],
        "citations": 20,
        "references": 96,
        "year": 2024
    },
    {
        "title": "Red Teaming Visual Language Models",
        "abstract": "VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.",
        "authors": [
            "Mukai Li",
            "Lei Li",
            "Yuwei Yin",
            "Masood Ahmed",
            "Zhenguang Liu",
            "Qi Liu"
        ],
        "citations": 20,
        "references": 48,
        "year": 2024
    },
    {
        "title": "Embodied Understanding of Driving Scenarios",
        "abstract": "Embodied scene understanding serves as the cornerstone for autonomous agents to perceive, interpret, and respond to open driving scenarios. Such understanding is typically founded upon Vision-Language Models (VLMs). Nevertheless, existing VLMs are restricted to the 2D domain, devoid of spatial awareness and long-horizon extrapolation proficiencies. We revisit the key aspects of autonomous driving and formulate appropriate rubrics. Hereby, we introduce the Embodied Language Model (ELM), a comprehensive framework tailored for agents' understanding of driving scenes with large spatial and temporal spans. ELM incorporates space-aware pre-training to endow the agent with robust spatial localization capabilities. Besides, the model employs time-aware token selection to accurately inquire about temporal cues. We instantiate ELM on the reformulated multi-faced benchmark, and it surpasses previous state-of-the-art approaches in all aspects. All code, data, and models will be publicly shared.",
        "authors": [
            "Yunsong Zhou",
            "Linyan Huang",
            "Qingwen Bu",
            "Jia Zeng",
            "Tianyu Li",
            "Hang Qiu",
            "Hongzi Zhu",
            "Minyi Guo",
            "Yu Qiao",
            "Hongyang Li"
        ],
        "citations": 19,
        "references": 98,
        "year": 2024
    },
    {
        "title": "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models",
        "abstract": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks. Our source code is available at \\url{https://github.com/Haochen-Luo/CroPA}.",
        "authors": [
            "Haochen Luo",
            "Jindong Gu",
            "Fengyuan Liu",
            "Philip Torr"
        ],
        "citations": 16,
        "references": 46,
        "year": 2024
    },
    {
        "title": "JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models",
        "abstract": "The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. More details can be found on our website: \\url{https://chonghan-chen.com/llm-jailbreak-zoo-survey/}.",
        "authors": [
            "Haibo Jin",
            "Leyang Hu",
            "Xinuo Li",
            "Peiyan Zhang",
            "Chonghan Chen",
            "Jun Zhuang",
            "Haohan Wang"
        ],
        "citations": 15,
        "references": 160,
        "year": 2024
    },
    {
        "title": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning",
        "abstract": "Video summarization aims to create short, accurate, and cohesive summaries of longer videos. Despite the existence of various video summarization datasets, a notable limitation is their limited amount of source videos, which hampers the effective training of advanced large vision-language models (VLMs). Additionally, most existing datasets are created for video-to-video summarization, overlooking the contemporary need for multimodal video content summarization. Recent efforts have been made to expand from unimodal to multimodal video summarization, categorizing the task into three sub-tasks based on the summary's modality: video-to-video (V2V), video-to-text (V2T), and a combination of video and text summarization (V2VT). However, the textual summaries in previous multimodal datasets are inadequate. To address these issues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset featuring 30,000 diverse videos sourced from YouTube, with lengths ranging from 40 to 940 seconds and an average summarization ratio of 16.39%. Each video summary in Instruct-V2Xum is paired with a textual summary that references specific frame indexes, facilitating the generation of aligned video and textual summaries. In addition, we propose a new video summarization framework named V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the first framework that unifies different video summarization tasks into one large language model's (LLM) text decoder and achieves task-controllable video summarization with temporal prompts and task instructions. Experiments show that V2Xum-LLaMA outperforms strong baseline models on multiple video summarization tasks. Furthermore, we propose an enhanced evaluation metric for V2V and V2VT summarization tasks.",
        "authors": [
            "Hang Hua",
            "Yunlong Tang",
            "Chenliang Xu",
            "Jiebo Luo"
        ],
        "citations": 15,
        "references": 95,
        "year": 2024
    },
    {
        "title": "Vision language models are blind",
        "abstract": "While large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini 1.5 Pro, are powering various image-text applications and scoring high on many vision-understanding benchmarks, we find that they are surprisingly still struggling with low-level vision tasks that are easy to humans. Specifically, on BlindTest, our suite of 7 very simple tasks such as identifying (a) whether two circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d) counting circles in an Olympic-like logo, four state-of-the-art VLMs are only 58.57% accurate on average. Claude 3.5 Sonnet performs the best at 74.94% accuracy, but this is still far from the human expected accuracy of 100%. Across different image resolutions and line widths, VLMs consistently struggle with tasks that require precise spatial information and recognizing geometric primitives that overlap or are close together. Code and data are available at: https://vlmsareblind.github.io",
        "authors": [
            "Pooyan Rahmanzadehgervi",
            "Logan Bolton",
            "Mohammad Reza Taesiri",
            "A. Nguyen"
        ],
        "citations": 17,
        "references": 0,
        "year": 2024
    },
    {
        "title": "MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?",
        "abstract": "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly proliferating, they often encounter challenges such as hallucination, bias, and the production of unsafe, low-quality output. To effectively address these issues, it is crucial to align these models with desired behaviors based on feedback from a multimodal judge. Despite their significance, current multimodal judges frequently undergo inadequate evaluation of their capabilities and limitations, potentially leading to misalignment and unsafe fine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, we evaluate a large variety of multimodal judges including smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in average. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. Notably, human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. All data, code, models are available at https://huggingface.co/MJ-Bench.",
        "authors": [
            "Zhaorun Chen",
            "Yichao Du",
            "Zichen Wen",
            "Yiyang Zhou",
            "Chenhang Cui",
            "Zhenzhen Weng",
            "Haoqin Tu",
            "Chaoqi Wang",
            "Zhengwei Tong",
            "Qinglan Huang",
            "Canyu Chen",
            "Qinghao Ye",
            "Zhihong Zhu",
            "Yuqing Zhang",
            "Jiawei Zhou",
            "Zhuokai Zhao",
            "Rafael Rafailov",
            "Chelsea Finn",
            "Huaxiu Yao"
        ],
        "citations": 15,
        "references": 95,
        "year": 2024
    },
    {
        "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner",
        "abstract": "Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20\\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app under https://github.com/snap-stanford/med-flamingo.",
        "authors": [
            "Michael Moor",
            "Qian Huang",
            "Shirley Wu",
            "Michihiro Yasunaga",
            "Cyril Zakka",
            "Yashodhara Dalmia",
            "E. Reis",
            "P. Rajpurkar",
            "J. Leskovec"
        ],
        "citations": 162,
        "references": 34,
        "year": 2023
    },
    {
        "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
        "abstract": "In this work, we present a novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID. Current VLMs, while proficient in tasks like image captioning and visual question answering, face computational burdens when processing long videos due to the excessive visual tokens. LLaMA-VID addresses this issue by representing each frame with two distinct tokens, namely context token and content token. The context token encodes the overall image context based on user input, whereas the content token encapsulates visual cues in each frame. This dual-token strategy significantly reduces the overload of long videos while preserving critical information. Generally, LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. It is proved to surpass previous methods on most of video- or image-based benchmarks. Code is available https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID",
        "authors": [
            "Yanwei Li",
            "Chengyao Wang",
            "Jiaya Jia"
        ],
        "citations": 140,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review",
        "abstract": "This comprehensive review delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). The development of Artificial Intelligence (AI), from its inception in the 1950s to the emergence of advanced neural networks and deep learning architectures, has made a breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in Vision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt engineering is the process of structuring inputs, which has emerged as a crucial technique to maximize the utility and accuracy of these models. This paper explores both foundational and advanced methodologies of prompt engineering, including techniques such as self-consistency, chain-of-thought, and generated knowledge, which significantly enhance model performance. Additionally, it examines the prompt method of VLMs through innovative approaches such as Context Optimization (CoOp), Conditional Context Optimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this discussion is the aspect of AI security, particularly adversarial attacks that exploit vulnerabilities in prompt engineering. Strategies to mitigate these risks and enhance model robustness are thoroughly reviewed. The evaluation of prompt methods is also addressed, through both subjective and objective metrics, ensuring a robust analysis of their efficacy. This review also reflects the essential role of prompt engineering in advancing AI capabilities, providing a structured framework for future research and application.",
        "authors": [
            "Banghao Chen",
            "Zhaofeng Zhang",
            "Nicolas Langren'e",
            "Shengxin Zhu"
        ],
        "citations": 136,
        "references": 129,
        "year": 2023
    },
    {
        "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents",
        "abstract": "Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse.",
        "authors": [
            "Wentong Chen",
            "Junbo Cui",
            "Jinyi Hu",
            "Yujia Qin",
            "Junjie Fang",
            "Yue Zhao",
            "Chongyi Wang",
            "Jun Liu",
            "Gui-Fang Chen",
            "Yupeng Huo",
            "Yuan Yao",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "citations": 14,
        "references": 56,
        "year": 2024
    },
    {
        "title": "ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference",
        "abstract": "Despite the success of large-scale pretrained Vision-Language Models (VLMs) especially CLIP in various open-vocabulary tasks, their application to semantic segmentation remains challenging, producing noisy segmentation maps with mis-segmented regions. In this paper, we carefully re-investigate the architecture of CLIP, and identify residual connections as the primary source of noise that degrades segmentation quality. With a comparative analysis of statistical properties in the residual connection and the attention output across different pretrained models, we discover that CLIP's image-text contrastive training paradigm emphasizes global features at the expense of local discriminability, leading to noisy segmentation results. In response, we propose ClearCLIP, a novel approach that decomposes CLIP's representations to enhance open-vocabulary semantic segmentation. We introduce three simple modifications to the final layer: removing the residual connection, implementing the self-self attention, and discarding the feed-forward network. ClearCLIP consistently generates clearer and more accurate segmentation maps and outperforms existing approaches across multiple benchmarks, affirming the significance of our discoveries.",
        "authors": [
            "Mengcheng Lan",
            "Chaofeng Chen",
            "Yiping Ke",
            "Xinjiang Wang",
            "Litong Feng",
            "Wayne Zhang"
        ],
        "citations": 12,
        "references": 56,
        "year": 2024
    },
    {
        "title": "CoLLaVO: Crayon Large Language and Vision mOdel",
        "abstract": "The remarkable success of Large Language Models (LLMs) and instruction tuning drives the evolution of Vision Language Models (VLMs) towards a versatile general-purpose model. Yet, it remains unexplored whether current VLMs genuinely possess quality object-level image understanding capabilities determined from 'what objects are in the image?' or 'which object corresponds to a specified bounding box?'. Our findings reveal that the image understanding capabilities of current VLMs are strongly correlated with their zero-shot performance on vision language (VL) tasks. This suggests that prioritizing basic image understanding is crucial for VLMs to excel at VL tasks. To enhance object-level image understanding, we propose Crayon Large Language and Vision mOdel (CoLLaVO), which incorporates instruction tuning with Crayon Prompt as a new visual prompt tuning scheme based on panoptic color maps. Furthermore, we present a learning strategy of Dual QLoRA to preserve object-level image understanding without forgetting it during visual instruction tuning, thereby achieving a significant leap in numerous VL benchmarks in a zero-shot setting.",
        "authors": [
            "Byung-Kwan Lee",
            "Beomchan Park",
            "Chae Won Kim",
            "Yonghyun Ro"
        ],
        "citations": 14,
        "references": 84,
        "year": 2024
    },
    {
        "title": "CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-Spoofing",
        "abstract": "Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model's performance on unseen domains. Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization. In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier's weights for exploring generalizable visual features. Specifically, we propose a novel Class Free Prompt Learning (CFPL) paradigm for DG FAS, which utilizes two lightweight transformers, namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic prompts conditioned on content and style features by using a set of learnable query vectors, respectively. Thus, the generalizable prompt can be learned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description. (2) A Diversified Style Prompt (DSP) technology is proposed to diversify the learning of style prompts by mixing feature statistics between instance-specific styles. Finally, the learned text features modulate visual features to generalization through the designed Prompt Modulation (PM). Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets.",
        "authors": [
            "Ajian Liu",
            "Shuai Xue",
            "Jianwen Gan",
            "Jun Wan",
            "Yanyan Liang",
            "Jiankang Deng",
            "Sergio Escalera",
            "Zhen Lei"
        ],
        "citations": 14,
        "references": 61,
        "year": 2024
    },
    {
        "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
        "abstract": "There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.",
        "authors": [
            "Xijia Tao",
            "Shuai Zhong",
            "Lei Li",
            "Qi Liu",
            "Lingpeng Kong"
        ],
        "citations": 14,
        "references": 21,
        "year": 2024
    },
    {
        "title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
        "abstract": "Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning -- a fundamental component of human cognition -- remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.",
        "authors": [
            "Jiayu Wang",
            "Yifei Ming",
            "Zhenmei Shi",
            "Vibhav Vineet",
            "Xin Wang",
            "Neel Joshi"
        ],
        "citations": 14,
        "references": 0,
        "year": 2024
    },
    {
        "title": "On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities",
        "abstract": "In this paper, we highlight the critical issues of robustness 001 and safety associated with integrating large language models 002 (LLMs) and vision-language models (VLMs) into robotics 003 applications. Recent works have focused on using LLMs and 004 VLMs to improve the performance of robotics tasks, such 005 as manipulation, navigation, etc. However, such integration 006 can introduce significant vulnerabilities, in terms of their 007 susceptibility to adversarial attacks due to the language mod-008 els, potentially leading to catastrophic consequences. By 009 examining recent works at the interface of LLMs/VLMs and 010 robotics, we show that it is easy to manipulate or misguide the 011 robot‚Äôs actions, leading to safety hazards. We define and pro-012 vide examples of several plausible adversarial attacks, and 013 conduct experiments on three prominent robot frameworks 014 integrated with a language model, including KnowNo [40], 015 VIMA [21], and Instruct2Act [20], to assess their susceptibil-016 ity to these attacks. Our empirical findings reveal a striking 017 vulnerability of LLM/VLM-robot integrated systems: simple 018 adversarial attacks can significantly undermine the effective-019 ness of LLM/VLM-robot integrated systems. Specifically, our 020 data demonstrate an average performance deterioration of 021 21.2% under prompt attacks and a more alarming 30.2% un-022 der perception attacks. These results underscore the critical 023 need for robust countermeasures to ensure the safe and reli-024 able deployment of the advanced LLM/VLM-based robotic 025 systems. 026",
        "authors": [
            "Xiyang Wu",
            "Ruiqi Xian",
            "Tianrui Guan",
            "Jing Liang",
            "Souradip Chakraborty",
            "Fuxiao Liu",
            "Brian M. Sadler",
            "Dinesh Manocha",
            "A. S. Bedi"
        ],
        "citations": 13,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Empowering Unsupervised Domain Adaptation with Large-scale Pre-trained Vision-Language Models",
        "abstract": "Unsupervised Domain Adaptation (UDA) aims to leverage the labeled source domain to solve the tasks on the unlabeled target domain. Traditional UDA methods face the challenge of the tradeoff between domain alignment and semantic class discriminability, especially when a large domain gap exists between the source and target domains. The efforts of applying large-scale pre-training to bridge the domain gaps remain limited. In this work, we propose that Vision-Language Models (VLMs) can empower UDA tasks due to their training pattern with language alignment and their large-scale pre-trained datasets. For example, CLIP and GLIP have shown promising zero-shot generalization in classification and detection tasks. However, directly fine-tuning these VLMs into downstream tasks may be computationally expensive and not scalable if we have multiple domains that need to be adapted. Therefore, in this work, we first study an efficient adaption of VLMs to preserve the original knowledge while maximizing its flexibility for learning new knowledge. Then, we design a domain-aware pseudo-labeling scheme tailored to VLMs for domain disentanglement. We show the superiority of the proposed methods in four UDA-classification and two UDA-detection benchmarks, with a significant improvement (+9.9%) on DomainNet.",
        "authors": [
            "Zhengfeng Lai",
            "Haoping Bai",
            "Haotian Zhang",
            "Xianzhi Du",
            "Jiulong Shan",
            "Yinfei Yang",
            "Chen-Nee Chuah",
            "Meng Cao"
        ],
        "citations": 12,
        "references": 80,
        "year": 2024
    },
    {
        "title": "Large Language Model for Table Processing: A Survey",
        "abstract": null,
        "authors": [
            "Weizheng Lu",
            "Jiaming Zhang",
            "Jing Zhang",
            "Yueguo Chen"
        ],
        "citations": 13,
        "references": 130,
        "year": 2024
    },
    {
        "title": "VLM2Scene: Self-Supervised Image-Text-LiDAR Learning with Foundation Models for Autonomous Driving Scene Understanding",
        "abstract": "Vision and language foundation models (VLMs) have showcased impressive capabilities in 2D scene understanding. However, their latent potential in elevating the understanding of 3D autonomous driving scenes remains untapped. In this paper, we propose VLM2Scene, which exploits the potential of VLMs to enhance 3D self-supervised representation learning through our proposed image-text-LiDAR contrastive learning strategy. Specifically, in the realm of autonomous driving scenes, the inherent sparsity of LiDAR point clouds poses a notable challenge for point-level contrastive learning methods. This method often grapples with limitations tied to a restricted receptive field and the presence of noisy points. To tackle this challenge, our approach emphasizes region-level learning, leveraging regional masks without semantics derived from the vision foundation model. This approach capitalizes on valuable contextual information to enhance the learning of point cloud representations. First, we introduce Region Caption Prompts to generate fine-grained language descriptions for the corresponding regions, utilizing the language foundation model. These region prompts then facilitate the establishment of positive and negative text-point pairs within the contrastive loss framework. Second, we propose a Region Semantic Concordance Regularization, which involves a semantic-filtered region learning and a region semantic assignment strategy. The former aims to filter the false negative samples based on the semantic distance, and the latter mitigates potential inaccuracies in pixel semantics, thereby enhancing overall semantic consistency. Extensive experiments on representative autonomous driving datasets demonstrate that our self-supervised method significantly outperforms other counterparts. Codes are available at https://github.com/gbliao/VLM2Scene.",
        "authors": [
            "Guibiao Liao",
            "Jiankun Li",
            "Xiaoqing Ye"
        ],
        "citations": 12,
        "references": 34,
        "year": 2024
    },
    {
        "title": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models",
        "abstract": "Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images. Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels. After pretraining, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder. In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts. The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain. Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student. Extensive experiments on 11 datasets demonstrate the effectiveness of our method. Code is publicly available at https://github.com/zhengli97/PromptKD.",
        "authors": [
            "Zheng Li",
            "Xiang Li",
            "Xinyi Fu",
            "Xing Zhang",
            "Weiqiang Wang",
            "Shuo Chen",
            "Jian Yang"
        ],
        "citations": 13,
        "references": 73,
        "year": 2024
    },
    {
        "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
        "abstract": "Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.",
        "authors": [
            "Yuxuan Kuang",
            "Hai Lin",
            "Meng Jiang"
        ],
        "citations": 12,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases",
        "abstract": "We present a system for generating indoor scenes in response to text prompts. The prompts are not limited to a fixed vocabulary of scene descriptions, and the objects in generated scenes are not restricted to a fixed set of object categories -- we call this setting indoor scene generation. Unlike most prior work on indoor scene generation, our system does not require a large training dataset of existing 3D scenes. Instead, it leverages the world knowledge encoded in pre-trained large language models (LLMs) to synthesize programs in a domain-specific layout language that describe objects and spatial relations between them. Executing such a program produces a specification of a constraint satisfaction problem, which the system solves using a gradient-based optimization scheme to produce object positions and orientations. To produce object geometry, the system retrieves 3D meshes from a database. Unlike prior work which uses databases of category-annotated, mutually-aligned meshes, we develop a pipeline using vision-language models (VLMs) to retrieve meshes from massive databases of un-annotated, inconsistently-aligned meshes. Experimental evaluations show that our system outperforms generative models trained on 3D data for traditional, closed-universe scene generation tasks; it also outperforms a recent LLM-based layout generation method on open-universe scene generation.",
        "authors": [
            "Rio Aguina-Kang",
            "Maxim Gumin",
            "Do Heon Han",
            "Stewart Morris",
            "Seung Jean Yoo",
            "Aditya Ganeshan",
            "R. K. Jones",
            "Qiuhong Anna Wei",
            "Kailiang Fu",
            "Daniel Ritchie"
        ],
        "citations": 12,
        "references": 78,
        "year": 2024
    },
    {
        "title": "On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study",
        "abstract": "Recently, large language models (LLMs) have taken the spotlight in natural language processing. Further, integrating LLMs with vision enables the users to explore emergent abilities with multimodal data. Visual language models (VLMs), such as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on various visio-linguistic tasks. Consequently, there are enormous applications of large models that could be potentially used in the biomedical imaging field. Along that direction, there is a lack of related work to show the ability of large models to diagnose the diseases. In this work, we study the zero-shot and few-shot robustness of VLMs on the medical imaging analysis tasks. Our comprehensive experiments demonstrate the effectiveness of VLMs in analyzing biomedical images such as brain MRIs, microscopic images of blood cells, and chest X- rays. While VLMs can not outperform classic vision models like CNN or ResNet, it is worth noting that VLMs can serve as chat assistants to provide pre-diagnosis before making decisions without the need for retraining or finetuning stages.",
        "authors": [
            "Minh-Hao Van",
            "Prateek Verma",
            "Xintao Wu"
        ],
        "citations": 12,
        "references": 26,
        "year": 2024
    },
    {
        "title": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection",
        "abstract": "Task-oriented object detection aims to find objects suitable for accomplishing specific tasks. As a challenging task, it requires simultaneous visual data processing and reasoning under ambiguous semantics. Recent solutions are mainly all-in-one models. However, the object detection backbones are pre-trained without text supervision. Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance, laborious training, and poor generalizability. In contrast, we propose TaskCLIP, a more natural two-stage design composed of general object detection and task-guided object selection. Particularly for the latter, we resort to the recently successful large Vision-Language Models (VLMs) as our backbone, which provides rich semantic knowledge and a uniform embedding space for images and texts. Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases. To this end, we design a transformer-based aligner after the pre-trained VLMs to re-calibrate both embeddings. Finally, we employ a trainable score function to post-process the VLM matching results for object selection. Experimental results demonstrate that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by 3.5% and only requires a single NVIDIA RTX 4090 for both training and inference.",
        "authors": [
            "Hanning Chen",
            "Wenjun Huang",
            "Yang Ni",
            "Sanggeon Yun",
            "Fei Wen",
            "Hugo Latapie",
            "Mohsen Imani"
        ],
        "citations": 13,
        "references": 51,
        "year": 2024
    },
    {
        "title": "Towards Multimodal In-Context Learning for Vision & Language Models",
        "abstract": "State-of-the-art Vision-Language Models (VLMs) ground the vision and the language modality primarily via projecting the vision tokens from the encoder to language-like tokens, which are directly fed to the Large Language Model (LLM) decoder. While these models have shown unprecedented performance in many downstream zero-shot tasks (eg image captioning, question answers, etc), still little emphasis has been put on transferring one of the core LLM capability of In-Context Learning (ICL). ICL is the ability of a model to reason about a downstream task with a few examples demonstrations embedded in the prompt. In this work, through extensive evaluations, we find that the state-of-the-art VLMs somewhat lack the ability to follow ICL instructions. In particular, we discover that even models that underwent large-scale mixed modality pre-training and were implicitly guided to make use of interleaved image and text information (intended to consume helpful context from multiple images) under-perform when prompted with few-shot demonstrations (in an ICL way), likely due to their lack of direct ICL instruction tuning. To enhance the ICL abilities of the present VLM, we propose a simple yet surprisingly effective multi-turn curriculum-based learning methodology with effective data mixes, leading up to a significant 21.03% (and 11.3% on average) ICL performance boost over the strongest VLM baselines and a variety of ICL benchmarks. Furthermore, we also contribute new benchmarks for ICL evaluation in VLMs and discuss their advantages over the prior art.",
        "authors": [
            "Sivan Doveh",
            "Shaked Perek",
            "M. J. Mirza",
            "Amit Alfassy",
            "Assaf Arbelle",
            "S. Ullman",
            "Leonid Karlinsky"
        ],
        "citations": 12,
        "references": 65,
        "year": 2024
    },
    {
        "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-Trained Vision-Language Models",
        "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness ($\\epsilon=4/255$) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
        "authors": [
            "Lin Li",
            "Haoyan Guan",
            "Jianing Qiu",
            "Michael W. Spratling"
        ],
        "citations": 10,
        "references": 76,
        "year": 2024
    },
    {
        "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model",
        "abstract": "The emergence of Vision Language Models (VLMs) has brought unprecedented advances in understanding multimodal information. The combination of textual and visual semantics in VLMs is highly complex and diverse, making the safety alignment of these models challenging. Furthermore, due to the limited study on the safety alignment of VLMs, there is a lack of large-scale, high-quality datasets. To address these limitations, we propose a Safety Preference Alignment dataset for Vision Language Models named SPA-VL. In terms of breadth, SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and contains 100,788 samples of the quadruple (question, image, chosen response, rejected response). In terms of depth, the responses are collected from 12 open- (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure diversity. The experimental results indicate that models trained with alignment techniques on the SPA-VL dataset exhibit substantial improvements in harmlessness and helpfulness while maintaining core capabilities. SPA-VL, as a large-scale, high-quality, and diverse dataset, represents a significant milestone in ensuring that VLMs achieve both harmlessness and helpfulness. We have made our code https://github.com/EchoseChen/SPA-VL-RLHF and SPA-VL dataset url https://huggingface.co/datasets/sqrti/SPA-VL publicly available.",
        "authors": [
            "Yongting Zhang",
            "Luyao Chen",
            "Guodong Zheng",
            "Yifeng Gao",
            "Rui Zheng",
            "Jinlan Fu",
            "Zhen-fei Yin",
            "Senjie Jin",
            "Yu Qiao",
            "Xuanjing Huang",
            "Feng Zhao",
            "Tao Gui",
            "Jing Shao"
        ],
        "citations": 11,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Low-Rank Few-Shot Adaptation of Vision-Language Models",
        "abstract": "Recent progress in the few-shot adaptation of VisionLanguage Models (VLMs) has further pushed their generalization capabilities, at the expense of just a few labeled samples within the target downstream task. However, this promising, already quite abundant few-shot literature has focused principally on prompt learning and, to a lesser extent, on adapters, overlooking the recent advances in Parameter-Efficient Fine-Tuning (PEFT). Furthermore, existing few-shot learning methods for VLMs often rely on heavy training procedures and/or carefully chosen, taskspecific hyper-parameters, which might impede their applicability. In response, we introduce Low-Rank Adaptation (LoRA) in few-shot learning for VLMs, and show its potential on 11 datasets, in comparison to current state-of-the-art prompt- and adapter-based approaches. Surprisingly, our simple CLIP-LoRA method exhibits substantial improvements, while reducing the training times and keeping the same hyper-parameters in all the target tasks, i.e., across all the datasets and numbers of shots. Certainly, our surprising results do not dismiss the potential of promptlearning and adapter-based research. However, we believe that our strong baseline could be used to evaluate progress in these emergent subjects in few-shot VLMs.",
        "authors": [
            "Maxime Zanella",
            "Ismail Ben Ayed"
        ],
        "citations": 11,
        "references": 65,
        "year": 2024
    },
    {
        "title": "Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions",
        "abstract": "The zero-shot performance of existing vision-language models (VLMs) such as CLIP [29] is limited by the availability of large-scale, aligned image and text datasets in specific domains. In this work, we leverage two complementary sources of information-descriptions of categories generated by large language models (LLMs) and abundant, fine-grained image classification datasets-to improve the zero-shot classification performance of VLMs across fine-grained domains. On the technical side, we develop methods to train VLMs with this ‚Äúbag-level‚Äù image-text super-vision. We find that simply using these attributes at test-time does not improve performance, but our training strategy, for example, on the iNaturalist [41] dataset, leads to an average improvement of 4-5% in zero-shot classification accuracy for novel categories of birds [42] and flow-ers [23]. Similar improvements are observed in domains where a subset of the categories was used to fine-tune the model. By prompting LLMs in various ways, we generate descriptions that capture visual appearance, habitat, and geographic regions and pair them with existing attributes such as the taxonomic structure of the categories. We systematically evaluate their ability to improve zero-shot categorization in natural domains. Our findings suggest that geographic priors can be just as effective and are complementary to visual appearance. Our method also outperforms prior work on prompt-based tuning of VLMs. We release the benchmark, consisting of 14 datasets at https://github.com/cvl-umass/AdaptCLIPZS, which will contribute to future research in zero-shot recognition.",
        "authors": [
            "Oindrila Saha",
            "Grant Van Horn",
            "Subhransu Maji"
        ],
        "citations": 11,
        "references": 52,
        "year": 2024
    },
    {
        "title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models",
        "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in various multi-modal tasks, but they are often bottlenecked by the limited context window and high computational cost of processing high-resolution image inputs and videos. Vision compression can alleviate this problem by reducing the vision token count. Previous approaches compress vision tokens with external modules and force LLMs to understand the compressed ones, leading to visual information loss. However, the LLMs' understanding paradigm of vision tokens is not fully utilised in the compression learning process. We propose VoCo-LLaMA, the first approach to compress vision tokens using LLMs. By introducing Vision Compression tokens during the vision instruction tuning phase and leveraging attention distillation, our method distill how LLMs comprehend vision tokens into their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision compression and improves the computational efficiency during the inference stage. Specifically, our method achieves minimal performance loss with a compression ratio of 576$\\times$, resulting in up to 94.8$\\%$ fewer FLOPs and 69.6$\\%$ acceleration in inference time. Furthermore, through continuous training using time-series compressed token sequences of video frames, VoCo-LLaMA demonstrates the ability to understand temporal correlations, outperforming previous methods on popular video question-answering benchmarks. Our approach presents a promising way to unlock the full potential of VLMs' contextual window, enabling more scalable multi-modal applications. The project page, along with the associated code, can be accessed via $\\href{https://yxxxb.github.io/VoCo-LLaMA-page/}{\\text{this https URL}}$.",
        "authors": [
            "Xubing Ye",
            "Yukang Gan",
            "Xiaoke Huang",
            "Yixiao Ge",
            "Ying Shan",
            "Yansong Tang"
        ],
        "citations": 10,
        "references": 49,
        "year": 2024
    },
    {
        "title": "MouSi: Poly-Visual-Expert Vision-Language Models",
        "abstract": "Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1. Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated. We have open-sourced the training code used in this report. All of these resources can be found on our project website.",
        "authors": [
            "Xiaoran Fan",
            "Tao Ji",
            "Changhao Jiang",
            "Shuo Li",
            "Senjie Jin",
            "Sirui Song",
            "Junke Wang",
            "Boyang Hong",
            "Luyao Chen",
            "Guodong Zheng",
            "Ming Zhang",
            "Caishuang Huang",
            "Rui Zheng",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Shihan Dou",
            "Junjie Ye",
            "Hang Yan",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Zuxuan Wu",
            "Yunchun Jiang"
        ],
        "citations": 11,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models",
        "abstract": "This study addresses the Domain-Class Incremental Learning problem, a realistic but challenging continual learning scenario where both the domain distribution and target classes vary across tasks. To handle these diverse tasks, pre-trained Vision-Language Models (VLMs) are introduced for their strong generalizability. However, this incurs a new problem: the knowledge encoded in the pre-trained VLMs may be disturbed when adapting to new tasks, compromising their inherent zero-shot ability. Existing methods tackle it by tuning VLMs with knowledge distillation on extra datasets, which demands heavy computation overhead. To address this problem efficiently, we propose the Distribution-aware Interference-free Knowledge Integration (DIKI) framework, retaining pre-trained knowledge of VLMs from a perspective of avoiding information interference. Specifically, we design a fully residual mechanism to infuse newly learned knowledge into a frozen backbone, while introducing minimal adverse impacts on pre-trained knowledge. Besides, this residual property enables our distribution-aware integration calibration scheme, explicitly controlling the information implantation process for test data from unseen distributions. Experiments demonstrate that our DIKI surpasses the current state-of-the-art approach using only 0.86% of the trained parameters and requiring substantially less training time. Code is available at: https://github.com/lloongx/DIKI .",
        "authors": [
            "Longxiang Tang",
            "Zhuotao Tian",
            "Kai Li",
            "Chunming He",
            "Hantao Zhou",
            "Hengshuang Zhao",
            "Xiu Li",
            "Jiaya Jia"
        ],
        "citations": 10,
        "references": 88,
        "year": 2024
    },
    {
        "title": "Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs",
        "abstract": "An elusive goal in navigation research is to build an intelligent agent that can understand multimodal instructions including natural language and image, and perform useful navigation. To achieve this, we study a widely useful category of navigation tasks we call Multimodal Instruction Navigation with demonstration Tours (MINT), in which the environment prior is provided through a previously recorded demonstration video. Recent advances in Vision Language Models (VLMs) have shown a promising path in achieving this goal as it demonstrates capabilities in perceiving and reasoning about multimodal inputs. However, VLMs are typically trained to predict textual output and it is an open research question about how to best utilize them in navigation. To solve MINT, we present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation policy that combines the environment understanding and common sense reasoning power of long-context VLMs and a robust low-level navigation policy based on topological graphs. The high-level policy consists of a long-context VLM that takes the demonstration tour video and the multimodal user instruction as input to find the goal frame in the tour video. Next, a low-level policy uses the goal frame and an offline constructed topological graph to generate robot actions at every timestep. We evaluated Mobility VLA in a 836m^2 real world environment and show that Mobility VLA has a high end-to-end success rates on previously unsolved multimodal instructions such as\"Where should I return this?\"while holding a plastic bin. A video demonstrating Mobility VLA can be found here: https://youtu.be/-Tof__Q8_5s",
        "authors": [
            "Hao-Tien Lewis Chiang",
            "Zhuo Xu",
            "Zipeng Fu",
            "M. Jacob",
            "Tingnan Zhang",
            "T. Lee",
            "Wenhao Yu",
            "Connor Schenck",
            "David Rendleman",
            "Dhruv Shah",
            "Fei Xia",
            "Jasmine Hsu",
            "Jonathan Hoech",
            "Pete Florence",
            "Sean Kirmani",
            "Sumeet Singh",
            "Vikas Sindhwani",
            "Carolina Parada",
            "Chelsea Finn",
            "Peng Xu",
            "Sergey Levine",
            "Jie Tan"
        ],
        "citations": 10,
        "references": 63,
        "year": 2024
    },
    {
        "title": "PromptFix: You Prompt and We Fix the Photo",
        "abstract": "Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks. The dataset and code are available at https://www.yongshengyu.com/PromptFix-Page.",
        "authors": [
            "Yongsheng Yu",
            "Ziyun Zeng",
            "Hang Hua",
            "Jianlong Fu",
            "Jiebo Luo"
        ],
        "citations": 10,
        "references": 82,
        "year": 2024
    },
    {
        "title": "\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors",
        "abstract": "Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic.",
        "authors": [
            "L. Guan",
            "Yifan Zhou",
            "Denis Liu",
            "Yantian Zha",
            "H. B. Amor",
            "Subbarao Kambhampati"
        ],
        "citations": 11,
        "references": 64,
        "year": 2024
    },
    {
        "title": "Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",
        "abstract": "Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs Visual Question Answering for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher CIDEr and ROUGE-L scores than the existing baseline on the DriveLM dataset. EM-VLM4AD also exhibits the ability to extract relevant information from traffic views related to prompts and can answer questions for various autonomous driving subtasks. We release our code to train and evaluate our model at https://github.com/akshaygopalkr/EM-VLM4AD.",
        "authors": [
            "Akshay Gopalkrishnan",
            "Ross Greer",
            "Mohan M. Trivedi"
        ],
        "citations": 10,
        "references": 37,
        "year": 2024
    },
    {
        "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
        "abstract": "Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.",
        "authors": [
            "Paul Gavrikov",
            "Jovita Lukasik",
            "Steffen Jung",
            "Robert Geirhos",
            "Bianca Lamm",
            "M. J. Mirza",
            "M. Keuper",
            "Janis Keuper"
        ],
        "citations": 9,
        "references": 82,
        "year": 2024
    },
    {
        "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
        "abstract": "The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.",
        "authors": [
            "Akash Ghosh",
            "Arkadeep Acharya",
            "Sriparna Saha",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "citations": 9,
        "references": 106,
        "year": 2024
    },
    {
        "title": "Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction",
        "abstract": "Whole Slide Image (WSI) classification is often formu-lated as a Multiple Instance Learning (MIL) problem. Re-cently, Vision-Language Models (VLMs) have demonstrated remarkable performance in WSI classification. However, existing methods leverage coarse-grained pathogenetic de-scriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of mod-els on diverse downstream tasks. Additionally, processing high-resolution WSIs can be computationally expensive. In this paper, we propose a novel ‚ÄúFine-grained Visual-Semantic Interaction‚Äù (FiVE) framework for WSI classi-fication. It is designed to enhance the model's general-izability by leveraging the interaction between localized visual patterns and fine-grained pathological semantics. Specifically, with meticulously designed queries, we start by utilizing a large language model to extract fine-grained pathological descriptions from various non-standardized raw reports. The output descriptions are then reconstructed into fine-grained labels used for training. By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable prompts to capture crucial visual information in WSIs, which enhances representation learning and aug-ments generalization capabilities significantly. Further-more, given that pathological visual patterns are redun-dantly distributed across tissue slices, we sample a subset of visual instances during training. Our method demon-strates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accu-racy in few-shot experiments. The code is available at: https://github.com/lslrius/WSI_FiVE.",
        "authors": [
            "Hao Li",
            "Ying Chen",
            "Yifei Chen",
            "Wenxian Yang",
            "Bowen Ding",
            "Yuchen Han",
            "Liansheng Wang",
            "Rongshan Yu"
        ],
        "citations": 9,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Cross-Modal Safety Alignment: Is textual unlearning all you need?",
        "abstract": "Recent studies reveal that integrating new modalities into Large Language Models (LLMs), such as Vision-Language Models (VLMs), creates a new attack surface that bypasses existing safety training techniques like Supervised Fine-tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). While further SFT and RLHF-based safety training can be conducted in multi-modal settings, collecting multi-modal training datasets poses a significant challenge. Inspired by the structural design of recent multi-modal models, where, regardless of the combination of input modalities, all inputs are ultimately fused into the language space, we aim to explore whether unlearning solely in the textual domain can be effective for cross-modality safety alignment. Our evaluation across six datasets empirically demonstrates the transferability -- textual unlearning in VLMs significantly reduces the Attack Success Rate (ASR) to less than 8\\% and in some cases, even as low as nearly 2\\% for both text-based and vision-text-based attacks, alongside preserving the utility. Moreover, our experiments show that unlearning with a multi-modal dataset offers no potential benefits but incurs significantly increased computational demands, possibly up to 6 times higher.",
        "authors": [
            "Trishna Chakraborty",
            "Erfan Shayegani",
            "Zikui Cai",
            "Nael B. Abu-Ghazaleh",
            "M. S. Asif",
            "Yue Dong",
            "A. Roy-Chowdhury",
            "Chengyu Song"
        ],
        "citations": 9,
        "references": 81,
        "year": 2024
    },
    {
        "title": "Diagnostic accuracy of vision-language models on Japanese diagnostic radiology, nuclear medicine, and interventional radiology specialty board examinations",
        "abstract": "Purpose: The performance of vision-language models (VLMs) with image interpretation capabilities, such as GPT-4 omni (GPT-4o), GPT-4 vision (GPT-4V), and Claude-3, has not been compared and remains unexplored in specialized radiological fields, including nuclear medicine and interventional radiology. This study aimed to evaluate and compare the diagnostic accuracy of various VLMs, including GPT-4 + GPT-4V, GPT-4o, Claude-3 Sonnet, and Claude-3 Opus, using Japanese diagnostic radiology, nuclear medicine, and interventional radiology (JDR, JNM, and JIR, respectively) board certification tests. Methods: In total, 383 questions from the JDR test (358 images), 300 from the JNM test (92 images), and 322 from the JIR test (96 images) from 2019 to 2023 were consecutively collected. The accuracy rates of the GPT-4 + GPT-4V, GPT-4o, Claude-3 Sonnet, and Claude-3 Opus were calculated for all questions or questions with images. The accuracy rates of the VLMs were compared using McNemar's test. Results: GPT-4o demonstrated the highest accuracy rates across all evaluations with the JDR (all questions, 49%; questions with images, 48%), JNM (all questions, 64%; questions with images, 59%), and JIR tests (all questions, 43%; questions with images, 34%), followed by Claude-3 Opus with the JDR (all questions, 40%; questions with images, 38%), JNM (all questions, 51%; questions with images, 43%), and JIR tests (all questions, 40%; questions with images, 30%). For all questions, McNemar's test showed that GPT-4o significantly outperformed the other VLMs (all P < 0.007), except for Claude-3 Opus in the JIR test. For questions with images, GPT-4o outperformed the other VLMs in the JDR and JNM tests (all P < 0.001), except Claude-3 Opus in the JNM test. Conclusion: The GPT-4o had the highest success rates for questions with images and all questions from the JDR, JNM, and JIR board certification tests.",
        "authors": [
            "Tatsushi Oura",
            "Hiroyuki Tatekawa",
            "Daisuke Horiuchi",
            "Shunichi Matsushita",
            "H. Takita",
            "Natsuko Atsukawa",
            "Yasuhito Mitsuyama",
            "Atsushi Yoshida",
            "Kazuki Murai",
            "Rikako Tanaka",
            "T. Shimono",
            "Akira Yamamoto",
            "Yukio Miki",
            "D. Ueda"
        ],
        "citations": 9,
        "references": 16,
        "year": 2024
    },
    {
        "title": "MMA: Multi-Modal Adapter for Vision-Language Models",
        "abstract": "Pretrained Vision-Language Models (VLMs) have served as excellent foundation models for transfer learning in diverse downstream tasks. However, tuning VLMs for few-shot generalization tasks faces a discrimination - generalization dilemma, i.e., general knowledge should be preserved and task-specific knowledge should be fine-tuned. How to precisely identify these two types of representations remains a challenge. In this paper, we propose a Multi-Modal Adapter (MMA) for VLMs to improve the alignment between representations from text and vision branches. MMA aggregates features from different branches into a shared feature space so that gradients can be communicated across branches. To determine how to incorporate MMA, we systematically analyze the discriminability and generalizability of features across diverse datasets in both the vision and language branches, and find that (1) higher lay-ers contain discriminable dataset-specific knowledge, while lower layers contain more generalizable knowledge, and (2) language features are more discriminable than visual features, and there are large semantic gaps between the features of the two modalities, especially in the lower layers. Therefore, we only incorporate MMA to a few higher lay-ers of transformers to achieve an optimal balance between discrimination and generalization. We evaluate the effectiveness of our approach on three tasks: generalization to novel classes, novel target datasets, and domain generalization. Compared to many state-of-the-art methods, our MMA achieves leading performance in all evaluations. Code is at https://github.com/ZjjConan/Multi-Modal-Adapter",
        "authors": [
            "Lingxiao Yang",
            "Ru-Yuan Zhang",
            "Yanchen Wang",
            "Xiaohua Xie"
        ],
        "citations": 8,
        "references": 86,
        "year": 2024
    },
    {
        "title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning",
        "abstract": "The advancement of large language models (LLMs) has significantly broadened the scope of applications in natural language processing, with multi-modal LLMs extending these capabilities to integrate and interpret visual data. However, existing benchmarks for visual language models (VLMs) predominantly focus on single-image inputs, neglecting the crucial aspect of multi-image understanding. In this paper, we introduce a Multi-Image Relational Benchmark MIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across multiple images. Our benchmark encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. Through a comprehensive evaluation of a wide range of open-source and closed-source models, we demonstrate that while open-source VLMs were shown to approach the performance of GPT-4V in single-image tasks, a significant performance gap remains in multi-image reasoning tasks. Our findings also reveal that even the state-of-the-art GPT-4V model struggles with our benchmark, underscoring the need for further research and development in this area. We believe our contribution of MIRB could serve as a testbed for developing the next-generation multi-modal models.",
        "authors": [
            "Bingchen Zhao",
            "Yongshuo Zong",
            "Letian Zhang",
            "Timothy M. Hospedales"
        ],
        "citations": 8,
        "references": 53,
        "year": 2024
    },
    {
        "title": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference",
        "abstract": "In vision-language models (VLMs), visual tokens usually consume a significant amount of computational overhead, despite their sparser information density compared to text tokens. To address this, most existing methods learn a network to prune redundant visual tokens and require additional training data. Differently, we propose an efficient training-free token optimization mechanism dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely, given that visual tokens complement text tokens in VLMs for linguistic reasoning, we select visual-relevant text tokens to rate the significance of vision tokens within the self-attention matrix extracted from the VLMs. Then we progressively prune irrelevant tokens. To maximize sparsity while retaining essential information, we introduce a rank-based strategy to adaptively determine the sparsification ratio for each layer, alongside a token recycling method that compresses pruned tokens into more compact representations. Experimental results show that our SparseVLM improves the efficiency of various VLMs across a range of image and video understanding tasks. In particular, LLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio of 78% while maintaining 93% of the accuracy. Our code is available at https://github.com/Gumpest/SparseVLMs.",
        "authors": [
            "Yuan Zhang",
            "Chun-Kai Fan",
            "Junpeng Ma",
            "Wenzhao Zheng",
            "Tao Huang",
            "Kuan Cheng",
            "Denis A Gudovskiy",
            "Tomoyuki Okuno",
            "Yohei Nakata",
            "Kurt Keutzer",
            "Shanghang Zhang"
        ],
        "citations": 8,
        "references": 49,
        "year": 2024
    },
    {
        "title": "Is It Safe to Cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing",
        "abstract": "Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context - a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages vision-language models (VLMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual knowledge, extracted from images and text prompts, we evaluate a VLM for safety score prediction and scene description. Our findings highlight the reasoning and safety score prediction capabilities of the VLM, activated by various prompts, as a pathway to developing a trustworthy system, crucial for applications requiring reliable decision-making support.",
        "authors": [
            "Hochul Hwang",
            "Sunjae Kwon",
            "Yekyung Kim",
            "Donghyun Kim"
        ],
        "citations": 8,
        "references": 40,
        "year": 2024
    },
    {
        "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
        "abstract": "Vision-Language Models (VLMs) have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks. A detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.",
        "authors": [
            "Yizhe Zhang",
            "Richard He Bai",
            "Ruixiang Zhang",
            "Jiatao Gu",
            "Shuangfei Zhai",
            "J. Susskind",
            "N. Jaitly"
        ],
        "citations": 8,
        "references": 40,
        "year": 2024
    },
    {
        "title": "CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection",
        "abstract": "The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, diffusion-based and commercial tools. Code and pre-trained models: https://github.com/sohailahmedkhan/CLIPping-the-Deception",
        "authors": [
            "Sohail Ahmed Khan",
            "Duc-Tien Dang-Nguyen"
        ],
        "citations": 8,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
        "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
        "authors": [
            "Xiangyu Qi",
            "Kaixuan Huang",
            "Ashwinee Panda",
            "Mengdi Wang",
            "Prateek Mittal"
        ],
        "citations": 98,
        "references": 89,
        "year": 2023
    },
    {
        "title": "What does CLIP know about a red circle? Visual prompt engineering for VLMs",
        "abstract": "Large-scale Vision-Language Models, such as CLIP, learn powerful image-text representations that have found numerous applications, from zero-shot classification to text-to-image generation. Despite that, their capabilities for solving novel discriminative tasks via prompting fall behind those of large language models, such as GPT-3. Here we explore the idea of visual prompt engineering for solving computer vision tasks beyond classification by editing in image space instead of text. In particular, we discover an emergent ability of CLIP, where, by simply drawing a red circle around an object, we can direct the model‚Äôs attention to that region, while also maintaining global information. We show the power of this simple approach by achieving state-of-the-art in zero-shot referring expressions comprehension and strong performance in keypoint localization tasks. Finally, we draw attention to some potential ethical concerns of large language-vision models.",
        "authors": [
            "Aleksandar Shtedritski",
            "C. Rupprecht",
            "A. Vedaldi"
        ],
        "citations": 102,
        "references": 70,
        "year": 2023
    },
    {
        "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
        "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
        "authors": [
            "Yunqing Zhao",
            "Tianyu Pang",
            "Chao Du",
            "Xiao Yang",
            "Chongxuan Li",
            "Ngai-Man Cheung",
            "Min Lin"
        ],
        "citations": 116,
        "references": 108,
        "year": 2023
    },
    {
        "title": "RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulaiton",
        "abstract": "Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a novel paradigm, aiming to enhance the model's ability to generalize to new objects and instructions. However, due to variations in camera specifications and mounting positions, existing methods exhibit significant performance disparities across different robotic platforms. To address this challenge, we propose RoboUniView in this paper, an innovative approach that decouples visual feature extraction from action learning. We first learn a unified view representation from multi-perspective views by pre-training on readily accessible data, and then derive actions from this unified view representation to control robotic manipulation. This unified view representation more accurately mirrors the physical world and is not constrained by the robotic platform's camera parameters. Thanks to this methodology, we achieve state-of-the-art performance on the demanding CALVIN benchmark, enhancing the success rate in the $D \\to D$ setting from 93.0% to 96.2%, and in the $ABC \\to D$ setting from 92.2% to 94.2%. Moreover, our model exhibits outstanding adaptability and flexibility: it maintains high performance under unseen camera parameters, can utilize multiple datasets with varying camera parameters, and is capable of joint cross-task learning across datasets. Code is provided for re-implementation. https://github.com/liufanfanlff/RoboUniview",
        "authors": [
            "Fanfan Liu",
            "Feng Yan",
            "Liming Zheng",
            "Chengjian Feng",
            "Yiyang Huang",
            "Lin Ma"
        ],
        "citations": 7,
        "references": 57,
        "year": 2024
    },
    {
        "title": "RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation",
        "abstract": "This work proposes a retrieve-and-transfer framework for zero-shot robotic manipulation, dubbed RAM, featuring generalizability across various objects, environments, and embodiments. Unlike existing approaches that learn manipulation from expensive in-domain demonstrations, RAM capitalizes on a retrieval-based affordance transfer paradigm to acquire versatile manipulation capabilities from abundant out-of-domain data. First, RAM extracts unified affordance at scale from diverse sources of demonstrations including robotic data, human-object interaction (HOI) data, and custom data to construct a comprehensive affordance memory. Then given a language instruction, RAM hierarchically retrieves the most similar demonstration from the affordance memory and transfers such out-of-domain 2D affordance to in-domain 3D executable affordance in a zero-shot and embodiment-agnostic manner. Extensive simulation and real-world evaluations demonstrate that our RAM consistently outperforms existing works in diverse daily tasks. Additionally, RAM shows significant potential for downstream applications such as automatic and efficient data collection, one-shot visual imitation, and LLM/VLM-integrated long-horizon manipulation. For more details, please check our website at https://yxkryptonite.github.io/RAM/.",
        "authors": [
            "Yuxuan Kuang",
            "Junjie Ye",
            "Haoran Geng",
            "Jiageng Mao",
            "Congyue Deng",
            "Leonidas J. Guibas",
            "He Wang",
            "Yue Wang"
        ],
        "citations": 7,
        "references": 73,
        "year": 2024
    },
    {
        "title": "MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries",
        "abstract": "In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries. This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care. Our dataset, code, and pre-trained models will be made publicly available.",
        "authors": [
            "Akash Ghosh",
            "Arkadeep Acharya",
            "Prince Jha",
            "Aniket Gaudgaul",
            "Rajdeep Majumdar",
            "Sriparna Saha",
            "Aman Chadha",
            "Raghav Jain",
            "Setu Sinha",
            "Shivani Agarwal"
        ],
        "citations": 7,
        "references": 36,
        "year": 2024
    },
    {
        "title": "ViTamin: Designing Scalable Vision Models in the Vision-Language Era",
        "abstract": "Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).",
        "authors": [
            "Jieneng Chen",
            "Qihang Yu",
            "Xiaohui Shen",
            "Alan L. Yuille",
            "Liang-Chieh Chen"
        ],
        "citations": 7,
        "references": 173,
        "year": 2024
    },
    {
        "title": "Diffusion Feedback Helps CLIP See Better",
        "abstract": "Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedback from text-to-image diffusion models to optimize CLIP representations, with only images (without corresponding text). We demonstrate that DIVA improves CLIP's performance on the challenging MMVP-VLM benchmark which assesses fine-grained visual abilities to a large extent (e.g., 3-7%), and enhances the performance of MLLMs and vision models on multimodal understanding and segmentation tasks. Extensive evaluation on 29 image classification and retrieval benchmarks confirms that our framework preserves CLIP's strong zero-shot capabilities. The code is available at https://github.com/baaivision/DIVA.",
        "authors": [
            "Wenxuan Wang",
            "Quan Sun",
            "Fan Zhang",
            "Yepeng Tang",
            "Jing Liu",
            "Xinlong Wang"
        ],
        "citations": 7,
        "references": 106,
        "year": 2024
    },
    {
        "title": "NEVLP: Noise-Robust Framework for Efficient Vision-Language Pre-training",
        "abstract": "The success of Vision Language Models (VLMs) on various vision-language tasks heavily relies on pre-training with large scale web-crawled datasets. However, the noisy and incomplete nature of web data makes dataset scale crucial for performance, rendering end-to-end training increasingly prohibitive. In this paper, we propose NEVLP, a noise-robust framework for efficient vision-language pre-training that requires less pre-training data. Specifically, we bridge the modality gap between a frozen image encoder and a large language model with a transformer and introduce two innovative learning strategies: noise-adaptive learning and concept-enhanced learning to mitigate the impact of noise. In noise-adaptive learning, we estimate the noise probability of each image-text pair based on the transformer's memorization effect and employ noise-adaptive regularization on image-text contrastive learning to condition cross-modal alignment. In concept-enhanced learning, we enrich incomplete text by incorporating visual concepts (objects in the image) to provide prior information about existing objects for image-text matching and image-grounded text generation, thereby mitigating text incompletion. Our framework effectively utilizes noisy web data and achieves state-of-the-art performance with less pre-training data across a wide range of vision-language tasks, including image-text retrieval, image captioning, and visual question answering.",
        "authors": [
            "Yiyi Tao",
            "Zhuoyue Wang",
            "Hang Zhang",
            "Lun Wang"
        ],
        "citations": 7,
        "references": 56,
        "year": 2024
    },
    {
        "title": "Can LLMs' Tuning Methods Work in Medical Multimodal Domain?",
        "abstract": "While Large Language Models (LLMs) excel in world knowledge understanding, adapting them to specific subfields requires precise adjustments. Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization. To address this challenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT) methods have emerged and achieved remarkable success in both LLMs and Large Vision-Language Models (LVLMs). In the medical domain, fine-tuning a medical Vision-Language Pretrained (VLP) model is essential for adapting it to specific tasks. Can the fine-tuning methods for large models be transferred to the medical field to enhance transfer learning efficiency? In this paper, we delve into the fine-tuning methods of LLMs and conduct extensive experiments to investigate the impact of fine-tuning methods for large models on the existing multimodal model in the medical domain from the training data level and the model structure level. We show the different impacts of fine-tuning methods for large models on medical VLMs and develop the most efficient ways to fine-tune medical VLP models. We hope this research can guide medical domain researchers in optimizing VLMs' training costs, fostering the broader application of VLMs in healthcare fields. The code and dataset have been released at https://github.com/TIMMY-CHAN/MILE.",
        "authors": [
            "Jiawei Chen",
            "Yue Jiang",
            "Dingkang Yang",
            "Mingcheng Li",
            "Jinjie Wei",
            "Ziyun Qian",
            "Lihua Zhang"
        ],
        "citations": 7,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Unveiling Encoder-Free Vision-Language Models",
        "abstract": "Existing vision-language models (VLMs) mostly rely on vision encoders to extract visual features followed by large language models (LLMs) for visual-language tasks. However, the vision encoders set a strong inductive bias in abstracting visual representation, e.g., resolution, aspect ratio, and semantic priors, which could impede the flexibility and efficiency of the VLMs. Training pure VLMs that accept the seamless vision and language inputs, i.e., without vision encoders, remains challenging and rarely explored. Empirical observations reveal that direct training without encoders results in slow convergence and large performance gaps. In this work, we bridge the gap between encoder-based and encoder-free models, and present a simple yet effective training recipe towards pure VLMs. Specifically, we unveil the key aspects of training encoder-free VLMs efficiently via thorough experiments: (1) Bridging vision-language representation inside one unified decoder; (2) Enhancing visual recognition capability via extra supervision. With these strategies, we launch EVE, an encoder-free vision-language model that can be trained and forwarded efficiently. Notably, solely utilizing 35M publicly accessible data, EVE can impressively rival the encoder-based VLMs of similar capacities across multiple vision-language benchmarks. It significantly outperforms the counterpart Fuyu-8B with mysterious training procedures and undisclosed training data. We believe that EVE provides a transparent and efficient route for developing a pure decoder-only architecture across modalities. Our code and models are publicly available at: https://github.com/baaivision/EVE.",
        "authors": [
            "Haiwen Diao",
            "Yufeng Cui",
            "Xiaotong Li",
            "Yueze Wang",
            "Huchuan Lu",
            "Xinlong Wang"
        ],
        "citations": 7,
        "references": 84,
        "year": 2024
    },
    {
        "title": "Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection",
        "abstract": "Open-vocabulary human-object interaction (HOI) detection, which is concerned with the problem of detecting novel HOIs guided by natural language, is crucial for understanding human-centric scenes. However, prior zero-shot HOI detectors often employ the same levels of feature maps to model HOIs with varying distances, leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances. In addition, these detectors primarily rely on category names and over-look the rich contextual information that language can provide, which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone. In this paper, we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE), harnessing the potential of Visual-Language Models (VLMs). Specifically, we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process. Furthermore, by leveraging large language models (LLMs) such as GPT models, we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions. Then we integrate the generalizable and fine- grained semantics of human body parts to improve interaction recognition. Experimental results on two datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection. The code and models are available at https://github.com/ltttpku/CMD-SE-release.",
        "authors": [
            "Ting Lei",
            "Shaofeng Yin",
            "Yang Liu"
        ],
        "citations": 7,
        "references": 68,
        "year": 2024
    },
    {
        "title": "PALO: A Polyglot Large Multimodal Model for 5B People",
        "abstract": "In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called PALO. PALO offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of ~5B people (65% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages. Code: https://github.com/mbzuai-oryx/PALO.",
        "authors": [
            "Muhammad Maaz",
            "H. Rasheed",
            "Abdelrahman M. Shaker",
            "Salman H. Khan",
            "Hisham Cholakkal",
            "R. Anwer",
            "Timothy Baldwin",
            "M. Felsberg",
            "F. Khan"
        ],
        "citations": 7,
        "references": 26,
        "year": 2024
    },
    {
        "title": "Unified Physical-Digital Face Attack Detection",
        "abstract": "Face Recognition (FR) systems can suffer from physical (i.e., print photo) and digital (i.e., DeepFake) attacks. However, previous related work rarely considers both situations at the same time. This implies the deployment of multiple models and thus more computational burden. The main reasons for this lack of an integrated model are caused by two factors: (1) The lack of a dataset including both physical and digital attacks which the same ID covers the real face and all attack types; (2) Given the large intra-class variance between these two attacks, it is difficult to learn a compact feature space to detect both attacks simultaneously. To address these issues, we collect a Unified physical-digital Attack dataset, called UniAttackData. The dataset consists of 1,800 participations of 2 and 12 physical and digital attacks, respectively, resulting in a total of 28,706 videos. Then, we propose a Unified Attack Detection framework based on Vision-Language Models (VLMs), namely UniAttackDetection, which includes three main modules: the Teacher-Student Prompts (TSP) module, focused on acquiring unified and specific knowledge respectively; the Unified Knowledge Mining (UKM) module, designed to capture a comprehensive feature space; and the Sample-Level Prompt Interaction (SLPI) module, aimed at grasping sample-level semantics. These three modules seamlessly form a robust unified attack detection framework. Extensive experiments on UniAttackData and three other datasets demonstrate the superiority of our approach for unified face attack detection. Dataset link: https://sites.google.com/view/face-anti-spoofing-challenge/dataset-download/uniattackdatacvpr2024",
        "authors": [
            "Hao Fang",
            "Ajian Liu",
            "Haocheng Yuan",
            "Junze Zheng",
            "Dingheng Zeng",
            "Yanhong Liu",
            "Jiankang Deng",
            "Sergio Escalera",
            "Xiaoming Liu",
            "Jun Wan",
            "Zhen Lei"
        ],
        "citations": 7,
        "references": 58,
        "year": 2024
    },
    {
        "title": "FINEMATCH: Aspect-based Fine-grained Image and Text Mismatch Detection and Correction",
        "abstract": "Recent progress in large-scale pre-training has led to the development of advanced vision-language models (VLMs) with remarkable proficiency in comprehending and generating multimodal content. Despite the impressive ability to perform complex reasoning for VLMs, current models often struggle to effectively and precisely capture the compositional information on both the image and text sides. To address this, we propose FineMatch, a new aspect-based fine-grained text and image matching benchmark, focusing on text and image mismatch detection and correction. This benchmark introduces a novel task for boosting and evaluating the VLMs' compositionality for aspect-based fine-grained text and image matching. In this task, models are required to identify mismatched aspect phrases within a caption, determine the aspect's class, and propose corrections for an image-text pair that may contain between 0 and 3 mismatches. To evaluate the models' performance on this new task, we propose a new evaluation metric named ITM-IoU for which our experiments show a high correlation to human evaluation. In addition, we also provide a comprehensive experimental analysis of existing mainstream VLMs, including fully supervised learning and in-context learning settings. We have found that models trained on FineMatch demonstrate enhanced proficiency in detecting fine-grained text and image mismatches. Moreover, models (e.g., GPT-4V, Gemini Pro Vision) with strong abilities to perform multimodal in-context learning are not as skilled at fine-grained compositional image and text matching analysis. With FineMatch, we are able to build a system for text-to-image generation hallucination detection and correction.",
        "authors": [
            "Hang Hua",
            "Jing Shi",
            "Kushal Kafle",
            "Simon Jenni",
            "Daoan Zhang",
            "John P. Collomosse",
            "Scott Cohen",
            "Jiebo Luo"
        ],
        "citations": 7,
        "references": 69,
        "year": 2024
    },
    {
        "title": "A Concept-Based Explainability Framework for Large Multimodal Models",
        "abstract": "Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as ``multi-modal concepts''. We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. Our code is publicly available at https://github.com/mshukor/xl-vlms",
        "authors": [
            "Jayneel Parekh",
            "Pegah Khayatan",
            "Mustafa Shukor",
            "A. Newson",
            "Matthieu Cord"
        ],
        "citations": 7,
        "references": 48,
        "year": 2024
    },
    {
        "title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models",
        "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have underscored their superiority in various multimodal tasks. However, the adversarial robustness of VLMs has not been fully explored. Existing methods mainly assess robustness through unimodal adversarial attacks that perturb images, while assuming inherent resilience against text-based attacks. Different from existing attacks, in this work we propose a more comprehensive strategy that jointly attacks both text and image modalities to exploit a broader spectrum of vulnerability within VLMs. Specifically, we propose a dual optimization objective aimed at guiding the model to generate affirmative responses with high toxicity. Our attack method begins by optimizing an adversarial image prefix from random noise to generate diverse harmful responses in the absence of text input, thus imbuing the image with toxic semantics. Subsequently, an adversarial text suffix is integrated and co-optimized with the adversarial image prefix to maximize the probability of eliciting affirmative responses to various harmful instructions. The discovered adversarial image prefix and text suffix are collectively denoted as a Universal Master Key (UMK). When integrated into various malicious queries, UMK can circumvent the alignment defenses of VLMs and lead to the generation of objectionable content, known as jailbreaks. The experimental results demonstrate that our universal attack strategy can effectively jailbreak MiniGPT-4 with a 96% success rate, highlighting the vulnerability of VLMs and the urgent need for new alignment strategies.",
        "authors": [
            "Ruofan Wang",
            "Xingjun Ma",
            "Hanxu Zhou",
            "Chuanjun Ji",
            "Guangnan Ye",
            "Yu-Gang Jiang"
        ],
        "citations": 7,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Too Many Frames, not all Useful: Efficient Strategies for Long-Form Video QA",
        "abstract": "Long-form videos that span across wide temporal intervals are highly information redundant and contain multiple distinct events or entities that are often loosely related. Therefore, when performing long-form video question answering (LVQA), all information necessary to generate a correct response can often be contained within a small subset of frames. Recent literature explore the use of large language models (LLMs) in LVQA benchmarks, achieving exceptional performance, while relying on vision language models (VLMs) to convert all visual content within videos into natural language. Such VLMs often independently caption a large number of frames uniformly sampled from long videos, which is not efficient and can mostly be redundant. Questioning these decision choices, we explore optimal strategies for key-frame selection that can significantly reduce these redundancies, namely Hierarchical Keyframe Selector. Our proposed framework, LVNet, achieves state-of-the-art performance at a comparable caption scale across three benchmark LVQA datasets: EgoSchema, NExT-QA, IntentQA. The code can be found at https://github.com/jongwoopark7978/LVNet",
        "authors": [
            "Jong Sung Park",
            "Kanchana Ranasinghe",
            "Kumara Kahatapitiya",
            "Wonjeong Ryoo",
            "Donghyun Kim",
            "M. Ryoo"
        ],
        "citations": 6,
        "references": 57,
        "year": 2024
    },
    {
        "title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples",
        "abstract": "Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.",
        "authors": [
            "Baiqi Li",
            "Zhiqiu Lin",
            "Wenxuan Peng",
            "Jean de Dieu Nyandwi",
            "Daniel Jiang",
            "Zixian Ma",
            "Simran Khanuja",
            "Ranjay Krishna",
            "Graham Neubig",
            "Deva Ramanan"
        ],
        "citations": 6,
        "references": 90,
        "year": 2024
    },
    {
        "title": "CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning",
        "abstract": "This paper explores the problem of continual learning (CL) of vision-language models (VLMs) in open domains, where the models need to perform continual updating and inference on a streaming of datasets from diverse seen and unseen domains with novel classes. Such a capability is crucial for various applications in open environments, e.g., AI assistants, autonomous driving systems, and robotics. Current CL studies mostly focus on closed-set scenarios in a single domain with known classes. Large pre-trained VLMs like CLIP have demonstrated superior zero-shot recognition ability, and a number of recent studies leverage this ability to mitigate catastrophic forgetting in CL, but they focus on closed-set CL in a single domain dataset. Open-domain CL of large VLMs is significantly more challenging due to 1) large class correlations and domain gaps across the datasets and 2) the forgetting of zero-shot knowledge in the pre-trained VLMs in addition to the knowledge learned from the newly adapted datasets. In this work we introduce a novel approach, termed CoLeCLIP, that learns an open-domain CL model based on CLIP. It addresses these challenges by a joint learning of a set of task prompts and a cross-domain class vocabulary. Extensive experiments on 11 domain datasets show that CoLeCLIP outperforms state-of-the-art methods for open-domain CL under both task- and class-incremental learning settings.",
        "authors": [
            "Yukun Li",
            "Guansong Pang",
            "Wei Suo",
            "Chenchen Jing",
            "Yuling Xi",
            "Lingqiao Liu",
            "Hao Chen",
            "Guoqiang Liang",
            "Peng Wang"
        ],
        "citations": 6,
        "references": 45,
        "year": 2024
    },
    {
        "title": "A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models",
        "abstract": "Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of popular vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.",
        "authors": [
            "Ashutosh Sathe",
            "Prachi Jain",
            "Sunayana Sitaram"
        ],
        "citations": 6,
        "references": 38,
        "year": 2024
    },
    {
        "title": "Defending Jailbreak Attack in VLMs via Cross-modality Information Detector",
        "abstract": "Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively understand vision information, achieving remarkable performance in many vision-centric tasks. Despite that, recent studies have shown that these models are susceptible to jailbreak attacks, which refer to an exploitative technique where malicious users can break the safety alignment of the target model and generate mis-leading and harmful answers. This potential threat is caused by both the inherent vulnerabilities of LLM and the larger attack scope introduced by vision input. To enhance the security of VLMs against jailbreak attacks, researchers have developed various defense techniques. However, these methods either require modifications to the model‚Äôs internal structure or demand significant computational resources during the inference phase. Multimodal information is a double-edged sword. While it increases the risk of attacks, it also provides additional data that can enhance safeguards. Inspired by this, we propose C ross-modality I nformation DE tecto R ( CIDER ), a plug-and-play jailbreaking detector designed to identify maliciously perturbed image inputs, utilizing the cross-modal similarity between harmful queries and adversarial images. This simple yet effective cross-modality information detector, CIDER , is independent of the target VLMs and requires less computation cost. Extensive experimental results demonstrate the effectiveness and efficiency of CIDER , as well as its transferability to both white-box and black-box VLMs.",
        "authors": [
            "Yue Xu",
            "Xiuyuan Qi",
            "Zhan Qin",
            "Wenjie Wang"
        ],
        "citations": 6,
        "references": 33,
        "year": 2024
    },
    {
        "title": "Learn \"No\" to Say \"Yes\" Better: Improving Vision-Language Models via Negations",
        "abstract": "Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This paper highlights the limitations of popular VLMs such as CLIP, at understanding the implications of negations, i.e., the effect of the word\"not\"in a given prompt. To enable evaluation of VLMs on fluent prompts with negations, we present CC-Neg, a dataset containing 228,246 images, true captions and their corresponding negated captions. Using CC-Neg along with modifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework, has an improved understanding of negations. This training paradigm improves CoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average gain in top-1 accuracy for zero-shot image classification across 8 datasets. Further, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks such as SugarCREPE by 4.4%, showcasing emergent compositional understanding of objects, relations, and attributes in text. Overall, our work addresses a crucial limitation of VLMs by introducing a dataset and framework that strengthens semantic associations between images and text, demonstrating improved large-scale foundation models with significantly reduced computational cost, promoting efficiency and accessibility.",
        "authors": [
            "Jaisidh Singh",
            "Ishaan Shrivastava",
            "M. Vatsa",
            "Richa Singh",
            "Aparna Bharati"
        ],
        "citations": 6,
        "references": 61,
        "year": 2024
    },
    {
        "title": "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners",
        "abstract": "Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of ‚Äònon-human‚Äô agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs in this setup remain unattested and underexplored. In this work, we study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.",
        "authors": [
            "Chengzu Li",
            "Caiqi Zhang",
            "Han Zhou",
            "Nigel Collier",
            "Anna Korhonen",
            "Ivan Vuli'c"
        ],
        "citations": 6,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models",
        "abstract": "Vision-Large-Language-models(VLMs) have great application prospects in autonomous driving. Despite the ability of VLMs to comprehend and make decisions in complex scenarios, their integration into safety-critical autonomous driving systems poses serious security risks. In this paper, we propose BadVLMDriver, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects. Unlike existing backdoor attacks against VLMs that rely on digital modifications, BadVLMDriver uses common physical items, such as a red balloon, to induce unsafe actions like sudden acceleration, highlighting a significant real-world threat to autonomous vehicle safety. To execute BadVLMDriver, we develop an automated pipeline utilizing natural language instructions to generate backdoor training samples with embedded malicious behaviors. This approach allows for flexible trigger and behavior selection, enhancing the stealth and practicality of the attack in diverse scenarios. We conduct extensive experiments to evaluate BadVLMDriver for two representative VLMs, five different trigger objects, and two types of malicious backdoor behaviors. BadVLMDriver achieves a 92% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon. Thus, BadVLMDriver not only demonstrates a critical security risk but also emphasizes the urgent need for developing robust defense mechanisms to protect against such vulnerabilities in autonomous driving technologies.",
        "authors": [
            "Zhenyang Ni",
            "Rui Ye",
            "Yuxian Wei",
            "Zhen Xiang",
            "Yanfeng Wang",
            "Siheng Chen"
        ],
        "citations": 6,
        "references": 90,
        "year": 2024
    },
    {
        "title": "A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models",
        "abstract": "Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of popular vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.",
        "authors": [
            "Ashutosh Sathe",
            "Prachi Jain",
            "Sunayana Sitaram"
        ],
        "citations": 6,
        "references": 38,
        "year": 2024
    },
    {
        "title": "Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts",
        "abstract": "Current weakly supervised video anomaly detection (WSVAD) task aims to achieve frame-level anomalous event detection with only coarse video-level annotations available. Existing works typically involve extracting global features from full-resolution video frames and training frame-level classifiers to detect anomalies in the temporal dimension. However, most anomalous events tend to occur in localized spatial regions rather than the entire video frames, which implies existing frame-level feature based works may be misled by the dominant background information and lack the interpretation of the detected anomalies. To address this dilemma, this paper introduces a novel method called STPrompt that learns spatio-temporal prompt embeddings for weakly supervised video anomaly detection and localization (WSVADL) based on pre-trained vision-language models (VLMs). Our proposed method employs a two-stream network structure, with one stream focusing on the temporal dimension and the other primarily on the spatial dimension. By leveraging the learned knowledge from pre-trained VLMs and incorporating natural motion priors from raw videos, our model learns prompt embeddings that are aligned with spatio-temporal regions of videos (e.g., patches of individual frames) for identify specific local regions of anomalies, enabling accurate video anomaly detection while mitigating the influence of background information. Without relying on detailed spatio-temporal annotations or auxiliary object detection/tracking, our method achieves state-of-the-art performance on three public benchmarks for the WSVADL task.",
        "authors": [
            "Peng Wu",
            "Xuerong Zhou",
            "Guansong Pang",
            "Zhiwei Yang",
            "Qingsen Yan",
            "Peng Wang",
            "Yanning Zhang"
        ],
        "citations": 5,
        "references": 73,
        "year": 2024
    },
    {
        "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
        "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
        "authors": [
            "Andy Zeng",
            "Adrian S. Wong",
            "Stefan Welker",
            "K. Choromanski",
            "F. Tombari",
            "Aveek Purohit",
            "M. Ryoo",
            "Vikas Sindhwani",
            "Johnny Lee",
            "Vincent Vanhoucke",
            "Peter R. Florence"
        ],
        "citations": 524,
        "references": 165,
        "year": 2022
    },
    {
        "title": "The Conversation is the Command: Interacting with Real-World Autonomous Robots Through Natural Language",
        "abstract": "In recent years, autonomous agents have surged in real-world environments such as our homes, offices, and public spaces. However, natural human-robot interaction remains a key challenge. In this paper, we introduce an approach that synergistically exploits the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs) to enable humans to interact naturally with autonomous robots through conversational dialogue. We leveraged the LLMs to decode the high-level natural language instructions from humans and abstract them into precise robot actionable commands or queries. Further, we utilised the VLMs to provide a visual and semantic understanding of the robot's task environment. Our results with 99.13% command recognition accuracy and 97.96% commands execution success show that our approach can enhance human-robot interaction in real-world applications. The video demonstrations of this paper can be found at https://osf.io/wzyf6 and the code is available at our GitHub repository.",
        "authors": [
            "Linus Nwankwo",
            "Elmar Rueckert"
        ],
        "citations": 5,
        "references": 34,
        "year": 2024
    },
    {
        "title": "ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty",
        "abstract": "Compositionality is a critical capability in Text-to-Image (T2I) models, as it reflects their ability to understand and combine multiple concepts from text descriptions. Existing evaluations of compositional capability rely heavily on human-designed text prompts or fixed templates, limiting their diversity and complexity, and yielding low discriminative power. We propose ConceptMix, a scalable, controllable, and customizable benchmark which automatically evaluates compositional generation ability of T2I models. This is done in two stages. First, ConceptMix generates the text prompts: concretely, using categories of visual concepts (e.g., objects, colors, shapes, spatial relationships), it randomly samples an object and k-tuples of visual concepts, then uses GPT4-o to generate text prompts for image generation based on these sampled concepts. Second, ConceptMix evaluates the images generated in response to these prompts: concretely, it checks how many of the k concepts actually appeared in the image by generating one question per visual concept and using a strong VLM to answer them. Through administering ConceptMix to a diverse set of T2I models (proprietary as well as open ones) using increasing values of k, we show that our ConceptMix has higher discrimination power than earlier benchmarks. Specifically, ConceptMix reveals that the performance of several models, especially open models, drops dramatically with increased k. Importantly, it also provides insight into the lack of prompt diversity in widely-used training datasets. Additionally, we conduct extensive human studies to validate the design of ConceptMix and compare our automatic grading with human judgement. We hope it will guide future T2I model development.",
        "authors": [
            "Xindi Wu",
            "Dingli Yu",
            "Yangsibo Huang",
            "Olga Russakovsky",
            "Sanjeev Arora"
        ],
        "citations": 5,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Recent Advances of Foundation Language Models-based Continual Learning: A Survey",
        "abstract": "Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV). Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. Despite these capabilities, LMs still struggle with catastrophic forgetting, hindering their ability to learn continuously like humans. To address this, continual learning (CL) methodologies have been introduced, allowing LMs to adapt to new tasks while retaining learned knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking. In this paper, we delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models (PLMs), large language models (LLMs) and vision-language models (VLMs). We divide these studies into offline and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.",
        "authors": [
            "Yutao Yang",
            "Jie Zhou",
            "Xuanwen Ding",
            "Tianyu Huai",
            "Shunyu Liu",
            "Qin Chen",
            "Liang He",
            "Yuan Xie"
        ],
        "citations": 5,
        "references": 214,
        "year": 2024
    },
    {
        "title": "Efficiency in Focus: LayerNorm as a Catalyst for Fine-tuning Medical Visual Language Pre-trained Models",
        "abstract": "In the realm of Medical Visual Language Models (Med-VLMs), the quest for universal efficient fine-tuning mechanisms remains paramount, especially given researchers in interdisciplinary fields are often extremely short of training resources, yet largely unexplored. Given the unique challenges in the medical domain, such as limited data scope and significant domain-specific requirements, evaluating and adapting Parameter-Efficient Fine-Tuning (PEFT) methods specifically for Med-VLMs is essential. Most of the current PEFT methods on Med-VLMs have yet to be comprehensively investigated but mainly focus on adding some components to the model's structure or input. However, fine-tuning intrinsic model components often yields better generality and consistency, and its impact on the ultimate performance of Med-VLMs has been widely overlooked and remains understudied. In this paper, we endeavour to explore an alternative to traditional PEFT methods, especially the impact of fine-tuning LayerNorm layers, FFNs and Attention layers on the Med-VLMs. Our comprehensive studies span both small-scale and large-scale Med-VLMs, evaluating their performance under various fine-tuning paradigms across tasks such as Medical Visual Question Answering and Medical Imaging Report Generation. The findings reveal unique insights into the effects of intrinsic parameter fine-tuning methods on fine-tuning Med-VLMs to downstream tasks and expose fine-tuning solely the LayerNorm layers not only surpasses the efficiency of traditional PEFT methods but also retains the model's accuracy and generalization capabilities across a spectrum of medical downstream tasks. The experiments show LayerNorm fine-tuning's superior adaptability and scalability, particularly in the context of large-scale Med-VLMs.",
        "authors": [
            "Jiawei Chen",
            "Dingkang Yang",
            "Yue Jiang",
            "Mingcheng Li",
            "Jinjie Wei",
            "Xiaolu Hou",
            "Lihua Zhang"
        ],
        "citations": 5,
        "references": 45,
        "year": 2024
    },
    {
        "title": "GraspSplats: Efficient Manipulation with 3D Feature Splatting",
        "abstract": "The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.",
        "authors": [
            "Mazeyu Ji",
            "Ri-Zhao Qiu",
            "Xueyan Zou",
            "Xiaolong Wang"
        ],
        "citations": 5,
        "references": 50,
        "year": 2024
    },
    {
        "title": "VoxAct-B: Voxel-Based Acting and Stabilizing Policy for Bimanual Manipulation",
        "abstract": "Bimanual manipulation is critical to many robotics applications. In contrast to single-arm manipulation, bimanual manipulation tasks are challenging due to higher-dimensional action spaces. Prior works leverage large amounts of data and primitive actions to address this problem, but may suffer from sample inefficiency and limited generalization across various tasks. To this end, we propose VoxAct-B, a language-conditioned, voxel-based method that leverages Vision Language Models (VLMs) to prioritize key regions within the scene and reconstruct a voxel grid. We provide this voxel grid to our bimanual manipulation policy to learn acting and stabilizing actions. This approach enables more efficient policy learning from voxels and is generalizable to different tasks. In simulation, we show that VoxAct-B outperforms strong baselines on fine-grained bimanual manipulation tasks. Furthermore, we demonstrate VoxAct-B on real-world $\\texttt{Open Drawer}$ and $\\texttt{Open Jar}$ tasks using two UR5s. Code, data, and videos are available at https://voxact-b.github.io.",
        "authors": [
            "I-Chun Arthur Liu",
            "Sicheng He",
            "Daniel Seita",
            "Gaurav Sukhatme"
        ],
        "citations": 5,
        "references": 70,
        "year": 2024
    },
    {
        "title": "A3VLM: Actionable Articulation-Aware Vision Language Model",
        "abstract": "Vision Language Models (VLMs) have received significant attention in recent years in the robotics community. VLMs are shown to be able to perform complex visual reasoning and scene understanding tasks, which makes them regarded as a potential universal solution for general robotics problems such as manipulation and navigation. However, previous VLMs for robotics such as RT-1, RT-2, and ManipLLM have focused on directly learning robot-centric actions. Such approaches require collecting a significant amount of robot interaction data, which is extremely costly in the real world. Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model. A3VLM focuses on the articulation structure and action affordances of objects. Its representation is robot-agnostic and can be translated into robot actions using simple action primitives. Extensive experiments in both simulation benchmarks and real-world settings demonstrate the effectiveness and stability of A3VLM. We release our code and other materials at https://github.com/changhaonan/A3VLM.",
        "authors": [
            "Siyuan Huang",
            "Haonan Chang",
            "Yuhan Liu",
            "Yimeng Zhu",
            "Hao Dong",
            "Peng Gao",
            "Abdeslam Boularias",
            "Hongsheng Li"
        ],
        "citations": 5,
        "references": 39,
        "year": 2024
    },
    {
        "title": "OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments",
        "abstract": "Environment representations endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks. Open-vocabulary representation, powered by Visual-Language models (VLMs), possesses inherent advantages, including zero-shot learning and open-set cognition. However, existing open-vocabulary maps are primarily designed for small-scale environments, such as desktops or rooms, and are typically geared towards limited-area tasks involving robotic indoor navigation or in-place manipulation. They face challenges in direct generalization to outdoor environments characterized by numerous objects and complex tasks, owing to limitations in both understanding level and map structure. In this work, we propose OpenGraph, a novel open-vocabulary hierarchical graph representation designed for large-scale outdoor environments. OpenGraph initially extracts instances and their captions from visual images, enhancing textual reasoning by encoding captions. Subsequently, it achieves 3D incremental object-centric mapping with feature embedding by projecting images onto LiDAR point clouds. Finally, the environment is segmented based on lane graph connectivity to construct a hierarchical representation. Validation results from SemanticKITTI and real-world scene demonstrate that OpenGraph achieves high segmentation and query accuracy.",
        "authors": [
            "Yinan Deng",
            "Jiahui Wang",
            "Jingyu Zhao",
            "Xinyu Tian",
            "Guangyan Chen",
            "Yi Yang",
            "Yufeng Yue"
        ],
        "citations": 5,
        "references": 48,
        "year": 2024
    },
    {
        "title": "RS-Agent: Automating Remote Sensing Tasks through Intelligent Agents",
        "abstract": "An increasing number of models have achieved great performance in remote sensing tasks with the recent development of Large Language Models (LLMs) and Visual Language Models (VLMs). However, these models are constrained to basic vision and language instruction-tuning tasks, facing challenges in complex remote sensing applications. Additionally, these models lack specialized expertise in professional domains. To address these limitations, we propose a LLM-driven remote sensing intelligent agent named RS-Agent. Firstly, RS-Agent is powered by a large language model (LLM) that acts as its\"Central Controller,\"enabling it to understand and respond to various problems intelligently. Secondly, our RS-Agent integrates many high-performance remote sensing image processing tools, facilitating multi-tool and multi-turn conversations. Thirdly, our RS-Agent can answer professional questions by leveraging robust knowledge documents. We conducted experiments using several datasets, e.g., RSSDIVCS, RSVQA, and DOTAv1. The experimental results demonstrate that our RS-Agent delivers outstanding performance in many tasks, i.e., scene classification, visual question answering, and object counting tasks.",
        "authors": [
            "Wenjia Xu",
            "Zijian Yu",
            "Yixu Wang",
            "Jiuniu Wang",
            "Mugen Peng"
        ],
        "citations": 5,
        "references": 83,
        "year": 2024
    },
    {
        "title": "LRQ-Fact: LLM-Generated Relevant Questions for Multimodal Fact-Checking",
        "abstract": "Human fact-checkers have specialized domain knowledge that allows them to formulate precise questions to verify information accuracy. However, this expert-driven approach is labor-intensive and is not scalable, especially when dealing with complex multimodal misinformation. In this paper, we propose a fully-automated framework, LRQ-Fact, for multimodal fact-checking. Firstly, the framework leverages Vision-Language Models (VLMs) and Large Language Models (LLMs) to generate comprehensive questions and answers for probing multimodal content. Next, a rule-based decision-maker module evaluates both the original content and the generated questions and answers to assess the overall veracity. Extensive experiments on two benchmarks show that LRQ-Fact improves detection accuracy for multimodal misinformation. Moreover, we evaluate its generalizability across different model backbones, offering valuable insights for further refinement.",
        "authors": [
            "Alimohammad Beigi",
            "Bohan Jiang",
            "Dawei Li",
            "Tharindu Kumarage",
            "Zhen Tan",
            "Pouya Shaeri",
            "Huan Liu"
        ],
        "citations": 5,
        "references": 83,
        "year": 2024
    },
    {
        "title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments",
        "abstract": "High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to the need to encode multiple partitions of a high-resolution image input. Processing such a large number of visual tokens through multiple transformer networks poses significant computational challenges, particularly for resource-constrained commodity GPUs. To address this challenge, we propose High-Resolution Early Dropping (HiRED), a plug-and-play token-dropping method designed to operate within a fixed token budget. HiRED leverages the attention of CLS token in the vision transformer (ViT) to assess the visual content of the image partitions and allocate an optimal token budget for each partition accordingly. The most informative visual tokens from each partition within the allocated budget are then selected and passed to the subsequent Large Language Model (LLM). We showed that HiRED achieves superior accuracy and performance, compared to existing token-dropping methods. Empirically, HiRED-20% (i.e., a 20% token budget) on LLaVA-Next-7B achieves a 4.7x increase in token generation throughput, reduces response latency by 78%, and saves 14% of GPU memory for single inference on an NVIDIA TESLA P40 (24 GB). For larger batch sizes (e.g., 4), HiRED-20% prevents out-of-memory errors by cutting memory usage by 30%, while preserving throughput and latency benefits. Code - https://github.com/hasanar1f/HiRED",
        "authors": [
            "Kazi Hasan Ibn Arif",
            "JinYi Yoon",
            "Dimitrios S. Nikolopoulos",
            "Hans Vandierendonck",
            "Deepu John",
            "Bo Jin"
        ],
        "citations": 5,
        "references": 32,
        "year": 2024
    },
    {
        "title": "The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models",
        "abstract": "‚ÄòScale the model, scale the data, scale the GPU farms‚Äô is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. All the meta-datasets curated in this endeavor and the code used are shared at: https://github.com/SepehrDehdashtian/the-dark-side-of-dataset-scaling. Content warning: This article contains racially dehumanising and offensive descriptions.",
        "authors": [
            "Abeba Birhane",
            "Sepehr Dehdashtian",
            "Vinay Prabhu",
            "Vishnu Naresh Boddeti"
        ],
        "citations": 4,
        "references": 105,
        "year": 2024
    },
    {
        "title": "Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models",
        "abstract": "Online user generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studies reveal a new understanding of this problem and the urgent need for automatically flagging illicit UGCG promotions. We additionally create a cutting-edge system, UGCG-Guard, designed to aid social media platforms in effectively identifying images used for illicit UGCG promotions. This system leverages recently introduced large vision-language models (VLMs) and employs a novel conditional prompting strategy for zero-shot domain adaptation, along with chain-of-thought (CoT) reasoning for contextual identification. UGCG-Guard achieves outstanding results, with an accuracy rate of 94% in detecting these images used for the illicit promotion of such games in real-world scenarios.",
        "authors": [
            "Keyan Guo",
            "Ayush Utkarsh",
            "Wenbo Ding",
            "Isabelle Ondracek",
            "Ziming Zhao",
            "Guo Freeman",
            "Nishant Vishwamitra",
            "Hongxin Hu"
        ],
        "citations": 4,
        "references": 69,
        "year": 2024
    },
    {
        "title": "LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models",
        "abstract": "Out-of-distribution (OOD) detection is crucial for model reliability, as it identifies samples from unknown classes and reduces errors due to unexpected inputs. Vision-Language Models (VLMs) such as CLIP are emerging as powerful tools for OOD detection by integrating multi-modal information. However, the practical application of such systems is challenged by manual prompt engineering, which demands domain expertise and is sensitive to linguistic nuances. In this paper, we introduce Label-driven Automated Prompt Tuning (LAPT), a novel approach to OOD detection that reduces the need for manual prompt engineering. We develop distribution-aware prompts with in-distribution (ID) class names and negative labels mined automatically. Training samples linked to these class labels are collected autonomously via image synthesis and retrieval methods, allowing for prompt learning without manual effort. We utilize a simple cross-entropy loss for prompt optimization, with cross-modal and cross-distribution mixing strategies to reduce image noise and explore the intermediate space between distributions, respectively. The LAPT framework operates autonomously, requiring only ID class names as input and eliminating the need for manual intervention. With extensive experiments, LAPT consistently outperforms manually crafted prompts, setting a new standard for OOD detection. Moreover, LAPT not only enhances the distinction between ID and OOD samples, but also improves the ID classification accuracy and strengthens the generalization robustness to covariate shifts, resulting in outstanding performance in challenging full-spectrum OOD detection tasks. Codes are available at \\url{https://github.com/YBZh/LAPT}.",
        "authors": [
            "Yabin Zhang",
            "Wen-Qing Zhu",
            "Chenhang He",
            "Lei Zhang"
        ],
        "citations": 4,
        "references": 74,
        "year": 2024
    },
    {
        "title": "ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs",
        "abstract": "Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM-only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.",
        "authors": [
            "Irene Huang",
            "Wei Lin",
            "M. J. Mirza",
            "Jacob A. Hansen",
            "Sivan Doveh",
            "V. Butoi",
            "Roei Herzig",
            "Assaf Arbelle",
            "Hilde Kuhene",
            "Trevor Darrel",
            "Chuang Gan",
            "Aude Oliva",
            "Rog√©rio Feris",
            "Leonid Karlinsky"
        ],
        "citations": 4,
        "references": 64,
        "year": 2024
    },
    {
        "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
        "abstract": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
        "authors": [
            "Michael S. Ryoo",
            "Honglu Zhou",
            "Shrikant B. Kendre",
            "Can Qin",
            "Le Xue",
            "Manli Shu",
            "Silvio Savarese",
            "Ran Xu",
            "Caiming Xiong",
            "Juan Carlos Niebles"
        ],
        "citations": 4,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Retrieval-Augmented Open-Vocabulary Object Detection",
        "abstract": "Open-vocabulary object detection (OVD) has been stud-ied with Vision-Language Models (VLMs) to detect novel objects beyond the pre-trained categories. Previous ap-proaches improve the generalization ability to expand the knowledge of the detector, using ‚Äòpositive‚Äô pseudo-labels with additional ‚Äòclass' names, e.g., sock, iPod, and alli-gator. To extend the previous methods in two aspects, we propose Retrieval-Augmented Losses and visual Features (RALF). Our method retrieves related ‚Äònegative‚Äô classes and augments loss functions. Also, visual features are aug-mented with ‚Äòverbalized concepts' of classes, e.g., worn on the feet, handheld music player, and sharp teeth. Specif-ically, RALF consists of two modules: Retrieval Aug-mented Losses (RAL) and Retrieval-Augmented visual Fea-tures (RAF). RAL constitutes two losses reflecting the se-mantic similarity with negative vocabularies. In addition, RAF augments visual features with the verbalized con-cepts from a large language model (LLM). Our experiments demonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. We achieve improvement up to 3.4 box APN50 on novel categories of the COCO dataset and 3.6 mask APr gains on the LVIS dataset. Code is available at https://github.com/mlvlab/RALF.",
        "authors": [
            "Jooyeon Kim",
            "Eulrang Cho",
            "Sehyung Kim",
            "Hyunwoo J. Kim"
        ],
        "citations": 4,
        "references": 38,
        "year": 2024
    },
    {
        "title": "MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs",
        "abstract": "While Vision-Language Models (VLMs) hold promise for tasks requiring extensive collaboration, traditional multi-agent simulators have facilitated rich explorations of an interactive artificial society that reflects collective behavior. However, these existing simulators face significant limitations. Firstly, they struggle with handling large numbers of agents due to high resource demands. Secondly, they often assume agents possess perfect information and limitless capabilities, hindering the ecological validity of simulated social interactions. To bridge this gap, we propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing three key features: large-scale scalability, limited multimodal senses, and physical needs. Our simulator supports 64 or more agents. Agents have limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. Additionally, we further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior.The source code of MineLand and Alex is openly available at https://github.com/cocacola-lab/MineLand.",
        "authors": [
            "Xianhao Yu",
            "Jiaqi Fu",
            "Renjia Deng",
            "Wenjuan Han"
        ],
        "citations": 4,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning",
        "abstract": "We propose a generalized method for boosting the generalization ability of pre-trained vision-language models (VLMs) while fine-tuning on downstream few-shot tasks. The idea is realized by exploiting out-of-distribution (OOD) detection to predict whether a sample belongs to a base distribution or a novel distribution and then using the score generated by a dedicated competition based scoring function to fuse the zero-shot and few-shot classifier. The fused classifier is dynamic, which will bias towards the zero-shot classifier if a sample is more likely from the distribution pre-trained on, leading to improved base-to-novel generalization ability. Our method is performed only in test stage, which is applicable to boost existing methods without time-consuming re-training. Extensive experiments show that even weak distribution detectors can still improve VLMs' generalization ability. Specifically, with the help of OOD detectors, the harmonic mean of CoOp and ProGrad increase by 2.6 and 1.5 percentage points over 11 recognition datasets in the base-to-novel setting.",
        "authors": [
            "Kun Ding",
            "Haojian Zhang",
            "Qiang Yu",
            "Ying Wang",
            "Shiming Xiang",
            "Chunhong Pan"
        ],
        "citations": 4,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Affordance-Guided Reinforcement Learning via Visual Prompting",
        "abstract": "Robots equipped with reinforcement learning (RL) have the potential to learn a wide range of skills solely from a reward signal. However, obtaining a robust and dense reward signal for general manipulation tasks remains a challenge. Existing learning-based approaches require significant data, such as human demonstrations of success and failure, to learn task-specific reward functions. Recently, there is also a growing adoption of large multi-modal foundation models for robotics that can perform visual reasoning in physical contexts and generate coarse robot motions for manipulation tasks. Motivated by this range of capability, in this work, we present Keypoint-based Affordance Guidance for Improvements (KAGI), a method leveraging rewards shaped by vision-language models (VLMs) for autonomous RL. State-of-the-art VLMs have demonstrated impressive reasoning about affordances through keypoints in zero-shot, and we use these to define dense rewards that guide autonomous robotic learning. On real-world manipulation tasks specified by natural language descriptions, KAGI improves the sample efficiency of autonomous RL and enables successful task completion in 20K online fine-tuning steps. Additionally, we demonstrate the robustness of KAGI to reductions in the number of in-domain demonstrations used for pre-training, reaching similar performance in 35K online fine-tuning steps. Project website: https://sites.google.com/view/affordance-guided-rl",
        "authors": [
            "Olivia Y. Lee",
            "Annie Xie",
            "Kuan Fang",
            "Karl Pertsch",
            "Chelsea Finn"
        ],
        "citations": 4,
        "references": 71,
        "year": 2024
    },
    {
        "title": "Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment",
        "abstract": "Existing image-text modality alignment in Vision Language Models (VLMs) treats each text token equally in an autoregressive manner. Despite being simple and effective, this method results in sub-optimal cross-modal alignment by over-emphasizing the text tokens that are less correlated with or even contradictory with the input images. In this paper, we advocate for assigning distinct contributions for each text token based on its visual correlation. Specifically, we present by contrasting image inputs, the difference in prediction logits on each text token provides strong guidance of visual correlation. We therefore introduce Contrastive ALignment (CAL), a simple yet effective re-weighting strategy that prioritizes training visually correlated tokens. Our experimental results demonstrate that CAL consistently improves different types of VLMs across different resolutions and model sizes on various benchmark datasets. Importantly, our method incurs minimal additional computational overhead, rendering it highly efficient compared to alternative data scaling strategies. Codes are available at https://github.com/foundation-multimodal-models/CAL.",
        "authors": [
            "Xin Xiao",
            "Bohong Wu",
            "Jiacong Wang",
            "Chunyuan Li",
            "Xun Zhou",
            "Haoyuan Guo"
        ],
        "citations": 4,
        "references": 76,
        "year": 2024
    },
    {
        "title": "BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models",
        "abstract": "Vision language models (VLMs) perceive the world through a combination of a visual encoder and a large language model (LLM). The visual encoder, pre-trained on large-scale vision-text datasets, provides zero-shot generalization to visual data, and the LLM endows its high reasoning ability to VLMs. It leads VLMs to achieve high performance on wide benchmarks without fine-tuning, exhibiting zero or few-shot capability. However, recent studies show that VLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility, thereby making users unable to fully trust the output from VLMs. To enhance trustworthiness and better tackle the hallucination of VLMs, we curate a new evaluation dataset, called the BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID). Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes. This allows us to clearly assess whether VLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize image-wise object relationship by virtue of our two-axis view: vision and text. Upon evaluating VLMs with our dataset, we observed that our metrics reveal different aspects of VLM hallucination that have not been reported before. Project page: \\url{https://beafbench.github.io/}",
        "authors": [
            "Moon Ye-Bin",
            "Nam Hyeon-Woo",
            "Wonseok Choi",
            "Tae-Hyun Oh"
        ],
        "citations": 4,
        "references": 43,
        "year": 2024
    },
    {
        "title": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture",
        "abstract": "Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision‚Äìlanguage Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively. While LLMs excel at text-based question answering, surpassing human accuracy, the open-sourced VLMs still fall short by 41% on multi-image and 21% on single-image VQA tasks, although closed-weights models perform closer to human levels (within 10%). Our findings highlight that understanding food and its cultural implications remains a challenging and under-explored direction.",
        "authors": [
            "Wenyan Li",
            "Xinyu Zhang",
            "Jiaang Li",
            "Qiwei Peng",
            "Raphael Tang",
            "Li Zhou",
            "Weijia Zhang",
            "Guimin Hu",
            "Yifei Yuan",
            "Anders Sogaard",
            "Daniel Hershcovich",
            "Desmond Elliott"
        ],
        "citations": 4,
        "references": 37,
        "year": 2024
    },
    {
        "title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis",
        "abstract": "We explore multi-step reasoning in vision-language models (VLMs). The problem is challenging, as reasoning data consisting of multiple steps of visual and language processing are barely available. To overcome the challenge, we first introduce a least-to-most visual reasoning paradigm, which interleaves steps of decomposing a question into sub-questions and invoking external tools for resolving sub-questions. Based on the paradigm, we further propose a novel data synthesis approach that can automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complex synthesis task into a few simple sub-tasks, and (almost entirely) relies on open-sourced models to accomplish the sub-tasks. Therefore, the entire synthesis process is reproducible and cost-efficient, and the synthesized data is quality guaranteed. With the approach, we construct 50k visual reasoning examples. Then, we develop a visual reasoner through supervised fine-tuning, which is capable of generally enhancing the reasoning abilities of a wide range of existing VLMs in a plug-and-play fashion. Extensive experiments indicate that the visual reasoner can consistently and significantly improve four VLMs on four VQA benchmarks.",
        "authors": [
            "Chuanqi Cheng",
            "Jian Guan",
            "Wei Wu",
            "Rui Yan"
        ],
        "citations": 4,
        "references": 53,
        "year": 2024
    },
    {
        "title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs",
        "abstract": "Large pre-trained Vision-Language Models (VLMs) have become ubiquitous foundational components of other models and downstream tasks. Although powerful, our empirical results reveal that such models might not be able to identify fine-grained concepts. Specifically, the explanations of VLMs with respect to fine-grained concepts are entangled and mislocalized. To address this issue, we propose to DisEntAngle and Localize (DEAL) the concept-level explanations for VLMs without human annotations. The key idea is encouraging the concept-level explanations to be distinct while maintaining consistency with category-level explanations. We conduct extensive experiments and ablation studies on a wide range of benchmark datasets and vision-language models. Our empirical results demonstrate that the proposed method significantly improves the concept-level explanations of the model in terms of disentanglability and localizability. Surprisingly, the improved explainability alleviates the model's reliance on spurious correlations, which further benefits the prediction accuracy.",
        "authors": [
            "Tang Li",
            "Mengmeng Ma",
            "Xi Peng"
        ],
        "citations": 4,
        "references": 70,
        "year": 2024
    },
    {
        "title": "Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning",
        "abstract": "Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic anomaly detection model applicable across multiple scenarios. To achieve this, we customize generic visual-language foundation models that possess extensive knowledge and robust reasoning abilities into anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers multi-modal prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling multi-modal anomaly detection and reasoning. Our preliminary studies demonstrate that combining visual and language prompts as conditions for customizing the models enhances anomaly detection performance. The customized models showcase the ability to detect anomalies across different data modalities such as images and point clouds. Qualitative case studies further highlight the anomaly detection and reasoning capabilities, particularly for multi-object scenes and temporal data. Our code is available at https://github.com/Xiaohao-Xu/Customizable-VLM.",
        "authors": [
            "Xiaohao Xu",
            "Yunkang Cao",
            "Yongqi Chen",
            "Weiming Shen",
            "Xiaonan Huang"
        ],
        "citations": 4,
        "references": 15,
        "year": 2024
    },
    {
        "title": "SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations",
        "abstract": "Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly in object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We show that all the models which achieve better performance on compositionality datasets need not perform equally well on SUGARCREPE++, signifying that compositionality alone may not be sufficient for understanding semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community.",
        "authors": [
            "Sri Harsha Dumpala",
            "Aman Jaiswal",
            "Chandramouli Sastry",
            "E. Milios",
            "Sageev Oore",
            "Hassan Sajjad"
        ],
        "citations": 4,
        "references": 100,
        "year": 2024
    },
    {
        "title": "GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs",
        "abstract": "The ability to understand and reason about spatial relationships between objects in images is an important component of visual reasoning. This skill rests on the ability to recognize and localize objects of interest and determine their spatial relation. Early vision and language models (VLMs) have been shown to struggle to recognize spatial relations. We extend the previously released What'sUp dataset and propose a novel comprehensive evaluation for spatial relationship understanding that highlights the strengths and weaknesses of 27 different models. In addition to the VLMs evaluated in What'sUp, our extensive evaluation encompasses 3 classes of Multimodal LLMs (MLLMs) that vary in their parameter sizes (ranging from 7B to 110B), training/instruction-tuning methods, and visual resolution to benchmark their performances and scrutinize the scaling laws in this task.",
        "authors": [
            "Navid Rajabi",
            "J. Kosecka"
        ],
        "citations": 4,
        "references": 30,
        "year": 2024
    },
    {
        "title": "Learning to Learn Better Visual Prompts",
        "abstract": "Prompt tuning provides a low-cost way of adapting vision-language models (VLMs) for various downstream vision tasks without requiring updating the huge pre-trained parameters. Dispensing with the conventional manual crafting of prompts, the recent prompt tuning method of Context Optimization (CoOp) introduces adaptable vectors as text prompts. Nevertheless, several previous works point out that the CoOp-based approaches are easy to overfit to the base classes and hard to generalize to novel classes. In this paper, we reckon that the prompt tuning works well only in the base classes because of the limited capacity of the adaptable vectors. The scale of the pre-trained model is hundreds times the scale of the adaptable vector, thus the learned vector has a very limited ability to absorb the knowledge of novel classes. To minimize this excessive overfitting of textual knowledge on the base class, we view prompt tuning as learning to learn (LoL) and learn the prompt in the way of meta-learning, the training manner of dividing the base classes into many different subclasses could fully exert the limited capacity of prompt tuning and thus transfer it power to recognize the novel classes. To be specific, we initially perform fine-tuning on the base class based on the CoOp method for pre-trained CLIP. Subsequently, predicated on the fine-tuned CLIP model, we carry out further fine-tuning in an N-way K-shot manner from the perspective of meta-learning on the base classes. We finally apply the learned textual vector and VLM for unseen classes.Extensive experiments on benchmark datasets validate the efficacy of our meta-learning-informed prompt tuning, affirming its role as a robust optimization strategy for VLMs.",
        "authors": [
            "Fengxiang Wang",
            "Wanrong Huang",
            "Shaowu Yang",
            "Qi Fan",
            "Long Lan"
        ],
        "citations": 4,
        "references": 60,
        "year": 2024
    },
    {
        "title": "OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal Omni-Scale Feature Learning",
        "abstract": "Recent Vision-Language Models (VLMs) \\textit{e.g.} CLIP have made great progress in video recognition. Despite the improvement brought by the strong visual backbone in extracting spatial features, CLIP still falls short in capturing and integrating spatial-temporal features which is essential for video recognition. In this paper, we propose OmniCLIP, a framework that adapts CLIP for video recognition by focusing on learning comprehensive features encompassing spatial, temporal, and dynamic spatial-temporal scales, which we refer to as omni-scale features. This is achieved through the design of spatial-temporal blocks that include parallel temporal adapters (PTA), enabling efficient temporal modeling. Additionally, we introduce a self-prompt generator (SPG) module to capture dynamic object spatial features. The synergy between PTA and SPG allows OmniCLIP to discern varying spatial information across frames and assess object scales over time. We have conducted extensive experiments in supervised video recognition, few-shot video recognition, and zero-shot recognition tasks. The results demonstrate the effectiveness of our method, especially with OmniCLIP achieving a top-1 accuracy of 74.30\\% on HMDB51 in a 16-shot setting, surpassing the recent MotionPrompt approach even with full training data. The code is available at \\url{https://github.com/XiaoBuL/OmniCLIP}.",
        "authors": [
            "Mushui Liu",
            "Bozheng Li",
            "Yunlong Yu"
        ],
        "citations": 4,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
        "abstract": "Do vision-language models (VLMs) pre-trained to caption an image of a\"durian\"learn visual concepts such as\"brown\"(color) and\"spiky\"(texture) at the same time? We aim to answer this question as visual concepts learned\"for free\"would enable wide applications such as neuro-symbolic reasoning or human-interpretable object classification. We assume that the visual concepts, if captured by pre-trained VLMs, can be extracted by their vision-language interface with text-based concept prompts. We observe that recent works prompting VLMs with concepts often differ in their strategies to define and evaluate the visual concepts, leading to conflicting conclusions. We propose a new concept definition strategy based on two observations: First, certain concept prompts include shortcuts that recognize correct concepts for wrong reasons; Second, multimodal information (e.g. visual discriminativeness, and textual knowledge) should be leveraged when selecting the concepts. Our proposed concept discovery and learning (CDL) framework is thus designed to identify a diverse list of generic visual concepts (e.g.\"spiky\"as opposed to\"spiky durian\"), which are ranked and selected based on visual and language mutual information. We carefully design quantitative and human evaluations of the discovered concepts on six diverse visual recognition datasets, which confirm that pre-trained VLMs do learn visual concepts that provide accurate and thorough descriptions for the recognized objects. All code and models are publicly released.",
        "authors": [
            "Yuan Zang",
            "Tian Yun",
            "Hao Tan",
            "Trung Bui",
            "Chen Sun"
        ],
        "citations": 4,
        "references": 40,
        "year": 2024
    },
    {
        "title": "Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model",
        "abstract": "Large Vision-Language Models (LVLMs) rely on vision encoders and Large Language Models (LLMs) to exhibit remarkable capabilities on various multi-modal tasks in the joint space of vision and language. However, typographic attacks, which disrupt Vision-Language Models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), have also been expected to be a security threat to LVLMs. Firstly, we verify typographic attacks on current well-known commercial and open-source LVLMs and uncover the widespread existence of this threat. Secondly, to better assess this vulnerability, we propose the most comprehensive and largest-scale Typographic Dataset to date. The Typographic Dataset not only considers the evaluation of typographic attacks under various multi-modal tasks but also evaluates the effects of typographic attacks, influenced by texts generated with diverse factors. Based on the evaluation results, we investigate the causes why typographic attacks impacting VLMs and LVLMs, leading to three highly insightful discoveries. During the process of further validating the rationality of our discoveries, we can reduce the performance degradation caused by typographic attacks from 42.07\\% to 13.90\\%. Code and Dataset are available in \\href{https://github.com/ChaduCheng/TypoDeceptions}",
        "authors": [
            "Hao Cheng",
            "Erjia Xiao",
            "Jindong Gu",
            "Le Yang",
            "Jinhao Duan",
            "Jize Zhang",
            "Jiahang Cao",
            "Kaidi Xu",
            "Renjing Xu"
        ],
        "citations": 4,
        "references": 71,
        "year": 2024
    },
    {
        "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
        "abstract": "Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 11 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.",
        "authors": [
            "Wenbo Hu",
            "Y. Xu",
            "Y. Li",
            "W. Li",
            "Z. Chen",
            "Z. Tu"
        ],
        "citations": 93,
        "references": 62,
        "year": 2023
    },
    {
        "title": "FLoRA: Enhancing Vision-Language Models with Parameter-Efficient Federated Learning",
        "abstract": "In the rapidly evolving field of artificial intelligence, multimodal models, e.g., integrating vision and language into visual-language models (VLMs), have become pivotal for many applications, ranging from image captioning to multimodal search engines. Among these models, the Contrastive Language-Image Pre-training (CLIP) model has demonstrated remarkable performance in understanding and generating nuanced relationships between text and images. However, the conventional training of such models often requires centralized aggregation of vast datasets, posing significant privacy and data governance challenges. To address these concerns, this paper proposes a novel approach that leverages Federated Learning and parameter-efficient adapters, i.e., Low-Rank Adaptation (LoRA), to train VLMs. This methodology preserves data privacy by training models across decentralized data sources and ensures model adaptability and efficiency through LoRA's parameter-efficient fine-tuning. Our approach accelerates training time by up to 34.72 times and requires 2.47 times less memory usage than full fine-tuning.",
        "authors": [
            "Duy Phuong Nguyen",
            "J. P. Mu√±oz",
            "Ali Jannesari"
        ],
        "citations": 3,
        "references": 43,
        "year": 2024
    },
    {
        "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
        "abstract": "The emergence of Vision Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to produce detailed text descriptions based on visual inputs, yet it introduces new security vulnerabilities. Unlike prior work that centered on single modalities or classification tasks, this study introduces TrojVLM, the first exploration of backdoor attacks aimed at VLMs engaged in complex image-to-text generation. Specifically, TrojVLM inserts predetermined target text into output text when encountering poisoned images. Moreover, a novel semantic preserving loss is proposed to ensure the semantic integrity of the original image content. Our evaluation on image captioning and visual question answering (VQA) tasks confirms the effectiveness of TrojVLM in maintaining original semantic content while triggering specific target text outputs. This study not only uncovers a critical security risk in VLMs and image-to-text generation but also sets a foundation for future research on securing multimodal models against such sophisticated threats.",
        "authors": [
            "Weimin Lyu",
            "Lu Pang",
            "Teng Ma",
            "Haibin Ling",
            "Chao Chen"
        ],
        "citations": 3,
        "references": 47,
        "year": 2024
    },
    {
        "title": "See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding",
        "abstract": "Vision-language models (VLMs) can respond to queries about images in many languages. However, beyond language, culture affects how we see things. For example, individuals from Western cultures focus more on the central figure in an image while individuals from Eastern cultures attend more to scene context. In this work, we present a novel investigation that demonstrates and localizes VLMs' Western bias in image understanding. We evaluate large VLMs across subjective and objective visual tasks with culturally diverse images and annotations. We find that VLMs perform better on the Western subset than the Eastern subset of each task. Controlled experimentation tracing the source of this bias highlights the importance of a diverse language mix in text-only pre-training for building equitable VLMs, even when inference is performed in English. Moreover, while prompting in the language of a target culture can lead to reductions in bias, it is not a substitute for building AI more representative of the world's languages.",
        "authors": [
            "Amith Ananthram",
            "Elias Stengel-Eskin",
            "Carl Vondrick",
            "Mohit Bansal",
            "Kathleen McKeown"
        ],
        "citations": 3,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Veagle: Advancements in Multimodal Representation Learning",
        "abstract": "Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by the successes and insights of previous works. Veagle leverages a dynamic mechanism to project encoded visual information directly into the language model. This dynamic approach allows for a more nuanced understanding of intricate details present in visual contexts. To validate the effectiveness of Veagle, we conduct comprehensive experiments on benchmark datasets, emphasizing tasks such as visual question answering and image understanding. Our results indicate a improvement of 5-6 \\% in performance, with Veagle outperforming existing models by a notable margin. The outcomes underscore the model's versatility and applicability beyond traditional benchmarks.",
        "authors": [
            "Rajat Chawla",
            "Arkajit Datta",
            "Tushar Verma",
            "Adarsh Jha",
            "Anmol Gautam",
            "Ayush Vatsal",
            "Sukrit Chaterjee",
            "NS Mukunda",
            "Ishaan Bhola"
        ],
        "citations": 3,
        "references": 28,
        "year": 2024
    },
    {
        "title": "Emergent Visual-Semantic Hierarchies in Image-Text Representations",
        "abstract": "While recent vision-and-language models (VLMs) like CLIP are a powerful tool for analyzing text and images in a shared semantic space, they do not explicitly model the hierarchical nature of the set of texts which may describe an image. Conversely, existing multimodal hierarchical representation learning methods require costly training from scratch, failing to leverage the knowledge encoded by state-of-the-art multimodal foundation models. In this work, we study the knowledge of existing foundation models, finding that they exhibit emergent understanding of visual-semantic hierarchies despite not being directly trained for this purpose. We propose the Radial Embedding (RE) framework for probing and optimizing hierarchical understanding, and contribute the HierarCaps dataset, a benchmark facilitating the study of hierarchical knowledge in image--text representations, constructed automatically via large language models. Our results show that foundation VLMs exhibit zero-shot hierarchical understanding, surpassing the performance of prior models explicitly designed for this purpose. Furthermore, we show that foundation models may be better aligned to hierarchical reasoning via a text-only fine-tuning phase, while retaining pretraining knowledge.",
        "authors": [
            "Morris Alper",
            "Hadar Averbuch-Elor"
        ],
        "citations": 3,
        "references": 74,
        "year": 2024
    },
    {
        "title": "Multi-Modal Attribute Prompting for Vision-Language Models",
        "abstract": "Pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong generalization ability to downstream tasks but struggle in few-shot scenarios. Existing prompting techniques primarily focus on global text and image representations, yet overlooking multi-modal attribute characteristics. This limitation hinders the model‚Äôs ability to perceive fine-grained visual details and restricts its generalization ability to a broader range of unseen classes. To address this issue, we propose a Multi-modal Attribute Prompting method (MAP) by jointly exploring textual attribute prompting, visual attribute prompting, and attribute-level alignment. The proposed MAP enjoys several merits. First, we introduce learnable visual attribute prompts enhanced by textual attribute semantics to adaptively capture visual attributes for images from unknown categories, boosting fine-grained visual perception capabilities for CLIP. Second, the proposed attribute-level alignment complements the global alignment to enhance the robustness of cross-modal alignment for open-vocabulary objects. To our knowledge, this is the first work to establish cross-modal attribute-level alignment for CLIP-based few-shot adaptation. Extensive experimental results on 11 datasets demonstrate that our method performs favorably against state-of-the-art approaches.",
        "authors": [
            "Xin Liu",
            "Jiamin Wu",
            "Wenfei Yang",
            "Xu Zhou",
            "Tianzhu Zhang"
        ],
        "citations": 3,
        "references": 50,
        "year": 2024
    },
    {
        "title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models",
        "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/",
        "authors": [
            "Hang Hua",
            "Yunlong Tang",
            "Ziyun Zeng",
            "Liangliang Cao",
            "Zhengyuan Yang",
            "Hangfeng He",
            "Chenliang Xu",
            "Jiebo Luo"
        ],
        "citations": 3,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA",
        "abstract": "In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at https://github.com/matthewdm0816/BridgeQA.",
        "authors": [
            "Wentao Mo",
            "Yang Liu"
        ],
        "citations": 3,
        "references": 18,
        "year": 2024
    },
    {
        "title": "Wolf: Captioning Everything with a World Summarization Framework",
        "abstract": "We propose Wolf, a WOrLd summarization Framework for accurate video captioning. Wolf is an automated captioning framework that adopts a mixture-of-experts approach, leveraging complementary strengths of Vision Language Models (VLMs). By utilizing both image and video models, our framework captures different levels of information and summarizes them efficiently. Our approach can be applied to enhance video understanding, auto-labeling, and captioning. To evaluate caption quality, we introduce CapScore, an LLM-based metric to assess the similarity and quality of generated captions compared to the ground truth captions. We further build four human-annotated datasets in three domains: autonomous driving, general scenes, and robotics, to facilitate comprehensive comparisons. We show that Wolf achieves superior captioning performance compared to state-of-the-art approaches from the research community (VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For instance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise by 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally, we establish a benchmark for video captioning and introduce a leaderboard, aiming to accelerate advancements in video understanding, captioning, and data alignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.",
        "authors": [
            "Boyi Li",
            "Ligeng Zhu",
            "Ran Tian",
            "Shuhan Tan",
            "Yuxiao Chen",
            "Yao Lu",
            "Yin Cui",
            "Sushant Veer",
            "Max Ehrlich",
            "Jonah Philion",
            "Xinshuo Weng",
            "Fuzhao Xue",
            "Andrew Tao",
            "Mingqiang Liu",
            "Sanja Fidler",
            "B. Ivanovic",
            "Trevor Darrell",
            "Jitendra Malik",
            "Song Han",
            "Marco Pavone"
        ],
        "citations": 3,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions",
        "abstract": "Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments. More videos can be found at https://ut-austin-rpl.github.io/Harmon/.",
        "authors": [
            "Zhenyu Jiang",
            "Yuqi Xie",
            "Jinhan Li",
            "Ye Yuan",
            "Yifeng Zhu",
            "Yuke Zhu"
        ],
        "citations": 3,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
        "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.",
        "authors": [
            "Vasily Kostumov",
            "Bulat Nutfullin",
            "Oleg Pilipenko",
            "Eugene Ilyushin"
        ],
        "citations": 3,
        "references": 64,
        "year": 2024
    },
    {
        "title": "WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks",
        "abstract": "The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress toward capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus.",
        "authors": [
            "L'eo Boisvert",
            "Megh Thakkar",
            "Maxime Gasse",
            "Massimo Caccia",
            "Thibault Le Sellier De Chezelles",
            "Quentin Cappart",
            "Nicolas Chapados",
            "Alexandre Lacoste",
            "Alexandre Drouin"
        ],
        "citations": 3,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Label Propagation for Zero-shot Classification with Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) have demonstrated im-pressive performance on zero-shot classification, i.e. classi-fication when provided merely with a list of class names. In this paper, we tackle the case of zero-shot classification in the presence of unlabeled data. We leverage the graph structure of the unlabeled data and introduce ZLaP, a method based on label propagation (LP) that utilizes geodesic distances for classification. We tailor LP to graphs containing both text and image features and further pro-pose an efficient method for performing inductive infer-ence based on a dual solution and a sparsification step. We perform extensive experiments to evaluate the effectiveness of our method on 14 common datasets and show that ZLaP outperforms the latest related works. Code: https://github.com/vladan-stojnic/ZLaP",
        "authors": [
            "Vladan Stojni'c",
            "Yannis Kalantidis",
            "Giorgos Tolias"
        ],
        "citations": 3,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Private Attribute Inference from Images with Vision-Language Models",
        "abstract": "As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus. While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that LLMs can make accurate privacy-infringing inferences from previously unseen texts. With the rise of vision-language models (VLMs), capable of understanding both images and text, a key question is whether this concern transfers to the previously unexplored domain of benign images posted online. To answer this question, we compile an image dataset with human-annotated labels of the image owner's personal attributes. In order to understand the privacy risks posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans. On this dataset, we evaluate 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger inferential adversaries, establishing an imperative for the development of adequate defenses.",
        "authors": [
            "Batuhan T√∂mek√ße",
            "Mark Vero",
            "Robin Staab",
            "Martin T. Vechev"
        ],
        "citations": 3,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Long Story Short: Story-level Video Understanding from 20K Short Films",
        "abstract": "Recent developments in vision-language models have significantly advanced video understanding. Existing datasets and tasks, however, have notable limitations. Most datasets are confined to short videos with limited events and narrow narratives. For example, datasets with instructional and egocentric videos often depict activities of one person in a single scene. Although existing movie datasets offer richer content, they are often limited to short-term tasks, lack publicly available videos, and frequently encounter data leakage issues given the use of subtitles and other information about commercial movies during LLM pretraining. To address the above limitations, we propose Short-Films 20K (SF20K), the largest publicly available movie dataset. SF20K is composed of 20,143 amateur films and offers long-term video tasks in the form of multiple-choice and open-ended question answering. Our extensive analysis of SF20K reveals minimal data leakage, emphasizes the need for long-term reasoning, and demonstrates the strong performance of recent VLMs. Finally, we show that instruction tuning on the SF20K-Train set substantially improves model performance, paving the way for future progress in long-term video understanding.",
        "authors": [
            "Ridouane Ghermi",
            "Xi Wang",
            "Vicky Kalogeiton",
            "Ivan Laptev"
        ],
        "citations": 3,
        "references": 86,
        "year": 2024
    },
    {
        "title": "Safety Alignment for Vision Language Models",
        "abstract": "Benefiting from the powerful capabilities of Large Language Models (LLMs), pre-trained visual encoder models connected to an LLMs can realize Vision Language Models (VLMs). However, existing research shows that the visual modality of VLMs is vulnerable, with attackers easily bypassing LLMs' safety alignment through visual modality features to launch attacks. To address this issue, we enhance the existing VLMs' visual modality safety alignment by adding safety modules, including a safety projector, safety tokens, and a safety head, through a two-stage training process, effectively improving the model's defense against risky images. For example, building upon the LLaVA-v1.5 model, we achieve a safety score of 8.26, surpassing the GPT-4V on the Red Teaming Visual Language Models (RTVLM) benchmark. Our method boasts ease of use, high flexibility, and strong controllability, and it enhances safety while having minimal impact on the model's general performance. Moreover, our alignment strategy also uncovers some possible risky content within commonly used open-source multimodal datasets. Our code will be open sourced after the anonymous review.",
        "authors": [
            "Zhendong Liu",
            "Yuanbi Nie",
            "Yingshui Tan",
            "Xiangyu Yue",
            "Qiushi Cui",
            "Chong-Jun Wang",
            "Xiaoyong Zhu",
            "Bo Zheng"
        ],
        "citations": 3,
        "references": 39,
        "year": 2024
    },
    {
        "title": "OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields With Fine-Grained Understanding",
        "abstract": "In recent years, there has been a surge of interest in open-vocabulary 3D scene reconstruction facilitated by visual language models (VLMs), which showcase remarkable capabilities in open-set retrieval tasks. Although the semantic ambiguity of existing point-wise feature maps is alleviated by open-vocabulary mask segmenters for object-level understanding, effectively retaining fine-grained features within objects simultaneously remains challenging. To address these challenges, we introduce OpenObj, an innovative approach to build open-vocabulary object-level Neural Radiance Fields (NeRF) with fine-grained understanding. In essence, OpenObj establishes a robust framework for efficient and watertight scene modeling and comprehension at the object level. Specifically, we obtain cross-frame consistent instance-level masks for supervision through our two-stage mask clustering module. Moreover, by incorporating part-level features into the object NeRF models, OpenObj not only captures object-level instances but also preserves an understanding of their internal granularity. The results on multiple datasets demonstrate that OpenObj achieves superior performance in zero-shot segmentation and retrieval tasks. Additionally, OpenObj supports real-world robotics tasks at several levels, including global movement and local manipulation.",
        "authors": [
            "Yinan Deng",
            "Jiahui Wang",
            "Jingyu Zhao",
            "Jianyu Dou",
            "Yi Yang",
            "Yufeng Yue"
        ],
        "citations": 3,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Revisiting the Adversarial Robustness of Vision Language Models: a Multimodal Perspective",
        "abstract": "Pretrained vision-language models (VLMs) like CLIP exhibit exceptional generalization across diverse downstream tasks. While recent studies reveal their vulnerability to adversarial attacks, research to date has primarily focused on enhancing the robustness of image encoders against image-based attacks, with defenses against text-based and multimodal attacks remaining largely unexplored. To this end, this work presents the first comprehensive study on improving the adversarial robustness of VLMs against attacks targeting image, text, and multimodal inputs. This is achieved by proposing multimodal contrastive adversarial training (MMCoA). Such an approach strengthens the robustness of both image and text encoders by aligning the clean text embeddings with adversarial image embeddings, and adversarial text embeddings with clean image embeddings. The robustness of the proposed MMCoA is examined against existing defense methods over image, text, and multimodal attacks on the CLIP model. Extensive experiments on 15 datasets across two tasks reveal the characteristics of different adversarial defense methods under distinct distribution shifts and dataset complexities across the three attack types. This paves the way for a unified framework of adversarial robustness against different modality attacks, opening up new possibilities for securing VLMs against multimodal attacks. The code is available at https://github.com/ElleZWQ/MMCoA.git.",
        "authors": [
            "Wanqi Zhou",
            "Shuanghao Bai",
            "Qibin Zhao",
            "Badong Chen"
        ],
        "citations": 3,
        "references": 62,
        "year": 2024
    },
    {
        "title": "XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization",
        "abstract": "Utilizing potent representations of the large vision-language models (VLMs) to accomplish various downstream tasks has attracted increasing attention. Within this research field, soft prompt learning has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification. However, most existing prompt learning methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare. To address this issue, we propose a novel explainable prompt learning framework that leverages medical knowledge by aligning the semantics of images, learnable prompts, and clinical concept-driven prompts at multiple granularities. Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from large language models and offers both visual and textual explanations for the prompts. Extensive experiments and explainability analyses conducted on various datasets, with and without concept labels, demonstrate that our method simultaneously achieves superior diagnostic performance, flexibility, and interpretability, shedding light on the effectiveness of foundation models in facilitating XAI. The code will be made publically available.",
        "authors": [
            "Yequan Bie",
            "Luyang Luo",
            "Zhixuan Chen",
            "Hao Chen"
        ],
        "citations": 3,
        "references": 33,
        "year": 2024
    },
    {
        "title": "A Survey of Robot Intelligence with Large Language Models",
        "abstract": "Since the emergence of ChatGPT, research on large language models (LLMs) has actively progressed across various fields. LLMs, pre-trained on vast text datasets, have exhibited exceptional abilities in understanding natural language and planning tasks. These abilities of LLMs are promising in robotics. In general, traditional supervised learning-based robot intelligence systems have a significant lack of adaptability to dynamically changing environments. However, LLMs help a robot intelligence system to improve its generalization ability in dynamic and complex real-world environments. Indeed, findings from ongoing robotics studies indicate that LLMs can significantly improve robots‚Äô behavior planning and execution capabilities. Additionally, vision-language models (VLMs), trained on extensive visual and linguistic data for the vision question answering (VQA) problem, excel at integrating computer vision with natural language processing. VLMs can comprehend visual contexts and execute actions through natural language. They also provide descriptions of scenes in natural language. Several studies have explored the enhancement of robot intelligence using multimodal data, including object recognition and description by VLMs, along with the execution of language-driven commands integrated with visual information. This review paper thoroughly investigates how foundation models such as LLMs and VLMs have been employed to boost robot intelligence. For clarity, the research areas are categorized into five topics: reward design in reinforcement learning, low-level control, high-level planning, manipulation, and scene understanding. This review also summarizes studies that show how foundation models, such as the Eureka model for automating reward function design in reinforcement learning, RT-2 for integrating visual data, language, and robot actions in vision-language-action models, and AutoRT for generating feasible tasks and executing robot behavior policies via LLMs, have improved robot intelligence.",
        "authors": [
            "Hyeongyo Jeong",
            "Haechan Lee",
            "Changwon Kim",
            "Sungtae Shin"
        ],
        "citations": 3,
        "references": 72,
        "year": 2024
    },
    {
        "title": "D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions",
        "abstract": "Large vision language models (VLMs) have progressed incredibly from research to applicability for general-purpose use cases. LLaVA-Med, a pioneering large language and vision assistant for biomedicine, can perform multi-modal biomedical image and data analysis to provide a natural language interface for radiologists. While it is highly generalizable and works with multi-modal data, it is currently limited by well-known challenges that exist in the large language model space. Hallucinations and imprecision in responses can lead to misdiagnosis which currently hinder the clinical adaptability of VLMs. To create precise, user-friendly models in healthcare, we propose D-Rax -- a domain-specific, conversational, radiologic assistance tool that can be used to gain insights about a particular radiologic image. In this study, we enhance the conversational analysis of chest X-ray (CXR) images to support radiological reporting, offering comprehensive insights from medical imaging and aiding in the formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the LLaVA-Med architecture on our curated enhanced instruction-following data, comprising of images, instructions, as well as disease diagnosis and demographic predictions derived from MIMIC-CXR imaging data, CXR-related visual question answer (VQA) pairs, and predictive outcomes from multiple expert AI models. We observe statistically significant improvement in responses when evaluated for both open and close-ended conversations. Leveraging the power of state-of-the-art diagnostic models combined with VLMs, D-Rax empowers clinicians to interact with medical images using natural language, which could potentially streamline their decision-making process, enhance diagnostic accuracy, and conserve their time.",
        "authors": [
            "Hareem Nisar",
            "Syed Muhammad Anwar",
            "Zhifan Jiang",
            "Abhijeet Parida",
            "Vishwesh Nath",
            "Holger Roth",
            "M. Linguraru"
        ],
        "citations": 3,
        "references": 26,
        "year": 2024
    },
    {
        "title": "HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning",
        "abstract": "Hallucination has been a major problem for large language models and remains a critical challenge when it comes to multimodality in which vision-language models (VLMs) have to deal with not just textual but also visual inputs. Despite rapid progress in VLMs, resources for evaluating and addressing multimodal hallucination are limited and mostly focused on evaluation. This work introduces HaloQuest, a novel visual question answering dataset that captures various aspects of multimodal hallucination such as false premises, insufficient contexts, and visual challenges. A novel idea from HaloQuest is to leverage synthetic images, apart from real ones, to enable dataset creation at scale. With over 7.7K examples spanning across a wide variety of categories, HaloQuest was designed to be both a challenging benchmark for VLMs and a fine-tuning dataset for advancing multimodal reasoning. Our experiments reveal that current models struggle with HaloQuest, with all open-source VLMs achieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest significantly reduces hallucination rates while preserving performance on standard reasoning tasks. Our results discover that benchmarking with generated images is highly correlated (r=0.97) with real images. Last but not least, we propose a novel Auto-Eval mechanism that is highly correlated with human raters (r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards understanding, evaluating, and mitigating hallucination in VLMs, serving as an important step towards more reliable multimodal AI systems in the future.",
        "authors": [
            "Zhecan Wang",
            "Garrett Bingham",
            "Adams Yu",
            "Quoc V. Le",
            "Thang Luong",
            "Golnaz Ghiasi"
        ],
        "citations": 3,
        "references": 64,
        "year": 2024
    },
    {
        "title": "No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models",
        "abstract": "We study cultural and socioeconomic diversity in contrastive vision-language models (VLMs). Using a broad range of benchmark datasets and evaluation metrics, we bring to attention several important findings. First, the common filtering of training data to English image-text pairs disadvantages communities of lower socioeconomic status and negatively impacts cultural understanding. Notably, this performance gap is not captured by - and even at odds with - the currently popular evaluation metrics derived from the Western-centric ImageNet and COCO datasets. Second, pretraining with global, unfiltered data before fine-tuning on English content can improve cultural understanding without sacrificing performance on said popular benchmarks. Third, we introduce the task of geo-localization as a novel evaluation metric to assess cultural diversity in VLMs. Our work underscores the value of using diverse data to create more inclusive multimodal systems and lays the groundwork for developing VLMs that better represent global perspectives.",
        "authors": [
            "Angeline Pouget",
            "Lucas Beyer",
            "Emanuele Bugliarello",
            "Xiao Wang",
            "A. Steiner",
            "Xiao-Qi Zhai",
            "Ibrahim M. Alabdulmohsin"
        ],
        "citations": 3,
        "references": 77,
        "year": 2024
    },
    {
        "title": "ReplanVLM: Replanning Robotic Tasks With Visual Language Models",
        "abstract": "Large language models (LLMs) have gained increasing popularity in robotic task planning due to their exceptional abilities in text analytics and generation, as well as their broad knowledge of the world. However, they fall short in decoding visual cues. LLMs have limited direct perception of the world, which leads to a deficient grasp of the current state of the world. By contrast, the emergence of visual language models (VLMs) fills this gap by integrating visual perception modules, which can enhance the autonomy of robotic task planning. Despite these advancements, VLMs still face challenges, such as the potential for task execution errors, even when provided with accurate instructions. To address such issues, this letter proposes a ReplanVLM framework for robotic task planning. In this study, we focus on error correction interventions. An internal error correction mechanism and an external error correction mechanism are presented to correct errors under corresponding phases. A replan strategy is developed to replan tasks or correct error codes when task execution fails. Experimental results on real robots and in simulation environments have demonstrated the superiority of the proposed framework, with higher success rates and robust error correction capabilities in open-world tasks.",
        "authors": [
            "Aoran Mei",
            "Guo-Niu Zhu",
            "Huaxiang Zhang",
            "Zhongxue Gan"
        ],
        "citations": 3,
        "references": 24,
        "year": 2024
    },
    {
        "title": "VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models",
        "abstract": "Visual Language Models (VLMs) have rapidly progressed with the recent success of large language models. However, there have been few attempts to incorporate efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In this study, we introduce VisualRWKV, the first application of a linear RNN model to multimodal learning tasks, leveraging the pre-trained RWKV language model. We propose a data-dependent recurrence and sandwich prompts to enhance our modeling capabilities, along with a 2D image scanning mechanism to enrich the processing of visual sequences. Extensive experiments demonstrate that VisualRWKV achieves competitive performance compared to Transformer-based models like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV has a speed advantage of 3.98 times and can save 54% of GPU memory when reaching an inference length of 24K tokens. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at the following GitHub repository: see https://github.com/howard-hou/VisualRWKV.",
        "authors": [
            "Haowen Hou",
            "Peigen Zeng",
            "Fei Ma",
            "F. Yu"
        ],
        "citations": 3,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models",
        "abstract": "Vision-Language Large Models (VLMs) recently become primary backbone of AI, due to the impressive performance. However, their expensive computation costs, i.e., throughput and delay, impede potentials in the real-world scenarios. To achieve acceleration for VLMs, most existing methods focus on the model perspective: pruning, distillation, quantization, but completely overlook the data-perspective redundancy. To fill the overlook, this paper pioneers the severity of data redundancy, and designs one plug-and-play Turbo module guided by information degree to prune inefficient tokens from visual or textual data. In pursuit of efficiency-performance trade-offs, information degree takes two crucial factors into consideration: mutual redundancy and semantic value. Concretely, the former evaluates data duplication between sequential tokens; while the latter evaluates each token by its contribution to the overall semantics. As a result, tokens with high information degree carry less redundancy and stronger semantics. For VLMs' calculation, Turbo works as a user-friendly plug-in that sorts data referring to information degree, utilizing only top-level ones to save costs. Its advantages are multifaceted, e.g., being generally compatible to various VLMs across understanding and generation, simple use without re-training and trivial engineering efforts. On multiple VLMs benchmarks, we fully experiment to demonstrate the good acceleration of Turbo, under negligible performance drop.",
        "authors": [
            "Chen Ju",
            "Haicheng Wang",
            "Haozhe Cheng",
            "Xu Chen",
            "Zhonghua Zhai",
            "Weilin Huang",
            "Jinsong Lan",
            "Shuai Xiao",
            "Bo Zheng"
        ],
        "citations": 3,
        "references": 69,
        "year": 2024
    },
    {
        "title": "Zero shot VLMs for hate meme detection: Are we there yet?",
        "abstract": "Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we observe that large VLMs are still vulnerable for zero-shot hate meme detection.",
        "authors": [
            "Naquee Rizwan",
            "Paramananda Bhaskar",
            "Mithun Das",
            "Swadhin Satyaprakash Majhi",
            "Punyajoy Saha",
            "Animesh Mukherjee"
        ],
        "citations": 3,
        "references": 26,
        "year": 2024
    },
    {
        "title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
        "abstract": "Vision language models (VLMs) are an exciting emerging class of language models (LMs) that have merged classic LM capabilities with those of image processing systems. However, the ways that these capabilities combine are not always intuitive and warrant direct investigation. One understudied capability in VLMs is visual spatial planning -- the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates the spatial planning capability in these models in general, and 2) breaks down the visual planning task into finer-grained sub-tasks, including perception and reasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation shows that both open-source and private VLMs fail to generate effective plans for even simple spatial planning tasks. Evaluations on the fine-grained analytical tasks further reveal fundamental deficiencies in the models' visual perception and bottlenecks in reasoning abilities, explaining their worse performance in the general spatial planning tasks. Our work illuminates future directions for improving VLMs' abilities in spatial planning. Our benchmark is publicly available at https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.",
        "authors": [
            "Qiucheng Wu",
            "Handong Zhao",
            "Michael Stephen Saxon",
            "T. Bui",
            "William Yang Wang",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "citations": 3,
        "references": 66,
        "year": 2024
    },
    {
        "title": "Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models",
        "abstract": "Large-scale vision-language models (VLMs) have shown a strong zero-shot generalization capability on unseen-domain data. However, adapting pre-trained VLMs to a sequence of downstream tasks often leads to the forgetting of previously learned knowledge and a reduction in zero-shot classification performance. To tackle this problem, we propose a unique Selective Dual-Teacher Knowledge Transfer framework that leverages the most recent fine-tuned and the original pre-trained VLMs as dual teachers to preserve the previously learned knowledge and zero-shot capabilities, respectively. With only access to an unlabeled reference dataset, our proposed framework performs a selective knowledge distillation mechanism by measuring the feature discrepancy from the dual-teacher VLMs. Consequently, our selective dual-teacher knowledge distillation mitigates catastrophic forgetting of previously learned knowledge while preserving the zero-shot capabilities of pre-trained VLMs. Extensive experiments on benchmark datasets demonstrate that our framework is favorable against state-of-the-art continual learning approaches for preventing catastrophic forgetting and zero-shot degradation. Project page: https://chuyu.org/research/snd",
        "authors": [
            "Yu-Chu Yu",
            "Chi-Pin Huang",
            "Jr-Jen Chen",
            "Kai-Po Chang",
            "Yung-Hsuan Lai",
            "Fu-En Yang",
            "Yu-Chiang Frank Wang"
        ],
        "citations": 3,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case",
        "abstract": "Recently, large language model (LLM)-based agents have made significant advances across various fields. One of the most popular research areas involves applying these agents to video games. Traditionally, these methods have relied on game APIs to access in-game environmental and action data. However, this approach is limited by the availability of APIs and does not reflect how humans play games. With the advent of vision language models (VLMs), agents now have enhanced visual understanding capabilities, enabling them to interact with games using only visual inputs. Despite these advances, current approaches still face challenges in action-oriented tasks, particularly in action role-playing games (ARPGs), where reinforcement learning methods are prevalent but suffer from poor generalization and require extensive training. To address these limitations, we select an ARPG, ``Black Myth: Wukong'', as a research platform to explore the capability boundaries of existing VLMs in scenarios requiring visual-only input and complex action output. We define 12 tasks within the game, with 75% focusing on combat, and incorporate several state-of-the-art VLMs into this benchmark. Additionally, we will release a human operation dataset containing recorded gameplay videos and operation logs, including mouse and keyboard actions. Moreover, we propose a novel VARP (Vision Action Role-Playing) agent framework, consisting of an action planning system and a visual trajectory system. Our framework demonstrates the ability to perform basic tasks and succeed in 90% of easy and medium-level combat scenarios. This research aims to provide new insights and directions for applying multimodal agents in complex action game environments. The code and datasets will be made available at https://varp-agent.github.io/.",
        "authors": [
            "Peng Chen",
            "Pi Bu",
            "Jun Song",
            "Yuan Gao",
            "Bo Zheng"
        ],
        "citations": 3,
        "references": 22,
        "year": 2024
    },
    {
        "title": "Open-Vocabulary Spatio-Temporal Action Detection",
        "abstract": "Spatio-temporal action detection (STAD) is an important fine-grained video understanding task. Current methods require box and label supervision for all action classes in advance. However, in real-world applications, it is very likely to come across new action classes not seen in training because the action category space is large and hard to enumerate. Also, the cost of data annotation and model training for new classes is extremely high for traditional methods, as we need to perform detailed box annotations and re-train the whole network from scratch. In this paper, we propose a new challenging setting by performing open-vocabulary STAD to better mimic the situation of action detection in an open world. Open-vocabulary spatio-temporal action detection (OV-STAD) requires training a model on a limited set of base classes with box and label supervision, which is expected to yield good generalization performance on novel action classes. For OV-STAD, we build two benchmarks based on the existing STAD datasets and propose a simple but effective method based on pretrained video-language models (VLM). To better adapt the holistic VLM for the fine-grained action detection task, we carefully fine-tune it on the localized video region-text pairs. This customized fine-tuning endows the VLM with better motion understanding, thus contributing to a more accurate alignment between video regions and texts. Local region feature and global video feature fusion before alignment is adopted to further improve the action detection performance by providing global context. Our method achieves a promising performance on novel classes.",
        "authors": [
            "Tao Wu",
            "Shuqiu Ge",
            "Jie Qin",
            "Gangshan Wu",
            "Limin Wang"
        ],
        "citations": 3,
        "references": 67,
        "year": 2024
    },
    {
        "title": "VHELM: A Holistic Evaluation of Vision Language Models",
        "abstract": "Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website (https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.",
        "authors": [
            "Tony Lee",
            "Haoqin Tu",
            "Chi Heem Wong",
            "Wenhao Zheng",
            "Yiyang Zhou",
            "Yifan Mai",
            "Josselin Somerville Roberts",
            "Michihiro Yasunaga",
            "Huaxiu Yao",
            "Cihang Xie",
            "Percy Liang"
        ],
        "citations": 3,
        "references": 49,
        "year": 2024
    },
    {
        "title": "A Design of Interface for Visual-Impaired People to Access Visual Information from Images Featuring Large Language Models and Visual Language Models",
        "abstract": "We propose a design of interface for visual-impaired People to access visual information from images utilizing Large Language Models(LLMs), Visual Language Models (VLMs), and Segment-Anything. We use Semantic-Segment-Anything to generate the segmentation of semantic objects in images. The segmentation includes two parts: a term set describing the semantic object, and segmented mask which represents the shape of the semantic object. We provide two methods for the visual-impaired user to access the information of the semantic object and its peripheral information in image. In one method, the LLM summarize the term set to create an description. In the other method, the image with the object masked is provided to Visual Language Models which is prompted to respond with a description. In both methods, the mask can be accessed with dot display after processed for the visual-impaired people to access, and the description is prompted to the user in synthesized voice.",
        "authors": [
            "Zhe-Xin Zhang"
        ],
        "citations": 3,
        "references": 4,
        "year": 2024
    },
    {
        "title": "Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification",
        "abstract": "Remote-sensing fine-grained ship classification (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pretrained vision-language models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multigranularity prompt design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the model‚Äôs generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available.",
        "authors": [
            "Long Lan",
            "Fengxiang Wang",
            "Xiangtao Zheng",
            "Zengmao Wang",
            "Xinwang Liu"
        ],
        "citations": 3,
        "references": 48,
        "year": 2024
    },
    {
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "abstract": "Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.",
        "authors": [
            "Zhiyong Wu",
            "Zhenyu Wu",
            "Fangzhi Xu",
            "Yian Wang",
            "Qiushi Sun",
            "Chengyou Jia",
            "Kanzhi Cheng",
            "Zichen Ding",
            "Liheng Chen",
            "Paul Pu Liang",
            "Yu Qiao"
        ],
        "citations": 3,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Hyperbolic Learning with Synthetic Captions for Open-World Detection",
        "abstract": "Open-world detection poses significant challenges, as it requires the detection of any object using either object class labels or free-form texts. Existing related works often use large-scale manual annotated caption datasets for training, which are extremely expensive to collect. Instead, we propose to transfer knowledge from vision-language models (VLMs) to enrich the open-vocabulary descriptions au-tomatically. Specifically, we bootstrap dense synthetic captions using pretrained VLMs to provide rich descriptions on different regions in images, and incorporate these captions to train a novel detector that generalizes to novel concepts. To mitigate the noise caused by hallucination in syn-thetic captions, we also propose a novel hyperbolic vision-language learning approach to impose a hierarchy between visual and caption embeddings. We call our detector ‚ÄúHy-perLearner‚Äù. We conduct extensive experiments on a wide variety of open-world detection benchmarks (COCO, LVIS, Object Detection in the Wild, RefCoCo) and our results show that our model consistently outperforms existing state-of-the-art methods, such as GLIP, GLIPv2 and Grounding DINO, when using the same backbone.",
        "authors": [
            "Fanjie Kong",
            "Yanbei Chen",
            "Jiarui Cai",
            "Davide Modolo"
        ],
        "citations": 3,
        "references": 71,
        "year": 2024
    },
    {
        "title": "AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation",
        "abstract": "Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks. However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes. To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space. AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module. We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales.",
        "authors": [
            "Yuhan Zhu",
            "Yuyang Ji",
            "Zhiyu Zhao",
            "Gangshan Wu",
            "Limin Wang"
        ],
        "citations": 3,
        "references": 136,
        "year": 2024
    },
    {
        "title": "PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization",
        "abstract": "Domain Generalization (DG) aims to resolve distribution shifts between source and target domains, and current DG methods are default to the setting that data from source and target domains share identical categories. Nevertheless, there exists unseen classes from target domains in practical scenarios. To address this issue, Open Set Domain Generalization (OSDG) has emerged and several methods have been exclusively proposed. However, most existing methods adopt complex architectures with slight improvement compared with DG methods. Recently, vision-language models (VLMs) have been introduced in DG following the fine-tuning paradigm, but consume huge training overhead with large vision models. Therefore, in this paper, we innovate to transfer knowledge from VLMs to lightweight vision models and improve the robustness by introducing Perturbation Distillation (PD) from three perspectives, including Score, Class and Instance (SCI), named SCI-PD. Moreover, previous methods are oriented by the benchmarks with identical and fixed splits, ignoring the divergence between source domains. These methods are revealed to suffer from sharp performance decay with our proposed new benchmark Hybrid Domain Generalization (HDG) and a novel metric H2-CV, which construct various splits to comprehensively assess the robustness of algorithms. Extensive experiments demonstrate that our method outperforms state-of-the-art algorithms on multiple datasets, especially improving the robustness when confronting data scarcity.",
        "authors": [
            "Zining Chen",
            "Weiqiu Wang",
            "Zhicheng Zhao",
            "Fei Su",
            "Aidong Men",
            "Hongying Meng"
        ],
        "citations": 3,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Toward Automatic Relevance Judgment using Vision-Language Models for Image-Text Retrieval Evaluation",
        "abstract": "Vision--Language Models (VLMs) have demonstrated success across diverse applications, yet their potential to assist in relevance judgments remains uncertain. This paper assesses the relevance estimation capabilities of VLMs, including CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc} retrieval task tailored for multimedia content creation in a zero-shot fashion. Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V, encompassing open-source and closed-source visual-instruction-tuned Large Language Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared to human relevance judgments, surpassing the CLIPScore metric. (2) While CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based retrieval systems. (3) GPT-4V's score distribution aligns more closely with human judgments than other models, achieving a Cohen's $\\kappa$ value of around 0.08, which outperforms CLIPScore at approximately -0.096. These findings underscore the potential of LLM-powered VLMs in enhancing relevance judgments.",
        "authors": [
            "Jheng-Hong Yang",
            "Jimmy Lin"
        ],
        "citations": 3,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?",
        "abstract": "Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by\"receiving\"feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures. We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal. We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs. Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, we find that this issue can be mitigated via a binary verification mechanism. Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated. Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism. The project website is hosted at https://andrewliao11.github.io/vlms_feedback",
        "authors": [
            "Yuan-Hong Liao",
            "Rafid Mahmood",
            "Sanja Fidler",
            "David Acuna"
        ],
        "citations": 3,
        "references": 56,
        "year": 2024
    },
    {
        "title": "MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation",
        "abstract": "Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination. Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies. Despite the development of several meme generation tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies. Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements. MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention. Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism is embedded to curb hateful meme production. Our assessment, focusing on two UN Sustainable Development Goals-Climate Action and Gender Equality-shows MemeCraft's prowess in creating memes that are both funny and supportive of advocacy goals. This paper highlights how generative AI can promote social good and pioneers the use of LLMs and VLMs in meme generation.",
        "authors": [
            "Han Wang",
            "Roy Ka-Wei Lee"
        ],
        "citations": 3,
        "references": 50,
        "year": 2024
    },
    {
        "title": "PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration",
        "abstract": "Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model to generate captions for these images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models.",
        "authors": [
            "Yuxuan Sun",
            "Yunlong Zhang",
            "Yixuan Si",
            "Chenglu Zhu",
            "Zhongyi Shui",
            "Kai Zhang",
            "Jingxiong Li",
            "Xingheng Lyu",
            "Tao Lin",
            "Lin Yang"
        ],
        "citations": 3,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments",
        "abstract": "In this work, we tackle the limitations of current LiDAR-based 3D object detection systems, which are hindered by a restricted class vocabulary and the high costs associated with annotating new object classes. Our exploration of open-vocabulary (OV) learning in urban environments aims to capture novel instances using pre-trained vision-language models (VLMs) with multi-sensor data. We design and benchmark a set of four potential solutions as baselines, categorizing them into either top-down or bottom-up approaches based on their input data strategies. While effective, these methods exhibit certain limitations, such as missing novel objects in 3D box estimation or applying rigorous priors, leading to biases towards objects near the camera or of rectangular geometries. To overcome these limitations, we introduce a universal \\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the recall of novel objects and propagating this detection capability to more distant areas thereby progressively capturing more. In particular, we utilize a greedy box seeker to search against 3D novel boxes of varying orientations and depth in each generated frustum and ensure the reliability of newly identified boxes by cross alignment and density ranker. Additionally, the inherent bias towards camera-proximal objects is alleviated by the proposed remote simulator, which randomly diversifies pseudo-labeled novel instances in the self-training process, combined with the fusion of base samples in the memory bank. Extensive experiments demonstrate a 53% improvement in novel recall across diverse OV settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold increase in Average Precision (AP) for novel object classes. The source code is made available at https://github.com/djamahl99/findnpropagate.",
        "authors": [
            "Djamahl Etchegaray",
            "Zi Huang",
            "Tatsuya Harada",
            "Yadan Luo"
        ],
        "citations": 3,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Vision-Language Foundation Models as Effective Robot Imitators",
        "abstract": "Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.",
        "authors": [
            "Xinghang Li",
            "Minghuan Liu",
            "Hanbo Zhang",
            "Cunjun Yu",
            "Jie Xu",
            "Hongtao Wu",
            "Chi-Hou Cheang",
            "Ya Jing",
            "Weinan Zhang",
            "Huaping Liu",
            "Hang Li",
            "Tao Kong"
        ],
        "citations": 80,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
        "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
        "authors": [
            "Erfan Shayegani",
            "Yue Dong",
            "Nael B. Abu-Ghazaleh"
        ],
        "citations": 80,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
        "abstract": "In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.",
        "authors": [
            "Yingdong Hu",
            "Fanqi Lin",
            "Tong Zhang",
            "Li Yi",
            "Yang Gao"
        ],
        "citations": 74,
        "references": 97,
        "year": 2023
    },
    {
        "title": "Cobra Effect in Reference-Free Image Captioning Metrics",
        "abstract": "Evaluating the compatibility between textual descriptions and corresponding images represents a core endeavor within multi-modal research. In recent years, a proliferation of reference-free methods, leveraging visual-language pre-trained models (VLMs), has emerged. Empirical evidence has substantiated that these innovative approaches exhibit a higher correlation with human judgment, marking a significant advancement in the field. However, does a higher correlation with human evaluations alone sufficiently denote the complete of a metric? In response to this question, in this paper, we study if there are any deficiencies in reference-free metrics. Specifically, inspired by the Cobra Effect, we utilize metric scores as rewards to direct the captioning model toward generating descriptions that closely align with the metric's criteria. If a certain metric has flaws, it will be exploited by the model and reflected in the generated sentences. Our findings reveal that descriptions guided by these metrics contain significant flaws, e.g. incoherent statements and excessive repetition. Subsequently, we propose a novel method termed Self-Improving to rectify the identified shortcomings within these metrics. We employ GPT-4V as an evaluative tool to assess generated sentences and the result reveals that our approach achieves state-of-the-art (SOTA) performance. In addition, we also introduce a challenging evaluation benchmark called Flaws Caption to evaluate reference-free image captioning metrics comprehensively. Our code is available at https://github.com/aaronma2020/robust_captioning_metric",
        "authors": [
            "Zheng Ma",
            "Changxin Wang",
            "Yawen Ouyang",
            "Fei Zhao",
            "Jianbing Zhang",
            "Shujian Huang",
            "Jiajun Chen"
        ],
        "citations": 2,
        "references": 35,
        "year": 2024
    },
    {
        "title": "The Minimum Information about CLinical Artificial Intelligence Checklist for Generative Modeling Research (MI-CLAIM-GEN)",
        "abstract": "Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data (\"zero-\"or\"few-shot\"approaches), as well as the open-ended nature of their outputs, necessitate the development of new guidelines for robust reporting of clinical generative model research. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the original MI-CLAIM checklist. The new checklist, MI-CLAIM-GEN (Table 1), aims to address differences in training, evaluation, interpretability, and reproducibility of new generative models compared to non-generative (\"predictive\") AI models. This MI-CLAIM-GEN checklist also seeks to clarify cohort selection reporting with unstructured clinical data and adds additional items on alignment with ethical standards for clinical AI research.",
        "authors": [
            "Brenda Y Miao",
            "Irene Y. Chen",
            "Christopher Y K Williams",
            "Jays√≥n M. Davidson",
            "Augusto Garcia-Agundez",
            "Harry Sun",
            "T. Zack",
            "A. Butte",
            "Madhumita Sushil"
        ],
        "citations": 2,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Direct Preference Optimization for Suppressing Hallucinated Prior Exams in Radiology Report Generation",
        "abstract": "Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm. Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations. We apply our method to the prevention of hallucinations of prior exams, addressing a long-established problem behavior in models performing chest X-ray report generation. Across our experiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics. Our work is, to the best of our knowledge, the first work to apply DPO to medical VLMs, providing a data- and compute- efficient way to suppress problem behaviors while maintaining overall clinical accuracy.",
        "authors": [
            "Oishi Banerjee",
            "Hong-Yu Zhou",
            "Subathra Adithan",
            "Stephen Kwak",
            "Kay Wu",
            "P. Rajpurkar"
        ],
        "citations": 2,
        "references": 29,
        "year": 2024
    },
    {
        "title": "Multimodal Foundation Models for Medical Imaging - A Systematic Review and Implementation Guidelines",
        "abstract": "Advancements in artificial intelligence (AI) offer promising solutions for enhancing clinical workflows and patient care, potentially revolutionizing healthcare delivery. However, the traditional paradigm of AI integration in healthcare is limited by models that rely on single input modalities during training and require extensive labeled data, failing to capture the multimodal nature of medical practice. Multimodal foundation models, particularly Large Vision Language Models (VLMs), have the potential to overcome these limitations by processing diverse data types and learning from large-scale unlabeled datasets or natural pairs of different modalities, thereby significantly contributing to the development of more robust and versatile AI systems in healthcare. In this review, we establish a unified terminology for multimodal foundation models for medical imaging applications and provide a systematic analysis of papers published between 2012 and 2024. In total, we screened 1,144 papers from medical and AI domains and extracted data from 97 included studies. Our comprehensive effort aggregates the collective knowledge of prior work, evaluates the current state of multimodal AI in healthcare, and delineates both prevailing limitations and potential growth areas. We provide implementation guidelines and actionable recommendations for various stakeholders, including model developers, clinicians, policymakers, and dataset curators.",
        "authors": [
            "S.-C. Huang",
            "M. E. Jensen",
            "S. Yeung-Levy",
            "M. Lungren",
            "H. Poon",
            "A. Chaudhari"
        ],
        "citations": 2,
        "references": 0,
        "year": 2024
    },
    {
        "title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning",
        "abstract": "With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, fine-tuning large pre-trained models has recently become a prevalent strategy in Continual Learning. This has led to the development of numerous prompting strategies to adapt transformer-based models without incurring catastrophic forgetting. However, these strategies often compromise the original zero-shot capabilities of the pre-trained CLIP model and struggle to adapt to domains that significantly deviate from the pre-training data. In this work, we propose Continual Generative training for Incremental prompt-Learning, a simple and novel approach to mitigate forgetting while adapting CLIP. Briefly, we employ Variational Autoencoders (VAEs) to learn class-conditioned distributions within the embedding space of the visual encoder. We then exploit these distributions to sample new synthetic visual embeddings and train the corresponding class-specific textual prompts during subsequent tasks. Through extensive experiments on different domains, we show that such a generative replay approach can adapt to new tasks while improving zero-shot capabilities, evaluated using a novel metric tailored for CL scenarios. Notably, further analysis reveals that our approach can bridge the gap with joint prompt tuning. The codebase is available at https://github.com/aimagelab/mammoth.",
        "authors": [
            "Emanuele Frascaroli",
            "Aniello Panariello",
            "Pietro Buzzega",
            "Lorenzo Bonicelli",
            "Angelo Porrello",
            "Simone Calderara"
        ],
        "citations": 2,
        "references": 55,
        "year": 2024
    },
    {
        "title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks",
        "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite its importance and practicality. In this work, we aim to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets covering both in-distribution and out-of-distribution tasks, and (2) VLM2Vec (Vision-Language Model ->Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, which encodes text or images independently without any task instruction, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. We build a series of VLM2Vec models on SoTA VLMs like Phi-3.5-V, LLaVA-1.6 and evaluate them on MMEB's evaluation split. Our results show that VLM2Vec achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB. We show that VLMs are secretly strong embedding models.",
        "authors": [
            "Ziyan Jiang",
            "Rui Meng",
            "Xinyi Yang",
            "Semih Yavuz",
            "Yingbo Zhou",
            "Wenhu Chen"
        ],
        "citations": 2,
        "references": 81,
        "year": 2024
    },
    {
        "title": "Towards Open-World Grasping with Large Vision-Language Models",
        "abstract": "The ability to grasp objects in-the-wild from open-ended language instructions constitutes a fundamental challenge in robotics. An open-world grasping system should be able to combine high-level contextual with low-level physical-geometric reasoning in order to be applicable in arbitrary scenarios. Recent works exploit the web-scale knowledge inherent in large language models (LLMs) to plan and reason in robotic context, but rely on external vision and action models to ground such knowledge into the environment and parameterize actuation. This setup suffers from two major bottlenecks: a) the LLM's reasoning capacity is constrained by the quality of visual grounding, and b) LLMs do not contain low-level spatial understanding of the world, which is essential for grasping in contact-rich scenarios. In this work we demonstrate that modern vision-language models (VLMs) are capable of tackling such limitations, as they are implicitly grounded and can jointly reason about semantics and geometry. We propose OWG, an open-world grasping pipeline that combines VLMs with segmentation and grasp synthesis models to unlock grounded world understanding in three stages: open-ended referring segmentation, grounded grasp planning and grasp ranking via contact reasoning, all of which can be applied zero-shot via suitable visual prompting mechanisms. We conduct extensive evaluation in cluttered indoor scene datasets to showcase OWG's robustness in grounding from open-ended language, as well as open-world robotic grasping experiments in both simulation and hardware that demonstrate superior performance compared to previous supervised and zero-shot LLM-based methods. Project material is available at https://gtziafas.github.io/OWG_project/ .",
        "authors": [
            "Georgios Tziafas",
            "H. Kasaei"
        ],
        "citations": 2,
        "references": 77,
        "year": 2024
    },
    {
        "title": "DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection",
        "abstract": "Vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot capabilities for various downstream tasks. Their performance can be further enhanced through few-shot prompt tuning methods. However, current studies evaluate the performance of learned prompts separately on base and new classes. This evaluation lacks practicality for real-world applications since downstream tasks cannot determine whether the data belongs to base or new classes in advance. In this paper, we explore a problem setting called Open-world Prompt Tuning (OPT), which involves tuning prompts on base classes and evaluating on a combination of base and new classes. By introducing Decomposed Prompt Tuning framework (DePT), we theoretically demonstrate that OPT can be solved by incorporating out-of-distribution detection into prompt tuning, thereby enhancing the base-to-new discriminability. Based on DePT, we present a novel prompt tuning approach, namely, Decomposed Context Optimization (DeCoOp), which introduces new-class detectors and sub-classifiers to further enhance the base-class and new-class discriminability. Experimental results on 11 benchmark datasets validate the effectiveness of DePT and demonstrate that DeCoOp outperforms current state-of-the-art methods, providing a significant 2% average accuracy improvement.",
        "authors": [
            "Zhi Zhou",
            "Ming Yang",
            "Jiang-Xin Shi",
            "Lan-Zhe Guo",
            "Yu-Feng Li"
        ],
        "citations": 2,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing",
        "abstract": "This study aims to comprehensively review and empirically evaluate the application of multimodal large language models (MLLMs) and Large Vision Models (VLMs) in object detection for transportation systems. In the first fold, we provide a background about the potential benefits of MLLMs in transportation applications and conduct a comprehensive review of current MLLM technologies in previous studies. We highlight their effectiveness and limitations in object detection within various transportation scenarios. The second fold involves providing an overview of the taxonomy of end-to-end object detection in transportation applications and future directions. Building on this, we proposed empirical analysis for testing MLLMs on three real-world transportation problems that include object detection tasks namely, road safety attributes extraction, safety-critical event detection, and visual reasoning of thermal images. Our findings provide a detailed assessment of MLLM performance, uncovering both strengths and areas for improvement. Finally, we discuss practical limitations and challenges of MLLMs in enhancing object detection in transportation, thereby offering a roadmap for future research and development in this critical area.",
        "authors": [
            "Huthaifa I. Ashqar",
            "Ahmed Jaber",
            "Taqwa I. Alhadidi",
            "Mohammed Elhenawy"
        ],
        "citations": 2,
        "references": 82,
        "year": 2024
    },
    {
        "title": "Multi-modal Situated Reasoning in 3D Scenes",
        "abstract": "Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.",
        "authors": [
            "Xiongkun Linghu",
            "Jiangyong Huang",
            "Xuesong Niu",
            "Xiaojian Ma",
            "Baoxiong Jia",
            "Siyuan Huang"
        ],
        "citations": 2,
        "references": 73,
        "year": 2024
    },
    {
        "title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines",
        "abstract": "Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.",
        "authors": [
            "Genta Indra Winata",
            "Frederikus Hudi",
            "Patrick Amadeus Irawan",
            "David Anugraha",
            "Rifki Afina Putri",
            "Yutong Wang",
            "Adam Nohejl",
            "Ubaidillah Ariq Prathama",
            "N. Ousidhoum",
            "Afifa Amriani",
            "Anar Rzayev",
            "Anirban Das",
            "Ashmari Pramodya",
            "Aulia Adila",
            "Bryan Wilie",
            "C. Mawalim",
            "Ching Lam Cheng",
            "D. Abolade",
            "Emmanuele Chersoni",
            "Enrico Santus",
            "Fariz Ikhwantri",
            "Garry Kuwanto",
            "Hanyang Zhao",
            "Haryo Akbarianto Wibowo",
            "Holy Lovenia",
            "Jan Christian Blaise Cruz",
            "Jan Wira Gotama Putra",
            "Junho Myung",
            "Lucky Susanto",
            "Maria Angelica Riera Machin",
            "Marina Zhukova",
            "Michael Anugraha",
            "Muhammad Farid Adilazuarda",
            "Natasha Santosa",
            "Peerat Limkonchotiwat",
            "Raj Dabre",
            "Rio Alexander Audino",
            "Samuel Cahyawijaya",
            "Shi-Xiong Zhang",
            "Stephanie Yulia Salim",
            "Yi Zhou",
            "Yinxuan Gui",
            "D. Adelani",
            "En-Shiun Annie Lee",
            "Shogo Okada",
            "Ayu Purwarianti",
            "Alham Fikri Aji",
            "Taro Watanabe",
            "Derry Tanti Wijaya",
            "Alice Oh",
            "Chong-Wah Ngo"
        ],
        "citations": 2,
        "references": 52,
        "year": 2024
    },
    {
        "title": "MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices",
        "abstract": "The attainment of autonomous operations in mobile computing devices has consistently been a goal of human pursuit. With the development of Large Language Models (LLMs) and Visual Language Models (VLMs), this aspiration is progressively turning into reality. While contemporary research has explored automation of simple tasks on mobile devices via VLMs, there remains significant room for improvement in handling complex tasks and reducing high reasoning costs. In this paper, we introduce MobileExperts, which for the first time introduces tool formulation and multi-agent collaboration to address the aforementioned challenges. More specifically, MobileExperts dynamically assembles teams based on the alignment of agent portraits with the human requirements. Following this, each agent embarks on an independent exploration phase, formulating its tools to evolve into an expert. Lastly, we develop a dual-layer planning mechanism to establish coordinate collaboration among experts. To validate our effectiveness, we design a new benchmark of hierarchical intelligence levels, offering insights into algorithm's capability to address tasks across a spectrum of complexity. Experimental results demonstrate that MobileExperts performs better on all intelligence levels and achieves ~ 22% reduction in reasoning costs, thus verifying the superiority of our design.",
        "authors": [
            "Jiayi Zhang",
            "Chuang Zhao",
            "Yihan Zhao",
            "Zhaoyang Yu",
            "Ming He",
            "Jianpin Fan"
        ],
        "citations": 2,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data",
        "abstract": "Fine-tuning vision-language models (VLMs) with abundant unlabeled data recently has attracted increasing attention. Existing methods that resort to the pseudolabeling strategy would suffer from heavily incorrect hard pseudolabels when VLMs exhibit low zero-shot performance in downstream tasks. To alleviate this issue, we propose a Candidate Pseudolabel Learning method, termed CPL, to fine-tune VLMs with suitable candidate pseudolabels of unlabeled data in downstream tasks. The core of our method lies in the generation strategy of candidate pseudolabels, which progressively generates refined candidate pseudolabels by both intra- and inter-instance label selection, based on a confidence score matrix for all unlabeled data. This strategy can result in better performance in true label inclusion and class-balanced instance selection. In this way, we can directly apply existing loss functions to learn with generated candidate psueudolabels. Extensive experiments on nine benchmark datasets with three learning paradigms demonstrate the effectiveness of our method. Our code can be found at https://github.com/vanillaer/CPL-ICML2024.",
        "authors": [
            "Jiahan Zhang",
            "Qi Wei",
            "Feng Liu",
            "Lei Feng"
        ],
        "citations": 2,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Task Vectors are Cross-Modal",
        "abstract": "We investigate the internal representations of vision-and-language models (VLMs) and how they encode task representations. We consider tasks specified through examples or instructions, using either text or image inputs. Surprisingly, we find that conceptually similar tasks are mapped to similar task vector representations, regardless of how they are specified. Our findings suggest that to output answers, tokens in VLMs undergo three distinct phases: input, task, and answer, a process which is consistent across different modalities and specifications. The task vectors we identify in VLMs are general enough to be derived in one modality (e.g., text) and transferred to another (e.g., image). Additionally, we find that ensembling exemplar and instruction based task vectors produce better task representations. Taken together, these insights shed light on the underlying mechanisms of VLMs, particularly their ability to represent tasks in a shared manner across different modalities and task specifications. Project page: https://task-vectors-are-cross-modal.github.io.",
        "authors": [
            "Grace Luo",
            "Trevor Darrell",
            "Amir Bar"
        ],
        "citations": 2,
        "references": 43,
        "year": 2024
    },
    {
        "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
        "abstract": "In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced.",
        "authors": [
            "Bocheng Zou",
            "Mu Cai",
            "Jianrui Zhang",
            "Yong Jae Lee"
        ],
        "citations": 2,
        "references": 43,
        "year": 2024
    },
    {
        "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
        "abstract": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.",
        "authors": [
            "Nick Jiang",
            "Anish Kachinthaya",
            "Suzie Petryk",
            "Yossi Gandelsman"
        ],
        "citations": 2,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector",
        "abstract": "Visual Language Models (VLMs) are vulnerable to adversarial attacks, especially those from adversarial images, which is however under-explored in literature. To facilitate research on this critical safety problem, we first construct a new laRge-scale Adervsarial images dataset with Diverse hArmful Responses (RADAR), given that existing datasets are either small-scale or only contain limited types of harmful responses. With the new RADAR dataset, we further develop a novel and effective iN-time Embedding-based AdveRSarial Image DEtection (NEARSIDE) method, which exploits a single vector that distilled from the hidden states of VLMs, which we call the attacking direction, to achieve the detection of adversarial images against benign ones in the input. Extensive experiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the effectiveness, efficiency, and cross-model transferrability of our proposed method. Our code is available at https://github.com/mob-scu/RADAR-NEARSIDE",
        "authors": [
            "Youcheng Huang",
            "Fengbin Zhu",
            "Jingkun Tang",
            "Pan Zhou",
            "Wenqiang Lei",
            "Jiancheng Lv",
            "Tat-seng Chua"
        ],
        "citations": 2,
        "references": 54,
        "year": 2024
    },
    {
        "title": "D\\'ej\\`a Vu Memorization in Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\\'ej\\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\\'ej\\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.",
        "authors": [
            "Bargav Jayaraman",
            "Chuan Guo",
            "Kamalika Chaudhuri"
        ],
        "citations": 2,
        "references": 43,
        "year": 2024
    },
    {
        "title": "Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector",
        "abstract": "Visual Language Models (VLMs) are vulnerable to adversarial attacks, especially those from adversarial images, which is however under-explored in literature. To facilitate research on this critical safety problem, we first construct a new laRge-scale Adervsarial images dataset with Diverse hArmful Responses (RADAR), given that existing datasets are either small-scale or only contain limited types of harmful responses. With the new RADAR dataset, we further develop a novel and effective iN-time Embedding-based AdveRSarial Image DEtection (NEARSIDE) method, which exploits a single vector that distilled from the hidden states of VLMs, which we call the attacking direction, to achieve the detection of adversarial images against benign ones in the input. Extensive experiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the effectiveness, efficiency, and cross-model transferrability of our proposed method. Our code is available at https://github.com/mob-scu/RADAR-NEARSIDE",
        "authors": [
            "Youcheng Huang",
            "Fengbin Zhu",
            "Jingkun Tang",
            "Pan Zhou",
            "Wenqiang Lei",
            "Jiancheng Lv",
            "Tat-seng Chua"
        ],
        "citations": 2,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
        "abstract": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.",
        "authors": [
            "Nick Jiang",
            "Anish Kachinthaya",
            "Suzie Petryk",
            "Yossi Gandelsman"
        ],
        "citations": 2,
        "references": 50,
        "year": 2024
    },
    {
        "title": "Enhancing Vision-Language Few-Shot Adaptation with Negative Learning",
        "abstract": "Large-scale pre-trained Vision-Language Models (VLMs) have exhibited impressive zero-shot performance and transferability, allowing them to adapt to downstream tasks in a data-efficient manner. However, when only a few labeled samples are available, adapting VLMs to distinguish subtle differences between similar classes in specific downstream tasks remains challenging. In this work, we propose a Simple yet effective Negative Learning approach, SimNL, to more efficiently exploit the task-specific knowledge from few-shot labeled samples. Unlike previous methods that focus on identifying a set of representative positive features defining\"what is a {CLASS}\", SimNL discovers a complementary set of negative features that define\"what is not a {CLASS}\", providing additional insights that supplement the positive features to enhance task-specific recognition capability. Further, we identify that current adaptation approaches are particularly vulnerable to potential noise in the few-shot sample set. To mitigate this issue, we introduce a plug-and-play few-shot instance reweighting technique to suppress noisy outliers and amplify clean samples for more stable adaptation. Our extensive experimental results across 15 datasets validate that the proposed SimNL outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency. Code is available at https://github.com/zhangce01/SimNL.",
        "authors": [
            "Ce Zhang",
            "Simon Stepputtis",
            "Katia P. Sycara",
            "Yaqi Xie"
        ],
        "citations": 2,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Boosting Vision-Language Models with Transduction",
        "abstract": "Transduction is a powerful paradigm that leverages the structure of unlabeled data to boost predictive accuracy. We present TransCLIP, a novel and computationally efficient transductive approach designed for Vision-Language Models (VLMs). TransCLIP is applicable as a plug-and-play module on top of popular inductive zero- and few-shot models, consistently improving their performances. Our new objective function can be viewed as a regularized maximum-likelihood estimation, constrained by a KL divergence penalty that integrates the text-encoder knowledge and guides the transductive learning process. We further derive an iterative Block Majorize-Minimize (BMM) procedure for optimizing our objective, with guaranteed convergence and decoupled sample-assignment updates, yielding computationally efficient transduction for large-scale datasets. We report comprehensive evaluations, comparisons, and ablation studies that demonstrate: (i) Transduction can greatly enhance the generalization capabilities of inductive pretrained zero- and few-shot VLMs; (ii) TransCLIP substantially outperforms standard transductive few-shot learning methods relying solely on vision features, notably due to the KL-based language constraint.",
        "authors": [
            "Maxime Zanella",
            "Benoit G'erin",
            "Ismail Ben Ayed"
        ],
        "citations": 2,
        "references": 73,
        "year": 2024
    },
    {
        "title": "Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization",
        "abstract": "The vocabulary size in temporal action localization (TAL) is limited by the scarcity of large-scale annotated datasets. To overcome this, recent works integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL (OV-TAL). However, despite the success of VLMs trained on extensive datasets, existing OV-TAL methods still rely on human-labeled TAL datasets of limited size to train action localizers, limiting their generalizability. In this paper, we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic action localizer is trained on a human-labeled TAL dataset to generate pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled dataset is then used to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally, we identify limitations in existing OV-TAL evaluation schemes and propose a new benchmark for thorough assessment. Finally, we showcase the TAL performance of the large multimodal model Gemini-1.5 on our new benchmark. Code is released at https://github.com/HYUNJS/STOV-TAL.",
        "authors": [
            "Jeongseok Hyun",
            "Su Ho Han",
            "Hyolim Kang",
            "Joon-Young Lee",
            "Seon Joo Kim"
        ],
        "citations": 2,
        "references": 60,
        "year": 2024
    },
    {
        "title": "Task Vectors are Cross-Modal",
        "abstract": "We investigate the internal representations of vision-and-language models (VLMs) and how they encode task representations. We consider tasks specified through examples or instructions, using either text or image inputs. Surprisingly, we find that conceptually similar tasks are mapped to similar task vector representations, regardless of how they are specified. Our findings suggest that to output answers, tokens in VLMs undergo three distinct phases: input, task, and answer, a process which is consistent across different modalities and specifications. The task vectors we identify in VLMs are general enough to be derived in one modality (e.g., text) and transferred to another (e.g., image). Additionally, we find that ensembling exemplar and instruction based task vectors produce better task representations. Taken together, these insights shed light on the underlying mechanisms of VLMs, particularly their ability to represent tasks in a shared manner across different modalities and task specifications. Project page: https://task-vectors-are-cross-modal.github.io.",
        "authors": [
            "Grace Luo",
            "Trevor Darrell",
            "Amir Bar"
        ],
        "citations": 2,
        "references": 43,
        "year": 2024
    },
    {
        "title": "CLIPArTT: Adaptation of CLIP to New Domains at Test Time",
        "abstract": "Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate remarkable adaptability across zero-shot classification tasks without additional training. However, their performance diminishes in the presence of domain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision. Our method employs a unique, minimally invasive text prompt tuning process, wherein multiple predicted classes are aggregated into a single new text prompt, used as \\emph{pseudo label} to re-classify inputs in a transductive manner. Additionally, we pioneer the standardization of TTA benchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that, without requiring additional transformations nor new trainable modules, CLIPArTT enhances performance dynamically across non-corrupted datasets such as CIFAR-100, corrupted datasets like CIFAR-100-C and ImageNet-C, alongside synthetic datasets such as VisDA-C. This research underscores the potential for improving VLMs' adaptability through novel test-time strategies, offering insights for robust performance across varied datasets and environments. The code can be found at: https://github.com/dosowiechi/CLIPArTT.git",
        "authors": [
            "G. A. V. Hakim",
            "David Osowiechi",
            "Mehrdad Noori",
            "Milad Cheraghalikhani",
            "Ali Bahri",
            "Moslem Yazdanpanah",
            "Ismail Ben Ayed",
            "Christian Desrosiers"
        ],
        "citations": 2,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss",
        "abstract": "The fusion of vision and language has brought about a transformative shift in computer vision through the emergence of Vision-Language Models (VLMs). However, the resource-intensive nature of existing VLMs poses a significant challenge. We need an accessible method for developing the next generation of VLMs. To address this issue, we propose Zoom-shot, a novel method for transferring the zero-shot capabilities of CLIP to any pre-trained vision encoder. We do this by exploiting the multimodal information (i.e. text and image) present in the CLIP latent space through the use of specifically designed multimodal loss functions. These loss functions are (1) cycle-consistency loss and (2) our novel prompt-guided knowledge distillation loss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's zero-shot classification, to capture the interactions between text and image features. With our multimodal losses, we train a $\\textbf{linear mapping}$ between the CLIP latent space and the latent space of a pre-trained vision encoder, for only a $\\textbf{single epoch}$. Furthermore, Zoom-shot is entirely unsupervised and is trained using $\\textbf{unpaired}$ data. We test the zero-shot capabilities of a range of vision encoders augmented as new VLMs, on coarse and fine-grained classification datasets, outperforming the previous state-of-the-art in this problem domain. In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; and our state-of-the-art results can be obtained by reducing training from 20% to 1% of the ImageNet training data with 20 epochs. All code and models are available on GitHub.",
        "authors": [
            "Jordan Shipard",
            "Arnold Wiliem",
            "Kien Nguyen Thanh",
            "Wei Xiang",
            "C. Fookes"
        ],
        "citations": 2,
        "references": 47,
        "year": 2024
    },
    {
        "title": "Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything",
        "abstract": "Large Visual Language Model\\textbfs (VLMs) such as GPT-4V have achieved remarkable success in generating comprehensive and nuanced responses. Researchers have proposed various benchmarks for evaluating the capabilities of VLMs. With the integration of visual and text inputs in VLMs, new security issues emerge, as malicious attackers can exploit multiple modalities to achieve their objectives. This has led to increasing attention on the vulnerabilities of VLMs to jailbreak. Most existing research focuses on generating adversarial images or nonsensical image to jailbreak these models. However, no researchers evaluate whether logic understanding capabilities of VLMs in flowchart can influence jailbreak. Therefore, to fill this gap, this paper first introduces a novel dataset Flow-JD specifically designed to evaluate the logic-based flowchart jailbreak capabilities of VLMs. We conduct an extensive evaluation on GPT-4o, GPT-4V, other 5 SOTA open source VLMs and the jailbreak rate is up to 92.8%. Our research reveals significant vulnerabilities in current VLMs concerning image-to-text jailbreak and these findings underscore the the urgency for the development of robust and effective future defenses.",
        "authors": [
            "Xiaotian Zou",
            "Yongkang Chen"
        ],
        "citations": 2,
        "references": 34,
        "year": 2024
    },
    {
        "title": "Multimodal Foundation Models for Medical Imaging - A Systematic Review and Implementation Guidelines",
        "abstract": "Advancements in artificial intelligence (AI) offer promising solutions for enhancing clinical workflows and patient care, potentially revolutionizing healthcare delivery. However, the traditional paradigm of AI integration in healthcare is limited by models that rely on single input modalities during training and require extensive labeled data, failing to capture the multimodal nature of medical practice. Multimodal foundation models, particularly Large Vision Language Models (VLMs), have the potential to overcome these limitations by processing diverse data types and learning from large-scale unlabeled datasets or natural pairs of different modalities, thereby significantly contributing to the development of more robust and versatile AI systems in healthcare. In this review, we establish a unified terminology for multimodal foundation models for medical imaging applications and provide a systematic analysis of papers published between 2012 and 2024. In total, we screened 1,144 papers from medical and AI domains and extracted data from 97 included studies. Our comprehensive effort aggregates the collective knowledge of prior work, evaluates the current state of multimodal AI in healthcare, and delineates both prevailing limitations and potential growth areas. We provide implementation guidelines and actionable recommendations for various stakeholders, including model developers, clinicians, policymakers, and dataset curators.",
        "authors": [
            "S.-C. Huang",
            "M. E. Jensen",
            "S. Yeung-Levy",
            "M. Lungren",
            "H. Poon",
            "A. Chaudhari"
        ],
        "citations": 2,
        "references": 0,
        "year": 2024
    },
    {
        "title": "CityLLaVA: Efficient Fine-Tuning for VLMs in City Scenario",
        "abstract": "In the vast and dynamic landscape of urban settings, Traffic Safety Description and Analysis plays a pivotal role in applications ranging from insurance inspection to accident prevention. This paper introduces CityLLaVA, a novel fine-tuning framework for Visual Language Models (VLMs) designed for urban scenarios. CityLLaVA enhances model comprehension and prediction accuracy through (1) employing bounding boxes for optimal visual data preprocessing, including video best-view selection and visual prompt engineering during both training and testing phases; (2) constructing concise Question-Answer sequences and designing textual prompts to refine instruction comprehension; (3) implementing block expansion to finetune large VLMs efficiently; and (4) advancing prediction accuracy via a unique sequential questioning-based prediction augmentation. Demonstrating top-tier performance, our method achieved a benchmark score of 33.4308, securing the leading position on the leaderboard. The code will be released soon.",
        "authors": [
            "Zhizhao Duan",
            "Hao Cheng",
            "Duo Xu",
            "Xi Wu",
            "Xiangxie Zhang",
            "Xi Ye",
            "Zhen Xie"
        ],
        "citations": 2,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Image Analysis in Autonomous Vehicles: A Review of the Latest AI Solutions and Their Comparison",
        "abstract": "The integration of advanced image analysis using artificial intelligence (AI) is pivotal for the evolution of autonomous vehicles (AVs). This article provides a thorough review of the most significant datasets and latest state-of-the-art AI solutions employed in image analysis for AVs. Datasets such as Cityscapes, NuScenes, CARLA, and Talk2Car form the benchmarks for training and evaluating different AI models, with unique characteristics catering to various aspects of autonomous driving. Key AI methodologies, including Convolutional Neural Networks (CNNs), Transformer models, Generative Adversarial Networks (GANs), and Vision Language Models (VLMs), are discussed. The article also presents a comparative analysis of various AI techniques in real-world scenarios, focusing on semantic image segmentation, 3D object detection, vehicle control in virtual environments, and vehicle interaction using natural language. Simultaneously, the roles of multisensor datasets and simulation platforms like AirSim, TORCS, and SUMMIT in enriching the training data and testing environments for AVs are highlighted. By synthesizing information on datasets, AI solutions, and comparative performance evaluations, this article serves as a crucial resource for researchers, developers, and industry stakeholders, offering a clear view of the current landscape and future directions in autonomous vehicle image analysis technologies.",
        "authors": [
            "Micha≈Ç Koz≈Çowski",
            "S. Racewicz",
            "S≈Çawomir Wierzbicki"
        ],
        "citations": 2,
        "references": 74,
        "year": 2024
    },
    {
        "title": "NVILA: Efficient Frontier Visual Language Models",
        "abstract": "Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This\"scale-then-compress\"approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.",
        "authors": [
            "Zhijian Liu",
            "Ligeng Zhu",
            "Baifeng Shi",
            "Zhuoyang Zhang",
            "Yuming Lou",
            "Shang Yang",
            "Haocheng Xi",
            "Shiyi Cao",
            "Yuxian Gu",
            "Dacheng Li",
            "Xiuyu Li",
            "Yunhao Fang",
            "Yukang Chen",
            "Cheng-Yu Hsieh",
            "De-An Huang",
            "An-Chieh Cheng",
            "Vishwesh Nath",
            "Jinyi Hu",
            "Sifei Liu",
            "Ranjay Krishna",
            "Daguang Xu",
            "Xiaolong Wang",
            "Pavlo Molchanov",
            "Jan Kautz",
            "Hongxu Yin",
            "Song Han",
            "Yao Lu"
        ],
        "citations": 2,
        "references": 0,
        "year": 2024
    },
    {
        "title": "HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation",
        "abstract": "Enabling robots to autonomously perform hybrid motions in diverse environments can be beneficial for long-horizon tasks such as material handling, household chores, and work assistance. This requires extensive exploitation of intrinsic motion capabilities, extraction of affordances from rich environmental information, and planning of physical interaction behaviors. Despite recent progress has demonstrated impressive humanoid whole-body control abilities, they struggle to achieve versatility and adaptability for new tasks. In this work, we propose HYPERmotion, a framework that learns, selects and plans behaviors based on tasks in different scenarios. We combine reinforcement learning with whole-body optimization to generate motion for 38 actuated joints and create a motion library to store the learned skills. We apply the planning and reasoning features of the large language models (LLMs) to complex loco-manipulation tasks, constructing a hierarchical task graph that comprises a series of primitive behaviors to bridge lower-level execution with higher-level planning. By leveraging the interaction of distilled spatial geometry and 2D observation with a visual language model (VLM) to ground knowledge into a robotic morphology selector to choose appropriate actions in single- or dual-arm, legged or wheeled locomotion. Experiments in simulation and real-world show that learned motions can efficiently adapt to new tasks, demonstrating high autonomy from free-text commands in unstructured scenes. Videos and website: hy-motion.github.io/",
        "authors": [
            "Jin Wang",
            "Rui Dai",
            "Weijie Wang",
            "Luca Rossini",
            "Francesco Ruscelli",
            "Nikos Tsagarakis"
        ],
        "citations": 2,
        "references": 70,
        "year": 2024
    },
    {
        "title": "VLDadaptor: Domain Adaptive Object Detection With Vision-Language Model Distillation",
        "abstract": "Domain adaptive object detection (DAOD) aims to develop a detector trained on labeled source domains to identify objects in unlabeled target domains. A primary challenge in DAOD is the domain shift problem. Most existing methods learn domain-invariant features within single domain embedding space, often resulting in heavy model biases due to the intrinsic data properties of source domains. To mitigate the model biases, this paper proposes VLDadaptor, a domain adaptive object detector based on vision-language models (VLMs) distillation. Firstly, the proposed method integrates domain-mixed contrastive knowledge distillation between the visual encoder of CLIP and the detector by transferring category-level instance features, which guarantees the detector can extract domain-invariant visual instance features across domains. Then, VLDadaptor employs domain-mixed consistency distillation between the text encoder of CLIP and detector by aligning text prompt embeddings with visual instance features, which helps to maintain the category-level feature consistency among the detector, text encoder and the visual encoder of VLMs. Finally, the proposed method further promotes the adaptation ability by adopting a prompt-based memory bank to generate semantic-complete features for graph matching. These contributions enable VLDadaptor to extract visual features into the visual-language embedding space without any evident model bias towards specific domains. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance on Pascal VOC to Clipart adaptation tasks and exhibits high accuracy on driving scenario tasks with significantly less training time.",
        "authors": [
            "Junjie Ke",
            "Lihuo He",
            "Bo Han",
            "Jie Li",
            "Di Wang",
            "Xinbo Gao"
        ],
        "citations": 2,
        "references": 74,
        "year": 2024
    },
    {
        "title": "DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection",
        "abstract": "Vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot capabilities for various downstream tasks. Their performance can be further enhanced through few-shot prompt tuning methods. However, current studies evaluate the performance of learned prompts separately on base and new classes. This evaluation lacks practicality for real-world applications since downstream tasks cannot determine whether the data belongs to base or new classes in advance. In this paper, we explore a problem setting called Open-world Prompt Tuning (OPT), which involves tuning prompts on base classes and evaluating on a combination of base and new classes. By introducing Decomposed Prompt Tuning framework (DePT), we theoretically demonstrate that OPT can be solved by incorporating out-of-distribution detection into prompt tuning, thereby enhancing the base-to-new discriminability. Based on DePT, we present a novel prompt tuning approach, namely, Decomposed Context Optimization (DeCoOp), which introduces new-class detectors and sub-classifiers to further enhance the base-class and new-class discriminability. Experimental results on 11 benchmark datasets validate the effectiveness of DePT and demonstrate that DeCoOp outperforms current state-of-the-art methods, providing a significant 2% average accuracy improvement.",
        "authors": [
            "Zhi Zhou",
            "Ming Yang",
            "Jiang-Xin Shi",
            "Lan-Zhe Guo",
            "Yu-Feng Li"
        ],
        "citations": 2,
        "references": 57,
        "year": 2024
    },
    {
        "title": "Direct Preference Optimization for Suppressing Hallucinated Prior Exams in Radiology Report Generation",
        "abstract": "Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm. Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations. We apply our method to the prevention of hallucinations of prior exams, addressing a long-established problem behavior in models performing chest X-ray report generation. Across our experiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics. Our work is, to the best of our knowledge, the first work to apply DPO to medical VLMs, providing a data- and compute- efficient way to suppress problem behaviors while maintaining overall clinical accuracy.",
        "authors": [
            "Oishi Banerjee",
            "Hong-Yu Zhou",
            "Subathra Adithan",
            "Stephen Kwak",
            "Kay Wu",
            "P. Rajpurkar"
        ],
        "citations": 2,
        "references": 29,
        "year": 2024
    },
    {
        "title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings",
        "abstract": "Currently, inspired by the success of vision-language models (VLMs), an increasing number of researchers are focusing on improving VLMs and have achieved promising results. However, most existing methods concentrate on optimizing the connector and enhancing the language model component, while neglecting improvements to the vision encoder itself. In contrast, we propose Text Guided LLaVA (TG-LLaVA) in this paper, which optimizes VLMs by guiding the vision encoder with text, offering a new and orthogonal optimization direction. Specifically, inspired by the purpose-driven logic inherent in human behavior, we use learnable latent embeddings as a bridge to analyze textual instruction and add the analysis results to the vision encoder as guidance, refining it. Subsequently, another set of latent embeddings extracts additional detailed text-guided information from high-resolution local patches as auxiliary information. Finally, with the guidance of text, the vision encoder can extract text-related features, similar to how humans focus on the most relevant parts of an image when considering a question. This results in generating better answers. Experiments on various datasets validate the effectiveness of the proposed method. Remarkably, without the need for additional training data, our propsoed method can bring more benefits to the baseline (LLaVA-1.5) compared with other concurrent methods. Furthermore, the proposed method consistently brings improvement in different settings.",
        "authors": [
            "Dawei Yan",
            "Pengcheng Li",
            "Yang Li",
            "Hao Chen",
            "Qingguo Chen",
            "Weihua Luo",
            "Wei Dong",
            "Qingsen Yan",
            "Haokui Zhang",
            "Chunhua Shen"
        ],
        "citations": 2,
        "references": 44,
        "year": 2024
    },
    {
        "title": "Towards Open-World Grasping with Large Vision-Language Models",
        "abstract": "The ability to grasp objects in-the-wild from open-ended language instructions constitutes a fundamental challenge in robotics. An open-world grasping system should be able to combine high-level contextual with low-level physical-geometric reasoning in order to be applicable in arbitrary scenarios. Recent works exploit the web-scale knowledge inherent in large language models (LLMs) to plan and reason in robotic context, but rely on external vision and action models to ground such knowledge into the environment and parameterize actuation. This setup suffers from two major bottlenecks: a) the LLM's reasoning capacity is constrained by the quality of visual grounding, and b) LLMs do not contain low-level spatial understanding of the world, which is essential for grasping in contact-rich scenarios. In this work we demonstrate that modern vision-language models (VLMs) are capable of tackling such limitations, as they are implicitly grounded and can jointly reason about semantics and geometry. We propose OWG, an open-world grasping pipeline that combines VLMs with segmentation and grasp synthesis models to unlock grounded world understanding in three stages: open-ended referring segmentation, grounded grasp planning and grasp ranking via contact reasoning, all of which can be applied zero-shot via suitable visual prompting mechanisms. We conduct extensive evaluation in cluttered indoor scene datasets to showcase OWG's robustness in grounding from open-ended language, as well as open-world robotic grasping experiments in both simulation and hardware that demonstrate superior performance compared to previous supervised and zero-shot LLM-based methods. Project material is available at https://gtziafas.github.io/OWG_project/ .",
        "authors": [
            "Georgios Tziafas",
            "H. Kasaei"
        ],
        "citations": 2,
        "references": 77,
        "year": 2024
    },
    {
        "title": "Multi-agent Planning using Visual Language Models",
        "abstract": "Large Language Models (LLMs) and Visual Language Models (VLMs) are attracting increasing interest due to their improving performance and applications across various domains and tasks. However, LLMs and VLMs can produce erroneous results, especially when a deep understanding of the problem domain is required. For instance, when planning and perception are needed simultaneously, these models often struggle because of difficulties in merging multi-modal information. To address this issue, fine-tuned models are typically employed and trained on specialized data structures representing the environment. This approach has limited effectiveness, as it can overly complicate the context for processing. In this paper, we propose a multi-agent architecture for embodied task planning that operates without the need for specific data structures as input. Instead, it uses a single image of the environment, handling free-form domains by leveraging commonsense knowledge. We also introduce a novel, fully automatic evaluation procedure, PG2S, designed to better assess the quality of a plan. We validated our approach using the widely recognized ALFRED dataset, comparing PG2S to the existing KAS metric to further evaluate the quality of the generated plans.",
        "authors": [
            "Michele Brienza",
            "F. Argenziano",
            "Vincenzo Suriani",
            "D. Bloisi",
            "Daniele Nardi"
        ],
        "citations": 2,
        "references": 39,
        "year": 2024
    },
    {
        "title": "Improving Zero-Shot Generalization for CLIP with Variational Adapter",
        "abstract": null,
        "authors": [
            "Ziqian Lu",
            "Fengli Shen",
            "Mushui Liu",
            "Yunlong Yu",
            "Xi Li"
        ],
        "citations": 2,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Language-Guided Manipulation with Diffusion Policies and Constrained Inpainting",
        "abstract": "Diffusion policies have demonstrated robust performance in generative modeling, prompting their application in robotic manipulation controlled via language descriptions. In this paper, we introduce a zero-shot, open-vocabulary diffusion policy method for robot manipulation. Using Vision-Language Models (VLMs), our method transforms linguistic task descriptions into actionable keyframes in 3D space. These keyframes serve to guide the diffusion process via inpainting. However, naively enforcing the diffusion process to adhere to the generated keyframes is problematic: the keyframes from the VLMs may be incorrect and lead to action sequences where the diffusion model performs poorly. To address these challenges, we develop an inpainting optimization strategy that balances adherence to the keyframes v.s. the training data distribution. Experimental evaluations demonstrate that our approach surpasses the performance of traditional fine-tuned language-conditioned methods in both simulated and real-world settings.",
        "authors": [
            "Ce Hao",
            "Kelvin Lin",
            "Siyuan Luo",
            "Harold Soh"
        ],
        "citations": 2,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data",
        "abstract": "Fine-tuning vision-language models (VLMs) with abundant unlabeled data recently has attracted increasing attention. Existing methods that resort to the pseudolabeling strategy would suffer from heavily incorrect hard pseudolabels when VLMs exhibit low zero-shot performance in downstream tasks. To alleviate this issue, we propose a Candidate Pseudolabel Learning method, termed CPL, to fine-tune VLMs with suitable candidate pseudolabels of unlabeled data in downstream tasks. The core of our method lies in the generation strategy of candidate pseudolabels, which progressively generates refined candidate pseudolabels by both intra- and inter-instance label selection, based on a confidence score matrix for all unlabeled data. This strategy can result in better performance in true label inclusion and class-balanced instance selection. In this way, we can directly apply existing loss functions to learn with generated candidate psueudolabels. Extensive experiments on nine benchmark datasets with three learning paradigms demonstrate the effectiveness of our method. Our code can be found at https://github.com/vanillaer/CPL-ICML2024.",
        "authors": [
            "Jiahan Zhang",
            "Qi Wei",
            "Feng Liu",
            "Lei Feng"
        ],
        "citations": 2,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing",
        "abstract": "This study aims to comprehensively review and empirically evaluate the application of multimodal large language models (MLLMs) and Large Vision Models (VLMs) in object detection for transportation systems. In the first fold, we provide a background about the potential benefits of MLLMs in transportation applications and conduct a comprehensive review of current MLLM technologies in previous studies. We highlight their effectiveness and limitations in object detection within various transportation scenarios. The second fold involves providing an overview of the taxonomy of end-to-end object detection in transportation applications and future directions. Building on this, we proposed empirical analysis for testing MLLMs on three real-world transportation problems that include object detection tasks namely, road safety attributes extraction, safety-critical event detection, and visual reasoning of thermal images. Our findings provide a detailed assessment of MLLM performance, uncovering both strengths and areas for improvement. Finally, we discuss practical limitations and challenges of MLLMs in enhancing object detection in transportation, thereby offering a roadmap for future research and development in this critical area.",
        "authors": [
            "Huthaifa I. Ashqar",
            "Ahmed Jaber",
            "Taqwa I. Alhadidi",
            "Mohammed Elhenawy"
        ],
        "citations": 2,
        "references": 82,
        "year": 2024
    },
    {
        "title": "Œº-Bench: A Vision-Language Benchmark for Microscopy Understanding",
        "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing researchers' efficiency, identifying new image biomarkers, and accelerating hypothesis generation and scientific discovery. However, there is a lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs' perception and cognition capabilities in biological image understanding. To address this gap, we introduce {\\mu}-Bench, an expert-curated benchmark encompassing 22 biomedical tasks across various scientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light), scales (subcellular, cellular, tissue), and organisms in both normal and abnormal states. We evaluate state-of-the-art biomedical, pathology, and general VLMs on {\\mu}-Bench and find that: i) current models struggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii) current specialist models fine-tuned on biomedical data often perform worse than generalist models; iii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior biomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned and pre-trained models offers one solution to forgetting and improves general performance across biomedical tasks. We release {\\mu}-Bench under a permissive license to accelerate the research and development of microscopy foundation models.",
        "authors": [
            "Alejandro Lozano",
            "Jeffrey J Nirschl",
            "James Burgess",
            "S. Gupte",
            "Yuhui Zhang",
            "Alyssa Unell",
            "S. Yeung-Levy"
        ],
        "citations": 2,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Open-Vocabulary Calibration for Fine-tuned CLIP",
        "abstract": "Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes. The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed. Our code is available at https://github.com/ml-stat-Sustech/CLIP_Calibration.",
        "authors": [
            "Shuoyuan Wang",
            "Jindong Wang",
            "Guoqing Wang",
            "Bob Zhang",
            "Kaiyang Zhou",
            "Hongxin Wei"
        ],
        "citations": 2,
        "references": 49,
        "year": 2024
    },
    {
        "title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns",
        "abstract": "Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis. However, the interaction between these models and radiologists has been primarily limited to input images. This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using Vision-Language Models (VLMs) enhanced with radiologists' attention by incorporating eye gaze data alongside textual prompts. Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist's focus during chest X-ray evaluation. We evaluate this methodology in tasks such as visual question answering, chest X-ray report automation, error detection, and differential diagnosis. Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis. Also, the impact of eye gaze on fine-tuning was confirmed as it outperformed other medical VLMs in all tasks except visual question answering. This work marks the potential of leveraging both the VLM's capabilities and the radiologist's domain knowledge to improve the capabilities of AI models in medical imaging, paving a novel way for Computer Assisted Diagnosis with a human-centred AI.",
        "authors": [
            "Yunsoo Kim",
            "Jinge Wu",
            "Yusuf Abdulle",
            "Yue Gao",
            "Honghan Wu"
        ],
        "citations": 2,
        "references": 28,
        "year": 2024
    },
    {
        "title": "Understanding Figurative Meaning through Explainable Visual Entailment",
        "abstract": "Large Vision-Language Models (VLMs) have demonstrated strong capabilities in tasks requiring a fine-grained understanding of literal meaning in images and text, such as visual question-answering or visual entailment. However, there has been little exploration of these models' capabilities when presented with images and captions containing figurative meaning, such as metaphors or humor. To close this gap, we propose a new task framing the figurative meaning understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a caption (hypothesis) and justify the predicted label with a textual explanation. The figurative phenomena can be present either in the image, the caption, or both. Utilizing a human-AI collaboration approach, we build the accompanying expert-verified dataset V-FLUTE, containing 6,027 {image, caption, label, explanation} instances spanning five diverse figurative phenomena: metaphors, similes, idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs struggle to generalize from literal to figurative meaning, particularly when it is present in images. Further, we identify common types of errors in VLM reasoning via human evaluation.",
        "authors": [
            "Arkadiy Saakyan",
            "Shreyas Kulkarni",
            "Tuhin Chakrabarty",
            "S. Muresan"
        ],
        "citations": 2,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models",
        "abstract": "Learning-based methods have achieved strong performance for quadrupedal locomotion. However, several challenges prevent quadrupeds from learning helpful indoor skills that require interaction with environments and humans: lack of end-effectors for manipulation, limited semantic understanding using only simulation data, and low traversability and reachability in indoor environments. We present a system for quadrupedal mobile manipulation in indoor environments. It uses a front-mounted gripper for object manipulation, a low-level controller trained in simulation using egocentric depth for agile skills like climbing and whole-body tilting, and pre-trained vision-language models (VLMs) with a third-person fisheye and an egocentric RGB camera for semantic understanding and command generation. We evaluate our system in two unseen environments without any real-world data collection or training. Our system can zero-shot generalize to these environments and complete tasks, like following user's commands to fetch a randomly placed stuff toy after climbing over a queen-sized bed, with a 60% success rate. Project website: https://helpful-doggybot.github.io/",
        "authors": [
            "Qi Wu",
            "Zipeng Fu",
            "Xuxin Cheng",
            "Xiaolong Wang",
            "Chelsea Finn"
        ],
        "citations": 2,
        "references": 84,
        "year": 2024
    },
    {
        "title": "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) are becoming increasingly vulnerable to adversarial attacks as various novel attack strategies are being proposed against these models. While existing defenses excel in unimodal contexts, they currently fall short in safeguarding VLMs against adversarial threats. To mitigate this vulnerability, we propose a novel, yet elegantly simple approach for detecting adversarial samples in VLMs. Our method leverages Text-to-Image (T2I) models to generate images based on captions produced by target VLMs. Subsequently, we calculate the similarities of the embeddings of both input and generated images in the feature space to identify adversarial samples. Empirical evaluations conducted on different datasets validate the efficacy of our approach, outperforming baseline methods adapted from image classification domains. Furthermore, we extend our methodology to classification tasks, showcasing its adaptability and model-agnostic nature. Theoretical analyses and empirical findings also show the resilience of our approach against adaptive attacks, positioning it as an excellent defense mechanism for real-world deployment against adversarial threats.",
        "authors": [
            "Samar Fares",
            "Klea Ziu",
            "Toluwani Aremu",
            "N. Durasov",
            "Martin Tak'avc",
            "Pascal Fua",
            "Karthik Nandakumar",
            "Ivan Laptev"
        ],
        "citations": 2,
        "references": 99,
        "year": 2024
    },
    {
        "title": "Training-free Video Temporal Grounding using Large-scale Pre-trained Models",
        "abstract": "Video temporal grounding aims to identify video segments within untrimmed videos that are most relevant to a given natural language query. Existing video temporal localization models rely on specific datasets for training and have high data collection costs, but they exhibit poor generalization capability under the across-dataset and out-of-distribution (OOD) settings. In this paper, we propose a Training-Free Video Temporal Grounding (TFVTG) approach that leverages the ability of pre-trained large models. A naive baseline is to enumerate proposals in the video and use the pre-trained visual language models (VLMs) to select the best proposal according to the vision-language alignment. However, most existing VLMs are trained on image-text pairs or trimmed video clip-text pairs, making it struggle to (1) grasp the relationship and distinguish the temporal boundaries of multiple events within the same video; (2) comprehend and be sensitive to the dynamic transition of events (the transition from one event to another) in the video. To address these issues, we propose leveraging large language models (LLMs) to analyze multiple sub-events contained in the query text and analyze the temporal order and relationships between these events. Secondly, we split a sub-event into dynamic transition and static status parts and propose the dynamic and static scoring functions using VLMs to better evaluate the relevance between the event and the description. Finally, for each sub-event description, we use VLMs to locate the top-k proposals and leverage the order and relationships between sub-events provided by LLMs to filter and integrate these proposals. Our method achieves the best performance on zero-shot video temporal grounding on Charades-STA and ActivityNet Captions datasets without any training and demonstrates better generalization capabilities in cross-dataset and OOD settings.",
        "authors": [
            "Minghang Zheng",
            "Xinhao Cai",
            "Qingchao Chen",
            "Yuxin Peng",
            "Yang Liu"
        ],
        "citations": 2,
        "references": 65,
        "year": 2024
    },
    {
        "title": "Multi-modal Situated Reasoning in 3D Scenes",
        "abstract": "Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.",
        "authors": [
            "Xiongkun Linghu",
            "Jiangyong Huang",
            "Xuesong Niu",
            "Xiaojian Ma",
            "Baoxiong Jia",
            "Siyuan Huang"
        ],
        "citations": 2,
        "references": 73,
        "year": 2024
    },
    {
        "title": "MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video Detection on YouTube and Bilibili",
        "abstract": "Hate speech is a pressing issue in modern society, with significant effects both online and offline. Recent research in hate speech detection has primarily centered on text-based media, largely overlooking multimodal content such as videos. Existing studies on hateful video datasets have predominantly focused on English content within a Western context and have been limited to binary labels (hateful or non-hateful), lacking detailed contextual information. This study presents MultiHateClip1 , an novel multilingual dataset created through hate lexicons and human annotation. It aims to enhance the detection of hateful videos on platforms such as YouTube and Bilibili, including content in both English and Chinese languages. Comprising 2,000 videos annotated for hatefulness, offensiveness, and normalcy, this dataset provides a cross-cultural perspective on gender-based hate speech. Through a detailed examination of human annotation results, we discuss the differences between Chinese and English hateful videos and underscore the importance of different modalities in hateful and offensive video analysis. Evaluations of state-of-the-art video classification models, such as VLM, GPT-4V and Qwen-VL, on MultiHateClip highlight the existing challenges in accurately distinguishing between hateful and offensive content and the urgent need for models that are both multimodally and culturally nuanced. MultiHateClip represents a foundational advance in enhancing hateful video detection by underscoring the necessity of a multimodal and culturally sensitive approach in combating online hate speech.",
        "authors": [
            "Han Wang",
            "Tan Rui Yang",
            "Usman Naseem",
            "Roy Ka-Wei Lee"
        ],
        "citations": 2,
        "references": 51,
        "year": 2024
    },
    {
        "title": "RSGPT: A Remote Sensing Vision Language Model and Benchmark",
        "abstract": "The emergence of large-scale large language models, with GPT-4 as a prominent example, has significantly propelled the rapid advancement of artificial general intelligence and sparked the revolution of Artificial Intelligence 2.0. In the realm of remote sensing (RS), there is a growing interest in developing large vision language models (VLMs) specifically tailored for data analysis in this domain. However, current research predominantly revolves around visual recognition tasks, lacking comprehensive, large-scale image-text datasets that are aligned and suitable for training large VLMs, which poses significant challenges to effectively training such models for RS applications. In computer vision, recent research has demonstrated that fine-tuning large vision language models on small-scale, high-quality datasets can yield impressive performance in visual and language understanding. These results are comparable to state-of-the-art VLMs trained from scratch on massive amounts of data, such as GPT-4. Inspired by this captivating idea, in this work, we build a high-quality Remote Sensing Image Captioning dataset (RSICap) that facilitates the development of large VLMs in the RS field. Unlike previous RS datasets that either employ model-generated captions or short descriptions, RSICap comprises 2,585 human-annotated captions with rich and high-quality information. This dataset offers detailed descriptions for each image, encompassing scene descriptions (e.g., residential area, airport, or farmland) as well as object information (e.g., color, shape, quantity, absolute position, etc). To facilitate the evaluation of VLMs in the field of RS, we also provide a benchmark evaluation dataset called RSIEval. This dataset consists of human-annotated captions and visual question-answer pairs, allowing for a comprehensive assessment of VLMs in the context of RS.",
        "authors": [
            "Yuan Hu",
            "Jianlong Yuan",
            "Congcong Wen",
            "Xiaonan Lu",
            "Xiang Li"
        ],
        "citations": 69,
        "references": 85,
        "year": 2023
    },
    {
        "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection",
        "abstract": "Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \\eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.",
        "authors": [
            "Qihang Zhou",
            "Guansong Pang",
            "Yu Tian",
            "Shibo He",
            "Jiming Chen"
        ],
        "citations": 67,
        "references": 61,
        "year": 2023
    },
    {
        "title": "When and why vision-language models behave like bags-of-words, and what to do about it?",
        "abstract": "Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO&Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on retrieval over existing datasets without using the composition and order information. Given that contrastive pretraining optimizes for retrieval on datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality.",
        "authors": [
            "Mert Yuksekgonul",
            "Federico Bianchi",
            "Pratyusha Kalluri",
            "Dan Jurafsky",
            "James Y. Zou"
        ],
        "citations": 286,
        "references": 63,
        "year": 2022
    },
    {
        "title": "Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities",
        "abstract": "This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs‚Äô spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.",
        "authors": [
            "Shiyu Xia",
            "Junyu Xiong",
            "Haoyu Dong",
            "Jianbo Zhao",
            "Yuzhang Tian",
            "Mengyu Zhou",
            "Yeye He",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "citations": 1,
        "references": 27,
        "year": 2024
    },
    {
        "title": "CAST: Cross-modal Alignment Similarity Test for Vision Language Models",
        "abstract": "Vision Language Models (VLMs) are typically evaluated with Visual Question Answering (VQA) tasks which assess a model's understanding of scenes. Good VQA performance is taken as evidence that the model will perform well on a broader range of tasks that require both visual and language inputs. However, scene-aware VQA does not fully capture input biases or assess hallucinations caused by a misalignment between modalities. To address this, we propose a Cross-modal Alignment Similarity Test (CAST) to probe VLMs for self-consistency across modalities. This test involves asking the models to identify similarities between two scenes through text-only, image-only, or both and then assess the truthfulness of the similarities they generate. Since there is no ground-truth to compare against, this evaluation does not focus on objective accuracy but rather on whether VLMs are internally consistent in their outputs. We argue that while not all self-consistent models are capable or accurate, all capable VLMs must be self-consistent.",
        "authors": [
            "Gautier Dagan",
            "Olga Loginova",
            "Anil Batra"
        ],
        "citations": 1,
        "references": 29,
        "year": 2024
    },
    {
        "title": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images",
        "abstract": "Vision language models (VLMs) have shown strong zero-shot generalization across various tasks, especially when integrated with large language models (LLMs). However, their ability to comprehend rhetorical and persuasive visual media, such as advertisements, remains understudied. Ads often employ atypical imagery, using surprising object juxtapositions to convey shared properties. For example, Fig. 1 (e) shows a beer with a feather-like texture. This requires advanced reasoning to deduce that this atypical representation signifies the beer's lightness. We introduce three novel tasks, Multi-label Atypicality Classification, Atypicality Statement Retrieval, and Aypical Object Recognition, to benchmark VLMs' understanding of atypicality in persuasive images. We evaluate how well VLMs use atypicality to infer an ad's message and test their reasoning abilities by employing semantically challenging negatives. Finally, we pioneer atypicality-aware verbalization by extracting comprehensive image descriptions sensitive to atypical elements. Our findings reveal that: (1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple, effective strategies can extract atypicality-aware information, leading to comprehensive image verbalization; (3) atypicality aids persuasive advertisement understanding. Code and data will be made available.",
        "authors": [
            "Sina Malakouti",
            "Aysan Aghazadeh",
            "Ashmit Khandelwal",
            "Adriana Kovashka"
        ],
        "citations": 1,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Boosting Medical Image-based Cancer Detection via Text-guided Supervision from Reports",
        "abstract": "The absence of adequately sufficient expert-level tumor annotations hinders the effectiveness of supervised learning based opportunistic cancer screening on medical imaging. Clinical reports (that are rich in descriptive textual details) can offer a\"free lunch'' supervision information and provide tumor location as a type of weak label to cope with screening tasks, thus saving human labeling workloads, if properly leveraged. However, predicting cancer only using such weak labels can be very changeling since tumors are usually presented in small anatomical regions compared to the whole 3D medical scans. Weakly semi-supervised learning (WSSL) utilizes a limited set of voxel-level tumor annotations and incorporates alongside a substantial number of medical images that have only off-the-shelf clinical reports, which may strike a good balance between minimizing expert annotation workload and optimizing screening efficacy. In this paper, we propose a novel text-guided learning method to achieve highly accurate cancer detection results. Through integrating diagnostic and tumor location text prompts into the text encoder of a vision-language model (VLM), optimization of weakly supervised learning can be effectively performed in the latent space of VLM, thereby enhancing the stability of training. Our approach can leverage clinical knowledge by large-scale pre-trained VLM to enhance generalization ability, and produce reliable pseudo tumor masks to improve cancer detection. Our extensive quantitative experimental results on a large-scale cancer dataset, including 1,651 unique patients, validate that our approach can reduce human annotation efforts by at least 70% while maintaining comparable cancer detection accuracy to competing fully supervised methods (AUC value 0.961 versus 0.966).",
        "authors": [
            "Guangyu Guo",
            "Jiawen Yao",
            "Yingda Xia",
            "Tony C. W. Mok",
            "Zhilin Zheng",
            "Junwei Han",
            "Le Lu",
            "Dingwen Zhang",
            "Jian Zhou",
            "Ling Zhang"
        ],
        "citations": 1,
        "references": 66,
        "year": 2024
    },
    {
        "title": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models",
        "abstract": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to ‚Äúhear‚Äù via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for in silico experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size.",
        "authors": [
            "Tyler Loakman",
            "Yucheng Li",
            "Chenghua Lin"
        ],
        "citations": 1,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Hard Cases Detection in Motion Prediction by Vision-Language Foundation Models",
        "abstract": "Addressing hard cases in autonomous driving, such as anomalous road users, extreme weather conditions, and complex traffic interactions, presents significant challenges. To ensure safety, it is crucial to detect and manage these scenarios effectively for autonomous driving systems. However, the rarity and high-risk nature of these cases demand extensive, diverse datasets for training robust models. Vision-Language Foundation Models (VLMs) have shown remarkable zero-shot capabilities as being trained on extensive datasets. This work explores the potential of VLMs in detecting hard cases in autonomous driving. We demonstrate the capability of VLMs such as GPT-4v in detecting hard cases in traffic participant motion prediction on both agent and scenario levels. We introduce a feasible pipeline where VLMs, fed with sequential image frames with designed prompts, effectively identify challenging agents or scenarios, which are verified by existing prediction models. Moreover, by taking advantage of this detection of hard cases by VLMs, we further improve the training efficiency of the existing motion prediction pipeline by performing data selection for the training samples suggested by GPT. We show the effectiveness and feasibility of our pipeline incorporating VLMs with state-of-the-art methods on NuScenes datasets. The code is accessible at https://github.com/KTH-RPL/Detect_VLM.",
        "authors": [
            "Yi Yang",
            "Qingwen Zhang",
            "Kei Ikemura",
            "Nazre Batool",
            "John Folkesson"
        ],
        "citations": 1,
        "references": 63,
        "year": 2024
    },
    {
        "title": "Diagnosing the Compositional Knowledge of Vision Language Models from a Game-Theoretic View",
        "abstract": "Compositional reasoning capabilities are usually considered as fundamental skills to characterize human perception. Recent studies show that current Vision Language Models (VLMs) surprisingly lack sufficient knowledge with respect to such capabilities. To this end, we propose to thoroughly diagnose the composition representations encoded by VLMs, systematically revealing the potential cause for this weakness. Specifically, we propose evaluation methods from a novel game-theoretic view to assess the vulnerability of VLMs on different aspects of compositional understanding, e.g., relations and attributes. Extensive experimental results demonstrate and validate several insights to understand the incapabilities of VLMs on compositional reasoning, which provide useful and reliable guidance for future studies. The deliverables will be updated at https://vlms-compositionality-gametheory.github.io/.",
        "authors": [
            "Jin Wang",
            "Shichao Dong",
            "Yapeng Zhu",
            "Kelu Yao",
            "Weidong Zhao",
            "Chao Li",
            "Ping Luo"
        ],
        "citations": 1,
        "references": 70,
        "year": 2024
    },
    {
        "title": "Teach CLIP to Develop a Number Sense for Ordinal Regression",
        "abstract": "Ordinal regression is a fundamental problem within the field of computer vision, with customised well-trained models on specific tasks. While pre-trained vision-language models (VLMs) have exhibited impressive performance on various vision tasks, their potential for ordinal regression has received less exploration. In this study, we first investigate CLIP's potential for ordinal regression, from which we expect the model could generalise to different ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP fails on this task, since current VLMs have a well-documented limitation of encapsulating compositional concepts such as number sense. We propose a simple yet effective method called NumCLIP to improve the quantitative understanding of VLMs. We disassemble the exact image to number-specific text matching problem into coarse classification and fine prediction stages. We discretize and phrase each numerical bin with common language concept to better leverage the available pre-trained alignment in CLIP. To consider the inherent continuous property of ordinal regression, we propose a novel fine-grained cross-modal ranking-based regularisation loss specifically designed to keep both semantic and ordinal alignment in CLIP's feature space. Experimental results on three general ordinal regression tasks demonstrate the effectiveness of NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating and image aesthetics assessment task, respectively. Code is publicly available at https://github.com/xmed-lab/NumCLIP.",
        "authors": [
            "Yao Du",
            "Qiang Zhai",
            "Weihang Dai",
            "Xiaomeng Li"
        ],
        "citations": 1,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models",
        "abstract": "With the prevalence of large-scale pretrained vision-language models (VLMs), such as CLIP, soft-prompt tuning has become a popular method for adapting these models to various downstream tasks. However, few works delve into the inherent properties of learnable soft-prompt vectors, specifically the impact of their norms to the performance of VLMs. This motivates us to pose an unexplored research question: ``Do we need to normalize the soft prompts in VLMs?'' To fill this research gap, we first uncover a phenomenon, called the \\textbf{Low-Norm Effect} by performing extensive corruption experiments, suggesting that reducing the norms of certain learned prompts occasionally enhances the performance of VLMs, while increasing them often degrades it. To harness this effect, we propose a novel method named \\textbf{N}ormalizing th\\textbf{e} soft-pro\\textbf{m}pt v\\textbf{e}ctors of vi\\textbf{si}on-language model\\textbf{s} (\\textbf{Nemesis}) to normalize soft-prompt vectors in VLMs. To the best of our knowledge, our work is the first to systematically investigate the role of norms of soft-prompt vector in VLMs, offering valuable insights for future research in soft-prompt tuning. The code is available at \\texttt{\\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}.",
        "authors": [
            "Shuai Fu",
            "Xiequn Wang",
            "Qiushi Huang",
            "Yu Zhang"
        ],
        "citations": 1,
        "references": 40,
        "year": 2024
    },
    {
        "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
        "abstract": "As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Diffusion is incorporated. By carefully balancing the two components during optimization, our method can produce high-quality images with near state-of-the-art performance on T2I-Compbench.",
        "authors": [
            "Xuantong Liu",
            "Tianyang Hu",
            "Wenjia Wang",
            "Kenji Kawaguchi",
            "Yuan Yao"
        ],
        "citations": 1,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Privacy-Aware Visual Language Models",
        "abstract": "This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life. To this end, we introduce a new benchmark PrivBench, which contains images from 8 sensitive categories such as passports, or fingerprints. We evaluate 10 state-of-the-art VLMs on this benchmark and observe a generally limited understanding of privacy, highlighting a significant area for model improvement. Based on this we introduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs with knowledge about visual privacy. By tuning two pretrained VLMs, TinyLLaVa and MiniGPT-v2, on this small dataset, we achieve strong gains in their ability to recognize sensitive content, outperforming even GPT4-V. At the same time, we show that privacy-tuning only minimally affects the VLMs performance on standard benchmarks such as VQA. Overall, this paper lays out a crucial challenge for making VLMs effective in handling real-world data safely and provides a simple recipe that takes the first step towards building privacy-aware VLMs.",
        "authors": [
            "Laurens Samson",
            "Nimrod Barazani",
            "S. Ghebreab",
            "Yukiyasu Asano"
        ],
        "citations": 1,
        "references": 74,
        "year": 2024
    },
    {
        "title": "Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation",
        "abstract": "Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation. Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across domains using a modality discriminator. Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS.",
        "authors": [
            "Xinyao Li",
            "Yuke Li",
            "Zhekai Du",
            "Fengling Li",
            "Ke Lu",
            "Jingjing Li"
        ],
        "citations": 1,
        "references": 63,
        "year": 2024
    },
    {
        "title": "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models",
        "abstract": "Fine-tuning pre-trained Vision-Language Models (VLMs) has shown remarkable capabilities in medical image and textual depiction synergy. Nevertheless, many pre-training datasets are restricted by patient privacy concerns, potentially containing noise that can adversely affect downstream performance. Moreover, the growing reliance on multi-modal generation exacerbates this issue because of its susceptibility to adversarial attacks. To investigate how VLMs trained on adversarial noisy data perform on downstream medical tasks, we first craft noisy upstream datasets using multi-modal adversarial attacks. Through our comprehensive analysis, we unveil that moderate noise enhances model robustness and transferability, but increasing noise levels negatively impact downstream task performance. To mitigate this issue, we propose rectify adversarial noise (RAN) framework, a recipe designed to effectively defend adversarial attacks and rectify the influence of upstream noise during fine-tuning.",
        "authors": [
            "Xu Han",
            "Linghao Jin",
            "Xuezhe Ma",
            "Xiaofeng Liu"
        ],
        "citations": 1,
        "references": 62,
        "year": 2024
    },
    {
        "title": "ComiCap: A VLMs pipeline for dense captioning of Comic Panels",
        "abstract": "The comic domain is rapidly advancing with the development of single- and multi-page analysis and synthesis models. Recent benchmarks and datasets have been introduced to support and assess models' capabilities in tasks such as detection (panels, characters, text), linking (character re-identification and speaker identification), and analysis of comic elements (e.g., dialog transcription). However, to provide a comprehensive understanding of the storyline, a model must not only extract elements but also understand their relationships and generate highly informative captions. In this work, we propose a pipeline that leverages Vision-Language Models (VLMs) to obtain dense, grounded captions. To construct our pipeline, we introduce an attribute-retaining metric that assesses whether all important attributes are identified in the caption. Additionally, we created a densely annotated test set to fairly evaluate open-source VLMs and select the best captioning model according to our metric. Our pipeline generates dense captions with bounding boxes that are quantitatively and qualitatively superior to those produced by specifically trained models, without requiring any additional training. Using this pipeline, we annotated over 2 million panels across 13,000 books, which will be available on the project page https://github.com/emanuelevivoli/ComiCap.",
        "authors": [
            "Emanuele Vivoli",
            "Niccol√≥ Biondi",
            "Marco Bertini",
            "Dimosthenis Karatzas"
        ],
        "citations": 1,
        "references": 31,
        "year": 2024
    },
    {
        "title": "Towards Specific Domain Prompt Learning via Improved Text Label Optimization",
        "abstract": "Prompt learning has emerged as a thriving parameter-efficient fine-tuning technique for adapting pre-trained vision-language models (VLMs) to various downstream tasks. However, existing prompt learning approaches still exhibit limited capability for adapting foundational VLMs to specific domains that require specialized and expert-level knowledge. Since this kind of specific knowledge is primarily embedded in the pre-defined text labels, we infer that foundational VLMs cannot directly interpret semantic meaningful information from these specific text labels, which causes the above limitation. From this perspective, this paper additionally models text labels with learnable tokens and casts this operation into traditional prompt learning framework. By optimizing label tokens, semantic meaningful text labels are automatically learned for each class. Nevertheless, directly optimizing text label still remains two critical problems, i.e., insufficient optimization and biased optimization. We further address these problems by proposing Modality Interaction Text Label Optimization (MITLOp) and Color-based Consistency Augmentation (CCAug) respectively, thereby effectively improving the quality of the optimized text labels. Extensive experiments indicate that our proposed method achieves significant improvements in VLM adaptation on specific domains.",
        "authors": [
            "Liangchen Liu",
            "Nannan Wang",
            "Decheng Liu",
            "Xi Yang",
            "Xinbo Gao",
            "Tongliang Liu"
        ],
        "citations": 1,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types",
        "abstract": "Visual Question-Answering (VQA) has become key to user experience, particularly after improved generalization capabilities of Vision-Language Models (VLMs). But evaluating VLMs for an application requirement using a standardized framework in practical settings is still challenging. This paper aims to solve that using an end-to-end framework. We present VQA360 - a novel dataset derived from established VQA benchmarks, annotated with task types, application domains, and knowledge types, for a comprehensive evaluation. We also introduce GoEval, a multimodal evaluation metric developed using GPT-4o, achieving a correlation factor of 56.71% with human judgments. Our experiments with state-of-the-art VLMs reveal that no single model excels universally, thus, making a right choice a key design decision. Proprietary models such as Gemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source models like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive strengths, while providing additional advantages. Our framework can also be extended to other tasks.",
        "authors": [
            "Neelabh Sinha",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "citations": 1,
        "references": 36,
        "year": 2024
    },
    {
        "title": "Cropper: Vision-Language Model for Image Cropping through In-Context Learning",
        "abstract": "The goal of image cropping is to identify visually appealing crops within an image. Conventional methods rely on specialized architectures trained on specific datasets, which struggle to be adapted to new requirements. Recent breakthroughs in large vision-language models (VLMs) have enabled visual in-context learning without explicit training. However, effective strategies for vision downstream tasks with VLMs remain largely unclear and underexplored. In this paper, we propose an effective approach to leverage VLMs for better image cropping. First, we propose an efficient prompt retrieval mechanism for image cropping to automate the selection of in-context examples. Second, we introduce an iterative refinement strategy to iteratively enhance the predicted crops. The proposed framework, named Cropper, is applicable to a wide range of cropping tasks, including free-form cropping, subject-aware cropping, and aspect ratio-aware cropping. Extensive experiments and a user study demonstrate that Cropper significantly outperforms state-of-the-art methods across several benchmarks.",
        "authors": [
            "Seung Hyun Lee",
            "Junjie Ke",
            "Yinxiao Li",
            "Junfeng He",
            "Steven Hickson",
            "Katie Datsenko",
            "Sangpil Kim",
            "Ming-Hsuan Yang",
            "Irfan Essa",
            "Feng Yang"
        ],
        "citations": 1,
        "references": 46,
        "year": 2024
    },
    {
        "title": "Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning",
        "abstract": "Generative vision-language models (VLMs) have shown impressive performance in zero-shot vision-language tasks like image captioning and visual question answering. How-ever, improving their zero-shot reasoning typically requires second-stage instruction tuning, which relies heavily on human-labeled or large language model-generated annotation, incurring high labeling costs. To tackle this challenge, we introduce Image-Conditioned Caption Correction (ICCC), a novel pre-training task designed to enhance VLMs' zero-shot performance without the need for labeled task-aware data. The ICCC task compels VLMs to rectify mismatches between visual and language concepts, thereby enhancing instruction following and text generation conditioned on visual inputs. Leveraging language structure and a lightweight dependency parser, we construct data samples of ICCC taskfrom image-text datasets with low labeling and computation costs. Experimental results on BLIP-2 and InstructBLIP demonstrate significant improvements in zero-shot image-text generation-based VL tasks through ICCC instruction tuning.",
        "authors": [
            "Rongjie Li",
            "Yu Wu",
            "Xuming He"
        ],
        "citations": 1,
        "references": 44,
        "year": 2024
    },
    {
        "title": "Boosting Vision-Language Models for Histopathology Classification: Predict all at once",
        "abstract": "The development of vision-language models (VLMs) for histo-pathology has shown promising new usages and zero-shot performances. However, current approaches, which decompose large slides into smaller patches, focus solely on inductive classification, i.e., prediction for each patch is made independently of the other patches in the target test data. We extend the capability of these large models by introducing a transductive approach. By using text-based predictions and affinity relationships among patches, our approach leverages the strong zero-shot capabilities of these new VLMs without any additional labels. Our experiments cover four histopathology datasets and five different VLMs. Operating solely in the embedding space (i.e., in a black-box setting), our approach is highly efficient, processing $10^5$ patches in just a few seconds, and shows significant accuracy improvements over inductive zero-shot classification. Code available at https://github.com/FereshteShakeri/Histo-TransCLIP.",
        "authors": [
            "Maxime Zanella",
            "Fereshteh Shakeri",
            "Yunshi Huang",
            "H. Bahig",
            "Ismail Ben Ayed"
        ],
        "citations": 1,
        "references": 33,
        "year": 2024
    },
    {
        "title": "VividMed: Vision Language Model with Versatile Visual Grounding for Medicine",
        "abstract": "Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable promise in generating visually grounded responses. However, their application in the medical domain is hindered by unique challenges. For instance, most VLMs rely on a single method of visual grounding, whereas complex medical tasks demand more versatile approaches. Additionally, while most VLMs process only 2D images, a large portion of medical images are 3D. The lack of medical data further compounds these obstacles. To address these challenges, we present VividMed, a vision language model with versatile visual grounding for medicine. Our model supports generating both semantic segmentation masks and instance-level bounding boxes, and accommodates various imaging modalities, including both 2D and 3D data. We design a three-stage training procedure and an automatic data synthesis pipeline based on open datasets and models. Besides visual grounding tasks, VividMed also excels in other common downstream tasks, including Visual Question Answering (VQA) and report generation. Ablation studies empirically show that the integration of visual grounding ability leads to improved performance on these tasks. Our code is publicly available at https://github.com/function2-llx/MMMM.",
        "authors": [
            "Lingxiao Luo",
            "Bingda Tang",
            "Xuanzhong Chen",
            "Rong Han",
            "Ting Chen"
        ],
        "citations": 1,
        "references": 59,
        "year": 2024
    },
    {
        "title": "Aligning Medical Images with General Knowledge from Large Language Models",
        "abstract": "Pre-trained large vision-language models (VLMs) like CLIP have revolutionized visual representation learning using natural language as supervisions, and demonstrated promising generalization ability. In this work, we propose ViP, a novel visual symptom-guided prompt learning framework for medical image analysis, which facilitates general knowledge transfer from CLIP. ViP consists of two key components: a visual symptom generator (VSG) and a dual-prompt network. Specifically, VSG aims to extract explicable visual symptoms from pre-trained large language models, while the dual-prompt network utilizes these visual symptoms to guide the training on two learnable prompt modules, i.e., context prompt and merge prompt, which effectively adapts our framework to medical image analysis via large VLMs. Extensive experimental results demonstrate that ViP can outperform state-of-the-art methods on two challenging datasets.",
        "authors": [
            "Xiao Fang",
            "Yi-Mou Lin",
            "Dong-Ming Zhang",
            "Kwang-Ting Cheng",
            "Hao Chen"
        ],
        "citations": 1,
        "references": 27,
        "year": 2024
    },
    {
        "title": "Improving Multi-label Recognition using Class Co-Occurrence Probabilities",
        "abstract": "Multi-label Recognition (MLR) involves the identification of multiple objects within an image. To address the additional complexity of this problem, recent works have leveraged information from vision-language models (VLMs) trained on large text-images datasets for the task. These methods learn an independent classifier for each object (class), overlooking correlations in their occurrences. Such co-occurrences can be captured from the training data as conditional probabilities between a pair of classes. We propose a framework to extend the independent classifiers by incorporating the co-occurrence information for object pairs to improve the performance of independent classifiers. We use a Graph Convolutional Network (GCN) to enforce the conditional probabilities between classes, by refining the initial estimates derived from image and text sources obtained using VLMs. We validate our method on four MLR datasets, where our approach outperforms all state-of-the-art methods.",
        "authors": [
            "Samyak Rawlekar",
            "Shubhang Bhatnagar",
            "Vishnuvardhan Pogunulu Srinivasulu",
            "Narendra Ahuja"
        ],
        "citations": 1,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Advancing Prompt Learning through an External Layer",
        "abstract": "Prompt learning represents a promising method for adapting pre-trained vision-language models (VLMs) to various downstream tasks by learning a set of text embeddings. One challenge inherent to these methods is the poor generalization performance due to the invalidity of the learned text embeddings for unseen tasks. A straightforward approach to bridge this gap is to freeze the text embeddings in prompts, which results in a lack of capacity to adapt VLMs for downstream tasks. To address this dilemma, we propose a paradigm called EnPrompt with a novel External Layer (EnLa). Specifically, we propose a textual external layer and learnable visual embeddings for adapting VLMs to downstream tasks. The learnable external layer is built upon valid embeddings of pre-trained CLIP. This design considers the balance of learning capabilities between the two branches. To align the textual and visual features, we propose a novel two-pronged approach: i) we introduce the optimal transport as the discrepancy metric to align the vision and text modalities, and ii) we introduce a novel strengthening feature to enhance the interaction between these two modalities. Four representative experiments (i.e., base-to-novel generalization, few-shot learning, cross-dataset generalization, domain shifts generalization) across 15 datasets demonstrate that our method outperforms the existing prompt learning method.",
        "authors": [
            "Fangming Cui",
            "Xun Yang",
            "Chao Wu",
            "Liang Xiao",
            "Xinmei Tian"
        ],
        "citations": 1,
        "references": 80,
        "year": 2024
    },
    {
        "title": "Video Anomaly Detection in 10 Years: A Survey and Outlook",
        "abstract": "Video anomaly detection (VAD) holds immense importance across diverse domains such as surveillance, healthcare, and environmental monitoring. While numerous surveys focus on conventional VAD methods, they often lack depth in exploring specific approaches and emerging trends. This survey explores deep learning-based VAD, expanding beyond traditional supervised training paradigms to encompass emerging weakly supervised, self-supervised, and unsupervised approaches. A prominent feature of this review is the investigation of core challenges within the VAD paradigms including large-scale datasets, features extraction, learning methods, loss functions, regularization, and anomaly score prediction. Moreover, this review also investigates the vision language models (VLMs) as potent feature extractors for VAD. VLMs integrate visual data with textual descriptions or spoken language from videos, enabling a nuanced understanding of scenes crucial for anomaly detection. By addressing these challenges and proposing future research directions, this review aims to foster the development of robust and efficient VAD systems leveraging the capabilities of VLMs for enhanced anomaly detection in complex real-world scenarios. This comprehensive analysis seeks to bridge existing knowledge gaps, provide researchers with valuable insights, and contribute to shaping the future of VAD research.",
        "authors": [
            "Moshira Abdalla",
            "Sajid Javed",
            "Muaz Al Radi",
            "Anwaar Ulhaq",
            "N. Werghi"
        ],
        "citations": 1,
        "references": 89,
        "year": 2024
    },
    {
        "title": "Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead",
        "abstract": "As Vision Language Models (VLMs) gain widespread use, their fairness remains under-explored. In this paper, we analyze demographic biases across five models and six datasets. We find that portrait datasets like UTKFace and CelebA are the best tools for bias detection, finding gaps in performance and fairness between LLaVa and CLIP models. However, scene based datasets like PATA, VLStereoSet fail to be useful benchmarks for bias due to their construction. As for pronoun based datasets like VisoGender, we receive mixed signals as only some subsets of the data are useful in providing insights. To alleviate this problem, we introduce a more difficult version of VisoGender to serve as a more rigorous evaluation. Based on these results, we call for more effective and carefully designed datasets to ensure VLMs are both fair and reliable.",
        "authors": [
            "Kuleen Sasse",
            "Shan Chen",
            "Jackson Pond",
            "D. Bitterman",
            "John D. Osborne"
        ],
        "citations": 1,
        "references": 31,
        "year": 2024
    },
    {
        "title": "VLMimic: Vision Language Models are Visual Imitation Learner for Fine-grained Actions",
        "abstract": "Visual imitation learning (VIL) provides an efficient and intuitive strategy for robotic systems to acquire novel skills. Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable performance in vision and language reasoning capabilities for VIL tasks. Despite the progress, current VIL methods naively employ VLMs to learn high-level plans from human videos, relying on pre-defined motion primitives for executing physical interactions, which remains a major bottleneck. In this work, we present VLMimic, a novel paradigm that harnesses VLMs to directly learn even fine-grained action levels, only given a limited number of human videos. Specifically, VLMimic first grounds object-centric movements from human videos, and learns skills using hierarchical constraint representations, facilitating the derivation of skills with fine-grained action levels from limited human videos. These skills are refined and updated through an iterative comparison strategy, enabling efficient adaptation to unseen environments. Our extensive experiments exhibit that our VLMimic, using only 5 human videos, yields significant improvements of over 27% and 21% in RLBench and real-world manipulation tasks, and surpasses baselines by over 37% in long-horizon tasks.",
        "authors": [
            "Guanyan Chen",
            "Meiling Wang",
            "Te Cui",
            "Yao Mu",
            "Haoyang Lu",
            "Tianxing Zhou",
            "Zicai Peng",
            "Mengxiao Hu",
            "Haizhou Li",
            "Yuan Li",
            "Yi Yang",
            "Yufeng Yue"
        ],
        "citations": 1,
        "references": 68,
        "year": 2024
    },
    {
        "title": "VisMin: Visual Minimal-Change Understanding",
        "abstract": "Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). Existing benchmarks primarily focus on evaluating VLMs' capability to distinguish between two very similar captions given an image. In this paper, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. The image pair and caption pair contain minimal changes, i.e., only one aspect changes at a time from among the following: object, attribute, count, and spatial relation. These changes test the models' understanding of objects, attributes (such as color, material, shape), counts, and spatial relationships between objects. We built an automatic framework using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. We also generate a large-scale training dataset to finetune CLIP and Idefics2, showing significant improvements in fine-grained understanding across benchmarks and in CLIP's general image-text alignment. We release all resources, including the benchmark, training data, and finetuned model checkpoints, at https://vismin.net/.",
        "authors": [
            "Rabiul Awal",
            "Saba Ahmadi",
            "Le Zhang",
            "Aishwarya Agrawal"
        ],
        "citations": 1,
        "references": 71,
        "year": 2024
    },
    {
        "title": "Generative Semantic Communication via Textual Prompts: Latency Performance Tradeoffs",
        "abstract": "This paper develops an edge-device collaborative Generative Semantic Communications (Gen SemCom) framework leveraging pre-trained Multi-modal/Vision Language Models (M/VLMs) for ultra-low-rate semantic communication via textual prompts. The proposed framework optimizes the use of M/VLMs on the wireless edge/device to generate high-fidelity textual prompts through visual captioning/question answering, which are then transmitted over a wireless channel for SemCom. Specifically, we develop a multi-user Gen SemCom framework using pre-trained M/VLMs, and formulate a joint optimization problem of prompt generation offloading, communication and computation resource allocation to minimize the latency and maximize the resulting semantic quality. Due to the nonconvex nature of the problem with highly coupled discrete and continuous variables, we decompose it as a two-level problem and propose a low-complexity swap/leaving/joining (SLJ)-based matching algorithm. Simulation results demonstrate significant performance improvements over the conventional semanticunaware/non-collaborative offloading benchmarks.",
        "authors": [
            "Mengmeng Ren",
            "Li Qiao",
            "Long Yang",
            "Zhen Gao",
            "Jian Chen",
            "Mahdi Boloursaz Mashhadi",
            "Pei Xiao",
            "Rahim Tafazolli",
            "Mehdi Bennis"
        ],
        "citations": 1,
        "references": 30,
        "year": 2024
    },
    {
        "title": "ViLBias: A Comprehensive Framework for Bias Detection through Linguistic and Visual Cues , presenting Annotation Strategies, Evaluation, and Key Challenges",
        "abstract": "The integration of Large Language Models (LLMs) and Vision-Language Models (VLMs) opens new avenues for addressing complex challenges in multimodal content analysis, particularly in biased news detection. This study introduces VLBias, a framework that leverages state-of-the-art LLMs and VLMs to detect linguistic and visual biases in news content. We present a multimodal dataset comprising textual content and corresponding images from diverse news sources. We propose a hybrid annotation framework that combines LLM-based annotations with human review to ensure high-quality labeling while reducing costs and enhancing scalability. Our evaluation compares the performance of state-of-the-art SLMs and LLMs for both modalities (text and images) and the results reveal that while SLMs are computationally efficient, LLMs demonstrate superior accuracy in identifying subtle framing and text-visual inconsistencies. Furthermore, empirical analysis shows that incorporating visual cues alongside textual data improves bias detection accuracy by 3 to 5%. This study provides a comprehensive exploration of LLMs, SLMs, and VLMs as tools for detecting multimodal biases in news content and highlights their respective strengths, limitations, and potential for future applications",
        "authors": [
            "Shaina Raza",
            "Caesar Saleh",
            "Emrul Hasan",
            "Franklin Ogidi",
            "Maximus Powers",
            "Veronica Chatrath",
            "Marcelo Lotif",
            "Roya Javadi",
            "Anam Zahid",
            "Vahid Reza Khazaie"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Creating a Lens of Chinese Culture: A Multimodal Dataset for Chinese Pun Rebus Art Understanding",
        "abstract": "Large vision-language models (VLMs) have demonstrated remarkable abilities in understanding everyday content. However, their performance in the domain of art, particularly culturally rich art forms, remains less explored. As a pearl of human wisdom and creativity, art encapsulates complex cultural narratives and symbolism. In this paper, we offer the Pun Rebus Art Dataset, a multimodal dataset for art understanding deeply rooted in traditional Chinese culture. We focus on three primary tasks: identifying salient visual elements, matching elements with their symbolic meanings, and explanations for the conveyed messages. Our evaluation reveals that state-of-the-art VLMs struggle with these tasks, often providing biased and hallucinated explanations and showing limited improvement through in-context learning. By releasing the Pun Rebus Art Dataset, we aim to facilitate the development of VLMs that can better understand and interpret culturally specific content, promoting greater inclusiveness beyond English-based corpora.",
        "authors": [
            "Tuo Zhang",
            "Tiantian Feng",
            "Yibin Ni",
            "Mengqin Cao",
            "Ruying Liu",
            "Katharine Butler",
            "Yanjun Weng",
            "Mi Zhang",
            "Shrikanth S. Narayanan",
            "S. Avestimehr"
        ],
        "citations": 1,
        "references": 23,
        "year": 2024
    },
    {
        "title": "Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback",
        "abstract": "Radiologists play a crucial role by translating medical images into medical reports. However, the field faces staffing shortages and increasing workloads. While automated approaches using vision-language models (VLMs) show promise as assistants, they require exceptionally high accuracy. Most current VLMs in radiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the general domain, additional preference fine-tuning has become standard practice. The challenge in radiology lies in the prohibitive cost of obtaining radiologist feedback. We propose a scalable automated preference alignment technique for VLMs in radiology, focusing on chest X-ray (CXR) report generation. Our method leverages publicly available datasets with an LLM-as-a-Judge mechanism, eliminating the need for additional expert radiologist feedback. We evaluate and benchmark five direct alignment algorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN scores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in an average across six metrics (domain specific and general), compared to the SFT baseline. We study reward overoptimization via length exploitation, with reports lengthening by up to 3.2x. To assess a potential alignment tax, we benchmark on six additional diverse tasks, finding no significant degradations. A reader study involving four board-certified radiologists indicates win rates of up to 0.62 over the SFT baseline, while significantly penalizing verbosity. Our analysis provides actionable insights for the development of VLMs in high-stakes fields like radiology.",
        "authors": [
            "Dennis Hein",
            "Zhihong Chen",
            "Sophie Ostmeier",
            "Justin Xu",
            "Maya Varma",
            "E. Reis",
            "Arne Edward Michalson",
            "Christian Bluethgen",
            "Hyun Joo Shin",
            "Curtis P. Langlotz",
            "Akshay Chaudhari"
        ],
        "citations": 1,
        "references": 49,
        "year": 2024
    },
    {
        "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
        "abstract": "We present LoCoVQA, a dynamic benchmark generator for evaluating long-context extractive reasoning in vision language models (VLMs). LoCoVQA augments test examples for mathematical reasoning, VQA, and character recognition tasks with increasingly long visual contexts composed of both in-distribution and out-of-distribution distractor images. Across these tasks, a diverse set of VLMs rapidly lose performance as the visual context length grows, often exhibiting a striking logarithmic decay trend. This test assesses how well VLMs can ignore irrelevant information when answering queries -- a task that is quite easy for language models (LMs) in the text domain -- demonstrating that current state-of-the-art VLMs lack this essential capability for many long-context applications.",
        "authors": [
            "Aditya Sharma",
            "Michael Stephen Saxon",
            "William Yang Wang",
            "Haim-ing Bao",
            "Mo Bavarian",
            "J. Belgum",
            "Ir-wan Bello",
            "Jake Berdine",
            "Gabriel Bernadett-Shapiro",
            "Christopher Berner",
            "Lenny Bogdonoff",
            "Oleg Boiko",
            "Made-laine Boyd",
            "Anna-Luisa Brakman",
            "Greg Brock-man",
            "Tim Brooks",
            "M. Brundage",
            "Kevin Button",
            "Trevor Cai",
            "Rosie Campbell",
            "Andrew Cann",
            "Brittany Carey",
            "Chelsea Carlson",
            "Rory Carmichael",
            "Brooke Chan",
            "Che Chang",
            "Fotis Chantzis",
            "Derek Chen",
            "Sully Chen",
            "Ruby Chen",
            "Jason Chen",
            "Mark Chen",
            "B. Chess",
            "Chester Cho",
            "Hyung Casey Chu",
            "Won Chung",
            "Dave Cummings",
            "Jeremiah Currier",
            "Yunxing Dai",
            "Tarun Goel",
            "Gabriel Gogineni",
            "Rapha Goh",
            "Jonathan Gontijo-Lopes",
            "Morgan Gordon",
            "Scott Grafstein",
            "Ryan Gray",
            "Joshua Greene",
            "Shixiang Shane Gross",
            "Yufei Gu",
            "Chris Guo",
            "Jesse Hallacy",
            "Jeff Han",
            "Harris Yuchen",
            "Mike He",
            "Johannes Heaton",
            "C. Heidecke",
            "Alan Hesse",
            "Wade Hickey",
            "Peter Hickey",
            "Hoeschele Brandon",
            "Kenny Houghton",
            "Shengli Hsu",
            "Xin Hu",
            "Joost Hu",
            "Shantanu Huizinga",
            "Shawn Jain",
            "Jain Joanne",
            "Angela Jang",
            "Roger Jiang",
            "Haozhun Jiang",
            "Denny Jin",
            "Shino Jin",
            "Billie Jomoto",
            "Hee-woo Jonn",
            "Tomer Jun",
            "≈Åukasz Kaftan",
            "Ali Kaiser",
            "Ingmar Ka-mali",
            "Kanitscheider",
            "Nitish Shirish",
            "Keskar Tabarak",
            "Logan Khan",
            "J. Kilpatrick",
            "Kim Christina",
            "Yongjik Kim",
            "Jan Hendrik Kim",
            "Jamie Kirch-ner",
            "Matt Kiros",
            "Daniel Knight",
            "Kokotajlo ≈Åukasz",
            "A. Kondraciuk",
            "Aris Kondrich",
            "Kyle Kon-stantinidis",
            "G. Kosic",
            "Vishal Krueger",
            "Michael Kuo",
            "Ikai Lampe",
            "Teddy Lan",
            "Jan Lee",
            "Jade Leike",
            "Daniel Leung",
            "Chak Ming Levy",
            "Li Rachel",
            "Molly Lim",
            "Stephanie Lin",
            "Mateusz Lin",
            "Theresa Litwin",
            "Ryan Lopez",
            "Patricia Lowe",
            "Lue Anna",
            "Kim Makanju",
            "S. Malfacini",
            "Todor Manning",
            "Yaniv Markov",
            "Bianca Markovski",
            "Katie Martin",
            "Andrew Mayer",
            "Bob Mayne",
            "Scott Mayer McGrew",
            "Christine McKinney",
            "Paul McLeavey",
            "McMillan Jake",
            "David McNeil",
            "Aalok Medina",
            "Jacob Mehta",
            "Luke Menick",
            "Andrey Metz",
            "Pamela Mishchenko",
            "Vinnie Mishkin",
            "Evan Monaco",
            "Daniel Morikawa",
            "Tong Mossing",
            "Mira Mu",
            "Oleg Murati",
            "David Murk",
            "Ashvin M√©ly",
            "Reiichiro Nair",
            "Rajeev Nakano",
            "Nayak Arvind",
            "Richard Neelakantan",
            "Hyeonwoo Ngo",
            "Noh Long",
            "Cullen Ouyang",
            "Jakub O‚ÄôKeefe",
            "Alex Pachocki",
            "J. Paino",
            "Ashley Palermo",
            "Giambat-tista Pantuliano",
            "Joel Parascandolo",
            "Emy Parish",
            "Alex Parparita",
            "Mikhail Passos",
            "Andrew Pavlov",
            "Adam Peng",
            "Filipe Perel-man",
            "de Avila Belbute",
            "Michael Peres",
            "Petrov Henrique",
            "Pond√©",
            "Michael Oliveira Pinto",
            "Michelle Pokrass",
            "Vitchyr H. Pong",
            "Tolly Pow-ell",
            "Alethea Power",
            "Boris Power",
            "Elizabeth Proehl",
            "Raul Puri",
            "Alec Radford",
            "Jack W. Rae",
            "Aditya Ramesh",
            "Cameron Raymond",
            "Francis Real",
            "Kendra Rimbach",
            "Carl Ross",
            "Bob Rotsted",
            "Henri Roussez",
            "Nick Ry-der",
            "Mario Saltarelli",
            "Ted Sanders",
            "Shibani Santurkar",
            "Girish Sastry",
            "Heather Schmidt",
            "David Schnurr",
            "John Schulman",
            "Daniel Selsam",
            "Kyla Sheppard",
            "Toki Sherbakov",
            "Jessica Shieh",
            "Sarah Shoker",
            "Pranav Shyam",
            "Szymon Sidor",
            "Eric Sigler",
            "Maddie Simens",
            "Jordan Sitkin",
            "Katarina Slama",
            "Ian Sohl",
            "Benjamin Sokolowsky",
            "Yang Song",
            "Natalie Staudacher",
            "Clemens Winter",
            "Samuel Wolrich",
            "Hannah Wong",
            "Lauren Workman",
            "Sherwin Wu",
            "Jeff Wu",
            "Michael Wu",
            "Kai Xiao",
            "Tao Xu",
            "Sarah Yoo",
            "Kevin Yu",
            "Qim-ing Yuan",
            "Wojciech Zaremba",
            "Rowan Zellers",
            "Chong Zhang",
            "Marvin Zhang",
            "Tianhao Shengjia Zhao"
        ],
        "citations": 1,
        "references": 66,
        "year": 2024
    },
    {
        "title": "AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving",
        "abstract": "Recent advancements in large vision language models (VLMs) tailored for autonomous driving (AD) have shown strong scene understanding and reasoning capabilities, making them undeniable candidates for end-to-end driving systems. However, limited work exists on studying the trustworthiness of DriveVLMs -- a critical factor that directly impacts public transportation safety. In this paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for large vision-language models in autonomous driving (DriveVLMs), considering diverse perspectives -- including trustfulness, safety, robustness, privacy, and fairness. We constructed the largest visual question-answering dataset for investigating trustworthiness issues in driving scenarios, comprising over 10k unique scenes and 18k queries. We evaluated six publicly available VLMs, spanning from generalist to specialist, from open-source to commercial models. Our exhaustive evaluations have unveiled previously undiscovered vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform specialized models fine-tuned for driving in terms of overall trustworthiness. DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing sensitive information. Additionally, both generalist and specialist VLMs remain susceptible to adversarial attacks and struggle to ensure unbiased decision-making across diverse environments and populations. Our findings call for immediate and decisive action to address the trustworthiness of DriveVLMs -- an issue of critical importance to public safety and the welfare of all citizens relying on autonomous transportation systems. Our benchmark is publicly available at \\url{https://github.com/taco-group/AutoTrust}, and the leaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.",
        "authors": [
            "Shuo Xing",
            "Hongyuan Hua",
            "Xiangbo Gao",
            "Shenzhe Zhu",
            "Renjie Li",
            "Kexin Tian",
            "Xiaopeng Li",
            "Heng Huang",
            "Tianbao Yang",
            "Zhangyang Wang",
            "Yang Zhou",
            "Huaxiu Yao",
            "Zhengzhong Tu"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "I Know About \"Up\"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction",
        "abstract": "Visual Language Models (VLMs) are essential for various tasks, particularly visual reasoning tasks, due to their robust multi-modal information integration, visual reasoning capabilities, and contextual awareness. However, existing \\VLMs{}' visual spatial reasoning capabilities are often inadequate, struggling even with basic tasks such as distinguishing left from right. To address this, we propose the \\ours{} model, designed to enhance the visual spatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D reconstruction model for obtaining different views of the input images and incorporates a prompting mechanism to further improve visual spatial reasoning. Experimental results on four visual spatial reasoning datasets show that our \\ours{} achieves up to 19.48% accuracy improvement, which indicates the effectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.",
        "authors": [
            "Zaiqiao Meng",
            "Hao Zhou",
            "Yifang Chen"
        ],
        "citations": 1,
        "references": 57,
        "year": 2024
    },
    {
        "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
        "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.",
        "authors": [
            "Chengke Zou",
            "Xing-ming Guo",
            "Rui Yang",
            "Junyu Zhang",
            "Bin Hu",
            "Huan Zhang"
        ],
        "citations": 1,
        "references": 59,
        "year": 2024
    },
    {
        "title": "MotIF: Motion Instruction Fine-tuning",
        "abstract": "While success in many robotics tasks can be determined by only observing the final state and how it differs from the initial state - e.g., if an apple is picked up - many tasks require observing the full motion of the robot to correctly determine success. For example, brushing hair requires repeated strokes that correspond to the contours and type of hair. Prior works often use off-the-shelf vision-language models (VLMs) as success detectors; however, when success depends on the full trajectory, VLMs struggle to make correct judgments for two reasons. First, modern VLMs are trained only on single frames, and cannot capture changes over a full trajectory. Second, even if we provide state-of-the-art VLMs with an aggregate input of multiple frames, they still fail to detect success due to a lack of robot data. Our key idea is to fine-tune VLMs using abstract representations that are able to capture trajectory-level information such as the path the robot takes by overlaying keypoint trajectories on the final image. We propose motion instruction fine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned abstract representations to semantically ground the robot's behavior in the environment. To benchmark and fine-tune VLMs for robotic motion understanding, we introduce the MotIF-1K dataset containing 653 human and 369 robot demonstrations across 13 task categories. MotIF assesses the success of robot motion given the image observation of the trajectory, task instruction, and motion description. Our model significantly outperforms state-of-the-art VLMs by at least twice in precision and 56.1% in recall, generalizing across unseen motions, tasks, and environments. Finally, we demonstrate practical applications of MotIF in refining and terminating robot planning, and ranking trajectories on how they align with task and motion descriptions. Project page: https://motif-1k.github.io",
        "authors": [
            "Minyoung Hwang",
            "Joey Hejna",
            "Dorsa Sadigh",
            "Yonatan Bisk"
        ],
        "citations": 1,
        "references": 43,
        "year": 2024
    },
    {
        "title": "FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models",
        "abstract": "Vision language models (VLMs) have achieved impressive progress in diverse applications, becoming a prevalent research direction. In this paper, we build FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn conversations that are derived from 27 source datasets, empowering VLMs to spontaneously refine their responses based on user feedback across diverse tasks. To scale up the data collection, FIRE is collected in two components: FIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is freely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a benchmark to comprehensively evaluate the feedback-refining capability of VLMs, which contains 11K feedback-refinement conversations as the test data, two evaluation settings, and a model to provide feedback for VLMs. We develop the FIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows remarkable feedback-refining capability on FIRE-Bench and outperforms untrained VLMs by 50%, making more efficient user-agent interactions and underscoring the significance of the FIRE dataset.",
        "authors": [
            "Pengxiang Li",
            "Zhi Gao",
            "Bofei Zhang",
            "Tao Yuan",
            "Yuwei Wu",
            "Mehrtash Harandi",
            "Yunde Jia",
            "Song-Chun Zhu",
            "Qing Li"
        ],
        "citations": 1,
        "references": 72,
        "year": 2024
    },
    {
        "title": "Investigating Compositional Challenges in Vision-Language Models for Visual Grounding",
        "abstract": "Pre-trained vision-language models (VLMs) have achieved high performance on various downstream tasks, which have been widely used for visual grounding tasks in a weakly supervised manner. However, despite the per-formance gains contributed by large vision and language pre-training, we find that state-of-the-art VLMs struggle with compositional reasoning on grounding tasks. To demonstrate this, we propose Attribute, Relation, and Pri-ority grounding (ARPGrounding) benchmark to test VLMs' compositional reasoning ability on visual grounding tasks. ARPGrounding contains 11,425 samples and evaluates the compositional understanding of VLMs in three dimensions: 1) attribute, denoting comprehension of objects' properties; 2) relation, indicating an understanding of relation between objects; 3) priority, reflecting an awareness of the part of speech associated with nouns. Using the ARPGrounding benchmark, we evaluate several mainstream VLMs. We empirically find that these models perform quite well on conventional visual grounding datasets, achieving performance comparable to or surpassing state-of-the-art methods but showing strong deficiencies in compositional reasoning. Furthermore, we propose a composition-aware fine-tuning pipeline, demonstrating the potential to lever-age cost-effective image-text annotations for enhancing the compositional understanding of VLMs in grounding tasks. Code is available at link.",
        "authors": [
            "Yunan Zeng",
            "Yan Huang",
            "Jinjin Zhang",
            "Zequn Jie",
            "Zhenhua Chai",
            "Liang Wang"
        ],
        "citations": 1,
        "references": 56,
        "year": 2024
    },
    {
        "title": "Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks",
        "abstract": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual and language reasoning tasks, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Specifically, we collect preferred and disfavored samples based on the correctness of initial and refined responses, which are obtained by two-turn self-correction with VLMs during the inference stage. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their self-generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance the reasoning abilities of models through additional training, enabling them to generate high-quality responses directly without further refinement.",
        "authors": [
            "Jiayi He",
            "Hehai Lin",
            "Qingyun Wang",
            "Y. Fung",
            "Heng Ji"
        ],
        "citations": 1,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics",
        "abstract": "Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that GPT-4 can estimate an object's reflectance using only text input with a mean error of 14.7%, lower than the image-only ResNet. Moreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained a competitive 19.9% compared to ResNet's 17.8%. These results suggest that the distributional semantics in LLMs and VLMs increases their generalization capabilities, and the knowledge acquired by VLMs benefits from the latent structure of language.",
        "authors": [
            "Masashi Osada",
            "G. A. G. Ricardez",
            "Yosuke Suzuki",
            "Tadahiro Taniguchi"
        ],
        "citations": 1,
        "references": 41,
        "year": 2024
    },
    {
        "title": "Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models",
        "abstract": "Image-based advertisements are complex multimodal stimuli that often contain unusual visual elements and figurative language. Previous research on automatic ad understanding has reported impressive zero-shot accuracy of contrastive vision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we examine the original task setup and show that contrastive VLMs can solve it by exploiting grounding heuristics. To control for this confound, we introduce TRADE, a new evaluation test set with adversarial grounded explanations. While these explanations look implausible to humans, we show that they\"fool\"four different contrastive VLMs. Our findings highlight the need for an improved operationalisation of automatic ad understanding that truly evaluates VLMs' multimodal reasoning abilities. We make our code and TRADE available at https://github.com/dmg-illc/trade .",
        "authors": [
            "A. Bavaresco",
            "A. Testoni",
            "R. Fern'andez"
        ],
        "citations": 1,
        "references": 31,
        "year": 2024
    },
    {
        "title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models",
        "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. In this paper, we conduct a thorough empirical analysis, focusing on attention modules across layers. We reveal several key insights about how these models process visual data: (i) the internal representation of the query tokens (e.g., representations of\"describe the image\"), is utilized by VLMs to store global image information; we demonstrate that these models generate surprisingly descriptive responses solely from these tokens, without direct access to image tokens. (ii) Cross-modal information flow is predominantly influenced by the middle layers (approximately 25% of all layers), while early and late layers contribute only marginally.(iii) Fine-grained visual attributes and object details are directly extracted from image tokens in a spatially localized manner, i.e., the generated tokens associated with a specific object or attribute attend strongly to their corresponding regions in the image. We propose novel quantitative evaluation to validate our observations, leveraging real-world complex visual scenes. Finally, we demonstrate the potential of our findings in facilitating efficient visual processing in state-of-the-art VLMs.",
        "authors": [
            "Omri Kaduri",
            "Shai Bagon",
            "Tali Dekel"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Automating Robot Failure Recovery Using Vision-Language Models With Optimized Prompts",
        "abstract": "Current robot autonomy struggles to operate beyond the assumed Operational Design Domain (ODD), the specific set of conditions and environments in which the system is designed to function, while the real-world is rife with uncertainties that may lead to failures. Automating recovery remains a significant challenge. Traditional methods often rely on human intervention to manually address failures or require exhaustive enumeration of failure cases and the design of specific recovery policies for each scenario, both of which are labor-intensive. Foundational Vision-Language Models (VLMs), which demonstrate remarkable common-sense generalization and reasoning capabilities, have broader, potentially unbounded ODDs. However, limitations in spatial reasoning continue to be a common challenge for many VLMs when applied to robot control and motion-level error recovery. In this paper, we investigate how optimizing visual and text prompts can enhance the spatial reasoning of VLMs, enabling them to function effectively as black-box controllers for both motion-level position correction and task-level recovery from unknown failures. Specifically, the optimizations include identifying key visual elements in visual prompts, highlighting these elements in text prompts for querying, and decomposing the reasoning process for failure detection and control generation. In experiments, prompt optimizations significantly outperform pre-trained Vision-Language-Action Models in correcting motion-level position errors and improve accuracy by 65.78% compared to VLMs with unoptimized prompts. Additionally, for task-level failures, optimized prompts enhanced the success rate by 5.8%, 5.8%, and 7.5% in VLMs' abilities to detect failures, analyze issues, and generate recovery plans, respectively, across a wide range of unknown errors in Lego assembly.",
        "authors": [
            "Hongyi Chen",
            "Yunchao Yao",
            "Ruixuan Liu",
            "Changliu Liu",
            "Jeffrey Ichnowski"
        ],
        "citations": 1,
        "references": 17,
        "year": 2024
    },
    {
        "title": "Information-Theoretical Principled Trade-off between Jailbreakability and Stealthiness on Vision Language Models",
        "abstract": "In recent years, Vision-Language Models (VLMs) have demonstrated significant advancements in artificial intelligence, transforming tasks across various domains. Despite their capabilities, these models are susceptible to jailbreak attacks, which can compromise their safety and reliability. This paper explores the trade-off between jailbreakability and stealthiness in VLMs, presenting a novel algorithm to detect non-stealthy jailbreak attacks and enhance model robustness. We introduce a stealthiness-aware jailbreak attack using diffusion models, highlighting the challenge of detecting AI-generated content. Our approach leverages Fano's inequality to elucidate the relationship between attack success rates and stealthiness scores, providing an explainable framework for evaluating these threats. Our contributions aim to fortify AI systems against sophisticated attacks, ensuring their outputs remain aligned with ethical standards and user expectations.",
        "authors": [
            "Ching-Chia Kao",
            "Chia-Mu Yu",
            "Chun-Shien Lu",
            "Chu-Song Chen"
        ],
        "citations": 1,
        "references": 54,
        "year": 2024
    },
    {
        "title": "Harnessing the Power of Large Vision Language Models for Synthetic Image Detection",
        "abstract": "In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the potential misuse of these images, including the creation of misleading content such as fake news and propaganda. This study investigates the effectiveness of using advanced vision-language models (VLMs) for synthetic image identification. Specifically, the focus is on tuning state-of-the-art image captioning models for synthetic image detection. By harnessing the robust understanding capabilities of large VLMs, the aim is to distinguish authentic images from synthetic images produced by diffusion-based models. This study contributes to the advancement of synthetic image detection by exploiting the capabilities of visual language models such as BLIP-2 and ViTGPT2. By tailoring image captioning models, we address the challenges associated with the potential misuse of synthetic images in real-world applications. Results described in this paper highlight the promising role of VLMs in the field of synthetic image detection, outperforming conventional image-based detection techniques. Code and models can be found at https://github.com/Mamadou-Keita/VLM-DETECT.",
        "authors": [
            "Mamadou Keita",
            "W. Hamidouche",
            "Hassen Bougueffa",
            "Abdenour Hadid",
            "Abdelmalik Taleb-Ahmed"
        ],
        "citations": 1,
        "references": 24,
        "year": 2024
    },
    {
        "title": "Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning",
        "abstract": "Vision language models (VLMs) have demonstrated impressive performance across a wide range of downstream tasks. However, their proficiency in spatial reasoning remains limited, despite its crucial role in tasks involving navigation and interaction with physical environments. Specifically, most of these tasks rely on the core spatial reasoning capabilities in two-dimensional (2D) environments, and our evaluation reveals that state-of-the-art VLMs frequently generate implausible and incorrect responses to composite spatial reasoning problems, including simple pathfinding tasks that humans can solve effortlessly at a glance. To address this, we explore an effective approach to enhance 2D spatial reasoning within VLMs by training the model solely on basic spatial capabilities. We begin by disentangling the key components of 2D spatial reasoning: direction comprehension, distance estimation, and localization. Our central hypothesis is that mastering these basic spatial capabilities can significantly enhance a model's performance on composite spatial tasks requiring advanced spatial understanding and combinatorial problem-solving, with generalized improvements in visual-spatial tasks. To investigate this hypothesis, we introduce Sparkle, a framework that fine-tunes VLMs on these three basic spatial capabilities by synthetic data generation and targeted supervision to form an instruction dataset for each capability. Our experiments demonstrate that VLMs fine-tuned with Sparkle achieve significant performance gains, not only in the basic tasks themselves but also in generalizing to composite and out-of-distribution spatial reasoning tasks. These findings underscore the effectiveness of mastering basic spatial capabilities in enhancing composite spatial problem-solving, offering insights into systematic strategies for improving VLMs' spatial reasoning capabilities.",
        "authors": [
            "Yihong Tang",
            "Ao Qu",
            "Zhaokai Wang",
            "Dingyi Zhuang",
            "Zhaofeng Wu",
            "Wei Ma",
            "Shenhao Wang",
            "Yunhan Zheng",
            "Zhan Zhao",
            "Jinhua Zhao"
        ],
        "citations": 1,
        "references": 59,
        "year": 2024
    },
    {
        "title": "DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving",
        "abstract": "The advancement of autonomous driving technologies necessitates increasingly sophisticated methods for understanding and predicting real-world scenarios. Vision language models (VLMs) are emerging as revolutionary tools with significant potential to influence autonomous driving. In this paper, we propose the DriveGenVLM framework to generate driving videos and use VLMs to understand them. To achieve this, we employ a video generation framework grounded in denoising diffusion probabilistic models (DDPM) aimed at predicting real-world video sequences. We then explore the adequacy of our generated videos for use in VLMs by employing a pre-trained model known as Efficient In-context Learning on Egocentric Videos (EILEV). The diffusion model is trained with the Waymo open dataset and evaluated using the Fr√©chet Video Distance (FVD) score to ensure the quality and realism of the generated videos. Corresponding narrations are provided by EILEV for these generated videos, which may be beneficial in the autonomous driving domain. These narrations can enhance traffic scene understanding, aid in navigation, and improve planning capabilities. The integration of video generation with VLMs in the DriveGenVLM framework represents a significant step forward in leveraging advanced AI models to address complex challenges in autonomous driving.",
        "authors": [
            "Yongjie Fu",
            "Anmol Jain",
            "Xuan Di",
            "Xu Chen",
            "Zhaobin Mo"
        ],
        "citations": 1,
        "references": 26,
        "year": 2024
    },
    {
        "title": "Backdooring Vision-Language Models with Out-Of-Distribution Data",
        "abstract": "The emergence of Vision-Language Models (VLMs) represents a significant advancement in integrating computer vision with Large Language Models (LLMs) to generate detailed text descriptions from visual inputs. Despite their growing importance, the security of VLMs, particularly against backdoor attacks, is under explored. Moreover, prior works often assume attackers have access to the original training data, which is often unrealistic. In this paper, we address a more practical and challenging scenario where attackers must rely solely on Out-Of-Distribution (OOD) data. We introduce VLOOD (Backdooring Vision-Language Models with Out-of-Distribution Data), a novel approach with two key contributions: (1) demonstrating backdoor attacks on VLMs in complex image-to-text tasks while minimizing degradation of the original semantics under poisoned inputs, and (2) proposing innovative techniques for backdoor injection without requiring any access to the original training data. Our evaluation on image captioning and visual question answering (VQA) tasks confirms the effectiveness of VLOOD, revealing a critical security vulnerability in VLMs and laying the foundation for future research on securing multimodal models against sophisticated threats.",
        "authors": [
            "Weimin Lyu",
            "Jiachen Yao",
            "Saumya Gupta",
            "Lu Pang",
            "Tao Sun",
            "Lingjie Yi",
            "Lijie Hu",
            "Haibin Ling",
            "Chao Chen"
        ],
        "citations": 1,
        "references": 40,
        "year": 2024
    },
    {
        "title": "A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks",
        "abstract": "Recent advancements in Vision-Language Models (VLMs) have enabled complex multimodal tasks by processing text and image data simultaneously, significantly enhancing the field of artificial intelligence. However, these models often exhibit biases that can skew outputs towards societal stereotypes, thus necessitating debiasing strategies. Existing debiasing methods focus narrowly on specific modalities or tasks, and require extensive retraining. To address these limitations, this paper introduces Selective Feature Imputation for Debiasing (SFID), a novel methodology that integrates feature pruning and low confidence imputation (LCI) to effectively reduce biases in VLMs. SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining. Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance. This approach not only enhances the fairness of VLMs applications but also preserves their efficiency and utility across diverse scenarios.",
        "authors": [
            "Hoin Jung",
            "T. Jang",
            "Xiaoqian Wang"
        ],
        "citations": 1,
        "references": 45,
        "year": 2024
    },
    {
        "title": "At First Sight: Zero-Shot Classification of Astronomical Images with Large Multimodal Models",
        "abstract": "Vision-Language multimodal Models (VLMs) offer the possibility for zero-shot classification in astronomy: i.e. classification via natural language prompts, with no training. We investigate two models, GPT-4o and LLaVA-NeXT, for zero-shot classification of low-surface brightness galaxies and artifacts, as well as morphological classification of galaxies. We show that with natural language prompts these models achieved significant accuracy (above 80 percent typically) without additional training/fine tuning. We discuss areas that require improvement, especially for LLaVA-NeXT, which is an open source model. Our findings aim to motivate the astronomical community to consider VLMs as a powerful tool for both research and pedagogy, with the prospect that future custom-built or fine-tuned models could perform better.",
        "authors": [
            "Dimitrios Tanoglidis",
            "Bhuvnesh Jain"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Vision Language Models Are Few-Shot Audio Spectrogram Classifiers",
        "abstract": "We demonstrate that vision language models (VLMs) are capable of recognizing the content in audio recordings when given corresponding spectrogram images. Specifically, we instruct VLMs to perform audio classification tasks in a few-shot setting by prompting them to classify a spectrogram image given example spectrogram images of each class. By carefully designing the spectrogram image representation and selecting good few-shot examples, we show that GPT-4o can achieve 59.00% cross-validated accuracy on the ESC-10 environmental sound classification dataset. Moreover, we demonstrate that VLMs currently outperform the only available commercial audio language model with audio understanding capabilities (Gemini-1.5) on the equivalent audio classification task (59.00% vs. 49.62%), and even perform slightly better than human experts on visual spectrogram classification (73.75% vs. 72.50% on first fold). We envision two potential use cases for these findings: (1) combining the spectrogram and language understanding capabilities of VLMs for audio caption augmentation, and (2) posing visual spectrogram classification as a challenge task for VLMs.",
        "authors": [
            "Satvik Dixit",
            "Laurie M. Heller",
            "Chris Donahue"
        ],
        "citations": 1,
        "references": 14,
        "year": 2024
    },
    {
        "title": "SenCLIP: Enhancing zero-shot land-use mapping for Sentinel-2 with ground-level prompting",
        "abstract": "Pre-trained vision-language models (VLMs), such as CLIP, demonstrate impressive zero-shot classification capabilities with free-form prompts and even show some generalization in specialized domains. However, their performance on satellite imagery is limited due to the underrepresentation of such data in their training sets, which predominantly consist of ground-level images. Existing prompting techniques for satellite imagery are often restricted to generic phrases like a satellite image of ..., limiting their effectiveness for zero-shot land-use and land-cover (LULC) mapping. To address these challenges, we introduce SenCLIP, which transfers CLIPs representation to Sentinel-2 imagery by leveraging a large dataset of Sentinel-2 images paired with geotagged ground-level photos from across Europe. We evaluate SenCLIP alongside other SOTA remote sensing VLMs on zero-shot LULC mapping tasks using the EuroSAT and BigEarthNet datasets with both aerial and ground-level prompting styles. Our approach, which aligns ground-level representations with satellite imagery, demonstrates significant improvements in classification accuracy across both prompt styles, opening new possibilities for applying free-form textual descriptions in zero-shot LULC mapping.",
        "authors": [
            "P. Jain",
            "Dino Ienco",
            "R. Interdonato",
            "Tristan Berchoux",
            "Diego Marcos"
        ],
        "citations": 1,
        "references": 0,
        "year": 2024
    },
    {
        "title": "An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation",
        "abstract": "Radiological services are experiencing unprecedented demand, leading to increased interest in automating radiology report generation. Existing Vision-Language Models (VLMs) suffer from hallucinations, lack interpretability, and require expensive fine-tuning. We introduce SAE-Rad, which uses sparse autoencoders (SAEs) to decompose latent representations from a pre-trained vision transformer into human-interpretable features. Our hybrid architecture combines state-of-the-art SAE advancements, achieving accurate latent reconstructions while maintaining sparsity. Using an off-the-shelf language model, we distil ground-truth reports into radiological descriptions for each SAE feature, which we then compile into a full report for each image, eliminating the need for fine-tuning large models for this task. To the best of our knowledge, SAE-Rad represents the first instance of using mechanistic interpretability techniques explicitly for a downstream multi-modal reasoning task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific metrics compared to state-of-the-art models while using significantly fewer computational resources for training. Qualitative analysis reveals that SAE-Rad learns meaningful visual concepts and generates reports aligning closely with expert interpretations. Our results suggest that SAEs can enhance multimodal reasoning in healthcare, providing a more interpretable alternative to existing VLMs.",
        "authors": [
            "Ahmed Abdulaal",
            "Hugo Fry",
            "Nina Monta√±a Brown",
            "Ayodeji Ijishakin",
            "Jack Gao",
            "Stephanie L. Hyland",
            "Daniel C. Alexander",
            "Daniel C. Castro"
        ],
        "citations": 1,
        "references": 81,
        "year": 2024
    },
    {
        "title": "IndiFoodVQA: Advancing Visual Question Answering and Reasoning with a Knowledge-Infused Synthetic Data Generation Pipeline",
        "abstract": "Large Vision Language Models (VLMs) like GPT-4, LLaVA, and InstructBLIP exhibit extraordinary capabilities for both knowledge understanding and reasoning. However, the reasoning capabilities of such models on sophisticated problems that require external knowledge of a specific domain have not been assessed well, due to the unavailability of necessary datasets. In this work, we release a first-of-its-kind dataset called IndiFoodVQA with around 16.7k data samples, consisting of explicit knowledge-infused questions, answers, and reasons. We also release IndiFoodKG, a related Knowledge Graph (KG) with 79k triples. The data has been created with minimal human intervention via an automated pipeline based on InstructBlip and GPT-3.5. We also present a methodology to extract knowledge from the KG and use it to both answer and reason upon the questions. We employ different models to report baseline zero-shot and fine-tuned results. Fine-tuned VLMs on our data showed an improvement of ~25% over the corresponding base model, highlighting the fact that current VLMs need domain-specific fine-tuning to excel in specialized settings. Our findings reveal that (1) explicit knowledge infusion during question generation helps in making questions that have more grounded knowledge, and (2) proper knowledge retrieval can often lead to better-answering potential in such cases. The data and code is available at https://github.com/SLSravanthi/IndifoodVQA.",
        "authors": [
            "Pulkit Agarwal",
            "S. Sravanthi",
            "Pushpak Bhattacharyya"
        ],
        "citations": 1,
        "references": 62,
        "year": 2024
    },
    {
        "title": "Vision Language Model for Interpretable and Fine-grained Detection of Safety Compliance in Diverse Workplaces",
        "abstract": "Workplace accidents due to personal protective equipment (PPE) non-compliance raise serious safety concerns and lead to legal liabilities, financial penalties, and reputational damage. While object detection models have shown the capability to address this issue by identifying safety items, most existing models, such as YOLO, Faster R-CNN, and SSD, are limited in verifying the fine-grained attributes of PPE across diverse workplace scenarios. Vision language models (VLMs) are gaining traction for detection tasks by leveraging the synergy between visual and textual information, offering a promising solution to traditional object detection limitations in PPE recognition. Nonetheless, VLMs face challenges in consistently verifying PPE attributes due to the complexity and variability of workplace environments, requiring them to interpret context-specific language and visual cues simultaneously. We introduce Clip2Safety, an interpretable detection framework for diverse workplace safety compliance, which comprises four main modules: scene recognition, the visual prompt, safety items detection, and fine-grained verification. The scene recognition identifies the current scenario to determine the necessary safety gear. The visual prompt formulates the specific visual prompts needed for the detection process. The safety items detection identifies whether the required safety gear is being worn according to the specified scenario. Lastly, the fine-grained verification assesses whether the worn safety equipment meets the fine-grained attribute requirements. We conduct real-world case studies across six different scenarios. The results show that Clip2Safety not only demonstrates an accuracy improvement over state-of-the-art question-answering based VLMs but also achieves inference times two hundred times faster.",
        "authors": [
            "Zhiling Chen",
            "Hanning Chen",
            "Mohsen Imani",
            "Ruimin Chen",
            "Farhad Imani"
        ],
        "citations": 1,
        "references": 38,
        "year": 2024
    },
    {
        "title": "Exploring Text-Guided Single Image Editing for Remote Sensing Images",
        "abstract": "Artificial intelligence generative content (AIGC) has significantly impacted image generation in the field of remote sensing. However, the equally important area of remote sensing image (RSI) editing has not received sufficient attention. Deep learning based editing methods generally involve two sequential stages: generation and editing. During the generation stage, consistency in content and details between the original and edited images must be maintained, while in the editing stage, controllability and accuracy of the edits should be ensured. For natural images, these challenges can be tackled by training generative backbones on large-scale benchmark datasets and using text guidance based on vision-language models (VLMs). However, these previously effective approaches become less viable for RSIs due to two reasons: First, existing generative RSI benchmark datasets do not fully capture the diversity of remote sensing scenarios, particularly in terms of variations in sensors, object types, and resolutions. Consequently, the generalization capacity of the trained backbone model is often inadequate for universal editing tasks on RSIs. Second, the large spatial resolution of RSIs exacerbates the problem in VLMs where a single text semantic corresponds to multiple image semantics, leading to the introduction of incorrect semantics when using text to guide RSI editing. To solve above problems, this paper proposes a text-guided RSI editing method that is controllable but stable, and can be trained using only a single image. It adopts a multi-scale training approach to preserve consistency without the need for training on extensive benchmark datasets, while leveraging RSI pre-trained VLMs and prompt ensembling (PE) to ensure accuracy and controllability in the text-guided editing process.",
        "authors": [
            "Fangzhou Han",
            "Lingyu Si",
            "Hongwei Dong",
            "Lamei Zhang",
            "Hao Chen",
            "Bo Du"
        ],
        "citations": 1,
        "references": 64,
        "year": 2024
    },
    {
        "title": "Soft Prompt Generation for Domain Generalization",
        "abstract": "Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.",
        "authors": [
            "Shuanghao Bai",
            "Yuedi Zhang",
            "Wanqi Zhou",
            "Zhirong Luan",
            "Badong Chen"
        ],
        "citations": 1,
        "references": 40,
        "year": 2024
    },
    {
        "title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation",
        "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a parameter-efficient method for adapting VLMs, but state-of-the-art approaches require annotated samples. In this paper we propose a novel approach to prompt learning based on unsupervised knowledge distillation from more powerful models. Our approach, which we call Knowledge Distillation Prompt Learning (KDPL), can be integrated into existing prompt learning techniques and eliminates the need for labeled examples during adaptation. Our experiments on more than ten standard benchmark datasets demonstrate that KDPL is very effective at improving generalization of learned prompts for zero-shot domain generalization, zero-shot cross-dataset generalization, and zero-shot base-to-novel class generalization problems. KDPL requires no ground-truth labels for adaptation, and moreover we show that even in the absence of any knowledge of training class names it can be used to effectively transfer knowledge. The code is publicly available at https://github.com/miccunifi/KDPL.",
        "authors": [
            "Marco Mistretta",
            "Alberto Baldrati",
            "M. Bertini",
            "Andrew D. Bagdanov"
        ],
        "citations": 1,
        "references": 68,
        "year": 2024
    },
    {
        "title": "Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem",
        "abstract": "Recent work has documented striking heterogeneity in the performance of state-of-the-art vision language models (VLMs), including both multimodal language models and text-to-image models. These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks -- such as counting, localization, and simple forms of visual analogy -- that humans perform with near perfect accuracy. To better understand this puzzling pattern of successes and failures, we turn to theoretical accounts of the binding problem in cognitive science and neuroscience, a fundamental problem that arises when a shared set of representational resources must be used to represent distinct entities (e.g., to represent multiple objects in an image), necessitating the use of serial processing to avoid interference. We find that many of the puzzling failures of state-of-the-art VLMs can be explained as arising due to the binding problem, and that these failure modes are strikingly similar to the limitations exhibited by rapid, feedforward processing in the human brain.",
        "authors": [
            "Declan Campbell",
            "Sunayana Rane",
            "Tyler Giallanza",
            "Nicol√≤ De Sabbata",
            "Kia Ghods",
            "Amogh Joshi",
            "Alexander Ku",
            "Steven M. Frankland",
            "Thomas L. Griffiths",
            "Jonathan D. Cohen",
            "Taylor Webb"
        ],
        "citations": 1,
        "references": 40,
        "year": 2024
    },
    {
        "title": "VisualRWKV-HD and UHD: Advancing High-Resolution Processing for Visual Language Models",
        "abstract": "Accurately understanding complex visual information is crucial for visual language models (VLMs). Enhancing image resolution can improve visual perception capabilities, not only reducing hallucinations but also boosting performance in tasks that demand high resolution, such as text-rich or document analysis. In this paper, we present VisualRWKV-HD and VisualRWKV-UHD, two advancements in the VisualRWKV model family, specifically designed to process high-resolution visual inputs. For VisualRWKV-HD, we developed a lossless downsampling method to effectively integrate a high-resolution vision encoder with low-resolution encoders, without extending the input sequence length. For the VisualRWKV-UHD model, we enhanced image representation by dividing the image into four segments, which are then recombined with the original image. This technique allows the model to incorporate both high-resolution and low-resolution features, effectively balancing coarse and fine-grained information. As a result, the model supports resolutions up to 4096 x 4096 pixels, offering a more detailed and comprehensive visual processing capability. Both VisualRWKV-HD and VisualRWKV-UHD not only achieve strong results on VLM benchmarks but also show marked improvements in performance for text-rich tasks.",
        "authors": [
            "Zihang Li",
            "Haowen Hou"
        ],
        "citations": 1,
        "references": 21,
        "year": 2024
    },
    {
        "title": "DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization",
        "abstract": "Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data. With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains. With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.",
        "authors": [
            "Feng Hou",
            "Jin Yuan",
            "Ying Yang",
            "Yang Liu",
            "Yang Zhang",
            "Cheng Zhong",
            "Zhongchao Shi",
            "Jianping Fan",
            "Yong Rui",
            "Zhiqiang He"
        ],
        "citations": 1,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers",
        "abstract": "Recent advancements in vision-language models (VLMs) have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. In the widely used fully autoregressive transformer-based models like LLaVA, projected visual tokens are prepended to textual tokens. Oftentimes, visual tokens are significantly more than prompt tokens, resulting in increased computational overhead during both training and inference. In this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower of VLMs. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance. In our experiment, with merely 8 visual registers--about 1% of the original tokens--Victor shows less than a 4% accuracy drop while reducing the total training time by 43% and boosting the inference throughput by 3.3X.",
        "authors": [
            "Yuxin Wen",
            "Qingqing Cao",
            "Qichen Fu",
            "Sachin Mehta",
            "Mahyar Najibi"
        ],
        "citations": 1,
        "references": 47,
        "year": 2024
    },
    {
        "title": "A mRNA Vaccine for Crimean‚ÄìCongo Hemorrhagic Fever Virus Expressing Non-Fusion GnGc Using NSm Linker Elicits Unexpected Immune Responses in Mice",
        "abstract": "Crimean‚ÄìCongo hemorrhagic fever (CCHF), caused by Crimean‚ÄìCongo Hemorrhagic virus (CCHFV), is listed in the World Health Organization‚Äôs list of priority diseases. The high fatality rate in humans, the widespread distribution of CCHFV, and the lack of approved specific vaccines are the primary concerns regarding this disease. We used microfluidic technology to optimize the mRNA vaccine delivery system and demonstrated that vaccination with nucleoside-modified CCHFV mRNA vaccines encoding GnNSmGc (vLMs), Gn (vLMn), or Gc (vLMc) induced different immune responses. We found that both T-cell and B-cell immune responses induced by vLMc were better than those induced by vLMn. Interestingly, immune responses were found to be lower for vLMs, which employed NSm to link Gn and Gc for non-fusion expression, compared to those for vLMc. In conclusion, our results indicated that NSm could be a factor that leads to decreased specific immune responses in the host and should be avoided in the development of CCHFV vaccine antigens.",
        "authors": [
            "Tong Chen",
            "Zhe Ding",
            "Xuejie Li",
            "Yingwen Li",
            "Jiaming Lan",
            "Gary Wong"
        ],
        "citations": 1,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Towards Foundation Models for 3D Vision: How Close Are We?",
        "abstract": "Building a foundation model for 3D vision is a complex challenge that remains unsolved. Towards that goal, it is important to understand the 3D reasoning capabilities of current models as well as identify the gaps between these models and humans. Therefore, we construct a new 3D visual understanding benchmark named UniQA-3D. UniQA-3D covers fundamental 3D vision tasks in the Visual Question Answering (VQA) format. We evaluate state-of-the-art Vision-Language Models (VLMs), specialized models, and human subjects on it. Our results show that VLMs generally perform poorly, while the specialized models are accurate but not robust, failing under geometric perturbations. In contrast, human vision continues to be the most reliable 3D visual system. We further demonstrate that neural networks align more closely with human 3D vision mechanisms compared to classical computer vision methods, and Transformer-based networks such as ViT align more closely with human 3D vision mechanisms than CNNs. We hope our study will benefit the future development of foundation models for 3D vision. Code is available at https://github.com/princeton-vl/UniQA-3D .",
        "authors": [
            "Yiming Zuo",
            "Karhan Kayan",
            "Maggie Wang",
            "Kevin Jeon",
            "Jia Deng",
            "Thomas L. Griffiths"
        ],
        "citations": 1,
        "references": 96,
        "year": 2024
    },
    {
        "title": "DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization",
        "abstract": "Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data. With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains. With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.",
        "authors": [
            "Feng Hou",
            "Jin Yuan",
            "Ying Yang",
            "Yang Liu",
            "Yang Zhang",
            "Cheng Zhong",
            "Zhongchao Shi",
            "Jianping Fan",
            "Yong Rui",
            "Zhiqiang He"
        ],
        "citations": 1,
        "references": 42,
        "year": 2024
    },
    {
        "title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation",
        "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a parameter-efficient method for adapting VLMs, but state-of-the-art approaches require annotated samples. In this paper we propose a novel approach to prompt learning based on unsupervised knowledge distillation from more powerful models. Our approach, which we call Knowledge Distillation Prompt Learning (KDPL), can be integrated into existing prompt learning techniques and eliminates the need for labeled examples during adaptation. Our experiments on more than ten standard benchmark datasets demonstrate that KDPL is very effective at improving generalization of learned prompts for zero-shot domain generalization, zero-shot cross-dataset generalization, and zero-shot base-to-novel class generalization problems. KDPL requires no ground-truth labels for adaptation, and moreover we show that even in the absence of any knowledge of training class names it can be used to effectively transfer knowledge. The code is publicly available at https://github.com/miccunifi/KDPL.",
        "authors": [
            "Marco Mistretta",
            "Alberto Baldrati",
            "M. Bertini",
            "Andrew D. Bagdanov"
        ],
        "citations": 1,
        "references": 68,
        "year": 2024
    },
    {
        "title": "BlenderAlchemy: Editing 3D Graphics with Vision-Language Models",
        "abstract": "Graphics design is important for various applications, including movie production and game design. To create a high-quality scene, designers usually need to spend hours in software like Blender, in which they might need to interleave and repeat operations, such as connecting material nodes, hundreds of times. Moreover, slightly different design goals may require completely different sequences, making automation difficult. In this paper, we propose a system that leverages Vision-Language Models (VLMs), like GPT-4V, to intelligently search the design action space to arrive at an answer that can satisfy a user's intent. Specifically, we design a vision-based edit generator and state evaluator to work together to find the correct sequence of actions to achieve the goal. Inspired by the role of visual imagination in the human design process, we supplement the visual reasoning capabilities of VLMs with\"imagined\"reference images from image-generation models, providing visual grounding of abstract language descriptions. In this paper, we provide empirical evidence suggesting our system can produce simple but tedious Blender editing sequences for tasks such as editing procedural materials and geometry from text and/or reference images, as well as adjusting lighting configurations for product renderings in complex scenes.",
        "authors": [
            "Ian Huang",
            "Guandao Yang",
            "Leonidas J. Guibas"
        ],
        "citations": 1,
        "references": 69,
        "year": 2024
    },
    {
        "title": "A mRNA Vaccine for Crimean‚ÄìCongo Hemorrhagic Fever Virus Expressing Non-Fusion GnGc Using NSm Linker Elicits Unexpected Immune Responses in Mice",
        "abstract": "Crimean‚ÄìCongo hemorrhagic fever (CCHF), caused by Crimean‚ÄìCongo Hemorrhagic virus (CCHFV), is listed in the World Health Organization‚Äôs list of priority diseases. The high fatality rate in humans, the widespread distribution of CCHFV, and the lack of approved specific vaccines are the primary concerns regarding this disease. We used microfluidic technology to optimize the mRNA vaccine delivery system and demonstrated that vaccination with nucleoside-modified CCHFV mRNA vaccines encoding GnNSmGc (vLMs), Gn (vLMn), or Gc (vLMc) induced different immune responses. We found that both T-cell and B-cell immune responses induced by vLMc were better than those induced by vLMn. Interestingly, immune responses were found to be lower for vLMs, which employed NSm to link Gn and Gc for non-fusion expression, compared to those for vLMc. In conclusion, our results indicated that NSm could be a factor that leads to decreased specific immune responses in the host and should be avoided in the development of CCHFV vaccine antigens.",
        "authors": [
            "Tong Chen",
            "Zhe Ding",
            "Xuejie Li",
            "Yingwen Li",
            "Jiaming Lan",
            "Gary Wong"
        ],
        "citations": 1,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Exploring Text-Guided Single Image Editing for Remote Sensing Images",
        "abstract": "Artificial intelligence generative content (AIGC) has significantly impacted image generation in the field of remote sensing. However, the equally important area of remote sensing image (RSI) editing has not received sufficient attention. Deep learning based editing methods generally involve two sequential stages: generation and editing. During the generation stage, consistency in content and details between the original and edited images must be maintained, while in the editing stage, controllability and accuracy of the edits should be ensured. For natural images, these challenges can be tackled by training generative backbones on large-scale benchmark datasets and using text guidance based on vision-language models (VLMs). However, these previously effective approaches become less viable for RSIs due to two reasons: First, existing generative RSI benchmark datasets do not fully capture the diversity of remote sensing scenarios, particularly in terms of variations in sensors, object types, and resolutions. Consequently, the generalization capacity of the trained backbone model is often inadequate for universal editing tasks on RSIs. Second, the large spatial resolution of RSIs exacerbates the problem in VLMs where a single text semantic corresponds to multiple image semantics, leading to the introduction of incorrect semantics when using text to guide RSI editing. To solve above problems, this paper proposes a text-guided RSI editing method that is controllable but stable, and can be trained using only a single image. It adopts a multi-scale training approach to preserve consistency without the need for training on extensive benchmark datasets, while leveraging RSI pre-trained VLMs and prompt ensembling (PE) to ensure accuracy and controllability in the text-guided editing process.",
        "authors": [
            "Fangzhou Han",
            "Lingyu Si",
            "Hongwei Dong",
            "Lamei Zhang",
            "Hao Chen",
            "Bo Du"
        ],
        "citations": 1,
        "references": 64,
        "year": 2024
    },
    {
        "title": "Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers",
        "abstract": "Recent advancements in vision-language models (VLMs) have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. In the widely used fully autoregressive transformer-based models like LLaVA, projected visual tokens are prepended to textual tokens. Oftentimes, visual tokens are significantly more than prompt tokens, resulting in increased computational overhead during both training and inference. In this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower of VLMs. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance. In our experiment, with merely 8 visual registers--about 1% of the original tokens--Victor shows less than a 4% accuracy drop while reducing the total training time by 43% and boosting the inference throughput by 3.3X.",
        "authors": [
            "Yuxin Wen",
            "Qingqing Cao",
            "Qichen Fu",
            "Sachin Mehta",
            "Mahyar Najibi"
        ],
        "citations": 1,
        "references": 47,
        "year": 2024
    },
    {
        "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models",
        "abstract": "Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. Due to extensive training on vast amounts of human-curated data, LLMs and VLMs excel in common-sense reasoning tasks, however still struggle with more complex reasoning that demands deeper cognitive understanding. We introduce NTSEBench, a new dataset designed to evaluate cognitive multi-modal reasoning and problem-solving skills of large models. The dataset contains 2728 multiple-choice questions, accompanied by a total of 4,642 images, categorized into 26 different types. These questions are drawn from the nationwide NTSE examination in India and feature a mix of visual and textual general aptitude challenges, designed to assess intelligence and critical thinking skills beyond mere rote learning. We establish baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison between open source and propriety models, we propose four distinct modeling strategies to handle different modalities -- text and images -- in the dataset instances.",
        "authors": [
            "Pranshu Pandya",
            "Agney S Talwarr",
            "Vatsal Gupta",
            "Tushar Kataria",
            "Vivek Gupta",
            "Dan Roth"
        ],
        "citations": 1,
        "references": 70,
        "year": 2024
    },
    {
        "title": "Soft Prompt Generation for Domain Generalization",
        "abstract": "Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.",
        "authors": [
            "Shuanghao Bai",
            "Yuedi Zhang",
            "Wanqi Zhou",
            "Zhirong Luan",
            "Badong Chen"
        ],
        "citations": 1,
        "references": 40,
        "year": 2024
    },
    {
        "title": "Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem",
        "abstract": "Recent work has documented striking heterogeneity in the performance of state-of-the-art vision language models (VLMs), including both multimodal language models and text-to-image models. These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks -- such as counting, localization, and simple forms of visual analogy -- that humans perform with near perfect accuracy. To better understand this puzzling pattern of successes and failures, we turn to theoretical accounts of the binding problem in cognitive science and neuroscience, a fundamental problem that arises when a shared set of representational resources must be used to represent distinct entities (e.g., to represent multiple objects in an image), necessitating the use of serial processing to avoid interference. We find that many of the puzzling failures of state-of-the-art VLMs can be explained as arising due to the binding problem, and that these failure modes are strikingly similar to the limitations exhibited by rapid, feedforward processing in the human brain.",
        "authors": [
            "Declan Campbell",
            "Sunayana Rane",
            "Tyler Giallanza",
            "Nicol√≤ De Sabbata",
            "Kia Ghods",
            "Amogh Joshi",
            "Alexander Ku",
            "Steven M. Frankland",
            "Thomas L. Griffiths",
            "Jonathan D. Cohen",
            "Taylor Webb"
        ],
        "citations": 1,
        "references": 40,
        "year": 2024
    },
    {
        "title": "GRS: Generating Robotic Simulation Tasks from Real-World Images",
        "abstract": "We introduce GRS (Generating Robotic Simulation tasks), a novel system to address the challenge of real-to-sim in robotics, computer vision, and AR/VR. GRS enables the creation of digital twin simulations from single real-world RGB-D observations, complete with diverse, solvable tasks for virtual agent training. We use state-of-the-art vision-language models (VLMs) to achieve a comprehensive real-to-sim pipeline. GRS operates in three stages: 1) scene comprehension using SAM2 for object segmentation and VLMs for object description, 2) matching identified objects with simulation-ready assets, and 3) generating contextually appropriate robotic tasks. Our approach ensures simulations align with task specifications by generating test suites designed to verify adherence to the task specification. We introduce a router that iteratively refines the simulation and test code to ensure the simulation is solvable by a robot policy while remaining aligned to the task specification. Our experiments demonstrate the system's efficacy in accurately identifying object correspondence, which allows us to generate task environments that closely match input environments, and enhance automated simulation task generation through our novel router mechanism.",
        "authors": [
            "Alex Zook",
            "Fan-Yun Sun",
            "Josef Spjut",
            "Valts Blukis",
            "Stanley T. Birchfield",
            "Jonathan Tremblay"
        ],
        "citations": 1,
        "references": 32,
        "year": 2024
    },
    {
        "title": "Visual Prompt Engineering for Medical Vision Language Models in Radiology",
        "abstract": "Medical image classification in radiology faces significant challenges, particularly in generalizing to unseen pathologies. In contrast, CLIP offers a promising solution by leveraging multimodal learning to improve zero-shot classification performance. However, in the medical domain, lesions can be small and might not be well represented in the embedding space. Therefore, in this paper, we explore the potential of visual prompt engineering to enhance the capabilities of Vision Language Models (VLMs) in radiology. Leveraging BiomedCLIP, trained on extensive biomedical image-text pairs, we investigate the impact of embedding visual markers directly within radiological images to guide the model's attention to critical regions. Our evaluation on the JSRT dataset, focusing on lung nodule malignancy classification, demonstrates that incorporating visual prompts $\\unicode{x2013}$ such as arrows, circles, and contours $\\unicode{x2013}$ significantly improves classification metrics including AUROC, AUPRC, F1 score, and accuracy. Moreover, the study provides attention maps, showcasing enhanced model interpretability and focus on clinically relevant areas. These findings underscore the efficacy of visual prompt engineering as a straightforward yet powerful approach to advance VLM performance in medical image analysis.",
        "authors": [
            "Stefan Denner",
            "Markus Bujotzek",
            "Dimitrios Bounias",
            "David Zimmerer",
            "Raphael Stock",
            "Paul F. J√§ger",
            "Klaus H. Maier-Hein"
        ],
        "citations": 1,
        "references": 16,
        "year": 2024
    },
    {
        "title": "CLIP-Count: Towards Text-Guided Zero-Shot Object Counting",
        "abstract": "Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to downstream tasks such as object detection and segmentation. Adapting these models for object counting, however, remains a formidable challenge. In this study, we first investigate transferring vision-language models (VLMs) for class-agnostic object counting. Specifically, we propose CLIP-Count, the first end-to-end pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner. To align the text embedding with dense visual features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level visual representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module to propagate semantic information across different resolution levels of visual features. Benefiting from the full exploitation of the rich image-text alignment knowledge of pretrained VLMs, our method effectively generates high-quality density maps for objects-of-interest. Extensive experiments on FSC-147, CARPK, and ShanghaiTech crowd counting datasets demonstrate state-of-the-art accuracy and generalizability of the proposed method. Code is available: https://github.com/songrise/CLIP-Count. https://github.com/songrise/CLIP-Count.",
        "authors": [
            "Ruixia Jiang",
            "Lin Liu",
            "Changan Chen"
        ],
        "citations": 45,
        "references": 52,
        "year": 2023
    },
    {
        "title": "DeAR: Debiasing Vision-Language Models with Additive Residuals",
        "abstract": "Large pre-trained vision-language models (VLMs) reduce the time for developing predictive models for various vision-grounded language downstream tasks by providing rich, adaptable image and text representations. However, these models suffer from societal biases owing to the skewed distribution of various identity groups in the training data. These biases manifest as the skewed similarity between the representations for specific text concepts and images of people of different identity groups and, therefore, limit the usefulness of such models in real-world high-stakes applications. In this work, we present Dear(Debiasing with Additive Residuals), a novel debiasing method that learns additive residual image representations to offset the original representations, ensuring fair output representations. In doing so, it reduces the ability of the representations to distinguish between the different identity groups. Further, we observe that the current fairness tests are performed on limited face image datasets that fail to indicate why a specific text concept should/should not apply to them. To bridge this gap and better evaluate Dear,we introduce the Protected Attribute Tag Association (pata)dataset - a new context-based bias benchmarking dataset for evaluating the fairness of large pre-trained VLMs. Additionally, Pataprovides visual context for a diverse human population in different scenarios with both positive and negative connotations. Experimental results for fairness and zero-shot performance preservation using multiple datasets demonstrate the efficacy of our framework. The dataset is released here.",
        "authors": [
            "Ashish Seth",
            "Mayur Hemani",
            "Chirag Agarwal"
        ],
        "citations": 43,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Raising the Bar of AI-generated Image Detection with CLIP",
        "abstract": "The aim of this work is to explore the potential of pre-trained vision-language models (VLMs) for universal detection of AI-generated images. We develop a lightweight detection strategy based on CLIP features and study its performance in a wide variety of challenging scenarios. We find that, contrary to previous beliefs, it is neither necessary nor convenient to use a large domain-specific dataset for training. On the contrary, by using only a handful of example images from a single generative model, a CLIP-based detector exhibits surprising generalization ability and high robustness across different architectures, including recent commercial tools such as Dalle-3, Midjourney v5, and Firefly. We match the state-of-the-art (SoTA) on in-distribution data and significantly improve upon it in terms of generalization to out-of-distribution data (+6% AUC) and robustness to impaired/laundered data (+13%). Our project is available at https://grip-unina.github.io/ClipBased-SyntheticImageDetection/",
        "authors": [
            "D. Cozzolino",
            "G. Poggi",
            "Riccardo Corvi",
            "Matthias Nie√üner",
            "L. Verdoliva"
        ],
        "citations": 43,
        "references": 87,
        "year": 2023
    },
    {
        "title": "RoboCLIP: One Demonstration is Enough to Learn Robot Policies",
        "abstract": "Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, doing so using only one video/text demonstration.",
        "authors": [
            "S. Sontakke",
            "Jesse Zhang",
            "S'ebastien M. R. Arnold",
            "Karl Pertsch",
            "Erdem Biyik",
            "Dorsa Sadigh",
            "Chelsea Finn",
            "Laurent Itti"
        ],
        "citations": 44,
        "references": 46,
        "year": 2023
    },
    {
        "title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts",
        "abstract": "Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~ 7%), SUN397 (~ 4.6%), and CUB ( ~3.3%) when compared to CLIP‚Äôs default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets. The code, prompts, and auxiliary text dataset is available at github.com/mayug/VDT-Adapter.",
        "authors": [
            "Mayug Maniparambil",
            "Chris Vorster",
            "D. Molloy",
            "N. Murphy",
            "Kevin McGuinness",
            "Noel E. O'Connor"
        ],
        "citations": 39,
        "references": 58,
        "year": 2023
    },
    {
        "title": "Equivariant Similarity for Vision-Language Foundation Models",
        "abstract": "This study explores the concept of equivariance in vision-language foundation models (VLMs), focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. Unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched pairs as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. This allows VLMs to generalize better to nuanced and unseen multimodal compositions. However, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. For example, given an image-text pair about a dog, it is unclear to what extent the similarity changes when the pixel is changed from dog to cat? To this end, we propose EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we present a new challenging benchmark EqBen. Compared to the existing evaluation sets, EqBen is the first to focus on \"visual-minimal change\". Extensive experiments show the lack of equivariance in current VLMs1 and validate the effectiveness of EqSim2.",
        "authors": [
            "Tan Wang",
            "Kevin Lin",
            "Linjie Li",
            "Chung-Ching Lin",
            "Zhengyuan Yang",
            "Hanwang Zhang",
            "Zicheng Liu",
            "Lijuan Wang"
        ],
        "citations": 41,
        "references": 83,
        "year": 2023
    },
    {
        "title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models",
        "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising billions of image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-explored. While few works have recently explored LLMs-based conversational medical models, they mainly focus on text-based analysis. In this paper, we introduce XrayGPT, a conversational medical vision-language (VLMs) model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder with a fine-tuned LLM to possess visual conversation abilities, grounded in an understanding of radiographs and medical knowledge. For improved alignment of chest radiograph data, we generate ~217k interactive and high-quality summaries from free-text radiology reports. Extensive experiments are conducted to validate the merits of XrayGPT. To conduct an expert evaluation, certified medical doctors evaluated the output of our XrayGPT on a test subset and the results reveal that more than 70% of the responses are scientifically accurate, with an average score of 4/5. We hope our simple and effective method establishes a solid baseline, facilitating future research toward automated analysis and summarization of chest radiographs. Code, models, and instruction sets will be publicly released.",
        "authors": [
            "Omkar Thawakar",
            "Abdelrahman M. Shaker",
            "Sahal Shaji Mullappilly",
            "Hisham Cholakkal",
            "R. Anwer",
            "Salman Siddique Khan",
            "J. Laaksonen",
            "F. Khan"
        ],
        "citations": 43,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Fine-Grained Visual Prompting",
        "abstract": "Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive zero-shot transfer capabilities in image-level visual perception. However, these models have shown limited performance in instance-level tasks that demand precise localization and recognition. Previous works have suggested that incorporating visual prompts, such as colorful boxes or circles, can improve the ability of models to recognize objects of interest. Nonetheless, compared to language prompting, visual prompting designs are rarely explored. Existing approaches, which employ coarse visual cues such as colorful boxes or circles, often result in sub-optimal performance due to the inclusion of irrelevant and noisy pixels. In this paper, we carefully study the visual prompting designs by exploring more fine-grained markings, such as segmentation masks and their variations. In addition, we introduce a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting. Consequently, our investigation reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting strategy leverages the precise mask annotations to reduce focus on weakly related regions while retaining spatial coherence between the target and the surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates superior performance in zero-shot comprehension of referring expressions on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an average margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the RefCOCO+ testA subset. Code is available at https://github.com/ylingfeng/FGVP.",
        "authors": [
            "Lingfeng Yang",
            "Yueze Wang",
            "Xiang Li",
            "Xinlong Wang",
            "Jian Yang"
        ],
        "citations": 43,
        "references": 69,
        "year": 2023
    },
    {
        "title": "Scaling Vision-Language Models with Sparse Mixture of Experts",
        "abstract": "The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of multimedia data. However, as these models become larger and more complex, they also become more challenging to train and deploy. One approach to addressing this challenge is the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the model into smaller, specialized sub-models that can jointly solve a task. In this paper, we explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. Our research offers valuable insights into stabilizing the training of MoE models, understanding the impact of MoE on model interpretability, and balancing the trade-offs between compute performance when scaling VLMs. We hope our work will inspire further research into the use of MoE for scaling large-scale vision-language models and other multimodal machine learning applications.",
        "authors": [
            "Sheng Shen",
            "Z. Yao",
            "Chunyuan Li",
            "Trevor Darrell",
            "K. Keutzer",
            "Yuxiong He"
        ],
        "citations": 49,
        "references": 96,
        "year": 2023
    },
    {
        "title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning",
        "abstract": "Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.",
        "authors": [
            "Yang Bai",
            "Yang Zhou",
            "Jun Zhou",
            "R. Goh",
            "Daniel Shu Wei Ting",
            "Yong Liu"
        ],
        "citations": 0,
        "references": 61,
        "year": 2024
    },
    {
        "title": "Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?",
        "abstract": "Large vision-language models (VLMs) have become state-of-the-art for many computer vision tasks, with in-context learning (ICL) as a popular adaptation strategy for new ones. But can VLMs learn novel concepts purely from visual demonstrations, or are they limited to adapting to the output format of ICL examples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks (SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks in-context. We find that VLMs fail to do this zero-shot, and sometimes continue to fail after finetuning. However, adding simpler data to the training by curriculum learning leads to improved ICL performance.",
        "authors": [
            "Bowen Zhao",
            "Leo Parker Dirac",
            "Paulina Varshavskaya"
        ],
        "citations": 0,
        "references": 34,
        "year": 2024
    },
    {
        "title": "Interleaved-Modal Chain-of-Thought",
        "abstract": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to produce a series of intermediate reasoning steps before arriving at the final answer. However, when transitioning to vision-language models (VLMs), their text-only rationales struggle to express the fine-grained associations with the original image. In this paper, we propose an image-incorporated multimodal Chain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)}, which generates sequential reasoning steps consisting of paired visual and textual rationales to infer the final answer. Intuitively, the novel ICoT requires VLMs to enable the generation of fine-grained interleaved-modal content, which is hard for current VLMs to fulfill. Considering that the required visual information is usually part of the input image, we propose \\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs. ADS intelligently inserts regions of the input image to generate the interleaved-modal reasoning steps with ignorable additional latency. ADS relies solely on the attention map of VLMs without the need for parameterization, and therefore it is a plug-and-play strategy that can be generalized to a spectrum of VLMs. We apply ADS to realize ICoT on two popular VLMs of different architectures. Extensive evaluations of three benchmarks have shown that ICoT prompting achieves substantial performance (up to 14\\%) and interpretability improvements compared to existing multimodal CoT prompting methods.",
        "authors": [
            "Jun Gao",
            "Yongqing Li",
            "Ziqiang Cao",
            "Wenjie Li"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Can VLMs be used on videos for action recognition? LLMs are Visual Reasoning Coordinators",
        "abstract": "Recent advancements have introduced multiple vision-language models (VLMs) demonstrating impressive commonsense reasoning across various domains. Despite their individual capabilities, the potential of synergizing these complementary VLMs remains underexplored. The Cola Framework addresses this by showcasing how a large language model (LLM) can efficiently coordinate multiple VLMs through natural language communication, leveraging their distinct strengths. We have verified this claim on the challenging A-OKVQA dataset, confirming the effectiveness of such coordination. Building on this, our study investigates whether the same methodology can be applied to surveillance videos for action recognition. Specifically, we explore if leveraging the combined knowledge base of VLMs and LLM can effectively deduce actions from a video when presented with only a few selectively important frames and minimal temporal information. Our experiments demonstrate that LLM, when coordinating different VLMs, can successfully recognize patterns and deduce actions in various scenarios despite the weak temporal signals. However, our findings suggest that to enhance this approach as a viable alternative solution, integrating a stronger temporal signal and exposing the models to slightly more frames would be beneficial.",
        "authors": [
            "Harsh Lunia"
        ],
        "citations": 0,
        "references": 19,
        "year": 2024
    },
    {
        "title": "MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models",
        "abstract": "Warning:This paper contains unsafe model responses. As deep learning advances, Large Language Models (LLMs) and their multimodal counter-parts, Vision-Language Models (VLMs), have shown exceptional performance in many real-world tasks. However, VLMs face significant security challenges, such as jailbreak attacks, where attackers attempt to bypass the model‚Äôs safety alignment to elicit harmful responses. The threat of jailbreak attacks on VLMs arises from both the inherent vulnerabilities of LLMs and the multiple information channels that VLMs process. While various attacks and defenses have been proposed, there is a notable gap in unified and comprehensive evaluations, as each method is evaluated on different dataset and metrics, making it impossible to compare the effectiveness of each method. To address this gap, we introduce MMJ-Bench , a unified pipeline for evaluating jailbreak attacks and defense techniques for VLMs. Through extensive experiments, we assess the effectiveness of various attack methods against SoTA VLMs and evaluate the impact of defense mechanisms on both defense effectiveness and model utility for normal tasks. Our comprehensive evaluation contribute to the field by offering a unified and systematic evaluation framework and the first public-available benchmark for VLM jailbreak research. We also demonstrate several insightful findings that highlights directions for future studies.",
        "authors": [
            "Fenghua Weng",
            "Yue Xu",
            "Chengyan Fu",
            "Wenjie Wang"
        ],
        "citations": 0,
        "references": 29,
        "year": 2024
    },
    {
        "title": "Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model's Prediction Rationality",
        "abstract": "Vision-Language Models (VLMs), such as CLIP, have already seen widespread applications. Researchers actively engage in further fine-tuning VLMs in safety-critical domains. In these domains, prediction rationality is crucial: the prediction should be correct and based on valid evidence. Yet, for VLMs, the impact of fine-tuning on prediction rationality is seldomly investigated. To study this problem, we proposed two new metrics called Prediction Trustworthiness and Inference Reliability. We conducted extensive experiments on various settings and observed some interesting phenomena. On the one hand, we found that the well-adopted fine-tuning methods led to more correct predictions based on invalid evidence. This potentially undermines the trustworthiness of correct predictions from fine-tuned VLMs. On the other hand, having identified valid evidence of target objects, fine-tuned VLMs were more likely to make correct predictions. Moreover, the findings are also consistent under distributional shifts and across various experimental settings. We hope our research offer fresh insights to VLM fine-tuning.",
        "authors": [
            "Qitong Wang",
            "Tang Li",
            "Kien X. Nguyen",
            "Xi Peng"
        ],
        "citations": 0,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Rethinking Pruning for Vision-Language Models: Strategies for Effective Sparsity and Performance Restoration",
        "abstract": "Vision-Language Models (VLMs) integrate information from multiple modalities and have shown remarkable success across various tasks. However, deploying large-scale VLMs in resource-constrained scenarios is challenging. Pruning followed by finetuning offers a potential solution but remains underexplored for VLMs. This study addresses two key questions: how to distribute sparsity across different modality-specific models, and how to restore the performance of pruned sparse VLMs. Our preliminary studies identified two effective pruning settings: applying the same sparsity to both vision and language models, and pruning only the language models. While LoRA finetuning aims to restore sparse models, it faces challenges due to incompatibility with sparse models, disrupting the pruned sparsity. To overcome these issues, we propose SparseLoRA, which applies sparsity directly to LoRA weights. Our experimental results demonstrate significant improvements, including an 11.3\\% boost under 2:4 sparsity and a 47.6\\% enhancement under unstructured 70\\% sparsity. Code is released at: \\url{https://github.com/Shwai-He/VLM-Compression}.",
        "authors": [
            "Shwai He",
            "Tianlong Chen"
        ],
        "citations": 0,
        "references": 52,
        "year": 2024
    },
    {
        "title": "Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models",
        "abstract": "Large language models (LLMs) have increased interest in vision language models (VLMs), which process image-text pairs as input. Studies investigating the visual understanding ability of VLMs have been proposed, but such studies are still preliminary because existing datasets do not permit a comprehensive evaluation of the fine-grained visual linguistic abilities of VLMs across multiple languages. To further explore the strengths of VLMs, such as GPT-4V \\cite{openai2023GPT4}, we developed new datasets for the systematic and qualitative analysis of VLMs. Our contribution is four-fold: 1) we introduced nine vision-and-language (VL) tasks (including object recognition, image-text matching, and more) and constructed multilingual visual-text datasets in four languages: English, Japanese, Swahili, and Urdu through utilizing templates containing \\textit{questions} and prompting GPT4-V to generate the \\textit{answers} and the \\textit{rationales}, 2) introduced a new VL task named \\textit{unrelatedness}, 3) introduced rationales to enable human understanding of the VLM reasoning process, and 4) employed human evaluation to measure the suitability of proposed datasets for VL tasks. We show that VLMs can be fine-tuned on our datasets. Our work is the first to conduct such analyses in Swahili and Urdu. Also, it introduces \\textit{rationales} in VL analysis, which played a vital role in the evaluation.",
        "authors": [
            "Jesse Atuhurra",
            "Iqra Ali",
            "Tatsuya Hiraoka",
            "Hidetaka Kamigaito",
            "Tomoya Iwakura",
            "Taro Watanabe"
        ],
        "citations": 0,
        "references": 66,
        "year": 2024
    },
    {
        "title": "AnyAttack: Targeted Adversarial Attacks on Vision-Language Models toward Any Images",
        "abstract": "Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks, particularly targeted adversarial images that manipulate the model to generate harmful content specified by the adversary. Current attack methods rely on predefined target labels to create targeted adversarial attacks, which limits their scalability and applicability for large-scale robustness evaluations. In this paper, we propose AnyAttack, a self-supervised framework that generates targeted adversarial images for VLMs without label supervision, allowing any image to serve as a target for the attack. Our framework employs the pre-training and fine-tuning paradigm, with the adversarial noise generator pre-trained on the large-scale LAION-400M dataset. This large-scale pre-training endows our method with powerful transferability across a wide range of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks (image-text retrieval, multimodal classification, and image captioning) demonstrate the effectiveness of our attack. Additionally, we successfully transfer AnyAttack to multiple commercial VLMs, including Google Gemini, Claude Sonnet, Microsoft Copilot and OpenAI GPT. These results reveal an unprecedented risk to VLMs, highlighting the need for effective countermeasures.",
        "authors": [
            "Jiaming Zhang",
            "Junhong Ye",
            "Xingjun Ma",
            "Yige Li",
            "Yunfan Yang",
            "Jitao Sang",
            "Dit-Yan Yeung"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Improving Fine-grained Visual Understanding in VLMs through Text-Only Training",
        "abstract": "Visual-Language Models (VLMs) have become a powerful tool for bridging the gap between visual and linguistic understanding. However, the conventional learning approaches for VLMs often suffer from limitations, such as the high resource requirements of collecting and training image-text paired data. Recent research has suggested that language understanding plays a crucial role in the performance of VLMs, potentially indicating that text-only training could be a viable approach. In this work, we investigate the feasibility of enhancing fine-grained visual understanding in VLMs through text-only training. Inspired by how humans develop visual concept understanding, where rich textual descriptions can guide visual recognition, we hypothesize that VLMs can also benefit from leveraging text-based representations to improve their visual recognition abilities. We conduct comprehensive experiments on two distinct domains: fine-grained species classification and cultural visual understanding tasks. Our findings demonstrate that text-only training can be comparable to conventional image-text training while significantly reducing computational costs. This suggests a more efficient and cost-effective pathway for advancing VLM capabilities, particularly valuable in resource-constrained environments.",
        "authors": [
            "Dasol Choi",
            "Guijin Son",
            "Soo Yong Kim",
            "Gio Paik",
            "Seunghyeok Hong"
        ],
        "citations": 0,
        "references": 22,
        "year": 2024
    },
    {
        "title": "A pen mark is all you need - Incidental prompt injection attacks on Vision Language Models in real-life histopathology",
        "abstract": "Vision-language models (VLMs) can analyze multimodal medical data. However, a significant weakness of VLMs, as we have recently described, is their susceptibility to prompt injection attacks. Here, the model receives conflicting instructions, leading to potentially harmful outputs. In this study, we hypothesized that handwritten labels and watermarks on pathological images could act as inadvertent prompt injections, influencing decision-making in histopathology. We conducted a quantitative study with a total of N = 3888 observations on the state-of-the-art VLMs Claude 3 Opus, Claude 3.5 Sonnet and GPT-4o. We designed various real-world inspired scenarios in which we show that VLMs rely entirely on (false) labels and watermarks if presented with those next to the tissue. All models reached almost perfect accuracies (90 - 100 %) for ground-truth leaking labels and abysmal accuracies (0 - 10 %) for misleading watermarks, despite baseline accuracies between 30-65 % for various multiclass problems. Overall, all VLMs accepted human-provided labels as infallible, even when those inputs contained obvious errors. Furthermore, these effects could not be mitigated by prompt engineering. It is therefore imperative to consider the presence of labels or other influencing features during future evaluation of VLMs in medicine and other fields.",
        "authors": [
            "J. Clusmann",
            "S. J. K. Schulz",
            "D. Ferber",
            "I. Wiest",
            "A. Fernandez",
            "M. Eckstein",
            "F. Lange",
            "N. G. Reitsam",
            "F. Kellers",
            "M. Schmitt",
            "P. Neidlinger",
            "P.-H. Koop",
            "C. V. Schneider",
            "D. Truhn",
            "W. Roth",
            "M. Jesinghaus",
            "J. N. Kather",
            "S. Foersch"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "GeoMeter: Probing Depth and Height Perception of Large Visual-Language Models",
        "abstract": "Geometric understanding is crucial for navigating and interacting with our environment. While large Vision Language Models (VLMs) demonstrate impressive capabilities, deploying them in real-world scenarios necessitates a comparable geometric understanding in visual perception. In this work, we focus on the geometric comprehension of these models; specifically targeting the depths and heights of objects within a scene. Our observations reveal that, although VLMs excel in basic geometric properties perception such as shape and size, they encounter significant challenges in reasoning about the depth and height of objects. To address this, we introduce GeoMeter, a suite of benchmark datasets encompassing Synthetic 2D, Synthetic 3D, and Real-World scenarios to rigorously evaluate these aspects. We benchmark 17 state-of-the-art VLMs using these datasets and find that they consistently struggle with both depth and height perception. Our key insights include detailed analyses of the shortcomings in depth and height reasoning capabilities of VLMs and the inherent bias present in these models. This study aims to pave the way for the development of VLMs with enhanced geometric understanding, crucial for real-world applications.",
        "authors": [
            "Shehreen Azad",
            "Yash Jain",
            "Rishit Garg",
            "Y. S. Rawat",
            "Vibhav Vineet"
        ],
        "citations": 0,
        "references": 35,
        "year": 2024
    },
    {
        "title": "Right this way: Can VLMs Guide Us to See More to Answer Questions?",
        "abstract": "In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating ``where to know'' scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans.",
        "authors": [
            "Li Liu",
            "Diji Yang",
            "Sijia Zhong",
            "Kalyana Suma Sree Tholeti",
            "Lei Ding",
            "Yi Zhang",
            "Leilani Gilpin"
        ],
        "citations": 0,
        "references": 51,
        "year": 2024
    },
    {
        "title": "VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models",
        "abstract": "The rapid advancement of vision-language models (VLMs) has established a new paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously detect anomalies and provide comprehendible explanations for the decisions. Existing work in this direction often assumes the complex reasoning required for VAD exceeds the capabilities of pretrained VLMs. Consequently, these approaches either incorporate specialized reasoning modules during inference or rely on instruction tuning datasets through additional training to adapt VLMs for VAD. However, such strategies often incur substantial computational costs or data annotation overhead. To address these challenges in explainable VAD, we introduce a verbalized learning framework named VERA that enables VLMs to perform VAD without model parameter modifications. Specifically, VERA automatically decomposes the complex reasoning required for VAD into reflections on simpler, more focused guiding questions capturing distinct abnormal patterns. It treats these reflective questions as learnable parameters and optimizes them through data-driven verbal interactions between learner and optimizer VLMs, using coarsely labeled training data. During inference, VERA embeds the learned questions into model prompts to guide VLMs in generating segment-level anomaly scores, which are then refined into frame-level scores via the fusion of scene and temporal contexts. Experimental results on challenging benchmarks demonstrate that the learned questions of VERA are highly adaptable, significantly improving both detection performance and explainability of VLMs for VAD.",
        "authors": [
            "Muchao Ye",
            "Weiyang Liu",
            "Pan He"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "Sensitivity of Generative VLMs to Semantically and Lexically Altered Prompts",
        "abstract": "Despite the significant influx of prompt-tuning techniques for generative vision-language models (VLMs), it remains unclear how sensitive these models are to lexical and semantic alterations in prompts. In this paper, we evaluate the ability of generative VLMs to understand lexical and semantic changes in text using the SugarCrepe++ dataset. We analyze the sensitivity of VLMs to lexical alterations in prompts without corresponding semantic changes. Our findings demonstrate that generative VLMs are highly sensitive to such alterations. Additionally, we show that this vulnerability affects the performance of techniques aimed at achieving consistency in their outputs.",
        "authors": [
            "Sri Harsha Dumpala",
            "Aman Jaiswal",
            "Chandramouli Sastry",
            "E. Milios",
            "Sageev Oore",
            "Hassan Sajjad"
        ],
        "citations": 0,
        "references": 30,
        "year": 2024
    },
    {
        "title": "Disease-informed Adaptation of Vision-Language Models",
        "abstract": "Expertise scarcity and high cost of data annotation hinder the development of artificial intelligence (AI) foundation models for medical image analysis. Transfer learning provides a way to utilize the off-the-shelf foundation models to address the clinical challenges. However, such models encounter difficulties when adapting to new diseases not presented in their original pre-training datasets. Compounding this challenge is the limited availability of example cases for a new disease, which further leads to the poor performance of the existing transfer learning techniques. This paper proposes a novel method for transfer learning of foundation Vision-Language Models (VLMs) to efficiently adapt them to a new disease with only a few examples. Such an effective adaptation of VLMs hinges on learning the nuanced representation of new disease concepts. By capitalizing on the joint visual-linguistic capabilities of VLMs, we introduce disease-informed contextual prompting in a novel disease prototype learning framework, which enables VLMs to quickly grasp the concept of the new disease, even with limited data. Extensive experiments across multiple pre-trained medical VLMs and multiple tasks showcase the notable enhancements in performance compared to other existing adaptation techniques. The code will be made publicly available at https://github.com/ RPIDIAL/Disease-informed-VLM-Adaptation.",
        "authors": [
            "Jiajin Zhang",
            "Ge Wang",
            "M. Kalra",
            "P. Yan"
        ],
        "citations": 0,
        "references": 31,
        "year": 2024
    },
    {
        "title": "Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs",
        "abstract": "In the study of LLMs, sycophancy represents a prevalent hallucination that poses significant challenges to these models. Specifically, LLMs often fail to adhere to original correct responses, instead blindly agreeing with users' opinions, even when those opinions are incorrect or malicious. However, research on sycophancy in visual language models (VLMs) has been scarce. In this work, we extend the exploration of sycophancy from LLMs to VLMs, introducing the MM-SY benchmark to evaluate this phenomenon. We present evaluation results from multiple representative models, addressing the gap in sycophancy research for VLMs. To mitigate sycophancy, we propose a synthetic dataset for training and employ methods based on prompts, supervised fine-tuning, and DPO. Our experiments demonstrate that these methods effectively alleviate sycophancy in VLMs. Additionally, we probe VLMs to assess the semantic impact of sycophancy and analyze the attention distribution of visual tokens. Our findings indicate that the ability to prevent sycophancy is predominantly observed in higher layers of the model. The lack of attention to image knowledge in these higher layers may contribute to sycophancy, and enhancing image attention at high layers proves beneficial in mitigating this issue.",
        "authors": [
            "Shuo Li",
            "Tao Ji",
            "Xiaoran Fan",
            "Linsheng Lu",
            "Leyi Yang",
            "Yuming Yang",
            "Zhiheng Xi",
            "Rui Zheng",
            "Yuran Wang",
            "Xiaohui Zhao",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "citations": 0,
        "references": 34,
        "year": 2024
    },
    {
        "title": "COREval: A Comprehensive and Objective Benchmark for Evaluating the Remote Sensing Capabilities of Large Vision-Language Models",
        "abstract": "With the rapid development of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing Earth observation, have demonstrated exceptional perception and reasoning abilities within this specific field. However, the current absence of a comprehensive benchmark for holistically evaluating the remote sensing capabilities of these VLMs represents a significant gap. To bridge this gap, we propose COREval, the first benchmark designed to comprehensively and objectively evaluate the hierarchical remote sensing capabilities of VLMs. Concentrating on 2 primary capability dimensions essential to remote sensing: perception and reasoning, we further categorize 6 secondary dimensions and 22 leaf tasks to ensure a well-rounded assessment coverage for this specific field. COREval guarantees the quality of the total of 6,263 problems through a rigorous process of data collection from 50 globally distributed cities, question construction and quality control, and the format of multiple-choice questions with definitive answers allows for an objective and straightforward evaluation of VLM performance. We conducted a holistic evaluation of 13 prominent open-source VLMs from both the general and remote sensing domains, highlighting current shortcomings in their remote sensing capabilities and providing directions for improvements in their application within this specialized context. We hope that COREval will serve as a valuable resource and offer deeper insights into the challenges and potential of VLMs in the field of remote sensing.",
        "authors": [
            "Xiao An",
            "Jiaxing Sun",
            "Zihan Gui",
            "Wei He"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models",
        "abstract": "Vision-language models (VLMs) are intensively used in many downstream tasks, including those requiring assessments of individuals appearing in the images. While VLMs perform well in simple single-person scenarios, in real-world applications, we often face complex situations in which there are persons of different genders doing different activities. We show that in such cases, VLMs are biased towards identifying the individual with the expected gender (according to ingrained gender stereotypes in the model or other forms of sample selection bias) as the performer of the activity. We refer to this bias in associating an activity with the gender of its actual performer in an image or text as the Gender-Activity Binding (GAB) bias and analyze how this bias is internalized in VLMs. To assess this bias, we have introduced the GAB dataset with approximately 5500 AI-generated images that represent a variety of activities, addressing the scarcity of real-world images for some scenarios. To have extensive quality control, the generated images are evaluated for their diversity, quality, and realism. We have tested 12 renowned pre-trained VLMs on this dataset in the context of text-to-image and image-to-text retrieval to measure the effect of this bias on their predictions. Additionally, we have carried out supplementary experiments to quantify the bias in VLMs' text encoders and to evaluate VLMs' capability to recognize activities. Our experiments indicate that VLMs experience an average performance decline of about 13.2% when confronted with gender-activity binding bias.",
        "authors": [
            "Ali Abdollahi",
            "Mahdi Ghaznavi",
            "Mohammad Reza Karimi Nejad",
            "Arash Mari Oriyad",
            "Reza Abbasi",
            "Ali Salesi",
            "Melika Behjati",
            "M. H. Rohban",
            "M. Baghshah"
        ],
        "citations": 0,
        "references": 45,
        "year": 2024
    },
    {
        "title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?",
        "abstract": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an understanding of diverse multi-vision sensor data, such as thermal, depth, and X-ray information, is essential. However, we find that current VLMs process multi-vision sensor images without deep understanding of sensor information, disregarding each sensor's unique physical properties. This limitation restricts their capacity to interpret and respond to complex questions requiring multi-vision sensor reasoning. To address this, we propose a novel Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark, assessing VLMs on their capacity for sensor-specific reasoning. Moreover, we introduce Diverse Negative Attributes (DNA) optimization to enable VLMs to perform deep reasoning on multi-vision sensor tasks, helping to bridge the core information gap between images and sensor data. Extensive experimental results validate that the proposed DNA method can significantly improve the multi-vision sensor reasoning for VLMs.",
        "authors": [
            "Sangyun Chung",
            "Youngjoon Yu",
            "Youngchae Chee",
            "Se Yeon Kim",
            "Byung-Kwan Lee",
            "Yonghyun Ro"
        ],
        "citations": 0,
        "references": 0,
        "year": 2024
    },
    {
        "title": "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph",
        "abstract": "Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the inter-class relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms previous adapter-based methods. The code will be released at https://github.com/lixinustc/GraphAdapter",
        "authors": [
            "Xin Li",
            "Dongze Lian",
            "Zhihe Lu",
            "Jiawang Bai",
            "Zhibo Chen",
            "Xinchao Wang"
        ],
        "citations": 40,
        "references": 92,
        "year": 2023
    },
    {
        "title": "Visual Classification via Description from Large Language Models",
        "abstract": "Vision-language models (VLMs) such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what features the model uses to construct its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.",
        "authors": [
            "Sachit Menon",
            "Carl Vondrick"
        ],
        "citations": 236,
        "references": 51,
        "year": 2022
    },
    {
        "title": "Large Language Models are Visual Reasoning Coordinators",
        "abstract": "Visual reasoning requires multimodal perception and commonsense cognition of the world. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains. However, how to harness the collective power of these complementary VLMs is rarely explored. Existing methods like ensemble still struggle to aggregate these models with the desired higher-order communications. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a large language model (LLM) can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.",
        "authors": [
            "Liangyu Chen",
            "Boyi Li",
            "Sheng Shen",
            "Jingkang Yang",
            "Chunyuan Li",
            "Kurt Keutzer",
            "Trevor Darrell",
            "Ziwei Liu"
        ],
        "citations": 36,
        "references": 125,
        "year": 2023
    },
    {
        "title": "Vision-Language Models in Remote Sensing: Current progress and future trends",
        "abstract": "The remarkable achievements of ChatGPT and Generative Pre-trained Transformer 4 (GPT-4) have sparked a wave of interest and research in the field of large language models (LLMs) for artificial general intelligence (AGI). These models provide intelligent solutions that are closer to human thinking, enabling us to use general artificial intelligence (AI) to solve problems in various applications. However, in the field of remote sensing (RS), the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research in RS focuses primarily on visual-understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-LMs (VLMs) excel as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. VLMs can go beyond visual recognition of RS images and can model semantic relationships as well as generate natural language descriptions of the image. This makes them better suited for tasks that require both visual and textual understanding, such as image captioning and visual question answering (VQA). This article provides a comprehensive review of the research on VLMs in RS, summarizing the latest progress, highlighting current challenges, and identifying potential research opportunities. Specifically, we review the application of VLMs in mainstream RS tasks, including image captioning, text-based image generation, text-based image retrieval (TBIR), VQA, scene classification, semantic segmentation, and object detection. For each task, we analyze representative works and discuss research progress. Finally, we summarize the limitations of existing works and provide possible directions for future development. This review aims to provide a comprehensive overview of the current research progress of VLMs in RS (see Figure 1), and to inspire further research in this exciting and promising field.",
        "authors": [
            "Congcong Wen",
            "Yuan Hu",
            "Xiang Li",
            "Zhenghang Yuan",
            "Xiao Xiang Zhu"
        ],
        "citations": 47,
        "references": 291,
        "year": 2023
    },
    {
        "title": "Vision Language Models in Autonomous Driving and Intelligent Transportation Systems",
        "abstract": "‚ÄîThe applications of Vision-Language Models (VLMs) in the fields of Autonomous Driving (AD) and Intelligent Transportation Systems (ITS) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By integrating language data, the vehicles, and transportation systems are able to deeply understand real-world environments, improving driving safety and efficiency. In this work, we present a comprehensive survey of the advances in language models in this domain, encompassing current models and datasets. Additionally, we explore the potential applications and emerging research directions. Finally, we thoroughly discuss the challenges and research gap. The paper aims to provide researchers with the current work and future trends of VLMs in AD and ITS.",
        "authors": [
            "Xingcheng Zhou",
            "Mingyu Liu",
            "B. L. ≈Ωagar",
            "Ekim Yurtsever",
            "Alois C. Knoll"
        ],
        "citations": 44,
        "references": 104,
        "year": 2023
    },
    {
        "title": "CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning",
        "abstract": "Nowadays, the research on Large Vision-Language Models (LVLMs) has been significantly promoted thanks to the success of Large Language Models (LLM). Nevertheless, these Vision-Language Models (VLMs) are suffering from the drawback of hallucination -- due to insufficient understanding of vision and language modalities, VLMs may generate incorrect perception information when doing downstream applications, for example, captioning a non-existent entity. To address the hallucination phenomenon, on the one hand, we introduce a Contrastive Instruction Evaluation Method (CIEM), which is an automatic pipeline that leverages an annotated image-text dataset coupled with an LLM to generate factual/contrastive question-answer pairs for the evaluation of the hallucination of VLMs. On the other hand, based on CIEM, we further propose a new instruction tuning method called CIT (the abbreviation of Contrastive Instruction Tuning) to alleviate the hallucination of VLMs by automatically producing high-quality factual/contrastive question-answer pairs and corresponding justifications for model tuning. Through extensive experiments on CIEM and CIT, we pinpoint the hallucination issues commonly present in existing VLMs, the disability of the current instruction-tuning dataset to handle the hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM and public datasets.",
        "authors": [
            "Hongyu Hu",
            "Jiyuan Zhang",
            "Minyi Zhao",
            "Zhenbang Sun"
        ],
        "citations": 30,
        "references": 28,
        "year": 2023
    },
    {
        "title": "DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment",
        "abstract": "Large language models (LLMs) encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous work has explored how to ground LLMs in robotic tasks to generate feasible and executable textual plans. However, low-level execution in the physical world may deviate from the high-level textual plan due to environmental perturbations or imperfect controller design. In this paper, we propose DoReMi, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, we leverage LLMs to play a dual role, aiding not only in high-level planning but also generating constraints that can indicate misalignment during execution. Then vision language models (VLMs) are utilized to detect constraint violations continuously. Our pipeline can monitor the low-level execution and enable timely recovery if certain plan-execution misalignment occurs. Experiments on various complex tasks including robot arms and humanoid robots demonstrate that our method can lead to higher task success rates and shorter task completion times.",
        "authors": [
            "Yanjiang Guo",
            "Yen-Jen Wang",
            "Lihan Zha",
            "Zheyuan Jiang",
            "Jianyu Chen"
        ],
        "citations": 30,
        "references": 62,
        "year": 2023
    },
    {
        "title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning",
        "abstract": "Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge. We release the dataset for further research in this area.",
        "authors": [
            "Mehran Kazemi",
            "Hamidreza Alvari",
            "Ankit Anand",
            "Jialin Wu",
            "Xi Chen",
            "Radu Soricut"
        ],
        "citations": 34,
        "references": 64,
        "year": 2023
    },
    {
        "title": "Semantic Scene Understanding with Large Language Models on Unmanned Aerial Vehicles",
        "abstract": "Unmanned Aerial Vehicles (UAVs) are able to provide instantaneous visual cues and a high-level data throughput that could be further leveraged to address complex tasks, such as semantically rich scene understanding. In this work, we built on the use of Large Language Models (LLMs) and Visual Language Models (VLMs), together with a state-of-the-art detection pipeline, to provide thorough zero-shot UAV scene literary text descriptions. The generated texts achieve a GUNNING Fog median grade level in the range of 7‚Äì12. Applications of this framework could be found in the filming industry and could enhance user experience in theme parks or in the advertisement sector. We demonstrate a low-cost highly efficient state-of-the-art practical implementation of microdrones in a well-controlled and challenging setting, in addition to proposing the use of standardized readability metrics to assess LLM-enhanced descriptions.",
        "authors": [
            "Federico Tombari",
            "J. Curt√≤",
            "I. D. Zarz√†",
            "C. Calafate"
        ],
        "citations": 34,
        "references": 36,
        "year": 2023
    },
    {
        "title": "Can I Trust Your Answer? Visually Grounded Video Question Answering",
        "abstract": "We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA - an extension of NExT-QA with 10.5K temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy, we further explore and propose a grounded-QA method via Gaussian mask optimization and cross-modal learning. Experiments with different backbones demonstrate that this grounding mechanism improves both grounding and QA. With these efforts, we aim to push towards trustworthy VLMs in VQA systems. Our dataset and code are available at https://github.com/doc-doc/NExT-GQA.",
        "authors": [
            "Junbin Xiao",
            "Angela Yao",
            "Yicong Li",
            "Tat-Seng Chua"
        ],
        "citations": 24,
        "references": 79,
        "year": 2023
    },
    {
        "title": "RoboVQA: Multimodal Long-Horizon Reasoning for Robotics",
        "abstract": "We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple embodiments (robot, human, human with grasping tool). With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We explore the economics of collection costs and find that for a fixed budget it is beneficial to take advantage of the cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zeroshot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Thanks to video conditioning and dataset diversity, the model can be used as general video value functions (e.g. success and affordance) in situations where actions needs to be recognized rather than states, expanding capabilities and environment understanding for robots. Data and videos are available at robovqa.github.io",
        "authors": [
            "P. Sermanet",
            "Tianli Ding",
            "Jeffrey Zhao",
            "Fei Xia",
            "Debidatta Dwibedi",
            "K. Gopalakrishnan",
            "Christine Chan",
            "Gabriel Dulac-Arnold",
            "Sharath Maddineni",
            "Nikhil J. Joshi",
            "Pete Florence",
            "Wei Han",
            "Robert Baruch",
            "Yao Lu",
            "Suvir Mirchandani",
            "Peng Xu",
            "Pannag R. Sanketi",
            "Karol Hausman",
            "Izhak Shafran",
            "Brian Ichter",
            "Yuan Cao"
        ],
        "citations": 27,
        "references": 47,
        "year": 2023
    },
    {
        "title": "A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions",
        "abstract": "Curation methods for massive vision-language datasets trade off between dataset size and quality. However, even the highest quality of available curated captions are far too short to capture the rich visual detail in an image. To show the value of dense and highly-aligned image-text pairs, we collect the Densely Captioned Images (DCI) dataset, containing 7805 natural images human-annotated with mask-aligned descriptions averaging above 1000 words each. With precise and reliable captions associated with specific parts of an image, we can evaluate vision-language models' (VLMs) understanding of image content with a novel task that matches each caption with its corresponding subcrop. As current models are often limited to 77 text tokens, we also introduce a summarized version (sDCI) in which each caption length is limited. We show that modern techniques that make progress on standard benchmarks do not correspond with significant improvement on our sDCI based benchmark. Lastly, we finetune CLIP using sDCI and show significant improvements over the baseline despite a small training set. By releasing the first human annotated dense image captioning dataset, we hope to enable the development of new benchmarks or finetuning recipes for the next generation of VLMs to come.",
        "authors": [
            "Jack Urbanek",
            "Florian Bordes",
            "Pietro Astolfi",
            "Mary Williamson",
            "Vasu Sharma",
            "Adriana Romero-Soriano"
        ],
        "citations": 27,
        "references": 47,
        "year": 2023
    },
    {
        "title": "A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis",
        "abstract": "Zero-shot medical image classification is a critical process in real-world scenarios where we have limited access to all possible diseases or large-scale annotated data. It involves computing similarity scores between a query medical image and possible disease categories to determine the diagnostic result. Recent advances in pretrained vision-language models (VLMs) such as CLIP have shown great performance for zero-shot natural image recognition and exhibit benefits in medical applications. However, an explainable zero-shot medical image recognition framework with promising performance is yet under development. In this paper, we propose a novel CLIP-based zero-shot medical image classification framework supplemented with ChatGPT for explainable diagnosis, mimicking the diagnostic process performed by human experts. The key idea is to query large language models (LLMs) with category names to automatically generate additional cues and knowledge, such as disease symptoms or descriptions other than a single category name, to help provide more accurate and explainable diagnosis in CLIP. We further design specific prompts to enhance the quality of generated texts by ChatGPT that describe visual medical features. Extensive results on one private dataset and four public datasets along with detailed analysis demonstrate the effectiveness and explainability of our training-free zero-shot diagnosis pipeline, corroborating the great potential of VLMs and LLMs for medical applications.",
        "authors": [
            "Jiaxiang Liu",
            "Tianxiang Hu",
            "Yan Zhang",
            "Xiaotang Gai",
            "Yang Feng",
            "Zuozhu Liu"
        ],
        "citations": 25,
        "references": 33,
        "year": 2023
    },
    {
        "title": "CLIP4STR: A Simple Baseline for Scene Text Recognition With Pre-Trained Vision-Language Model",
        "abstract": "Pre-trained vision-language models (VLMs) are the de-facto foundation models for various downstream tasks. However, scene text recognition methods still prefer backbones pre-trained on a single modality, namely, the visual modality, despite the potential of VLMs to serve as powerful scene text readers. For example, CLIP can robustly identify regular (horizontal) and irregular (rotated, curved, blurred, or occluded) text in images. With such merits, we transform CLIP into a scene text reader and introduce CLIP4STR, a simple yet effective STR method built upon image and text encoders of CLIP. It has two encoder-decoder branches: a visual branch and a cross-modal branch. The visual branch provides an initial prediction based on the visual feature, and the cross-modal branch refines this prediction by addressing the discrepancy between the visual feature and text semantics. To fully leverage the capabilities of both branches, we design a dual predict-and-refine decoding scheme for inference. We scale CLIP4STR in terms of the model size, pre-training data, and training data, achieving state-of-the-art performance on 13 STR benchmarks. Additionally, a comprehensive empirical study is provided to enhance the understanding of the adaptation of CLIP to STR. Our method establishes a simple yet strong baseline for future STR research with VLMs.",
        "authors": [
            "Shuai Zhao",
            "Xiaohan Wang",
            "Linchao Zhu",
            "Yezhou Yang"
        ],
        "citations": 25,
        "references": 120,
        "year": 2023
    },
    {
        "title": "Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving",
        "abstract": "Large vision-language models (VLMs) have garnered increasing interest in autonomous driving areas, due to their advanced capabilities in complex reasoning tasks essential for highly autonomous vehicle behavior. Despite their potential, research in autonomous systems is hindered by the lack of datasets with annotated reasoning chains that explain the decision-making processes in driving. To bridge this gap, we present Reason2Drive, a benchmark dataset with over 600K video-text pairs, aimed at facilitating the study of interpretable reasoning in complex driving environments. We distinctly characterize the autonomous driving process as a sequential combination of perception, prediction, and reasoning steps, and the question-answer pairs are automatically collected from a diverse range of open-source outdoor driving datasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novel aggregated evaluation metric to assess chain-based reasoning performance in autonomous systems, addressing the semantic ambiguities of existing metrics such as BLEU and CIDEr. Based on the proposed benchmark, we conduct experiments to assess various existing VLMs, revealing insights into their reasoning capabilities. Additionally, we develop an efficient approach to empower VLMs to leverage object-level perceptual elements in both feature extraction and prediction, further enhancing their reasoning accuracy. The code and dataset will be released.",
        "authors": [
            "Ming Nie",
            "Renyuan Peng",
            "Chunwei Wang",
            "Xinyue Cai",
            "Jianhua Han",
            "Hang Xu",
            "Li Zhang"
        ],
        "citations": 25,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs",
        "abstract": "Vision and language models (VLMs) have demonstrated remarkable zero-shot (ZS) performance in a variety of tasks. However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states. In contrast, obtaining structured annotations, such as scene graphs (SGs), that could improve these models is time-consuming and costly, and thus cannot be used on a large scale. Here we ask whether small SG datasets can provide sufficient information for enhancing structured understanding of pretrained VLMs. We show that it is indeed possible to improve VLMs when learning from SGs by integrating components that incorporate structured information into both visual and textual representations. For the visual side, we incorporate a special\"SG Component\"in the image transformer trained to predict SG information, while for the textual side, we utilize SGs to generate fine-grained captions that highlight different compositional aspects of the scene. Our method improves the performance of several popular VLMs on multiple VL datasets with only a mild degradation in ZS capabilities.",
        "authors": [
            "Roei Herzig",
            "Alon Mendelson",
            "Leonid Karlinsky",
            "Assaf Arbelle",
            "R. Feris",
            "Trevor Darrell",
            "A. Globerson"
        ],
        "citations": 25,
        "references": 99,
        "year": 2023
    },
    {
        "title": "DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback",
        "abstract": "Despite their wide-spread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user's input text. We introduce DreamSync, a model-agnostic training algorithm by design that improves T2I models to be faithful to the text input. DreamSync builds off a recent insight from TIFA's evaluation framework -- that large vision-language models (VLMs) can effectively identify the fine-grained discrepancies between generated images and the text inputs. DreamSync uses this insight to train T2I models without any labeled data; it improves T2I models using its own generations. First, it prompts the model to generate several candidate images for a given input text. Then, it uses two VLMs to select the best generation: a Visual Question Answering model that measures the alignment of generated images to the text, and another that measures the generation's aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I model to guide its generation towards the selected best generations. DreamSync does not need any additional human annotation. model architecture changes, or reinforcement learning. Despite its simplicity, DreamSync improves both the semantic alignment and aesthetic appeal of two diffusion-based T2I models, evidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA aesthetic) and human evaluation.",
        "authors": [
            "Jiao Sun",
            "Deqing Fu",
            "Yushi Hu",
            "Su Wang",
            "Royi Rassin",
            "Da-Cheng Juan",
            "Dana Alon",
            "Charles Herrmann",
            "Sjoerd van Steenkiste",
            "Ranjay Krishna",
            "Cyrus Rashtchian"
        ],
        "citations": 25,
        "references": 51,
        "year": 2023
    },
    {
        "title": "LLM Multimodal Traffic Accident Forecasting",
        "abstract": "With the rise in traffic congestion in urban centers, predicting accidents has become paramount for city planning and public safety. This work comprehensively studied the efficacy of modern deep learning (DL) methods in forecasting traffic accidents and enhancing Level-4 and Level-5 (L-4 and L-5) driving assistants with actionable visual and language cues. Using a rich dataset detailing accident occurrences, we juxtaposed the Transformer model against traditional time series models like ARIMA and the more recent Prophet model. Additionally, through detailed analysis, we delved deep into feature importance using principal component analysis (PCA) loadings, uncovering key factors contributing to accidents. We introduce the idea of using real-time interventions with large language models (LLMs) in autonomous driving with the use of lightweight compact LLMs like LLaMA-2 and Zephyr-7b-Œ±. Our exploration extends to the realm of multimodality, through the use of Large Language-and-Vision Assistant (LLaVA)‚Äîa bridge between visual and linguistic cues by means of a Visual Language Model (VLM)‚Äîin conjunction with deep probabilistic reasoning, enhancing the real-time responsiveness of autonomous driving systems. In this study, we elucidate the advantages of employing large multimodal models within DL and deep probabilistic programming for enhancing the performance and usability of time series forecasting and feature weight importance, particularly in a self-driving scenario. This work paves the way for safer, smarter cities, underpinned by data-driven decision making.",
        "authors": [
            "I. D. Zarz√†",
            "J. Curt√≤",
            "Gemma Roig",
            "C. Calafate"
        ],
        "citations": 24,
        "references": 41,
        "year": 2023
    },
    {
        "title": "Vision-Language Models as a Source of Rewards",
        "abstract": "Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.",
        "authors": [
            "Kate Baumli",
            "Satinder Baveja",
            "Feryal M. P. Behbahani",
            "Harris Chan",
            "Gheorghe Comanici",
            "Sebastian Flennerhag",
            "Maxime Gazeau",
            "Kristian Holsheimer",
            "Dan Horgan",
            "Michael Laskin",
            "Clare Lyle",
            "Hussain Masoom",
            "Kay McKinney",
            "Volodymyr Mnih",
            "Alexander Neitz",
            "Fabio Pardo",
            "Jack Parker-Holder",
            "John Quan",
            "Tim Rocktaschel",
            "Himanshu Sahni",
            "T. Schaul",
            "Yannick Schroecker",
            "Stephen Spencer",
            "Richie Steigerwald",
            "Luyu Wang",
            "Lei Zhang"
        ],
        "citations": 22,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Exploring Vision-Language Models for Imbalanced Learning",
        "abstract": "Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%, and 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. We further analyze the influence of pre-training data size, backbones, and training cost. Our study highlights the significance of imbalanced learning algorithms in face of VLMs pre-trained by huge data. We release our code at https://github.com/Imbalance-VLM/Imbalance-VLM.",
        "authors": [
            "Yidong Wang",
            "Zhuohao Yu",
            "Jindong Wang",
            "Qiang Heng",
            "Haoxing Chen",
            "Wei Ye",
            "Rui Xie",
            "Xingxu Xie",
            "Shi-Bo Zhang"
        ],
        "citations": 22,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Prompt-based Distribution Alignment for Unsupervised Domain Adaptation",
        "abstract": "Recently, despite the unprecedented success of large pre-trained visual-language models (VLMs) on a wide range of downstream tasks, the real-world unsupervised domain adaptation (UDA) problem is still not well explored. Therefore, in this paper, we first experimentally demonstrate that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA. However, a major challenge for directly deploying such models on downstream UDA tasks is prompt engineering, which requires aligning the domain knowledge of source and target domains, since the performance of UDA is severely influenced by a good domain-invariant representation. We further propose a Prompt-based Distribution Alignment (PDA) method to incorporate the domain knowledge into prompt learning. Specifically, PDA employs a two-branch prompt-tuning paradigm, namely base branch and alignment branch. The base branch focuses on integrating class-related representation into prompts, ensuring discrimination among different classes. To further minimize domain discrepancy, for the alignment branch, we construct feature banks for both the source and target domains and propose image-guided feature tuning (IFT) to make the input attend to feature banks, which effectively integrates self-enhanced and cross-domain features into the model. In this way, these two branches can be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct extensive experiments on three benchmarks to demonstrate that our proposed PDA achieves state-of-the-art performance. The code is available at https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.",
        "authors": [
            "Shuanghao Bai",
            "Min Zhang",
            "Wanqi Zhou",
            "Siteng Huang",
            "Zhirong Luan",
            "Donglin Wang",
            "Badong Chen"
        ],
        "citations": 22,
        "references": 42,
        "year": 2023
    },
    {
        "title": "From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design",
        "abstract": "Engineering design is undergoing a transformative shift with the advent of AI, marking a new era in how we approach product, system, and service planning. Large language models have demonstrated impressive capabilities in enabling this shift. Yet, with text as their only input modality, they cannot leverage the large body of visual artifacts that engineers have used for centuries and are accustomed to. This gap is addressed with the release of multimodal vision-language models (VLMs), such as GPT-4V, enabling AI to impact many more types of tasks. Our work presents a comprehensive evaluation of VLMs across a spectrum of engineering design tasks, categorized into four main areas: Conceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, and Engineering Education Tasks. Specifically in this paper, we assess the capabilities of two VLMs, GPT-4V and LLaVA 1.6 34B, in design tasks such as sketch similarity analysis, CAD generation, topology optimization, manufacturability assessment, and engineering textbook problems. Through this structured evaluation, we not only explore VLMs' proficiency in handling complex design challenges but also identify their limitations in complex engineering design applications. Our research establishes a foundation for future assessments of vision language models. It also contributes a set of benchmark testing datasets, with more than 1000 queries, for ongoing advancements and applications in this field.",
        "authors": [
            "Cyril Picard",
            "Kristen M. Edwards",
            "Anna C. Doris",
            "Brandon Man",
            "Giorgio Giannone",
            "Md Ferdous Alam",
            "Faez Ahmed"
        ],
        "citations": 22,
        "references": 91,
        "year": 2023
    },
    {
        "title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
        "abstract": "Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io.",
        "authors": [
            "Boyuan Chen",
            "F. Xia",
            "Brian Ichter",
            "Kanishka Rao",
            "K. Gopalakrishnan",
            "M. Ryoo",
            "Austin Stone",
            "Daniel Kappler"
        ],
        "citations": 162,
        "references": 44,
        "year": 2022
    },
    {
        "title": "Transferable Decoding with Visual Entities for Zero-Shot Image Captioning",
        "abstract": "Image-to-text generation aims to describe images using natural language. Recently, zero-shot image captioning based on pre-trained vision-language models (VLMs) and large language models (LLMs) has made significant progress. However, we have observed and empirically demonstrated that these methods are susceptible to modality bias induced by LLMs and tend to generate descriptions containing objects (entities) that do not actually exist in the image but frequently appear during training (i.e., object hallucination). In this paper, we propose ViECap, a transferable decoding model that leverages entity-aware decoding to generate descriptions in both seen and unseen scenarios. ViECap incorporates entity-aware hard prompts to guide LLMs‚Äô attention toward the visual entities present in the image, enabling coherent caption generation across diverse scenes. With entity-aware hard prompts, ViECap is capable of maintaining performance when transferring from in-domain to out-of-domain scenarios. Extensive experiments demonstrate that ViECap sets a new state-of-the-art cross-domain (transferable) captioning and performs competitively in-domain captioning compared to previous VLMs-based zero-shot methods. Our code is available at: https://github.com/FeiElysia/ViECap",
        "authors": [
            "Junjie Fei",
            "Teng Wang",
            "Jinrui Zhang",
            "Zhenyu He",
            "Chengjie Wang",
            "Feng Zheng"
        ],
        "citations": 21,
        "references": 57,
        "year": 2023
    },
    {
        "title": "Language Models as Black-Box Optimizers for Vision-Language Models",
        "abstract": "Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities on downstream tasks when fine-tuned with minimal data. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. As such, we aim to develop a black-box approach to optimize VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or even output logits. We propose employing chat-based LLMs to search for the best text prompt for VLMs. Specifically, we adopt an automatic ‚Äúhill-climbing‚Äù procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot image classification setup, our simple approach surpasses the white-box continuous prompting method (CoOp) by an average of1.5% across 11 datasets including ImageNet. Our approach also outperforms both human-engineered and LLM-generated prompts. We high-light the advantage of conversational feedback that incor-porates both positive and negative prompts, suggesting that LLMs can utilize the implicit ‚Äúgradient‚Äù direction in textual feedback for a more efficient search. In addition, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different VLM architectures in a black-box manner. Lastly, we demonstrate our framework on a state-of-the-art black-box VLM (DALL-E 3) for text-to-image optimization.",
        "authors": [
            "Samuel Yu",
            "Shihong Liu",
            "Zhiqiu Lin",
            "Deepak Pathak",
            "Deva Ramanan"
        ],
        "citations": 19,
        "references": 114,
        "year": 2023
    },
    {
        "title": "APPLeNet: Visual Attention Parameterized Prompt Learning for Few-Shot Remote Sensing Image Generalization using CLIP",
        "abstract": "In recent years, the success of large-scale vision-language models (VLMs) such as CLIP has led to their increased usage in various computer vision tasks. These models enable zero-shot inference through carefully crafted instructional text prompts without task-specific supervision. However, the potential of VLMs for generalization tasks in remote sensing (RS) has not been fully realized. To address this research gap, we propose a novel image-conditioned prompt learning strategy called the Visual Attention Parameterized Prompts Learning Network (APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning in RS scene classification and disentangles visual style and content primitives for domain generalization tasks. To achieve this, APPLeNet combines visual content features obtained from different layers of the vision encoder and style properties obtained from feature statistics of domain-specific batches. An attention-driven injection module is further introduced to generate visual tokens from this information. We also introduce an anti-correlation regularizer to ensure discrimination among the token embeddings, as this visual information is combined with the textual tokens. To validate APPLeNet, we curated four available RS benchmarks and introduced experimental protocols and datasets for three domain generalization tasks. Our results consistently outperform the relevant literature and code is available at https://github.com/mainaksingha01/APPLeNet",
        "authors": [
            "M. Singha",
            "Ankit Jha",
            "Bhupendra S. Solanki",
            "Shirsha Bose",
            "Biplab Banerjee"
        ],
        "citations": 19,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Visual Spatial Reasoning",
        "abstract": "Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs‚Äô by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1",
        "authors": [
            "Fangyu Liu",
            "Guy Edward Toh Emerson",
            "Nigel Collier"
        ],
        "citations": 125,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Can Language Models Understand Physical Concepts?",
        "abstract": "Language models~(LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is not yet clear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85\\% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x. Our dataset is available at \\url{https://github.com/TobiasLee/VEC}",
        "authors": [
            "Lei Li",
            "Jingjing Xu",
            "Qingxiu Dong",
            "Ce Zheng",
            "Qi Liu",
            "Lingpeng Kong",
            "Xu Sun"
        ],
        "citations": 15,
        "references": 76,
        "year": 2023
    },
    {
        "title": "AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors",
        "abstract": "Deep generative models can create remarkably photorealistic fake images while raising concerns about misinformation and copyright infringement, known as deepfake threats. Deepfake detection technique is developed to distinguish between real and fake images, where the existing methods typically learn classifiers in the image domain or various feature domains. However, the generalizability of deepfake detection against emerging and more advanced generative models remains challenging. In this paper, being inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfake detection accuracy over unseen data. We formulate deepfake detection as a visual question answering problem, and tune soft prompts for InstructBLIP to answer the real/fake information of a query image. We conduct full-spectrum experiments on datasets from 3 held-in and 13 held-out generative models, covering modern text-to-image generation, image editing and image attacks. Results demonstrate that (1) the deepfake detection accuracy can be significantly and consistently improved (from 58.8% to 91.31%, in average accuracy over unseen data) using pretrained vision-language models with prompt tuning; (2) our superior performance is at less cost of trainable parameters, resulting in an effective and efficient solution for deepfake detection. Code and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.",
        "authors": [
            "You-Ming Chang",
            "Chen Yeh",
            "Wei-Chen Chiu",
            "Ning Yu"
        ],
        "citations": 15,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Sieve: Multimodal Dataset Pruning Using Image Captioning Models",
        "abstract": "Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy web-crawled datasets. This underscores the critical need for dataset pruning, as the quality of these datasets is strongly correlated with the performance of VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train models using highly-aligned samples is one of the most successful methods for pruning. We argue that this approach suffers from multiple limitations including: false positives and negatives due to CLIP's pretraining on noisy labels. We propose a pruning signal, Sieve, that employs synthetic captions generated by image-captioning models pretrained on small, diverse, and well-aligned image-text pairs to evaluate the alignment of noisy image-text pairs. To bridge the gap between the limited diversity of generated captions and the high diversity of alternative text (alt-text), we estimate the semantic textual similarity in the embedding space of a language model pretrained on unlabeled text corpus. Using DataComp, a multimodal dataset filtering benchmark, when evaluating on 38 downstream tasks, our pruning approach, surpasses CLIPScore by 2.6% and 1.7% on medium and large scale respectively. In addition, on retrieval tasks, Sieve leads to a significant improvement of 2.7% and 4.5% on medium and large scale respectively.",
        "authors": [
            "Anas Mahmoud",
            "Mostafa Elhoushi",
            "Amro Abbas",
            "Yu Yang",
            "Newsha Ardalani",
            "Hugh Leather",
            "Ari S. Morcos"
        ],
        "citations": 14,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning",
        "abstract": "Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is often necessary to optimize their performance. However, a major obstacle is the limited availability of labeled data. We study the use of pseudolabels, i.e., heuristic labels for unlabeled data, to enhance CLIP via prompt tuning. Conventional pseudolabeling trains a model on labeled data and then generates labels for unlabeled data. VLMs' zero-shot capabilities enable a\"second generation\"of pseudolabeling approaches that do not require task-specific training on labeled data. By using zero-shot pseudolabels as a source of supervision, we observe that learning paradigms such as semi-supervised, transductive zero-shot, and unsupervised learning can all be seen as optimizing the same loss function. This unified view enables the development of versatile training strategies that are applicable across learning paradigms. We investigate them on image classification tasks where CLIP exhibits limitations, by varying prompt modalities, e.g., textual or visual prompts, and learning paradigms. We find that (1) unexplored prompt tuning strategies that iteratively refine pseudolabels consistently improve CLIP accuracy, by 19.5 points in semi-supervised learning, by 28.4 points in transductive zero-shot learning, and by 15.2 points in unsupervised learning, and (2) unlike conventional semi-supervised pseudolabeling, which exacerbates model biases toward classes with higher-quality pseudolabels, prompt tuning leads to a more equitable distribution of per-class accuracy. The code to reproduce the experiments is at https://github.com/BatsResearch/menghini-neurips23-code.",
        "authors": [
            "Cristina Menghini",
            "Andrew T. Delworth",
            "Stephen H. Bach"
        ],
        "citations": 14,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models",
        "abstract": "Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",
        "authors": [
            "Yangyi Chen",
            "Karan Sikka",
            "Michael Cogswell",
            "Heng Ji",
            "Ajay Divakaran"
        ],
        "citations": 17,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Revisiting the Role of Language Priors in Vision-Language Models",
        "abstract": "Vision-language models (VLMs) are impactful in part because they can be applied to a variety of visual understanding tasks in a zero-shot fashion, without any fine-tuning. We study $\\textit{generative VLMs}$ that are trained for next-word generation given an image. We explore their zero-shot performance on the illustrative task of image-text retrieval across 8 popular vision-language benchmarks. Our first observation is that they can be repurposed for discriminative tasks (such as image-text retrieval) by simply computing the match score of generating a particular text string given an image. We call this probabilistic score the $\\textit{Visual Generative Pre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on others. We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions. In fact, we demonstrate that even a\"blind\"language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago. We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at test time without having to retrain or fine-tune the model. We show that the VisualGPTScore, when appropriately debiased, is a strong zero-shot baseline for vision-language understanding, oftentimes producing state-of-the-art accuracy.",
        "authors": [
            "Zhiqiu Lin",
            "Xinyue Chen",
            "Deepak Pathak",
            "Pengchuan Zhang",
            "Deva Ramanan"
        ],
        "citations": 15,
        "references": 79,
        "year": 2023
    },
    {
        "title": "Black-box Prompt Tuning for Vision-Language Model as a Service",
        "abstract": "In the scenario of Model-as-a-Service (MaaS), pre-trained models are usually released as inference APIs. Users are allowed to query those models with manually crafted prompts. Without accessing the network structure and gradient information, it's tricky to perform continuous prompt tuning on MaaS, especially for vision-language models (VLMs) considering cross-modal interaction. In this paper, we propose a black-box prompt tuning framework for VLMs to learn task-relevant prompts without back-propagation. In particular, the vision and language prompts are jointly optimized in the intrinsic parameter subspace with various evolution strategies. Different prompt variants are also explored to enhance the cross-model interaction. Experimental results show that our proposed black-box prompt tuning framework outperforms both hand-crafted prompt engineering and gradient-based prompt learning methods, which serves as evidence of its capability to train task-relevant prompts in a derivative-free manner.",
        "authors": [
            "Lang-Chi Yu",
            "Qin Chen",
            "Jiaju Lin",
            "Liang He"
        ],
        "citations": 15,
        "references": 38,
        "year": 2023
    },
    {
        "title": "Open-Vocabulary Object Detection using Pseudo Caption Labels",
        "abstract": "Recent open-vocabulary detection methods aim to detect novel objects by distilling knowledge from vision-language models (VLMs) trained on a vast amount of image-text pairs. To improve the effectiveness of these methods, researchers have utilized datasets with a large vocabulary that contains a large number of object classes, under the assumption that such data will enable models to extract comprehensive knowledge on the relationships between various objects and better generalize to unseen object classes. In this study, we argue that more fine-grained labels are necessary to extract richer knowledge about novel objects, including object attributes and relationships, in addition to their names. To address this challenge, we propose a simple and effective method named Pseudo Caption Labeling (PCL), which utilizes an image captioning model to generate captions that describe object instances from diverse perspectives. The resulting pseudo caption labels offer dense samples for knowledge distillation. On the LVIS benchmark, our best model trained on the de-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6, comparable to the state-of-the-art performance. PCL's simplicity and flexibility are other notable features, as it is a straightforward pre-processing technique that can be used with any image captioning model without imposing any restrictions on model architecture or training process.",
        "authors": [
            "Han-Cheol Cho",
            "Won Young Jhoo",
            "Woohyun Kang",
            "Byungseok Roh"
        ],
        "citations": 16,
        "references": 37,
        "year": 2023
    },
    {
        "title": "RadOcc: Learning Cross-Modality Occupancy Knowledge through Rendering Assisted Distillation",
        "abstract": "3D occupancy prediction is an emerging task that aims to estimate the occupancy states and semantics of 3D scenes using multi-view images. However, image-based scene perception encounters significant challenges in achieving accurate prediction due to the absence of geometric priors. In this paper, we address this issue by exploring cross-modal knowledge distillation in this task, i.e., we leverage a stronger multi-modal model to guide the visual model during training. In practice, we observe that directly applying features or logits alignment, proposed and widely used in bird's-eye-view (BEV) perception, does not yield satisfactory results. To overcome this problem, we introduce RadOcc, a Rendering assisted distillation paradigm for 3D Occupancy prediction. By employing differentiable volume rendering, we generate depth and semantic maps in perspective views and propose two novel consistency criteria between the rendered outputs of teacher and student models. Specifically, the depth consistency loss aligns the termination distributions of the rendered rays, while the semantic consistency loss mimics the intra-segment similarity guided by vision foundation models (VLMs). Experimental results on the nuScenes dataset demonstrate the effectiveness of our proposed method in improving various 3D occupancy prediction approaches, e.g., our proposed methodology enhances our baseline by 2.2% in the metric of mIoU and achieves 50% in Occ3D benchmark.",
        "authors": [
            "Haiming Zhang",
            "Xu Yan",
            "Dongfeng Bai",
            "Jiantao Gao",
            "Pan Wang",
            "Bingbing Liu",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "citations": 15,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
        "abstract": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and 1552.5 in MME-perception.",
        "authors": [
            "Yuechen Zhang",
            "Shengju Qian",
            "Bohao Peng",
            "Shu Liu",
            "Jiaya Jia"
        ],
        "citations": 14,
        "references": 53,
        "year": 2023
    },
    {
        "title": "DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning",
        "abstract": "Compositional Zero-shot Learning (CZSL) aims to recognize novel concepts composed of known knowledge without training samples. Standard CZSL either identifies visual primitives or enhances unseen composed entities, and as a result, entanglement between state and object primitives cannot be fully utilized. Admittedly, vision- language models (VLMs) could naturally cope with CZSL through tuning prompts, while uneven entanglement leads prompts to be dragged into local optimum. In this paper, we take a further step to introduce a novel Disentangled and Recurrent Prompt Tuning framework termed DRPT to better tap the potential of VLMs in CZSL. Specifically, the state and object primitives are deemed as learnable tokens of vocabulary embedded in prompts and tuned on seen compositions. Instead of jointly tuning state and object, we devise a disentangled and recurrent tuning strategy to suppress the traction force caused by entanglement and gradually optimize the token parameters, leading to a better prompt space. Notably, we develop a progressive fine-tuning procedure that allows for incremental updates to the prompts, optimizing the object first, then the state, and vice versa. Meanwhile, the optimization of state and object is independent, thus clearer features can be learned to further alleviate the issue of entangling misleading optimization. Moreover, we quantify and analyze the entanglement in CZSL and supplement entanglement rebalancing optimization schemes. DRPT surpasses representative state-of-the-art methods on extensive benchmark datasets, demonstrating superiority in both accuracy and efficiency.",
        "authors": [
            "Xiaocheng Lu",
            "Ziming Liu",
            "Song Guo",
            "Jingcai Guo",
            "Fushuo Huo",
            "Sikai Bai",
            "Tao Han"
        ],
        "citations": 14,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models",
        "abstract": "We study open-world 3D scene understanding, a family of tasks that require agents to reason about their 3D environment with an open-set vocabulary and out-of-domain visual inputs - a critical skill for robots to operate in the unstructured 3D world. Towards this end, we propose Semantic Abstraction (SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D spatial capabilities, while maintaining their zero-shot robustness. We achieve this abstraction using relevancy maps extracted from CLIP, and learn 3D spatial and geometric reasoning skills on top of those abstractions in a semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two open-world 3D scene understanding tasks: 1) completing partially observed objects and 2) localizing hidden objects from language descriptions. Experiments show that SemAbs can generalize to novel vocabulary, materials/lighting, classes, and domains (i.e., real-world scans) from training on limited 3D synthetic data. Code and data is available at https://semantic-abstraction.cs.columbia.edu/",
        "authors": [
            "Huy Ha",
            "Shuran Song"
        ],
        "citations": 89,
        "references": 62,
        "year": 2022
    },
    {
        "title": "EventCLIP: Adapting CLIP for Event-based Object Recognition",
        "abstract": "Recent advances in zero-shot and few-shot classification heavily rely on the success of pre-trained vision-language models (VLMs) such as CLIP. Due to a shortage of large-scale datasets, training such models for event camera data remains infeasible. Thus, adapting existing VLMs across modalities to event vision is an important research challenge. In this work, we introduce EventCLIP, a novel approach that utilizes CLIP for zero-shot and few-shot event-based object recognition. We first generalize CLIP's image encoder to event data by converting raw events to 2D grid-based representations. To further enhance performance, we propose a feature adapter to aggregate temporal information over event frames and refine text embeddings to better align with the visual inputs. We evaluate EventCLIP on N-Caltech, N-Cars, and N-ImageNet datasets, achieving state-of-the-art few-shot performance. When fine-tuned on the entire dataset, our method outperforms all existing event classifiers. Moreover, we explore practical applications of EventCLIP including robust event classification and label-free event recognition, where our approach surpasses previous baselines designed specifically for these tasks.",
        "authors": [
            "Ziyi Wu",
            "Xudong Liu",
            "Igor Gilitschenski"
        ],
        "citations": 12,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Adversarial Prompt Tuning for Vision-Language Models",
        "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
        "authors": [
            "Jiaming Zhang",
            "Xingjun Ma",
            "Xin Wang",
            "Lingyu Qiu",
            "Jiaqi Wang",
            "Yu-Gang Jiang",
            "Jitao Sang"
        ],
        "citations": 13,
        "references": 50,
        "year": 2023
    },
    {
        "title": "VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores",
        "abstract": "Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as P ( match | text , image ) have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the V isual G enerative P re-T raining Score ( Visu-alGPTScore ) of P ( text | image ) , a multimodal generative score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the marginal P(text) and the Pointwise Mutual Information (PMI). This helps to (a) diagnose datasets with strong language bias, and (b) debias results on other benchmarks like Winoground using an information-theoretic framework. Visual-GPTScore provides valuable insights and serves as a strong baseline for future evaluation of visio-linguistic compositionality.",
        "authors": [
            "Zhiqiu Lin",
            "Xinyue Chen",
            "Deepak Pathak",
            "Pengchuan Zhang",
            "Deva Ramanan"
        ],
        "citations": 13,
        "references": 99,
        "year": 2023
    },
    {
        "title": "GIPCOL: Graph-Injected Soft Prompting for Compositional Zero-Shot Learning",
        "abstract": "Pre-trained vision-language models (VLMs) have achieved promising success in many fields, especially with prompt learning paradigm. In this work, we propose GIPCOL (Graph-Injected Soft Prompting for Compositional Learning) to better explore the compositional zero-shot learning (CZSL) ability of VLMs within the prompt-based learning framework. The soft prompt in GIPCOL is structured and consists of the prefix learnable vectors, attribute label and object label. In addition, the attribute and object labels in the soft prompt are designated as nodes in a compositional graph. The compositional graph is constructed based on the compositional structure of the objects and attributes extracted from the training data and consequently feeds the updated concept representation into the soft prompt to capture this compositional structure for a better prompting for CZSL. With the new prompting strategy, GIPCOL achieves state-of-the-art AUC results on all three CZSL benchmarks, including MIT-States, UT-Zappos, and C-GQA datasets in both closed and open settings compared to previous non-CLIP as well as CLIP-based methods. We analyze when and why GIPCOL operates well given the CLIP backbone and its training data limitations, and our findings shed light on designing more effective prompts for CZSL.",
        "authors": [
            "Guangyue Xu",
            "Joyce Chai",
            "Parisa Kordjamshidi"
        ],
        "citations": 12,
        "references": 36,
        "year": 2023
    },
    {
        "title": "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models",
        "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model. Code is available at https://github.com/ExplainableML/ProbVLM",
        "authors": [
            "Uddeshya Upadhyay",
            "Shyamgopal Karthik",
            "Massimiliano Mancini",
            "Zeynep Akata"
        ],
        "citations": 12,
        "references": 109,
        "year": 2023
    },
    {
        "title": "Distribution-Aware Prompt Tuning for Vision-Language Models",
        "abstract": "Pre-trained vision-language models (VLMs) have shown impressive performance on various downstream tasks by utilizing knowledge learned from large data. In general, the performance of VLMs on target tasks can be further improved by prompt tuning, which adds context to the input image or text. By leveraging data from target tasks, various prompt-tuning methods have been studied in the literature. A key to prompt tuning is the feature space alignment between two modalities via learnable vectors with model parameters fixed. We observed that the alignment becomes more effective when embeddings of each modality are ‚Äòwell-arranged‚Äô in the latent space. Inspired by this observation, we proposed distribution-aware prompt tuning (DAPT) for vision-language models, which is simple yet effective. Specifically, the prompts are learned by maximizing inter-dispersion, the distance between classes, as well as minimizing the intra-dispersion measured by the distance between embeddings from the same class. Our extensive experiments on 11 benchmark datasets demonstrate that our method significantly improves generalizability. The code is available at https://github.com/mlvlab/DAPT.",
        "authors": [
            "Eulrang Cho",
            "Jooyeon Kim",
            "Hyunwoo J. Kim"
        ],
        "citations": 12,
        "references": 48,
        "year": 2023
    },
    {
        "title": "FunQA: Towards Surprising Video Comprehension",
        "abstract": "Surprising videos, such as funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question-answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose higher-level tasks, such as attributing a fitting and vivid title to the video and scoring the video creativity. In total, the FunQA benchmark consists of 312K free-text QA pairs derived from 4.3K video clips, spanning a total of 24 video hours. Moreover, we propose FunMentor, an agent designed for Vision-Language Models (VLMs) that uses multi-turn dialogues to enhance models' understanding of counter-intuitiveness. Extensive experiments with existing VLMs demonstrate the effectiveness of FunMentor and reveal significant performance gaps for the FunQA videos across spatial-temporal reasoning, visual-centered reasoning, and free-text generation.",
        "authors": [
            "Binzhu Xie",
            "Sicheng Zhang",
            "Zitang Zhou",
            "Bo Li",
            "Yuanhan Zhang",
            "Jack Hessel",
            "Jingkang Yang",
            "Ziwei Liu"
        ],
        "citations": 12,
        "references": 75,
        "year": 2023
    },
    {
        "title": "VicTR: Video-conditioned Text Representations for Activity Recognition",
        "abstract": "Vision-Language models (VLMs) have excelled in the image-domain- especially in zero-shot settings- thanks to the availability of vast pretraining data (i.e., paired image-text samples). However for videos, such paired data is not as abundant. Therefore, video- VLMs are usually designed by adapting pretrained image- VLMs to the video-domain, instead of training from scratch. All such recipes rely on aug-menting visual embeddings with temporal information (i.e., image -+ video), often keeping text embeddings unchanged or even being discarded. In this paper, we argue the contrary, that better video- VLMs can be designed by focusing more on augmenting text, rather than visual information. More specifically, we introduce Video-conditioned Text Representations (Vi c TR): a form of text embeddings optimized w.r.t. vi-sual embeddings, creating a more-flexible contrastive latent space. Our model canfurther make use offreely-available semantic information, in the form of visually- grounded aux-iliary text (e.g. object or scene information). We evaluate our model on few-shot, zero-shot (HMDB-51, UCF-10l), short-form (Kinetics-400) and long-form (Charades) activ-ity recognition benchmarks, showing strong performance among video-VLMs.",
        "authors": [
            "Kumara Kahatapitiya",
            "Anurag Arnab",
            "Arsha Nagrani",
            "M. Ryoo"
        ],
        "citations": 13,
        "references": 101,
        "year": 2023
    },
    {
        "title": "Grounding Classical Task Planners via Vision-Language Models",
        "abstract": "Classical planning systems have shown great advances in utilizing rule-based human knowledge to compute accurate plans for service robots, but they face challenges due to the strong assumptions of perfect perception and action executions. To tackle these challenges, one solution is to connect the symbolic states and actions generated by classical planners to the robot's sensory observations, thus closing the perception-action loop. This research proposes a visually-grounded planning framework, named TPVQA, which leverages Vision-Language Models (VLMs) to detect action failures and verify action affordances towards enabling successful plan execution. Results from quantitative experiments show that TPVQA surpasses competitive baselines from previous studies in task completion rate.",
        "authors": [
            "Xiaohan Zhang",
            "Yan Ding",
            "S. Amiri",
            "Hao Yang",
            "Andy Kaminski",
            "Chad Esselink",
            "Shiqi Zhang"
        ],
        "citations": 13,
        "references": 39,
        "year": 2023
    },
    {
        "title": "Probing Conceptual Understanding of Large Visual-Language Models",
        "abstract": "In recent years large visual-language (V+L) models have achieved great success in various downstream tasks. However, it is not well studied whether these models have a conceptual grasp of the visual content. In this work we focus on conceptual understanding of these large V+L models. To facilitate this study, we propose novel benchmarking datasets for probing three different aspects of content understanding, 1) relations, 2) composition, and 3) context. Our probes are grounded in cognitive science and help determine if a V+L model can, for example, determine if snow garnished with a man is implausible, or if it can identify beach furniture by knowing it is located on a beach. We experimented with many recent state-of-the-art V+L models and observe that these models mostly fail to demonstrate a conceptual understanding. This study reveals several interesting insights such as that cross-attention helps learning conceptual understanding, and that CNNs are better with texture and patterns, while Transformers are better at color and shape. We further utilize some of these insights and investigate a simple finetuning technique that rewards the three conceptual understanding measures with promising initial results. The proposed benchmarks will drive the community to delve deeper into conceptual understanding and foster advancements in the capabilities of large V+L models. The code and dataset is available at: https://tinyurl.com/vlm-robustness",
        "authors": [
            "Madeline Chantry Schiappa",
            "Michael Cogswell",
            "Ajay Divakaran",
            "Y. Rawat"
        ],
        "citations": 12,
        "references": 60,
        "year": 2023
    },
    {
        "title": "Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?",
        "abstract": "\n The field of multimedia research has witnessed significant interest in leveraging multimodal pretrained neural network models to perceive and represent the physical world. Among these models, vision-language pretraining (VLP) has emerged as a captivating topic. Currently, the prevalent approach in VLP involves supervising the training process with paired image-text data. However, limited efforts have been dedicated to exploring the extraction of essential linguistic knowledge, such as semantics and syntax, during VLP and understanding its impact on multimodal alignment. In response, our study aims to shed light on the influence of comprehensive linguistic knowledge encompassing semantic expression and syntactic structure on multimodal alignment. To achieve this, we introduce SNARE, a large-scale multimodal alignment probing benchmark designed specifically for the detection of vital linguistic components, including lexical, semantic, and syntax knowledge. SNARE offers four distinct tasks: Semantic Structure, Negation Logic, Attribute Ownership, and Relationship Composition. Leveraging SNARE, we conduct holistic analyses of six advanced VLP models (BLIP, CLIP, Flava, X-VLM, BLIP2, and GPT-4), along with human performance, revealing key characteristics of the VLP model:\n i)\n Insensitivity to complex syntax structures, relying primarily on content words for sentence comprehension.\n ii)\n Limited comprehension of sentence combinations and negations.\n iii)\n Challenges in determining actions or spatial relations within visual information, as well as difficulties in verifying the correctness of ternary relationships. Based on these findings, we propose the following strategies to enhance multimodal alignment in VLP: 1) Utilize a large generative language model as the language backbone in VLP to facilitate the understanding of complex sentences. 2) Establish high-quality datasets that emphasize content words and employ simple syntax, such as short-distance semantic composition, to improve multimodal alignment. 3) Incorporate more fine-grained visual knowledge, such as spatial relationships, into pretraining objectives.\n \n 1\n \n",
        "authors": [
            "Fei Wang",
            "Liang Ding",
            "Jun Rao",
            "Ye Liu",
            "Li-juan Shen",
            "Changxing Ding"
        ],
        "citations": 13,
        "references": 78,
        "year": 2023
    },
    {
        "title": "A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues",
        "abstract": "Conditional inference on joint textual and visual clues is a multi-modal reasoning task that textual clues provide prior permutation or external knowledge, which are complementary with visual content and pivotal to deducing the correct option. Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information. To address this issue, we propose a Multi-modal Context Reasoning approach, named ModCR. Compared to VLMs performing reasoning via cross modal semantic alignment, it regards the given textual abstract semantic and objective image information as the pre-context information and embeds them into the language model to perform context reasoning. Different from recent vision-aided language models used in natural language processing, ModCR incorporates the multi-view semantic alignment information between language and vision by introducing the learnable alignment prefix between image and text in the pretrained language model. This makes the language model well-suitable for such multi-modal reasoning scenario on joint textual and visual clues. We conduct extensive experiments on two corresponding data sets and experimental results show significantly improved performance (exact gain by 4.8% on PMR test set) compared to previous strong baselines.",
        "authors": [
            "Yunxin Li",
            "Baotian Hu",
            "Xinyu Chen",
            "Yuxin Ding",
            "Lin Ma",
            "Min Zhang"
        ],
        "citations": 12,
        "references": 56,
        "year": 2023
    },
    {
        "title": "Task Residual for Tuning Vision-Language Models",
        "abstract": "Large-scale vision-language models (VLMs) pre-trained on billion-level data have learned general visual representations and broad visual concepts. In principle, the welllearned knowledge structure of the VLMs should be inherited appropriately when being transferred to downstream tasks with limited data. However, most existing efficient transfer learning (ETL) approaches for VLMs either damage or are excessively biased towards the prior knowledge, e.g., prompt tuning (PT) discards the pre-trained text-based classifier and builds a new one while adapter-style tuning (AT) fully relies on the pre-trained features. To address this, we propose a new efficient tuning approach for VLMs named Task Residual Tuning (TaskRes), which performs directly on the text-based classifier and explicitly decouples the prior knowledge of the pre-trained models and new knowledge regarding a target task. Specifically, TaskRes keeps the original classifier weights from the VLMs frozen and obtains a new classifier for the target task by tuning a set of prior-independent parameters as a residual to the original one, which enables reliable prior knowledge preservation and flexible task-specific knowledge exploration. The proposed TaskRes is simple yet effective, which significantly outperforms previous ETL methods (e.g., PT and AT) on 11 benchmark datasets while requiring minimal effort for the implementation. Our code is available at https://github.com/geekyutao/TaskRes.",
        "authors": [
            "Tao Yu",
            "Zhihe Lu",
            "Xin Jin",
            "Zhibo Chen",
            "Xinchao Wang"
        ],
        "citations": 58,
        "references": 78,
        "year": 2022
    },
    {
        "title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning",
        "abstract": "We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP also outperforms CoOp, a soft prompting method that fine-tunes the prefix context tokens, by an average of 5.8 percentage points on AUC. We perform additional experiments to show that CSP improves generalization to higher-order attribute-attribute-object compositions (e.g., old white cat) and combinations of pretrained attributes and fine-tuned objects. The code is available at https://github.com/BatsResearch/csp.",
        "authors": [
            "Nihal V. Nayak",
            "Peilin Yu",
            "Stephen H. Bach"
        ],
        "citations": 48,
        "references": 79,
        "year": 2022
    },
    {
        "title": "Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning",
        "abstract": "The pre-train and fine-tune paradigm in machine learning has had dramatic success in a wide range of domains because the use of existing data or pre-trained models on the internet enables quick and easy learning of new tasks. We aim to enable this paradigm in robotic reinforcement learning, allowing a robot to learn a new task with little human effort by leveraging data and models from the Internet. However, reinforcement learning often requires significant human effort in the form of manual reward specification or environment resets, even if the policy is pre-trained. We introduce RoboFuME, a reset-free fine-tuning system that pre-trains a multi-task manipulation policy from diverse datasets of prior experiences and self-improves online to learn a target task with minimal human intervention. Our insights are to utilize calibrated offline reinforcement learning techniques to ensure efficient online fine-tuning of a pre-trained policy in the presence of distribution shifts and leverage pre-trained vision language models (VLMs) to build a robust reward classifier for autonomously providing reward signals during the online fine-tuning process. In a diverse set of five real robot manipulation tasks, we show that our method can incorporate data from an existing robot dataset collected at a different institution and improve on a target task within as little as 3 hours of autonomous real-world experience. We also demonstrate in simulation experiments that our method outperforms prior works that use different RL algorithms or different approaches for predicting rewards. Project website: https://robofume.github.io",
        "authors": [
            "Jingyun Yang",
            "Max Sobol Mark",
            "Brandon Vu",
            "Archit Sharma",
            "Jeannette Bohg",
            "Chelsea Finn"
        ],
        "citations": 11,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?",
        "abstract": "Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at https://github.com/vl-illusion/dataset.",
        "authors": [
            "Yichi Zhang",
            "Jiayi Pan",
            "Yuchen Zhou",
            "Rui Pan",
            "Joyce Chai"
        ],
        "citations": 11,
        "references": 37,
        "year": 2023
    },
    {
        "title": "Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation",
        "abstract": "Pre-trained Vision-Language Models (VLMs), such as CLIP, have shown enhanced performance across a range of tasks that involve the integration of visual and linguistic modalities. When CLIP is used for depth estimation tasks, the patches, divided from the input images, can be combined with a series of semantic descriptions of the depth information to obtain similarity results. The coarse estimation of depth is then achieved by weighting and summing the depth values, called depth bins, corresponding to the predefined semantic descriptions. The zero-shot approach circumvents the computational and time-intensive nature of traditional fully-supervised depth estimation methods. However, this method, utilizing fixed depth bins, may not effectively generalize as images from different scenes may exhibit distinct depth distributions. To address this challenge, we propose a few-shot-based method which learns to adapt the VLMs for monocular depth estimation to balance training costs and generalization capabilities. Specifically, it assigns different depth bins for different scenes, which can be selected by the model during inference. Additionally, we incorporate learnable prompts to preprocess the input text to convert the easily human-understood text into easily model-understood vectors and further enhance the performance. With only one image per scene for training, our extensive experiment results on the NYU V2 and KITTI dataset demonstrate that our method outperforms the previous state-of-the-art method by up to 10.6% in terms of MARE1.",
        "authors": [
            "Xue-mei Hu",
            "Ce Zhang",
            "Yi Zhang",
            "Bowen Hai",
            "Ke Yu",
            "Zhihai He"
        ],
        "citations": 10,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models",
        "abstract": "Medical image segmentation allows quantifying target structure size and shape, aiding in disease diagnosis, prognosis, surgery planning, and comprehension.Building upon recent advancements in foundation Vision-Language Models (VLMs) from natural image-text pairs, several studies have proposed adapting them to Vision-Language Segmentation Models (VLSMs) that allow using language text as an additional input to segmentation models. Introducing auxiliary information via text with human-in-the-loop prompting during inference opens up unique opportunities, such as open vocabulary segmentation and potentially more robust segmentation models against out-of-distribution data. Although transfer learning from natural to medical images has been explored for image-only segmentation models, the joint representation of vision-language in segmentation problems remains underexplored. This study introduces the first systematic study on transferring VLSMs to 2D medical images, using carefully curated $11$ datasets encompassing diverse modalities and insightful language prompts and experiments. Our findings demonstrate that although VLSMs show competitive performance compared to image-only models for segmentation after finetuning in limited medical image datasets, not all VLSMs utilize the additional information from language prompts, with image features playing a dominant role. While VLSMs exhibit enhanced performance in handling pooled datasets with diverse modalities and show potential robustness to domain shifts compared to conventional segmentation models, our results suggest that novel approaches are required to enable VLSMs to leverage the various auxiliary information available through language prompts. The code and datasets are available at https://github.com/naamiinepal/medvlsm.",
        "authors": [
            "K. Poudel",
            "Manish Dhakal",
            "Prasiddha Bhandari",
            "Rabin Adhikari",
            "Safal Thapaliya",
            "Bishesh Khanal"
        ],
        "citations": 10,
        "references": 83,
        "year": 2023
    },
    {
        "title": "BDC-Adapter: Brownian Distance Covariance for Better Vision-Language Reasoning",
        "abstract": "Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP and ALIGN, have introduced a new paradigm for learning transferable visual representations. Recently, there has been a surge of interest among researchers in developing lightweight fine-tuning techniques to adapt these models to downstream visual tasks. We recognize that current state-of-the-art fine-tuning methods, such as Tip-Adapter, simply consider the covariance between the query image feature and features of support few-shot training samples, which only captures linear relations and potentially instigates a deceptive perception of independence. To address this issue, in this work, we innovatively introduce Brownian Distance Covariance (BDC) to the field of vision-language reasoning. The BDC metric can model all possible relations, providing a robust metric for measuring feature dependence. Based on this, we present a novel method called BDC-Adapter, which integrates BDC prototype similarity reasoning and multi-modal reasoning network prediction to perform classification tasks. Our extensive experimental results show that the proposed BDC-Adapter can freely handle non-linear relations and fully characterize independence, outperforming the current state-of-the-art methods by large margins.",
        "authors": [
            "Yi Zhang",
            "Ce Zhang",
            "Zihan Liao",
            "Yushun Tang",
            "Zhihai He"
        ],
        "citations": 10,
        "references": 75,
        "year": 2023
    },
    {
        "title": "GenZI: Zero-Shot 3D Human-Scene Interaction Generation",
        "abstract": "Can we synthesize 3D humans interacting with scenes without learning from any 3D human-scene interaction data? We propose GenZI11Project page: craigleili.github.io/projects/genzi, the first zero-shot approach to generating 3D human-scene interactions. Key to GenZI is our distillation of interaction priors from large vision-language models (VLMs), which have learned a rich semantic space of 2D human-scene compositions. Given a natural language description and a coarse point location of the desired interaction in a 3D scene, we first leverage VLMs to imagine plausible 2D human interactions inpainted into multiple rendered views of the scene. We then formulate a robust iterative optimization to synthesize the pose and shape of a 3D human model in the scene, guided by consistency with the 2D interaction hypotheses. In contrast to existing learning-based approaches, GenZI circumvents the conventional need for captured 3D interaction data, and allows for flexible control of the 3D interaction synthesis with easy-to-use text prompts. Extensive experiments show that our zero-shot approach has high flexibility and generality, making it applicable to diverse scene types, including both indoor and outdoor environments.",
        "authors": [
            "Lei Li",
            "Angela Dai"
        ],
        "citations": 10,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling",
        "abstract": "Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both text-only and multimodal tasks, with experiments that account for both parameter-count scaling and training regime (with and without instruction tuning).",
        "authors": [
            "Yaqing Wang",
            "Jialin Wu",
            "T. Dabral",
            "Jiageng Zhang",
            "Geoff Brown",
            "Chun-Ta Lu",
            "Frederick Liu",
            "Yi Liang",
            "Bo Pang",
            "Michael Bendersky",
            "Radu Soricut"
        ],
        "citations": 11,
        "references": 46,
        "year": 2023
    },
    {
        "title": "SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples",
        "abstract": "While vision-language models (VLMs) have achieved re-markable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of imagetext pairs for various combinations of social attributes. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intersectional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender). Through our over-generate-then-filter methodology, we produce SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics. We conduct extensive experiments to demonstrate the usefulness of our generated dataset for probing and mitigating intersectional social biases in state-of-the-art VLMs.",
        "authors": [
            "Phillip Howard",
            "Avinash Madasu",
            "Tiep Le",
            "Gustavo Lujan Moreno",
            "Anahita Bhiwandiwalla",
            "Vasudev Lal"
        ],
        "citations": 10,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Bridging Language and Action: A Survey of Language-Conditioned Robot Manipulation",
        "abstract": "Language-conditioned robot manipulation is an emerging field aimed at enabling seamless communication and cooperation between humans and robotic agents by teaching robots to comprehend and execute instructions conveyed in natural language. This interdisciplinary area integrates scene understanding, language processing, and policy learning to bridge the gap between human instructions and robotic actions. In this comprehensive survey, we systematically explore recent advancements in language-conditioned robotic manipulation. We categorize existing methods into language-conditioned reward shaping, language-conditioned policy learning, neuro-symbolic artificial intelligence, and the utilization of foundational models (FMs) such as large language models (LLMs) and vision-language models (VLMs). Specifically, we analyze state-of-the-art techniques concerning semantic information extraction, environment and evaluation, auxiliary tasks, and task representation strategies. By conducting a comparative analysis, we highlight the strengths and limitations of current approaches in bridging language instructions with robot actions. Finally, we discuss open challenges and future research directions, focusing on potentially enhancing generalization capabilities and addressing safety issues in language-conditioned robot manipulators. The GitHub repository of this paper can be found at https://github.com/hk-zh/language-conditioned-robot-manipulation-models.",
        "authors": [
            "Hongkuan Zhou",
            "Xiangtong Yao",
            "Oier Mees",
            "Yuan Meng",
            "Ted Xiao",
            "Yonatan Bisk",
            "Jean Oh",
            "Edward Johns",
            "Mohit Shridhar",
            "Dhruv Shah",
            "Jesse Thomason",
            "Kai Huang",
            "Joyce Chai",
            "Zhenshan Bing",
            "Alois Knoll"
        ],
        "citations": 10,
        "references": 342,
        "year": 2023
    },
    {
        "title": "Cross-Modal Concept Learning and Inference for Vision-Language Models",
        "abstract": "Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP, establish the correlation between texts and images, achieving remarkable success on various downstream tasks with fine-tuning. In existing fine-tuning methods, the class-specific text description is matched against the whole image. We recognize that this whole image matching is not effective since images from the same class often contain a set of different semantic objects, and an object further consists of a set of semantic parts or concepts. Individual semantic parts or concepts may appear in image samples from different classes. To address this issue, in this paper, we develop a new method called cross-model concept learning and inference (CCLI). Using the powerful text-image correlation capability of CLIP, our method automatically learns a large set of distinctive visual concepts from images using a set of semantic text concepts. Based on these visual concepts, we construct a discriminative representation of images and learn a concept inference network to perform downstream image classification tasks, such as few-shot learning and domain generalization. Extensive experimental results demonstrate that our CCLI method is able to improve the performance upon the current state-of-the-art methods by large margins, for example, by up to 8.0% improvement on few-shot learning and by up to 1.3% for domain generalization.",
        "authors": [
            "Yi Zhang",
            "Ce Zhang",
            "Yushun Tang",
            "Z. He"
        ],
        "citations": 10,
        "references": 99,
        "year": 2023
    },
    {
        "title": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities",
        "abstract": "The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of vision and language understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capacity. In this work, we introduce Acoustic Prompt Turning (APT), a new adapter extending LLMs and VLMs to the audio domain by soft prompting only. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as language model inputs. To mitigate the data scarcity in the audio domain, a multi-task learning strategy is proposed by formulating diverse audio tasks in a sequence-to-sequence manner. Moreover, we improve the framework of audio language model by using interleaved audio-text embeddings as the input sequence. This improved framework imposes zero constraints on the input format and thus is capable of tackling more understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate the reasoning ability of audio networks, we propose natural language audio reasoning (NLAR), a new task that analyses across two audio clips by comparison and summarization. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the targeted datasets) across various tasks. We finally demonstrate the APT's ability in extending frozen VLMs to the audio domain without finetuning, achieving promising results in the audio-visual question and answering task. Our code and model weights are released at https://github.com/JinhuaLiang/APT.",
        "authors": [
            "Jinhua Liang",
            "Xubo Liu",
            "Wenwu Wang",
            "Mark D. Plumbley",
            "Huy Phan",
            "Emmanouil Benetos"
        ],
        "citations": 9,
        "references": 35,
        "year": 2023
    },
    {
        "title": "Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models",
        "abstract": "Vision-language models (VLMs) pre-trained on large- scale image-text pairs have demonstrated impressive transferability on various visual tasks. Transferring knowledge from such powerful VLMs is a promising direction for building effective video recognition models. However, current exploration in this field is still limited. We believe that the greatest value of pre-trained VLMs lies in building a bridge between visual and textual domains. In this paper, we propose a novel framework called BIKE, which utilizes the cross-modal bridge to explore bidirectional knowledge: i) We introduce the Video Attribute Association mechanism, which leverages the Video-to-Text knowledge to generate textual auxiliary attributes for complementing video recognition. ii) We also present a Temporal Concept Spotting mechanism that uses the Text-to-Video expertise to capture temporal saliency in a parameter-free manner, leading to enhanced video representation. Extensive studies on six popular video datasets, including Kinetics-400 & 600, UCF-101, HMDB-51, ActivityNet and Charades, show that our method achieves state-of-the-art performance in various recognition scenarios, such as general, zero-shot, and few-shot video recognition. Our best model achieves a state-of-the-art accuracy of 88.6% on the challenging Kinetics-400 using the released CLIP model. The code is available at https://github.com/whwu95/BIKE.",
        "authors": [
            "Wenhao Wu",
            "Xiaohan Wang",
            "Haipeng Luo",
            "Jingdong Wang",
            "Yi Yang",
            "Wanli Ouyang"
        ],
        "citations": 43,
        "references": 76,
        "year": 2022
    },
    {
        "title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
        "abstract": "In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.",
        "authors": [
            "Ted Xiao",
            "Harris Chan",
            "P. Sermanet",
            "Ayzaan Wahid",
            "Anthony Brohan",
            "Karol Hausman",
            "S. Levine",
            "Jonathan Tompson"
        ],
        "citations": 54,
        "references": 56,
        "year": 2022
    },
    {
        "title": "Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering",
        "abstract": "In this paper, we explore effective prompting techniques to enhance zero- and few-shot Visual Question Answering (VQA) performance in contemporary Vision-Language Models (VLMs). Central to our investigation is the role of question templates in guiding VLMs to generate accurate answers. We identify that specific templates significantly influence VQA outcomes, underscoring the need for strategic template selection. Another pivotal aspect of our study is augmenting VLMs with image captions, providing them with additional visual cues alongside direct image features in VQA tasks. Surprisingly, this augmentation significantly improves the VLMs' performance in many cases, even though VLMs\"see\"the image directly! We explore chain-of-thought (CoT) reasoning and find that while standard CoT reasoning causes drops in performance, advanced methods like self-consistency can help recover it. Furthermore, we find that text-only few-shot examples enhance VLMs' alignment with the task format, particularly benefiting models prone to verbose zero-shot answers. Lastly, to mitigate the challenges associated with evaluating free-form open-ended VQA responses using string-matching based VQA metrics, we introduce a straightforward LLM-guided pre-processing technique to adapt the model responses to the expected ground-truth answer distribution. In summary, our research sheds light on the intricacies of prompting strategies in VLMs for VQA, emphasizing the synergistic use of captions, templates, and pre-processing to enhance model efficacy.",
        "authors": [
            "Rabiul Awal",
            "Le Zhang",
            "Aishwarya Agrawal"
        ],
        "citations": 9,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Towards General Purpose Medical AI: Continual Learning Medical Foundation Model",
        "abstract": "Inevitable domain and task discrepancies in real-world scenarios can impair the generalization performance of the pre-trained deep models for medical data. Therefore, we audaciously propose that we should build a general-purpose medical AI system that can be seamlessly adapted to downstream domains/tasks. Since the domain/task adaption procedures usually involve additional labeling work for the target data, designing a data-efficient adaption algorithm is desired to save the cost of transferring the learned knowledge. Our recent work found that vision-language models (VLMs) are efficient learners with extraordinary cross-domain ability. Therefore, in this work, we further explore the possibility of leveraging pre-trained VLMs as medical foundation models for building general-purpose medical AI, where we thoroughly investigate three machine-learning paradigms, i.e., domain/task-specialized learning, joint learning, and continual learning, for training the VLMs and evaluate their generalization performance on cross-domain and cross-task test sets. To alleviate the catastrophic forgetting during sequential training, we employ rehearsal learning and receive a sharp boost in terms of generalization capability. In a nutshell, our empirical evidence suggests that continual learning may be a practical and efficient learning paradigm for the medical foundation model. And we hope researchers can use our empirical evidence as basement to further explore the path toward medical foundation model.",
        "authors": [
            "Huahui Yi",
            "Ziyuan Qin",
            "Qicheng Lao",
            "Wei Xu",
            "Zekun Jiang",
            "Dequan Wang",
            "Shaoting Zhang",
            "Kang Li"
        ],
        "citations": 9,
        "references": 41,
        "year": 2023
    },
    {
        "title": "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation",
        "abstract": "State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual structural information of multiple granularities, such as concepts, relations, and events, in a well-organized structured format. Second, we introduce curriculum-based learning for VLMs to progressively comprehend visual structures, from fundamental visual concepts to intricate event structures. Our intuition is that lower-level knowledge may contribute to complex visual structure understanding. Furthermore, we compile and release a collection of datasets tailored for visual structural knowledge extraction. We adopt a weakly-supervised approach to directly generate visual event structures from captions for ViStruct training, capitalizing on abundant image-caption pairs from the web. In experiments, we evaluate ViStruct on visual structure prediction tasks, demonstrating its effectiveness in improving the understanding of visual structures. The code is public at \\url{https://github.com/Yangyi-Chen/vi-struct}.",
        "authors": [
            "Yangyi Chen",
            "Xingyao Wang",
            "Manling Li",
            "Derek Hoiem",
            "Heng Ji"
        ],
        "citations": 9,
        "references": 92,
        "year": 2023
    },
    {
        "title": "Understanding and Mitigating Overfitting in Prompt Tuning for Vision-Language Models",
        "abstract": "Pretrained vision-language models (VLMs) such as CLIP have shown impressive generalization capability in downstream vision tasks with appropriate text prompts. Instead of designing prompts manually, Context Optimization (CoOp) has been recently proposed to learn continuous prompts using task-specific training data. Despite the performance improvements on downstream tasks, several studies have reported that CoOp suffers from the overfitting issue in two aspects: (i) the test accuracy on base classes first improves and then worsens during training; (ii) the test accuracy on novel classes keeps decreasing. However, none of the existing studies can understand and mitigate such overfitting problems. In this study, we first explore the cause of overfitting by analyzing the gradient flow. Comparative experiments reveal that CoOp favors generalizable and spurious features in the early and later training stages, respectively, leading to the non-overfitting and overfitting phenomena. Given those observations, we propose Subspace Prompt Tuning (Sub PT) to project the gradients in back-propagation onto the low-rank subspace spanned by the early-stage gradient flow eigenvectors during the entire training process and successfully eliminate the overfitting problem. In addition, we equip CoOp with a Novel Feature Learner (NFL) to enhance the generalization ability of the learned prompts onto novel categories beyond the training set, needless of image training data. Extensive experiments on 11 classification datasets demonstrate that Sub PT+NFL consistently boost the performance of CoOp and outperform the state-of-the-art CoCoOp approach. Experiments on more challenging vision downstream tasks, including open-vocabulary object detection and zero-shot semantic segmentation, also verify the effectiveness of the proposed method. Codes can be found at https://tinyurl.com/mpe64f89.",
        "authors": [
            "Cheng Ma",
            "Yang Liu",
            "Jiankang Deng",
            "Lingxi Xie",
            "Weiming Dong",
            "Changsheng Xu"
        ],
        "citations": 37,
        "references": 85,
        "year": 2022
    },
    {
        "title": "Detecting and Correcting Hate Speech in Multimodal Memes with Large Visual Language Model",
        "abstract": "Recently, large language models (LLMs) have taken the spotlight in natural language processing. Further, integrating LLMs with vision enables the users to explore more emergent abilities in multimodality. Visual language models (VLMs), such as LLaVA, Flamingo, or GPT-4, have demonstrated impressive performance on various visio-linguistic tasks. Consequently, there are enormous applications of large models that could be potentially used on social media platforms. Despite that, there is a lack of related work on detecting or correcting hateful memes with VLMs. In this work, we study the ability of VLMs on hateful meme detection and hateful meme correction tasks with zero-shot prompting. From our empirical experiments, we show the effectiveness of the pretrained LLaVA model and discuss its strengths and weaknesses in these tasks.",
        "authors": [
            "Minh-Hao Van",
            "Xintao Wu"
        ],
        "citations": 8,
        "references": 36,
        "year": 2023
    },
    {
        "title": "LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions",
        "abstract": "Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We attribute this to two primary factors: 1) the reliance on single-turn textual interactions with LLMs, leading to a mismatch between generated text and visual concepts for VLMs; 2) the oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. In this paper, we propose a novel framework that integrates LLMs and VLMs to find the optimal class descriptors. Our training-free approach develops an LLM-based agent with an evolutionary optimization strategy to iteratively refine class descriptors. We demonstrate our optimized descriptors are of high quality which effectively improves classification accuracy on a wide range of benchmarks. Additionally, these descriptors offer explainable and robust features, boosting performance across various backbone models and complementing fine-tuning-based methods.",
        "authors": [
            "Songhao Han",
            "Le Zhuo",
            "Yue Liao",
            "Si Liu"
        ],
        "citations": 8,
        "references": 46,
        "year": 2023
    },
    {
        "title": "EventBind: Learning a Unified Representation to Bind Them All for Event-Based Open-World Understanding",
        "abstract": null,
        "authors": [
            "Jiazhou Zhou",
            "Xueye Zheng",
            "Yuanhuiyi Lyu",
            "Lin Wang"
        ],
        "citations": 8,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models",
        "abstract": "Prompt tuning and adapter tuning have shown great potential in transferring pre-trained vision-language models (VLMs) to various downstream tasks. In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through a learnable selection. Inspired by neural pathways, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage. To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen. When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data. Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives. It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters. Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them. Project page can be found here.",
        "authors": [
            "Kecheng Zheng",
            "Wei Wu",
            "Ruili Feng",
            "Kai Zhu",
            "Jiawei Liu",
            "Deli Zhao",
            "Zhengjun Zha",
            "Wei Chen",
            "Yujun Shen"
        ],
        "citations": 8,
        "references": 60,
        "year": 2023
    },
    {
        "title": "CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding",
        "abstract": "A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make\"infinite use of finite means\". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their\"bag-of-words\"behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose CoVLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.",
        "authors": [
            "Junyan Li",
            "Delin Chen",
            "Yining Hong",
            "Zhenfang Chen",
            "Peihao Chen",
            "Yikang Shen",
            "Chuang Gan"
        ],
        "citations": 8,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Prompting Scientific Names for Zero-Shot Species Recognition",
        "abstract": "Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g.,\"a photo of Lepus Timidus\"(which is a scientific name in Latin). Because these names are usually not included in CLIP's training set. To improve performance, prior works propose to use large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. We find that they bring only marginal gains. Differently, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP's training set, and prompting them achieves 2$\\sim$5 times higher accuracy on benchmarking datasets of fine-grained species recognition.",
        "authors": [
            "Shubham Parashar",
            "Zhiqiu Lin",
            "Yanan Li",
            "Shu Kong"
        ],
        "citations": 8,
        "references": 19,
        "year": 2023
    },
    {
        "title": "Simple Image-level Classification Improves Open-vocabulary Object Detection",
        "abstract": "Open-Vocabulary Object Detection (OVOD) aims to detect novel objects beyond a given set of base categories on which the detection model is trained. Recent OVOD methods focus on adapting the image-level pre-trained vision-language models (VLMs), such as CLIP, to a region-level object detection task via, eg., region-level knowledge distillation, regional prompt learning, or region-text pre-training, to expand the detection vocabulary. These methods have demonstrated remarkable performance in recognizing regional visual concepts, but they are weak in exploiting the VLMs' powerful global scene understanding ability learned from the billion-scale image-level text descriptions. This limits their capability in detecting hard objects of small, blurred, or occluded appearance from novel/base categories, whose detection heavily relies on contextual information. To address this, we propose a novel approach, namely Simple Image-level Classification for Context-Aware Detection Scoring (SIC-CADS), to leverage the superior global knowledge yielded from CLIP for complementing the current OVOD models from a global perspective. The core of SIC-CADS is a multi-modal multi-label recognition (MLR) module that learns the object co-occurrence-based contextual information from CLIP to recognize all possible object categories in the scene. These image-level MLR scores can then be utilized to refine the instance-level detection scores of the current OVOD models in detecting those hard objects. This is verified by extensive empirical results on two popular benchmarks, OV-LVIS and OV-COCO, which show that SIC-CADS achieves significant and consistent improvement when combined with different types of OVOD models. Further, SIC-CADS also improves the cross-dataset generalization ability on Objects365 and OpenImages. Code is available at https://github.com/mala-lab/SIC-CADS.",
        "authors": [
            "Ru Fang",
            "Guansong Pang",
            "Xiaolong Bai"
        ],
        "citations": 8,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models",
        "abstract": "Fine-tuning pre-trained vision-language models (VLMs), e.g., CLIP, for the open-world generalization has gained increasing popularity due to its practical value. However, performance advancements are limited when relying solely on intricate algorithmic designs for a single model, even one exhibiting strong performance, e.g., CLIP-ViT-B/16. This paper, for the first time, explores the collaborative potential of leveraging much weaker VLMs to enhance the generalization of a robust single model. The affirmative findings motivate us to address the generalization problem from a novel perspective, i.e., ensemble of pre-trained VLMs. We introduce three customized ensemble strategies, each tailored to one specific scenario. Firstly, we introduce the zero-shot ensemble, automatically adjusting the logits of different models based on their confidence when only pre-trained VLMs are available. Furthermore, for scenarios with extra few-shot samples, we propose the training-free and tuning ensemble, offering flexibility based on the availability of computing resources. The proposed ensemble strategies are evaluated on zero-shot, base-to-new, and cross-dataset generalization, achieving new state-of-the-art performance. Notably, this work represents an initial stride toward enhancing the generalization performance of VLMs via ensemble. The code is available at https://github.com/zhiheLu/Ensemble_VLM.git.",
        "authors": [
            "Zhihe Lu",
            "Jiawang Bai",
            "Xin Li",
            "Zeyu Xiao",
            "Xinchao Wang"
        ],
        "citations": 8,
        "references": 77,
        "year": 2023
    },
    {
        "title": "CLIP Is Also a Good Teacher: A New Learning Framework for Inductive Zero-shot Semantic Segmentation",
        "abstract": "Generalized Zero-shot Semantic Segmentation aims to segment both seen and unseen categories only under the supervision of the seen ones. To tackle this, existing methods adopt the large-scale Vision Language Models (VLMs) which obtain outstanding zero-shot performance. However, as the VLMs are designed for classification tasks, directly adapting the VLMs may lead to sub-optimal performance. Consequently, we propose CLIP-ZSS (Zero-shot Semantic Segmentation), a simple but effective training framework that enables any image encoder designed for closed-set segmentation applied in zero-shot and open-vocabulary tasks in testing without combining with VLMs or inserting new modules. CLIP-ZSS consists of two key modules: Global Learning Module (GLM) and Pixel Learning Module (PLM). GLM is proposed to probe the knowledge from the CLIP visual encoder by pulling the CLS token and the dense features from the image encoder of the same image and pushing others apart. Moreover, to enhance the ability to discriminate unseen categories, PLM consisting of pseudo labels and weight generation is designed. To generate semantically discriminated pseudo labels, a multi-scale K-Means with mask fusion working on the dense tokens is proposed. In pseudo weight generation, a synthesizer generating pseudo semantic features for the unannotated area is introduced. Experiments on three benchmarks show large performance gains compared with SOTA methods.",
        "authors": [
            "Jialei Chen",
            "Daisuke Deguchi",
            "Chenkai Zhang",
            "Xu Zheng",
            "Hiroshi Murase"
        ],
        "citations": 8,
        "references": 76,
        "year": 2023
    },
    {
        "title": "Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models",
        "abstract": "Large vision-and-language models (VLMs) trained to match images with text on large-scale datasets of image-text pairs have shown impressive generalization ability on several vision and language tasks. Several recent works, however, showed that these models lack fine-grained understanding, such as the ability to count and recognize verbs, attributes, or relationships. The focus of this work is to study the understanding of spatial relations. This has been tackled previously using image-text matching (e.g., Visual Spatial Reasoning benchmark) or visual question answering (e.g., GQA or VQAv2), both showing poor performance and a large gap compared to human performance. In this work, we show qualitatively (using explainability tools) and quantitatively (using object detectors) that the poor object localization\"grounding\"ability of the models is a contributing factor to the poor image-text matching performance. We propose an alternative fine-grained, compositional approach for recognizing and ranking spatial clauses that combines the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative VLMs (such as LXMERT, GPV, and MDETR) and compare and highlight their abilities to reason about spatial relationships.",
        "authors": [
            "Navid Rajabi",
            "J. Kosecka"
        ],
        "citations": 8,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Tackling Vision Language Tasks Through Learning Inner Monologues",
        "abstract": "Visual language tasks such as Visual Question Answering (VQA) or Visual Entailment (VE) require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. \nTo tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating Inner Monologue, a cognitive process in which an individual engages in silent verbal communication with themselves. More specifically, we enable LLMs and VLMs to interact through natural language conversation (i.e., Inner Monologue) and propose to use a two-stage training process to learn how to do Inner Monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and achieves competitive performance with less training data when compared with state-of-the-art models while concurrently keeping the interpretability. The results suggest that by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, broadening its potential applications across various AI challenges beyond vision and language tasks.",
        "authors": [
            "Diji Yang",
            "Kezhen Chen",
            "Jinmeng Rao",
            "Xiaoyuan Guo",
            "Yawen Zhang",
            "Jie Yang",
            "Y. Zhang"
        ],
        "citations": 8,
        "references": 44,
        "year": 2023
    },
    {
        "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
        "abstract": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even conceived a neuro-symbolic reasoning approach that reconciles LLMs&VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.",
        "authors": [
            "Rujie Wu",
            "Xiaojian Ma",
            "Qing Li",
            "Wei Wang",
            "Zhenliang Zhang",
            "Song-Chun Zhu",
            "Yizhou Wang"
        ],
        "citations": 6,
        "references": 65,
        "year": 2023
    },
    {
        "title": "Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining",
        "abstract": "Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing image and text understanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused on mining negative examples from existing datasets. However, the mined negative examples might not be difficult for the model to discriminate from the positive. An alternative to mining would be negative sample generation 2) But existing generative approaches primarily focus on generating hard negative texts associated with a given image. Mining in the other direction, i.e., generating negative image samples associated with a given text has been ignored. To overcome both these limitations, we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities, i.e., images and texts. Leveraging these generative hard negative samples, we significantly enhance VLMs‚Äô performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.",
        "authors": [
            "U. Sahin",
            "Hang Li",
            "Qadeer Ahmad Khan",
            "Daniel Cremers",
            "Volker Tresp"
        ],
        "citations": 7,
        "references": 55,
        "year": 2023
    },
    {
        "title": "Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models",
        "abstract": "Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \\textit{data-types}, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual data-types through scaling. By analyzing the pre-training distributions of these models and incorporating data-type information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released at https://github.com/bethgelab/DataTypeIdentification.",
        "authors": [
            "Vishaal Udandarao",
            "Max F. Burg",
            "Samuel Albanie",
            "Matthias Bethge"
        ],
        "citations": 6,
        "references": 131,
        "year": 2023
    },
    {
        "title": "Unified Visual Relationship Detection with Vision and Language Models",
        "abstract": "This work focuses on training a single visual relationship detector predicting over the union of label spaces from multiple datasets. Merging labels spanning different datasets could be challenging due to inconsistent taxonomies. The issue is exacerbated in visual relationship detection when second-order visual semantics are introduced between pairs of objects. To address this challenge, we propose UniVRD, a novel bottom-up method for Unified Visual Relationship Detection by leveraging vision and language models (VLMs). VLMs provide well-aligned image and text embeddings, where similar relationships are optimized to be close to each other for semantic unification. Our bottom-up design enables the model to enjoy the benefit of training with both object detection and visual relationship datasets. Empirical results on both human-object interaction detection and scene-graph generation demonstrate the competitive performance of our model. UniVRD achieves 38.07 mAP on HICO-DET, outperforming the current best bottom-up HOI detector by 14.26 mAP. More importantly, we show that our unified detector performs as well as dataset-specific models in mAP, and achieves further improvements when we scale up the model. Our code will be made publicly available on GitHub1.",
        "authors": [
            "Long Zhao",
            "Liangzhe Yuan",
            "Boqing Gong",
            "Yin Cui",
            "Florian Schroff",
            "Ming Yang",
            "Hartwig Adam",
            "Ting Liu"
        ],
        "citations": 7,
        "references": 98,
        "year": 2023
    },
    {
        "title": "Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning",
        "abstract": "Recent compositional zero-shot learning (CZSL) methods adapt pre-trained vision-language models (VLMs) by constructing trainable prompts only for composed state-object pairs. Relying on learning the joint representation of seen compositions, these methods ignore the explicit modeling of the state and object, thus limiting the exploitation of pre-trained knowledge and generalization to unseen compositions. With a particular focus on the universality of the solution, in this work, we propose a novel paradigm for CZSL models that establishes three identification branches (i.e., Multi-Path) to jointly model the state, object, and composition. The presented Troika is an outstanding implementation that aligns the branch-specific prompt representations with decomposed visual features. To calibrate the bias between semantically similar multi-modal representations, we further devise a Cross-Modal Traction module into Troika that shifts the prompt representation towards the current visual content. We conduct extensive experiments on three popular benchmarks, where our method significantly outperforms existing methods in both closed-world and open-world settings. The code will be available at https://github.com/bighuang624/Troika.",
        "authors": [
            "Siteng Huang",
            "Biao Gong",
            "Yutong Feng",
            "Yiliang Lv",
            "Donglin Wang"
        ],
        "citations": 7,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Large Language Models as Automated Aligners for benchmarking Vision-Language Models",
        "abstract": "With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence. In this work, we address the limitations via Auto-Bench, which delves into exploring LLMs as proficient aligners, measuring the alignment between VLMs and human intelligence and value through automatic data curation and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs (e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning triplets via prompting on visual symbolic representations (e.g., captions, object locations, instance relationships, and etc.). The curated data closely matches human intent, owing to the extensive world knowledge embedded in LLMs. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to serve as judges, implementing the quantitative and qualitative automated assessments to facilitate a comprehensive evaluation of VLMs. Our validation results reveal that LLMs are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85%. We envision Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating the evolving sophisticated VLMs.",
        "authors": [
            "Yuanfeng Ji",
            "Chongjian Ge",
            "Weikai Kong",
            "Enze Xie",
            "Zhengying Liu",
            "Zhengguo Li",
            "Ping Luo"
        ],
        "citations": 6,
        "references": 45,
        "year": 2023
    },
    {
        "title": "Towards Robust Prompts on Vision-Language Models",
        "abstract": "With the advent of vision-language models (VLMs) that can perform in-context and prompt-based learning, how can we design prompting approaches that robustly generalize to distribution shift and can be used on novel classes outside the support set of the prompts? In this work, we first define two types of robustness to distribution shift on VLMs, namely, robustness on base classes (the classes included in the support set of prompts) and robustness on novel classes. Then, we study the robustness of existing in-context learning and prompt learning approaches, where we find that prompt learning performs robustly on test images from base classes, while it does not generalize well on images from novel classes. We propose robust prompt learning by integrating multiple-scale image features into the prompt, which improves both types of robustness. Comprehensive experiments are conducted to study the defined robustness on six benchmarks and show the effectiveness of our proposal.",
        "authors": [
            "Jindong Gu",
            "Ahmad Beirami",
            "Xuezhi Wang",
            "Alex Beutel",
            "Philip H. S. Torr",
            "Yao Qin"
        ],
        "citations": 6,
        "references": 54,
        "year": 2023
    },
    {
        "title": "Taming Self-Training for Open-Vocabulary Object Detection",
        "abstract": "Recent studies have shown promising performance in open-vocabulary object detection (OVD) by utilizing pseudo labels (PLs) from pre-trained vision and language models (VLMs). However, teacher-student self-training, a powerful and widely used paradigm to leverage PLs, is rarely explored for OVD. This work identifies two challenges of using self-training in OVD: noisy PLs from VLMs and frequent distribution changes of PLs. To address these challenges, we propose SAS-Det that tames self-training for OVD from two key perspectives. First, we present a split-and-fusion (SAF) head that splits a standard detection into an open-branch and a closed-branch. This design can reduce noisy supervision from pseudo boxes. More-over, the two branches learn complementary knowledge from different training data, significantly enhancing performance when fused together. Second, in our view, un-like in closed-set tasks, the PL distributions in OVD are solely determined by the teacher model. We introduce a periodic update strategy to decrease the number of up-dates to the teacher, thereby decreasing the frequency of changes in PL distributions, which stabilizes the training process. Extensive experiments demonstrate SAS-Det is both efficient and effective. SAS-Det outperforms recent models of the same scale by a clear margin and achieves 37.4 AP50 and 29.1 APr on novel categories of the COCO and LVIS benchmarks, respectively. Code is available at https://github.com/xiaofeng94/SAS-Det.",
        "authors": [
            "Shiyu Zhao",
            "S. Schulter",
            "Long Zhao",
            "Zhixing Zhang",
            "Vijay Kumar B.G",
            "Yumin Suh",
            "Manmohan Chandraker",
            "Dimitris N. Metaxas"
        ],
        "citations": 7,
        "references": 65,
        "year": 2023
    },
    {
        "title": "HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding",
        "abstract": "Object categories are typically organized into a multi-granularity taxonomic hierarchy. When classifying categories at different hierarchy levels, traditional uni-modal approaches focus primarily on image features, revealing limitations in complex scenarios. Recent studies integrating Vision-Language Models (VLMs) with class hierarchies have shown promise, yet they fall short of fully exploiting the hierarchical relationships. These efforts are constrained by their inability to perform effectively across varied granularity of categories. To tackle this issue, we propose a novel framework (HGCLIP) that effectively combines CLIP with a deeper exploitation of the Hierarchical class structure via Graph representation learning. We explore constructing the class hierarchy into a graph, with its nodes representing the textual or image features of each category. After passing through a graph encoder, the textual features incorporate hierarchical structure information, while the image features emphasize class-aware features derived from prototypes through the attention mechanism. Our approach demonstrates significant improvements on 11 diverse visual recognition benchmarks. Our codes are fully available at https://github.com/richard-peng-xia/HGCLIP.",
        "authors": [
            "Peng Xia",
            "Xingtong Yu",
            "Ming Hu",
            "Lie Ju",
            "Zhiyong Wang",
            "Peibo Duan",
            "Zongyuan Ge"
        ],
        "citations": 7,
        "references": 71,
        "year": 2023
    },
    {
        "title": "Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning",
        "abstract": "Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even surpass human capability in reasoning times and location. To address this question, we propose a two-stage Recognition & Reasoning probing task applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the studies, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In extensive evaluation experiments, we find that although VLMs can effectively retain times and location-relevant features in visual encoders, they still fail to make perfect reasoning with context-conditioned visual features. The dataset is available at https://github.com/gengyuanmax/WikiTiLo.",
        "authors": [
            "Gengyuan Zhang",
            "Yurui Zhang",
            "Kerui Zhang",
            "Volker Tresp"
        ],
        "citations": 7,
        "references": 46,
        "year": 2023
    },
    {
        "title": "OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data",
        "abstract": "In the era of big data and large models, automatic annotating functions for multi-modal data are of great significance for real-world AI-driven applications, such as autonomous driving and embodied AI. Unlike traditional closed-set annotation, open-vocabulary annotation is essential to achieve human-level cognition capability. However, there are few open-vocabulary auto-labeling systems for multi-modal 3D data. In this paper, we introduce OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can automatically generate 2D masks, 3D masks, and 3D bounding box annotations for vision and point cloud data. Our system integrates the chain-of-thought capabilities of Large Language Models (LLMs) and the cross-modality capabilities of vision-language models (VLMs). To the best of our knowledge, OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal 3D auto-labeling. We conduct comprehensive evaluations on both public and in-house real-world datasets, which demonstrate that the system significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results.",
        "authors": [
            "Yijie Zhou",
            "Likun Cai",
            "Xianhui Cheng",
            "Zhongxue Gan",
            "Xiangyang Xue",
            "Wenchao Ding"
        ],
        "citations": 7,
        "references": 32,
        "year": 2023
    },
    {
        "title": "The autonomic nervous system: A potential link to the efficacy of acupuncture",
        "abstract": "The autonomic nervous system (ANS) is a diffuse network that regulates physiological systems to maintain body homeostasis by integrating inputs from the internal and external environment, including the sympathetic, parasympathetic, and enteric nervous systems (ENS). Recent evidence suggests that ANS is one of the key neural pathways for acupuncture signal transduction, which has attracted worldwide attention in the acupuncture field. Here, we reviewed the basic and clinical research published in PubMed over the past 20 years on the effects of acupuncture on ANS regulation and homeostasis maintenance. It was found that acupuncture effectively alleviates ANS dysfunction-associated symptoms in its indications, such as migraine, depression, insomnia, functional dyspepsia, functional constipation. Acupuncture stimulation on some specific acupoints activates sensory nerve fibers, the spinal cord, and the brain. Using information integration and efferents from a complex network of autonomic nuclei of the brain, such as the insular cortex (IC), prefrontal cortex, anterior cingulate cortex (ACC), amygdala (AMG), hypothalamus, periaqueductal gray (PAG), nucleus tractus solitarius (NTS), ventrolateral medulla (VLM), nucleus ambiguus (AMB), acupuncture alleviates visceral dysfunction, inflammation via efferent autonomic nerves, and relieves pain and pain affect. The modulating pattern of sympathetic and parasympathetic nerves is associated with acupuncture stimulation on specific acupoints, intervention parameters, and disease models, and the relationships among them require further exploration. In conclusion, ANS is one of the therapeutic targets for acupuncture and mediates acupuncture‚Äôs actions, which restores homeostasis. A systemic study is needed to determine the rules and mechanisms underlying the effects of acupoint stimulation on corresponding organs mediated by specific central nervous networks and the efferent ANS.",
        "authors": [
            "Yan-wei Li",
            "Wei Li",
            "Songtao Wang",
            "Yinan Gong",
            "Baomin Dou",
            "Zhongxi Lyu",
            "Luis Ulloa",
            "Shenjun Wang",
            "Zhifang Xu",
            "Yi-song Guo"
        ],
        "citations": 29,
        "references": 94,
        "year": 2022
    },
    {
        "title": "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text",
        "abstract": "Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this paper, we regard linguistically complex texts as compound proposition texts composed of multiple simple proposition sentences and propose an end-to-end Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three main components: 1) Divide: a proposition generator divides the compound proposition text into simple proposition sentences and produces their corresponding representations, 2) Conquer: a pretrained VLMs-based visual-linguistic interactor achieves the interaction between decomposed proposition sentences and images, 3) Combine: a neural-symbolic reasoner combines the above reasoning states to obtain the final solution via a neural logic reasoning approach. According to the dual-process theory, the visual-linguistic interactor and neural-symbolic reasoner could be regarded as analogical reasoning System 1 and logical reasoning System 2. We conduct extensive experiments on a challenging image retrieval from contextual descriptions data set. Experimental results and analyses indicate NDCR significantly improves performance in the complex image-text reasoning problem.",
        "authors": [
            "Yunxin Li",
            "Baotian Hu",
            "Yunxin Ding",
            "Lin Ma",
            "M. Zhang"
        ],
        "citations": 5,
        "references": 39,
        "year": 2023
    },
    {
        "title": "KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models",
        "abstract": "Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry.",
        "authors": [
            "Zhiwei Jia",
            "P. Narayana",
            "Arjun Reddy Akula",
            "G. Pruthi",
            "Haoran Su",
            "Sugato Basu",
            "Varun Jampani"
        ],
        "citations": 4,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Strong but Simple: A Baseline for Domain Generalized Dense Perception by CLIP-Based Transfer Learning",
        "abstract": null,
        "authors": [
            "Christoph H√ºmmer",
            "Manuel Schwonberg",
            "Liangwei Zhou",
            "Hu Cao",
            "Alois Knoll",
            "Hanno Gottschalk"
        ],
        "citations": 4,
        "references": 119,
        "year": 2023
    },
    {
        "title": "Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment",
        "abstract": "Learning to ground natural language queries to target objects or regions in 3D point clouds is quite essential for 3D scene understanding. Nevertheless, existing 3D visual grounding approaches require a substantial number of bounding box annotations for text queries, which is time-consuming and labor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly supervised approach for 3D visual grounding based on Visual Linguistic Alignment. Our 3D-VLA exploits the superior ability of current large-scale vision-language models (VLMs) on aligning the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds with no need for fine-grained box annotations in the training procedure. During the inference stage, the learned text-3D correspondence will help us ground the text queries to the 3D target objects even without 2D images. To the best of our knowledge, this is the first work to investigate 3D visual grounding in a weakly supervised manner by involving large scale vision-language models, and extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even superior results over the fully supervised methods.",
        "authors": [
            "Xiaoxu Xu",
            "Yitian Yuan",
            "Qiudan Zhang",
            "Wen-Bin Wu",
            "Zequn Jie",
            "Lin Ma",
            "Xu Wang"
        ],
        "citations": 4,
        "references": 66,
        "year": 2023
    },
    {
        "title": "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering",
        "abstract": "Recently, finetuning pretrained vision-language models (VLMs) has been a prevailing paradigm for achieving state-of-the-art performance in VQA. However, as VLMs scale, it becomes computationally expensive, storage inefficient, and prone to overfitting when tuning full model parameters for a specific task in low-resource settings. Although current parameter-efficient tuning methods dramatically reduce the number of tunable parameters, there still exists a significant performance gap with full finetuning. In this paper, we propose MixPHM, a redundancy-aware parameter-efficient tuning method that outperforms full finetuning in low-resource VQA. Specifically, MixPHM is a lightweight module implemented by multiple PHM-experts in a mixture-of-experts manner. To reduce parameter redundancy, we reparameterize expert weights in a low-rank subspace and share part of the weights inside and across MixPHM. Moreover, based on our quantitative analysis of representation redundancy, we propose Redundancy Regularization, which facilitates MixPHM to reduce task-irrelevant redundancy while promoting task-relevant correlation. Experiments conducted on VQA v2, GQA, and OK-VQA with different low-resource settings show that our MixPHM outperforms state-of-the-art parameter-efficient methods and is the only one consistently surpassing full finetuning.",
        "authors": [
            "Jingjing Jiang",
            "Nanning Zheng"
        ],
        "citations": 5,
        "references": 66,
        "year": 2023
    },
    {
        "title": "Domain Prompt Learning with Quaternion Networks",
        "abstract": "Prompt learning has emerged as a potent and resource-efficient technique in large Vision-Language Models (VLMs). However, its application in adapting VLMs to specialized domains like remote sensing and medical imaging, termed domain prompt learning, remains relatively unexplored. Although large-scale domain-specific foundation models offer a potential solution, their focus on a singular vision level presents challenges in prompting both vision and language modalities. To address this limitation, we propose leveraging domain-specific knowledge from these foundation models to transfer the robust recognition abilities of VLMs from generalized to specialized domains, employing quaternion networks. Our method entails utilizing domain-specific vision features from domain-specific foundation models to guide the transformation of generalized contextual embeddings from the language branch into a specialized space within quaternion networks. Furthermore, we introduce a hierarchical approach that derives vision prompt features by analyzing intermodal relationships between hierarchical language prompt features and domain-specific vision features. Through this mechanism, quaternion networks can effectively explore intermodal relationships in specific domains, facilitating domain-specific vision-language contrastive learning. Extensive experiments conducted on domain-specific datasets demonstrate that our proposed method achieves new state-of-the-art results in prompt learning. Codes are available at https://github.com/caoq198/DPLQ.",
        "authors": [
            "Qinglong Cao",
            "Zhengqin Xu",
            "Yuntian Chen",
            "Chao Ma",
            "Xiaokang Yang"
        ],
        "citations": 5,
        "references": 55,
        "year": 2023
    },
    {
        "title": "EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models",
        "abstract": "Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person per-spective. However, the capability of VLMs to ‚Äúthink‚Äù from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from ego-centric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate twenty-one popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics.",
        "authors": [
            "Sijie Cheng",
            "Zhicheng Guo",
            "Jingwen Wu",
            "Kechen Fang",
            "Peng Li",
            "Huaping Liu",
            "Yang Liu"
        ],
        "citations": 5,
        "references": 91,
        "year": 2023
    },
    {
        "title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Models",
        "abstract": "Vision-Language Large Models (VLMs) have become primary backbone of AI, due to the impressive performance. However, their expensive computation costs, i.e., throughput and delay, impede potentials in real-world scenarios. To achieve acceleration for VLMs, most existing methods focus on the model perspective: pruning, distillation, quantification, but completely overlook the data-perspective redundancy. To fill the overlook, this paper pioneers the severity of data redundancy, and designs one plug-and-play Turbo module guided by information degree to prune inefficient tokens from visual or textual data. In pursuit of efficiency-performance trade-offs, information degree takes two key factors into consideration: mutual redundancy and semantic value. Concretely, the former evaluates the data duplication between sequential tokens; while the latter evaluates each token by its contribution to the overall semantics. As a result, tokens with high information degree carry less redundancy and stronger semantics. For VLMs' calculation, Turbo works as a user-friendly plug-in that sorts data referring to information degree, utilizing only top-level ones to save costs. Its advantages are multifaceted, e.g., being generally compatible to various VLMs across understanding and generation, simple use without retraining and trivial engineering efforts. On multiple public VLMs benchmarks, we conduct extensive experiments to reveal the gratifying acceleration of Turbo, under negligible performance drop.",
        "authors": [
            "Chen Ju",
            "Haicheng Wang",
            "Zeqian Li",
            "Xu Chen",
            "Zhonghua Zhai",
            "Weilin Huang",
            "Shuai Xiao"
        ],
        "citations": 5,
        "references": 48,
        "year": 2023
    },
    {
        "title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties",
        "abstract": "A major reason behind the recent success of large language models (LLMs) is their \\textit{in-context learning} capability, which makes it possible to rapidly adapt them to downstream text-based tasks by prompting them with a small number of relevant demonstrations. While large vision-language models (VLMs) have recently been developed for tasks requiring both text and images, they largely lack in-context learning over visual information, especially in understanding and generating text about videos. In this work, we implement \\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos (\\eilev{}), a novel training paradigm that induces in-context learning over video and text by capturing key properties of pre-training data found by prior work to be essential for in-context learning in transformers. In our experiments, we show that \\eilev-trained models outperform other off-the-shelf VLMs in few-shot video narration for novel, rare actions. Furthermore, we demonstrate that these key properties of bursty distributions, skewed marginal distributions, and dynamic meaning each contribute to varying degrees to VLMs' in-context learning capability in narrating procedural videos. Our results, analysis, and \\eilev{}-trained models yield numerous insights about the emergence of in-context learning over video and text, creating a foundation for future work to optimize and scale VLMs for open-domain video understanding and reasoning. Our code and demo are available at \\url{https://github.com/yukw777/EILEV}.",
        "authors": [
            "Keunwoo Peter Yu",
            "Zheyuan Zhang",
            "Fengyuan Hu",
            "Joyce Chai"
        ],
        "citations": 5,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Zelda: Video Analytics using Vision-Language Models",
        "abstract": "Advances in ML have motivated the design of video analytics systems that allow for structured queries over video datasets. However, existing systems limit query expressivity, require users to specify an ML model per predicate, rely on complex optimizations that trade off accuracy for performance, and return large amounts of redundant and low-quality results. This paper focuses on the recently developed Vision-Language Models (VLMs) that allow users to query images using natural language like\"cars during daytime at traffic intersections.\"Through an in-depth analysis, we show VLMs address three limitations of current video analytics systems: general expressivity, a single general purpose model to query many predicates, and are both simple and fast. However, VLMs still return large numbers of redundant and low-quality results, which can overwhelm and burden users. We present Zelda: a video analytics system that uses VLMs to return both relevant and semantically diverse results for top-K queries on large video datasets. Zelda prompts the VLM with the user's query in natural language and additional terms to improve accuracy and identify low-quality frames. Zelda improves result diversity by leveraging the rich semantic information encoded in VLM embeddings. We evaluate Zelda across five datasets and 19 queries and quantitatively show it achieves higher mean average precision (up to 1.15$\\times$) and improves average pairwise similarity (up to 1.16$\\times$) compared to using VLMs out-of-the-box. We also compare Zelda to a state-of-the-art video analytics engine and show that Zelda retrieves results 7.5$\\times$ (up to 10.4$\\times$) faster for the same accuracy and frame diversity.",
        "authors": [
            "Francisco Romero",
            "Caleb Winston",
            "Johann Hauswald",
            "M. Zaharia",
            "Christos Kozyrakis"
        ],
        "citations": 5,
        "references": 75,
        "year": 2023
    },
    {
        "title": "Few-shot medical image classification with simple shape and texture text descriptors using vision-language models",
        "abstract": "In this work, we investigate the usefulness of vision-language models (VLMs) and large language models for binary few-shot classification of medical images. We utilize the GPT-4 model to generate text descriptors that encapsulate the shape and texture characteristics of objects in medical images. Subsequently, these GPT-4 generated descriptors, alongside VLMs pre-trained on natural images, are employed to classify chest X-rays and breast ultrasound images. Our results indicate that few-shot classification of medical images using VLMs and GPT-4 generated descriptors is a viable approach. However, accurate classification requires to exclude certain descriptors from the calculations of the classification scores. Moreover, we assess the ability of VLMs to evaluate shape features in breast mass ultrasound images. We further investigate the degree of variability among the sets of text descriptors produced by GPT-4. Our work provides several important insights about the application of VLMs for medical image analysis.",
        "authors": [
            "Michal Byra",
            "M. F. Rachmadi",
            "Henrik Skibbe"
        ],
        "citations": 4,
        "references": 13,
        "year": 2023
    },
    {
        "title": "Distilling Functional Rearrangement Priors from Large Models",
        "abstract": "‚ÄîObject rearrangement, a fundamental challenge in robotics, demands versatile strategies to handle diverse objects, configurations, and functional needs. To achieve this, the AI robot needs to learn functional rearrangement priors in order to specify precise goals that meet the functional requirements. Previous methods typically learn such priors from either laborious human annotations or manually designed heuristics, which limits scalability and generalization. In this work, we propose a novel approach that leverages large models to distill functional rearrangement priors. Specifically, our approach collects diverse arrangement examples using both LLMs and VLMs and then distills the examples into a diffusion model. During test time, the learned diffusion model is conditioned on the initial configuration and guides the positioning of objects to meet functional requirements. In this manner, we create a handshaking point that combines the strengths of conditional generative models and large models. Extensive experiments on multiple domains, including real-world scenarios, demonstrate the effectiveness of our approach in generating compatible goals for object rearrangement tasks, significantly outperforming baseline methods. Our real-world results can be seen on https://sites.google.com/view/lvdiffusion .",
        "authors": [
            "Yiming Zeng",
            "Mingdong Wu",
            "Long Yang",
            "Jiyao Zhang",
            "Hao Ding",
            "Hui Cheng",
            "Hao Dong"
        ],
        "citations": 4,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Improving Pseudo Labels for Open-Vocabulary Object Detection",
        "abstract": "Recent studies show promising performance in open-vocabulary object detection (OVD) using pseudo labels (PLs) from pretrained vision and language models (VLMs). However, PLs generated by VLMs are extremely noisy due to the gap between the pretraining objective of VLMs and OVD, which blocks further advances on PLs. In this paper, we aim to reduce the noise in PLs and propose a method called online Self-training And a Split-and-fusion head for OVD (SAS-Det). First, the self-training finetunes VLMs to generate high quality PLs while prevents forgetting the knowledge learned in the pretraining. Second, a split-and-fusion (SAF) head is designed to remove the noise in localization of PLs, which is usually ignored in existing methods. It also fuses complementary knowledge learned from both precise ground truth and noisy pseudo labels to boost the performance. Extensive experiments demonstrate SAS-Det is both efficient and effective. Our pseudo labeling is 3 times faster than prior methods. SAS-Det outperforms prior state-of-the-art models of the same scale by a clear margin and achieves 37.4 AP 50 and 27.3 AP r on novel categories of the COCO and LVIS benchmarks, respectively.",
        "authors": [
            "Shiyu Zhao",
            "S. Schulter",
            "Long Zhao",
            "Zhixing Zhang",
            "Vijay Kumar",
            "Yumin Suh",
            "M. Chandraker",
            "Dimitris N. Metaxas"
        ],
        "citations": 4,
        "references": 60,
        "year": 2023
    },
    {
        "title": "ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data",
        "abstract": "Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up to 37% on retrieval tasks. In order to address this issue, we introduce ViLLA as our second key contribution. ViLLA, which is trained to capture fine-grained region-attribute relationships from complex datasets, involves two components: (a) a lightweight, self-supervised mapping model to decompose image-text samples into region-attribute pairs, and (b) a contrastive VLM to learn representations from generated region-attribute pairs. We demonstrate with experiments across four domains (synthetic, product, medical, and natural images) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks, such as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP points on LVIS) and retrieval (up to 14.2 R-Precision points1",
        "authors": [
            "M. Varma",
            "Jean-Benoit Delbrouck",
            "Sarah Hooper",
            "Akshay Chaudhari",
            "C. Langlotz"
        ],
        "citations": 3,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Voila-A: Aligning Vision-Language Models with User's Gaze Attention",
        "abstract": "In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VLMs while preserving their pretrained knowledge. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE Testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications.",
        "authors": [
            "Kun Yan",
            "Lei Ji",
            "Zeyu Wang",
            "Yuntao Wang",
            "Nan Duan",
            "Shuai Ma"
        ],
        "citations": 3,
        "references": 59,
        "year": 2023
    },
    {
        "title": "Compositional Zero-shot Learning via Progressive Language-based Observations",
        "abstract": "Compositional zero-shot learning aims to recognize unseen state-object compositions by leveraging known primitives (state and object) during training. However, effectively modeling interactions between primitives and generalizing knowledge to novel compositions remains a perennial challenge. There are two key factors: object-conditioned and state-conditioned variance, i.e., the appearance of states (or objects) can vary significantly when combined with different objects (or states). For instance, the state\"old\"can signify a vintage design for a\"car\"or an advanced age for a\"cat\". In this paper, we argue that these variances can be mitigated by predicting composition categories based on pre-observed primitive. To this end, we propose Progressive Language-based Observations (PLO), which can dynamically determine a better observation order of primitives. These observations comprise a series of concepts or languages that allow the model to understand image content in a step-by-step manner. Specifically, PLO adopts pre-trained vision-language models (VLMs) to empower the model with observation capabilities. We further devise two variants: 1) PLO-VLM: a two-step method, where a pre-observing classifier dynamically determines the observation order of two primitives. 2) PLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to craft composition-specific prompts for step-by-step observing. Extensive ablations on three challenging datasets demonstrate the superiority of PLO compared with state-of-the-art methods, affirming its abilities in compositional recognition.",
        "authors": [
            "Lin Li",
            "Guikun Chen",
            "Jun Xiao",
            "Long Chen"
        ],
        "citations": 3,
        "references": 42,
        "year": 2023
    },
    {
        "title": "3VL: Using Trees to Improve Vision-Language Models‚Äô Interpretability",
        "abstract": "Vision-Language models (VLMs) have proven to be effective at aligning image and text representations, producing superior zero-shot results when transferred to many downstream tasks. However, these representations suffer from some key shortcomings in understanding Compositional Language Concepts (CLC), such as recognizing objects‚Äô attributes, states, and relations between different objects. Moreover, VLMs typically have poor interpretability, making it challenging to debug and mitigate compositional-understanding failures. In this work, we introduce the architecture and training technique of Tree-augmented Vision-Language (3VL) model accompanied by our proposed Anchor inference method and Differential Relevance (DiRe) interpretability tool. By expanding the text of an arbitrary image-text pair into a hierarchical tree structure using language analysis tools, 3VL allows the induction of this structure into the visual representation learned by the model, enhancing its interpretability and compositional reasoning. Additionally, we show how Anchor, a simple technique for text unification, can be used to filter nuisance factors while increasing CLC understanding performance, e.g., on the fundamental VL-Checklist benchmark. We also show how DiRe, which performs a differential comparison between VLM relevancy maps, enables us to generate compelling visualizations of the reasons for a model‚Äôs success or failure.",
        "authors": [
            "Nir Yellinek",
            "Leonid Karlinsky",
            "Raja Giryes"
        ],
        "citations": 3,
        "references": 119,
        "year": 2023
    },
    {
        "title": "Semantic Mechanical Search with Large Vision and Language Models",
        "abstract": "Moving objects to find a fully-occluded target object, known as mechanical search, is a challenging problem in robotics. As objects are often organized semantically, we conjecture that semantic information about object relationships can facilitate mechanical search and reduce search time. Large pretrained vision and language models (VLMs and LLMs) have shown promise in generalizing to uncommon objects and previously unseen real-world environments. In this work, we propose a novel framework called Semantic Mechanical Search (SMS). SMS conducts scene understanding and generates a semantic occupancy distribution explicitly using LLMs. Compared to methods that rely on visual similarities offered by CLIP embeddings, SMS leverages the deep reasoning capabilities of LLMs. Unlike prior work that uses VLMs and LLMs as end-to-end planners, which may not integrate well with specialized geometric planners, SMS can serve as a plug-in semantic module for downstream manipulation or navigation policies. For mechanical search in closed-world settings such as shelves, we compare with a geometric-based planner and show that SMS improves mechanical search performance by 24% across the pharmacy, kitchen, and office domains in simulation and 47.1% in physical experiments. For open-world real environments, SMS can produce better semantic distributions compared to CLIP-based methods, with the potential to be integrated with downstream navigation policies to improve object navigation tasks. Code, data, videos, and the appendix are available: https://sites.google.com/view/semantic-mechanical-search",
        "authors": [
            "Satvik Sharma",
            "K. Shivakumar",
            "Huang Huang",
            "Ryan Hoque",
            "A. Imran",
            "Brian Ichter",
            "Ken Goldberg"
        ],
        "citations": 3,
        "references": 108,
        "year": 2023
    },
    {
        "title": "Orthogonal Temporal Interpolation for Zero-Shot Video Recognition",
        "abstract": "Zero-shot video recognition (ZSVR) is a task that aims to recognize video categories that have not been seen during the model training process. Recently, vision-language models (VLMs) pre-trained on large-scale image-text pairs have demonstrated impressive transferability for ZSVR. To make VLMs applicable to the video domain, existing methods often use an additional temporal learning module after the image-level encoder to learn the temporal relationships among video frames. Unfortunately, for video from unseen categories, we observe an abnormal phenomenon where the model that uses spatial-temporal feature performs much worse than the model that removes temporal learning module and uses only spatial feature. We conjecture that improper temporal modeling on video disrupts the spatial feature of the video. To verify our hypothesis, we propose Feature Factorization to retain the orthogonal temporal feature of the video and use interpolation to construct refined spatial-temporal feature. The model using appropriately refined spatial-temporal feature performs better than the one using only spatial feature, which verifies the effectiveness of the orthogonal temporal feature for the ZSVR task. Therefore, an Orthogonal Temporal Interpolation module is designed to learn a better refined spatial-temporal video feature during training. Additionally, a Matching Loss is introduced to improve the quality of the orthogonal temporal feature. We propose a model called OTI for ZSVR by employing orthogonal temporal interpolation and the matching loss based on VLMs. The ZSVR accuracies on popular video datasets (i.e., Kinetics-600, UCF101 and HMDB51) show that OTI outperforms the previous state-of-the-art method by a clear margin.Our codes are publicly available at https://github.com/yanzhu/mm2023_oti.",
        "authors": [
            "Yan Zhu",
            "Junbao Zhuo",
            "B. Ma",
            "Jiajia Geng",
            "Xiaoming Wei",
            "Xiaolin Wei",
            "Shuhui Wang"
        ],
        "citations": 3,
        "references": 47,
        "year": 2023
    },
    {
        "title": "Revisiting Few-Shot Object Detection with Vision-Language Models",
        "abstract": "The era of vision-language models (VLMs) trained on web-scale datasets challenges conventional formulations of\"open-world\"perception. In this work, we revisit the task of few-shot object detection (FSOD) in the context of recent foundational VLMs. First, we point out that zero-shot predictions from VLMs such as GroundingDINO significantly outperform state-of-the-art few-shot detectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance, such foundation models may still be sub-optimal. For example, trucks on the web may be defined differently from trucks for a target application such as autonomous vehicle perception. We argue that the task of few-shot recognition can be reformulated as aligning foundation models to target concepts using a few examples. Interestingly, such examples can be multi-modal, using both text and visual cues, mimicking instructions that are often given to human annotators when defining a target concept of interest. Concretely, we propose Foundational FSOD, a new benchmark protocol that evaluates detectors pre-trained on any external data and fine-tuned on multi-modal (text and visual) K-shot examples per target class. We repurpose nuImages for Foundational FSOD, benchmark several popular open-source VLMs, and provide an empirical analysis of state-of-the-art methods. Lastly, we discuss our recent CVPR 2024 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 23.3 mAP! Our code and dataset splits are available at https://github.com/anishmadan23/foundational_fsod",
        "authors": [
            "Anish Madan",
            "Neehar Peri",
            "Shu Kong",
            "Deva Ramanan"
        ],
        "citations": 3,
        "references": 83,
        "year": 2023
    },
    {
        "title": "Linear Spaces of Meanings: the Compositional Language of VLMs",
        "abstract": ",",
        "authors": [
            "Matthew Trager",
            "Pramuditha Perera",
            "L. Zancato",
            "A. Achille",
            "Parminder Bhatia",
            "Bing Xiang",
            "S. Soatto"
        ],
        "citations": 3,
        "references": 44,
        "year": 2023
    },
    {
        "title": "CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction",
        "abstract": "In this paper, we explore the potential of Vision-Language Models (VLMs), specifically CLIP, in predicting visual object relationships, which involves interpreting visual features from images into language-based relations. Current state-of-the-art methods use complex graphical models that utilize language cues and visual features to address this challenge. We hypothesize that the strong language priors in CLIP embeddings can simplify these graphical models paving for a simpler approach. We adopt the UVTransE relation prediction framework, which learns the relation as a translational embedding with subject, object, and union box embeddings from a scene. We systematically explore the design of CLIP-based subject, object, and union-box representations within the UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate Estimation). CREPE utilizes text-based representations for all three bounding boxes and introduces a novel contrastive training strategy to automatically infer the text prompt for union-box. Our approach achieves state-of-the-art performance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual Genome benchmark, achieving a 15.3\\% gain in performance over recent state-of-the-art at mR@20. This work demonstrates CLIP's effectiveness in object relation prediction and encourages further research on VLMs in this challenging domain.",
        "authors": [
            "Rakshith Subramanyam",
            "T. S. Jayram",
            "Rushil Anirudh",
            "J. Thiagarajan"
        ],
        "citations": 3,
        "references": 40,
        "year": 2023
    },
    {
        "title": "Self-PT: Adaptive Self-Prompt Tuning for Low-Resource Visual Question Answering",
        "abstract": "Pretraining and finetuning large vision-language models (VLMs) have achieved remarkable success in visual question answering (VQA). However, finetuning VLMs requires heavy computation, expensive storage costs, and is prone to overfitting for VQA in low-resource settings. Existing prompt tuning methods have reduced the number of tunable parameters, but they cannot capture valid context-aware information during prompt encoding, resulting in 1) poor generalization of unseen answers and 2) lower improvements with more parameters. To address these issues, we propose a prompt tuning method for low-resource VQA named Adaptive Self-Prompt Tuning (Self-PT), which utilizes representations of question-image pairs as conditions to obtain context-aware prompts. To enhance the generalization of unseen answers, Self-PT uses dynamic instance-level prompts to avoid overfitting the correlations between static prompts and seen answers observed during training. To reduce parameters, we utilize hyper-networks and low-rank parameter factorization to make Self-PT more flexible and efficient. The hyper-network decouples the number of parameters and prompt length to generate flexible-length prompts by the fixed number of parameters. While the low-rank parameter factorization decomposes and reparameterizes the weights of the prompt encoder into a low-rank subspace for better parameter efficiency. Experiments conducted on VQA v2, GQA, and OK-VQA with different low-resource settings show that our Self-PT outperforms the state-of-the-art parameter-efficient methods, especially in lower-shot settings, e.g., 6% average improvements cross three datasets in 16-shot. Code is available at https://github.com/NJUPT-MCC/Self-PT.",
        "authors": [
            "Bowen Yuan",
            "Sisi You",
            "Bing-Kun Bao"
        ],
        "citations": 3,
        "references": 52,
        "year": 2023
    },
    {
        "title": "Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning",
        "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel concepts formed by known states and objects during training. Existing methods either learn the combined state-object representation, challenging the generalization of unseen compositions, or design two classifiers to identify state and object separately from image features, ignoring the intrinsic relationship between them. To jointly eliminate the above issues and construct a more robust CZSL system, we propose a novel framework termed Decomposed Fusion with Soft Prompt (DFSP)11Code is available at: https://github.corn/Forest-art/DFSP.git, by involving vision-language models (VLMs)for unseen composition recognition. Specifically, DFSP constructs a vector combination of learnable soft prompts with state and object to establish the joint representation of them. In addition, a cross-modal decomposed fusion module is designed between the language and image branches, which decomposes state and object among language features instead of image features. Notably, being fused with the decomposed features, the image features can be more expressive for learning the relationship with states and objects, respectively, to improve the response of unseen compositions in the pair space, hence narrowing the domain gap between seen and unseen sets. Experimental results on three challenging benchmarks demonstrate that our approach significantly outperforms other state-of-the-art methods by large margins.",
        "authors": [
            "Xiaocheng Lu",
            "Ziming Liu",
            "Song Guo",
            "Jingcai Guo"
        ],
        "citations": 24,
        "references": 47,
        "year": 2022
    },
    {
        "title": "The Impact of Future Sea-Level Rise on Low-Lying Subsiding Coasts: A Case Study of Tavoliere Delle Puglie (Southern Italy)",
        "abstract": "Low-lying coastal zones are highly subject to coastal hazards as a result of sea-level rise enhanced by natural or anthropogenic land subsidence. A combined analysis using sea-level data and remote sensing techniques allows the estimation of the current rates of land subsidence and shoreline retreat, supporting the development of quantified relative sea-level projections and flood maps, which are appropriate for specific areas. This study focuses on the coastal plain of Tavoliere delle Puglie (Apulia, Southern Italy), facing the Adriatic Sea. In this area, land subsidence is mainly caused by long-term tectonic movements and sediment compaction driven by high anthropogenic pressure, such as groundwater exploitation and constructions of buildings. To assess the expected effects of relative sea-level rise for the next decades, we considered the following multidisciplinary source data: (i) sea-level-rise projections for different climatic scenarios, as reported in the Sixth Assessment Report of the Intergovernmental Panel on Climate Change, (ii) coastal topography from airborne and terrestrial LiDAR data, (iii) Vertical Land Movement (VLM) from the analysis of InSAR and GNSS data, and (iv) shoreline changes obtained from the analysis of orthophotos, historic maps, and satellite images. To assess the expected evolution of the coastal belt, the topographic data were corrected for VLM values, assuming that the rates of land subsidence will remain constant up to 2150. The sea-level-rise projections and expected flooded areas were estimated for the Shared Socioeconomic Pathways SSP1-2.6 and SSP5-8.5, corresponding to low and high greenhouse-gas concentrations, respectively. From our analysis, we estimate that in 2050, 2100, and 2150, up to 50.5 km2, 118.7 km2 and 147.7 km2 of the coast could be submerged, respectively, while beaches could retreat at rates of up to 5.8 m/yr. In this area, sea-level rise will be accelerated by natural and anthropogenic land subsidence at rates of up to ‚àí7.5 ¬± 1.7 mm/yr. Local infrastructure and residential areas are thus highly exposed to an increasing risk of severe inundation by storm surges and sea-level rise in the next decades.",
        "authors": [
            "G. Scardino",
            "M. Anzidei",
            "Paolo Petio",
            "E. Serpelloni",
            "V. Santis",
            "A. Rizzo",
            "Serena Isabella Liso",
            "Marina Zingaro",
            "D. Capolongo",
            "A. Vecchio",
            "A. Refice",
            "G. Scicchitano"
        ],
        "citations": 23,
        "references": 82,
        "year": 2022
    },
    {
        "title": "Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction",
        "abstract": "The Graphical User Interface (GUI) is pivotal for human interaction with the digital world, enabling efficient device control and the completion of complex tasks. Recent progress in Large Language Models (LLMs) and Vision Language Models (VLMs) offers the chance to create advanced GUI agents. To ensure their effectiveness, there's a pressing need for qualified benchmarks that provide trustworthy and reproducible evaluations -- a challenge current benchmarks often fail to address. To tackle this issue, we introduce Mobile-Env, a comprehensive toolkit tailored for creating GUI benchmarks in the Android mobile environment. Mobile-Env offers an isolated and controllable setting for reliable evaluations, and accommodates intermediate instructions and rewards to reflect real-world usage more naturally. Utilizing Mobile-Env, we collect an open-world task set across various real-world apps and a fixed world set, WikiHow, which captures a significant amount of dynamic online contents for fully controllable and reproducible evaluation. We conduct comprehensive evaluations of LLM agents using these benchmarks. Our findings reveal that even advanced models (e.g., GPT-4V and LLaMA-3) struggle with tasks that are relatively simple for humans. This highlights a crucial gap in current models and underscores the importance of developing more capable foundation models and more effective GUI agent frameworks.",
        "authors": [
            "Danyang Zhang",
            "Hongshen Xu",
            "Zihan Zhao",
            "Lu Chen",
            "Ruisheng Cao",
            "Kai Yu"
        ],
        "citations": 2,
        "references": 50,
        "year": 2023
    },
    {
        "title": "Holocene Sea Level Recorded by Beach Rocks at Ionian Coasts of Apulia (Italy)",
        "abstract": "Beach rocks are located along many coasts of the Mediterranean basin. The early diagenesis environment and the mean sea level along the shoreline make these landforms useful in the reconstruction of relative sea-level changes and, in particular, as SLIPs (sea-level index points). The beach rocks surveyed along the Ionian coast of Apulia were found to be well preserved at three specific depth ranges: 6‚Äì9 m, 3‚Äì4 m, and from the foreshore to about 1.20 m. Morpho-bathymetric and dive surveys were performed to assess both the geometries and the extension of the submerged beach rocks. Samples were collected at these different depths in the localities of Lido Torretta, Campomarino di Maruggio, San Pietro in Bevagna, and Porto Cesareo. Bivalve shells were identified and isolated from the beach rock samples collected at a depth of 7 m; AMS dating provided a calibrated age of about 7.8 ka BP. Their morphology and petrological features, along with the time constraints, enabled us to (i) reconstruct the local sea-level curve during the Holocene, (ii) corroborate acquired knowledge of the relative sea-level history, and (iii) identify possible local vertical land movement (VLM).",
        "authors": [
            "G. Mastronuzzi",
            "F. De Giosa",
            "G. Quarta",
            "M. Pallara",
            "G. Scardino",
            "G. Scicchitano",
            "Cosmo Peluso",
            "C. Antropoli",
            "C. Caporale",
            "M. Demarte"
        ],
        "citations": 2,
        "references": 67,
        "year": 2023
    },
    {
        "title": "Videoprompter: an ensemble of foundational models for zero-shot video understanding",
        "abstract": "Vision-language models (VLMs) classify the query video by calculating a similarity score between the visual features and text-based class label representations. Recently, large language models (LLMs) have been used to enrich the text-based class labels by enhancing the descriptiveness of the class names. However, these improvements are restricted to the text-based classifier only, and the query visual features are not considered. In this paper, we propose a framework which combines pre-trained discriminative VLMs with pre-trained generative video-to-text and text-to-text models. We introduce two key modifications to the standard zero-shot setting. First, we propose language-guided visual feature enhancement and employ a video-to-text model to convert the query video to its descriptive form. The resulting descriptions contain vital visual cues of the query video, such as what objects are present and their spatio-temporal interactions. These descriptive cues provide additional semantic knowledge to VLMs to enhance their zeroshot performance. Second, we propose video-specific prompts to LLMs to generate more meaningful descriptions to enrich class label representations. Specifically, we introduce prompt techniques to create a Tree Hierarchy of Categories for class names, offering a higher-level action context for additional visual cues, We demonstrate the effectiveness of our approach in video understanding across three different zero-shot settings: 1) video action recognition, 2) video-to-text and textto-video retrieval, and 3) time-sensitive video tasks. Consistent improvements across multiple benchmarks and with various VLMs demonstrate the effectiveness of our proposed framework. Our code will be made publicly available.",
        "authors": [
            "Adeel Yousaf",
            "Muzammal Naseer",
            "Salman H. Khan",
            "F. Khan",
            "Mubarak Shah"
        ],
        "citations": 2,
        "references": 45,
        "year": 2023
    },
    {
        "title": "GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction",
        "abstract": "Predicting pedestrian behavior is the key to ensure safety and reliability of autonomous vehicles. While deep learning methods have been promising by learning from annotated video frame sequences, they often fail to fully grasp the dynamic interactions between pedestrians and traffic, crucial for accurate predictions. These models also lack nuanced common sense reasoning. Moreover, the manual annotation of datasets for these models is expensive and challenging to adapt to new situations. The advent of Vision Language Models (VLMs) introduces promising alternatives to these issues, thanks to their advanced visual and causal reasoning skills. To our knowledge, this research is the first to conduct both quantitative and qualitative evaluations of VLMs in the context of pedestrian behavior prediction for autonomous driving. We evaluate GPT-4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our quantitative analysis focuses on GPT-4V's ability to predict pedestrian behavior in current and future frames. The model achieves a 57% accuracy in a zero-shot manner, which, while impressive, is still behind the state-of-the-art domain-specific models (70%) in predicting pedestrian crossing actions. Qualitatively, GPT-4V shows an impressive ability to process and interpret complex traffic scenarios, differentiate between various pedestrian behaviors, and detect and analyze groups. However, it faces challenges, such as difficulty in detecting smaller pedestrians and assessing the relative motion between pedestrians and the ego vehicle.",
        "authors": [
            "Jia Huang",
            "Peng Jiang",
            "Alvika Gautam",
            "S. Saripalli"
        ],
        "citations": 2,
        "references": 23,
        "year": 2023
    },
    {
        "title": "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models",
        "abstract": "Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue.",
        "authors": [
            "Zheng Ma",
            "Mianzhi Pan",
            "Wenhan Wu",
            "Ka Leong Cheng",
            "Jianbing Zhang",
            "Shujian Huang",
            "Jiajun Chen"
        ],
        "citations": 2,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Enhancing HOI Detection with Contextual Cues from Large Vision-Language Models",
        "abstract": "Human-Object Interaction (HOI) detection aims at detecting human-object pairs and predicting their interactions. However, conventional HOI detection methods often struggle to fully capture the contextual information needed to accurately identify these interactions. While large Vision-Language Models (VLMs) show promise in tasks involving human interactions, they are not tailored for HOI detection. The complexity of human behavior and the diverse contexts in which these interactions occur make it further challenging. Contextual cues, such as the participants involved, body language, and the surrounding environment, play crucial roles in predicting these interactions, especially those that are unseen or ambiguous. Moreover, large VLMs are trained on vast image and text data, enabling them to generate contextual cues that help in understanding real-world contexts, object relationships, and typical interactions. Building on this, in this paper we introduce ConCue, a novel approach for improving visual feature extraction in HOI detection. Specifically, we first design specialized prompts to utilize large VLMs to generate contextual cues within an image. To fully leverage these cues, we develop a transformer-based feature extraction module with a multi-tower architecture that integrates contextual cues into both instance and interaction detectors. Extensive experiments and analyses demonstrate the effectiveness of using these contextual cues for HOI detection. The experimental results show that integrating ConCue with existing state-of-the-art methods significantly enhances their performance on two widely used datasets.",
        "authors": [
            "Yu-Wei Zhan",
            "Fan Liu",
            "Xin Luo",
            "Liqiang Nie",
            "Xin-Shun Xu",
            "Mohan S. Kankanhalli"
        ],
        "citations": 2,
        "references": 62,
        "year": 2023
    },
    {
        "title": "A Retrospect to Multi-prompt Learning across Vision and Language",
        "abstract": "The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.",
        "authors": [
            "Ziliang Chen",
            "Xin Huang",
            "Quanlong Guan",
            "Liang Lin",
            "Weiqi Luo"
        ],
        "citations": 2,
        "references": 62,
        "year": 2023
    },
    {
        "title": "Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding",
        "abstract": "Vision-Language Models (VLMs), such as CLIP, exhibit strong image-text comprehension abilities, facilitating advances in several downstream tasks such as zero-shot image classification, image-text retrieval, and text-to-image generation. However, the compositional reasoning abilities of existing VLMs remains subpar. The root of this limitation lies in the inadequate alignment between the images and captions in the pretraining datasets. Additionally, the current contrastive learning objective fails to focus on fine-grained grounding components like relations, actions, and attributes, resulting in ‚Äúbag-of-words‚Äù representations. We introduce a simple and effective method to improve compositional reasoning in VLMs. Our method better leverages available datasets by refining and expanding the standard image-text contrastive learning framework. Our approach does not require specific annotations and does not incur extra parameters. When integrated with CLIP, our technique yields notable improvement over state-of-the-art baselines across five vision-language compositional benchmarks.11We open-source our code at https://github.com/lezhang7/Enhance-FineGrained.",
        "authors": [
            "Le Zhang",
            "Rabiul Awal",
            "Aishwarya Agrawal"
        ],
        "citations": 2,
        "references": 70,
        "year": 2023
    },
    {
        "title": "SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering",
        "abstract": "Video question‚Äìanswering is a fundamental task in the field of video understanding. Though current state-of-the-art video‚Äìlanguage pretrained models have yielded appealing performance, they are at the cost of huge computational power and thus hard to deploy on many platforms with limited resources. An economical workaround simply samples a small portion of frames to tune an image‚Äìtext model on these sampled frames. However, the sampling methods adopted by these VLMs are quite simple and straightforward‚Äî such methods are aimless and often inevitably omit the key frames from which the correct answer can be deduced, and the situation becomes worse as the sampling sparsity increases, which particularly requires when the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namely the most dominant frames (MDF) and most implied frames (MIF), to maximally preserve those frames that are most likely vital to the given questions. MDF passively minimizes the risk of key frame omission in a bootstrap manner, while MIF actively searches key frames customized for each video‚Äìquestion pair with the assistance of auxiliary models. The experimental results on three public datasets and three advanced VLMs (CLIP, GIT and All-in-one) demonstrate that our proposed strategies can boost the performance for image‚Äìtext pretrained models. The source codes pertaining to the method proposed in this paper are publicly available at https://github.com/declare-lab/sas-vqa.",
        "authors": [
            "Wei Han",
            "Hui Chen",
            "MingSung Kan",
            "Soujanya Poria"
        ],
        "citations": 2,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment",
        "abstract": "Large-scale pre-trained Vision Language Models (VLMs) have proven effective for zero-shot classification. Despite the success, most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal target vocabularies, which rarely satisfy the open-world scenario. In this paper, we aim at a more challenging setting, Realistic Zero-Shot Classification, which assumes no annotation but instead a broad vocabulary. To address the new problem, we propose the Self Structural Semantic Alignment (S3A) framework, which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S3A framework adopts a unique Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR algorithm includes iterative clustering on images, voting within each cluster to identify initial class candidates from the vocabulary, generating discriminative prompts with large language models to discern confusing candidates, and realigning images and the vocabulary as structural semantic alignment. Finally, we propose to self-train the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S3A method substantially improves over existing VLMs-based approaches, achieving a more than 15% accuracy improvement over CLIP on average. Our codes, models, and prompts are publicly released at https://github.com/sheng-eatamath/S3A.",
        "authors": [
            "Shengxiang Zhang",
            "Muzammal Naseer",
            "Guangyi Chen",
            "Zhiqiang Shen",
            "Salman A. Khan",
            "Kun Zhang",
            "F. Khan"
        ],
        "citations": 2,
        "references": 73,
        "year": 2023
    },
    {
        "title": "Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples",
        "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race&gender). We conduct extensive experiments using our generated dataset which reveal the intersectional social biases present in state-of-the-art VLMs.",
        "authors": [
            "Phillip Howard",
            "Avinash Madasu",
            "Tiep Le",
            "Gustavo Lujan Moreno",
            "Vasudev Lal"
        ],
        "citations": 2,
        "references": 19,
        "year": 2023
    },
    {
        "title": "SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models",
        "abstract": "Despite achieving remarkable performance on various vision-language tasks, Transformer-based Vision-Language Models (VLMs) suffer from redundancy in inputs and parameters, significantly hampering their efficiency in real-world applications. Moreover, the degree of redundancy in token representations and model parameters, such as attention heads, varies significantly for different inputs. In light of the challenges, we propose SmartTrim, an adaptive acceleration framework for VLMs, which adjusts the computational overhead per instance. Specifically, we integrate lightweight modules into the original backbone to identify and prune redundant token representations and attention heads within each layer. Furthermore, we devise a self-distillation strategy to enhance the consistency between the predictions of the pruned model and its fully-capacity counterpart. Experimental results across various vision-language tasks consistently demonstrate that SmartTrim accelerates the original model by 2-3 times with minimal performance degradation, highlighting the effectiveness and efficiency compared to previous approaches. Code will be available at https://github.com/kugwzk/SmartTrim.",
        "authors": [
            "Zekun Wang",
            "Jingchang Chen",
            "Wangchunshu Zhou",
            "Haichao Zhu",
            "Jiafeng Liang",
            "Liping Shan",
            "Ming Liu",
            "Dongliang Xu",
            "Qing Yang",
            "Bing Qin"
        ],
        "citations": 2,
        "references": 70,
        "year": 2023
    },
    {
        "title": "LT at SemEval-2023 Task 1: Effective Zero-Shot Visual Word Sense Disambiguation Approaches using External Knowledge Sources",
        "abstract": "The objective of the SemEval-2023 Task 1: Visual Word Sense Disambiguation (VWSD) is to identify the image illustrating the indented meaning of a target word and some minimal additional context. The omnipresence of textual and visual data in the task strongly suggests the utilization of the recent advances in multi-modal machine learning, i.e., pretrained visiolinguistic models (VLMs). Often referred to as foundation models due to their strong performance on many vision-language downstream tasks, these models further demonstrate powerful zero-shot capabilities. In this work, we utilize various pertained VLMs in a zero-shot fashion for multiple approaches using external knowledge sources to enrich the contextual information. Further, we evaluate our methods on the final test data and extensively analyze the suitability of different knowledge sources, the influence of training data, model sizes, multi-linguality, and different textual prompting strategies. Although we are not among the best-performing systems (rank 20 of 56), our experiments described in this work prove competitive results. Moreover, we aim to contribute meaningful insights and propel multi-modal machine learning tasks like VWSD.",
        "authors": [
            "Florian Schneider",
            "Chris Biemann"
        ],
        "citations": 2,
        "references": 22,
        "year": 2023
    },
    {
        "title": "Unseen And Adverse Outdoor Scenes Recognition Through Event-based Captions",
        "abstract": "This paper presents EventCAP, i.e., event-based captions, for refined and enriched qualitative and quantitative captions by Deep Learning (DL) models and Vision Language Models (VLMs) with different tasks in a complementary manner. Indoor and outdoor images are used for object recognition and captioning. However, outdoor images in events change in wide ranges due to natural phenomena, i.e., weather changes. Such dynamical changes may degrade segmentation by illumination and object shape changes. This increases unseen objects and scenes under such adverse conditions. On the other hand, single state-of-art (SOTA) DLs and VLMs work with single or limited tasks, Therefore, this paper proposes EventCAP with captions with physical scales and objects‚Äô surface properties. Moreover, an iterative VQA model is proposed to refine in-complete segmented images with the prompts. A higher se-mantic level in captions for real-world scene descriptions is experimentally shown compared to SOTA VLMs.",
        "authors": [
            "Hidetomo Sakaino"
        ],
        "citations": 1,
        "references": 53,
        "year": 2023
    },
    {
        "title": "Self-Guided Open-Vocabulary Semantic Segmentation",
        "abstract": "Vision-Language Models (VLMs) have emerged as promising tools for open-ended image understanding tasks, including open vocabulary segmentation. Yet, direct application of such VLMs to segmentation is non-trivial, since VLMs are trained with image-text pairs and naturally lack pixel-level granularity. Recent works have made advancements in bridging this gap, often by leveraging the shared image-text space in which the image and a provided text prompt are represented. In this paper, we challenge the capabilities of VLMs further and tackle open-vocabulary segmentation without the need for any textual input. To this end, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework. Self-Seg is capable of automatically detecting relevant class names from clustered BLIP embeddings and using these for accurate semantic segmentation. In addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) to effectively assess predicted open-vocabulary class names. We achieve state-of-the-art results on Pascal VOC, ADE20K and CityScapes for open-vocabulary segmentation without given class names, as well as competitive performance with methods where class names are given. All code and data will be released.",
        "authors": [
            "Osman √úlger",
            "Maksymilian Kulicki",
            "Yuki Asano",
            "Martin R. Oswald"
        ],
        "citations": 1,
        "references": 37,
        "year": 2023
    },
    {
        "title": "SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models",
        "abstract": "Despite achieving remarkable performance on various vision-language tasks, Transformer-based pretrained vision-language models (VLMs) still suffer from efficiency issues arising from long inputs and numerous parameters, limiting their real-world applications. However, the huge computation is redundant for most samples and the degree of redundancy and the respective components vary significantly depending on tasks and input instances. In this work, we propose an adaptive acceleration method S MART T RIM for VLMs, which adjusts the inference overhead based on the complexity of instances. Specifically, S MART T RIM incorporates lightweight trimming modules into the backbone to perform task-specific pruning on redundant inputs and parameters, without the need for additional pre-training or data augmentation. Since visual and textual representations complement each other in VLMs, we propose to leverage cross-modal interaction information to provide more critical semantic guidance for identifying redundant parts. Meanwhile, we introduce a self-distillation strategy that encourages the trimmed model to be consistent with the full-capacity model, which yields further performance gains. Experimental results demonstrate that S MART T RIM significantly reduces the computation overhead ( 2 - 3 times) of various VLMs with comparable performance (only a 1 - 2% degradation) on various vision-language tasks. Compared to previous acceleration methods, S MART T RIM attains a better efficiency-performance trade-off, demonstrating great potential for application in resource-constrained scenarios.",
        "authors": [
            "Zekun Wang",
            "Jingchang Chen",
            "Wangchunshu Zhou",
            "Ming Liu",
            "Bing Qin"
        ],
        "citations": 1,
        "references": 87,
        "year": 2023
    },
    {
        "title": "Compositional Semantics for Open Vocabulary Spatio-semantic Representations",
        "abstract": "General-purpose mobile robots need to complete tasks without exact human instructions. Large language models (LLMs) is a promising direction for realizing commonsense world knowledge and reasoning-based planning. Vision-language models (VLMs) transform environment percepts into vision-language semantics interpretable by LLMs. However, completing complex tasks often requires reasoning about information beyond what is currently perceived. We propose latent compositional semantic embeddings z* as a principled learning-based knowledge representation for queryable spatio-semantic memories. We mathematically prove that z* can always be found, and the optimal z* is the centroid for any set Z. We derive a probabilistic bound for estimating separability of related and unrelated semantics. We prove that z* is discoverable by iterative optimization by gradient descent from visual appearance and singular descriptions. We experimentally verify our findings on four embedding spaces incl. CLIP and SBERT. Our results show that z* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics for ideal uniformly distributed high-dimensional embeddings. We demonstrate that a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181 overlapping semantics by 42.23 mIoU, while improving conventional non-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared with a popular SOTA model.",
        "authors": [
            "Robin Karlsson",
            "Francisco Lepe-Salazar",
            "K. Takeda"
        ],
        "citations": 1,
        "references": 104,
        "year": 2023
    },
    {
        "title": "Vulvar Leiomyosarcomas: A Case Series with Clinical Comparison to Uterine Leiomyosarcomas and Review of the Literature",
        "abstract": "Case series Patients: ‚Äî Final Diagnosis: Vulvar leiomyosarcoma Symptoms: Vulvar mass ‚Ä¢ vulvar pain ‚Ä¢ vulvar swelling Clinical Procedure: ‚Äî Specialty: Obstetrics and Gynecology ‚Ä¢ Oncology Objective: Rare disease Background: Leiomyosarcomas of the vulva (VLMS) are very rare among gynecological malignancies, with a lack of knowledge on clinical presentation, prognosis, and therapeutic management. Case Reports: The database of the German Clinical Center of Competence for Genital Sarcomas and Mixed Tumors in Greifswald (DKSM) was reviewed between the years 2010 and 2020. A total of 8 cases of VLMS were retrieved and analyzed retrospectively. One exemplary case of VLMS was outlined in detail: A 45-year-old premenopausal woman presented with increasing vulvar swelling and discomfort. Given the suspicion of a Bartholin‚Äôs gland abscess, the mass was excised. Final pathology revealed a solid tumor consistent with a moderately differentiated leiomyosarcoma of the vulva. A wide local excision was subsequently performed followed by adjuvant external beam radiation. The clinical features of these 8 cases of VLMS were compared to 26 cases of VLMS found in a review of the literature and to a total of 276 cases of uterine leiomyosarcoma (ULMS) from the same database (DKSM). Conclusions: In addition to rapid growth, observed in both tumor entities, VLMS most commonly presented as Bartholin‚Äôs gland abscess or cyst and ULMS as leiomyoma. In this cohort, the prognosis of VLMS was much better than that of ULMS, most probably due to the significantly smaller tumor size of VLMS at diagnosis. Further data and larger studies on VLMS are needed to calculate recurrence and survival rates more accurately and define the role of adjuvant radiotherapy.",
        "authors": [
            "S. Verta",
            "Z. Alwafai",
            "Nika Schleede",
            "C. Brambs",
            "C. Christmann",
            "V. Reichert",
            "M. Zygmunt",
            "B. Plattner",
            "G. K√∂hler"
        ],
        "citations": 1,
        "references": 51,
        "year": 2023
    },
    {
        "title": "Assessing Language and Vision-Language Models on Event Plausibility",
        "abstract": "Transformer-based Language Models (LMs) excel in many tasks, but they appear to lack robustness in capturing crucial aspects of event knowledge due to their reliance on surface-level linguistic features and the mismatch between language descriptions and real-world occurrences. In this paper, we investigate the potential of Transformer-based Vision-Language Models (VLMs) in comprehending Generalized Event Knowledge (GEK) , aiming to determine whether the inclusion of a visual component affects the mastery of GEK. To do so, we compare multimodal Transformer models with unimodal ones on a task evaluating the plausibility of curated minimal sentence pairs. We show that current VLMs generally perform worse than their unimodal counterparts, suggesting that VL pre-training strategies are not yet as effective to model semantic understanding and resulting models are more akin to bag-of-words in this context.",
        "authors": [
            "Maria Cassese",
            "Alessandro Bondielli",
            "Alessandro Lenci"
        ],
        "citations": 1,
        "references": 32,
        "year": 2023
    },
    {
        "title": "Bridging Low-level Geometry to High-level Concepts in Visual Servoing of Robot Manipulation Task Using Event Knowledge Graphs and Vision-Language Models",
        "abstract": "In this paper, we propose a framework of building knowledgeable robot control in the scope of smart human-robot interaction, by empowering a basic uncalibrated visual servoing controller with contextual knowledge through the joint usage of event knowledge graphs (EKGs) and large-scale pretrained vision-language models (VLMs). The framework is expanded in twofold: first, we interpret low-level image geometry as high-level concepts, allowing us to prompt VLMs and to select geometric features of points and lines for motor control skills; then, we create an event knowledge graph (EKG) to conceptualize a robot manipulation task of interest, where the main body of the EKG is characterized by an executable behavior tree, and the leaves by semantic concepts relevant to the manipulation context. We demonstrate, in an uncalibrated environment with real robot trials, that our method lowers the reliance of human annotation during task interfacing, allows the robot to perform activities of daily living more easily by treating low-level geometric-based motor control skills as high-level concepts, and is beneficial in building cognitive thinking for smart robot applications.",
        "authors": [
            "Chen Jiang",
            "Martin J√§gersand"
        ],
        "citations": 1,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Subsampling of Frequent Words in Text for Pre-training a Vision-Language Model",
        "abstract": "In this paper, we introduce Subsampling of frequent Words for Contrastive Language-Image Pre-training (SW-CLIP), a novel approach for the training Vision-Language Models (VLMs). SW-CLIP uses frequency-based subsampling of words that has been previously proposed to train skip-gram models in natural language processing and applies it to the textual training data of VLMs. We report on experiments that demonstrate the ability of frequency-based subsampling to speed up training and also to deliver a substantial improvement in accuracy in a number of downstream zero-shot (i.e., transfer) classification tasks. We notice that the classification test sets on which SW-CLIP seems to be particularly effective are those in which the labels of the classes occur infrequently as words in the training data, and thus have a high probability of being retained during frequency-based subsampling of the model training data. Overall, the advantages of SW-CLIP demonstrated in this paper serves to motivated further future work in text subsampling for the training of VLMs. Our code and pre-trained weights are available at https://github.com/Anastasiais-ml/sw_clip.git",
        "authors": [
            "Mingliang Liang",
            "Martha Larson"
        ],
        "citations": 1,
        "references": 31,
        "year": 2023
    },
    {
        "title": "Towards Versatile and Efficient Visual Knowledge Integration into Pre-trained Language Models with Cross-Modal Adapters",
        "abstract": "Humans learn language via multi-modal knowledge. However, due to the text-only pre-training scheme, most existing pre-trained language models (PLMs) are hindered from the multi-modal information. To inject visual knowledge into PLMs, existing methods incorporate either the text or image encoder of vision-language models (VLMs) to encode the visual information and update all the original parameters of PLMs for knowledge fusion. In this paper, we propose a new plug-and-play module, X-adapter, to flexibly leverage the aligned visual and textual knowledge learned in pre-trained VLMs and efficiently inject them into PLMs. Specifically, we insert X-adapters into PLMs, and only the added parameters are updated during adaptation. To fully exploit the potential in VLMs, X-adapters consist of two sub-modules, V-expert and T-expert, to fuse VLMs' image and text representations, respectively. We can opt for activating different sub-modules depending on the downstream tasks. Experimental results show that our method can significantly improve the performance on object-color reasoning and natural language understanding (NLU) tasks compared with PLM baselines.",
        "authors": [
            "Xinyun Zhang",
            "Haochen Tan",
            "Han Wu",
            "Bei Yu"
        ],
        "citations": 1,
        "references": 42,
        "year": 2023
    },
    {
        "title": "Dynamic Texts From UAV Perspective Natural Images",
        "abstract": "Drone-based image processing offers valuable capabilities for surveillance, detection, and tracking in vast areas, aiding in disaster search and rescue, and monitoring artificial events like traffic jams and outdoor activities under adversarial weather conditions. Nonetheless, this technology encounters numerous challenges, including handling variations in scales and perspectives and coping with environmental factors like sky interference and the presence of far and small objects. Besides, ensuring high visibility distance in 3D depth is crucial for safe flights in various settings, including airports, cities, and fields. However, local weather conditions can change rapidly during flights, leading to visibility issues caused by fog and clouds. Due to the cost of visibility measurement sensors, lower-cost methods using portable apparatus are desired for flight routines. Therefore, this paper proposes a camera-based visibility and weather condition estimation approach using complementary multiple Deep Learning (DL) and Vision Language Models (VLM) under adversarial conditions. Experimental results show the superiority of enhanced 2D/3D captions with physical scales over SOTA VLMs.",
        "authors": [
            "Hidetomo Sakaino"
        ],
        "citations": 1,
        "references": 61,
        "year": 2023
    },
    {
        "title": "Towards Versatile and Efficient Visual Knowledge Injection into Pre-trained Language Models with Cross-Modal Adapters",
        "abstract": "Humans learn language via multi-modal knowledge. However, due to the text-only pre-training scheme, most existing pre-trained language models (PLMs) are hindered from the multi-modal information. To inject visual knowledge into PLMs, existing methods incorporate either the text or image encoder of vision-language models (VLMs) to encode the visual information and update all the original parameters of PLMs for knowledge fusion. In this paper, we propose a new plug-and-play module, X-adapter, to flexibly leverage the aligned visual and textual knowledge learned in pre-trained VLMs and efficiently inject them into PLMs. Specifically, we insert X-adapters into PLMs, and only the added parameters are updated during adaptation. To fully exploit the potential in VLMs, X-adapters consist of two sub-modules, V-expert and T-expert, to fuse VLMs‚Äô image and text representations, respectively. We can opt for activating different sub-modules depending on the down-stream tasks. Experimental results show that our method can significantly improve the performance on object-color reasoning and natural language understanding (NLU) tasks compared with PLM baselines.",
        "authors": [
            "Xinyun Zhang",
            "Haochen Tan",
            "Han Wu",
            "Mingjie Zhan",
            "Ding Liang",
            "Bei Yu"
        ],
        "citations": 1,
        "references": 43,
        "year": 2023
    },
    {
        "title": "Self-Adaptive Sampling for Efficient Video Question-Answering on Image--Text Models",
        "abstract": "Video question-answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namely the most domain frames (MDF) and most implied frames (MIF), to maximally preserve those frames that are most likely vital to the given questions. MDF passively minimizes the risk of key frame omission in a bootstrap manner, while MIS actively searches key frames customized for each video--question pair with the assistance of auxiliary models. The experimental results on three public datasets from three advanced VLMs (CLIP, GIT and All-in-one) demonstrate that our proposed strategies can boost the performance for image-text pretrained models. The source codes pertaining to the method proposed in this paper are publicly available at https://github.com/declare-lab/sas-vqa.",
        "authors": [
            "Wei Han",
            "Hui Chen",
            "MingSung Kan",
            "Soujanya Poria"
        ],
        "citations": 1,
        "references": 49,
        "year": 2023
    },
    {
        "title": "Active Prompt Learning in Vision Language Models",
        "abstract": "Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks, such as classification and retrieval. Despite their performance, because improving performance on new tasks requires task-specific knowledge, their adaptation is essential. While labels are needed for the adaptation, acquiring them is typically expensive. To overcome this challenge, active learning, a method of achieving a high performance by obtaining labels for a small number of samples from experts, has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leveraging them to train models. In this study, we pose the question, ‚Äúhow can the pre-trained VLMs be adapted under the active learning framework?‚Äù In response to this inquiry, we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates, and (2) the knowledge of VLMs can provide hints for achieving the balance before labeling. Based on these observations, we devise a novel active learning framework for VLMs, denoted as PCB. To assess the effectiveness of our approach, we conduct experiments on seven different real-world datasets, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods. Code is available at https://github.com/kaist-dmlab/pcb.",
        "authors": [
            "Jihwan Bang",
            "Sumyeong Ahn",
            "Jae-Gil Lee"
        ],
        "citations": 1,
        "references": 69,
        "year": 2023
    },
    {
        "title": "ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models",
        "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model. Code is available at https://github.com/ExplainableML/ProbVLM.",
        "authors": [
            "Uddeshya Upadhyay",
            "Shyamgopal Karthik",
            "Massimiliano Mancini",
            "Zeynep Akata"
        ],
        "citations": 1,
        "references": 107,
        "year": 2023
    },
    {
        "title": "VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training",
        "abstract": "Vision-and-language pre-trained models (VLMs) have achieved tremendous success in the cross-modal area, but most of them require a large amount of parallel image-caption data for pre-training. Collating such data is expensive and labor-intensive. In this work, we focus on reducing such need for generative vision-and-language pre-training (G-VLP) by taking advantage of the visual pre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2) as decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which hinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we conduct extensive experiments to empirically investigate how to design and pre-train our model. Based on our experimental results, we propose a novel G-VLP framework, Visual Conditioned GPT (VC-GPT) , and pre-train it with a small-scale image-caption corpus (Visual Genome, only 110k distinct images). Evaluating on the image captioning downstream tasks (MSCOCO and Flickr30k Captioning), VC-GPT achieves either the best or the second-best performance across all evaluation metrics over the previous works which consume around 30 times more distinct images during cross-modal pre-training.",
        "authors": [
            "Ziyang Luo",
            "Yadong Xi",
            "Rongsheng Zhang",
            "Jing Ma"
        ],
        "citations": 17,
        "references": 57,
        "year": 2022
    },
    {
        "title": "Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks",
        "abstract": "Text-to-image synthesis is an attractive but challenging task that aims to generate a photo-realistic and semantic consistent image from a specific text description. The images synthesized by off-the-shelf models usually contain limited components compared with the corresponding image and text description, which decreases the image quality and the textual-visual consistency. To address this issue, we propose a novel Vision-Language Matching strategy for text-to-image synthesis, named VLMGAN*, which introduces a dual vision-language matching mechanism to strengthen the image quality and semantic consistency. The dual vision-language matching mechanism considers textual-visual matching between the generated image and the corresponding text description, and visual-visual consistent constraints between the synthesized image and the real image. Given a specific text description, VLMGAN* firstly encodes it into textual features and then feeds them to a dual vision-language matching-based generative model to synthesize a photo-realistic and textual semantic consistent image. Besides, the popular evaluation metrics for text-to-image synthesis are borrowed from simple image generation, which mainly evaluate the reality and diversity of the synthesized images. Therefore, we introduce a metric named Vision-Language Matching Score (VLMS) to evaluate the performance of text-to-image synthesis which can consider both the image quality and the semantic consistency between the synthesized image and the description. The proposed dual multi-level vision-language matching strategy can be applied to other text-to-image synthesis methods. We implement this strategy on two popular baselines, which are marked with ${\\text{VLMGAN}_{+\\text{AttnGAN}}}$ and ${\\text{VLMGAN}_{+\\text{DFGAN}}}$. The experimental results on two widely-used datasets show that the model achieves significant improvements over other state-of-the-art methods.",
        "authors": [
            "Qingrong Cheng",
            "Keyu Wen",
            "X. Gu"
        ],
        "citations": 14,
        "references": 79,
        "year": 2022
    },
    {
        "title": "Lignin Distribution on Cell Wall Micro-Morphological Regions of Fibre in Developmental Phyllostachys pubescens Culms",
        "abstract": "Bamboo is a natural fibre reinforced composite with excellent performance which is, to a certain extent, an alternative to the shortage of wood resources. The heterogeneous distribution and molecular structure of lignin is one of the factors that determines its performance, and it is the key and most difficult component in the basic research into the chemistry of bamboo and in bamboo processing and utilization. In this study, the distribution of lignin components and lignin content in micro-morphological regions were measured in semi-quantitative level by age and radial location by means of visible-light microspectrophotometry (VLMS) coupled with the Wiesner and Maule reaction. There as guaiacyl lignin and syringyl lignin in the cell wall of the fibre. Lignin content of the secondary cell wall and cell corner increased at about 10 days, reached a maximum at 1 year, and then decreased gradually. From 17 days to 4 years, the lignin content of the secondary cell wall in the outer part of bamboo is higher than that in the middle part (which is, in turn, higher than that in the inner part of the bamboo). VLSM results of the micro-morphological regions showed that bamboo lignification developed by aging. Guaiacyl and syringl lignin units can be found in the cell wall of the fibre, parenchyma, and vessel. There was a difference in lignin content among different ages, different radial location, and different micro-morphological regions of the cell wall. The fibre walls were rich in guaiacyl lignin in the early stage of lignification and rich in syringyl units in the later stage of lignification. The guaiacyl and syringyl lignin deposition of bamboo green was earlier than that of the middle part of bamboo culm, and that of the middle part of bamboo culm was earlier than that of bamboo yellow. The single molecule lignin content of the thin layer is higher than that of thick layers, while the primary wall is higher than the secondary cell wall, showing that lignin deposition is consistent with the rules of cell wall formation. The obtained cytological information is helpful to understand the origin of the anisotropic, physical, mechanical, chemical, and machining properties of bamboo.",
        "authors": [
            "Bo Liu",
            "Lina Tang",
            "Qianqian Chen",
            "Liming Zhu",
            "Xianwu Zou",
            "Botao Li",
            "Q. Zhou",
            "Yuejin Fu",
            "Yun Lu"
        ],
        "citations": 11,
        "references": 33,
        "year": 2022
    },
    {
        "title": "Prompting Large Pre-trained Vision-Language Models For Compositional Concept Learning",
        "abstract": "This work explores the zero-shot compositional learning ability of large pre-trained vision-language models(VLMs) within the prompt-based learning framework and propose a model (\\textit{PromptCompVL}) to solve the compositonal zero-shot learning (CZSL) problem. \\textit{PromptCompVL} makes two design choices: first, it uses a soft-prompting instead of hard-prompting to inject learnable parameters to reprogram VLMs for compositional learning. Second, to address the compositional challenge, it uses the soft-embedding layer to learn primitive concepts in different combinations. By combining both soft-embedding and soft-prompting, \\textit{PromptCompVL} achieves state-of-the-art performance on the MIT-States dataset. Furthermore, our proposed model achieves consistent improvement compared to other CLIP-based methods which shows the effectiveness of the proposed prompting strategies for CZSL.",
        "authors": [
            "Guangyue Xu",
            "Parisa Kordjamshidi",
            "J. Chai"
        ],
        "citations": 9,
        "references": 17,
        "year": 2022
    },
    {
        "title": "MOMA-LRG: Language-Refined Graphs for Multi-Object Multi-Actor Activity Parsing",
        "abstract": "Video-language models (VLMs), large models pre-trained on numerous but noisy video-text pairs from the internet, have revolutionized activity recognition through their remarkable generalization and open-vocabulary capabilities. While complex human activities are often hierarchical and compositional, most existing tasks for evaluating VLMs focus only on high-level video understanding, making it difficult to accurately assess and interpret the ability of VLMs to understand complex and fine-grained human activities. Inspired by the recently proposed MOMA framework, we define activity graphs as a single universal representation of human activities that encompasses video understanding at the activity, subactivity, and atomic action level. We redefine activity parsing as the overarching task of activity graph generation, requiring understanding human activities across all three levels. To facilitate the evaluation of models on activity parsing, we introduce MOMA-LRG (Multi-Object Multi-Actor Language-Refined Graphs), a large dataset of complex human activities with activity graph annotations that can be readily transformed into natural language sentences. Lastly, we present a model-agnostic and lightweight approach to adapting and evaluating VLMs by incorporating structured knowledge from activity graphs into VLMs, addressing the individual limitations of language and graphical models. We demonstrate strong performance on activity parsing and few-shot video classification, and our framework is intended to foster future research in the joint modeling of videos, graphs, and language.",
        "authors": [
            "Zelun Luo",
            "Zane Durante",
            "Linden Li",
            "Wanze Xie",
            "Ruochen Liu",
            "Emily Jin",
            "Zhuoyi Huang",
            "Lun Yu Li",
            "Jiajun Wu",
            "Juan Carlos Niebles",
            "E. Adeli",
            "Li Fei-Fei"
        ],
        "citations": 8,
        "references": 103,
        "year": 2022
    },
    {
        "title": "Leveraging per Image-Token Consistency for Vision-Language Pre-training",
        "abstract": "Most existing vision-language pre-training (VLP) approaches adopt cross-modal masked language modeling (CMLM) to learn vision-language associations. However, we find that CMLM is insufficient for this purpose according to our observations: (1) Modality bias: a considerable amount of masked tokens in CMLM can be recovered with only the language information, ignoring the visual inputs. (2) Underutilization of the unmasked tokens: CMLM primarily focuses on the masked token but it cannot simultaneously leverage other tokens to learn vision-language associations. To handle those limitations, we propose EPIC (lEveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, for each image-sentence pair, we mask tokens that are salient to the image (i.e., Saliency-based Masking Strategy) and replace them with alternatives sampled from a language model (i.e., Inconsistent Token Generation Procedure), and then the model is required to determine for each token in the sentence whether it is consistent with the image (i.e., Image-Token Consistency Task). The proposed EPIC method is easily combined with pre-training methods. Extensive experiments show that the combination of the EPIC method and state-of-the-art pre-training approaches, including ViLT, ALBEF, METER, and X-VLM, leads to significant improvements on downstream tasks. Our coude is released at https://github.com/gyhdog99/epic",
        "authors": [
            "Yunhao Gou",
            "Tom Ko",
            "Hansi Yang",
            "J. Kwok",
            "Yu Zhang",
            "Mingxuan Wang"
        ],
        "citations": 8,
        "references": 39,
        "year": 2022
    },
    {
        "title": "Correlation Information Bottleneck: Towards Adapting Pretrained Multimodal Models for Robust Visual Question Answering",
        "abstract": null,
        "authors": [
            "Jingjing Jiang",
            "Zi-yi Liu",
            "Nanning Zheng"
        ],
        "citations": 7,
        "references": 104,
        "year": 2022
    },
    {
        "title": "VIPHY: Probing \"Visible\" Physical Commonsense Knowledge",
        "abstract": "In recent years, vision-language models (VLMs) have shown remarkable performance on visual reasoning tasks (e.g. attributes, location). While such tasks measure the requisite knowledge to ground and reason over a given visual instance, they do not, however, measure the ability of VLMs to retain and generalize such knowledge. In this work, we evaluate their ability to acquire\"visible\"physical knowledge -- the information that is easily accessible from images of static scenes, particularly across the dimensions of object color, size and space. We build an automatic pipeline to derive a comprehensive knowledge resource for calibrating and probing these models. Our results indicate a severe gap between model and human performance across all three tasks. Furthermore, our caption pretrained baseline (CapBERT) significantly outperforms VLMs on both size and spatial tasks -- highlighting that despite sufficient access to ground language with visual modality, they struggle to retain such knowledge. The dataset and code are available at https://github.com/Axe--/ViPhy .",
        "authors": [
            "Shikhar Singh",
            "Ehsan Qasemi",
            "Muhao Chen"
        ],
        "citations": 6,
        "references": 67,
        "year": 2022
    },
    {
        "title": "First steps of planet formation around very low mass stars and brown dwarfs",
        "abstract": null,
        "authors": [
            "P. Pinilla"
        ],
        "citations": 6,
        "references": 92,
        "year": 2022
    },
    {
        "title": "Pre-Trained Word Embedding and Language Model Improve Multimodal Machine Translation: A Case Study in Multi30K",
        "abstract": "Multimodal machine translation (MMT) is an attractive application of neural machine translation (NMT) that is commonly incorporated with image information. However, the MMT models proposed thus far have only comparable or slightly better performance than their text-only counterparts. One potential cause of this infeasibility is a lack of large-scale data. Most previous studies mitigate this limitation by employing large-scale textual parallel corpora, which are more accessible than multimodal parallel corpora, in various ways. However, these corpora are still available on only a limited scale in low-resource language pairs or domains. In this study, we leveraged monolingual (or multimodal monolingual) corpora, which are available at scale in most languages and domains, to improve MMT models. Our approach follows that of previous unimodal works that use monolingual corpora to train the word embedding or language model and incorporate them into NMT systems. While these methods demonstrated the advantage of using pre-trained representations, there is still room for MMT models to improve. To this end, our system employs debiasing procedures for the word embedding and multimodal extension of the language model (visual-language model, VLM) to make better use of the pre-trained knowledge in the MMT task. The results of evaluations conducted on the de facto MMT dataset for the English‚ÄìGerman translation indicate that the improvement obtained using well-tailored word embedding and VLM is approximately +1.84 BLEU and +1.63 BLEU, respectively. The evaluation on multiple language pairs reveals their adoptability across the languages. Beyond the success of our system, we also conducted an extensive analysis on VLM manipulation and showed promising areas for developing better MMT models by exploiting VLM; some benefits brought by either modality are missing, and MMT with VLM generates less fluent translations. Our code is available at https://github.com/toshohirasawa/mmt-with-monolingual-data.",
        "authors": [
            "Tosho Hirasawa",
            "Masahiro Kaneko",
            "Aizhan Imankulova",
            "Mamoru Komachi"
        ],
        "citations": 6,
        "references": 60,
        "year": 2022
    },
    {
        "title": "Orthostatic hypotension with nondipping: phenotype of neurodegenerative disease",
        "abstract": null,
        "authors": [
            "M. Nagai",
            "Masaya Kato",
            "K. Dote"
        ],
        "citations": 4,
        "references": 15,
        "year": 2022
    },
    {
        "title": "FashionVQA: A Domain-Specific Visual Question Answering System",
        "abstract": "Humans apprehend the world through various sensory modalities, yet language is their predominant communication channel. Machine learning systems need to draw on the same multimodal richness to have informed discourses with humans in natural language; this is particularly true for systems specialized in visually-dense information, such as dialogue, recommendations, and search engines for clothing. To this end, we train a visual question-answering (VQA) system to answer complex natural language questions about apparel in fashion photoshoot images. The key to the successful training of our VQA model is the automatic creation of a visual question-answering dataset with 168 million samples from item attributes of 207 thousand images using diverse templates. The sample generation employs a strategy that considers the difficulty of the question-answer pairs to emphasize challenging concepts. We see that using the same transformer for encoding the question and decoding the answer, as in language models, achieves maximum accuracy, showing that visual language models (VLMs) make the optimal visual question-answering systems for our dataset. The accuracy of the best model surpasses the human expert level. Our approach for generating a large-scale multimodal domain-specific dataset provides a path for training specialized models capable of communicating in natural language. The training of such domain-expert models, e.g., our fashion VLM model, cannot rely solely on the large-scale general-purpose datasets collected from the web.",
        "authors": [
            "Min Wang",
            "A. Mahjoubfar",
            "Anupama Joshi"
        ],
        "citations": 3,
        "references": 43,
        "year": 2022
    },
    {
        "title": "Components of 21¬†years (1995‚Äì2015) of absolute sea level trends in the Arctic",
        "abstract": "Abstract. The Arctic Ocean is at the frontier of the fast-changing climate in the northern latitudes, and sea level trends are a bulk measure of ongoing\nprocesses related to climate change. Observations of sea level in the Arctic Ocean are nonetheless difficult to validate with independent measurements,\nand this is globally the region where the sea level trend (SLT) is most uncertain. The aim of this study is to create a satellite-independent\nreconstruction of Arctic SLT, as it is observed by altimetry and tide gauges (TGs). Previous studies use Gravity Recovery and Climate Experiment\n(GRACE) observations to estimate the manometric (mass component of) SLT. GRACE estimates, however, are challenged by large mass changes on land, which\nare difficult to separate from much smaller ocean mass changes. Furthermore, GRACE is not available before 2003, which significantly limits the period\nand makes the trend more vulnerable to short-term changes. As an alternative approach, this study estimates the climate-change-driven Arctic\nmanometric SLT from the Arctic sea level fingerprints of glaciers, Greenland, Antarctica and glacial isostatic adjustment (GIA) with the addition of the\nlong-term inverse barometer (IB) effect. The halosteric and thermosteric components complete the reconstructed Arctic SLT and are estimated by\ninterpolating 300‚Äâ000 temperature¬†(T) and salinity¬†(S) in situ observations. The SLT from 1995‚Äì2015 is compared to the observed SLT from altimetry and 12¬†selected tide gauges (TGs) corrected for vertical land movement\n(VLM). The reconstructed estimate manifests the salinity-driven halosteric component as dominating the spatial SLT pattern with variations\nbetween ‚àí7 and 10‚Äâmm‚Äâyr‚àí1. The manometric SLT in comparison is estimated to be 1‚Äì2‚Äâmm‚Äâyr‚àí1 for most of the Arctic Ocean. The\nreconstructed SLT shows a larger sea level rise in the Beaufort Sea compared to altimetry, an issue that is also identified by previous studies. There is a\nTG-observed sea level rise in the Siberian Arctic in contrast to the sea level fall from the reconstructed and altimetric estimate. From 1995‚Äì2015 the reconstructed SLT agrees within the 68‚Äâ% confidence interval with the SLT from observed altimetry in 87‚Äâ% of the\nArctic between 65‚àò‚ÄâN and 82‚àò‚ÄâN (R=0.50) and with¬†5 of 12¬†TG-derived (VLM-corrected) SLT estimates. The residuals are seemingly\nsmaller than results from previous studies using GRACE estimates and modeled T‚ÄìS data. The spatial correlation of the reconstructed SLT to\naltimetric SLT during the GRACE period (2003‚Äì2015) is R=0.38 and R=0.34/R=0.37 if GRACE estimates are used instead of the constructed\nmanometric component. Thus, the reconstructed manometric component is suggested as a legitimate alternative to GRACE that can be projected into the\npast and future.\n",
        "authors": [
            "Carsten Bjerre Ludwigsen",
            "O. Andersen",
            "S. Rose"
        ],
        "citations": 3,
        "references": 88,
        "year": 2022
    },
    {
        "title": "VLMS based Channel Estimator for OTFS VLC System",
        "abstract": "Visible light communication (VLC) is an eco-friendly and low-cost emerging technology for beyond 5G communication systems. However, it has been found that the performance of visible light based communication system deteriorates significantly due to the non-linearity of light-emitting-diode (LED) and Doppler spread due to relative mobility between the transmitter and the receiver. In contrast to the conventional orthogonal frequency division multiplexing (OFDM) scheme, recently proposed orthogonal time frequency space (OTFS) modulation scheme addresses the problem of distortion due to mobility. However, LED non-linearity causes degradation in the channel estimation and overall bit error rate (BER) performance of the VLC system. These non-linear distortions significantly deteriorate the signal reception, thereby resulting in poor estimation of channel state information (CSI). Traditional linear channel estimation schemes such as zero-forcing (ZF) and minimum mean square error (MMSE) have been proposed for OTFS, however, they perform poorly in the presence of non-linearity. In this paper, we have analyzed the performance of OTFS employing Volterra least mean square (VLMS) based channel estimator for non-linear VLC system. Simulations performed over the mobile VLC channel model modelled by random way-point channel model indicate that OTFS with VLMS can be employed for channel estimation for VLC system impaired due to user mobility and non-linear LED characteristic.",
        "authors": [
            "Anupma Sharma",
            "V. Bhatia"
        ],
        "citations": 1,
        "references": 28,
        "year": 2022
    }
]