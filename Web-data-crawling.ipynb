{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978a6831",
   "metadata": {},
   "source": [
    "## Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cf7e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 0/1000 papers for 'Foundation Models' (2017-2021)Fetched 0/1000 papers for 'Generative Models' (2017-2021)\n",
      "\n",
      "Fetched 0/1000 papers for 'LLM' (2017-2021)\n",
      "Fetched 0/1000 papers for 'VLM' (2017-2021)\n",
      "Fetched 0/1000 papers for 'Diffusion Models' (2017-2021)\n",
      "Fetched 200/1000 papers for 'LLM' (2017-2021)\n",
      "Fetched 200/1000 papers for 'VLM' (2017-2021)\n",
      "Fetched 400/1000 papers for 'LLM' (2017-2021)\n",
      "Fetched 400/1000 papers for 'VLM' (2017-2021)\n",
      "No more papers found for 'VLM' (2017-2021).\n",
      "Saved 391 papers to scraped_data\\VLM_2017-2021.json\n",
      "Fetched 0/1609 papers for 'VLM' (2022-2024)\n",
      "Fetched 600/1000 papers for 'LLM' (2017-2021)\n",
      "Fetched 200/1000 papers for 'Diffusion Models' (2017-2021)\n",
      "Fetched 200/1000 papers for 'Foundation Models' (2017-2021)\n",
      "Fetched 800/1000 papers for 'LLM' (2017-2021)\n",
      "Fetched 200/1000 papers for 'Generative Models' (2017-2021)\n",
      "Fetched 200/1609 papers for 'VLM' (2022-2024)\n",
      "Fetched 1000/1000 papers for 'LLM' (2017-2021)\n",
      "No more papers found for 'LLM' (2017-2021).\n",
      "Saved 957 papers to scraped_data\\LLM_2017-2021.json\n",
      "Fetched 0/1043 papers for 'LLM' (2022-2024)\n",
      "Fetched 400/1609 papers for 'VLM' (2022-2024)\n",
      "Fetched 400/1000 papers for 'Foundation Models' (2017-2021)\n",
      "Fetched 400/1000 papers for 'Diffusion Models' (2017-2021)\n",
      "Fetched 600/1609 papers for 'VLM' (2022-2024)\n",
      "Fetched 200/1043 papers for 'LLM' (2022-2024)\n",
      "Fetched 800/1609 papers for 'VLM' (2022-2024)\n",
      "Fetched 600/1000 papers for 'Foundation Models' (2017-2021)\n",
      "Fetched 400/1000 papers for 'Generative Models' (2017-2021)\n",
      "Fetched 1000/1609 papers for 'VLM' (2022-2024)\n",
      "No more papers found for 'VLM' (2022-2024).\n",
      "Saved 1000 papers to scraped_data\\VLM_2022-2024.json\n",
      "✅ 391/2000 papers fetched for 'VLM'\n",
      "Fetched 600/1000 papers for 'Diffusion Models' (2017-2021)\n",
      "504 Gateway Timeout. Retrying...\n",
      "Fetched 400/1043 papers for 'LLM' (2022-2024)\n",
      "Fetched 800/1000 papers for 'Foundation Models' (2017-2021)\n",
      "Fetched 800/1000 papers for 'Diffusion Models' (2017-2021)\n",
      "Fetched 600/1043 papers for 'LLM' (2022-2024)\n",
      "504 Gateway Timeout. Retrying...\n",
      "Fetched 600/1000 papers for 'Generative Models' (2017-2021)\n",
      "504 Gateway Timeout. Retrying...\n",
      "Saved 1000 papers to scraped_data\\Foundation_Models_2017-2021.json\n",
      "Fetched 0/1000 papers for 'Foundation Models' (2022-2024)\n",
      "Saved 1000 papers to scraped_data\\Diffusion_Models_2017-2021.json\n",
      "Fetched 0/1000 papers for 'Diffusion Models' (2022-2024)\n",
      "504 Gateway Timeout. Retrying...\n",
      "Fetched 800/1043 papers for 'LLM' (2022-2024)\n",
      "Fetched 200/1000 papers for 'Foundation Models' (2022-2024)\n",
      "Fetched 800/1000 papers for 'Generative Models' (2017-2021)\n",
      "Fetched 200/1000 papers for 'Diffusion Models' (2022-2024)\n",
      "Fetched 400/1000 papers for 'Foundation Models' (2022-2024)\n",
      "Fetched 1000/1043 papers for 'LLM' (2022-2024)\n",
      "No more papers found for 'LLM' (2022-2024).\n",
      "Saved 1000 papers to scraped_data\\LLM_2022-2024.json\n",
      "✅ 957/2000 papers fetched for 'LLM'\n",
      "Fetched 400/1000 papers for 'Diffusion Models' (2022-2024)\n",
      "504 Gateway Timeout. Retrying...\n",
      "Fetched 600/1000 papers for 'Foundation Models' (2022-2024)\n",
      "Fetched 600/1000 papers for 'Diffusion Models' (2022-2024)\n",
      "Fetched 800/1000 papers for 'Foundation Models' (2022-2024)\n",
      "Saved 1000 papers to scraped_data\\Generative_Models_2017-2021.json\n",
      "Fetched 0/1000 papers for 'Generative Models' (2022-2024)\n",
      "Fetched 800/1000 papers for 'Diffusion Models' (2022-2024)\n",
      "Fetched 200/1000 papers for 'Generative Models' (2022-2024)\n",
      "Saved 1000 papers to scraped_data\\Foundation_Models_2022-2024.json\n",
      "✅ 1000/2000 papers fetched for 'Foundation Models'\n",
      "Saved 1000 papers to scraped_data\\Diffusion_Models_2022-2024.json\n",
      "✅ 1000/2000 papers fetched for 'Diffusion Models'\n",
      "Fetched 400/1000 papers for 'Generative Models' (2022-2024)\n",
      "Fetched 600/1000 papers for 'Generative Models' (2022-2024)\n",
      "Fetched 800/1000 papers for 'Generative Models' (2022-2024)\n",
      "Saved 1000 papers to scraped_data\\Generative_Models_2022-2024.json\n",
      "✅ 1000/2000 papers fetched for 'Generative Models'\n",
      "✅ Completed scraping all topics!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "RESEARCH_TOPICS = [\"Foundation Models\", \"Generative Models\", \"LLM\", \"VLM\", \"Diffusion Models\"]\n",
    "OUTPUT_DIR = \"scraped_data\"\n",
    "YEAR_RANGES = [(2017, 2021), (2022, 2024)]  # (Start Year, End Year)\n",
    "TOTAL_PAPERS_PER_TOPIC = 2000\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Function to fetch papers from Semantic Scholar API\n",
    "def fetch_papers(topic, start_year, end_year, max_papers):\n",
    "    api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    papers = []\n",
    "    offset = 0\n",
    "\n",
    "    while len(papers) < max_papers:\n",
    "        if offset % 200 == 0:\n",
    "            print(f\"Fetched {offset}/{max_papers} papers for '{topic}' ({start_year}-{end_year})\")\n",
    "\n",
    "        params = {\n",
    "            \"query\": topic,\n",
    "            \"fields\": \"title,abstract,authors,references,citations,year\",\n",
    "            \"offset\": offset,\n",
    "            \"limit\": 100,\n",
    "            \"year\": f\"{start_year}-{end_year}\",\n",
    "            \"sort\": \"relevance\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_url, params=params)\n",
    "\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit reached. Pausing for 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "\n",
    "            elif response.status_code == 504:\n",
    "                print(\"504 Gateway Timeout. Retrying...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            data = response.json().get(\"data\", [])\n",
    "            if not data:\n",
    "                print(f\"No more papers found for '{topic}' ({start_year}-{end_year}).\")\n",
    "                break\n",
    "\n",
    "            for paper in data:\n",
    "                papers.append({\n",
    "                    \"title\": paper.get(\"title\", \"No Title\"),\n",
    "                    \"abstract\": paper.get(\"abstract\", \"No Abstract\"),\n",
    "                    \"authors\": [author.get(\"name\", \"Unknown\") for author in paper.get(\"authors\", [])],\n",
    "                    \"citations\": len(paper.get(\"citations\", [])),\n",
    "                    \"references\": len(paper.get(\"references\", [])),\n",
    "                    \"year\": paper.get(\"year\", \"Unknown\")\n",
    "                })\n",
    "\n",
    "                if len(papers) >= max_papers:\n",
    "                    break\n",
    "\n",
    "            offset += 100\n",
    "            time.sleep(5)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Network error: {e}. Retrying in 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "\n",
    "    return papers\n",
    "\n",
    "# Function to save data as JSON\n",
    "def save_to_json(topic, year_range, papers):\n",
    "    if not papers:\n",
    "        print(f\"No data to save for '{topic}' ({year_range})\")\n",
    "        return\n",
    "\n",
    "    filename = os.path.join(OUTPUT_DIR, f\"{topic.replace(' ', '_')}_{year_range}.json\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(papers, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Saved {len(papers)} papers to {filename}\")\n",
    "\n",
    "# Function to fetch and save papers for a given topic while adjusting year range compensation\n",
    "def fetch_and_save_with_compensation(topic):\n",
    "    total_fetched = 0\n",
    "    all_papers = []\n",
    "\n",
    "    # Fetch first range (2017-2021)\n",
    "    start_year, end_year = YEAR_RANGES[0]\n",
    "    papers_2017_2021 = fetch_papers(topic, start_year, end_year, 1000)\n",
    "    total_fetched += len(papers_2017_2021)\n",
    "    all_papers.extend(papers_2017_2021)\n",
    "    save_to_json(topic, f\"{start_year}-{end_year}\", papers_2017_2021)\n",
    "\n",
    "    # Fetch second range (2022-2024), adjusting the limit if needed\n",
    "    start_year, end_year = YEAR_RANGES[1]\n",
    "    remaining_papers_needed = TOTAL_PAPERS_PER_TOPIC - total_fetched\n",
    "    if remaining_papers_needed > 0:\n",
    "        papers_2022_2024 = fetch_papers(topic, start_year, end_year, remaining_papers_needed)\n",
    "        all_papers.extend(papers_2022_2024)\n",
    "        save_to_json(topic, f\"{start_year}-{end_year}\", papers_2022_2024)\n",
    "\n",
    "    print(f\"✅ {total_fetched}/{TOTAL_PAPERS_PER_TOPIC} papers fetched for '{topic}'\")\n",
    "\n",
    "# Multithreading function to scrape data for all topics\n",
    "def fetch_and_save_papers_multithreaded():\n",
    "    threads = []\n",
    "\n",
    "    for topic in RESEARCH_TOPICS:\n",
    "        thread = threading.Thread(target=fetch_and_save_with_compensation, args=(topic,))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    print(\"✅ Completed scraping all topics!\")\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_and_save_papers_multithreaded()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0b7c8",
   "metadata": {},
   "source": [
    "- **Objective**\n",
    "    The goal of this stage is to **efficiently retrieve research papers** from the **Semantic Scholar API** by relevance, covering five research topics:  \n",
    "\n",
    "    - **Foundation Models**  \n",
    "    - **Generative Models**  \n",
    "    - **LLM (Large Language Models)**  \n",
    "    - **VLM (Vision-Language Models)**  \n",
    "    - **Diffusion Models**  \n",
    "\n",
    "    The script ensures **balanced data collection** from two year ranges \\((2017-2021)\\) and \\((2022-2024)\\), while **giving special attention to recent years** to reflect the latest advancements.  \n",
    "\n",
    "- **Approach and Implementation**\n",
    "\n",
    "  - **1. Data Collection Strategy**\n",
    "    The API allows querying papers based on a topic and sorting them by relevance. To ensure we gather enough data:  \n",
    "\n",
    "    - **Years 2017-2021**: We attempt to collect **1000 papers per topic**.  \n",
    "    - **Years 2022-2024**: We aim for **1000 additional papers per topic**, **prioritizing recent research trends**.  \n",
    "    - If **fewer than 1000 papers are found** in the first range, the second range **compensates** to ensure a total of **2000 papers per topic**.  \n",
    "\n",
    "    $$\n",
    "    P_{\\text{total}} = P_{2017-2021} + P_{2022-2024}\n",
    "    $$\n",
    "    $$\n",
    "    P_{\\text{total}} \\geq 2000, \\quad \\text{if possible}\n",
    "    $$\n",
    "\n",
    "    Since AI research is evolving rapidly, **we place greater emphasis on collecting papers from 2022-2024** to capture **state-of-the-art methodologies and breakthroughs**.\n",
    "\n",
    "\n",
    "  - **2. API Querying & Pagination**\n",
    "    Each request retrieves up to **100 papers** (API limit), so pagination is necessary. We use the `offset` parameter to **incrementally fetch results** until the required number of papers is collected or no more are available.  \n",
    "\n",
    "    - **Rate Limiting Handling**: If the API responds with **429 Too Many Requests**, the script **pauses for 10 seconds** before retrying.  \n",
    "    - **Error Handling**: Network errors and failed responses are caught and logged, ensuring robust execution.  \n",
    "\n",
    "  - **3. Constants**\n",
    " \n",
    "      Several constants define the scraping behavior:  \n",
    "\n",
    "      $$\n",
    "      \\text{RESEARCH\\_TOPICS} = \\{ \\text{\"Foundation Models\"}, \\text{\"Generative Models\"}\n",
    "      $$\n",
    "      $$\n",
    "        \\text{\"LLM\"}, \\text{\"VLM\"}, \\text{\"Diffusion Models\"} \\}\n",
    "      $$\n",
    "\n",
    "      $$\n",
    "      \\text{YEAR\\_RANGES} = \\{ (2017, 2021), (2022, 2024) \\}\n",
    "      $$\n",
    "\n",
    "      $$\n",
    "      P_{\\text{total}} = 2000 \\quad \\text{(papers per topic, if available)}\n",
    "      $$\n",
    "\n",
    "      $$\n",
    "      \\text{OUTPUT\\_DIR} = \"scraped\\_data\"\n",
    "      $$\n",
    "\n",
    "      We ensure that if older papers are insufficient, **recent papers (2022-2024) fill the gap**, reinforcing the importance of the latest research.\n",
    "\n",
    "\n",
    "  - **4. Multithreading for Efficiency**\n",
    "  \n",
    "      Fetching data sequentially for multiple topics would be **slow** due to API rate limits and network latency. To overcome this, we use **multithreading**:  \n",
    "\n",
    "      - Each topic is **assigned a separate thread** to run the `fetch_and_save_with_compensation()` function concurrently.  \n",
    "      - This allows multiple API requests to run **in parallel**, significantly reducing total execution time.  \n",
    "      - Threads are synchronized using `thread.join()` to ensure all data is fetched before the script completes.  \n",
    "\n",
    "      Let **\\( T \\)** be the number of research topics and **\\( N \\)** be the number of threads:  \n",
    "      $$\n",
    "      T = 5, \\quad N = 5\n",
    "      $$\n",
    "      $$\n",
    "      \\text{Total Execution Time} \\approx \\frac{\\text{Single Topic Fetch Time}}{N}\n",
    "      $$\n",
    "\n",
    "      Since research in AI is **progressing rapidly**, prioritizing the latest papers (2022-2024) ensures our dataset reflects cutting-edge developments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d410b",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b25fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database and table created successfully.\n",
      "✅ Inserted 1000 papers into database for topic 'Diffusion Models' (2017-2021).\n",
      "✅ Inserted 1000 papers into database for topic 'Diffusion Models' (2022-2024).\n",
      "✅ Inserted 1000 papers into database for topic 'Foundation Models' (2017-2021).\n",
      "✅ Inserted 1000 papers into database for topic 'Foundation Models' (2022-2024).\n",
      "✅ Inserted 1000 papers into database for topic 'Generative Models' (2017-2021).\n",
      "✅ Inserted 1000 papers into database for topic 'Generative Models' (2022-2024).\n",
      "✅ Inserted 957 papers into database for topic 'LLM' (2017-2021).\n",
      "✅ Inserted 1000 papers into database for topic 'LLM' (2022-2024).\n",
      "✅ Inserted 391 papers into database for topic 'VLM' (2017-2021).\n",
      "✅ Inserted 1000 papers into database for topic 'VLM' (2022-2024).\n",
      "✅ All JSON files processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "# Constants\n",
    "DB_FILE = \"papers.db\"\n",
    "INPUT_DIR = \"scraped_data\"\n",
    "\n",
    "# Function to create the database and table\n",
    "def create_database():\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT,\n",
    "            abstract TEXT,\n",
    "            authors TEXT,\n",
    "            citations INTEGER,\n",
    "            references_count INTEGER,  \n",
    "            year INTEGER,\n",
    "            topic TEXT,\n",
    "            year_range TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"✅ Database and table created successfully.\")\n",
    "\n",
    "# Function to insert data into the database\n",
    "def insert_data(papers, topic, year_range):\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for paper in papers:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO papers (title, abstract, authors, citations, references_count, year, topic, year_range)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            paper.get(\"title\", \"No Title\"),\n",
    "            paper.get(\"abstract\", \"No Abstract\"),\n",
    "            \", \".join(paper.get(\"authors\", [\"Unknown\"])),\n",
    "            paper.get(\"citations\", 0),\n",
    "            paper.get(\"references\", 0),  # Updated key to match new column name\n",
    "            paper.get(\"year\", None),\n",
    "            topic,\n",
    "            year_range\n",
    "        ))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"✅ Inserted {len(papers)} papers into database for topic '{topic}' ({year_range}).\")\n",
    "\n",
    "# Function to read all JSON files and save data to database\n",
    "def process_json_files():\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"❌ Error: Directory '{INPUT_DIR}' not found!\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(INPUT_DIR):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(INPUT_DIR, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                    data = json.load(file)\n",
    "                \n",
    "                if not data:\n",
    "                    print(f\"⚠️ Warning: No data in {filename}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract topic and year range from filename\n",
    "                parts = filename.replace(\".json\", \"\").split(\"_\")\n",
    "                topic = \" \".join(parts[:-1])  # Extracts topic name\n",
    "                year_range = parts[-1]  # Extracts year range\n",
    "\n",
    "                # Insert into database\n",
    "                insert_data(data, topic, year_range)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {filename}: {e}\")\n",
    "\n",
    "# Run the process\n",
    "if __name__ == \"__main__\":\n",
    "    create_database()\n",
    "    process_json_files()\n",
    "    print(\"✅ All JSON files processed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
